--- ABSTRACT ---
최근 연구에서는 대량의 쌍을 이룬 텍스트-오디오 데이터를 사용하여 텍스트-오디오 합성을 연구했습니다. 그러나 고품질 텍스트 주석이 있는 오디오 녹음은 획득하기 어려울 수 있습니다. 이 연구에서는 레이블이 지정되지 않은 비디오와 사전 학습된 언어-비전 모델을 사용하여 텍스트-오디오 합성에 접근합니다. 우리는 시각적 모달리티를 브리지로 활용하여 원하는 텍스트-오디오 대응 관계를 학습하는 것을 제안합니다. 우리는 사전 학습된 대조적 언어-이미지 사전 학습(CLIP) 모델로 인코딩된 비디오 프레임이 주어지면 비디오의 오디오 트랙을 생성하는 조건부 확산 모델을 학습합니다. 테스트 시간에는 먼저 제로 샷 모달리티 전송을 수행하고 CLIP 인코딩된 텍스트 쿼리로 확산 모델을 조건화합니다. 그러나 이미지 쿼리와 관련하여 눈에 띄는 성능 저하를 관찰합니다. 이 격차를 메우기 위해 사전 학습된 확산 사전 모델을 추가로 채택하여 CLIP 텍스트 임베딩이 주어진 CLIP 이미지 임베딩을 생성합니다. 우리의 결과는 제안된 방법의 효과성을 보여주며 사전 학습된 확산 사전 모델이 모달리티 전송 격차를 줄일 수 있음을 보여줍니다. 텍스트-오디오 합성에 집중하는 반면, 제안된 모델은 이미지 쿼리에서 오디오를 생성할 수도 있으며, 주관적 청취 테스트에서 최첨단 이미지-오디오 합성 모델과 경쟁적인 성과를 보였습니다. 이 연구는 비디오에서 자연스럽게 발생하는 오디오-비주얼 대응과 사전 학습된 언어-비전 모델의 힘을 활용하는 텍스트-오디오 합성에 접근하는 새로운 방향을 제시합니다. 색인 용어 사운드 합성, 오디오 생성, 멀티모달 학습, 확산 모델, 신경망, 머신 러닝 1.
--- INTRODUCTION ---
생성 모델링[1–3]과 언어-오디오 대조 학습[4–6]의 발전으로 다양한 딥 러닝 기반 텍스트-오디오 합성 시스템이 최근 등장했습니다[7–12]. 그러나 이러한 시스템은 일반적으로 학습을 위해 대량의 텍스트-오디오 쌍 데이터가 필요합니다. 광범위한 인간 주석 노력에도 불구하고 현재 가장 큰 공개 텍스트-오디오 데이터 세트에는 약 630k 텍스트-오디오 쌍이 포함되어 있습니다[4]. 텍스트-이미지 데이터에 비해 웹에서 텍스트-오디오 데이터가 상대적으로 부족하다는 점을 감안할 때, 텍스트-오디오 데이터 세트를 대규모 텍스트-이미지 데이터 세트(예: 58억 5천만 개의 텍스트-이미지 쌍이 포함된 LAION-5B 데이터 세트[13])와 비슷한 크기로 확장할 수 있는지 여부는 불분명합니다. 이 연구에서는 비디오에서 자연스럽게 발생하는 오디오-비주얼 대응과 사전 학습된 언어-비전 모델에서 학습한 다중 모달 표현을 활용하여 텍스트-오디오 쌍 없이 텍스트-오디오 합성에 접근합니다(그림 1 참조). 제안된 CLIPSonic 모델은 조건부 확산 모델[15], 구성적 언어-이미지 사전 학습(CLIP) * Dolby에서 인턴십을 하는 동안 수행한 작업. Hao-Wen은 박사 학위 연구를 지원해 준 대만 교육부에 감사드립니다. 연락처: hwdong@ucsd.edu 비디오에서 자연스럽게 발생하는 시청각적 대응 비디오 프레임 오디오 텍스트 기차 휘파람 소리 사진 사전 학습된 시각-언어 모델 원하는 오디오-텍스트 대응 그림 1: 비디오의 시청각적 대응과 사전 학습된 언어-비전 모델에서 학습한 다중 모달 표현을 활용하여 텍스트-오디오 대응을 학습합니다. 그림 2에서 볼 수 있듯이. 비디오가 주어지면, CLIPSonic은 비디오에서 무작위로 선택한 CLIP 인코딩된 프레임이 주어지면 오디오의 멜 스펙트로그램을 합성하도록 학습됩니다. CLIP은 이미지와 텍스트를 교차 모달 의미 공간에 임베딩하기 때문에 CLIPSonic은 CLIP 임베딩 공간을 오디오에 매핑하는 방법을 학습합니다. 테스트 시간에는 먼저 제로샷 모달리티 전송을 수행하고 CLIP 인코딩된 텍스트 쿼리로 직접 확산 모델을 조절하는 방법을 살펴봅니다. 그러나 실제로는 이미지 쿼리와 관련하여 눈에 띄는 성능 저하를 관찰합니다. 이 격차를 메우기 위해 사전 학습된 확산 사전 모델을 채택하여 CLIP 텍스트 임베딩이 주어진 CLIP 이미지 임베딩을 생성합니다. 제안된 시스템에는 조건부 확산 모델을 학습하기 위한 1) 레이블이 지정되지 않은 비디오와 언어-비전 모델을 사전 학습하기 위한 2) 이미지-텍스트 쌍만 필요합니다. 주관적 청취 테스트와 객관적 평가를 통해 실험 결과는 제안된 방법의 효과를 보여줍니다. 오디오 샘플은 데모 웹사이트에서 제공됩니다. 저희 연구는 여러 면에서 이전 연구와 다릅니다. 기존의 텍스트-오디오 모델은 대량의 텍스트-오디오 학습 쌍[7-12]에 의존하는 반면, CLIPSonic은 텍스트-오디오 쌍 없이 텍스트 쿼리 오디오 합성을 학습합니다. 이전 연구에서는 이미지-오디오 합성[1820]을 연구했지만 텍스트와 이미지 간의 제로샷 모달리티 전송을 조사하지 않았습니다. CLIPSep[21]과 CLIPSynth[22]는 각각 레이블이 지정되지 않은 비디오에서 텍스트 쿼리 소스 분리 및 오디오 합성을 학습하는 것을 제안하지만 제로샷 모달리티 전송 갭 문제는 다루지 않습니다. DALL-E 2[17]는 CLIP 기반 텍스트-이미지 합성에서 제로샷 모달리티 전송 갭을 해결하기 위해 확산 사전 모델을 제안하고, 비디오에서 학습한 지식을 텍스트-오디오 합성에 전송하기 위해 사전 학습된 확산 사전 모델을 활용하는 방법을 살펴봅니다. 기타
--- RELATED WORK ---
s는 AudioLDM[9] 및 MusicLM[12]으로, 제로샷 오디오-텍스트 모달리티 전송을 수행하기 위해 언어-오디오 모델[4,5]에 의존하지만 이러한 언어-오디오 모델은 오디오-텍스트 쌍에 대해 훈련됩니다. 한국어: https://salu133445.github.io/clipsonic/ CLIP-image(사전 학습됨; 고정) imgInput 비디오 Xt Xt-기차 휘파람 소리 사진 CLIP-text(사전 학습됨; 고정) Itext(b) 추론 - CLIPSonic-ZS(제로샷 전이) ✗t U-Net CLIP-text 확산 사전(사전 학습됨; 고정) (사전 학습됨; 고정) Xt 확산 모델(t = T, ..., 1) 기차 휘파람 소리 사진 + qtext img U-Net 확산 모델(t = T, ..., 1) U-Net Xt-Xt-(a) 학습 - CLIPSonic(c) 추론 CLIPSonic-PD(사전 학습된 확산 사전) 확산 모델(t=T,..., 1) 그림 2: 제안된 CLIPSonic 모델. 훈련 중에 CLIPSonic은 비디오 프레임의 이미지가 주어지면 비디오의 오디오 트랙을 합성하는 방법을 학습합니다. 추론 시간에, 우리는 텍스트-오디오 합성에 접근하기 위해 &quot;[레이블]의 사진&quot; 형태의 텍스트 쿼리를 제공하거나 사전 훈련된 확산 사전 모델을 사용하여 텍스트 쿼리(추론에 사용됨)와 이미지 쿼리(훈련에 사용됨) 간의 격차를 메웁니다. Xt는 확산 단계 t에서의 노이즈가 있는 스펙트로그램을 나타냅니다. 생성된 멜 스펙트로그램 ✰o는 사전 훈련된 BigVGAN 모델[14]에 의해 파형으로 다시 반전됩니다. 2. CLIPSonic 이 섹션에서는 레이블이 지정되지 않은 비디오에서 텍스트-오디오 합성을 학습하기 위한 제안된 CLIPSonic 모델을 소개합니다. 그림 2(a)에서 볼 수 있듯이, CLIPSonic은 오디오 합성을 위해 멜 스펙트로그램 기반 확산 모델을 사용합니다. 우리는 오디오 합성에서 강력한 성능을 보이는 확산 프레임워크를 채택합니다[9,23,24]. 비디오가 주어지면, CLIPSonic은 무작위로 추출된 비디오 프레임의 이미지에서 오디오의 멜 스펙트로그램을 합성하도록 훈련됩니다. 구체적으로, 우리는 먼저 이미지를 쿼리 벡터 Qimg로 인코딩하기 위한 사전 학습된 CLIP 이미지 인코더. 그런 다음 이 쿼리 벡터는 조건 신호로 사용되어 확산 모델을 안내하여 멜 스펙트로그램 ✰o를 생성합니다. 우리는 추론 중에 안내 수준 변수 w를 통해 조절 신호의 정도를 제어할 수 있는 잡음 제거 확산 확률 모델[15]과 분류기 없는 안내[25]를 채택합니다. 생성된 멜 스펙트로그램은 별도로 학습된 BigVGAN[14]을 사용하여 파형으로 다시 반전됩니다. 우리는 파형보다 차원이 낮고 BigVGAN이 멜 스펙트로그램에서 일반 오디오를 합성할 때 좋은 품질을 보여주기 때문에 멜 스펙트로그램 도메인에서 확산을 수행하기로 선택했습니다. CLIPSonic-ZS(제로샷 모달리티 전송). 추론 시간에는 CLIP에서 학습한 언어-비전 임베딩 공간을 활용하여 텍스트-오디오 합성을 달성하는 것을 목표로 합니다. CLIPSonic-ZS는 텍스트 쿼리를 제로샷 모달리티 전송 설정에서 사용하는 방법으로 CLIP 이미지 임베딩을 CLIP 텍스트 임베딩으로 바꾸는 것을 탐구합니다. 그림 2(b)에서 볼 수 있듯이, CLIP 텍스트 인코더를 사용하여 입력 텍스트 쿼리를 쿼리 벡터 qtext로 인코딩하고, 이를 확산 모델에 조건으로 공급합니다. 이 모델을 CLIPSonic-ZS라고 하며, 여기서 &quot;ZS&quot;는 제로샷 모달리티 전송을 의미합니다. CLIPSonic-PD(사전 학습된 확산 사전). 섹션 4에서 볼 수 있듯이, CLIP의 텍스트와 이미지 임베딩 공간 사이에 모달리티 갭이 있음을 관찰합니다. DALL-E 2[17]에 따라 확산 사전 모델을 사용하여 이 갭을 메우는 것을 탐구합니다. 그림 2(c)에 나와 있듯이, 먼저 입력 텍스트 쿼리를 CLIP 텍스트 임베딩 벡터 qtext로 인코딩한 다음, 사전 학습된 확산 사전 모델을 사용하여 ɖtext에서 CLIP 이미지 임베딩 벡터 img를 생성합니다. 생성된 쿼리 벡터 img는 컨디셔닝 신호로 확산 모델에 전달됩니다. 이 2 다음 공식을 사용합니다. Vx log pw(x|q) = (1 - w)√x log p(x) + wVx log p(xq). w가 클수록 컨디셔닝 신호가 강해지고, w = 1은 분류기 없는 안내가 없는 조건부 모델에 해당합니다. 이 모델을 CLIPSonic-PD(사전 학습된 확산 사전)라고 합니다. CLIPSonic-ZS와 CLIPSonic-PD는 모두 학습을 위해 텍스트-오디오 쌍이 필요하지 않습니다. 또한 CLIP과 확산 사전 모델은 모두 텍스트-이미지 쌍만 사용하여 사전 학습할 수 있으므로 쌍을 이룬 오디오-텍스트 데이터가 필요하지 않습니다. CLIPSonic-IQ와 CLIPSonic-SD. 여기에서는 텍스트-오디오에 초점을 맞추지만, CLIPSonic은 qimg 쿼리를 사용하여 이미지-오디오 합성 모델로도 사용할 수 있습니다. 이 변형을 CLIPSonic-IQ(이미지 쿼리)라고 합니다. 게다가 도메인별 데이터 세트에서 확산 사전 모델을 처음부터 학습하는 것이 가능하다는 것을 알게 되었고, 따라서 데이터 세트에서 텍스트-이미지 쌍을 사용하여 확산 사전 모델을 처음부터 학습하는 CLIPSonicSD(지도 확산 사전)라는 변형도 고려합니다. 섹션 3에서 설명하겠지만, 이 작업에서 CLIPSonic-SD에서 확산 사전을 학습하는 데 사용된 텍스트 데이터는 오디오 레이블에서 나오기 때문에 CLIPSonic-SD는 CLIPSonic-PD에 대한 오라클 모델 역할을 합니다. CLIPSonic-PD와 CLIPSonic-SD를 비교하여, 대상 데이터 세트에서 학습된 모델과 대량의 데이터에서 사전 학습된 확산 사전 모델을 사용하는 것의 효과를 연구하고자 합니다. 3. 실험 설정 데이터. 우리는 VGGSound[26]와 MUSIC[27]의 두 데이터 세트를 고려합니다. VGGSound 데이터 세트는 171,899개의 10초 YouTube 비디오로 구성되어 있으며, 야외에서 310개의 사운드 클래스를 포함하고 있으며, 데이터 세트와 함께 제공된 traintest 분할을 따릅니다. MUSIC 데이터 세트는 총 21개의 악기 유형을 사용하여 악기를 연주하는 사람들의 전체 길이 YouTube 비디오 1,055개로 구성되어 있습니다. 우리는 데이터 세트를 무작위로 9:1 train-test 분할로 분할했습니다. VGGSound는 야외에서 비정형 소스에서 수집한 크고 다양한 데이터 세트를 나타내는 반면, MUSIC은 관심 있는 특정 도메인의 작고 큐레이팅된 데이터 세트를 나타냅니다. 두 데이터 세트 모두 클래스 레이블만 제공되므로 이러한 레이블을 &quot;[레이블]의 사진&quot; 형태의 가상 텍스트로 변환합니다. 기준 모델. 우리는 CLIPSonic 모델을 다음의 텍스트-오디오(TTA) 및 재구성 모델과 비교합니다. CLIP-TTA는 텍스트-오디오 쌍을 사용하여 학습하는 CLIPSonic의 지도 학습 버전입니다. 사전 훈련된 CLIP 텍스트 임베딩은 조건화로 사용됩니다. ⚫ CLAP-TTA는 CLIP-TTA와 동일하지만 사전 훈련된 CLAP 텍스트 임베딩[4]을 사용합니다. 여기서 우리는 &quot;[레이블]의 소리&quot; 형태의 프롬프트를 사용합니다. CLIP-text 임베딩과 달리 CLAP-text Fréchet 오디오 거리(FAD) 6 5 4 3(a) FAD(VGGSound) 0.(b) CLAP 점수↑(VGGSound) 0.CLAP 점수 0.0.0.0.Fréchet 오디오 거리(FAD) 0.안내 수준(w) 안내 수준(w) CLIPSonic-IQ(이미지 쿼리) CLIPSonic-SD(지도 확산 사전) (c) FAD(음악) 0.0.30 CLAP 점수 0.0.0.0.260.안내 수준(w) CLIP-TTA CLAP-TTA 안내 수준(w) BigVGAN 재구성 (d) CLAP 점수↑(음악)• CLIPSonic-ZS(제로샷 모달리티 전송) CLIPSonic-PD(사전 학습된 확산 사전) 그림 3: 객관적 VGGSound 및 MUSIC에 대한 평가 결과. 임베딩은 시각적 접지 기능이 아닌 오디오 접지 기능을 인코딩할 것으로 예상됩니다. BigVGAN 멜 스펙트로그램 재구성은 BigVGAN 모델에 의해 기준 진실 멜 스펙트로그램에서 재구성된 파형입니다. 이는 BigVGAN을 역 모델로 사용하는 스펙트로그램 기반 합성 시스템의 상한으로 사용됩니다. 구현 세부 정보. 멜 스펙트로그램 계산의 경우 16kHz의 샘플링 속도, 512의 홉 크기, 2048의 FFT 필터 크기 및 64개의 멜 대역을 사용합니다. 학습하는 동안 2초의 오디오에 해당하는 64×64 크기의 멜 스펙트로그램을 사용합니다. 확산 모델의 경우 [15]에서 제안한 네트워크 아키텍처를 따르고 [28]의 오픈 소스 코드를 사용합니다. 학습하는 동안 4000개의 확산 단계와 추론 시간에 1000개의 단계가 있는 코사인 노이즈 일정을 사용합니다. 우리는 분류기 없는 안내에서 0.0001의 학습률, 32의 배치 크기, 0.1의 드롭아웃 률을 갖는 AdamW를 사용합니다. 모든 확산 모델은 두 개의 NVIDIA RTX 2080 Ti GPU를 사용하여 MUSIC에서 200k 단계, VGGSound에서 500k 단계에 대해 훈련되며, MUSIC에서는 하루, VGGSound에서는 이틀이 걸립니다. 사전 훈련된 CLIP 모델의 경우 4억 개의 이미지 텍스트 쌍에서 훈련된 &quot;ViT-L/14&quot; 버전을 사용합니다[29]. 동일한 백본 CLIP 모델을 사용하여 20억 개의 이미지-텍스트 쌍에서 훈련된 사전 훈련된 변압기 기반 확산 사전 모델을 사용합니다[30]. 확산 사전 모델 CLIPSonic-SD를 처음부터 훈련하기 위해 CLIPSonic-PD와 동일한 아키텍처를 따르고 [31]의 코드를 사용합니다. 우리는 0.0001의 학습률과 32의 배치 크기를 갖는 AdamW를 사용합니다. 확산 사전 모델은 각각 MUSIC과 VGGSound에서 학습되어 약 200k 단계에서 수렴하는데, 이는 NVIDIA RTX 2080 Ti GPU에서 하루가 걸립니다. CLAP 모델의 경우 [32]에서 출시된 &quot;630k-audiosetfusion&quot; 버전을 사용합니다. BigVGAN 모델의 경우 [33]의 코드를 사용하여 VGGSound에서 500k 단계 동안 사전 학습하고 모든 실험에서 이 사전 학습된 버전을 사용합니다. 평가 지표. 우리의 성능을 비교하려면
--- METHOD ---
, 그리고 사전 훈련된 확산 사전은 모달리티 전이 갭을 줄일 수 있습니다. 텍스트-오디오 합성에 초점을 맞추는 동안 제안된 모델은 이미지 쿼리에서 오디오를 생성할 수도 있으며 주관적 청취 테스트에서 최첨단 이미지-오디오 합성 모델과 경쟁적인 성능을 보여줍니다. 이 연구는 비디오에서 자연스럽게 발생하는 오디오-비주얼 대응과 사전 훈련된 언어-비전 모델의 힘을 활용하는 텍스트-오디오 합성에 접근하는 새로운 방향을 제시합니다. 색인 용어 사운드 합성, 오디오 생성, 멀티모달 학습, 확산 모델, 신경망, 머신 러닝 1. 서론 생성 모델링[1-3]과 언어-오디오 대조 학습[4-6]의 발전으로 다양한 딥 러닝 기반 텍스트-오디오 합성 시스템이 최근 등장했습니다[7-12]. 그러나 이러한 시스템은 일반적으로 훈련을 위해 대량의 쌍을 이루는 텍스트-오디오 데이터가 필요합니다. 광범위한 인간 주석 노력에도 불구하고 현재 가장 큰 공개 텍스트-오디오 데이터 세트에는 약 630k 텍스트-오디오 쌍이 포함되어 있습니다[4]. 텍스트-이미지 데이터에 비해 웹에서 텍스트-오디오 데이터가 상대적으로 부족하다는 점을 감안할 때, 텍스트-오디오 데이터 세트를 대규모 텍스트-이미지 데이터 세트(예: 58억 5천만 개의 텍스트-이미지 쌍이 포함된 LAION-5B 데이터 세트[13])와 비슷한 크기로 확장할 수 있는지 여부는 불분명합니다. 이 연구에서는 비디오에서 자연스럽게 발생하는 오디오-비주얼 대응과 사전 학습된 언어-비전 모델에서 학습한 다중 모달 표현을 활용하여 텍스트-오디오 쌍 없이 텍스트-오디오 합성에 접근합니다(그림 1 참조). 제안된 CLIPSonic 모델은 조건부 확산 모델[15], 구성적 언어-이미지 사전 학습(CLIP) * Dolby에서 인턴십 중에 수행한 작업. Hao-Wen은 박사 학위 연구를 지원해 주신 대만 교육부에 감사드립니다. 연락처: hwdong@ucsd.edu 비디오에서 자연스럽게 발생하는 오디오-비주얼 대응 비디오 프레임 오디오 텍스트 기차 휘파람 사진 사전 학습된 시각-언어 모델 원하는 오디오-텍스트 대응 그림 1: 비디오의 오디오-비주얼 대응과 사전 학습된 언어-비전 모델에서 학습한 멀티모달 표현을 활용하여 텍스트-오디오 대응을 학습합니다. 모델[16], 그리고 그림 2에 나와 있는 사전 학습된 확산 사전 모델[17]. 비디오가 주어지면, CLIPSonic은 비디오에서 무작위로 선택된 CLIP 인코딩된 프레임이 주어진 오디오의 멜 스펙트로그램을 합성하도록 학습됩니다. CLIP은 이미지와 텍스트를 크로스 모달 의미 공간에 임베딩하므로 CLIPSonic은 CLIP 임베딩 공간을 오디오에 매핑하는 방법을 학습합니다. 테스트 시간에, 먼저 제로샷 모달 전송을 수행하고 CLIP 인코딩된 텍스트 쿼리로 직접 확산 모델을 조절하는 것을 살펴봅니다. 그러나 실제로 이미지 쿼리와 관련하여 눈에 띄는 성능 저하를 관찰했습니다. 이 격차를 메우기 위해 사전 학습된 확산 사전 모델을 채택하여 CLIP 텍스트 임베딩이 주어진 CLIP 이미지 임베딩을 생성합니다. 제안된 시스템은 조건부 확산 모델을 학습하기 위해 1) 레이블이 지정되지 않은 비디오만 필요하고 2) 언어-시각 모델을 사전 학습하기 위해 이미지-텍스트 쌍만 필요하다는 점에 유의합니다. 주관적 청취 테스트와 객관적 평가를 통해
--- EXPERIMENT ---
모든 결과는 제안된 방법의 효과를 입증합니다. 오디오 샘플은 데모 웹사이트에서 제공됩니다. 저희 연구는 여러 면에서 이전 연구와 다릅니다. 기존의 텍스트-오디오 모델은 대량의 텍스트-오디오 학습 쌍[7-12]에 의존하는 반면, CLIPSonic은 텍스트-오디오 쌍 없이 텍스트 쿼리 오디오 합성을 학습합니다. 이전 연구에서는 이미지-오디오 합성[1820]을 연구했지만 텍스트와 이미지 간의 제로샷 모달리티 전송을 조사하지 않았습니다. CLIPSep[21]과 CLIPSynth[22]는 각각 레이블이 지정되지 않은 비디오에서 텍스트 쿼리 소스 분리 및 오디오 합성을 학습하는 것을 제안하지만 제로샷 모달리티 전송 갭 문제는 다루지 않습니다. DALL-E 2[17]는 CLIP 기반 텍스트-이미지 합성에서 제로샷 모달리티 전송 갭을 해결하기 위해 확산 사전 모델을 제안하고, 사전 학습된 확산 사전 모델을 활용하여 비디오에서 학습한 지식을 텍스트-오디오 합성에 전송하는 방법을 살펴봅니다. 관련 작업으로는 언어-오디오 모델[4,5]을 사용하여 제로샷 오디오-텍스트 모달리티 전송을 수행하는 AudioLDM[9] 및 MusicLM[12]이 있지만 이러한 언어-오디오 모델은 오디오-텍스트 쌍을 사용하여 훈련됩니다. 한국어: https://salu133445.github.io/clipsonic/ CLIP-image(사전 학습됨; 고정) imgInput 비디오 Xt Xt-기차 휘파람 소리 사진 CLIP-text(사전 학습됨; 고정) Itext(b) 추론 - CLIPSonic-ZS(제로샷 전이) ✗t U-Net CLIP-text 확산 사전(사전 학습됨; 고정) (사전 학습됨; 고정) Xt 확산 모델(t = T, ..., 1) 기차 휘파람 소리 사진 + qtext img U-Net 확산 모델(t = T, ..., 1) U-Net Xt-Xt-(a) 학습 - CLIPSonic(c) 추론 CLIPSonic-PD(사전 학습된 확산 사전) 확산 모델(t=T,..., 1) 그림 2: 제안된 CLIPSonic 모델. 훈련 중에 CLIPSonic은 비디오 프레임의 이미지가 주어지면 비디오의 오디오 트랙을 합성하는 방법을 학습합니다. 추론 시간에, 우리는 텍스트-오디오 합성에 접근하기 위해 &quot;[레이블]의 사진&quot; 형태의 텍스트 쿼리를 제공하거나 사전 훈련된 확산 사전 모델을 사용하여 텍스트 쿼리(추론에 사용됨)와 이미지 쿼리(훈련에 사용됨) 간의 격차를 메웁니다. Xt는 확산 단계 t에서의 노이즈가 있는 스펙트로그램을 나타냅니다. 생성된 멜 스펙트로그램 ✰o는 사전 훈련된 BigVGAN 모델[14]에 의해 파형으로 다시 반전됩니다. 2. CLIPSonic 이 섹션에서는 레이블이 지정되지 않은 비디오에서 텍스트-오디오 합성을 학습하기 위한 제안된 CLIPSonic 모델을 소개합니다. 그림 2(a)에서 볼 수 있듯이, CLIPSonic은 오디오 합성을 위해 멜 스펙트로그램 기반 확산 모델을 사용합니다. 우리는 오디오 합성에서 강력한 성능을 보이는 확산 프레임워크를 채택합니다[9,23,24]. 비디오가 주어지면, CLIPSonic은 무작위로 추출된 비디오 프레임의 이미지에서 오디오의 멜 스펙트로그램을 합성하도록 훈련됩니다. 구체적으로, 우리는 먼저 이미지를 쿼리 벡터 Qimg로 인코딩하기 위한 사전 학습된 CLIP 이미지 인코더. 그런 다음 이 쿼리 벡터는 조건 신호로 사용되어 확산 모델을 안내하여 멜 스펙트로그램 ✰o를 생성합니다. 우리는 추론 중에 안내 수준 변수 w를 통해 조절 신호의 정도를 제어할 수 있는 잡음 제거 확산 확률 모델[15]과 분류기 없는 안내[25]를 채택합니다. 생성된 멜 스펙트로그램은 별도로 학습된 BigVGAN[14]을 사용하여 파형으로 다시 반전됩니다. 우리는 파형보다 차원이 낮고 BigVGAN이 멜 스펙트로그램에서 일반 오디오를 합성할 때 좋은 품질을 보여주기 때문에 멜 스펙트로그램 도메인에서 확산을 수행하기로 선택했습니다. CLIPSonic-ZS(제로샷 모달리티 전송). 추론 시간에는 CLIP에서 학습한 언어-비전 임베딩 공간을 활용하여 텍스트-오디오 합성을 달성하는 것을 목표로 합니다. CLIPSonic-ZS는 텍스트 쿼리를 제로샷 모달리티 전송 설정에서 사용하는 방법으로 CLIP 이미지 임베딩을 CLIP 텍스트 임베딩으로 바꾸는 것을 탐구합니다. 그림 2(b)에서 볼 수 있듯이, CLIP 텍스트 인코더를 사용하여 입력 텍스트 쿼리를 쿼리 벡터 qtext로 인코딩하고, 이를 확산 모델에 조건으로 공급합니다. 이 모델을 CLIPSonic-ZS라고 하며, 여기서 &quot;ZS&quot;는 제로샷 모달리티 전송을 의미합니다. CLIPSonic-PD(사전 학습된 확산 사전). 섹션 4에서 볼 수 있듯이, CLIP의 텍스트와 이미지 임베딩 공간 사이에 모달리티 갭이 있음을 관찰합니다. DALL-E 2[17]에 따라 확산 사전 모델을 사용하여 이 갭을 메우는 것을 탐구합니다. 그림 2(c)에 나와 있듯이, 먼저 입력 텍스트 쿼리를 CLIP 텍스트 임베딩 벡터 qtext로 인코딩한 다음, 사전 학습된 확산 사전 모델을 사용하여 ɖtext에서 CLIP 이미지 임베딩 벡터 img를 생성합니다. 생성된 쿼리 벡터 img는 컨디셔닝 신호로 확산 모델에 전달됩니다. 이 2 다음 공식을 사용합니다. Vx log pw(x|q) = (1 - w)√x log p(x) + wVx log p(xq). w가 클수록 컨디셔닝 신호가 강해지고, w = 1은 분류기 없는 안내가 없는 조건부 모델에 해당합니다. 이 모델을 CLIPSonic-PD(사전 학습된 확산 사전)라고 합니다. CLIPSonic-ZS와 CLIPSonic-PD는 모두 학습을 위해 텍스트-오디오 쌍이 필요하지 않습니다. 또한 CLIP과 확산 사전 모델은 모두 텍스트-이미지 쌍만 사용하여 사전 학습할 수 있으므로 쌍을 이룬 오디오-텍스트 데이터가 필요하지 않습니다. CLIPSonic-IQ와 CLIPSonic-SD. 여기에서는 텍스트-오디오에 초점을 맞추지만, CLIPSonic은 qimg 쿼리를 사용하여 이미지-오디오 합성 모델로도 사용할 수 있습니다. 이 변형을 CLIPSonic-IQ(이미지 쿼리)라고 합니다. 게다가 도메인별 데이터 세트에서 확산 사전 모델을 처음부터 학습하는 것이 가능하다는 것을 알게 되었고, 따라서 데이터 세트에서 텍스트-이미지 쌍을 사용하여 확산 사전 모델을 처음부터 학습하는 CLIPSonicSD(지도 확산 사전)라는 변형도 고려합니다. 섹션 3에서 설명하겠지만, 이 작업에서 CLIPSonic-SD에서 확산 사전을 학습하는 데 사용된 텍스트 데이터는 오디오 레이블에서 나오기 때문에 CLIPSonic-SD는 CLIPSonic-PD에 대한 오라클 모델 역할을 합니다. CLIPSonic-PD와 CLIPSonic-SD를 비교하여, 대상 데이터 세트에서 학습된 모델과 대량의 데이터에서 사전 학습된 확산 사전 모델을 사용하는 것의 효과를 연구하고자 합니다. 3. 실험 설정 데이터. 우리는 VGGSound[26]와 MUSIC[27]의 두 데이터 세트를 고려합니다. VGGSound 데이터 세트는 171,899개의 10초 YouTube 비디오로 구성되어 있으며, 야외에서 310개의 사운드 클래스를 포함하고 있으며, 데이터 세트와 함께 제공된 traintest 분할을 따릅니다. MUSIC 데이터 세트는 총 21개의 악기 유형을 사용하여 악기를 연주하는 사람들의 전체 길이 YouTube 비디오 1,055개로 구성되어 있습니다. 우리는 데이터 세트를 무작위로 9:1 train-test 분할로 분할했습니다. VGGSound는 야외에서 비정형 소스에서 수집한 크고 다양한 데이터 세트를 나타내는 반면, MUSIC은 관심 있는 특정 도메인의 작고 큐레이팅된 데이터 세트를 나타냅니다. 두 데이터 세트 모두 클래스 레이블만 제공되므로 이러한 레이블을 &quot;[레이블]의 사진&quot; 형태의 가상 텍스트로 변환합니다. 기준 모델. 우리는 CLIPSonic 모델을 다음의 텍스트-오디오(TTA) 및 재구성 모델과 비교합니다. CLIP-TTA는 텍스트-오디오 쌍을 사용하여 학습하는 CLIPSonic의 지도 학습 버전입니다. 사전 훈련된 CLIP 텍스트 임베딩은 조건화로 사용됩니다. ⚫ CLAP-TTA는 CLIP-TTA와 동일하지만 사전 훈련된 CLAP 텍스트 임베딩[4]을 사용합니다. 여기서 우리는 &quot;[레이블]의 소리&quot; 형태의 프롬프트를 사용합니다. CLIP-text 임베딩과 달리 CLAP-text Fréchet 오디오 거리(FAD) 6 5 4 3(a) FAD(VGGSound) 0.(b) CLAP 점수↑(VGGSound) 0.CLAP 점수 0.0.0.0.Fréchet 오디오 거리(FAD) 0.안내 수준(w) 안내 수준(w) CLIPSonic-IQ(이미지 쿼리) CLIPSonic-SD(지도 확산 사전) (c) FAD(음악) 0.0.30 CLAP 점수 0.0.0.0.260.안내 수준(w) CLIP-TTA CLAP-TTA 안내 수준(w) BigVGAN 재구성 (d) CLAP 점수↑(음악)• CLIPSonic-ZS(제로샷 모달리티 전송) CLIPSonic-PD(사전 학습된 확산 사전) 그림 3: 객관적 VGGSound 및 MUSIC에 대한 평가 결과. 임베딩은 시각적 접지 기능이 아닌 오디오 접지 기능을 인코딩할 것으로 예상됩니다. BigVGAN 멜 스펙트로그램 재구성은 BigVGAN 모델에 의해 기준 진실 멜 스펙트로그램에서 재구성된 파형입니다. 이는 BigVGAN을 역 모델로 사용하는 스펙트로그램 기반 합성 시스템의 상한으로 사용됩니다. 구현 세부 정보. 멜 스펙트로그램 계산의 경우 16kHz의 샘플링 속도, 512의 홉 크기, 2048의 FFT 필터 크기 및 64개의 멜 대역을 사용합니다. 학습하는 동안 2초의 오디오에 해당하는 64×64 크기의 멜 스펙트로그램을 사용합니다. 확산 모델의 경우 [15]에서 제안한 네트워크 아키텍처를 따르고 [28]의 오픈 소스 코드를 사용합니다. 학습하는 동안 4000개의 확산 단계와 추론 시간에 1000개의 단계가 있는 코사인 노이즈 일정을 사용합니다. 우리는 분류기 없는 안내에서 0.0001의 학습률, 32의 배치 크기, 0.1의 드롭아웃 률을 갖는 AdamW를 사용합니다. 모든 확산 모델은 두 개의 NVIDIA RTX 2080 Ti GPU를 사용하여 MUSIC에서 200k 단계, VGGSound에서 500k 단계에 대해 훈련되며, MUSIC에서는 하루, VGGSound에서는 이틀이 걸립니다. 사전 훈련된 CLIP 모델의 경우 4억 개의 이미지 텍스트 쌍에서 훈련된 &quot;ViT-L/14&quot; 버전을 사용합니다[29]. 동일한 백본 CLIP 모델을 사용하여 20억 개의 이미지-텍스트 쌍에서 훈련된 사전 훈련된 변압기 기반 확산 사전 모델을 사용합니다[30]. 확산 사전 모델 CLIPSonic-SD를 처음부터 훈련하기 위해 CLIPSonic-PD와 동일한 아키텍처를 따르고 [31]의 코드를 사용합니다. 우리는 0.0001의 학습률과 32의 배치 크기를 갖는 AdamW를 사용합니다.확산 사전 모델은 각각 MUSIC과 VGGSound에서 학습되어 약 200k 단계에서 수렴하는데, 이는 NVIDIA RTX 2080 Ti GPU에서 하루가 걸립니다.CLAP 모델의 경우 [32]에서 출시된 &quot;630k-audiosetfusion&quot; 버전을 사용합니다.BigVGAN 모델의 경우 [33]의 코드를 사용하여 VGGSound에서 500k 단계 동안 사전 학습하고 모든 실험에서 이 사전 학습된 버전을 사용합니다.평가 지표.기준선과 방법의 성능을 비교하기 위해 각 모델에서 512개의 오디오 샘플을 샘플링하고 Fréchet 오디오 거리(FAD) [34]와 CLAP 점수 [4,10]를 계산합니다. FAD는 생성된 오디오 샘플이 품질과 다양성 측면에서 참조 오디오와 얼마나 가까운지를 측정합니다.³ 우리는 [35]에서 제공하는 오픈 소스 구현을 채택하고 VGGish [36]를 FAD의 백본 모델로 사용합니다.CLAP 점수는 생성된 오디오와 입력 쿼리 텍스트 간의 관련성을 측정하며, 오디오의 CLAP 임베딩과 입력 텍스트 쿼리의 CLAP 임베딩 간의 코사인 유사도로 공식적으로 정의됩니다.주관적 테스트.우리는 생성된 오디오의 충실도와 텍스트(텍스트-오디오) 및 3과의 관련성을 연구하기 위해 청취 테스트를 수행합니다.[7,9]에 따라 생성된 스펙트로그램의 Fréchet 개시 거리(FID)도 계산했고 FID의 추세가 FAD의 추세와 잘 일치한다는 것을 발견했습니다.간결하게 FAD 결과만 보고하고 논의합니다.시각적(이미지-오디오) 프롬프트. 우리는 21명의 전문 청취자에게 생성된 오디오 샘플을 충실도와 관련성 측면에서 1~5점 척도로 평가해 달라고 요청했습니다. 충실도 실험은 생성된 오디오의 품질을 연구하고(의미적 근거를 평가하지 않음) 관련성 실험은 프롬프트와 관련된 의미적 대응을 연구합니다(오디오 품질을 평가하지 않음). 이 테스트에 사용된 오디오 샘플은 데모 웹사이트에서 사용할 수 있습니다. 4. 결과 4.1. 객관적 평가 결과 지침. 그림 3은 분류기 없는 지침 척도 w의 함수로서 연구된 모델의 결과를 보여줍니다. [25]에서 개념적으로 유사한 척도를 사용하여 언급했듯이, FAD와 CLAP 점수 간의 다른 곡선은 품질/다양성(FAD로 표현)과 쿼리 샘플 관련성(CLAP 점수로 표현) 간의 상충 관계를 의미합니다. 주목할 점은 CLAP 점수 측면에서 모든 모델(MUSIC의 CLIPSonic-PD 제외)이 두 데이터 세트 모두에서 BigVGAN 재구성보다 성능이 우수하다는 것입니다(그림 3(b) 및 (d) 참조). 분류기 없는 안내가 컨디셔닝 준수를 개선하는 것으로 나타났기 때문에 더 높은 CLAP 점수를 부여하지만 다양성을 희생합니다. 그림 3(a) 및 (c)에서 w가 증가함에 따라 FAD가 증가하는 것에 유의하세요. 따라서 실무자는 특정 요구 사항에 따라 w를 선택할 수 있습니다. 품질/다양성과 관련성 간에 좋은 균형을 제공하므로 w = 1.5를 사용하고 표 1에 결과를 보고합니다. 텍스트-오디오 쌍이 없는 모델. 먼저, 훈련 중에 텍스트-오디오 쌍을 사용하지 않는 표 1의 CLIPSonic 모델을 설명합니다. CLIPSonic-IQ(이미지 쿼리)는 두 데이터 세트 모두에서 강력한 성능을 달성합니다. 그러나 CLIPSonic-ZS에서 zeroshot 설정으로 텍스트 쿼리를 사용하도록 전환하면 두 데이터 세트 모두에서 FAD 측면에서 성능이 저하되는 것을 관찰했습니다. 이 성능 저하는 CLIP의 이미지(학습 중 사용)와 텍스트(추론 중 사용) 임베딩 공간 사이에 모달리티 갭이 있음을 시사합니다. 반면 사전 학습된 확산 사전 모델을 사용하면 CLIPSonic-PD는 다양한 w 값에서 CLIPSonic-ZS보다 낮은 FAD를 달성합니다(그림 3 참조). 이를 더 자세히 조사하기 위해 표 2에 쿼리 임베딩(ɖtext 또는 ĝimg)과 실제 CLIP 이미지 임베딩 ɖimg 사이의 평균 코사인 유사도를 보고합니다. CLIPSonicZS가 낮은 코사인 유사도로 이어지는 것을 알 수 있는데, 이는 CLIP의 임베딩 공간에 모달리티 갭이 있다는 가설을 뒷받침합니다. 대조적으로 CLIPSonic-PD는 상당히 높은 코사인 유사도를 달성하여 사전 학습된 확산 사전 모델이 모달리티 갭을 효과적으로 메울 수 있음을 보여줍니다. 게다가 MUSIC에서 CLIPSonic-PD의 CLAP 점수가 낮은 반면, 섹션 4.2에서 논의할 청취 테스트에서 관련성 기준에는 거의 차이가 없었습니다(표 3 참조). 이는 이러한 모든 모델이 모델 표 1: w = 1.5에서 평가된 VGGSound 및 MUSIC 데이터 세트에 대한 평가 결과. CLIPSonic-IQ(이미지 쿼리) CLIPSonic-ZS(제로샷 모달리티 전송) 쿼리 모달리티 없음 VGGSound 텍스트-오디오 쌍 학습 추론 FAD CLAP 점수 ↑ 음악 FAD CLAP 점수 ↑ 이미지 이미지 2.4.이미지 텍스트 3.0.19.0.CLIPSonic-PD(사전 학습된 확산 사전) 이미지 텍스트 3.0.13.0.CLIPSonic-SD(지도 확산 사전) ☑ 이미지 텍스트 2.0.12.0.CLIP-TTA ✓ 텍스트 텍스트 2.0.9.0.CLAP-TTA ☑ 텍스트 텍스트 2.0.10.0.BigVGAN mel spectrogram reconstruction 0.0.6.0.표 2: 다양한 쿼리 임베딩 간의 코사인 유사도.모델 표 4: 이미지-오디오 합성(MOS)에 대한 청취 테스트 결과. 유사도 계산 VGGSound MUSIC 모델 충실도 관련성 CLIPSonic-ZS sim(qtext, img, 0.0.CLIPSonic-PD sim(img, Qimg 0.0.CLIPSonic-SD sim(img, Qimg 0.0.CLIPSonic-IQ (이미지 쿼리) SpecVQGAN [19] IM2WAV [20] 3.29 ± 0.3.80 ± 0.2.15 ± 0.2.54 ± 0.2.19 ± 0.3.90 ± 0.표 3: 텍스트-오디오 합성(MOS)에 대한 청취 테스트 결과. 모델 VGGSound 충실도 관련성 MUSIC 충실도 관련성 CLIPSonic-ZS 2.55 ± 0.22 2.01 ± 0.27 2.98 ± 0.23 3.87±0.CLIPSonic-PD 3.04±0.20 2.86±0.25 3.67±0.18 3.91±0.CLIPSonic-SD 2.96±0.21 3.49±0.28 3.36±0.20 4.07 0.실제값 3.78 0.19 3.54±0.29 3.90±0.17 4.34±0.합리적인 수준의 오디오-텍스트 관련성.텍스트-오디오 쌍을 사용하는 모델.이제 이전 CLIPSonic 변형과 비교하여 텍스트-오디오 쌍을 사용하여 학습하는 기준 모델을 비교합니다.먼저, 타겟 데이터 세트에서 직접 학습된 확산 사전을 사용하는 CLIPSonic-SD가 사전 학습된 확산 사전을 사용하는 CLIPSonic-PD보다 낮은 FAD를 달성한다는 것을 알 수 있습니다.이는 분포 때문일 수 있습니다. 영어: 사전 학습된 사전 학습에 사용된 LAION-2B 데이터 세트와 대상 데이터 세트 간의 불일치.표 2에서 CLIPSonic-SD가 CLIPSonic-PD보다 대상 데이터 세트의 기준 진실 임베딩에 더 가까운 CLIP 이미지 임베딩을 생성할 수 있음을 알 수 있습니다.그러나 아래의 주관적 평가에서 CLIPSonic-PD는 지속적으로 CLIPSonic-ZS보다 성능이 뛰어나 다운스트림 데이터 세트에 대한 유리한 수준의 일반화를 여전히 보여줍니다.또한 CLIPSonic-PD의 성능과 CLIP-TTA 및 CLAP-TTA의 성능 사이에 격차가 있음을 관찰합니다.그러나 CLIP-TTA와 CLAP-TTA는 오디오-텍스트 쌍에서 학습된 반면 CLIPSonic-PD는 학습에 오디오-텍스트 쌍을 사용하지 않으므로 이는 불공평한 비교라는 점에 유의합니다.4.2. 주관적 청취 테스트 결과 텍스트-오디오 합성. 우리는 MUSIC과 VGGSound에서 CLIPSonic-ZS, -PD 및 -SD 변형을 비교하기 위해 절제 연구를 수행했습니다.표 3에서 볼 수 있듯이, CLIPSonic-ZS는 텍스트와 4이미지 임베딩 간의 앞서 언급한 불일치로 인해 지속적으로 성능이 떨어졌습니다.텍스트 쿼리의 의미에도 불일치가 있는데, 대상 데이터 세트에는 오디오별 레이블이 포함되어 있는 반면 LAION2B에는 시각적으로 근거가 있는 레이블이 포함되어 있습니다.그러나 CLIP-TTA와 CLAP-TTA의 유사한 성능은 이것이 사소한 효과임을 시사합니다.이에 기여한 두 가지 변형, 즉 CLIPSonicPD 및 -SD는 관련성과 충실도 측면에서 모두 CLIPSonicZS보다 지속적으로 높은 MOS를 달성합니다.특히 기준 진실 점수는 비교적 낮습니다(MOS는 3~4 사이).특히 VGGSound의 경우 MUSIC 데이터 세트보다 노이즈가 많기 때문에 눈에 띄게 나타납니다.이미지-오디오 합성. 우리의 초점은 텍스트-오디오 합성을 연구하는 것이지만, CLIPSonic-IQ는 이미지 쿼리로부터 오디오를 생성할 수도 있습니다.우리는 이를 대표적인 이미지-오디오 모델인 SpecVQGAN[19]과 이미지-오디오 합성을 위한 최첨단 모델인 IM2WAV[20]와 비교합니다.세 모델 모두 VGGSound에서 학습되었고 IMAGEHEAR[20]의 배포 외부 샘플에서 테스트되었습니다.선택된 샘플은 1) IM2WAV의 데모 웹사이트에서 선택되었고 2) 배포 외부이기 때문에 우리에게 까다로운 벤치마크를 따릅니다.표 4에서 볼 수 있듯이, CLIPSonic-IQ는 관련성 측면에서 경쟁력을 유지하는 동시에 충실도 면에서 최첨단 모델보다 성능이 뛰어납니다. 향상된 충실도는 IM2WAV에서 사용된 이산 VQ-VAE 표현과 비교했을 때 최첨단 역전 모델(BigVGAN)을 갖춘 연속 표현(mel spectrogram)을 사용하기 때문에 가능했습니다.
--- CONCLUSION ---
우리는 레이블이 지정되지 않은 비디오와 사전 훈련된 언어-비전 모델을 사용하여 텍스트-오디오 쌍 없이 텍스트-오디오 합성에 접근하는 것을 탐구했습니다. 객관적 평가와 주관적 평가를 통해 제안된 모델이 텍스트-오디오 쌍 없이도 텍스트-오디오 합성을 효과적으로 학습할 수 있으며, 사전 훈련된 확산 사전은 CLIP의 이미지(훈련에 사용됨)와 텍스트(추론에 사용됨) 임베딩 공간 간의 불일치로 인해 발생하는 모달리티 전이 갭을 줄일 수 있음을 보여주었습니다. 더욱이 주관적 청취 테스트에서 모달리티 전이의 기반이 되는 이미지-오디오 합성 모델은 최첨단 이미지-오디오 합성 모델과 경쟁적인 성능을 달성했습니다. 마지막으로, 우리는 이미지가 오디오 합성을 위한 풍부한 컨디셔닝 신호를 제공하며, 이러한 풍부한 신호를 활용하여 텍스트-오디오 합성을 개선하는 것이 유망한 연구 방향이라고 주장합니다. 이 방향에 따라 CLIPSonic은 비디오와 사전 훈련된 언어-비전 모델을 사용하는 예를 보여줍니다. 향후 작업을 위해 우리는 제안된 방법을 더 많은 양의 비디오로 확장하고 3중 모드 오디오-비전-언어 모델을 사용하여 탐색할 계획입니다[6,37,38]. 6. 참고문헌 [1] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, &quot;Language Models are Unsupervised Multitask Learners,&quot; Technical Report of OpenAI, 2019. [2] J. Ho, A. Jain, P. Abbeel, &quot;Denoising Diffusion Probabilistic Models,&quot; Proc. NeurIPS, 2020. [3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, &quot;High-resolution image synthesis with latent diffusion models,&quot; Proc. CVPR, 2022, pp. 10 684–10695. [4] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, S. Dubnov, “특징 융합 및 키워드-캡션 증강을 통한 대규모 대조 언어-오디오 사전 학습,&quot; Proc. ICASSP, 2023. [5] Q. Huang, A. Jansen, J. Lee, R. Ganti, JY Li, DPW Ellis, &quot;MuLan: 음악 오디오와 자연어의 공동 임베딩,&quot; Proc. ISMIR, 2022. [6] A. Guzhov, F. Raue, J. Hees, A. Dengel, &quot;AudioCLIP: CLIP을 이미지, 텍스트 및 오디오로 확장,&quot; Proc. ICASSP, 2022, 976-980쪽. [7] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, 및 D. Yu, &quot;Diffsound: 텍스트-사운드 생성을 위한 이산 확산 모델&quot;, arXiv 사전 인쇄본 arXiv:2207.09983, 2022. [8] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman, 및 Y. Adi, &quot;AudioGen: 텍스트로 안내되는 오디오 생성&quot;, Proc. ICLR, 2023. [9] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, 및 MD Plumbley, &quot;AudioLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성&quot;, Proc. ICML, 2023. [10] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin 및 Z. Zhao, &quot;Make-An-Audio: 프롬프트 향상 확산 모델을 사용한 텍스트-오디오 생성&quot;, Proc. ICML, 2023. [11] Q. Huang, DS Park, T. Wang, TI Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank, J. Engel, QV Le, W. Chan, Z. Chen 및 W. Han, &quot;Noise2Music: 확산 모델을 사용한 텍스트 조건화된 음악 생성&quot;, arXiv 사전 인쇄 arXiv:2302.03917, 2023. [12] A. Agostinelli, TI Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, M. Sharifi, N. Zeghidour 및 C. Frank, &quot;MusicLM: 텍스트에서 음악 생성&quot;, arXiv 프리프린트 arXiv:2302.03917, 2023. [13] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk, and J. Jitsev, “LAION-5B: 차세대 이미지-텍스트 모델을 훈련하기 위한 대규모 오픈 데이터세트,&quot; NeurIPS 2022 데이터세트 및 벤치마크, 2022. [14] S. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon, &quot;BigVGAN: 대규모 훈련을 제공하는 범용 신경 보코더,&quot; Proc. ICLR, 2023. [15] A. Nichol 및 P. Dhariwal, &quot;개선된 노이즈 제거 확산 확률적 모델&quot;, Proc. ICML, 2019. [16] A. Radford, JW Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger 및 I. Sutskever, &quot;자연어 감독에서 이전 가능한 시각적 모델 학습&quot;, Proc. ICML, 2021, 8748-8763쪽. [17] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu 및 M. Chen, &quot;CLIP 잠재 데이터를 사용한 계층적 텍스트 조건부 이미지 생성&quot;, arXiv 사전 인쇄본 arXiv:2204.06125, 2022. [18] A. Owens, P. Isola, J. McDermott, A. Torralba, EH Adelson, 및 WT Freeman, &quot;Visually Indicated Sounds,&quot; Proc. CVPR, 2016, pp. 2405-2413. [19] V. Iashin 및 E. Rahtu, &quot;Taming Visually Guided Sound Generation,&quot; Proc. BMVC, 2021. [20] R. Sheffer 및 Y. Adi, &quot;I Hear Your True Colors: Image Guided Audio Generation,&quot; Proc. ICASSP, 2023. [21] H.-W. 영어: Dong, N. Takahashi, Y. Mitsufuji, J. McAuley, and T. Berg-Kirkpatrick, “CLIPSep: 잡음이 많은 레이블이 지정되지 않은 비디오를 사용하여 텍스트 쿼리 사운드 분리 학습,&quot; Proc. ICLR, 2023. [22] H.-W. Dong, G. Sigurdsson, C. Tao, J.-Y. Kao, Y.-H. Lin, A. Narayan-Chen, A. Gupta, T. Chung, J. Huang, N. Peng, and W. Zhao, “CLIPSynth: CLIP 및 확산 모델을 사용하여 비디오에서 텍스트-오디오 합성 학습,&quot; CVPR Workshop on Sight and Sound, 2023. [23] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “DiffWave: 오디오 합성을 위한 다목적 확산 모델,&quot; Proc. ICLR, 2021. [24] S. Pascual, G. Bhattacharya, C. Yeh, J. Pons, 및 J. Serrà, &quot;점수 기반 확산을 통한 전체 대역 일반 오디오 합성&quot;, Proc. ICASSP, 2023. [25] TS Jonathan Ho, &quot;분류기 없는 확산 안내&quot;, NeurIPS 심층 생성 모델 및 다운스트림 애플리케이션 워크숍, 2021. [26] H. Chen, W. Xie, A. Vedaldi, 및 A. Zisserman, &quot;VGGSound: 대규모 오디오-비주얼 데이터 세트&quot;, Proc. ICASSP, 2020, 721-725쪽. [27] H. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. McDermott 및 A. Torralba, &quot;The Sound of Pixels&quot; in Proc. ECCV, 2018. [28] https://github.com/openai/improved-diffusion. [29] https://github.com/openai/CLIP. [30] https://huggingface.co/laion/DALLE2-PyTorch. [31] https://github.com/lucidrains/DALLE2-pytorch. [32] https://github.com/LAION-AI/CLAP. [33] K. Kilgour, M. Zuluaga, D. Roblek 및 M. Sharifi “Fréchet 오디오 거리: 측정 기준 영어: Evaluating Music Enhancement Algorithms,” in Proc. INTERSPEECH, 2019, pp. 2350–2354. [35] https://github.com/gudgud96/frechet-audio-distance. [36] S. Hershey, S. Chaudhuri, DP Ellis, JF Gemmeke, A. Jansen, RC Moore, M. Plakal, D. Platt, RA Saurous, B. Seybold, et al., &quot;대규모 오디오 분류를 위한 CNN 아키텍처,&quot; in Proc. ICASSP, 2017, pp. 131–135. [37] H.-H. Wu, P. Seetharaman, K. Kumar, and JP Bello, &quot;Wav2CLIP: Learning Robust Audio Representations From CLIP,&quot; in Proc. ICASSP, 2022, pp. 4563–4567. [38] A. Rouditchenko, A. Boggust, D. Harwath, B. Chen, D. Joshi, S. Thomas, K. Audhkhasi, H. Kuehne, R. Panda, R. Feris, B. Kingsbury, M. Picheny, A. Torralba, J. Glass, &quot;AVLnet: Learning Audio-Visual Language Representations from Instructional Videos,&quot; Proc. INTERSPEECH, 2021, pp. 1584-1588. A. 확산 사전 모델의 구현 세부 정보 이 논문에서 사용된 확산 사전 모델은 [31]의 DALL-E 2의 오픈소스 구현을 기반으로 합니다. 구체적으로, 모델에 대한 입력은 인코딩된 CLIP 텍스트 토큰, CLIP 텍스트 임베딩, 확산 단계 임베딩, 노이즈가 적용된 CLIP 이미지 임베딩 및 학습 가능한 최종 입력 임베딩. 이 시퀀스는 인과적 멀티헤드 셀프 어텐션 및 피드포워드 네트워크로 구성된 12계층 변환기에 공급됩니다. 최종 입력 임베딩에 해당하는 마지막 계층의 최종 출력 벡터는 대상 CLIP 이미지 임베딩의 예측으로 사용됩니다. CLIPSonic-SD에서 사용된 확산 사전 모델의 경우, 학습 중 1000개의 확산 단계와 추론 시 64개의 단계가 있는 코사인 노이즈 일정을 사용합니다. 학습 중 각 확산 단계에서 예측된 CLIP 이미지 임베딩과 대상 CLIP 이미지 임베딩 간의 평균 제곱 오차를 최소화합니다. DALL-E 2 [17]를 기반으로 인코딩된 텍스트 토큰과 CLIP 텍스트 임베딩을 10%의 시간 동안 학습 가능한 플레이스홀더로 무작위로 대체하여 확산 사전 모델을 학습하기 위한 분류기 없는 가이드도 탐색합니다. 그러나 추론 시 가이드를 사용하지 않는 것이 가장 좋은 결과를 낸다는 것을 경험적으로 발견했습니다. 추론 시간에, 각 CLIP 텍스트 임베딩에 대해 확산 사전 모델에서 두 개의 CLIP 이미지 임베딩을 생성하고, CLIP 텍스트 임베딩과 코사인 유사도가 더 높은 임베딩을 선택합니다. 모델을 학습하기 위해 학습 속도 0.0001, 배치 크기 32, 가중치 감소 0.06의 AdamW 옵티마이저를 사용하고 감소 계수 0.9999의 모델 매개변수에 지수 이동 평균을 적용합니다. CLIPSonic-SD의 확산 사전 모델은 약 200k 단계에서 수렴할 때까지 MUSIC과 VGGSound에서 독립적으로 학습됩니다. B. BIGVGAN 재구성을 위한 CLAP 점수 그림 3에서, 많은 경우 기준 진실 멜 스펙트로그램을 사용한 BigVGAN 재구성의 CLAP 점수가 제안된 시스템의 CLAP 점수보다 낮은 것을 관찰할 수 있는데, 이는 기준 진실 오디오와 텍스트 쿼리 간의 관련성이 낮음을 나타냅니다. 테스트 데이터의 길이를 준수하기 위해 BigVGAN CLAP 점수는 전체 10초 오디오 샘플을 기반으로 얻습니다. 그러나 경험적 청취를 통해 10초 샘플 내의 일부 세그먼트가 텍스트 쿼리와 잘 일치하지 않는다는 것을 발견했습니다. 대응 관계를 추가로 조사하기 위해 홉 크기가 0.5초인 4초 슬라이딩 윈도우(합성 샘플 길이와 일치)를 사용하여 BigVGAN CLAP 점수를 계산하고 10초 샘플 내의 모든 4초 세그먼트에 대한 최대, 평균 및 최소 점수를 해당 샘플의 전체 점수로 보고합니다. 표 5에서 볼 수 있듯이 두 데이터 세트의 최대 점수는 나머지보다 높으며, 이는 청취를 통한 관찰을 뒷받침합니다. VGGSound에서 최대 CLAP 점수는 CLIPSonic-ZS, CLIPSonic-PD 및 CLIPSonic-SD의 최대 CLAP 점수도 초과합니다(표 1 참조). MUSIC에서는 최대 CLAP 점수와 전체 10초 오디오를 사용하여 얻은 점수 간의 격차가 작아 샘플 내에서 관련성 수준이 더 균일함을 나타냅니다. 그러나 MUSIC에서 학습한 연구된 모델은 최대 CLAP 점수 측면에서 여전히 BigVGAN 재구성보다 성능이 우수합니다(CLIPSonic-PD 및 분류기 없는 안내를 사용하지 않는 CLIPSonic-ZS 제외, 그림 3 참조). 분류기 없는 안내의 기여(섹션 4.1) 외에도 나머지 이유는 추가 조사가 필요합니다. 가능한 방향에는 오디오-텍스트 대응성이 낮은 샘플을 수동으로 검사하여 제거하고 MUSIC에서 CLAP을 미세 조정하는 것이 포함됩니다. 표 5: 슬라이딩 윈도우를 사용하여 BigVGAN 재구성에서 계산된 CLAP 점수. 윈도우 크기 모드 VGGSound MUSIC 4초 최대 0.0.4초 평균 0.0.Min 0.0.10초 0.0.4초 C. 제한 사항 제안된 방법에는 몇 가지 제한 사항이 있습니다. 첫째, CLIPSonic은 단일 비디오 프레임의 CLIP 임베딩에 따라 조건화되므로 이벤트 시퀀스나 객체 간의 동적 상호 작용을 포함하는 보다 복잡한 텍스트 쿼리를 처리하는 데 쉽게 적용할 수 없습니다. 제안된 방법을 적용하여 비디오의 풍부한 시간 정보를 활용하려면 비디오를 이해할 수 있는 보다 강력한 언어-비전 모델이 필요합니다. 둘째, 컨디셔닝 신호가 비디오에서 추출되므로 CLIPSonic은 피치, 음조, 장르, 템포와 같이 시각적 영역에서 의미가 거의 없는 오디오 개념을 학습할 수 없습니다. 이는 시각적 영역을 브리지로 사용하여 텍스트-오디오 대응 관계를 학습하는 접근 방식의 근본적인 한계 중 하나입니다. 마지막으로, CLIPSonic은 각각 특정 단어나 악보가 주어진 음성이나 음악과 같이 의미적으로 복잡한 오디오를 생성하는 데 있어 제한적인 제어 가능성을 제공합니다. 그러나 제안된 방법은 언어-오디오 모델을 학습하기 위한 사전 학습 접근 방식으로 사용할 수 있으며, 먼저 레이블이 지정되지 않은 비디오만 있는 대규모 데이터 세트에서 언어-오디오 모델을 사전 학습한 다음 나중에 오디오-텍스트 쌍이 있는 소규모 데이터 세트에서 모델을 미세 조정할 수 있습니다.
