--- ABSTRACT ---
기존 Neural Radiance Fields(NeRF) 방법은 반사 객체의 존재로 인해 종종 흐릿하거나 왜곡된 렌더링이 발생합니다. 단일 광도 필드를 계산하는 대신 병렬 하위 공간의 특징 필드 그룹을 사용하여 장면을 나타내는 다중 공간 신경 광도 필드(MS-NeRF)를 제안하여 반사 및 굴절 객체의 존재에 대한 신경망을 더 잘 이해합니다. 다중 공간 방식은 기존 NERF 방법을 향상시키는 역할을 하며, 추가 공간 출력을 학습하고 추론하는 데 필요한 계산 오버헤드가 적습니다. 대표적인 NeRF 기반 모델인 NeRF, Mip-NeRF, Mip-NeRF 360을 사용하여 접근 방식의 우수성과 호환성을 보여줍니다. 합성 장면과 복잡한 반사 및 굴절이 있는 7개의 실제 캡처 장면으로 구성된 새롭게 구성된 데이터 세트에서 비교를 수행하며, 모두 360도 관점을 갖습니다. 광범위한 실험 결과, 거울과 같은 물체를 통과하는 복잡한 빛 경로와 관련된 고품질 장면을 렌더링하는 데 있어 저희의 접근 방식이 기존의 단일 공간 NeRF 방식보다 상당히 우수하다는 것이 밝혀졌습니다. 저희의 코드와 데이터 세트는 https://zx-yin.github.io/msnerf에서 공개적으로 제공됩니다. 1.
--- INTRODUCTION ---
Neural Radiance Fields(NeRF) [25]와 그 변형은 신경 렌더링 커뮤니티에 활력을 불어넣고 있으며, 더욱 유망한 응용 분야에 대한 잠재력은 여전히 탐구 중입니다.NeRF는 장면을 간단한 다층 퍼셉트론(MLP)에 의해 저장된 연속적인 광도장으로 표현하고 카메라에서 이미지 평면까지 광선을 따라 샘플링된 지점에서 MLP에서 쿼리된 밀도와 광도를 통합하여 새로운 뷰를 렌더링합니다.첫 번째 발표[25] 이후로 무한 장면으로 확장[2,50], 움직이는 객체 처리[29, 30, 37] 또는 야생의 사진에서 재구성[6,21,35,49]과 같이 방법을 개선하기 위한 많은 노력이 조사되었습니다.*Bo Ren이 해당 저자입니다. SALOON SALOON (a) Mip-NeRF 360 [2], SSIM=0.825 (b) 당사 모델, SSIM=0.그림 1. (a) Mip-NeRF 360은 무한 장면을 처리할 수 있지만 여전히 가상 이미지가 NeRF 기반 방법에 매우 중요한 다중 뷰 일관성을 위반하기 때문에 반사 표면으로 어려움을 겪습니다. (b) 당사 방법은 기존 NeRF 유사 방법이 거의 추가 비용 없이 가상 이미지를 학습하는 데 도움이 될 수 있습니다. 그러나 거울이 있는 장면을 렌더링하는 것은 최첨단 NeRF 유사 방법에 여전히 어려운 작업입니다. NeRF 방법의 주요 가정 중 하나는 대상 장면의 다중 뷰 일관성 속성입니다[16, 20, 36]. 공간에 거울이 있는 경우 시점을 장면 주위로 360도 이동하도록 허용하면 거울 표면과 반사된 가상 이미지가 작은 범위의 뷰에서만 볼 수 있기 때문에 거울의 앞면과 뒷면 뷰 사이에 일관성이 없습니다. 결과적으로, 최적이 아닌 수렴에 빠지는 것을 방지하기 위해 반사 표면에 수동으로 레이블을 지정해야 하는 경우가 많습니다[12]. 이 논문에서는 수동 레이블 지정 없이 360도 고화질 장면 렌더링에서 거울과 같은 객체를 자동으로 처리할 수 있는 새로운 다중 공간 NeRF 기반 방법을 제안합니다. 유클리드 장면 공간을 하나의 단일 공간으로 간주하는 대신, 위치와 뷰 방향에 따라 구성이 변경되는 여러 가상 하위 공간으로 구성된 것으로 처리합니다. 이러한 다중 공간 분해를 사용하는 접근 방식이 유클리드 실제 공간에서 다중 뷰 일관성이 크게 위반되는 복잡한 반사 및 굴절을 성공적으로 처리하는 데 도움이 됨을 보여줍니다. 또한 저비용 다중 공간 모듈을 설계하고 원래 출력 계층을 그것으로 대체함으로써 위의 이점을 얻을 수 있음을 보여줍니다. 따라서, 우리의 다중 공간 접근법은 NeRF 기반 백본에 대한 일반적인 향상 역할을 하여, 그림 1에서 보듯이 대부분의 NeRF 유사 방법에 복잡한 반사 및 굴절을 모델링하는 기능을 제공합니다.기존 데이터 세트는 RFFR[12]이 정면을 향한 장면만 가지고 있고, [42]의 Shiny 데이터 세트는 시점 변화가 작고 큰 각도 척도에서 뷰에 따른 효과를 나타낼 수 없기 때문에 거울과 같은 물체가 포함된 장면의 360도 렌더링에 충분한 주의를 기울이지 않았습니다.따라서 복잡한 반사 및 굴절이 포함된 장면의 360도 고화질 렌더링을 평가하는 데 전념하는 새로운 데이터 세트를 구성합니다.이 데이터 세트에서 우리는 25개의 합성 장면과 7개의 캡처된 실제 장면을 수집합니다.각 합성 장면은 반사 또는 굴절 물체 주위의 360도 이미지로 구성되며, 100개는 훈련용으로, 10개는 검증용으로, 10개는 평가용으로 무작위로 분할됩니다. 각 실제 세계 장면은 반사 및 굴절 객체가 있는 장면 주변에서 무작위로 캡처되며, 62~118개의 이미지로 구성되고 LLFF [24] 규칙에 따라 구성됩니다. 그런 다음 세 가지 대표적인 기준선 모델인 NeRF [25], Mip-NeRF [1], Mip-NeRF 360 [2]을 사용하여 다중 공간 모듈이 있는 경우와 없는 경우를 비교하여 접근 방식의 우수성과 호환성을 입증합니다. 실험 결과, 접근 방식이 반사 및 굴절이 있는 장면에서 정량적, 질적으로 성능을 대폭 개선하는 것으로 나타났습니다. 주요 기여는 다음과 같습니다. • 360도 고화질 장면 렌더링에서 거울과 같은 객체를 자동으로 처리하는 다중 공간 NeRF 방법을 제안하여 기존의 대표적인 기준선보다 정량적, 질적으로 상당한 개선을 달성합니다. • 대부분의 NeRF 유사 방법에 작은 계산 오버헤드로 반사 및 굴절을 모델링할 수 있는 기능을 제공할 수 있는 가벼운 모듈을 설계합니다. • 우리는 합성 장면과 7개의 실제 촬영 장면을 포함하여 복잡한 반사와 굴절을 포함하는 장면의 360도 고화질 렌더링을 평가하기 위한 데이터 세트를 구성합니다. 2.
--- RELATED WORK ---
s 좌표 기반 새로운 뷰 합성. NeRF[25]는 컴퓨터 비전과 컴퓨터 그래픽 간의 격차를 메우고 포즈를 취한 이미지만으로 고품질의 사실적인 장면을 렌더링하는 유망한 방법을 보여줍니다. 이 방식의 통찰력과 일반화 능력은 CV와 CG 모두에서 다양한 작업, 즉 3D 재구성[28,40], 3D 인식 생성[4,15,27], 3D 인식 편집[39,47], 아바타 재구성 및 조작[9,18,52]을 용이하게 합니다. 게다가 연구자들은 이 방식을 개선하기 위해 많은 노력을 기울였습니다. Mip-NeRF[1]는 통합 위치 인코딩을 사용하여 3D 원뿔대 기능을 제공하여 NeRF의 앤티 앨리어싱 기능을 향상시킵니다. [14,23]는 이 방식을 HDR 이미지에 적용합니다. [2,50]는 NeRF와 그 변형을 무한 장면으로 확장합니다. 또한 명시적 또는 하이브리드 표현을 사용하여 학습 및 추론 속도를 높이려는 많은 연구가 있습니다[5,7, 10,26,32,34,46]. 높은 반사율을 가진 광택 소재는 NeRF와 유사한
--- METHOD ---
s는 반사 물체의 존재로 인해 종종 흐릿하거나 왜곡된 렌더링이 발생합니다. 단일 광도장을 계산하는 대신, 병렬 하위 공간의 특징 필드 그룹을 사용하여 장면을 표현하는 다중 공간 신경 광도장(MS-NeRF)을 제안합니다. 이를 통해 반사 및 굴절 물체의 존재에 대한 신경망을 더 잘 이해할 수 있습니다. 다중 공간 방식은 기존 NERF 방법을 향상시키는 역할을 하며, 추가 공간 출력을 학습하고 추론하는 데 필요한 계산 오버헤드가 적습니다. 대표적인 NeRF 기반 모델인 NeRF, Mip-NeRF, Mip-NeRF 360을 사용하여 접근 방식의 우수성과 호환성을 보여줍니다. 합성 장면과 복잡한 반사 및 굴절이 있는 7개의 실제 캡처 장면으로 구성된 새롭게 구성된 데이터 세트에서 비교를 수행하며, 모두 360도 관점을 갖습니다. 광범위한
--- EXPERIMENT ---
s는 우리의 접근 방식이 거울과 같은 물체를 통과하는 복잡한 빛 경로와 관련된 고품질 장면을 렌더링하는 데 있어 기존의 단일 공간 NeRF 방법보다 상당히 우수한 성능을 보인다는 것을 보여줍니다. 우리의 코드와 데이터 세트는 https://zx-yin.github.io/msnerf에서 공개적으로 사용할 수 있습니다. 1. 소개 Neural Radiance Fields(NeRF)[25]와 그 변형은 신경 렌더링 커뮤니티에 활력을 불어넣고 있으며, 더욱 유망한 응용 프로그램에 대한 잠재력은 여전히 탐색 중입니다. NeRF는 장면을 간단한 다층 퍼셉트론(MLP)에 의해 저장된 연속적인 광도장으로 표현하고 카메라에서 이미지 평면까지 광선을 따라 샘플링된 지점에 의해 MLP에서 쿼리된 밀도와 광도를 통합하여 새로운 뷰를 렌더링합니다. 첫 번째 발표[25] 이후로, 무한한 장면으로 확장[2,50], 움직이는 물체 처리[29, 30, 37], 야생의 사진에서 재구성[6,21,35,49]하는 것과 같이 이 방법을 개선하기 위한 많은 노력이 조사되었습니다.*Bo Ren은 해당 저자입니다.SALOON SALOON (a) Mip-NeRF 360[2], SSIM=0.825 (b) 당사 모델, SSIM=0.그림 1. (a) Mip-NeRF 360은 무한한 장면을 처리할 수 있지만, 여전히 가상 이미지가 다중 뷰 일관성을 위반하기 때문에 반사 표면이 발생하는데, 이는 NeRF 기반 방법에 매우 중요합니다.(b) 당사 방법은 기존 NeRF 유사 방법이 추가 비용을 거의 들이지 않고 가상 이미지를 학습하는 데 도움이 될 수 있습니다. 그러나 거울이 있는 장면을 렌더링하는 것은 최첨단 NeRF 유사 방법에 여전히 어려운 작업입니다. NeRF 방법에 대한 주요 가정 중 하나는 대상 장면의 다중 뷰 일관성 속성입니다[16, 20, 36]. 공간에 거울이 있는 경우 시점을 장면 주위로 360도 이동하도록 허용하면 거울의 앞면과 뒷면 뷰 사이에 일관성이 없습니다.거울 표면과 반사된 가상 이미지는 작은 범위의 뷰에서만 볼 수 있기 때문입니다.결과적으로 최적이 아닌 수렴에 빠지는 것을 방지하기 위해 반사 표면에 수동으로 레이블을 지정해야 하는 경우가 많습니다[12].이 논문에서는 수동 레이블 지정 없이 장면의 360도 고화질 렌더링에서 거울과 같은 객체를 자동으로 처리할 수 있는 새로운 다중 공간 NeRF 기반 방법을 제안합니다.유클리드 장면 공간을 하나의 단일 공간으로 간주하는 대신 위치와 뷰 방향에 따라 구성이 변경되는 여러 가상 하위 공간으로 구성된 것으로 처리합니다. 우리는 이러한 다중 공간 분해를 사용하는 우리의 접근 방식이 유클리드 실수 공간에서 다중 뷰 일관성이 심하게 위반되는 복잡한 반사 및 굴절을 성공적으로 처리하는 데 도움이 됨을 보여줍니다. 나아가, 우리는 저비용 다중 공간 모듈을 설계하고 원래 출력 계층을 그것으로 대체함으로써 위의 이점을 얻을 수 있음을 보여줍니다. 따라서 우리의 다중 공간 접근 방식은 NeRF 기반 백본에 대한 일반적인 향상 역할을 하여 대부분의 NeRF 유사 방법에 그림 1과 같이 복잡한 반사 및 굴절을 모델링하는 기능을 제공합니다. 기존 데이터 세트는 RFFR [12]이 앞을 향한 장면만 가지고 있고 [42]의 Shiny 데이터 세트는 관점 변화가 작고 큰 각도 스케일에서 뷰 종속 효과를 나타낼 수 없는 것처럼 거울과 같은 물체가 포함된 장면의 360도 렌더링에 충분한 주의를 기울이지 않았습니다. 따라서 우리는 복잡한 반사 및 굴절이 포함된 장면의 360도 고정밀 렌더링을 평가하는 데 전념하는 새로운 데이터 세트를 구성합니다. 이 데이터 세트에서 우리는 25개의 합성 장면과 7개의 캡처된 실제 세계 장면을 수집합니다. 각 합성 장면은 반사 또는 굴절 물체 주변 360도 이미지로 구성되며, 100개는 훈련용으로, 10개는 검증용으로, 10개는 평가용으로 무작위로 분할됩니다. 각 실제 세계 장면은 반사 및 굴절 물체가 있는 장면 주변에서 무작위로 캡처되며, 62~118개의 이미지로 구성되고 LLFF [24] 규칙에 따라 구성됩니다. 그런 다음 세 가지 대표적인 기준 모델인 NeRF [25], Mip-NeRF [1], Mip-NeRF 360 [2]을 사용하여 다중 공간 모듈이 있는 경우와 없는 경우를 비교하여 우리 접근 방식의 우수성과 호환성을 입증합니다. 실험 결과, 우리 접근 방식은 반사 및 굴절이 있는 장면에서 정량적, 질적으로 모두 성능을 크게 향상시킵니다. 우리의 주요 기여는 다음과 같습니다. • 우리는 360도 고화질 장면 렌더링에서 거울과 같은 객체를 자동으로 처리하는 다중 공간 NeRF 방법을 제안하여 기존의 대표적 기준선에 비해 양적, 질적으로 상당한 개선을 달성했습니다. • 우리는 대부분의 NeRF 유사 방법에 작은 계산 오버헤드로 반사 및 굴절을 모델링하는 기능을 제공할 수 있는 가벼운 모듈을 설계합니다. • 우리는 합성된 장면과 7개의 실제 캡처된 장면을 포함하여 복잡한 반사 및 굴절이 포함된 장면의 360도 고화질 렌더링을 위한 평가 전용 데이터 세트를 구성합니다. 2. 관련 연구 좌표 기반의 새로운 뷰 합성. NeRF[25]는 컴퓨터 비전과 컴퓨터 그래픽 간의 격차를 메웠으며 포즈를 취한 이미지만으로 고품질의 사실적인 장면을 렌더링하는 유망한 방법을 보여줍니다. 이 계획의 통찰력과 일반화 능력은 또한 CV와 CG 모두에서 다양한 작업, 즉 3D 재구성[28,40], 3D 인식 생성[4,15,27], 3D 인식 편집[39,47], 아바타 재구성 및 조작[9,18,52]을 용이하게 합니다. 게다가, 연구자들은 이 계획을 향상시키기 위해 많은 노력을 기울였습니다. Mip-NeRF[1]는 통합 위치 인코딩을 사용하여 3D 원뿔대 기능을 제공하여 NeRF의 앤티 앨리어싱 기능을 향상시킵니다.[14,23]는 이 계획을 HDR 이미지에 적용합니다.[2,50]는 NeRF와 그 변형을 무한 장면으로 확장합니다. 명시적 또는 하이브리드 표현을 사용하여 학습 및 추론 속도를 높이려는 많은 연구도 있습니다[5,7, 10,26,32,34,46]. 높은 반사율을 가진 광택 재료는 NeRF 유사 방법에 큰 영향을 미치며, [38]은 컴퓨터 그래픽에서 뷰 종속 반사 및 반사를 표현하고 렌더링하기 위한 사전 계산 기반 기술[31]에서 영감을 받았지만 가상 이미지를 텍스처로 처리할 수 없기 때문에 거울과 같은 반사 표면을 처리하지 못합니다. Guo et al. [12]은 반사 표면을 투과 부분과 반사 부분으로 분해하는 것을 제안했는데, 이는 우리 연구와 가장 관련성이 높은 연구이지만 이러한 분해는 시점이 특정 각도를 넘어갈 때까지 가상 이미지가 실제 객체와 차이가 없기 때문에 거울과 같은 객체가 있는 360도 뷰를 처리할 수 없습니다. 우리와 비슷한 또 다른 연구 분야는 다중 신경 광도장이지만 서로 다른 목적을 위해 그렇게 합니다[11,27, 32, 43, 44]. [27]은 3D 인식 생성 및 구성을 위해 객체 수준의 신경 광도장을 사용합니다. [32, 44]는 효율적인 렌더링을 위해 여러 개의 작은 MLP를 사용합니다. [11,43]은 3D 장면 분해 및 편집을 위해 여러 객체 수준 신경 광도장을 사용합니다. 일반적으로 사용되는 데이터 세트입니다. 연구자들은 다양한 작업에서 NeRF 기반 방법의 개발을 용이하게 하기 위해 많은 다른 데이터 세트를 도입하거나 구축했습니다. Mildenhall et al. [25]는 8개의 객체에 대해 개별적으로 포즈를 취한 이미지의 8개 렌더링 세트와 COLMAP [33]에서 추정한 카메라 포즈와 내재적 특성을 가진 8개의 실제로 캡처된 전면 장면이 포함된 데이터 세트를 수집합니다. 그럼에도 불구하고 이러한 장면에는 매우 일반적인 반사와 굴절이 없습니다. Wizadwongsa et al. [42]는 NeRF와 유사한 방법을 뷰 종속 효과에 테스트하기 위한 8개의 더 어려운 장면이 포함된 Shiny라는 데이터 세트를 제안하지만, 대략 전면을 향한 방식으로 캡처됩니다. Verbin et al. [38]은 더 복잡한 재료를 모델링하는 방법을 테스트하기 위해 NeRF에서 수행한 것과 유사한 조건에서 렌더링되는 Shiny Blender라는 6개의 광택 객체의 데이터 세트를 만듭니다. 무제한 장면의 경우, Barron et al. [2]는 5개의 실외 장면과 4개의 실내 장면으로 구성된 데이터 세트를 구성하는 반면, Zhang et al. [50]은 Tanks and Temples(T&amp;T) 데이터 세트 [19]와 Light Field 데이터 세트 [48]를 채택했습니다.Bemanal et al. [3]은 넓은 범위에서 움직이는 카메라가 있는 4개의 장면으로 구성된 굴절 물체로 구성된 데이터 세트를 캡처합니다.Guo et al. [12]는 반사 및 반투명 재료가 있는 6개의 앞을 향한 장면을 수집합니다.이것은 현재까지 우리 데이터 세트와 가장 관련성이 있지만 우리의 데이터 세트는 훨씬 더 어렵습니다.DTU 데이터 세트 [17]와 BlendedMVS 데이터 세트 [45]는 일반적으로 3D 재구성을 평가하기 위한 벤치마크로 사용됩니다.(a) 장난감 장면 A의 훈련 뷰.(b) 장난감 장면 B의 훈련 뷰.그림 2. 거울에 의해 생성된 가상 이미지는 작은 범위의 뷰에서만 볼 수 있으며, 이는 다중 뷰 일관성을 위반합니다.3. 방법 3.1. 예비 지식: 신경 복사장 신경 복사장(NeRF) [25]은 연속적인 체적장 형태로 장면을 인코딩하여 다층 퍼셉트론(MLP)의 가중치로 변환하고, 기존의 체적 렌더링에서 흡수 전용 모델을 채택하여 새로운 뷰를 합성합니다. 학습 과정에는 포즈를 취한 이미지의 희소 집합만 필요하고 장면을 통해 r(t) = 0 + td의 광선을 투사합니다. 여기서 o Є R³는 카메라 중심이고 d = R³는 뷰 방향이며 광선은 내재 함수와 학습 데이터의 포즈를 통해 계산할 수 있습니다. 이러한 광선이 주어지면 NeRF는 유클리드 공간에서 카메라까지의 거리 t¿만큼 3D 점 집합 {pi = 0 + tid}을 샘플링하고 다음 함수를 사용하여 이러한 점을 더 높은 차원 공간으로 투영합니다.✅(p) = [sin(p), cos(p), ..., sin(2¹¹p), cos(2º¯¹p)] (1) 여기서 L은 하이퍼파라미터이고 p는 샘플링된 점입니다.투영된 피처 {(pi)}와 광선 방향 d가 주어지면 MLP는 밀도 {σ;}와 색상 {c¿}을 출력합니다.이는 Max [22]가 검토한 적분 규칙을 사용하여 광선의 색상 C(r)을 추정하는 데 사용됩니다.N Ĉ(r) = Σ Ti(1 = exp(−σidi))ci i=(2) 여기서 T₁ = exp(-Σ±¼¦σ¡§¡)이고 di = ti – ti−1입니다. 방정식이 미분 가능하므로 모델 매개변수는 평균 제곱 오차(MSE) 손실로 직접 최적화할 수 있습니다.L = ||R| Σ ||Ĉ(r) - C(r)||TER (3) 여기서 R은 광선의 학습 배치입니다.게다가 NeRF는 계층적 샘플링 전략을 채택하여 더 높은 가중치가 누적되는 더 많은 포인트를 샘플링합니다.이러한 설계를 통해 NeRF는 대부분의 경우 새로운 뷰 합성의 최첨단 사진과 같은 결과를 얻습니다.(c) 장난감 장면 A의 렌더링 뷰.(d) 장난감 장면 B의 렌더링 뷰.그림 3. 첫 번째 행은 두 장면의 학습 뷰 예입니다.장면 A에는 거울 앞에 식물만 있는 반면, 장면 B에서는 가상 이미지가 있는 정확한 위치와 일치하도록 다른 식물을 조심스럽게 배치합니다.두 번째 행은 장난감 장면에서 학습된 바닐라 NeRF에서 렌더링된 깊이가 있는 테스트 뷰입니다.시연된 바와 같이 NeRF는 &#39;가상 이미지&#39;가 다중 뷰 일관성을 충족할 때 반사된 이미지를 텍스처로 처리하는 함정을 피할 수 있습니다.3.2. 다중 공간 신경 복사장 체적 렌더링 방정식과 MLP의 연속 표현 능력은 새로운 뷰 합성에서 NeRF 기반 방법의 성공을 보장하지만 이전 연구[12, 16, 20]에서 지적했듯이 수렴을 돕는 학습 프로세스에 숨겨진 무시할 수 없는 속성이 있는데, 바로 다중 뷰 일관성입니다. 그러나 다중 뷰 일관성은 모든 반사 표면에 의해 쉽게 위반될 수 있습니다. 그림 2에서 예를 볼 수 있듯이 거울 앞을 볼 때 마치 뒤에 물체가 있는 것처럼 반사되는 가상 이미지를 관찰할 수 있지만 옆이나 뒤에서 볼 때는 실제로 거울 뒤에 아무것도 없습니다. 실제로 이는 MLP의 피팅 프로세스를 위반하는 완전히 충돌하는 학습 배치가 있음을 의미합니다. 다중 시점 일관성의 중요성과 기존 NeRF 네트워크 구조에 미치는 영향을 실험적으로 입증하기 위해 오픈 소스 소프트웨어 Blender[8]를 사용하여 두 개의 360도 장난감 장면을 만들었습니다.각 장면은 100개의 훈련 이미지와 10개의 테스트 이미지로 구성되었습니다.그림 3a와 그림 3b에 훈련 시점 예가 나와 있습니다.두 장면의 유일한 차이점은 후자 장면에서는 거울 뒤에 거울 포즈의 실제 객체를 배치하지만 전자에서는 배치하지 않는다는 것입니다. 우리는 동일한 설정에서 이러한 장난감 장면에서 바닐라 NeRF를 별도로 훈련하고 그림 3c 및 그림 3d와 같이 테스트 세트에서 일부 보기를 렌더링합니다.이는 일부 보기에서 사라진 가상 이미지(즉, 다중 보기 일관성 위반)가 3D 장면으로 이어진다는 것을 명확히 보여줍니다.학습된 피처 필드 학습된 광도 필드 NeRF 백본 가중 합 출력 계층 Ꮎ 디코더 MLP OG 게이트 MLP 피처 맵 출력 출력 OD ᎾᏀ G eD Ꮎ 그림 4. 다중 공간 모듈은 네트워크의 출력 및 체적 렌더링 부분만 수정합니다.원래 NeRF는 밀도 σ와 광도 c의 쌍을 계산하여 누적된 색상을 얻습니다.출력 계층은 여러 병렬 피처 필드에 해당하는 밀도 {0}과 피처 {f}의 쌍을 생성합니다.그런 다음 체적 렌더링을 사용하여 여러 피처 맵을 얻습니다.두 개의 간단한 MLP, 즉 디코더 MLP와 게이트 MLP를 사용하여 이러한 피처 맵에서 RGB 맵과 픽셀별 가중치를 디코딩합니다. 모델은 반사 관련 영역에서 최적이 아닌 결과를 도출하고 렌더링에서 흐림을 생성합니다. 흥미롭게도, 기존 NeRF는 여전히 프로세스에서 다중 뷰 일관성 가정을 충족하려고 합니다. 그림 3c의 깊이 맵에서, 기존 NeRF는 보이는 가상 이미지를 반사 표면의 &quot;텍스처&quot;로 처리하여 주요 가정과 훈련 데이터의 충돌 사이에서 타협을 이룬다는 결론을 쉽게 내릴 수 있지만, 이러한 타협은 실제 장면에 대한 잘못된 이해와 더 나쁜 렌더링 결과로 이어집니다. 기존 NeRF와 달리, 반사광은 거울 대칭 방향에서 &quot;직접 방출&quot;된 것으로 볼 수 있으며, 거울 속 가상 공간 내부의 &quot;가상 소스&quot;에서 가능하다는 물리 및 컴퓨터 그래픽스의 일반적인 관점에서 영감을 받아, 다음 가정에 따라 새로운 다중 공간 NeRF 접근 방식을 구축합니다. 가정 1 반사와 굴절이 존재할 때, 실제 유클리드 장면 공간은 여러 개의 가상 하위 공간으로 분해될 수 있습니다. 각 하위 공간은 다중 뷰 일관성 속성을 충족합니다. 따라서 부분 공간의 구성 가중치는 공간적 위치와 뷰 방향에 따라 변경될 수 있습니다. 따라서 모든 부분 공간은 최종 렌더링 결과에 동적으로 기여합니다. 이런 식으로 반사 표면이 있을 때 실제 유클리드 공간에서 다중 뷰 일관성 위반은 그림 5와 같이 특정 뷰에서만 볼 수 있는 특정 부분 공간에 가상 이미지를 배치하여 극복할 수 있습니다. 3.3. 다중 공간 모듈 다중 공간 NeRF 네트워크의 순진한 구현은 각각 하나의 부분 공간 정보를 나타내는 여러 개의 작은 병렬 MLP를 사용하여 네트워크를 구성하는 것이지만, 이는 매개변수를 크게 증가시키고 수렴하기 어려운 것으로 입증되었습니다[32]. (a) 구성된 렌더링 결과. (b) RGB 및 부분 공간의 가중치. 그림 5. 5.2절의 MS-NeRFB 모델에서 일부 부분 공간의 해당 가중치를 사용하여 새로운 뷰와 몇 개의 디코딩된 이미지를 시각화합니다. 결과에 따르면 우리 방법은 가상 이미지를 특정 부분 공간으로 성공적으로 분해합니다. 또한, 3.2절의 실험은 NeRF의 현재 네트워크 구조가 3D 장면을 이해할 수 있는 잠재력을 가지고 있음을 보여줍니다.따라서 우리는 메모리 절약을 위해 원래 설계된 신경 특징 필드 체계[27]를 활용하는 컴팩트한 다중 공간 모듈(MS 모듈)을 제안하여 작은 계산 오버헤드만으로 표준 NeRF 백본 네트워크 구조에서 다중 공간 정보를 충분히 추출합니다.특히, MS 모듈은 NeRF 백본의 원래 출력 계층을 대체합니다.아래에서 모듈의 자세한 아키텍처를 설명합니다.그림 4에서 볼 수 있듯이, MS 모듈은 vanilla NeRF의 출력 부분만 수정합니다.Vanilla NeRF는 장면을 통과하는 광선을 따라 각 위치에 대한 단일 밀도 σ; 및 광도 c¿를 계산하고 Eq. (2)를 사용하여 체적 렌더링을 수행하여 누적된 색상을 얻습니다.반대로, 우리의 다중 헤드 계층은 신경 광도 필드를 신경 특징 필드로 대체합니다[27]. 구체적으로, 수정된 출력 레이어는 각 쌍이 부분 공간에 해당하는 레이를 따라 각 위치에 대해 K 밀도 {0}와 d 차원의 피처 {f}를 제공합니다. 여기서 K와 d는 하이퍼 데이터 세트 Realistic Synthetic 360° 원점 애플리케이션 유형 관점 속성 숫자 실수 앞을 향함 Shiny [25] 뷰 합성 [24,25] 뷰 합성 R [42] 뷰 합성 RS 360도 앞을 향함 비램버시안 비램버시안 앞을 향함 고반사, 굴절 Tanks and Temples(T&amp;T) [19] 뷰 합성 R 360도 무제한 장면 Mip-NeRF[2] 뷰 합성 R 360도 EikonalFields [3] 뷰 합성 R 큰 각도 뷰 무제한 장면 굴절 RFFR [12] 뷰 합성 R 앞을 향함 반사, 반투명 DTU [17] 재구성 R 360도 비램버시안 객체 15* BlendedMVS [45] 재구성 S 360도 비램버트 장면 7* Shiny Blender [38] 합성 보기 S 360-degreeRef-NeRF 실제로 캡처된 장면 [13,38] 합성 보기 R 360-degreeglossy 소재 광택 소재 표 1. NeRF 기반 방법에 일반적으로 사용되는 데이터 세트의 속성. &#39;S&#39;와 &#39;R&#39;은 각각 합성 및 실제 캡처를 나타냅니다. 이름이 없는 데이터 세트는 방법 이름으로 표시합니다. &#39;*&#39;는 NeRF 기반 방법에서 일반적으로 사용되는 장면 수를 나타냅니다. 원본 데이터 세트에는 표시된 것보다 많은 장면이 포함되어 있고 이를 고려하지 않기 때문입니다. 각각 총 부분 공간 수와 신경 특징 필드의 특징 차원에 대한 매개변수입니다. 그런 다음 각 부분 공간의 광선을 따라 특징을 통합하여 특정 관점에서 각 부분 공간의 색상 정보와 가시성을 인코딩하는 K개의 특징 맵을 수집합니다. 모든 픽셀이 동일한 방식으로 계산되므로 단순화를 위해 각 픽셀을 {F}로 표시하고 픽셀 수준에서 작업을 설명합니다. 특징의 각 픽셀 {F}는 다음을 사용하여 계산됩니다. 맵 N k(r) = (1-exp(−σ½¿¿))f}, i=(4) 여기서 상위 첨자 k는 광선이 통과하는 부분 공간을 나타냅니다. k번째 밀도 σk와 특징 f½는 k번째 부분 공간에 해당합니다. T½ = exp(-Σ³) 및 Si = ti — ti−1은 Eq. (2)에서와 마찬가지로 유사하게 계산됩니다. 그런 다음 {F}는 각각 하나의 숨겨진 레이어만 있는 두 개의 작은 MLP로 디코딩됩니다. 첫 번째는 {F}를 입력으로 받고 RGB 벡터를 출력하는 디코더 MLP입니다. 두 번째는 {F}를 입력으로 받고 특정 부분 공간의 가시성을 제어하는 가중치를 출력하는 게이트 MLP입니다. 구체적으로 다음을 사용합니다. {Fk} ©D&gt; {Ck}, {Fk} ©G&gt; {wk}, (5) 여기서 ОD는 디코더 MLP를 나타내고 OG는 게이트 MLP를 나타냅니다. 결국, MS 모듈은 각 부분 공간의 색상 기여도로 {wk}에 소프트맥스 함수를 적용하여 최종 렌더링 결과를 형성합니다.Ĉ(r) =Σ exp(w²) K Σexp(wk)Ck k=(6) Eq. (6)은 바닐라 NeRF 방법과 비교하여 추가 손실 항이 필요하지 않습니다.결과적으로, 위의 가벼운 MS 모듈은 기존 NeRF 유사 백본 네트워크에 대한 향상 모듈 역할을 할 수 있으며, 우리의 접근 방식이 Sec. 5.2에서 상당한 향상을 달성한다는 것을 보여줄 것입니다.4. 데이터 세트 4.1. 기존 데이터 세트 우리는 일반적으로 사용되거나 우리 작업에 가장 관련성이 높은 데이터 세트를 간략히 다시 살펴보고 표 1에 속성을 나열합니다.보시다시피, 거울 앞에 있는 물잔과 같이 복잡한 빛 경로가 있는 장면에 대한 360도 벤치마크가 없습니다.4.2. 제안하는 데이터 세트 Sec.에 요약된 대로 4.1, 관련 연구를 용이하게 할 복잡한 반사 및 굴절로 구성된 360도 데이터 세트가 부족합니다. 따라서 25개의 합성 장면과 7개의 실제 촬영 장면으로 구성된 360도 데이터 세트를 수집합니다. 그림 6a에 표시된 합성 부분의 경우 오픈 소스 소프트웨어 Blender[8]를 사용하고 3D 모델을 공유하는 커뮤니티인 BlenderKit의 3D 모델로 장면을 디자인합니다. 데이터 세트는 단일 개체 대신 완전한 장면으로 구성되므로 카메라 위치의 높이를 고정하여 카메라가 장면의 중심을 바라보고 원을 따라 카메라를 움직여 전체 장면을 렌더링합니다. 모든 장면에서 원을 따라 120개의 지점을 균일하게 샘플링하고 무작위로 100개의 이미지를 훈련 세트에, 10개를 검증 세트에, 10개를 테스트 세트에 선택합니다. 구성된 데이터 세트에는 반사 및 굴절 개체가 포함된 다양한 장면이 있습니다. 우리는 장면에 있는 거울의 수와 레이아웃에 의해 제어되는 다양한 복잡성의 빛 경로를 포함합니다. 여기서 거울의 수는 1개에서 수십 개의 작은 조각까지 다양합니다. 우리의 데이터 세트에서 거울이 하나뿐인 장면조차도 카메라가 합성된 데이터 세트의 (a) A 부분에서 이동하기 때문에 RFFR [25]보다 더 어렵다는 점에 유의하십시오. PSNR↑ SSIM LPIPS↓ # 매개변수 NERF MS-NeRFs 30.82 0.0.1.159M MS-NeRFM MS-NeRFB 32.39 0.32.61 0.32.77 0.0.1.201M 0.0.195 1.311M 1.245M Mip-NeRF Ref-NeRF MS-Mip-NeRFS MS-Mip-NeRFM 31.42 0.874 0.215 0.613M 32.37 0.882 0.180 0.713M 33.63 0.886 0.195 0.634M 33.80 0.887 0.193 0.656M MS-Mip-NeRFB 33.90 0.888 0.191 0.689M 31.Mip-NeRF0.MS-Mip-NeRF 360 35.04 0.906 0.(a) 데이터 세트의 합성 부분에 대한 비교. 0.9.007M 9.052M (b) 실제로 캡처한 데이터 세트의 일부. 그림 6. 데이터 세트의 데모 장면(보충 자료에 자세히 나와 있음). 데이터 세트는 다양한 반사 및 굴절을 보여주는데, 이는 복잡한 빛 경로가 있는 새로운 뷰를 합성하는 능력을 검증하기 위한 벤치마크 역할을 할 수 있습니다. 거울의 정면에서 뒷면까지. 또한 본질적으로 경계 없는 장면으로 처리할 수 있는 거울 벽이 있는 방을 구성하여 방 중앙에 거울을 추가하고 경계 없는 가상 이미지를 만듭니다. 반사와 굴절의 조합을 포함하여 도전적인 장면을 추가로 구축합니다. 또한 그림 6b에 표시된 복잡한 빛 조건을 가진 7개의 캡처한 실제 장면을 포함합니다. 우리는 두 개의 거울, 매끄러운 표면의 유리 공 하나, 다이아몬드와 같은 표면의 유리 공 하나, 몇 개의 장난감, 몇 권의 책을 사용하여 장면을 구성합니다.우리는 360도 관점에서 무작위로 사진을 촬영합니다.5. 실험 5.1. 하이퍼파라미터와 벤치마크 우리는 다른 기준선과 다른 규모의 모듈을 사용하여 다른 데이터 세트를 기반으로 세 세트의 실험을 수행합니다.우리의 모듈은 매우 간단하기 때문에 우리는 우리의 모듈을 세 개의 하이퍼파라미터로 확장할 수 있으며, 우리는 이를 각각 디코더 MLP와 게이트 MLP의 부분 공간 번호에 대한 K, 출력 피처의 차원에 대한 d, 은닉 계층 차원에 대한 h라고 합니다.공정하게 비교하기 위해, 우리는 [1,2,12,25,38]의 대부분 기본 설정을 따르는 모든 실험을 수행하지만, 배치당 1024개의 광선을 사용하고 모든 장면에서 모든 실험에 대해 200k 반복을 훈련합니다.실험 세부 사항은 다음과 같습니다.우리는 세 개의 대표적인 NeRF 기반 모델을 우리의 기준선으로 선택하고 우리의 모듈을 이것들과 통합합니다. NERF [25] 및 Mip-NeRF [1] 기반 실험의 경우, 우리는 하이퍼파라미터 {K = 6, d = 24, h = 24}(마찬가지로 24})를 갖는 MS-NeRF 및 MS-Mip-NeRF를 구축하고, 마찬가지로 하이퍼파라미터 {K = 6, d = M = Mip-NeRFMS-Mip-NeRFPSNR↑ SSIM↑ LPIPS↓ # 매개변수 26.70 0.889 0.28.14 0.891 0.9.007M 9.052M (b) 데이터 세트의 실제 캡처된 부분에 대한 비교. SSIM↑ LPIPS↓ # 매개변수 PSNR↑ NeRFReN* 35.MS-NeRFT 35.0.0.0.0.1.264M 1.295M (c) RFFR 데이터 세트에 대한 비교. ***는 제공된 설정에 따라 공식 코드를 사용하여 모델을 다시 학습한다는 것을 나타내며, 반사 표면에 사용된 마스크 수는 0입니다.표 2. 기존 방법과의 정량적 비교. = = = 48, h 48}, 그리고 하이퍼파라미터 {K 8, d 64, h = 64}를 사용한 MS-NeRFB와 MS-Mip-NeRFB. MipNERF 360 [2] 기반 실험의 경우 하이퍼파라미터 {K {K = 8,d = 32, h = 64}를 사용하여 MS-MipNERF 360을 구성합니다. 또한 Mip-NeRF를 기준으로 사용하고 광택 재료를 모델링하는 뛰어난 기능을 보유하고 있기 때문에 RefNERF [38]와 비교합니다. 또한 RFFR 데이터 세트 [12]에서 NeRFReN과 방법을 비교합니다. NeRFReN은 앞을 향한 장면의 거울과 같은 표면을 위한 바닐라 NeRF 기반의 특별히 설계된 2분기 네트워크입니다. 따라서 우리는 NeRF에 기반한 MS-NeRFT라고 하는 우리 방법의 작은 버전을 구성합니다.이것은 하이퍼파라미터 {K = 2, d = 128, h = 128}를 갖습니다.여기서 우리는 NeRFReN이 반사 표면을 두 부분으로 분해하려고 하기 때문에 두 개의 부분 공간을 사용하고, 우리의 공간 분해가 더 효과적임을 보여주고자 합니다.공정한 비교를 위해 우리는 제공된 설정에서 RFFR 데이터 세트에 대한 공식 코드를 사용하여 NeRFReN을 다시 훈련합니다.단, 우리 방법에는 추가 마스크가 필요하지 않으므로 사용된 마스크의 개수를 0으로 설정합니다.모든 훈련 세부 사항은 보충 자료에서 찾을 수 있습니다.우리는 일반적으로 사용되는 세 가지 메트릭인 PSNR, SSIM [41], LPIPS [51]로 결과를 보고합니다.(a) Mip-NeRF(b) MS-Mip-NeRF그림 7. Mip-NeRF 360과 MSMip-NeRF 360의 시각적 비교.우리 모듈은 Mip-NeRF 360을 확장하여 무제한 가상 장면을 모델링할 수 있습니다. (a) Ref-NeRF (b) MS-Mip-NeRFB 그림 9. MS-Mip-NeRFB와 RefNeRF의 시각적 비교. 우리 방법은 반사 표면에서 Ref-NeRF보다 상당히 우수한 성능을 보입니다. (a) Mip-NeRF (b) MS-Mip-NeRF 그림 8. 데이터 세트의 실제로 캡처된 부분에서 Mip-NeRF 360과 MSMip-NeRF 360의 시각적 비교. 우리 방법은 현실 세계에서 가상 이미지를 복구할 만큼 충분히 강력합니다. 5.2. 비교 정량적 비교. 표 2a에 보고된 대로, 우리 모듈은 대부분의 NeRF 유사 모델에 통합될 수 있으며 최소한의 추가 비용으로 성능을 대폭 개선할 수 있습니다. 특히 Mip-NeRF 360 기반 실험에서 우리 모듈은 단지 0.5%의 추가 매개변수로 PSNR에서 3.46dB 개선의 더 나은 결과를 보여줍니다. 게다가, 우리의 Mip-NeRF 기반 모델은 광택 있는 재료를 모델링하는 뛰어난 능력을 가진 Mip-NeRF 기반 변형인 RefNERF[38]보다 큰 마진으로 성능이 뛰어납니다.또한 표 2b에서 데이터 세트의 실제로 캡처된 부분에 대한 최신 Mip-NeRF 360 결과와 비교한 결과를 보여줍니다.우리의 접근 방식도 큰 개선을 보여줍니다.표 2c에서 볼 수 있듯이, 우리의 접근 방식은 장면에 앞을 향한 반사 표면이 포함된 RFFR 데이터 세트에서 학습 시 수동으로 레이블이 지정된 마스크가 제공되지 않을 때 더 나은 결과를 얻습니다.위의 실험은 우리 방법의 우수성과 호환성을 보여줍니다.정성적 비교 및 논의.정성적 비교 외에도 모듈의 장점을 요약하고 다음을 통해 정성적 또는 정량적으로 뒷받침합니다.(a) NeRFReN (b) MS-NeRFB 그림 10.(a) 정확하게 레이블이 지정된 마스크로 학습한 NeRFReN은 거울이 있는 360도 장면에서 장면의 일반적인 부분을 렌더링하지도 못합니다. (b) 우리의 방법은 수동으로 레이블이 지정된 추가 마스크가 필요 없고 고품질 이미지를 렌더링합니다. 우리의 방법을 해당 기준선과 비교합니다. 최신 방법(예: Mip-NeRF 360)과의 정성적 비교는 그림 1, 그림 7 및 그림 8에 나와 있습니다. 우리의 방법은 합성 및 실제 장면 모두에서 경계가 있거나 없는 고화질 가상 이미지를 렌더링합니다. 기존 NeRF 백본을 사용하여 가상 이미지를 텍스처로 이해하는 Ref-NeRF[38]와의 정성적 비교는 그림 9에 나와 있습니다. Ref-NeRF도 Mip-NeRF에 기반을 두고 있으므로 동일한 기준선을 사용하여 Mip-NeRF 기반 변형을 Ref-NeRF와 비교하고 비교에서 비슷한 매개변수(특히 우리의 0.689M 및 Ref-NeRF 0.713M)를 사용합니다. 다시 한번 정성적 결과는 반사 표면 렌더링에서 상당한 개선을 보여줍니다. 또한, 훈련 중에 반사 영역의 정확하게 레이블이 지정된 마스크가 필요하고 정면 반사 표면만 처리하는 NeRFReN 모델과도 비교합니다. 이 비교에서, 우리는 제공된 매우 정확한 반사 마스크로 합성된 데이터 세트에서 그들의 모델을 훈련합니다. 그림 10은 그들의 모델이 360도 고충실도 렌더링을 복구하지 못하는 반면, 우리의 접근 방식은 성공한다는 것을 보여줍니다. 5.3. 절제 연구 이 섹션에서는 모듈의 설계를 평가하고 부분 공간의 수와 가상 이미지의 수 사이의 관계를 살펴봅니다. 개요 장면 장면 지상 진실 MS-NeRF MS-NeRF 평균 NeRF 그림 11. NeRF, MS-NeRF 평균 및 MS-NeRF 비교. NERF MS-NERF Avg MS-NeRFs PSNR↑ SSIM↑ LPIPS↓ 30.82 0.866 0.209 1.159M 31.45 0.868 0.205 1.166M 32.39 0.872 0.201 1.201M # Params PSNRMS-NeRFd=MS-NeRFd=MS-NeRFd=NERF8# of sub-spacesTable 3. 모듈 아키텍처에 대한 절제 연구. 신경 특징 필드를 사용한 절제. 3차원의 K 스칼라 {σ½}와 K RGB 벡터 {c}를 간단히 출력하는 모듈을 구현한 다음 NeRF와 동일한 적분 방정식을 사용하여 각 부분 공간의 픽셀 색상을 구하고 모든 부분 공간의 평균을 구하여 최종 픽셀 색상을 구합니다. 이 설계를 MS-NeRF Aug로 알려진 바닐라 NeRF에 통합합니다. 여기서 K = 6으로 설정하고 결과를 표에 보고합니다. 3. 또한 그림 11에서 몇 가지 시각적 결과를 보여주는데, 이는 단순한 다중 공간 복사장 가정이 모델이 반사 위반을 부분적으로 극복하는 데 도움이 될 수 있지만 효율적인 다중 공간 구성 전략이 부족하기 때문에 지나치게 매끄럽게 만드는 문제가 발생할 수 있음을 나타냅니다.부분 공간 수에 대한 소거. 유클리드 공간에서는 거울의 수와 레이아웃으로 가상 하위 공간의 수를 제어할 수 있습니다. 예를 들어 두 거울이 서로 마주보고 있는 경우 무한히 재귀적인 가상 이미지 공간이 있을 수 있지만 두 거울이 서로 마주보게 배치되면 각 거울 뒤에 가상 이미지가 하나만 있게 됩니다. 모듈 설계에 대한 지침을 제공하기 위해 데이터 세트의 합성된 부분에서 레이아웃이 다른 두 거울로 구성된 두 장면을 선택하고 다른 하위 공간 수와 다른 특징 크기의 NeRF 기반 변형을 학습합니다. 우리는 출력 특징 차원 d = {24, 48, 64}와 부분 공간 수 K = {2,4,6, ..., 16}를 갖는 NeRF 기반 변형을 구성한 다음 두 장면에서 모델을 학습하고 PSNR을 사용하여 결과를 보고합니다.그림 12의 결과는 부분 공간 수가 실제 가상 이미지 공간 수와 일치할 필요가 없으며 6개의 부분 공간이 다중 공간 광도 필드에 대해 안정적인 학습을 보장할 수 있음을 보여줍니다.또한 차원 d == 24인 특징 필드는 이미 구성에 충분한 정보를 인코딩하지만 안정적인 성능을 위해서는 d = 48이 더 좋습니다.6.
--- CONCLUSION ---
이 논문에서는 NeRF 기반 방법에서 반사 표면을 렌더링하는 오랜 문제를 해결합니다. 유클리드 공간을 여러 개의 가상 부분 공간으로 분해하는 다중 공간 NeRF 방법을 소개합니다. 제안하는 MS-NeRF 방식은 기존 NeRF 기반 방법에 비해 상당히 더 나은 결과를 얻습니다. 게다가 MS 모듈의 경량 설계를 통해 기존 NeRF 백본 네트워크를 향상시키는 데 도움이 됩니다. 또한 유사한 작업을 평가하기 위한 새로운 데이터 세트를 구성하여 커뮤니티의 미래 연구에 도움이 되기를 바랍니다. 감사의 말: 이 연구는 중국 자연과학기금(NO. 62132012)의 지원을 받았습니다. 참고문헌 [1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan. Mip-nerf: 신경 광도장의 안티앨리어싱을 위한 다중 스케일 표현. ICCV, 페이지 5855-5864, 2021. 2,[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan 및 Peter Hedman. Mip-nerf 360: 무제한 앤티앨리어싱 신경 복사 효과. IEEE CVPR, 페이지 5470-5479, 2022. 1, 2, 5,[3] Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad, Hans-Peter Seidel 및 Tobias Ritschel. 굴절성 신규시점 합성을 위한 에이코날 필드. ACM SIGGRAPHConference Proceedings, 1-9페이지, 2022. 2,[4] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. 효율적인 기하학 인식 3D 생성적 적대 네트워크. IEEE CVPR, 16123-16133페이지, 2022.[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su. Tensorf: 텐서 광도장. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022.[6] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, Jue Wang. 야생에서의 환각 신경 광채장. IEEE CVPR, 12943-12952페이지, 2022.[7] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, Andrea Tagliasacchi. Mobilenerf: 모바일 아키텍처에서 효율적인 신경장 렌더링을 위한 폴리곤 래스터화 파이프라인 활용. arXiv:2208.00277, 2022.[8] Blender 온라인 커뮤니티. Blender - 3D 모델링 및 렌더링 패키지. Blender Foundation, Stichting Blender Foundation, 암스테르담, 2022. 3,[9] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J Black, Timo Bolkart. 단안 비디오에서 신체 및 의류 캡처 및 애니메이션. arXiv:2210.01868, 2022.[10] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie Shotton, Julien Valentin. Fastnerf: 200fps에서의 고충실도 신경 렌더링. ICCV, 14346-14355페이지, 2021.[11] Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser. 객체 중심 신경 장면 렌더링. arXiv:2012.08503, 2020.[12] Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, SongHai Zhang. Nerfren: 반사를 포함한 신경 광도장. IEEE CVPR, 18409-18418페이지, 2022. 1, 2, 3, 5,[13] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, Paul Debevec. 실시간 뷰 합성을 위한 신경 광도 필드 베이킹. ICCV, 5875-5884페이지, 2021.[14] Xin Huang, Qi Zhang, Ying Feng, Hongdong Li, Xuan Wang, Qing Wang. Hdr-nerf: 고동적 범위 신경 광도 필드. IEEE CVPR, 18398-18408페이지, 2022.[15] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, Ben Poole. 꿈 필드를 사용한 제로샷 텍스트 가이드 객체 생성. IEEE CVPR, 867-876페이지, 2022.[16] Nishant Jain, Suryansh Kumar, Luc Van Gool. 신경 복사장의 다중 스케일 표현 강화. arXiv:2210.04233, 2022. 1,[17] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, Henrik Aanæs. 대규모 다중 뷰 입체시 평가. IEEE CVPR, 406-413페이지, 2014. 2,[18] Boyi Jiang, Yang Hong, Hujun Bao, Juyong Zhang. Selfrecon: 단안 비디오에서 디지털 아바타 자체 재구성. IEEE CVPR, 5605-5615페이지, 2022.[19] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM TOG, 36(4):1–13, 2017. 2,[20] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey. Barf: Bundle-adjusting nervous radiance fields. ICCV, 5741-5751쪽, 2021. 1,[21] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. IEEE CVPR, 72107219쪽, 2021.[22] Nelson Max. 직접 볼륨 렌더링을 위한 광학 모델. IEEE TVCG, 1(2):99–108, 1995.[23] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, Jonathan T Barron. 어둠 속의 Nerf: 노이즈가 많은 원시 이미지에서 높은 동적 범위 뷰 합성. IEEE CVPR, 16190–16199페이지, 2022.[24] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar. 국소 광장 융합: 처방적 샘플링 가이드라인을 사용한 실용적인 뷰 합성. ACM TOG, 38(4):1-14, 2019. 2,[25] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. ACM 커뮤니케이션, 65(1):99–106, 2021. 1, 2,3,5,[26] Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 원시. ACM TOG, 41(4), 2022년 7월.[27] Michael Niemeyer와 Andreas Geiger. 기린: 구성적 생성 신경 특징 필드로 장면 표현. IEEE CVPR, 페이지 11453–11464, 2021. 2,[28] Michael Oechsle, Songyou Peng 및 Andreas Geiger. Unisurf: 다중 뷰 재구성을 위해 신경 암시적 표면과 복사 필드를 통합합니다. ICCV, 페이지 5589–5599, 2021.[29] 박근홍, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, Ricardo Martin-Brualla. Nerfies: 변형 가능한 신경 방사 필드. ICCV, 페이지 5865-5874, 2021.[30] 알베르트 푸마롤라, 엔릭 코로나, 제라드 폰스-몰, 프란세스크 모레노-노게르. D-nerf: 역동적인 장면을 위한 신경 복사 필드. IEEE CVPR, 10318–10327페이지, 2021.[31] Ravi Ramamoorthi et al. 사전 계산 기반 렌더링. Foundations and TrendsⓇ in Computer Graphics and Vision, 3(4):281-369, 2009.[32] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: 수천 개의 작은 mlps로 신경 광도장 속도 높이기. ICCV, 14335–14345페이지, 2021. 2,[33] Johannes L Schonberger and Jan-Michael Frahm. Structurefrom-motion 재검토. IEEE CVPR, 4104-4113페이지, 2016.[34] Cheng Sun, Min Sun, and Hwann-Tzong Chen. 직접 복셀 그리드 최적화: 복사장 재구성을 위한 초고속 수렴. IEEE CVPR, 5459-5469페이지, 2022.[35] Jiaming Sun, Xi Chen, Qianqian Wang, Zhengqi Li, Hadar Averbuch-Elor, Xiaowei Zhou, Noah Snavely. 야생에서의 신경 3D 재구성. ACM SIGGRAPHConference Proceedings, 1-9페이지, 2022.[36] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, W Yifan, Christoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. 신경 렌더링의 발전. 컴퓨터 그래픽 포럼, 41(2):703-735, 2022.[37] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner, Christian Theobalt. 비강체 신경 광도장: 단안 비디오의 동적 장면의 재구성 및 새로운 뷰 합성. ICCV, 12959-12970페이지, 2021.[38] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, Pratul P. Srinivasan. Ref-NeRF: 신경 광도장에 대한 구조화된 뷰 종속 모양. CVPR, 2022. 2, 5, 6,[39] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao. Clip-nerf: 신경 광도장의 텍스트 및 이미지 기반 조작. IEEE CVPR, 38353844페이지, 2022.[40] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, Wenping Wang. Neus: 다중 뷰 재구성을 위한 볼륨 렌더링을 통한 신경 암묵적 표면 학습. 신경 정보 처리 시스템의 발전, 34:27171-27183, 2021.[41] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. IEEE 이미지 처리 거래, 13(4):600-612, 2004.[42] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, Supasorn Suwajanakorn. Nex: 신경 기반 확장을 통한 실시간 뷰 합성. IEEE CVPR, 페이지 8534-8543, 2021. 2,[43] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang 및 Zhaopeng Cui. 편집 가능한 장면 렌더링을 위한 객체 구성 신경 복사 필드를 학습합니다. ICCV, 페이지 13779-13788, 2021.[44] 양궈웨이(Guo-Wei Yang), 저우웬양(Wen-Yang Zhou), 펑하오양(Hao-Yang Peng), 둔량(Dun Liang), 무타이장(Tai-Jiang Mu), 후시민(Shi-Min Hu). 재귀적 너프(Recursive-nerf): 효율적이고 역동적으로 성장하는 너프입니다. IEEE TVCG, 2022.[45] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang 및 Long Quan. Blendedmvs: 일반화된 다중 뷰 스테레오 네트워크를 위한 대규모 데이터 세트. IEEE CVPR, 1790-1799페이지, 2020. 2,[46] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa. 신경 광도장의 실시간 렌더링을 위한 Plenoctrees. ICCV, 5752-5761페이지, 2021.[47] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, Lin Gao. Nerf-editing: 신경 광도장의 기하 편집. IEEE CVPR, 18353-18364페이지, 2022.[48] Kaan Yücer, Alexander Sorkine-Hornung, Oliver Wang, Olga Sorkine-Hornung. 고밀도 샘플링된 광장에서 효율적인 3D 객체 분할을 3D 재구성에 적용. ACM TOG, 35(3):1-15, 2016.[49] Jason Zhang, Gengshan Yang, Shubham Tulsiani, Deva Ramanan. Ners: 야생에서 희소 뷰 3D 재구성을 위한 신경 반사 표면. 신경 정보 처리 시스템의 발전, 34:29835-29847, 2021.[50] Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun. Nerf++: 신경 광도장 분석 및 개선. arXiv:2010.07492, 2020. 1,[51] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. 지각적 척도로서의 딥 피처의 비합리적인 효과. IEEE CVPR, 2018년 586~595페이지.[52] Yufeng Zheng, Victoria Fernández Abrevaya, Marcel C Bühler, Xu Chen, Michael J Black 및 Otmar Hilliges. Im 아바타: 비디오에서 암시적으로 변형 가능한 머리 아바타입니다. IEEE CVPR, 페이지 13545-13555, 2022.
