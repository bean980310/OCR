--- ABSTRACT ---
최근 텍스트-오디오(TTA) 생성에서 진전이 있었음에도 불구하고, AudioCaps와 같은 불균형 클래스 분포를 가진 데이터 세트에서 학습된 AudioLDM과 같은 최첨단 모델은 생성 성능에 편향되어 있음을 보여줍니다. 구체적으로, 이러한 모델은 일반적인 오디오 클래스를 생성하는 데는 뛰어나지만 드문 클래스에서는 성능이 떨어져 전체 생성 성능이 저하됩니다. 이 문제를 롱테일 텍스트-오디오 생성이라고 합니다. 이 문제를 해결하기 위해 TTA 모델에 대한 간단한 검색 증강 접근 방식을 제안합니다. 구체적으로, 입력 텍스트 프롬프트가 주어지면 먼저 대조 언어 오디오 사전 학습(CLAP) 모델을 활용하여 관련 텍스트-오디오 쌍을 검색합니다. 그런 다음 검색된 오디오-텍스트 데이터의 기능을 추가 조건으로 사용하여 TTA 모델의 학습을 안내합니다. 제안된 접근 방식으로 AudioLDM을 향상시키고 결과적으로 증강된 시스템을 Re-AudioLDM이라고 합니다. AudioCaps 데이터 세트에서 Re-AudioLDM은 1.37의 최첨단 Frechet Audio Distance(FAD)를 달성하여 기존 접근 방식보다 큰 차이로 성능이 뛰어납니다. 또한 Re-AudioLDM은 복잡한 장면, 희귀한 오디오 클래스, 심지어 보이지 않는 오디오 유형에 대해서도 사실적인 오디오를 생성할 수 있음을 보여주어 TTA 작업에서 잠재력을 보여줍니다. 색인 용어 오디오 생성, 검색 정보, 확산 모델, 딥 러닝, 롱테일 문제 1.
--- INTRODUCTION ---
텍스트-오디오(TTA) 생성의 풍경은 확산 기반 생성 모델링의 발전으로 혁신되었습니다[1, 2, 3]. CLAP[1] 및 대규모 언어 모델(LLM)[4]과 같은 강력한 백본 모델을 활용하여 이러한 모델은 의미 정보를 추출하고 텍스트 설명에서 고충실도 오디오를 생성할 수 있습니다. 이 연구에서 우리는 오디오 학습 데이터의 부족과 다양성으로 인해 이러한 최첨단 모델에 편향이 나타나 성능이 크게 저하된다는 것을 보여줍니다. 그림 1(위)은 가장 큰 오디오-텍스트 데이터 세트 중 하나인 AudioCaps[5]의 327개 레이블에 대해 수행한 통계 분석을 나타내며, 데이터 분포에 상당한 불균형이 있음을 나타냅니다. 그림 1의 왼쪽 아래 그래프는 AudioCaps로 학습한 최신 모델의 샘플 결과를 보여줍니다. &quot;남자가 말하고 샴페인을 터뜨리고 웃음&quot;이라는 프롬프트를 제공했을 때 모델은 &quot;남자가 말하고 있음&quot;에 대한 콘텐츠만 생성할 수 있었지만 &quot;샴페인이 터짐&quot;에 이어 &quot;웃음&quot;과 같은 흔하지 않거나 복잡한 이벤트는 놓쳤습니다. 따라서 학습 데이터 세트의 범위와 가변성이 제한되어 내재적인 한계가 보이는데, 생성된 사운드의 품질은 학습 중에 나타나는 빈도와 크게 상관 관계가 있는 것으로 보입니다. 이와 관련하여 이러한 모델은 일반적인 사운드 이벤트에 대해 사실적인 오디오 클립을 충실하게 생성할 수 있지만 덜 빈번하거나 보이지 않는 사운드 이벤트가 발생하면 잘못되거나 관련성이 없는 오디오 클립을 생성할 수 있습니다. 이를 롱테일 텍스트-오디오 생성 문제로 표시하며, 이는 다양성 측면에서 모델 성능에 영향을 미치고 빈도(숫자)를 제한합니다.40남자가 말하고 있음 AudioCaps 100 120 140 160 180 200 220 240 260 280 300---웃음 텍스트 프롬프트: 한 남자가 말을 하다가 샴페인을 터뜨리고 웃는다.샴페인을 터뜨리다 그림 1. AudioCaps 데이터 세트의 롱테일 문제(위).베이스라인 모델(왼쪽)과 Re-AudioLDM(오른쪽)으로 생성된 예시 오디오 클립(아래).특히 실제 시나리오에서 이러한 모델의 적용 가능성.저희의 동기는 불균형 데이터의 장벽을 깨고 다양한 음향 엔터티에서 현실적인 생성을 달성하는 강력한 TTA 프레임워크를 개발하는 것입니다.롱테일 생성 문제를 해결하기 위해 새로운 검색 증강 TTA 프레임워크를 제안합니다.최신 TTA 모델인 AudioLDM [1]을 ReAudioLDM이라는 검색 모듈로 향상시킵니다. 구체적으로, 먼저 입력 텍스트 프롬프트를 사용하여 데이터 세트(예: AudioCaps)에서 관련 참조(예: 텍스트-오디오 쌍)를 검색한 다음, 사전 훈련된 오디오 모델과 언어 모델을 사용하여 각각 음향 및 텍스트 특징을 추출합니다. 이러한 추출된 특징은 생성 프로세스를 안내하기 위해 LDM의 교차 주의[6] 모듈에 추가로 제공됩니다. 검색된 오디오-텍스트 쌍은 훈련 단계에서 저주파 오디오 이벤트의 모델링을 개선하는 데 도움이 되는 보충 정보 역할을 합니다. 추론 단계에서 검색 증강 전략은 또한 텍스트 프롬프트와 관련된 참조를 제공하여 보다 정확하고 충실한 오디오 생성 결과를 보장합니다. 데이터 세트에서 발생 빈도가 다른 이벤트에 대한 광범위한 실험을 수행합니다. Re-AudioLDM이 다양한 오디오 엔터티에서 안정적인 성능을 제공함을 보여줍니다. 기준 모델에 비해 테일 클래스의 성능을 크게 향상시켜 롱테일 TTA 문제에 대한 효과적인 완화를 제공할 수 있음을 보여줍니다. 또한 기준 모델과 비교했을 때 Re-AudioLDM은 희귀하고 복잡하거나 보이지 않는 오디오 이벤트를 포함하여 더욱 사실적이고 복잡한 오디오 클립을 생성할 수 있습니다. 그림(아래)에 표시된 동일한 프롬프트의 예에서 Re-AudioLDM(오른쪽 아래)은 복잡한 구조로 &quot;샴페인 터짐&quot;이라는 흔하지 않은 엔터티와 &quot;웃음&quot;이라는 소리를 모두 생성할 수 있어 모든 필수 엔터티와 의미적 순서를 올바르게 갖춘 기준 모델보다 더 나은 결과를 달성했습니다. 또한 Re-AudioLDM은 1.37의 FAD 점수를 달성하여 최첨단 TTA 모델보다 훨씬 우수한 성과를 거두었습니다. 이 논문의 나머지 부분은 다음과 같이 구성됩니다. 섹션 &quot;샴페인 한 병을 터뜨린 다음 유리잔에 붓습니다.&quot; 입력 프롬프트 데이터베이스 CLAP 인코더 검색 오디오 및 언어 기능 LDM 교차 주의 I AudioMAE 오디오 기능 VAE 디코더 HiFi-GAN &quot;샴페인을 터뜨리는 동안 한 남자가 &quot;물을 유리잔에 붓습니다.&quot; &quot;물 약간 Ttalks&quot; 유리잔에 물을 붓습니다.&quot; 언어 기능 출력 파형 그림 2. Re-AudioLDM의 개요 구조는 다음을 소개합니다.
--- RELATED WORK ---
오디오 생성 및 검색 기반 모델에 대한 자세한 내용은 3장에서 설명하고, Re-AudioLDM에 대한 세부 정보는 4장에서 다룹니다. 4장에서는 실험 설정을 제시하고 5장에서는 결과와 절제 연구를 보여줍니다. 결론은 6장에서 제공합니다. 2. 관련 연구 저희의 연구는 확산 기반 텍스트-오디오 모델과 검색 기반 생성 모델이라는 두 가지 주요 연구와 관련이 있습니다. 이 두 분야는 다음 하위 섹션에서 간략하게 설명합니다. 2.1. 오디오 생성 오디오 생성에 대한 최근 연구는 인코더-디코더 프레임워크를 따릅니다[1, 7]. 이 모델은 먼저 인코더를 사용하여 정보를 잠재적 표현으로 인코딩한 다음, 이를 멜 스펙트로그램 특징으로 압축 해제할 수 있습니다. 그런 다음 디코더는 변분 자동 인코더(VAE)와 생성적 적대 네트워크(GAN) 보코더를 사용하여 이러한 특징을 파형으로 변환합니다. Liu et al. [8]은 레이블을 나타내는 인코더로 PixelSNAIL [9]을 사용한 반면 Iashin과 Rahtu [10]는 입력 이미지를 인코딩하는 인코더로 GPT2 [11]를 적용했습니다. 이후, 잠재 토큰 생성을 위해 확산 기반 모델이 사용되었습니다. Yang et al. [12]은 변압기 기반 인코더를 확산 기반 인코더로 교체했습니다. Liu et al. [1]은 CLAP 모델 [13]을 사용하여 입력 데이터(오디오 또는 텍스트)에 대한 임베딩을 얻고 잠재 확산 모델(LDM)을 토큰 생성기로 사용했습니다. Ghosal et al. [4]는 CLAP을 LLM [14]으로 교체하여 이 프레임워크를 더욱 개선했습니다. 2.2. 검색된 정보 집계 이미지 생성 분야의 여러 연구에서 검색된 정보를 활용하는 것을 고려했습니다. Li et al. [15]은 학습 세트에서 이미지 특징을 추출하여 메모리 뱅크에 넣은 다음 오디오 생성을 위한 병렬 입력 조건으로 사용합니다. Blattmannet et al. [16]은 이웃 지역에서 관련 이미지 샘플을 선택하기 위한 최근접 이웃 전략을 제시합니다. KNN-Diffusion [17]은 추론 단계에서 대규모 검색 데이터베이스에서 얻은 이미지 특징을 사용하여 새로운 도메인 이미지 생성을 수행합니다. Chen et al. [18]은 이미지 전용 검색을 이미지-텍스트 쌍 검색으로 확장하여 확산 모델에 대한 고수준 의미론과 저수준 시각 정보를 모두 보강합니다. 이와 대조적으로 오디오 생성에 대한 유사한 작업은 수행되지 않았으며 Re-AudioLDM은 텍스트-오디오 생성 성능을 개선하기 위해 데이터 세트에서 검색된 정보를 도입한 최초의 시도입니다. 3. 제안
--- METHOD ---
이전 오디오 생성 작업[1, 4, 19]과 유사하게 Re-AudioLDM은 입력 임베딩, 확산 기반 기능 생성기, 잠재 기능에서 파형을 재구성하는 파이프라인의 세 부분으로 구성된 계단식 모델입니다.3.1. 텍스트 및 검색 임베딩 인코더 Re-AudioLDM은 두 개의 병렬 입력을 사용합니다.저수준 의미 정보인 텍스트 입력 ct와 고수준 의미-오디오 정보를 위한 검색 증강 cr인 텍스트-오디오 쌍 세트입니다.텍스트 임베딩 Et는 다음과 같이 얻습니다.Et = fclap (Ct) (1) 여기서 flap()은 AudioLDM [1]에서와 같이 텍스트 인코딩에 사용되는 CLAP 모델 [20]입니다.검색된 정보 cr. = = [<text₁, audio1 > &lt;text2, audio2&gt;, ..., &lt;text, audiok&gt;]는 대상 캡션의 임베딩과 검색 데이터 세트의 임베딩 사이의 유사성 비교를 통해 선택된 상위 k 이웃입니다.여기서 각 쌍에 대해 멀티모달 임베딩은 오디오 검색 Era와 텍스트 검색 Ert로 표현되는 두 개의 연결 그룹으로 나뉩니다.다음과 같이 인코딩됩니다.Era = CAT rt(audio1), ...,fmae(audiok)], Et CAT(text1), ...,fts(text)] = (2) 여기서 fts()는 텍스트 임베딩을 얻기 위한 사전 학습된 T5 모델[14]이고 fmae(•)는 쌍 오디오의 임베딩을 얻기 위한 사전 학습된 AudioMAE 모델[21]입니다.3.2. 검색 증강 확산 생성기 Re-AudioLDM은 생성기로 LDM을 사용하여 대상 오디오의 중간 잠재 토큰을 얻습니다.확산 모델에는 두 가지 프로세스가 포함됩니다.잠재 벡터에 점진적으로 노이즈를 추가하는 순방향 프로세스와 각 단계에서 잠재 벡터의 전이 노이즈를 점진적으로 예측하는 역방향 프로세스입니다.순방향 단계 동안 잠재 표현 zo는 연속 노이즈 주입과 함께 표준 가우시안 분포 Zn으로 변환됩니다.q(zn|Zn−1) = N(zn; √√1 – BnZn−1, ßnĪ), q(zn|z0) = N(Zn; √ānzo, (1 – ān)€) (4) (5) 여기서 e는 노이즈 레벨을 제어하는 an = 1 - ẞn이 있는 가우시안 노이즈를 나타냅니다. 역 프로세스에서 LDM은 방정식 (1)로 계산된 텍스트 임베딩 Et와 방정식 (2) 및 (3)으로 각각 계산된 검색된 임베딩 Era 및 Ert에서 주어진 조건에 따라 잠재 공간에서 노이즈 e의 분포를 추정하는 방법을 학습합니다.LDM 모델은 일반 구조로 UNet을 적용하며, 여기서 입력 계층은 노이즈가 있는 잠재 벡터 Zn, 텍스트 임베딩 Et 및 시간 단계 n을 조건으로 취합니다.그런 다음 텍스트와 오디오의 검색된 정보는 나머지 계층 내의 모든 교차 주의 블록과 공유됩니다.가중치가 다시 지정된 학습 목표[22]를 사용하여 LDM은 다음과 같이 학습됩니다.Ln(0) = Ezo,e,n ||€ – €0(Zn, n, E², Attn(Era, Ert))||2 (6) 3.3. VAE 디코더 및 Hifi-GAN 보코더 Re-AudioLDM은 잠재 특징 토큰에서 파형을 재구성하기 위한 일반 파이프라인으로 VAE와 HiFi-GAN의 조합을 활용합니다. 학습 단계에서 VAE는 멜 스펙트로그램을 중간 표현으로 인코딩한 다음 멜 스펙트로그램으로 다시 디코딩하는 방법을 학습하는 반면, Hifi-GAN은 멜 스펙트로그램을 파형으로 변환하도록 학습합니다. 추론을 위해 Re-AudioLDM은 멜 스펙트로그램 재구성을 위해 VAE 디코더를 적용하고 파형 생성을 위해 HiFi-GAN을 적용합니다. 4.1. 데이터 세트 4.
--- EXPERIMENT ---
데이터 세트에서 발생 빈도가 다른 이벤트에 대한 s. Re-AudioLDM이 다양한 오디오 엔터티에서 안정적인 성능을 제공함을 보여줍니다. 기준 모델에 비해 테일 클래스의 성능이 크게 향상되어 롱테일 TTA 문제에 대한 효과적인 완화를 제공할 수 있음을 보여줍니다. 또한 기준 모델과 비교할 때 Re-AudioLDM은 드물거나 복잡하거나 보이지 않는 오디오 이벤트를 포함하여 보다 사실적이고 복잡한 오디오 클립을 생성할 수 있습니다. 그림(아래)에 표시된 동일한 프롬프트의 예에서 Re-AudioLDM(오른쪽 아래)은 복잡한 구조로 &quot;샴페인 터짐&quot;이라는 흔하지 않은 엔터티와 &quot;웃음&quot;이라는 소리를 모두 생성할 수 있어 모든 필수 엔터티와 의미 순서를 올바르게 갖춘 기준 모델보다 더 나은 결과를 달성합니다. 또한 Re-AudioLDM은 1.37의 FAD 점수를 달성하여 최첨단 TTA 모델보다 큰 차이로 성능이 우수합니다. 이 논문의 나머지 부분은 다음과 같이 구성됩니다. 섹션 &quot;샴페인 한 병을 터뜨린 다음 유리잔에 붓습니다.&quot; 입력 프롬프트 데이터베이스 CLAP 인코더 검색 오디오 및 언어 기능 LDM 교차 주의 I AudioMAE 오디오 기능 VAE 디코더 HiFi-GAN &quot;샴페인을 터뜨리는 동안 한 남자가 &quot;물을 유리잔에 붓습니다.&quot; &quot;물 약간을 이야기합니다.&quot; 언어 기능 출력 파형 그림 2. Re-AudioLDM의 개요 구조는 오디오 생성 및 검색 기반 모델의 관련 작업을 소개한 다음 섹션 3에서 Re-AudioLDM에 대한 세부 정보를 제공합니다. 섹션 4에서는 실험 설정을 제시하고 섹션 5에서는 결과와 절제 연구를 보여줍니다.
--- CONCLUSION ---
6절에 s가 나와 있습니다.2. 관련 연구 저희의 연구는 확산 기반 텍스트-오디오 모델과 검색 기반 생성 모델이라는 두 가지 주요 연구와 관련이 있습니다.이 두 분야는 다음 하위 섹션에서 간략하게 설명합니다.2.1. 오디오 생성 오디오 생성에 대한 최근 연구는 인코더-디코더 프레임워크[1, 7]를 따릅니다.이 모델은 먼저 인코더를 사용하여 정보를 잠재 표현으로 인코딩한 다음 이를 멜-스펙트로그램 특징으로 압축 해제할 수 있습니다.그런 다음 디코더는 변이 자동 인코더(VAE)와 생성적 적대 네트워크(GAN) 보코더를 사용하여 이러한 특징을 파형으로 변환합니다.Liu et al.[8]은 PixelSNAIL[9]을 인코더로 사용하여 레이블을 나타내는 반면 Iashin과 Rahtu[10]는 GPT2[11]를 인코더로 적용하여 입력 이미지를 인코딩했습니다.그 후 확산 기반 모델이 잠재 토큰 생성에 사용되었습니다.Yang et al. [12]는 변압기 기반 인코더를 확산 기반 인코더로 대체합니다.Liu et al. [1]은 CLAP 모델 [13]을 사용하여 입력 데이터(오디오 또는 텍스트)에 대한 임베딩을 얻고, 토큰 생성기로 Latent Diffusion Model(LDM)을 사용합니다.Ghosal et al. [4]는 CLAP을 LLM [14]으로 대체하여 이 프레임워크를 더욱 개선합니다.2.2. 검색된 정보 집계 이미지 생성 분야의 여러 연구에서 검색된 정보를 활용하는 것을 고려했습니다.Li et al. [15]는 학습 세트에서 이미지 특징을 추출하여 메모리 뱅크에 넣은 다음 오디오 생성을 위한 병렬 입력 조건으로 사용합니다.Blattmannet et al. [16]은 이웃 영역에서 관련 이미지 샘플을 선택하기 위한 최근접 이웃 전략을 제시합니다.KNN-Diffusion [17]은 추론 단계에서 대규모 검색 데이터베이스에서 얻은 이미지 특징을 사용하여 새로운 도메인 이미지 생성을 수행합니다.Chen et al. [18] 이미지 전용 검색을 이미지-텍스트 쌍 검색으로 확장하여 확산 모델을 위해 고수준 의미론과 저수준 시각 정보를 모두 증강합니다. 반면 오디오 생성에 대한 유사한 작업은 수행되지 않았으며 Re-AudioLDM은 텍스트-오디오 생성 성능을 개선하기 위해 데이터 세트에서 검색된 정보를 도입한 최초의 시도입니다. 3. 제안된 방법 이전 오디오 생성 작업[1, 4, 19]과 유사하게 Re-AudioLDM은 입력 임베딩, 확산 기반 기능 생성기 및 잠재 기능에서 파형을 재구성하는 파이프라인의 세 부분으로 구성된 계단식 모델입니다. 3.1. 텍스트 및 검색 임베딩 인코더 Re-AudioLDM은 저수준 의미 정보인 텍스트 입력 ct와 고수준 의미-오디오 정보를 위한 검색 증강 cr인 텍스트-오디오 쌍 세트의 두 가지 병렬 입력을 사용합니다. 텍스트 임베딩 Et는 다음과 같이 얻습니다. Et = fclap (Ct) (1) 여기서 flap()은 AudioLDM [1]에서와 같이 텍스트 인코딩에 사용되는 CLAP 모델 [20]입니다. 검색된 정보 cr. = = [<text₁, audio1 > &lt;text2, audio2&gt;, ..., &lt;text, audiok&gt;]는 대상 캡션의 임베딩과 검색 데이터 세트의 임베딩 사이의 유사성 비교를 통해 선택된 상위 k 이웃입니다.여기서 각 쌍에 대해 멀티모달 임베딩은 오디오 검색 Era와 텍스트 검색 Ert로 표현되는 두 개의 연결 그룹으로 나뉩니다.다음과 같이 인코딩됩니다.Era = CAT rt(audio1), ...,fmae(audiok)], Et CAT(text1), ...,fts(text)] = (2) 여기서 fts()는 텍스트 임베딩을 얻기 위한 사전 학습된 T5 모델[14]이고 fmae(•)는 쌍 오디오의 임베딩을 얻기 위한 사전 학습된 AudioMAE 모델[21]입니다.3.2. 검색 증강 확산 생성기 Re-AudioLDM은 생성기로 LDM을 사용하여 대상 오디오의 중간 잠재 토큰을 얻습니다.확산 모델에는 두 가지 프로세스가 포함됩니다.잠재 벡터에 점진적으로 노이즈를 추가하는 순방향 프로세스와 각 단계에서 잠재 벡터의 전이 노이즈를 점진적으로 예측하는 역방향 프로세스입니다.순방향 단계 동안 잠재 표현 zo는 연속 노이즈 주입과 함께 표준 가우시안 분포 Zn으로 변환됩니다.q(zn|Zn−1) = N(zn; √√1 – BnZn−1, ßnĪ), q(zn|z0) = N(Zn; √ānzo, (1 – ān)€) (4) (5) 여기서 e는 노이즈 레벨을 제어하는 an = 1 - ẞn이 있는 가우시안 노이즈를 나타냅니다. 역 프로세스에서 LDM은 방정식 (1)로 계산된 텍스트 임베딩 Et와 방정식 (2) 및 (3)으로 각각 계산된 검색된 임베딩 Era 및 Ert에서 주어진 조건에 따라 잠재 공간에서 노이즈 e의 분포를 추정하는 방법을 학습합니다.LDM 모델은 일반 구조로 UNet을 적용하며, 여기서 입력 계층은 노이즈가 있는 잠재 벡터 Zn, 텍스트 임베딩 Et 및 시간 단계 n을 조건으로 취합니다.그런 다음 텍스트와 오디오의 검색된 정보는 나머지 계층 내의 모든 교차 주의 블록과 공유됩니다.가중치가 다시 지정된 학습 목표[22]를 사용하여 LDM은 다음과 같이 학습됩니다.Ln(0) = Ezo,e,n ||€ – €0(Zn, n, E², Attn(Era, Ert))||2 (6) 3.3. VAE 디코더 및 Hifi-GAN 보코더 Re-AudioLDM은 잠재 특징 토큰에서 파형을 재구성하기 위한 일반 파이프라인으로 VAE와 HiFi-GAN의 조합을 활용합니다. 학습 단계에서 VAE는 멜 스펙트로그램을 중간 표현으로 인코딩한 다음 다시 멜 스펙트로그램으로 디코딩하는 방법을 학습하는 반면, Hifi-GAN은 멜 스펙트로그램을 파형으로 변환하도록 학습합니다. 추론을 위해 Re-AudioLDM은 멜 스펙트로그램 재구성을 위해 VAE 디코더를 적용하고 파형 생성을 위해 HiFi-GAN을 적용합니다. 4.1. 데이터 세트 4. 실험 실험을 위해 AudioCaps 데이터 세트[25]를 사용하는데, 이는 46,000개의 10초 오디오 클립으로 구성되며 각각에 인간이 주석을 단 캡션이 있습니다. 각 학습 오디오 클립에 단일 캡션이 지정되는 공식 학습-테스트 분할을 따르는 반면 테스트 분할에서는 각 오디오 클립에 5개의 캡션이 주석으로 달립니다. 추론 단계에서는 테스트 분할에 나타나는 각 오디오 클립의 첫 번째 캡션을 텍스트 입력으로 사용합니다.나머지 네 개의 캡션은 섹션 5의 절제 연구에만 사용합니다.4.2 실험 설정 데이터 준비.검색 기반 AudioCaps 데이터 세트의 경우 CLAP 점수 기반 검색 함수를 적용하여 대상 텍스트 임베딩의 상위 50개 가장 가까운 이웃을 찾습니다.각 이웃의 파형과 텍스트는 텍스트-오디오 쌍으로 저장됩니다.교육 및 테스트 샘플 모두에서 대상 샘플은 검색 정보에서 제외되므로 교육 및 추론 단계에서 대상 데이터에 액세스할 수 없습니다.구현 세부 정보.캐스케이드 모델인 Re-AudioLDM의 인코더 및 디코더 부분은 16kHz로 샘플링된 오디오 클립으로 별도로 학습됩니다. 대상의 경우 1024개 샘플의 윈도우와 160개 샘플의 홉 크기를 갖는 단시간 푸리에 변환(STFT)을 사용하여 64개 멜 필터뱅크가 있는 멜 스펙트로그램을 생성합니다. 그런 다음 VAE 모델을 적용하여 스펙트로그램을 4의 비율로 압축하여 주파수 차원이 16인 피처 벡터를 생성합니다. 검색 전략에서 제공하는 정보의 경우 텍스트 피처는 사전 학습된 T5-medium 모델에서 직접 추출하여 고정된 시퀀스 길이 50을 제공합니다. 반면 오디오 피처는 먼저 128개 멜 빈이 있는 필터 뱅크로 변환한 다음 사전 학습된 AudioMAE 모델에서 처리하여 차원 32의 벡터를 생성합니다. 학습 세부 정보. LDM은 5.0 × 105의 학습률로 최적화되었습니다.Re-AudioLDM은 배치 크기 4로 최대 80에포크 동안 학습되고 평가는 100,단계마다 수행됩니다.Re-AudioLDM-S는 128개 채널로 구성된 UNet 아키텍처를 적용하는 반면, 우리는 더 복잡한 모델에 대한 실험을 위해 196개 채널의 Re-AudioLDM-L로 모델을 확대합니다.평가 지표.Liu et al.에 따라 Inception Score(IS), Fréchet Audio Distance(FAD), Kullback-Leibler(KL) 발산을 사용하여 Re-AudioLDM의 성능을 평가합니다.IS 점수가 높을수록 생성된 오디오의 다양성이 크고, KL 및 FAD 점수가 낮을수록 오디오 품질이 좋음을 나타냅니다. 의미 수준 평가를 위해 CLAP 인코더에서 계산한 출력 오디오 임베딩과 대상 텍스트 임베딩 간의 코사인 유사도를 계산하여 오디오와 텍스트 간의 상관 관계를 보여줍니다.5.1. 평가 결과 5. 결과 실험은 AudioCaps 평가 세트에서 수행되었습니다.AudioGen[23], AudioLDM[1] 및 Tango[4]를 포함한 여러 최신 프레임워크와 성능을 비교했습니다.각 오디오 클립의 첫 번째 캡션만 텍스트 설명으로 선택하면 각 프레임워크는 샘플링 속도가 16kHz인 975개의 10초 오디오 클립을 추론합니다.표 1은 다양한 텍스트-오디오 모델로 달성한 메트릭을 비교하며, Re-AudioLDM은 다른 방법보다 큰 차이로 성능이 우수합니다.검색에서 제공되는 추가 정보 없이 Re-AudioLDM은 어떠한 이점도 보이지 않으며 일반적으로 AudioCaps의 현재 최신 모델인 Tango보다 열등합니다. 그러나 검색 정보를 통합하면 Re-AudioLDM은 네 가지 평가 지표 모두에서 기준 모델보다 우수한 성과를 거두었습니다.LDM 구조에서 숨겨진 계층의 크기를 확대함으로써 10개의 검색된 쌍을 사용하는 Re-AudioLDM-L은 FAD 점수를 1.4 이하로 더욱 낮추어 기준 프레임워크에 비해 상당한 개선을 이루었습니다.5.2. 절제 연구 검색 유형.표 1의 실험은 오디오, 텍스트 또는 둘 다 아닌 다른 검색 정보에 대한 결과를 보여줍니다.AudioMAE에서 추출한 오디오 기능을 사용하면 Re-AudioLDM으로 약간의 개선만 달성되는데, 이는 주로 Re-AudioLDM이 관련 사운드 이벤트의 기능은 포착하지만 사운드 이벤트 간의 관계는 놓치기 때문입니다.검색된 각 오디오 클립의 쌍 텍스트 정보를 추가함으로써 Re-AudioLDM은 오디오 기능과 고수준 의미 정보 간의 관계를 학습하여 사운드 이벤트에 대한 매우 관련된 의미 기능을 포착하는 데 상당한 개선을 기여합니다. 검색된 쌍의 수. 여러 실험을 수행하여 검색된 오디오-텍스트 쌍의 수가 오디오 생성 성능에 미치는 영향을 평가했습니다. 그림 3에서 볼 수 있듯이 검색 정보를 통합하면 검색된 쌍의 수가 증가함에 따라 성능이 향상되지만, 수가 5에 도달한 후에는 향상 속도가 느려지고 10개 정도로 평평해집니다. 따라서 학습 비용과 모델 성능 간의 균형을 맞추기 위해 이 데이터의 검색된 쌍 수는 경험적으로 3~5 범위로 선택되었습니다. 롱테일 상황. Re-AudioLDM은 롱테일 생성 문제를 해결하고 흔하지 않거나 보이지 않는 사운드 이벤트에 대해 더욱 사실적인 오디오 클립을 생성하는 것을 목표로 합니다. 생성된 각 오디오 클립의 정확도를 평가하기 위해 CLAP 점수[20]를 적용하여 오디오 클립과 텍스트 설명 간의 관계를 보여주었습니다. 먼저 각 오디오 클립의 레이블을 세어 각 사운드 이벤트의 발생 빈도를 계산한 다음 모델 데이터 집합 검색 정보 검색 번호 KL↓ IS ↑ FAD↓ CLAP 점수(%)↑ AudioGen [23] AC+AS+8개 기타 Х Х 1.69 5.13 2.23.AudioLDM [1] Tango [4] AC+AS+2개 기타 Х Х 1.66 6.2.25.AudioCaps ✓ 1.32 6.45 1.29.AudioCaps Х 1.63 6.2.26.AudioCaps Audio1.6.1.31.Re-AudioLDM-S AudioCaps Audio &amp; Text1.7.1.37.AudioCaps Audio &amp; Text1.23에 대한 각 사운드 클래스의 CLAP 점수를 평균하여 모델 성능을 설명합니다. 7.1.37.Re-AudioLDM-L AudioCaps Audio &amp; Text1.20 7.39 1.37.표 1. 검색 정보가 있는 경우와 없는 경우의 다양한 프레임워크 비교.AC와 AS는 각각 AudioCaps[5]와 AudioSet[24]의 약자입니다.KL IS FAD AC_cap2.AC_cap1.AC_cap2 7.AC_cap2.1.6AC_capAC_capAC_cap4 7.2.AC_cap1.AC_cap1.6.1.36.AC_capAC_cap2 1.AC_capAC_capAC_capAC_cap1.1.이벤트 수 그림 3. 검색된 정보 수에 대한 성능 비교, 여기서 AC cap 1-5는 테스트 세트의 캡션 그룹을 나타냅니다.전체 성능에서. 믹스업 방법과 대조적으로, 제안하는 검색 증강 전략은 학습 프로세스의 복잡성을 줄여 전반적인 성능이 향상됩니다.LClap 점수(%)AudioLDM Tango Re-AM-S-rRe-AM-S-rRe-AM-L-r1 10 30 50 100 500빈도(숫자)10 30 50 100 500빈도(숫자) AudioCaps 테스트 세트.그림의 왼쪽에 있는 막대 그래프는 AudioCaps 학습 세트의 모든 327개 사운드 이벤트 클래스의 수량에 대한 통계 분석을 나타냅니다.그림 1(위)과 유사하게, 테일 클래스는 상당한 부분을 구성하며, 특히 1과 10의 레이블 그룹에서 그렇습니다.그림 4(오른쪽)는 학습 세트 내에서 이벤트 발생 빈도가 다른 이벤트에 대한 각 모델의 성능을 보여줍니다.Re-AudioLDM과 기준 모델 간에 매우 빈번한 오디오 이벤트에 대한 초기 격차에도 불구하고, 기준 모델은 테일이 있는 엔터티를 처리할 때 성능이 떨어집니다. 그러나 ReAudioLDM은 훈련 데이터에서 이벤트 발생 빈도가 감소함에 따라 CLAP 점수가 3 미만으로 감소하여 더 안정적인 결과를 보였습니다.따라서 Re-AudioLDM은 꼬리가 달린 사운드 이벤트를 생성할 때 출력 품질 저하를 줄여 전반적인 모델 성능을 향상시킬 수 있습니다.제로 샷 생성.보이지 않는 엔터티에 대한 실험의 경우 훈련 중에 제외된 이벤트가 있는 여러 시나리오를 평가합니다.그림 4(오른쪽)에서 기준 모델은 보이지 않는 오디오(발생 빈도 0)를 생성할 때 성능 저하를 보입니다.이는 모델이 보이지 않는 엔터티의 기능을 학습하지 못한 반면 Re-AudioLDM은 관련 오디오 및 의미 정보를 제공하여 여전히 현실적인 결과를 얻을 수 있기 때문일 수 있습니다.따라서 필수적인 검색 정보를 통해 Re-AudioLDM은 훈련 데이터에서 제외된 사운드를 생성할 수 있는 잠재력이 있습니다.검색 기반 생성은 앞으로 탐구할 방향 중 하나인 제로 샷 작업의 견고성을 크게 향상시킬 수 있습니다.믹스업 전략과의 비교. 클래스 불균형 문제를 해결하는 또 다른 방법은 믹스업 전략을 사용하는 것입니다[26]. 믹스업은 테일 엔터티의 발생 빈도를 높일 수 있지만, 더 복잡한 오디오 예제와 실제 분포와 일치하지 않을 수 있는 합성 오디오 데이터를 도입합니다. [1]의 결과에 따르면 믹스업 전략은 저하로 이어집니다. 그림 4. 다른 주파수 엔터티의 성능, 여기서 S와 L은 모델 크기를 나타내고 r은 검색된 클립의 수를 나타냅니다. 6. 결론 이 논문에서는 AudioCaps의 롱테일 문제를 해결하기 위해 검색 증강 모델인 ReAudioLDM을 제시했습니다. 여러 성능 지표(예: FAD, CLAPscore)를 사용하여 최신 모델(예: AudioLDM 및 Tango)과 비교한 결과 Re-AudioLDM이 고충실도 오디오 클립을 생성하는 데 있어 TTA 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다. 검색된 기능을 통합함으로써 Re-AudioLDM은 전반적인 성능을 개선할 뿐만 아니라 드물거나 보이지 않는 사운드 엔터티를 생성할 수 있습니다. 향후 작업에서는 외부의 대규모 데이터 세트로 모델을 조사하고 제로샷 생성과 같은 다운스트림 작업에서 모델의 잠재력을 탐색할 것입니다. 7. 감사의 말 이 연구는 부분적으로 China Scholarship Council(CSC)의 연구 장학금, British Broadcasting Corporation Research and Development(BBC R&amp;D), Engineering and Physical Sciences Research Council(EPSRC) Grant EP/T019751/1 &quot;AI for Sound&quot;의 자금 지원, 그리고 University of Surrey의 Centre for Vision, Speech and Signal Processing(CVSSP)의 박사 장학금으로 지원되었습니다. 오픈 액세스를 목적으로 저자는 발생하는 모든 Author Accepted Manuscript 버전에 Creative Commons Attribution(CC BY) 라이선스를 적용했습니다. 참고문헌 [1] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, MD Plumbley, &quot;AudioLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성&quot;, International Conference on Machine Learning, 2023. [2] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, Z. Zhao, &quot;오디오 만들기: 프롬프트 강화 확산 모델을 사용한 텍스트-오디오 생성&quot;, International Conference on Machine Learning, 2023. [3] X. Liu, Z. Zhu, H. Liu, Y. Yuan, M. Cui, Q. Huang, J. Liang, Y. Cao, Q. Kong, MD Plumbley, W. Wang, &quot;Wavjourney: 대규모 언어를 사용한 구성 오디오 생성 모델,&quot; arXiv 사전 인쇄본 arXiv:2307.14335, 2023. [4] D. Ghosal, N. Majumder, A. Mehrish, 및 S. Poria, &quot;명령 조정 LLM 및 잠재 확산 모델을 사용한 텍스트-오디오 생성,&quot; arXiv 사전 인쇄본 arXiv:2304.13731, 2023. [5] CD Kim, B. Kim, H. Lee, 및 G. Kim, &quot;AudioCaps: 야생 오디오에 대한 캡션 생성,&quot; 북미 전산 언어학 협회 연례 컨퍼런스, 2019. [6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, L. Kaiser, 및 I. Polosukhin, &quot;주의만 있으면 됩니다,&quot; 신경 정보 처리 시스템의 발전, 제 23권 4호, 2019년 11월. 30, 2017. [7] Y. Yuan, H. Liu, J. Liang, X. Liu, MD Plumbley, 및 W. Wang, &quot;사전 훈련된 오디오 학습을 사용하여 사운드 생성: 벤치마크 연구&quot;, 유럽 신호 처리 협회, 2023. [8] X. Liu, T. Iqbal, J. Zhao, Q. Huang, M. Plumbley, 및 W. Wang, &quot;신경 이산 시간-주파수 표현 학습을 사용한 조건부 사운드 생성&quot;, IEEE 신호 처리를 위한 기계 학습 국제 워크숍, 2021. [9] X. Chen, N. Mishra, M. Rohaninejad, 및 P. Abbeel, &quot;PixelSNAIL: 개선된 자기 회귀 생성 모델&quot;, 국제 기계 학습 컨퍼런스, 2018, 864-872쪽. [10] V. Iashin 및 E. Rahtu, &quot;시각적으로 유도되는 사운드 길들이기 영어: tion,&quot; in British Machine Vision Conference, 2021. genera[11] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, &quot;Language models are unsupervised multitask learners,&quot; OpenAI blog, vol. 1, no. 8, p. 9, 2019. [12] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, D. Yu, &quot;Diffsound: 텍스트-사운드 생성을 위한 이산 확산 모델&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 트랜잭션, 2023. [13] A. Radford, JW Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., &quot;자연어 감독에서 이전 가능한 시각적 모델 학습&quot;, 기계 학습 국제 컨퍼런스, 2021, 8748-8763쪽. [14] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, PJ Liu, &quot;한계 탐색 영어: 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 경우,&quot; Journal of Machine Learning Research, vol. 21, no. 1, pp. 54855551, 2020. [15] B. Li, PH Torr, and T. Lukasiewicz, &quot;메모리 기반 텍스트-이미지 생성,&quot; British Machine Vision Conference, 2022. [16] A. Blattmann, R. Rombach, K. Oktay, J. Müller, and B. Ommer, &quot;검색 증강 확산 모델,&quot; Advances in Neural Information Processing Systems, vol. 35, pp. 15 309-15 324, 2022. [17] S. Sheynin, O. Ashual, A. Polyak, U. Singer, O. Gafni, E. Nachmani, and Y. Taigman, “KNN-diffusion: Image generation via large-scale retrieval,” in International Conference on Learning Representations, 2023. [18] W. Chen, H. Hu, C. Saharia, and WW Cohen, “Re-Imagen: Retrieval-augmented text-to-image generator,” in International Conference on Learning Representations, 2023. [19] Y. Yuan, H. Liu, X. Kang, P. Wu, MD Plumbley, and W. Wang, “Text-driven foley sound generation with latent diffusion model,” in Proceedings of the Detection and Classification of Acoustic Scenes and Events Workshop, 2023, 231-235쪽. [20] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, &quot;특징 융합 및 키워드-캡션 증강을 통한 대규모 대조 언어-오디오 사전 학습&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, ICASSP, 2023. [21] H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, C. Feichtenhofer et al., &quot;수신하는 마스크 자동 인코더&quot;, arXiv 사전 인쇄:2207.06405, 2022. [22] J. Ho, A. Jain, and P. Abbeel, &quot;노이즈 제거 확산 확률적 모델&quot;, 신경 정보 처리 시스템, 2020. [23] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman, 및 Y. Adi, &quot;AudioGen: 텍스트로 안내되는 오디오 생성,&quot; 국제 학습 표현 컨퍼런스, 2023. [24] JF Gemmeke, DPW Ellis, D. Freedman, A. Jansen, W. Lawrence, RC Moore, M. Plakal, 및 M. Ritter, &quot;AudioSet: 오디오 이벤트를 위한 온톨로지 및 인간 레이블 데이터 세트,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2017, pp. 776-780. [25] T. Heittola, A. Mesaros 및 T. Virtanen, &quot;TAU Urban Acoustic Scenes 2019, Development dataset,&quot; 2019년 3월. [온라인]. 사용 가능: https://doi.org/10.5281/zenodo.[26] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang 및 MD Plumbley, &quot;PANN: 오디오 패턴 인식을 위한 대규모 사전 학습 오디오 신경망&quot;, IEEE/ACM 오디오, 음성 및 언어 처리 저널, 제28권, 2880-2894쪽, 2020년.
