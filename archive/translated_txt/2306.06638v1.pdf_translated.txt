--- INTRODUCTION ---
텍스트-이미지 합성 분야는 최근 확산 모델의 발전 덕분에 급속한 발전을 경험했습니다. 자유형 텍스트를 조건으로 하여 이제 전례 없는 품질과 다양성의 이미지를 생성할 수 있습니다. 그러나 사용자가 제공한 이미지에서 사람을 묘사한 이미지를 생성하는 것은 여전히 어려운 작업입니다. 이러한 격차를 극복하기 위해 기존 방법은 추론 시간 동안 최적화 문제를 해결하는 데 의존합니다. 예를 들어 모델 미세 조정[Ruiz et al. 2022] 또는 이미지를 텍스트 임베딩 공간으로 반전[Gal et al. 2022]합니다. 이러한 방법은 좋은 결과를 생성하지만 시간이나 메모리에 비용이 많이 듭니다. 그림 1. Face0의 생성 예. 단일 얼굴 이미지(왼쪽)와 텍스트 프롬프트(위)가 주어지면 이미지를 생성하는 데 몇 초 밖에 걸리지 않습니다. 미세 조정이나 반전과 같은 최적화 절차 없이 샘플 시간 내에 얼굴에 대한 텍스트-이미지 생성 모델을 즉시 조건 지정하는 새로운 방법인 Face0을 제시합니다. 우리는 포함된 얼굴의 임베딩으로 주석이 달린 이미지의 데이터 세트를 증강하고 증강된 데이터 세트에서 이미지 생성 모델을 훈련합니다. 훈련이 완료되면, 우리 시스템은 추론 시점에 기본 모델과 실질적으로 동일하므로 사용자가 제공한 얼굴 이미지와 프롬프트가 주어지면 불과 몇 초 만에 이미지를 생성할 수 있습니다. 우리의 방법은 만족스러운 결과를 얻고, 놀라울 정도로 간단하고, 매우 빠르며, 기본 모델에 텍스트를 통하거나 입력 얼굴 임베딩을 직접 조작하여 생성된 이미지를 제어하는 것과 같은 새로운 기능을 제공합니다. 또한, 사용자가 제공한 이미지의 얼굴 임베딩 대신 고정된 랜덤 벡터를 사용할 때, 우리 방법은 본질적으로 이미지 전체에서 일관된 문자 생성 문제를 해결합니다. 마지막으로, 추가 연구가 필요하지만, 모델의 텍스트 편향을 얼굴에 대한 편향에서 분리하는 우리 방법이 향후 텍스트-이미지 모델에서 편향을 어느 정도 완화하는 데 한 걸음 더 다가갈 수 있기를 바랍니다. 이 연구에서는 얼굴에 안정 확산을 사용하여 이미지 생성 모델을 즉각적으로 조절하는 새로운 방법을 개발합니다. 추론 시점에서 우리의 방법은 기본 확산 모델의 표준 추론과 실질적으로 동일하며, 단일 &quot;Equal contribution&quot;에서 사람과 유사한 이미지를 즉시 생성할 수 있습니다. 저자 주소: Dani Valevski, Google Research, daniv@google.com; Danny Wasserman, Google Research, dwasserman@google.com; Yossi Matias, Google Research, yossi@google.com; Yaniv Leviathan, Google Research, leviathan@google.com. 2. Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan 노이즈 추가 감지 및 삽입 자르기 훈련 &quot;테니스 선수&quot; CLIP으로 삽입 프로젝트 확산 모델 MSE 손실 그림 2. Face0에 대한 훈련 계획(2절 참조). 점선 빨간색 화살표를 제외한 모든 것은 표준 확산 모델 훈련 절차의 일부입니다. 단순화를 위해 픽셀 공간에서 잠재 공간으로 변환하는 세부 사항은 생략합니다. 사진. 예를 들어, 그림의 이미지는 1은 왼쪽의 원본 이미지와 위쪽의 텍스트 프롬프트를 감안했을 때 불과 몇 초 만에 생성되었습니다. Face0는 특히 간단하고 효율적이면서도 만족스러운 결과를 생성합니다(그림 1). 또한, 이 방법에는 몇 가지 다른 장점이 있습니다: (1) 텍스트 프롬프트를 통해서뿐만 아니라 얼굴 임베딩 벡터를 직접 조작하여 점진적으로 머리 스타일이나 방향을 변경하는 등 생성된 얼굴을 쉽고 자연스럽게 제어할 수 있습니다(그림 4~6). (2) 사용자가 제공한 얼굴 이미지 대신 고정된 무작위 얼굴 임베딩 벡터를 사용하여 생성된 이미지에서 일관된 문자를 생성하는 문제를 간단하게 해결합니다. (3) 텍스트 프롬프트 대신 얼굴 임베딩에서 얼굴 특징을 디코딩하도록 모델을 장려하기 때문에 모델의 텍스트 편향 중 일부를 얼굴 특징에 대한 편향에서 분리합니다. 추가 연구가 필요하지만 이것이 얼굴 특징에 대한 모델의 고유한 편향 중 일부를 완화하는 데 한 걸음 다가간 것이라고 기대합니다(그림 3). 우리의 핵심 아이디어는 얼굴 임베딩 모델을 활용하는 것입니다. 구체적으로, 우리는 훈련 이미지 데이터 세트를 가져와 얼굴이 포함된 이미지에 얼굴 임베딩을 추가합니다. 우리는 간단한 모듈(작은 MLP)을 사용하여 임베딩을 Stable Diffusion의 컨텍스트 공간에 투영한 다음 기본 확산 모델과 이 투영 모듈을 공동으로 훈련하여 얼굴 임베딩에 따라 조건화된 이미지를 생성합니다(그림 2 참조). 샘플링 시간에는 사용자가 제공한 이미지에서 얼굴 임베딩을 계산하여 Stable Diffusion의 컨텍스트에 추가하고 실질적으로 표준적인 방식으로 이미지를 샘플링합니다(약간 수정된 분류기 없는 안내 사용). 방법 주요 아이디어는 기본 확산 모델을 텍스트와 효율적인 얼굴 임베딩 메커니즘의 출력 모두에 따라 조건화되도록 훈련하는 것입니다. 2. 아키텍처 및 훈련 얼굴 임베딩 모듈의 경우 vggface2에서 훈련된 Inception Resnet V1 모델의 일부를 사용합니다(즉, 마지막 레이어를 삭제). [Cao et al. 2018]. 삭제된 레이어가 있는 모델은 정확한 식별에는 적합하지 않지만 고품질 생성에 필요한 충분한 시각적 세부 정보를 보존할 수 있습니다. 이 임베딩 모듈은 주로 얼굴 포즈와 표정을 수정합니다. 우리는 주석이 달린 이미지의 데이터 세트를 증강하여 각 (이미지, 캡션) 쌍을 (이미지, 캡션, 얼굴 임베딩) 트리플릿으로 변환합니다. 그런 다음 4개 계층 MLP를 훈련하여 얼굴 임베딩 공간에서 CLIP 임베딩 공간으로 변환하고 기본 모델을 공동으로 미세 조정하여 CLIP 임베딩과 투영된 얼굴 임베딩을 모두 조건으로 받습니다. 구체적으로, 우리는 모델 Mo의 매개변수 0을 미세 조정하여 표준 확산 모델 MSE 손실 목적을 최적화하고 얼굴 임베딩에 대한 추가 조건을 붙입니다. L(0) = Et,xo,d,f,€[wt||M₁(α+x + σ₁€, t, d, ƒ) − e)² ||] 여기서 Me는 0, t~ U(0, 1), e ~ N(0, 1), at, σt로 매개변수화된 투영 MLP를 포함한 전체 모델이고, we는 확산 노이즈 매개변수([Ho et al. 2020] 참조), xo는 데이터 세트에서 샘플링된 이미지, d는 연관된 텍스트 조건, 마지막으로 f는 새로 도입한 얼굴 임베딩 조건(그림 2 참조)입니다. Face0: Face2.2 샘플링에서 텍스트-이미지 모델을 즉시 조건화하기 샘플 시간에 사용자가 제공한 이미지가 주어지면 학습 시간에 사용한 것과 동일한 얼굴 추출 논리를 실행하고, 투영된 얼굴 임베딩을 계산하고, 이를 사용하여 마지막 세 토큰을 재정의합니다.그런 다음 분류자 없는 안내(CFG)의 약간의 변형을 사용합니다[Ho and Salimans 2022].표준 CFG와 유사하게 조건화되지 않은 결과(음의 가중치)와 조건화된 결과(양의 가중치)에 대해 다음 선형 조합을 계산합니다.특히 다음 표준 CFG 공식을 평가합니다.Єt = w • et (zt, d, f) + (1 − w) · Єt (Zt) . .여기서 d는 텍스트 프롬프트이고 f는 얼굴 임베딩입니다.실험에서는 w = 7.5의 분류자 없는 안내 가중치를 사용합니다. 표준 분류기 없는 안내와 달리, 결과에 대한 보다 정교한 제어를 허용하기 위해, 우리는 세 개의 별도 조건부 벡터의 가중 평균을 사용합니다. 조건부 벡터는 텍스트 프롬프트 d 단독, 얼굴 임베딩 단독 및 이들의 조합에 조건부로 주어진 벡터입니다. 우리는 결합된 벡터의 상대적 가중치를 c로 나타내고, 나머지에서 얼굴만을 조건부로 주어진 벡터의 상대적 가중치를 a로 나타내는 매개변수화를 선택합니다. 전반적으로 다음과 같습니다. êt(2t, d, f) = c€t(2t,d, f)+(1−c)(a-€t(2t,f)+(1−a).€t(2t,d)) 실제로, 이 세 가지 가중치 항 중 적어도 하나는 항상 0입니다. 3.4절 및 그림 7을 참조하십시오. 마지막으로, 우리의 방법은 세대 간에 얼굴에 대해 매우 높은 일관성을 유지하는 경향이 있음을 알 수 있습니다(그림 10 참조). 이것이 바람직하지 않은 경우, 입력 임베딩에 소량의 노이즈를 추가하면 다양성을 높일 수 있습니다. 2. 세부 정보 ~ LAION 데이터 세트[Schuhmann et al. 2022]에서 모델을 학습하고 미적 임계값 5.5로 필터링한 다음 추가로 필터링하여 20픽셀보다 큰 얼굴이 포함된 이미지만 포함합니다. 이 필터링 작업이 데이터 세트의 편향을 증폭할 수 있음을 유의합니다[Birhane et al. 2021]. 그 결과 10M(이미지, 캡션) 쌍이 생성됩니다. 얼굴 감지에는 MTCNN[Zhang et al. 2016]을 사용합니다. 이미지에 여러 얼굴이 있는 경우 가장 큰 얼굴만 가져옵니다(여러 얼굴이 있는 이미지는 여러 캐릭터에 대한 모델을 조절하는 데 유용하며 이는 향후 연구로 남겨둡니다). 얼굴 임베딩을 생성하기 위해 MTCNN의 출력을 기반으로 이미지를 자르고 종횡비를 유지하면서 160x160px 정사각형으로 크기를 조정합니다. MTCNN에서 반환된 사각형을 확장하여 헤어스타일과 같은 몇 가지 세부 정보를 포함합니다. 확장을 위해 왼쪽과 오른쪽 여백에 10픽셀, 위쪽 여백에 33픽셀, 아래쪽 여백에 15픽셀의 값을 수동으로 선택했습니다. 이러한 선택 사항을 더 이상 조정하지 않고 모든 실험에서 이 숫자를 사용했습니다. 그런 다음 얼굴 임베딩 모듈을 실행하여 임베딩 벡터를 출력합니다. 벡터를 CLIP 임베딩 공간으로 투영하기 위해 768차원의 간단한 4층 피드포워드 네트워크와 숨겨진 레이어 뒤에 ReLU 비선형성을 사용합니다. 이로 인해 총 10M개의 추가 매개변수가 발생합니다. 투영의 출력은 ~Stable Diffusion의 CLIP 임베딩 벡터에서 마지막 세 개의 토큰(토큰 75-77)을 재정의하는 데 사용하는 세 개의 768차원 벡터입니다. 분류기 없는 지침을 지원하기 위해 10%의 확률로 투영된 임베딩을 0으로 설정합니다. 동일한 작업을 텍스트 임베딩에 대해 수행하지 않는다는 점에 유의하세요.이는 세대의 품질을 개선했을 수 있으며 추가 연구에 흥미로운 방향이 될 수 있습니다.그런 다음 Stable Diffusion 1.4 체크포인트에서 시작하여 U-Net과 무작위로 초기화된 투영 네트워크를 공동으로 훈련합니다.학습률 2e-5와 배치 크기 256으로 64개 TPU-v4에서 500K 단계로 훈련합니다.EMA는 0.9999를 사용합니다.텍스트 프롬프트의 CLIP 인코더와 훈련 중에 VAE를 고정합니다.결과 3.1 모델 편향 텍스트-이미지 확산 모델은 기본 훈련 데이터에서 불공정한 편향을 상속받을 수 있습니다.한 가지 유형의 편향은 얼굴 특징을 얼굴 특징과 관련 없는 특정 단어와 연관시키는 것입니다.모델은 얼굴 특징을 텍스트 프롬프트에서 분리하도록 인센티브가 부여되므로 방법을 간단히 변형하면 이러한 유형의 편향을 어느 정도 완화할 수 있습니다. 구체적으로, 주어진 이미지에서 얼굴 임베딩 모듈에 의해 생성된 얼굴 임베딩 벡터를 취하는 대신, 모델의 모든 세대에 대해 무작위로 샘플링된 얼굴 임베딩 벡터를 사용할 수 있습니다. 이 절차는 텍스트 프롬프트에 대해 모델이 가질 수 있는 편향에서 분리된 무작위 임베딩에 대한 모델을 조건화합니다(그림 3 참조). 이는 모델의 실행 시간에 영향을 미치지 않으며 얼굴을 포함하는 모든 세대에 수평적으로 적용될 수 있습니다. 이는 예비 결과에 불과하며, 예를 들어 얼굴 임베딩 자체 내에 어떤 편향이 존재할 수 있는지, 얼굴 임베딩과 텍스트 프롬프트 간의 편향이 어떻게 상호 작용하는지, 무작위 얼굴 임베딩을 샘플링하는 방법(실험을 위해 간단한 가우시안 혼합을 사용) 등 여전히 많은 중요한 의문이 남아 있습니다. 이러한 단점에도 불구하고, 예비 결과가 이 중요한 주제에 대한 추가 연구를 촉진하기를 바랍니다. 3.2 일관된 캐릭터 확산 모델로 이미지를 생성할 때 사용자는 자신이 좋아하는 캐릭터를 만날 수 있습니다. 불행히도 기본 모델로 동일한 캐릭터를 재생성하는 것은 일반적으로 사소한 일이 아닙니다.Face0를 사용하면 일관된 캐릭터 생성이 사소한 일입니다.캐릭터를 생성할 때 임베딩을 무작위로 지정하고 최대 충실도를 위해 원하는 임베딩을 유지하거나 임베딩 없이 생성된 이미지에서 얼굴 임베딩을 계산하기만 하면 됩니다.역으로 수행할 수도 있습니다.원하는 세대를 얻었지만 다른 얼굴을 사용하고 싶은 경우 잠재 시드와 프롬프트를 고정하고 컨디셔닝 임베딩만 변경할 수 있습니다.이렇게 하면 때로는 더 큰 변화가 발생하지만 종종 주된 효과는 얼굴을 변경하는 것입니다(그림 1 참조).3.제어성 Face0에서 사용하는 임베딩 메커니즘은 다른 조건이 제공되지 않을 때 주로 얼굴 특징, 포즈 및 표정을 수정합니다.그러나 우리 모델은 두 가지 방법으로 생성된 얼굴을 제어할 수 있습니다.첫째, 텍스트 프롬프트를 사용하고 임베딩과 모순되는 특성을 지정하여 생성된 얼굴을 수정할 수 있습니다. Programmer CEO Doctor 4. Dani Valevski, Danny Wasserman, Yossi Matias, Yaniv Leviathan 기본 모델 얼굴(임의 임베딩 포함) 그림 3. 기본 모델(왼쪽)의 {&quot;doctor&quot;, &quot;CEO&quot;, &quot;programmer&quot;}에서 X에 대한 &quot;X의 스톡 사진&quot; 프롬프트 샘플과 무작위 얼굴 임베딩이 있는 모델(오른쪽). 예를 들어, &quot;파란 머리의 사람&quot;(그림 4 참조). 둘째, 이 모델은 텍스트로 설명하기 어려운 특징에 대한 추가 제어 가능성을 제공합니다. 예를 들어, 두 얼굴 임베딩 사이에 간단한 선형 보간을 사용하면 얼굴 사이에 의미 있는 의미적 전환을 만들 수 있습니다(그림 5 참조). 보간은 또한 같은 사람의 여러 얼굴 이미지를 고려하는 간단한 방법을 제공합니다. 예를 들어, 임베딩을 평균화하거나 같은 사람의 여러 이미지 간에 다른 가중치를 사용하여 가중 평균을 낼 수 있습니다. 생성이 즉시 이루어지므로 가중치를 대화형으로 조정할 수 있습니다. 우리는 이것으로 최소한의 실험만 했지만 이것은 추가 연구에 유망한 결과를 보여주었습니다.마지막으로, 우리는 두 가지 제어 방법을 결합하여 텍스트 제어의 강도를 약화시킬 수 있습니다.예를 들어, 얼굴 임베딩 esrc와 프롬프트 &quot;콧수염이 있는 사람&quot;이 있는 이미지를 만든 다음 임베딩 콧수염을 계산하고 임베딩 간의 선형 보간을 사용하여 콧수염의 양을 제어할 수 있습니다(그림 6 참조).3.4 샘플링 변형 Face0으로 이미지를 생성할 때 얼굴, 텍스트 및 결합 임베딩의 가중치를 독립적으로 변경하면 유용한 방식으로 결과 이미지를 세밀하게 제어할 수 있음을 발견했습니다.위에서 실험에서 언급했듯이 CFG 가중치를 w = 7.5로 고정했습니다.사진처럼 사실적인 결과가 필요할 때(예: 그림 1의 슈퍼히어로, 의사) c = 1로 설정하여 기본적으로 표준 CFG를 수행합니다.즉, 모든 CFG 강도가 결합된 벡터에 주어졌습니다. 비사진적 사실적 이미지 생성이 필요한 경우(예: 반 고흐 스타일 그림 또는 그림 1의 액션 피규어) 우리는 단순히 결합된 임베딩에 대한 텍스트 임베딩의 가중치를 증가시켰습니다. 0.4 범위의 값
--- RELATED WORK ---
텍스트 정렬.0.24 ± 0.0.23 ± 0.0.23 ± 0.0.24 ± 0.얼굴 정렬.0.72 ± 0.0.46 ± 0.전체 0.96 ± 0.0.69 ± 0.0.66 ± 0.0.39 ± 0.0.89 ± 0.0.62 ± 0.텍스트-이미지 모델.이미지 생성을 위한 딥 생성 모델은 최근 몇 년 동안 엄청난 진전을 보였습니다.초기 6. Dani Valevski, Danny Wasserman, Yossi Matias 및 Yaniv Leviathan 곱슬머리 빨간머리 콧수염 0.0.ra 0.0.0.1.그림 6. Face0은 얼굴 임베딩을 직접 조작하여 텍스트로 설명하기 어려운 얼굴 특징을 세밀하게 제어할 수 있습니다. 여기서 우리는 서로 다른 텍스트 프롬프트를 가진 동일한 소스(그림 4의 왼쪽 위 이미지)에서 생성된 두 사진의 얼굴 임베딩 사이에 간단한 선형 보간을 봅니다.원본 C=0.c=0.C=0.c=0.c=0.a=0.a=0.a=0.a=0.a=0.a=0.a=0.그림 7. Face0은 텍스트, 얼굴, 결합 및 비조건 임베딩을 독립적으로 가중치를 둘 수 있습니다.모든 이미지는 프롬프트 &quot;머그잔에 담긴 얼굴의 라떼 아트&quot;와 고정된 잠재 시드로 생성되었습니다.자세한 내용은 3.4절을 참조하십시오.표 2. 8개 시드 중 가장 좋은 점수를 선택할 때 Face0과 Dreambooth의 전체 점수 대 평균. FaceBest(SYN) Average(SYN) Best(LFW) Average(LFW) 1.04±0.0.96±0.0.98±0.0.89±0.DreamBooth 0.93±0.0.69±0.0.83±0.0.62±0.GAN[Goodfellow et al. 2014] 생성기(StyleGAN[Karras et al. 2018]과 유사)를 학습하고 다양한 CLIP[Radford et al. 2021]을 사용하여 안내하는 방법에 의존하는 접근 방식
--- METHOD ---
s는 추론 시간 동안 최적화 문제를 해결하는 데 의존합니다.예: 모델 미세 조정[Ruiz et al. 2022] 또는 이미지를 텍스트 임베딩 공간으로 반전[Gal et al. 2022]. 이러한 방법은 좋은 결과를 생성하지만 시간이나 메모리에 비용이 많이 듭니다.그림 1. Face0의 생성 예.단일 얼굴 이미지(왼쪽)와 텍스트 프롬프트(위)가 주어지면 이미지를 생성하는 데 몇 초 밖에 걸리지 않습니다.미세 조정이나 반전과 같은 최적화 절차 없이 샘플 시간 내에 얼굴에 대한 텍스트-이미지 생성 모델을 즉시 조건 지정하는 새로운 방법인 Face0를 제시합니다.포함된 얼굴의 임베딩으로 주석이 달린 이미지 데이터 세트를 증강하고 증강된 데이터 세트에서 이미지 생성 모델을 학습합니다.학습이 완료되면 시스템은 추론 시간에 기본 모델과 실질적으로 동일하므로 사용자가 제공한 얼굴 이미지와 프롬프트가 주어지면 불과 몇 초 만에 이미지를 생성할 수 있습니다. 우리의 방법은 만족스러운 결과를 얻고, 놀라울 정도로 간단하고, 매우 빠르며, 텍스트나 입력 얼굴 임베딩을 직접 조작하여 생성된 이미지를 제어하는 것과 같은 새로운 기능을 기본 모델에 제공합니다. 또한 사용자가 제공한 이미지의 얼굴 임베딩 대신 고정된 랜덤 벡터를 사용할 때, 우리의 방법은 본질적으로 이미지 전체에서 일관된 문자 생성 문제를 해결합니다. 마지막으로, 추가 연구가 필요하지만, 모델의 텍스트 편향을 얼굴에 대한 편향에서 분리하는 우리의 방법이 향후 텍스트-이미지 모델에서 편향을 어느 정도 완화하는 데 도움이 될 수 있기를 바랍니다. 이 작업에서 우리는 얼굴에 안정적 확산을 사용하여 이미지 생성 모델을 즉시 컨디셔닝하는 새로운 방법을 개발합니다. 추론 시점에서 우리의 방법은 기본 확산 모델의 표준 추론과 실질적으로 동일하며, 단일 &quot;Equal contribution&quot;에서 사람과 유사한 이미지를 즉시 생성할 수 있습니다. 저자 주소: Dani Valevski, Google Research, daniv@google.com; Danny Wasserman, Google Research, dwasserman@google.com; Yossi Matias, Google Research, yossi@google.com; Yaniv Leviathan, Google Research, leviathan@google.com. 2. Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv Leviathan 노이즈 추가 감지 및 삽입 자르기 훈련 &quot;테니스 선수&quot; CLIP으로 삽입 프로젝트 확산 모델 MSE 손실 그림 2. Face0에 대한 훈련 계획(2절 참조). 점선 빨간색 화살표를 제외한 모든 것은 표준 확산 모델 훈련 절차의 일부입니다. 단순화를 위해 픽셀 공간에서 잠재 공간으로 변환하는 세부 사항은 생략합니다. 사진. 예를 들어, 그림의 이미지는 1은 왼쪽의 원본 이미지와 위쪽의 텍스트 프롬프트를 감안했을 때 불과 몇 초 만에 생성되었습니다. Face0는 특히 간단하고 효율적이면서도 만족스러운 결과를 생성합니다(그림 1). 또한, 이 방법에는 몇 가지 다른 장점이 있습니다: (1) 텍스트 프롬프트를 통해서뿐만 아니라 얼굴 임베딩 벡터를 직접 조작하여 점진적으로 머리 스타일이나 방향을 변경하는 등 생성된 얼굴을 쉽고 자연스럽게 제어할 수 있습니다(그림 4~6). (2) 사용자가 제공한 얼굴 이미지 대신 고정된 무작위 얼굴 임베딩 벡터를 사용하여 생성된 이미지에서 일관된 문자를 생성하는 문제를 간단하게 해결합니다. (3) 텍스트 프롬프트 대신 얼굴 임베딩에서 얼굴 특징을 디코딩하도록 모델을 장려하기 때문에 모델의 텍스트 편향 중 일부를 얼굴 특징에 대한 편향에서 분리합니다. 추가 연구가 필요하지만 이것이 얼굴 특징에 대한 모델의 고유한 편향 중 일부를 완화하는 데 한 걸음 다가간 것이라고 기대합니다(그림 3). 우리의 핵심 아이디어는 얼굴 임베딩 모델을 활용하는 것입니다. 구체적으로, 우리는 훈련 이미지 데이터 세트를 가져와 얼굴이 포함된 이미지에 얼굴 임베딩을 추가합니다. 우리는 간단한 모듈(작은 MLP)을 사용하여 임베딩을 Stable Diffusion의 컨텍스트 공간에 투영한 다음 기본 확산 모델과 이 투영 모듈을 공동으로 훈련하여 얼굴 임베딩에 따라 조건화된 이미지를 생성합니다(그림 2 참조). 샘플링 시간에는 사용자가 제공한 이미지에서 얼굴 임베딩을 계산하여 Stable Diffusion의 컨텍스트에 추가하고 실질적으로 표준적인 방식으로 이미지를 샘플링합니다(약간 수정된 분류기 없는 안내 사용). 방법 주요 아이디어는 기본 확산 모델을 텍스트와 효율적인 얼굴 임베딩 메커니즘의 출력 모두에 따라 조건화되도록 훈련하는 것입니다. 2. 아키텍처 및 훈련 얼굴 임베딩 모듈의 경우 vggface2에서 훈련된 Inception Resnet V1 모델의 일부를 사용합니다(즉, 마지막 레이어를 삭제). [Cao et al. 2018]. 삭제된 레이어가 있는 모델은 정확한 식별에는 적합하지 않지만 고품질 생성에 필요한 충분한 시각적 세부 정보를 보존할 수 있습니다. 이 임베딩 모듈은 주로 얼굴 포즈와 표정을 수정합니다. 우리는 주석이 달린 이미지의 데이터 세트를 증강하여 각 (이미지, 캡션) 쌍을 (이미지, 캡션, 얼굴 임베딩) 트리플릿으로 변환합니다. 그런 다음 4개 계층 MLP를 훈련하여 얼굴 임베딩 공간에서 CLIP 임베딩 공간으로 변환하고 기본 모델을 공동으로 미세 조정하여 CLIP 임베딩과 투영된 얼굴 임베딩을 모두 조건으로 받습니다. 구체적으로, 우리는 모델 Mo의 매개변수 0을 미세 조정하여 표준 확산 모델 MSE 손실 목적을 최적화하고 얼굴 임베딩에 대한 추가 조건을 붙입니다. L(0) = Et,xo,d,f,€[wt||M₁(α+x + σ₁€, t, d, ƒ) − e)² ||] 여기서 Me는 0, t~ U(0, 1), e ~ N(0, 1), at, σt로 매개변수화된 투영 MLP를 포함한 전체 모델이고, we는 확산 노이즈 매개변수([Ho et al. 2020] 참조), xo는 데이터 세트에서 샘플링된 이미지, d는 연관된 텍스트 조건, 마지막으로 f는 새로 도입한 얼굴 임베딩 조건(그림 2 참조)입니다. Face0: Face2.2 샘플링에서 텍스트-이미지 모델을 즉시 조건화하기 샘플 시간에 사용자가 제공한 이미지가 주어지면 학습 시간에 사용한 것과 동일한 얼굴 추출 논리를 실행하고, 투영된 얼굴 임베딩을 계산하고, 이를 사용하여 마지막 세 토큰을 재정의합니다.그런 다음 분류자 없는 안내(CFG)[Ho 및 Salimans 2022]의 약간의 변형을 사용합니다.표준 CFG와 유사하게 조건 없는 결과(음의 가중치)와 조건화된 결과(양의 가중치)에 대해 다음 선형 조합을 계산합니다.특히 다음 표준 CFG 공식을 평가합니다.Єt = w • et (zt, d, f) + (1 − w) · Єt (Zt) . .여기서 d는 텍스트 프롬프트이고 f는 얼굴 임베딩입니다.우리의
