--- ABSTRACT ---
COCO 객체 탐지 및 분할 벤치마크에서 DETR 기반 접근 방식의 놀라운 성과에 동기를 부여받은 최근의 노력은 동결된 백본을 보존하면서 Transformers의 자체 감독 사전 학습을 통해 성능을 높이는 방향으로 진행되었습니다. 특정 연구에서는 정확도의 주목할 만한 발전이 기록되었습니다. 저희의 조사는 대표적인 접근 방식인 DETReg와 H-Deformable-DETR과 같은 새로운 모델의 맥락에서 성능 평가를 심층적으로 조사했습니다. 유감스럽게도 DETReg는 전체 데이터 조건에서 강력한 DETR 기반 모델의 성능을 향상시키는 데 부적합한 것으로 판명되었습니다. 근본 원인을 분석하기 위해 사전 학습 데이터 세트 선택 및 사전 학습 대상 생성 전략과 같은 COCO 및 PASCAL VOC 프로빙 요소에 대한 광범위한 실험을 수행합니다. 이와 대조적으로 저희는 개선된 상자 예측기와 Objects365 벤치마크를 결합하여 현저한 향상을 이룬 Simple Self-training이라는 최적화된 접근 방식을 사용합니다. 이러한 노력의 정점은 COCO 평가 세트에서 59.3%라는 놀라운 AP 점수를 얻었으며, 사전 학습 없이 HDeformable-DETR + Swin-L보다 1.4% 더 나은 성과를 보였습니다. 게다가 현대의 이미지-텍스트(LLaVA) 및 텍스트-이미지(SDXL) 모델을 병합하여 생성된 일련의 합성 사전 학습 데이터 세트는 객체 감지 기능을 크게 증폭합니다. 키워드 객체 감지, DETR, 사전 학습 1
--- INTRODUCTION ---
최근 DETR 기반 접근 방식(Carion 등, 2020; Jia 등, 2023; Li 등, 2023; Zhang 등, 2022; Zhu 등, 2020)은 상당한 진전을 이루었으며 ¹토론토 대학교 리버풀 대학교 &quot;Microsoft Research Asia 2베이징 대학교 3시안 교통 4펜실베이니아 대학교 5CUHK yuhui.yuan@microsoft.com은 객체 감지 및 분할 작업 모두에서 경계를 확장했습니다. 예를 들어, DINO-DETR(Zhang 등, 2022), H-Deformable-DETR(Jia 등, 2023) 및 GroupDETRV2(Chen 등, 2022)는 COCO 벤치마크에서 새로운 최첨단 객체 감지 성능을 설정했습니다. MaskDINO(Li 등, 2023)는 DINO-DETR을 더욱 확장하고 COCO 인스턴스 분할 및 파노라마 분할 작업에서 가장 좋은 결과를 얻었습니다. 어느 정도, 이는 엔드투엔드 트랜스포머 접근 방식이 합성곱을 기반으로 하는 기존의 강력하게 조정된 강력한 검출기(Li et al., 2021; Liu et al., 2022b)보다 더 나은 성능을 달성할 수 있는 최초의 사례입니다(예: Cascade Mask-RCNN 및 HTC++). 이러한 DETR 기반 접근 방식의 큰 성공에도 불구하고 여전히 무작위로 초기화된 트랜스포머를 선택하기 때문에 (Wei et al., 2021)과 같은 완전히 사전 학습된 검출 아키텍처의 잠재력을 발휘하지 못하며, 이는 사전 학습 아키텍처를 다운스트림 아키텍처와 정렬하는 이점을 이미 검증했습니다. 그림 1a 및 1b는 ResNet50 백본을 기반으로 하는 표준 Deformable-DETR 네트워크 내에서 매개변수 수와 GFLOPS의 분포를 보여줍니다. Transformer 인코더와 디코더가 매개변수의 34%와 GFLOP의 65%를 차지하는 것을 볼 수 있는데, 이는 DETR 내에서 Transformer 부분에 대한 사전 학습을 수행하는 경로에서 개선의 여지가 많다는 것을 의미합니다. 최근의 여러 연구에서는 백본을 동결하는 동안 Transformer 인코더와 디코더에 대한 자체 감독 사전 학습을 수행하여 DETR 기반 객체 감지 모델을 개선했습니다. 예를 들어, UP-DETR(Dai et al., 2021)은 Transformer를 사전 학습하여 이미지에서 무작위 패치를 감지하고, DETReg(Bar et al., 2022)는 Transformer를 사전 학습하여 객체 위치와 특징을 Selective Search 알고리즘에서 생성된 사전 정보와 일치시키고, 가장 최근에는 Siamese DETR이 다른 뷰의 해당 상자에서 추출한 쿼리 특징으로 대상 상자를 찾습니다. 그러나 이러한 연구에서는 바닐라 DETR 네트워크(객체 감지 성능 측면에서 AP=42.1%)를 활용합니다.Ma et al. AP(%) 인코더 백본 51% 기타 15% 17% 디코더 17% 인코더 60% 백본 30% (a) #매개변수 (b) #GFLOPS 42.45.2 45.43.DETR 변형 가능 DETR 기타 5% 디코더 5% 기준선 DETReg 49.6 49.H-변형 가능 DETR (c) DETReg의 COCO 객체 감지 결과.그림 1: ResNet백본이 있는 변형 가능 DETR 네트워크 내의 매개변수 수와 GFLOPS 분포와 DETReg의 사전 학습 성능.(a) 및 (b)에서 볼 수 있듯이, 약 34%의 매개변수와 65%의 GFLOP가 무작위로 초기화된 Transformer 인코더 및 디코더에 분포되어 있는 것을 볼 수 있습니다. (c), DETReg는 vanilla DETR과 Deformable-DETR을 각각 +1.6%와 +0.3%만 개선하는 반면, 더 강력한 H-Deformable-DETR에 비해 이득을 보이지 않았습니다. COCO) 또는 Deformable-DETR 변형(AP=45.2%)에서. 이들의 결과는 H-DeformableDETR(Jia et al., 2023)과 같은 최신의 훨씬 더 강력한 DETR 모델에서 사전 학습했을 때 상당히 떨어졌습니다(AP=49.6%). 그림 1c에서 두 가지 조건에서 COCO에서 다양한 DETR 모델의 객체 감지 결과를 보여줍니다. Transformer 구성 요소(기준선이라고 함)를 사전 학습하지 않은 경우와 DETReg 방법을 사용하여 사전 학습한 경우입니다. 두 경우 모두 이러한 모델의 백본은 SwAV(Caron et al., 2020)로 초기화된 ResNet50입니다. 특히, H-Deformable-DETR의 경우 DETReg 사전 학습을 활용하면 실제로 성능이 향상되기보다는 감소합니다. 이 작업에서 먼저 DETReg가 예시하는 자기 지도 사전 학습 방법이 COCO 객체 감지 벤치마크에서 점점 더 강력해지는 DETR 모델보다 얼마나 개선될 수 있는지 자세히 살펴봅니다. 저희의 조사에서는 SwAV 사전 학습된 백본, Deformable-DETR의 변형 가능 기술, H-Deformable-DETR의 하이브리드 매칭 방식과 같은 개선 사항으로 강화된 강화된 DETR 네트워크에 적용할 때 DETReg의 효능에 상당한 한계가 있음을 밝힙니다. 저희는 문제의 핵심이 선택적 검색과 같은 비지도 방법으로 생성된 신뢰할 수 없는 상자 제안에서 비롯된 것으로 지적합니다. 이는 노이즈가 많은 지역화 대상에 기여하고, 효율적인 분류 대상이 아닌 기능 재구성을 통해 제공되는 약한 의미 정보입니다. 이러한 단점으로 인해 자기 지도 사전 학습 방법은 이미 강력한 DETR 모델에 적용할 때 효과적이지 않습니다. 이를 해결하기 위해 COCO 객체 감지기를 사용하여 정보적인 가상 클래스 레이블이 있는 보다 정확한 가상 상자를 얻는 것을 제안합니다. 광범위한 절제 실험은 세 가지 핵심 요소의 영향을 강조합니다. 사전 학습 데이터 세트 선택(ImageNet 대 Objects365), 지역화 사전 학습 대상(선택적 검색 제안 대 가상 상자 예측), 분류 사전 학습 대상(객체 임베딩 대 가상 클래스 예측)입니다. 우리의 연구 결과에 따르면 가상 상자 및 가상 클래스 예측을 사전 학습 대상으로 사용하는 Simple Self-training 방식은 다양한 설정에서 DETReg 방식보다 성능이 뛰어납니다. 주목할 점은 이 간단한 설계가 사전 학습 벤치마크의 기준 진실 레이블에 액세스하지 않고도 최신 DETR 네트워크에서도 눈에 띄는 사전 학습 향상을 제공한다는 것입니다. 예를 들어, ResNetbackbone과 Objects365 사전 학습 데이터 세트를 사용하면 Simple Self-training은 H-Deformable-DETR에서 DETReg의 COCO 객체 감지 결과를 3.6% 높입니다. 또한 Swin-L 백본에서 놀라운 성능이 관찰되어 경쟁력 있는 결과인 59.3%를 산출합니다. 또한 객체 감지 사전 학습을 위한 일련의 합성 데이터 세트를 만드는 것을 목표로 현대 이미지-텍스트 및 텍스트-이미지 생성 모델을 탐구합니다. 경험적으로, 이러한 합성 데이터 세트를 사용한 사전 학습은 널리 채택된 Objectsbenchmark와 비교하더라도 칭찬할 만한 성능을 보여주기 때문에 우리의 관찰 결과는 격려적인 결과를 낳습니다. 이는 상당한 주석 비용을 수반합니다. 전반적으로, 우리의 노력은 DETR 사전 학습이라는 어려운 과제의 진행 상황을 보다 확실하게 평가할 준비가 되어 있습니다. 2
--- RELATED WORK ---
객체 감지를 위한 DETR. DETR(Carion et al., 2020)이 최초의 완전한 엔드투엔드 객체 감지기로 등장한 이후, 많은 연구에서 다양한 비전 작업에서 최첨단 결과를 얻기 위해 새로운 기술로 DETR을 확장했습니다. 원래 DETR의 수렴을 가속화하기 위해 Deformable-DETR(Zhu et al., 2020)은 참조 지점 주변의 중요한 샘플링 지점의 희소한 집합에 초점을 맞추기 위해 새로운 다중 스케일 변형 가능 자기/교차 주의를 제안합니다. 또한 다른 쿼리 공식을 사용한 DAB-DETR(Liu et al., 2022a)을 기반으로 하는 DINODETR(Zhang et al., 2022)은 쿼리 노이즈 제거 방식을 도입하고 객체 감지 작업에서 새로운 기록을 세웁니다. 또한 DETR에서 일대일 매칭으로 인해 발생하는 학습 효율성 병목 현상을 해결하기 위해 H-Deformable-DETR(Jia Revisiting DETR Pre-training for Object Detection et al., 2023)과 Group-DETR(Chen et al., 2022)은 추가적인 일대다 매칭 방식을 사용하여 변압기 디코더에서 더 많은 쿼리로 학습하는 것을 제안하여 더욱 빠른 수렴과 더 나은 성능을 달성하는 데 도움이 됩니다. 자기 감독 사전 학습. 자기 감독 학습(SSL)은 이미지 분류에서 놀라운 결과를 얻었습니다.
--- METHOD ---
. 두 경우 모두, 이러한 모델의 백본은 SwAV로 초기화된 ResNet50입니다(Caron et al., 2020). 주목할 점은 H-Deformable-DETR의 경우 DETReg 사전 학습을 활용하면 실제로 성능이 향상되기보다는 감소합니다. 이 작업에서 먼저 DETReg가 예시하는 자체 감독 사전 학습 방법이 COCO 객체 감지 벤치마크에서 점점 더 강력해지는 DETR 모델보다 얼마나 개선될 수 있는지 자세히 살펴봅니다. 저희의 조사에서는 SwAV 사전 학습된 백본, Deformable-DETR의 변형 가능 기술, H-Deformable-DETR의 하이브리드 매칭 방식과 같은 개선 사항으로 강화된 강화된 DETR 네트워크에 DETReg를 적용할 때의 효능에 상당한 한계가 있음을 밝힙니다. 우리는 문제의 핵심이 선택적 검색과 같은 비지도 방법에 의해 생성된 신뢰할 수 없는 상자 제안에서 비롯된 것이라고 지적합니다. 이는 노이즈가 많은 지역화 대상에 기여하고, 효율적인 분류 대상이 아닌 기능 재구성을 통해 제공되는 약한 의미 정보입니다. 이러한 단점은 이미 강력한 DETR 모델에 적용할 때 자체 지도 사전 학습 방법을 비효과적으로 만듭니다. 이를 해결하기 위해 COCO 객체 감지기를 사용하여 정보적인 의사 클래스 레이블이 있는 더 정확한 의사 상자를 얻는 것을 제안합니다. 광범위한 절제
--- EXPERIMENT ---
COCO 및 PASCAL VOC 프로빙 요소(예: 사전 학습 데이터 세트 선택 및 사전 학습 대상 생성 전략)에 대한 s. 이와 대조적으로, 우리는 개선된 상자 예측기와 Objects365 벤치마크를 결합하여 현저한 향상을 가져오는 Simple Self-training이라는 최적화된 접근 방식을 채택합니다. 이러한 노력의 정점은 COCO val 세트에서 59.3%의 놀라운 AP 점수를 얻어 사전 학습 없이 HDeformable-DETR + Swin-L보다 1.4% 더 우수한 성과를 거두었습니다. 게다가 현대의 이미지-텍스트(LLaVA) 및 텍스트-이미지(SDXL) 모델을 병합하여 생성된 일련의 합성 사전 학습 데이터 세트는 객체 감지 기능을 크게 증폭합니다. 키워드 객체 감지, DETR, 사전 학습 1 서론 최근 DETR 기반 접근 방식(Carion 등, 2020; Jia 등, 2023; Li 등, 2023; Zhang 등, 2022; Zhu 등, 2020)은 상당한 진전을 이루었으며 ¹토론토 대학교 리버풀 대학교 &quot;Microsoft Research Asia 2베이징 대학교 3시안 교통 4펜실베이니아 대학교 5CUHK yuhui.yuan@microsoft.com 객체 감지 및 분할 작업 모두에서 경계를 확장했습니다. 예를 들어, DINO-DETR(Zhang 등, 2022), H-Deformable-DETR(Jia 등, 2023) 및 GroupDETRV2(Chen 등, 2022)는 COCO 벤치마크에서 최첨단 객체 감지 성능을 설정했습니다. MaskDINO(Li 등, 2023)은 DINO-DETR을 더욱 확장하고 COCO 인스턴스 분할 및 파노라마 분할 작업에서 최상의 결과를 확립합니다. 어느 정도, 이는 엔드투엔드 트랜스포머 접근 방식이 합성곱을 기반으로 하는 기존의 강력하게 조정된 강력한 검출기(Li et al., 2021; Liu et al., 2022b)보다 더 나은 성능을 달성할 수 있는 최초의 사례입니다(예: Cascade Mask-RCNN 및 HTC++). 이러한 DETR 기반 접근 방식의 큰 성공에도 불구하고 여전히 무작위로 초기화된 트랜스포머를 선택하기 때문에 (Wei et al., 2021)과 같은 완전히 사전 학습된 검출 아키텍처의 잠재력을 발휘하지 못하며, 이는 사전 학습 아키텍처를 다운스트림 아키텍처와 정렬하는 이점을 이미 검증했습니다. 그림 1a 및 1b는 ResNet50 백본을 기반으로 하는 표준 Deformable-DETR 네트워크 내에서 매개변수 수와 GFLOPS의 분포를 보여줍니다. Transformer 인코더와 디코더가 매개변수의 34%와 GFLOP의 65%를 차지하는 것을 볼 수 있는데, 이는 DETR 내에서 Transformer 부분에 대한 사전 학습을 수행하는 경로에서 개선의 여지가 많다는 것을 의미합니다. 최근의 여러 연구에서는 백본을 동결하는 동안 Transformer 인코더와 디코더에 대한 자체 감독 사전 학습을 수행하여 DETR 기반 객체 감지 모델을 개선했습니다. 예를 들어, UP-DETR(Dai et al., 2021)은 Transformer를 사전 학습하여 이미지에서 무작위 패치를 감지하고, DETReg(Bar et al., 2022)는 Transformer를 사전 학습하여 객체 위치와 특징을 Selective Search 알고리즘에서 생성된 사전 정보와 일치시키고, 가장 최근에는 Siamese DETR이 다른 뷰의 해당 상자에서 추출한 쿼리 특징으로 대상 상자를 찾습니다. 그러나 이러한 작업에서는 바닐라 DETR 네트워크(객체 감지 성능 측면에서 AP=42.1%)를 활용합니다.Ma et al. AP(%) 인코더 백본 51% 기타 15% 17% 디코더 17% 인코더 60% 백본 30% (a) #매개변수 (b) #GFLOPS 42.45.2 45.43.DETR 변형 가능 DETR 기타 5% 디코더 5% 기준선 DETReg 49.6 49.H-변형 가능 DETR (c) DETReg의 COCO 객체 감지 결과.그림 1: ResNet백본이 있는 변형 가능 DETR 네트워크 내의 매개변수 수와 GFLOPS 분포와 DETReg의 사전 학습 성능.(a) 및 (b)에서 볼 수 있듯이, 약 34%의 매개변수와 65%의 GFLOP가 무작위로 초기화된 Transformer 인코더 및 디코더에 분포되어 있는 것을 볼 수 있습니다. (c), DETReg는 vanilla DETR과 Deformable-DETR을 각각 +1.6%와 +0.3%만 개선하는 반면, 더 강력한 H-Deformable-DETR에 비해 이득을 보이지 않았습니다. COCO) 또는 Deformable-DETR 변형(AP=45.2%)에서. 이들의 결과는 H-DeformableDETR(Jia et al., 2023)과 같은 최신의 훨씬 더 강력한 DETR 모델에서 사전 학습했을 때 상당히 떨어졌습니다(AP=49.6%). 그림 1c에서 두 가지 조건에서 COCO에서 다양한 DETR 모델의 객체 감지 결과를 보여줍니다. Transformer 구성 요소(기준선이라고 함)를 사전 학습하지 않은 경우와 DETReg 방법을 사용하여 사전 학습한 경우입니다. 두 경우 모두 이러한 모델의 백본은 SwAV(Caron et al., 2020)로 초기화된 ResNet50입니다. 특히, H-Deformable-DETR의 경우 DETReg 사전 학습을 활용하면 실제로 성능이 향상되기보다는 감소합니다. 이 작업에서 먼저 DETReg가 예시하는 자기 지도 사전 학습 방법이 COCO 객체 감지 벤치마크에서 점점 더 강력해지는 DETR 모델보다 얼마나 개선될 수 있는지 자세히 살펴봅니다. 저희의 조사에서는 SwAV 사전 학습된 백본, Deformable-DETR의 변형 가능 기술, H-Deformable-DETR의 하이브리드 매칭 방식과 같은 개선 사항으로 강화된 강화된 DETR 네트워크에 적용할 때 DETReg의 효능에 상당한 한계가 있음을 밝힙니다. 저희는 문제의 핵심이 선택적 검색과 같은 비지도 방법으로 생성된 신뢰할 수 없는 상자 제안에서 비롯된 것으로 지적합니다. 이는 노이즈가 많은 지역화 대상에 기여하고, 효율적인 분류 대상이 아닌 기능 재구성을 통해 제공되는 약한 의미 정보입니다. 이러한 단점으로 인해 자기 지도 사전 학습 방법은 이미 강력한 DETR 모델에 적용할 때 효과적이지 않습니다. 이를 해결하기 위해 COCO 객체 감지기를 사용하여 정보적인 가상 클래스 레이블이 있는 보다 정확한 가상 상자를 얻는 것을 제안합니다. 광범위한 절제 실험은 세 가지 핵심 요소의 영향을 강조합니다. 사전 학습 데이터 세트 선택(ImageNet 대 Objects365), 지역화 사전 학습 대상(선택적 검색 제안 대 가상 상자 예측), 분류 사전 학습 대상(객체 임베딩 대 가상 클래스 예측)입니다. 우리의 연구 결과에 따르면 가상 상자 및 가상 클래스 예측을 사전 학습 대상으로 사용하는 Simple Self-training 방식은 다양한 설정에서 DETReg 방식보다 성능이 뛰어납니다. 주목할 점은 이 간단한 설계가 사전 학습 벤치마크의 기준 진실 레이블에 액세스하지 않고도 최신 DETR 네트워크에서도 눈에 띄는 사전 학습 향상을 제공한다는 것입니다. 예를 들어, ResNetbackbone과 Objects365 사전 학습 데이터 세트를 사용하면 Simple Self-training은 H-Deformable-DETR에서 DETReg의 COCO 객체 감지 결과를 3.6% 높입니다. 나아가 Swin-L 백본에서 놀라운 성능이 관찰되어 경쟁력 있는 결과인 59.3%를 산출합니다. 또한 객체 감지 사전 학습을 위한 일련의 합성 데이터 세트를 생성하기 위해 현대의 이미지-텍스트 및 텍스트-이미지 생성 모델을 탐구합니다. 경험적으로, 이러한 합성 데이터 세트를 사용한 사전 학습은 널리 채택된 Objectsbenchmark와 비교하더라도 칭찬할 만한 성능을 보여주기 때문에 우리의 관찰 결과는 격려적인 결과를 낳았습니다. Objectsbenchmark는 상당한 주석 비용을 수반합니다. 전반적으로, 우리의 노력은 DETR 사전 학습이라는 어려운 과제의 진행 상황을 보다 확실하게 평가할 준비가 되어 있습니다. 2 관련 작업 객체 감지를 위한 DETR. DETR(Carion et al., 2020)이 최초의 완전한 엔드투엔드 객체 감지기로 등장한 이래로 많은 연구에서 다양한 비전 작업에서 최첨단 결과를 얻기 위해 새로운 기술로 DETR을 확장했습니다. 원래 DETR의 수렴을 가속화하기 위해 Deformable-DETR(Zhu et al., 2020)은 참조 지점 주변의 중요한 샘플링 지점의 희소한 집합에 초점을 맞추기 위해 새로운 다중 스케일 변형 가능 셀프/크로스 어텐션을 제안합니다. 또한 다른 쿼리 공식을 사용하는 DAB-DETR(Liu et al., 2022a)을 기반으로 하는 DINODETR(Zhang et al., 2022)은 쿼리 노이즈 제거 방식을 도입하고 객체 감지 작업에서 새로운 기록을 세웁니다. 또한 DETR에서 일대일 매칭으로 인한 학습 효율성 병목 현상을 해결하기 위해 H-Deformable-DETR(Jia Revisiting DETR Pre-training for Object Detection et al., 2023)과 Group-DETR(Chen et al., 2022)은 추가적인 일대다 매칭 방식을 사용하여 변압기 디코더에서 더 많은 쿼리로 학습하여 더 빠른 수렴과 더 나은 성능을 달성하는 것을 제안합니다.자기 감독 사전 학습.자기 감독 학습(SSL)은 MoCo(He et al., 2020), SimCLR(Chen et al., 2020), BYOL(Grill et al., 2020)과 같은 이미지 분류 방법에서 놀라운 결과를 얻었습니다. 그러나 객체 감지에 대한 SSL은 제한된 전이성을 보였습니다. 이러한 과제를 극복하기 위해 많은 연구에서 영역 또는 픽셀 현지화 단서를 활용하여 사전 학습 신호를 향상시키는 사전 텍스트 작업을 제안했습니다. 예를 들어, InsLoc(Yang 등, 2021a)은 전경 패치에서 대조 학습을 사용하여 인스턴스 로컬라이제이션을 학습합니다.UniVIP(Li 등, 2022)는 장면 유사성, 장면-인스턴스 상관 관계 및 인스턴스 구별을 활용하여 의미적 친화성을 포착합니다.CP2(Wang 등, 2022)는 픽셀 단위 대조 학습을 사용하여 이미지 수준 및 픽셀 수준 표현 학습을 모두 용이하게 합니다.Faster R-CNN 또는 Cascade R-CNN과 같은 기존 객체 감지기를 개선하는 것을 목표로 하는 이러한 방법의 대부분과 달리, 우리는 최첨단 DETR 기반 감지기에 대한 효과적인 사전 학습 방식을 설계하는 데 중점을 둡니다.DETR 사전 학습.DETR은 일반적으로 ImageNet의 감독 사전 학습된 백본과 트랜스포머 인코더 및 디코더의 무작위 초기화에 의존합니다.일부 최근 연구에서는 향상된 객체 감지 성능을 위해 DETR의 트랜스포머 구성 요소를 사전 학습하는 방법을 살펴보았습니다. 예를 들어, UP-DETR(Dai et al., 2021)은 입력의 무작위 패치를 탐지하고 재구성하기 위한 비지도 사전 텍스트 작업을 도입합니다.DETReg(Bar et al., 2022)는 무작위 패치 대신 Selective Search(Uijlings et al., 2013)의 비지도 영역 제안을 사용하여 사전 텍스트 작업을 개선하고 SwAV(Caron et al., 2020) 백본에서 이러한 영역의 객체 임베딩을 재구성하여 불변 표현을 학습합니다.Siamese DETR(Huang et al., 2023)은 각 분기가 하나의 뷰를 입력으로 받고 다른 뷰에서 해당 영역을 찾고 구별하는 것을 목표로 하는 대칭 파이프라인에서 DETR을 사전 학습하기 위해 siamese 자기 지도 학습 접근 방식을 사용합니다.그러나 이러한 사전 학습 방법은 DeformableDETR과 같은 강력한 DETR 변형에 대한 사소한 개선만 제공합니다.자기 학습. 셀프 트레이닝은 이미지 분류(Li et al., 2023; Sahito et al., 2022), 객체 감지(Vandeghen et al., 2022; Yang et al., 2021b), 분할(Zhu et al., 2021)과 같은 다양한 컴퓨터 비전 작업을 개선하기 위한 강력한 기술입니다. 일반적인 셀프 트레이닝 방법은 NoisyStudent(Xie et al., 2020)로, 레이블이 지정된 데이터에서 교사 모델을 훈련하고 이를 사용하여 레이블이 지정되지 않은 이미지에 대한 가상 레이블을 생성합니다. 그런 다음 이러한 가상 레이블을 사용하여 학생 모델을 훈련하고 이 프로세스를 반복하여 교사 모델을 이전 학생 모델로 업데이트하여 더 나은 모델을 얻습니다. ASTOD(Vandeghen et al., 2022) 프레임워크는 여러 이미지 뷰를 사용하여 고품질 가상 레이블을 생성하는 객체 감지를 위한 반복적 셀프 트레이닝 프로세스를 적용합니다. ST++(Yang et al., 2022)는 분할 작업을 위한 최근의 자체 학습 알고리즘으로, 신뢰도 점수를 사용하여 잘못된 가상 레이블을 걸러냅니다.(Zoph et al., 2020)는 자체 학습이 저데이터 및 고데이터 체제를 포함한 다양한 시나리오에서 기존의 사전 학습 방법보다 성능이 뛰어나며 사전 학습 방법이 실패하더라도 성공할 수 있음을 보여주었습니다. 가상 레이블을 정제하기 위해 반복적 접근 방식을 사용하는 이러한 복잡한 자체 학습 방식과 달리, 우리는 가장 신뢰할 수 있는 예측의 고정된 수를 유지하여 가상 레이블을 한 번만 생성하는 간단한 자체 학습 방식을 제안합니다.3 접근 방식 이 작업에서는 다음과 같은 객체 감지 작업을 위해 DETR 내의 Transformer 인코더 및 디코더 부분에 대한 사전 학습을 수행하는 방법을 연구하는 데 중점을 둡니다(Bar et al., 2022; Dai et al., 2021). DETR 사전 학습의 목표는 실제 경계 상자 주석이 없는 대규모 레이블이 지정되지 않은 데이터 세트를 최대한 활용할 수 있는 효과적인 사전 텍스트 작업을 설계하는 것입니다.3.1 공식화 기존 DETR 모델에는 세 가지 구성 요소가 있습니다.백본은 이미지 피처를 추출하고, 인코더는 셀프 어텐션 메커니즘으로 피처를 향상시키고, 디코더는 이미지 피처와 교차 어텐션을 통해 쿼리 입력을 객체 클래스 및 위치 예측으로 전환합니다.기존의 자체 감독 사전 학습 방법은 사전 학습된 백본을 동결하는 동안 사전 학습 데이터 세트에서 인코더 및 디코더 네트워크 매개변수를 최적화하는 유사한 방식을 공유합니다.사전 학습 후 세 가지 구성 요소가 모두 다운스트림 데이터 세트에서 함께 조정됩니다.파이프라인은 그림 2에 나와 있습니다.예비.다음 기사에서는 일반적인 자체 감독 사전 학습 프로세스를 여러 방정식으로 공식화합니다. 우리는 OB, OE 및 D로 매개변수화된 DETR 네트워크 내의 백본, Transformer 인코더 및 Transformer 디코더를 나타내기 위해 fo¸, ƒе, ƒе를 사용합니다. 사전 학습 및 다운스트림 데이터 세트의 입력 이미지는 각각 X {X1,,XN} 및 X {x1,...,xм}로 표시되며, 여기서 N&gt;M입니다. 다운스트림 데이터의 기준 진실 레이블은 Y={y1,···,YM|Yi=(ci, bi)}입니다. 여기서 c¿는 범주 레이블이고 b¿는 상자 위치 레이블입니다. 일반적으로 도메인별 사전 학습 데이터 레이블은 lack=사전 학습 데이터 세트 백본 Transformer 인코더 Transformer 디코더 의사 레이블 ImageNet (자기)지도 사전 학습 다운스트림 데이터 세트 백본 Transformer 인코더 Transformer 디코더 기준 진실 레이블 Random Initialized Detection 자기 지도 사전 학습 Ma et al. 그림 2: 자기 감독 사전 학습 계획의 전체 프레임워크. DETR 네트워크를 사전 학습하는 데는 두 단계가 있습니다. 첫 번째 단계에서는 백본을 동결하고 대규모 사전 학습 벤치마크에서 잘 설계된 사전 학습 대상으로 무작위로 초기화된 Transformer 인코더 및 디코더를 사전 학습합니다. 두 번째 단계에서는 사전 학습된 가중치로 인코더 및 디코더를 초기화하고 지상 진실 레이블로 감독되는 다운스트림 데이터 세트에서 DETR 네트워크의 모든 매개변수를 미세 조정합니다. ing 및 대부분 작업은 대신 Y = {ÿ₁,···,YN}과 같은 가상 레이블을 생성하기로 선택합니다. 사전 학습. 방정식 1과 2를 사용하여 DETR 사전 학습의 수학적 공식을 설명합니다. 구체적으로 사전 학습 입력 x¿는 백본 포그, 인코더 foε 및 디코더 fe를 통해 전달되어 예측 Zi를 얻습니다. 여기서 B, OE, OD는 각각 세 네트워크 구성 요소에 대한 학습 가능한 매개변수를 나타냅니다. OB는 SwAV(Caron et al., 2020) 자기 감독 사전 학습 방법으로 초기화되고 사전 학습 중에 동결됩니다.O와 OD는 무작위로 초기화된 다음 사전 학습 손실 pre(·)를 최소화하도록 최적화됩니다.이 손실은 네트워크 출력 Z¿와 사전 학습 대상 y¿로 계산됩니다.Zi = fød(foЕ(foB(xi)), Q), NÔD, E, Q = argmin pre(Zi, Vi), OD,OE,Q i=(1) (2) 여기서 Q = {q1,qk}는 디코더의 학습 가능한 객체 쿼리를 나타내며 인코더/디코더 매개변수와 함께 공동으로 최적화됩니다.AD, AE, Q는 사전 학습 후 디코더 매개변수, 인코더 매개변수 및 객체 쿼리를 나타냅니다.다음 섹션 3.2에서는 다양한 방법으로 pre를 공식화하는 방법을 설명합니다.미세 조정.사전 학습 중에 최적화된 인코더 및 디코더 매개변수 OE, OD를 얻습니다. 그런 다음 다운스트림 데이터 xi에서 동일한 네트워크를 조정합니다. 여기서 백본, 인코더 및 디코더 매개변수를 0B, Ô, ÔD로 초기화하고 네트워크 출력을 zi로 표시합니다. 세 구성 요소의 모든 매개변수와 학습 가능한 쿼리 Q는 z¿와 다운스트림 레이블 yi 사이의 다운스트림 손실 Lds(•)를 최소화하도록 최적화됩니다. Zi = ƒôò (ƒô (ƒ‰ (×¿)), Ô), M OD, OE, OB, Q argmin Las(Zi, Yi), OD, E, OB,Q i=(3) (4) 여기서 D, E, OB, Q는 다운스트림 미세 조정 후 최적화된 디코더, 인코더, 백본 매개변수 및 객체 쿼리입니다. 3.2 인스턴스화 ... i번째 사전 학습 입력의 대상을 yį {Vi1, Vim}으로 표시할 수 있다고 가정합니다. 여기서 m은 각 대상의 객체 수입니다. 네트워크 출력은 객체 쿼리 수와 동일한 k개의 바운딩 박스 예측으로 구성됩니다.해당 예측을 Zi = {Zi1, ……·‚Zik}로 표시합니다.일반적으로 y의 대상 수는 30개 미만인 반면 DETR 네트워크는 100개 또는 300개의 예측을 출력하도록 설정하므로 m &lt; k입니다.따라서 DETR(Carion et al., 2020)에 따라 대상을 비대상 범주 Ø로 패딩하여 크기가 k가 되도록 합니다.그런 다음 DETR은 헝가리 이분 매칭 알고리즘(Kuhn, 1955)을 통해 y; 및 Z에 대해 일대일 정렬을 수행합니다.각 예측에 대한 최적의 레이블 할당을 매칭 비용 함수 match()를 최소화하여 계산하는 방정식 5의 수학적 공식을 설명합니다.k σi = argmin match (Vij, Zio;(j)), σ; ΕΣΚ j=(5) 여기서 Σk는 k개 요소에 대한 모든 순열을 나타내고 σi(j)는 대상 상자 j를 i번째 입력 내에서 가장 유사한 예측 상자에 매핑합니다. 매칭 비용 함수 Lmatch(•)는 DETR에 따른 지역화 정확도와 분류 정확도를 포함한 두 가지 측면에서 예측을 측정합니다(Carion et al., 2020). 대부분의 자체 감독 사전 학습 방법은 사전 텍스트 작업의 설계를 통해 차별화되며, 이는 사전 학습 대상 y에 대한 다른 구조와 사전 학습 손실 pre의 구현을 초래합니다. 우수한 사전 텍스트 작업 설계는 최종 예측 성능을 개선할 수 있습니다. 다음에서 먼저 DETReg(Bar et al., 2022)라는 대표적인 방법의 인스턴스화를 소개합니다. 그런 다음 DETReg + 의사 상자 및 단순 자체 학습이라는 두 가지 더 효과적인 사전 학습 방식을 제안합니다. 두 방법 모두 지역화 및 분류 사전 학습 대상 품질을 향상시키는 데 중점을 둡니다. 그림 3에서 세 가지 방법의 사전 학습 파이프라인을 비교합니다. DETReg. DETReg는 Selective Search(ss)라는 비지도 영역 제안 방법을 사용하여 tarRevisiting DETR Pre-training for Object Detection -ss get 상자를 생성합니다. i번째 입력에 대한 j번째 &quot;상자 제안&quot;은 bij Є [0,1]4로 표시됩니다. 우리는 상단 / 선택적 -ss -SS = -ss -ss ss 검색 상자 제안 {₁₁,,}을 선택하여 네트워크 쿼리 번호 k(k &gt; k) {ō¾‚ Pik Pi¹‚·‚Ð¾ 1, Pi(k+1)&#39; 0}의 크기로 패딩된 이진 범주 대상과 페어링합니다. 여기서 Pij 1은 요소가 상자 제안임을 나타내고 pij = 0은 패딩된 0을 나타냅니다. 이진 범주의 의미 정보 부족을 보완하기 위해 DETReg 네트워크는 감지된 상자의 개체 임베딩 {fi₁,……‚fik|fij Є Rd}를 예측하기 위해 또 다른 개체 임베딩 재구성 분기를 통합합니다. 이는 대상 개체 설명자 {fswa ‚…‚f}의 감독을 받으며, fa는 고정된 SwAV 백본을 사용하여 i번째 입력의 j번째 상자 제안에서 이미지 패치에서 추출된 개체 임베딩을 나타냅니다. 따라서 사전 학습 대상과 네트워크 예측은 방정식 6으로 표시됩니다. Swav Vij = S swav ik SS swav wav ij (ēžƒ‚¯¡¡, f‚‚³), Zij = (ōij, bij, fij). 27&quot; ij, ij (6) 사전 학습 손실은 이진 분류 손실 bin(), 상자 손실 Lbox (·), 임베딩 손실 Lemb(•)의 합계이며, 모든 k 출력은 아래와 같습니다. k bin Lpre (ii) = Lois (Pij, Pioi (j)) j = -ss + Ab¹ {Ƒª; ±0} £box (bij, bioi (j)) + Ae Lemb(fij, fig(1)), (7) 여기서 bin()은 교차 엔트로피 손실 또는 초점 손실로 구현할 수 있는 이진 분류 손실입니다. Lbox (•)는 L1과 GIoU 손실의 합계이고 Lemb(•)는 L1 손실입니다. Ac, Ab, X는 손실 계수이고 σ; (j)는 대상 상자 j를 i번째 입력 내에서 가장 낮은 비용으로 할당된 예측 상자 σ; (j)에 매핑합니다. DETReg + 의사 상자. 다음과 같은 비지도 상자 제안 선택적 검색 상자는 품질이 매우 낮습니다. 이를 처리하기 위해 두 개의 기성품 잘 훈련된 COCO 객체 감지기를 사용하여 선택적 검색 제안을 대체하기 위한 사전 학습 데이터에 대한 의사 상자를 예측합니다. 구체적으로, 방정식 6의 (p, b¿¿)를 (pseudo, pseudo)로 대체합니다. 감지기 네트워크로 ij Pij ResNet50 또는 Swin-L 백본이 있는 H-Deformable-DETR을 사용합니다. 먼저 COCO에서 이를 훈련한 다음 훈련된 감지기를 사용하여 사전 학습 데이터 세트에서 의사 상자를 예측하고 상위 30개 예측을 k.ij Simple Self-training으로 선택합니다. 또한 이진 catpseudo egory target pseudo를 앞서 언급한 COCO 객체 감지기의 범주 예측 seЄ {0, C1,, Cn}으로 분류 대상으로 대체하고 이미 자세한 클래스 정보가 있으므로 fa를 제거합니다. 그로 인해 검출기는 swav ij 지역화 방법 선택적 탐색 AP AP50 AP75 APS APM APLAR@10 AR@30 AR@0.5 1.6 0.2 0.2 0.3 1.2 3.H-변형 가능-DETR + R50 28.4 40.4 30.2 12.7 26.7 43.1 26.H-변형 가능-DETR + Swin-L 30.7 41.3 33.0 15.2 29.0 44.28.8.37.38.15.47.47.표 1: R50 및 Swin-L 백본이 있는 선택적 탐색 상자 제안 및 H-변형 가능-DETR 기반 COCO 검출기의 가상 상자 예측에 대한 Objects356 AP 및 AR 점수. COCO에서 학습하고 예측하는 가상 범주 레이블이 80개 COCO 범주인 경우 이진 분류가 다중 클래스 분류로 바뀝니다.수식은 아래와 같습니다.Vij = (cdo), Zij ij pseudo.jk mul j=+Ado (čij, bij), Lpre (Yi, Zi) = cos(seudo, ioi (j))(8) (9) -pseudo , bio:(), 여기서 mul()은 다중 클래스 분류 손실입니다.cls 3.3 토론 두 가지 사전 학습 벤치마크로 ImageNet과 Objects365를 활용합니다.두 개의 기성품 COCO 객체 감지기에서 생성된 선택적 검색 제안과 가상 상자의 품질을 표시하기 위해 표 1에 Objects365 검증 세트에 대한 상자의 평균 정확도와 평균 재현율을 보고합니다.볼 수 있듯이 COCO 객체 감지기에서 생성된 가상 상자는 선택적 검색 상자보다 훨씬 정확합니다. 그림 4에서 상자 제안을 시각화합니다. 복잡한 증강 전략을 적용하여 가상 레이블의 품질을 높이고, NMS 임계값을 신중하게 조정하고, 미세 조정된 모델을 기반으로 반복적인 방식으로 더 정확한 가상 레이블을 재생성하는 기존의 자체 학습 방식(Xie et al., 2020; Zoph et al., 2020)과 달리, 당사의 단순 자체 학습 방법은 이러한 트릭 없이 한 번만 가상 레이블을 직접 생성하여 훨씬 간단한 접근 방식을 제공합니다. 4 실험 4.1 구현 세부 정보 데이터 세트. 당사의 객체 감지 네트워크는 ImageNet 또는 Objects365(Shao et al., 2019) 벤치마크에서 사전 학습한 다음 COCO train2017에서 미세 조정하고 COCO val2017에서 평가하거나 PASCAL VOC trainval07+12에서 미세 조정하고 PASCAL VOC test2007에서 평가합니다. 사전 학습 벤치마크의 경우 ImageNet에는 120만 개의 이미지가 있습니다.선택적 검색 상자 위치 상자 제안 자르기 백본 사전 학습 데이터 세트 백본 변환기 이진 클래스 개체 포함 대상 포함 (a) DETReg COCO 감지기 상자 위치 가상 상자 자르기 백본 사전 학습 데이터 세트 백본 변환기 이진 클래스 개체 포함 대상 포함 COCO 감지기 (b) DETReg+가상 상자 ImageNet (자기)지도 사전 학습 COCO 지도 사전 학습 상자 가상 상자 사전 학습 데이터 세트 백본 변환기 다중 클래스 무작위로 초기화된 가상 클래스 Ma 등 (c) 단순 자기 학습 그림 3: DETReg, DETReg+가상 상자 및 단순 자기 학습의 사전 학습 파이프라인. DETReg와 DETReg+pseudo-box에서 우리는 이미지 크롭에서 대상 객체 임베딩을 얻기 위해 여분의 동결된 백본 브랜치를 사용합니다. Transformer의 바이너리 클래스 출력은 탐지된 상자에 객체가 포함되어 있는지 예측합니다. Ground-Truth Selective Search H-Def-DETR + RH-Def-DETR + Swin-L 그림 4: Objects365에서 다양한 방법으로 생성된 상위 30개 바운딩 상자의 정성적 비교. 이 방법에는 Selective Search와 R50 또는 Swin-L 백본이 있는 훈련된 H-Deformable-DETR 감지기가 포함됩니다. 데이터 세트가 분류를 위해 생성되었기 때문에 대부분 하나의 객체를 포함합니다. Objects365는 200만 개의 이미지가 있는 객체 감지를 위한 대규모 데이터 세트입니다. 이미지 장면은 평균적으로 이미지당 약 15개의 Ground-Truth 바운딩 상자가 있어 더 복잡합니다. 섹션 4.2와 4.4의 모든 실험에 대한 기본 사전 학습 벤치마크로 Objects365를 사용합니다.복잡한 장면이 Simple Self-training 접근 방식에 더 나은 사전 학습 성능을 제공하기 때문입니다.아키텍처.ImageNet에서 SWAV에 의해 자체 감독 사전 학습된 ResNet50과 ImageNet에서 감독 사전 학습된 Swin-L을 포함하여 두 가지 종류의 DETR 백본을 사용합니다.섹션 4.3에서 vanilla DETR(Carion et al., 2020), Deformable-DETR(Zhu et al., 2020) 및 H-Deformable-DETR(Jia et al., 2023)을 포함하여 세 가지 DETR 기반 아키텍처를 사전 학습합니다.H-Deformable-DETR은 개선된 Deformable-DETR과 효과적인 하이브리드 매칭 방식을 결합한 최신 최첨단 객체 감지기입니다.이러한 아키텍처의 Transformer 모듈은 6개의 인코더 레이어와 6개의 디코더 레이어로 구성됩니다. vanilla DETR과 DeformableDETR은 트릭이 없는 평범한 반면, H-Deformable-DETR은 반복적 경계 상자 세분화, 2단계(Zhu et al., 2020), 혼합 쿼리 선택 및 2번의 기대 계획(Zhang et al., 2022)으로 개선되었습니다. 기본적으로 우리는 절제 연구를 위해 ResNet50 백본이 있는 H-Deformable-DETR을 사용합니다. 훈련. 우리는 DETReg에 따라 ImageNet forepoch에서 네트워크를 사전 훈련하거나, Objects365에서 3개의 에포크 동안 훈련하여 서로 다른 데이터 세트 크기에 따라 동일한 반복 횟수를 보장합니다. 미세 조정을 위해 150개의 에포크 동안 Revisiting DETR Pre-training for Object DetectionMethod Swin(Liu et al., 2021) HTC Group-DETR(Chen et al., 2022) DETRDINO-DETR(Zhang et al., 2022) DETR H-Deformable-DETR(Jia et al., 2023) DETR Ours(사전 훈련된 H-Deformable-DETR) DETR Swin-LFramework Backbone #epoch AP AP 50 AP75 APS APM APL Swin-L Swin-L Swin-L Swin-L Method Localization target Classification target AP AP 50 AP75 APS APM APL 36 57.1 75.6 62.5 42.4 60.7 71.58.41.0 62.5 73.58.5 77.0 64.1 41.5 62.3 74.57.9 76.9 63.7 42.4 61.9 73.59.3 77.9 65.1 44.1 62.9 73. 처음부터 DETReg 표 2: COCO val set에 대한 최신 DETR 기반 단일 규모 평가 결과와 시스템 수준 비교. 49.6 67.5 54.1 31.9 53.3 64.선택적 검색 객체 임베딩 손실 49.2 66.5 53.6 31.4 53.2 63.DETReg+가상 상자 가상 상자 예측 객체 임베딩 손실 52.0 69.6 56.7 36.1 55.9 65.간단한 자체 학습 가상 상자 예측 가상 클래스 예측 52.8 70.9 57.6 37.0 56.6 67.지도 학습 53.2 71.5 58.1 37.3 57.0 67.실제 값 실제 값 표 6: Objects365에서 다양한 지역화 및 분류 사전 학습 대상을 사용하여 다양한 방법으로 사전 학습한 후 COCO에 대한 미세 조정 결과. 방법 처음부터 DETReg DETR 모델 사전 학습 #쿼리 #에포크 AP AP 50 AP75 APS APM APL 우리의 DETR DETR DETR 100ImageNet 100ImageNet40.3 61.3 42.2 18.2 44.6 60. 방법 AP APAPAPS APM APL 40.2 60.7 42.3 17.6 44.3 59.41.9 62.7 44.0 20.7 46.0 62. 처음부터 63.82.70.26.50.70.DETReg 67.84.74.34.55.74.DETReg+가상 상자 71.87.79.36.59.77. 처음부터 DDETR-MSDETReg 우리의 DDETR-MS ImageNetDDETR-MS ImageNet처음부터 H-DDETR-MS DETRegH-DDETR-MS ImageNetours H-DDETR-MS ImageNet45.2 64.2 49.4 27.2 49.3 59.43.5 61.4 47.3 24.2 47.1 58.46.0 64.4 50.0 26.6 49.8 61.49.6 67.5 54.1 31.9 53.3 64.49.5 66.8 53.9 30.5 53.5 63.51.6 69.4 56.4 35.0 55.3 66.간단한 자기 학습 71.87.79.33.60.78.지도 학습 72.88.80.37.62.78.표 7: Objects365에서 다양한 방법으로 사전 학습한 후 PASCAL VOC에 대한 미세 조정 결과. #pseudo-box AP APAPAPS APM APL 표 3: COCO 다운스트림 벤치마크에서 자체 감독 사전 학습 방법 DETReg와 비교.52.69.56.36.55.65.51.69.56.34.55.65.51.68.56.34.54.65.Method DETR 모델 사전 학습 #query #epoch AP AP 50 AP 75 APS APM APL 처음부터 DETReg 우리의 DETR DETR 처음부터 DETR DDETR-MS100 150 56.3 80.3 60.6 10.2 36.0 65.ImageNet 100 150 60.9 82.0 65.9 15.1 40.8 69.ImageNet 100 150 63.5 83.8 68.6 22.5 44.3 72.50 61.1 83.1 68.0 25.5 47.4 67.표 8: DETReg+가상 상자 방법에 대한 가상 상자 수에 대한 절제 실험. 방법 인코더 디코더 AP AP50 AP 75 APS APM APL 52.69.56.DETReg+가상 상자 ✓ 49.67.53.36.1 55.9 65.32.0 53.2 63.51.69.ours 처음부터 H-DDETR-MS DETReg ours DETReg DDETR-MS ImageNetDDETR-MS ImageNetH-DDETR-MS ImageNet 300H-DDETR-MS ImageNet 30063.6 82.6 70.2 27.5 49.7 70.67.8 85.4 75.5 30.9 54.7 74.63.8 82.4 70.0 26.5 50.0 70.67.7 84.5 74.9 35.1 55.1 74.71.6 87.0 79.2 33.1 60.3 78.✓ ✓ 간단한 자체 학습 52.8 70.50.2 68.51.8 69.56.1 35.4 55.3 65.57.6 37.0 56.6 67.54.3 32.4 54.1 63.56.4 34.9 55.4 66.표 9: Transformer 인코더 또는 디코더 사전 학습의 효과.표 4: PASCAL VOC 다운스트림 벤치마크에서 자체 감독 사전 학습 방법 DETReg와의 비교. 방법 사전 학습 데이터 세트 DETReg DETReg+가상 상자 간단 자체 학습 ImageNetImageNetImageNetAP AP49.5 66.49.2 66.50.9 68.52.0 69.51.6 69.52.70.AP53.9 30.5 53.5 63.53.6 31.4 53.2 63.55.7 33.6 54.6 64.56.7 36.1 55.65.56.4 35.55.66.57.6 37.0 56.67.APS APM APL 표 5: 사전 학습 데이터 세트 선택의 효과. vanilla DETR, Deformable-DETR을 사용한 50개 에포크, H-Deformable-DETR을 사용한 에포크, 또는 섹션 4.2에서 HDeformable-DETR을 사용한 24개 에포크를 사용하여 더 나은 성능을 얻습니다. 학습률은 각각 120/150, 40/50, 11/12, 20/24로 떨어집니다. 사전 학습 및 미세 조정을 위한 배치 크기는 모두 16입니다. 메트릭. 상위 100개 감지된 경계 상자에 대한 객체 감지 정확도를 측정합니다. 구체적으로, 0.50~0.95 범위에서 IoU 임계값을 사용할 때 평균 정밀도로 AP, AP 50, AP 75를 계산하고 정확히 0.50 또는 0.75를 계산합니다. 또한 작고 중간 크기, 큰 경계 상자에 대한 AP로 AP, APM, APL을 계산합니다. 4.2 최신 기술과의 비교 표 2는 Objects365 벤치마크에서 사전 학습된 H-Deformable-DETR 네트워크의 COCO 검증 세트에 대한 객체 감지 결과를 우리 방법과 다른 최신 객체 감지 시스템과 비교하여 보여줍니다.우리의 Simple Self-training 방식은 더 적은 학습 에포크로 H-Deformable-DETR의 성능을 57.9%에서 59.3%로 크게 향상시킵니다.우리는 우리의 방식이 더 큰 배치 크기와 에포크 수로 더 나은 결과를 얻을 것으로 기대합니다.예를 들어, 샴 DETR의 자체 감독 사전 학습에 256의 배치 크기와 60의 에포크가 사용됩니다(Huang et al., 2023).4.3 다양한 DETR 아키텍처의 결과 표 3과 표 4에서 볼 수 있듯이 COCO와 PASCAL VOC 벤치마크에서 다양한 DETR 아키텍처를 사용한 DETReg 및 Simple Self-training의 결과를 표시합니다.Ma et al. AP (%)+18.+15.-- DETReg DETReg+가상 상자 단순 자가 학습+9.+8.+8.+3.B+ +4.+3.+1.+3.+2.+1.5% 10% 25% 50% 레이블이 지정된 데이터(%) 그림 5: 저데이터 체제에서의 절제 실험. 값은 처음부터의 기준선과 비교하여 세 가지 사전 학습 방식의 성능 향상을 보여줍니다. 처음부터의 선은 ResNet50 백본이 SWAV로 초기화되고 Transformer가 무작위로 초기화되었기 때문에 사전 학습 없이 기준선 결과를 보여줍니다. 결과에 따르면 보고된 실험 설정에서 DETReg 사전 학습은 COCO 벤치마크에서 처음부터의 기준선을 개선하지 못하는 반면 PASCAL VOC 벤치마크에서는 약간의 이득을 얻을 수 있습니다. 간단한 자가 학습은 두 벤치마크에서 세 가지 DETR 아키텍처 모두의 기준선 성능을 효과적으로 향상시킬 수 있습니다. 4.4 절제 실험 및 분석 사전 학습 데이터 세트 선택. 또한 표 5에서 H-DeformableDETR 아키텍처를 사용한 사전 학습 데이터 세트의 영향을 조사합니다.ImageNet과 비교하여 Objects365 벤치마크를 사용한 사전 학습은 DETReg+pseudo-box 및 Simple Selftraining 접근 방식에서 더 나은 성능을 제공하는 반면 DETReg 접근 방식에서는 그렇지 않습니다.DETReg+pseudo-box 및 Simple Selftraining은 사전 학습 대상으로 COCO 감지기의 정확한 가상 상자를 사용하므로 Objects365와 같은 더 풍부한 개체가 포함된 더 복잡한 이미지 장면에서 이점을 얻을 수 있는 반면 Selective Search의 Objects365에 대한 혼란스러운 제안은 ImageNet에 대한 제안보다 더 나은 지역화 대상이 아닐 수 있습니다.ImageNet은 사전 학습 일반 표현 능력에 대한 좋은 벤치마크이며 여러 다운스트림 작업으로 이전할 수 있다는 것이 입증되었습니다.그러나 특정 감지 네트워크를 사전 학습하는 경우 가상 상자의 품질이 좋으면 Objects365와 같은 대규모 개체 감지 벤치마크가 더 유용합니다. 따라서 다음 연구에 대한 기본 사전 학습 벤치마크로 Objects365를 사용합니다.사전 학습 방법.처음부터 시작하는 방법과 다른 사전 학습 방법의 COCO 벤치마크에 대한 다운스트림 결과를 표 6에 제시하고 PASCAL VOC 벤치마크에 대한 결과를 표 7에 제시합니다.처음부터 시작하는 방법을 제외한 모든 방법은 Objects365 벤치마크에서 사전 학습됩니다.중간 세 가지 사전 학습 방법은 Objects365 기준 진실 레이블을 사용하지 않는 반면 마지막 방법은 기준 진실에 의해 감독되므로 상한으로 사용됩니다.세 가지 비지도 사전 학습 파이프라인 간의 차이점은 그림 3에 나와 있습니다.표 6 및 7에서 볼 수 있듯이 DETReg+가상 상자 방법은 DETReg를 기반으로 하고 더 정확한 COCO 감지기 가상 상자를 활용하여 위치 지정 대상을 개선하여 상당한 개선을 이룹니다. Simple Self-training 방법은 객체 임베딩 손실을 버리고 대신 COCO 감지기의 클래스 예측으로 다중 클래스 분류 헤드를 감독하여 추가 성능 향상을 가져옵니다.감독 방법의 경우 Simple Self-training의 의사 상자 및 의사 클래스 대상을 실제 진실로 대체하고 Simple Self-training 전략보다 약간 더 나은 상한 성능을 달성합니다.이 단계별 비교는 더 나은 로컬리제이션 및 분류 대상을 도입하여 사전 학습 성능을 점진적으로 개선할 수 있는 방법을 보여줍니다.또한 객체 감지 작업에 대해 더 나은 분류 대상보다 더 나은 로컬리제이션 사전 학습 대상이 더 큰 영향을 미친다는 것을 관찰했습니다.의사 상자 수.표 8에서 DETReg + 의사 상자 방법의 의사 상자 수로 절제합니다. 사전 학습을 위해 30개가 넘는 가상 상자를 사용해도 성능이 향상되지 않는 것을 관찰했습니다.가상 상자가 많을수록 실제 진실에 대한 재현율이 더 높고(표 1에 표시된 대로 AR@10, 30, 100은 제안된 상자가 10, 30, 100개인 AR을 의미함) 더 많은 감독 신호를 제공하기 때문입니다.가능한 설명은 각 Objects365 이미지에 약 15개의 상자 주석이 포함되어 있고 상위 30개를 넘는 예측은 노이즈를 가상 상자 대상에 통합한 결과로 신뢰도가 낮고 정보가 덜 중요할 수 있다는 것입니다.인코더/디코더 사전 학습.표 9에서 DETReg+가상 상자와 Simple Self-training 접근 방식에서 Transformer 인코더 및 디코더 사전 학습의 중요성을 평가합니다.먼저 인코더와 디코더 사전 학습된 매개변수를 모두 사용한 성능을 보고한 다음 인코더 또는 디코더 사전 학습된 매개변수만 로드하고 다른 부분을 무작위로 초기화한 결과를 보고합니다. 두 사전 학습 방식 모두에서 인코더 사전 학습이 디코더 사전 학습보다 더 많은 기여를 하는 것을 관찰했는데, 이는 1b에 표시된 인코더 GFLOP의 높은 비율을 고려하면 타당합니다. 미세 조정 데이터 세트 크기. 우리는 제한된 양의 데이터만 사용할 수 있을 때 처음부터 학습하는 것과 비교하여 세 가지 사전 학습 계획의 효과를 조사합니다.객체 감지를 위한 DETR 사전 학습 재방문 COCO PASCAL VOC 텍스트 프롬프트 생성 모델 사전 학습 데이터 세트 현지화 대상 분류 대상 AP APAPAP APAPPseudo-box 예측 가상 클래스 예측 52.70.57.71.87.79.COCO 캡션 COCO 캡션 LLAVA 캡션 LLAVA 캡션 COCO 캡션 LLAVA 캡션 ControlNet Control-COCO 2M 실제 값 실제 값 51.69.55.8 71.87.79.ControlNet Control-COCO 2M 가상 상자 예측 가상 클래스 예측 52.6 70.57.5 72.0 87.80.ControlNet LLAVAControl-COCO 2M 실제 값 실제 값 50.7 69.55.4 71.6 87.79.ControlNet LLaVAControl-COCO 2M SDXL SDXL-COCO 2M SDXL SDXL-COCO 2M 가상 상자 예측 가상 상자 예측 가상 상자 예측 가상 클래스 예측 가상 클래스 예측 가상 클래스 예측 52.9 70.57.9 72.87.80.52.70.57.72.1 87.79.52.9 71.58.0 72.0 87.80.표 10: 텍스트-이미지 생성 모델 ControlNet 및 SDXL에 의해 생성한 COCO와 유사한 합성 이미지로 사전 학습한 평가 결과. 생성 모델에 주어진 텍스트 프롬프트는 COCO 기준 진실 캡션(COCO 캡션으로 표현)이거나 COCO 이미지를 기반으로 한 대규모 멀티모달 모델 LLaVA가 생성한 캡션(LLaVA 캡션으로 표현)입니다.COCO PASCAL VOC 텍스트 프롬프트 생성 모델 사전 학습 데이터 세트 현지화 대상 분류 대상 AP APAPAP APAPLLAVA 캡션 ControlNet LLAVA 캡션 SDXL LLaVAControl-0365 2M SDXL-0365 2M 가상 상자 예측 가상 상자 예측 가상 클래스 예측 52.가상 클래스 예측 52.70.57.71.87.79.70.57.6 71.6 87.4 79.표 11: ControlNet 및 SDXL에서 생성한 Objects365와 유사한 합성 이미지로 사전 학습한 평가 결과. Objects에는 기준 진실 캡션이 없으므로 생성 모델에 제공된 텍스트 프롬프트는 Objects 이미지(LLaVA 캡션으로 표현됨)를 기반으로 LLaVA가 생성한 캡션입니다.그림 5에서 미세 조정.특히, 사전 학습된 네트워크를 COCO 학습 세트의 5%, 10%, 25%, 50%에서 미세 조정하고 전체 COCO 검증 세트에서 평가합니다.세 가지 사전 학습 방식 모두 수렴 속도를 크게 높입니다.DETReg는 무작위 초기화보다 약간 더 높은 성능을 제공하는 것을 관찰했습니다.Simple Selftraining 접근 방식은 특히 매우 적은 양의 데이터(5%)만 사용할 수 있는 경우 가장 효과적입니다.질적 분석.미세 조정 없이 그림 6에서 사전 학습된 인코더의 구별 점수(Zong et al., 2022)를 시각화하여 인코더가 사전 학습에서 무엇을 학습했는지 조사합니다. 그림에서 DETReg의 특징 구별 능력이 배경에 의해 심각하게 방해받는 것을 볼 수 있습니다. 그러나 DETReg+pseudobox와 Simple Self-training 접근 방식에서 개선된 현지화 및 분류 대상을 활용하면 더 세부적인 정보가 포착됩니다. 특히 Simple Self-training 방법은 실제 진실을 사용한 사전 학습과 거의 같은 성능을 보여줍니다. 또한 그림 7에서 사전 학습된 디코더의 변형 가능한 교차 어텐션을 시각화합니다. 이미지의 색상이 있는 점은 모든 해상도 스케일의 샘플링 포인트를 나타내며, 색상은 어텐션 가중치를 나타내고 밝은 색상은 더 높은 어텐션을 나타냅니다. 무작위 초기화에서 알 수 있듯이 초기 주요 포인트는 중심에서 가장자리까지 방사형으로 샘플링됩니다. 모든 사전 학습 방법은 샘플링 포인트를 관심 객체 전체에 다른 패턴으로 분산시키는 방법을 학습하는 반면, Simple Self-training 사전 학습된 디코더는 정확한 범위의 객체에서 주요 포인트를 샘플링하고 어텐션 가중치를 보다 효과적으로 분산할 수 있습니다. 4.5 TLast에서 생성한 합성 데이터를 사용한 결과, 최근 대규모 텍스트-이미지 생성 모델을 사용하여 생성된 합성 데이터를 사용한 사전 학습의 효과를 조사합니다. 구체적으로, ControlNet(Zhang 및 Agrawala, 2023)과 SDXL(Podell 등, 2023)이라는 두 가지 대표적인 텍스트-이미지 모델을 활용하여 이미지를 생성합니다. 이러한 모델은 COCO 데이터 세트의 원래 캡션이나 LLAVA(Liu 등, 2023)에서 생성한 캡션을 이미지 합성을 위한 프롬프트로 사용합니다. ControlNet은 DPT(Ranftl 등, 2021)에서 예측된 깊이 맵을 조건부 입력으로 사용하여 깊이 맵과 캡션 모두와 일치하는 이미지를 생성합니다. 반면 SDXL은 추가 조건 없이 제공된 캡션에만 기반하여 이미지를 생성합니다. 230만 개의 생성된 이미지로 구성된 합성 데이터 세트를 만듭니다. 그림 8은 몇 가지 예를 보여줍니다. ControlNet에서 생성된 이미지를 분석한 결과, 원본 이미지의 레이아웃과 매우 유사함을 발견했습니다.이미지 랜덤 초기화 랜덤 초기화 DETReg Ma et al. DETReg+ 의사 상자 단순 자기 학습 지도 학습 그림 6: COCO val 이미지의 인코더에서 판별력 점수 시각화.DETReg South rep uth Tyne DETReg+ 의사 상자 단순 자기 학습 지도 학습 그림 7: COCO val 이미지의 마지막 Transformer 디코더 계층을 기반으로 한 변형 가능한 교차 주의 시각화.깊이 맵에 대한 조건화로 인해 이미지가 생성되었습니다.이러한 특성을 통해 ControlNet에서 생성한 합성 이미지를 사용할 때 COCO 기준 진실 데이터를 사용하여 사전 학습 프로세스를 지도 학습할 수 있습니다.또한 훈련된 COCO 감지기에서 생성된 의사 상자 및 의사 클래스 예측으로 사전 학습하여 합성 데이터에 대한 단순 자기 학습 접근 방식도 살펴봅니다. 이 프로세스에는 3개의 에포크 동안 합성 이미지로 H-Deformable-DETR 모델을 사전 학습한 다음, 12개의 에포크 동안 COCO 또는 PASCAL VOC 벤치마크에서 미세 조정하는 것이 포함됩니다. 이 평가의 결과는 표 10에 나와 있습니다. 흥미롭게도 COCO를 기반으로 생성된 합성 데이터 세트로 사전 학습한 것은 Simple Self-training 방식을 사용하여 Objects365 실제 데이터로 사전 학습한 것과 비슷한 개선을 보였습니다. 이 결과는 텍스트-이미지 합성이 사전 학습을 위해 원래 데이터 세트를 확장하는 효과적인 방법임을 나타냅니다. 또한 PASCAL VOC 벤치마크의 결과는 COCO를 기반으로 생성된 합성 데이터로 사전 학습한 것의 일반화 능력을 보여줍니다. 표 11은 LLaVA로 Objects365 이미지에 캡션을 먼저 붙인 다음 캡션에서 새 이미지를 합성하여 Objects365를 기반으로 생성된 합성 데이터로 사전 학습한 결과를 보여줍니다. 두 다운스트림 벤치마크 모두에서 COCO 기반 합성 데이터로 사전 학습한 것만큼 좋지 않습니다. 5
--- CONCLUSION ---
우리는 DETR을 위한 대표적인 자기 감독 사전 학습 방식인 DETReg의 효과를 세 가지 다른 DETR 아키텍처에서 조사합니다. 안타깝게도 우리의 연구 결과는 최근 아키텍처에서 DETReg의 성능 향상을 보여주지 않아 이전 결론의 타당성에 의문을 제기합니다. 이에 대응하여 로컬라이제이션 및 분류를 위한 사전 학습 대상을 포함한 중요한 설계 측면을 재평가합니다. 이 분석의 결과로 여러 가지 영향력 있는 향상 사항과 강력한 DETR 아키텍처에서 성능을 크게 향상시키는 간단한 자기 학습 방식을 소개합니다. 또한 강력한 텍스트-이미지 생성 모델을 활용하여 사전 학습 목적으로 합성 데이터 세트를 구성합니다. 놀랍게도 우리의 접근 방식은 Objects365를 사용한 사전 학습의 성과와 동등한 수준의 개선을 가져옵니다. 앞으로는 인스턴스 분할 및 포즈 추정과 같은 더 광범위한 비전 작업을 포함하도록 DETR 사전 학습을 확장할 계획입니다. 우리의 작업이 연구 커뮤니티가 강력한 DETR 모델의 맥락에서 사용될 때 기존 자기 지도 사전 학습 방법의 실제 용량을 재평가하고 이 어려운 과제의 진전을 촉진하도록 자극할 수 있기를 바랍니다.uc Store PHONE DV. 15,Ma et al. 원본 이미지 COCO 캡션 + ControlNet LLAVA 캡션 + ControlNet COCO 캡션 + SDXL LLAVA 캡션 + SDXL 그림 8: 다양한 캡션과 생성 모델을 사용한 합성 이미지의 예. 원본 이미지는 COCO 학습 세트에서 샘플링되었습니다. 객체 감지를 위한 DETR 사전 학습 재검토 데이터 가용성 설명 저자는 이 연구의 결과를 뒷받침하는 데이터가 기사 내에서 사용 가능하다는 것을 확인했습니다. 이 연구의 결과를 뒷받침하는 원시 데이터와 생성된 합성 데이터 세트는 합리적인 요청 시 해당 저자에게서 사용할 수 있습니다. 참고문헌 Bar A, Wang X, Kantorov V, Reed CJ, Herzig R, Chechik G, Rohrbach A, Darrell T, Globerson A (2022) Detreg: 객체 감지를 위한 영역 사전을 사용한 비지도 사전 학습. CVPR에서, pp 14605-Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S (2020) 변압기를 사용한 종단 간 객체 감지. ECCV에서, Springer에서, pp 213-Caron M, Misra I, Mairal J, Goyal P, Bojanowski P, Joulin A (2020) 클러스터 할당을 대조하여 시각적 특징에 대한 비지도 학습. NeurIPS 33:9912-Chen Q, Wang J, Han C, Zhang S, Li Z, Chen X, Chen J, Wang X, Han S, Zhang G, et al. (2022) Group detr v2: 인코더-디코더 사전 학습을 통한 강력한 객체 감지기. arXiv 사전 인쇄본 arXiv: Chen T, Kornblith S, Norouzi M, Hinton G (2020) 시각적 표현의 대조 학습을 위한 간단한 프레임워크. ICML, PMLR, pp 1597-Dai Z, Cai B, Lin Y, Chen J (2021) Up-detr: 변압기를 사용한 객체 감지를 위한 비지도 사전 학습. CVPR, pp 1601–Grill JB, Strub F, Altché F, Tallec C, Richemond P, Buchatskaya E, Doersch C, Avila Pires B, Guo Z, Gheshlaghi Azar M, et al. (2020) 자신의 잠재 객체를 부트스트랩하세요. 자기 지도 학습에 대한 새로운 접근 방식. NeurIPS 33:21271-He K, Fan H, Wu Y, Xie S, Girshick R(2020) 비지도 시각적 표현 학습을 위한 모멘텀 대비. In: CVPR, pp 9729-Huang G, Li W, Teng J, Wang K, Chen Z, Shao J, Loy CC, Sheng L (2023) Siamese detr. In: CVPR, pp 15722-Jia D, Yuan Y, He H, Wu X, Yu H, Lin W, Sun L, Zhang C, Hu H (2023) 하이브리드 매칭을 통한 Detrs. In: CVPR, pp 19702-Kuhn HW (1955) 할당 문제에 대한 헝가리식 방법. 해군 연구 물류 분기별 2(1-2):83-Li F, Zhang H, Xu H, Liu S, Zhang L, Ni LM, Shum HY(2023) Mask dino: 객체 감지 및 분할을 위한 통합된 변환기 기반 프레임워크를 향하여.CVPR에서: pp 3041-Li Y, Wu CY, Fan H, Mangalam K, Xiong B, Malik J, Feichtenhofer C(2021) 분류 및 감지를 위한 개선된 다중 스케일 비전 변환기.arXiv 사전 인쇄본 arXiv:Li Z, Zhu Y, Yang F, Li W, Zhao C, Chen Y, Chen Z, Xie J, Wu L, Zhao R, et al.(2022) Univip: 자체 감독 시각적 사전 학습을 위한 통합 프레임워크. In: CVPR, pp 14627-Liu H, Li C, Wu Q, Lee YJ (2023) Visual instruction tuning Liu S, Li F, Zhang H, Yang X, Qi X, Su H, Zhu J, Zhang L (2022a) Dab-detr: 동적 앵커 상자는 detr에 대한 더 나은 쿼리입니다.arXiv 사전 인쇄본 arXiv:Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B (2021) Swin transformer: 이동된 창을 사용하는 계층적 비전 변환기.In: ICCV, pp 10012-Liu Z, Hu H, Lin Y, Yao Z, Xie Z, Wei Y, Ning J, Cao Y, Zhang Z, Dong L, et al. (2022b) Swin transformer v2: 용량 및 해상도 확장. In: CVPR, pp 12009-Podell D, English Z, Lacey K, Blattmann A, Dockhorn T, Müller J, Penna J, Rombach R (2023) Sdxl: 고해상도 이미지 합성을 위한 잠재 확산 모델 개선. 2307. Ranftl R, Bochkovskiy A, Koltun V (2021) 밀도 예측을 위한 비전 변환기. In: ICCV, pp 12179-Sahito A, Frank E, Pfahringer B (2022) 자기 감독을 통한 이미지 분류를 위한 더 나은 자기 학습. In: AJCAI, Springer, pp 645-Shao S, Li Z, Zhang T, Peng C, Yu G, Zhang X, Li J, Sun J (2019) Objects365: 객체 감지를 위한 대규모 고품질 데이터 세트. In: ICCV, pp 8430-Uijlings JR, Van De Sande KE, Gevers T, Smeulders AW(2013) 객체 인식을 위한 선택적 검색. IJCV 104(2):154–Vandeghen R, Louppe G, Van Droogenbroeck M(2022) 물체 감지를 위한 적응형 자가 훈련. arXiv 사전 인쇄 arXiv:Wang F, Wang H, Wei C, Yuille A, Shen W (2022) Cp 2: 의미론적 세분화를 위한 복사 붙여넣기 대조 사전 훈련. In: ECCV, Springer, pp 499-Wei F, Gao Y, Wu Z, Hu H, Lin S(2021) 객체 수준 대조 학습을 통한 감지를 위한 사전 훈련 정렬. NeurIPS 34:22682-Xie Q, Luong MT, Hovy E, Le QV(2020) 노이즈가 있는 학생을 이용한 자기 학습으로 이미지넷 분류가 개선됨. CVPR에서, pp 10687-Yang C, Wu Z, Zhou B, Lin S(2021a) 자기 지도 감지 사전 학습을 위한 인스턴스 지역화. CVPR에서, pp 3987-Yang L, Zhuo W, Qi L, Shi Y, Gao Y(2022) St++: 반지도 의미 분할을 위한 자기 학습을 더 잘 작동하게 함. CVPR에서, pp 4268-Yang Q, Wei X, Wang B, Hua XS, Zhang L(2021b) 반지도 객체 감지를 위한 평균 교사를 이용한 상호 작용 자기 학습. In: CVPR, pp 5941-Zhang H, Li F, Liu S, Zhang L, Su H, Zhu J, Ni LM, Shum HY (2022) Dino: 엔드투엔드 객체 감지를 위한 개선된 노이즈 제거 앵커 상자를 갖춘 Detr. arXiv 사전 인쇄본 arXiv:Zhang L, Agrawala M (2023) 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄본 arXiv:Zhu X, Su W, Lu L, Li B, Wang X, Dai J (2020) 변형 가능한 detr: 엔드투엔드 객체 감지를 위한 변형 가능한 변환기. In: ICLR Zhu Y, Zhang Z, Wu C, Zhang Z, He T, Zhang H, Manmatha R, Li M, Smola AJ (2021) 효율적인 자체 학습을 통해 의미 분할 개선. PAMI Zong Z, Song G, Liu Y(2022) 협업 하이브리드 과제 교육을 통한 Detrs. arXiv 사전 인쇄 arXiv:Zoph B, Ghiasi G, Lin TY, Cui Y, Liu H, Cubuk ED, Le Q(2020) 사전 훈련과 자가 훈련을 재고합니다. NeurIPS 33:3833-
