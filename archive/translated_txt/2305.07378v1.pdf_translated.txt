--- ABSTRACT ---
대규모 언어 모델(LMS)이 공정하고 견고하며 유용하도록 하려면 입력에 대한 다양한 수정이 모델의 동작에 어떤 영향을 미치는지 이해해야 합니다. 그러나 오픈 텍스트 생성 작업의 맥락에서 이러한 평가는 사소한 일이 아닙니다. 예를 들어, 입력 텍스트와 그 텍스트의 교란된 &quot;대조적&quot; 버전이 있는 모델을 도입할 때 표준 디코딩 전략으로는 다음 토큰 예측의 의미 있는 차이가 드러나지 않을 수 있습니다. 이러한 동기를 염두에 두고 우리는 대조적 입력 디코딩(CID)을 제안합니다. 이는 두 개의 입력이 주어진 텍스트를 생성하는 디코딩 알고리즘으로, 생성된 텍스트는 한 입력이 주어졌을 때는 가능성이 높지만 다른 입력이 주어졌을 때는 가능성이 낮습니다. 이런 방식으로 대조적 세대는 간단하고 해석 가능한 방식으로 두 입력에 대한 LM 출력이 어떻게 다른지에 대한 잠재적으로 미묘한 차이를 강조할 수 있습니다. 우리는 CID를 사용하여 표준 디코딩 전략으로는 감지하기 어려운 컨텍스트별 편향을 강조하고 다양한 입력 교란의 효과를 정량화합니다. 1
--- INTRODUCTION ---
대규모 사전 학습된 언어 모델(LMS)은 최근 몇 년 동안 자연어 처리에 혁명을 일으켰습니다(Radford et al., 2019; Raffel et al., 2020). 그러나 인간이 중요하지 않다고 여기는 것(Belinkov and Bisk, 2017; Sun et al., 2018)을 포함하여 사소한 입력 섭동(자연적 및 적대적)에 대한 극도의 민감성으로 인해 실제 적용이 여전히 방해를 받고 있습니다. HealthSearchQA 데이터 세트(Singhal et al., 2022)에서와 같이 &quot;리스테리아를 치료하지 않으면 어떻게 되나요?&quot;와 같은 의학적 질문에 답하기 위해 LM을 사용하는 것을 고려하세요. 인구 통계 정보를 지정하는 것의 효과는 무엇입니까(예: &quot;남성의 경우 치료되지 않은 것?&quot; vs &quot;여성의 경우 치료되지 않은 것?&quot;)? 분류 작업(예: 목록에서 옵션 하나 선택)에서 gal.yona@gmail.com; Google에서 인턴으로 일하면서 완료한 작업. 모델의 예측이 변경되었는지 직접 평가할 수 있습니다. 하지만 오픈 텍스트 생성 작업에서는 관련 결과 공간이 이제 방대하기 때문에 섭동의 영향을 테스트하는 방법이 직접적으로 명확하지 않습니다. 두 입력(예: 탐욕적 디코딩 또는 빔 검색 사용)을 모두 고려하여 여러 가지 가능한 응답을 결정적으로 생성하여 비교할 수 있지만, 이는 피상적인 것에 불과할 수 있습니다. 이 비교에서는 매우 가능성이 높은 시퀀스의 작은 집합만 살펴보기 때문에 모델 동작의 의미 있는 차이가 드러나지 않을 수 있습니다. 이러한 차이는 미묘하지만 이해하고 정량화하는 것이 중요합니다(예: 악의적인 사용자가 탐욕적 디코딩 방법을 사용하더라도 문제가 되는 동작을 유발하기 위해 이를 증폭하려고 시도할 수 있음). 또는 각 입력(예: 온도 샘플링 사용)을 고려하여 확률적으로 가능한 응답을 생성할 수 있지만, 그러면 각 입력으로 얻은 출력을 비교하는 방법이 덜 명확합니다. 공정성과 견고성 문제를 넘어, 잘 정의된 많은 작업에서 성공은 문구의 작은 변화에 매우 민감하다는 것이 밝혀졌습니다(Srivastava et al., 2022; Efrat et al., 2022). 특히 &quot;프롬프트 엔지니어링&quot;이 표준 관행이 된 지금 더욱 그렇습니다. 이를 감안할 때 입력/프롬프트 수정의 영향을 이해하는 것이 매우 중요합니다. 이 작업에서 우리는 새로운 디코딩 전략인 대조 입력 디코딩(CID)을 도입하여 이러한 과제를 해결하기 위한 한 걸음을 내딛습니다. 우리의 디코딩 알고리즘은 두 개의 입력을 허용합니다. 일반 입력 x와 &quot;대조&quot; 입력 x&#39;이며, x가 주어지면 가능성이 높지만 x&#39;가 주어지면 가능성이 낮은 시퀀스를 생성하는 것을 목표로 합니다. 이러한 대조적 세대는 모델이 이 두 입력을 해석 가능한 방식으로 처리하는 방식의 차이점을 강조합니다. CID는 대조 정도를 제어하는 하이퍼 매개변수 λ = R에 의해 매개변수화됩니다(λ = 0은 표준, 비대조 디코딩을 복구함). 이런 식으로 \를 증가시키면 차이를 표면화하는 데 사용할 수 있습니다.¹예를 들어, Med-PaLM은 이 질문에 대한 한 문단 답변을 생성합니다. Singhal et al. (2022), 표 10 참조. 그렇지 않으면 감지하기 어려울 수 있습니다(그림 1). CID에 대한 두 가지 응용 프로그램을 보여줍니다. (1) 자기 회귀 LM에서 맥락별 편향 표면화: 섹션 4에서는 CID를 사용하여 반사실적 공정성(Kusner et al., 2017)과 같은 공정성 속성에 대해 LM을 감사하는 방법을 보여 주며, 때로는 그렇지 않으면 감지하기 어려운 편향을 드러냅니다. (2) 다양한 입력 섭동의 효과 정량화: 언어 모델링 수준에서 사소한 입력 수정에 대한 민감성이 결국 불가피하더라도, 신뢰를 구축하는 데 중요한 부분은 민감성의 크기가 사용자의 기대치와 일치하도록 보장하는 것입니다. 섹션 5에서는 CID를 사용하여 다양한 섭동 유형(예: 구문적 섭동 대 의미적 섭동)의 상대적 효과를 정량화하는 방법을 보여줍니다. 2
--- RELATED WORK ---
입력 섭동에 대한 견고성. 신경 언어 모델의 다양한 입력 섭동에 대한 민감도를 테스트하는 것은 모델 공정성(입력 섭동이 개인에 해당하는 경우)과 모델 견고성(섭동이 철자 오류 또는 적대적 수정과 같이 시스템이 테스트 시간에 경험할 가능성이 있는 조건에 해당하는 경우)의 관점에서 모두 연구되었습니다. 예를 들어, Prabhakaran et al.(2019)은 하나의 실제 엔터티를 동일한 유형의 다른 엔터티로 대체하는 섭동에 대한 텍스트 분류 모델의 민감도를 평가하고 Moradi와 Samwald(2021)는 다양한 유형의 문자 수준 및 단어 수준 섭동에 대한 견고성을 평가합니다. 이러한 모든 작업에서 공통적으로, 견고성은 다운스트림 분류 작업에 대해 평가되고 텍스트 생성에 대해 직접적으로 평가되지 않는다는 점이 있습니다. 여기서는 우리의 초점이 이와 같습니다. 대조적 풍미로 디코딩하는 것은 이전에 텍스트 생성의 품질을 개선하는 수단으로 제안되었습니다. Schick et al. (2021)은 독성 텍스트 생성을 유도하도록 제작된 프롬프트의 입력(예: &quot;이 텍스트는 인종차별적입니다&quot;)을 대조함으로써 LLM이 덜 독성이 강한 텍스트를 생성한다는 것을 보여줍니다. 마찬가지로 Li et al. (2022)은 동일한 입력에 대해 두 가지 다른 모델(&quot;아마추어&quot; 및 &quot;전문가&quot; 모델)의 예측을 대조하면 더 높은 품질의 생성이 생성된다는 것을 보여줍니다. 저희의 접근 방식은 이 작업 라인에서 영감을 받았지만 개념적으로 다릅니다. 저희는 생성 품질을 개선하는 것이 아니라 섭동의 영향을 이해하는 목표로 입력을 섭동된 버전과 대조합니다. 대조적 설명은 Jacovi et al. (2021)에서 텍스트 분류 모델을 해석하는 데 사용되고 Yin과 Neubig(2022)에서는 해석 가능한 언어 모델링에 사용됩니다. 이러한 작업은 단일 입력이 주어졌을 때 모델이 y&#39;보다 y를 선호하는 이유를 설명하는 것이 목표이기 때문에 저희 작업과 다릅니다. 즉, 대조는 입력이 아니라 결과에 관한 것입니다. 3
--- METHOD ---
s). 또는 각 입력에 대해 확률적으로 가능성 있는 응답을 생성할 수 있지만(예: 온도 샘플링 사용), 그러면 각 입력으로 얻은 출력을 비교하는 방법이 덜 명확합니다. 공정성과 견고성 문제를 넘어, 잘 정의된 많은 작업에서 성공은 문구의 작은 변화에 매우 민감하다는 것이 밝혀졌습니다(Srivastava et al., 2022; Efrat et al., 2022). 특히 &quot;프롬프트 엔지니어링&quot;이 표준 관행이 된 지금 더욱 그렇습니다. 따라서 입력/프롬프트 수정의 영향을 이해하는 것이 매우 중요합니다. 이 작업에서 우리는 새로운 디코딩 전략인 대조 입력 디코딩(CID)을 도입하여 이러한 과제를 해결하기 위한 한 걸음을 내딛습니다. 디코딩 알고리즘은 두 개의 입력을 허용합니다. 일반 입력 x와 &quot;대조&quot; 입력 x&#39;이며, x가 주어지면 가능성이 있지만 x&#39;가 주어지면 가능성이 낮은 시퀀스를 생성하는 것이 목표입니다. 이러한 대조적 세대는 모델이 이 두 입력을 해석 가능한 방식으로 처리하는 방식의 차이점을 강조합니다. CID는 대조 정도를 제어하는 하이퍼 매개변수 λ = R에 의해 매개변수화됩니다(λ = 0은 표준, 비대조 디코딩을 복구함). 이런 식으로 \를 증가시키면 차이를 표면화하는 데 사용할 수 있습니다.¹예를 들어, Med-PaLM은 이 질문에 대한 한 문단 답변을 생성합니다. Singhal et al. (2022), 표 10 참조. 그렇지 않으면 감지하기 어려울 수 있습니다(그림 1). CID에 대한 두 가지 응용 프로그램을 보여줍니다. (1) 자기 회귀 LM에서 맥락별 편향 표면화: 섹션 4에서는 CID를 사용하여 반사실적 공정성(Kusner et al., 2017)과 같은 공정성 속성에 대해 LM을 감사하는 방법을 보여 주며, 때로는 그렇지 않으면 감지하기 어려운 편향을 드러냅니다. (2) 다양한 입력 섭동의 효과 정량화: 언어 모델링 수준에서 사소한 입력 수정에 대한 민감성이 결국 불가피하더라도 신뢰를 구축하는 데 중요한 부분은 민감도 규모가 사용자의 기대치와 일치하도록 하는 것입니다.섹션 5에서는 CID를 사용하여 다양한 섭동 유형(예: 구문적 대 의미적)의 상대적 효과를 정량화하는 방법을 보여줍니다.2 관련 연구 입력 섭동에 대한 견고성. 신경 언어 모델의 다양한 입력 섭동에 대한 민감도를 테스트하는 것은 모델 공정성(입력 섭동이 개인에 해당하는 경우)과 모델 견고성(섭동이 철자 오류 또는 적대적 수정과 같이 시스템이 테스트 시간에 경험할 가능성이 있는 조건에 해당하는 경우)의 관점에서 모두 연구되었습니다.예를 들어, Prabhakaran et al. (2019)는 한 실제 세계 엔터티를 같은 유형의 다른 엔터티로 대체하는 섭동에 대한 텍스트 분류 모델의 민감도를 평가하고, Moradi와 Samwald(2021)는 다양한 유형의 문자 수준 및 단어 수준 섭동에 대한 견고성을 평가합니다. 이러한 모든 작업에서 공통적으로, 견고성은 다운스트림 분류 작업에 대해 평가되고 텍스트 생성에 대해 직접적으로 평가되지 않는다는 점이 있습니다. 여기서는 이에 초점을 맞춥니다. 대조적인 풍미로 디코딩하는 것은 이전에 텍스트 생성의 품질을 개선하는 수단으로 제안되었습니다. Schick et al.(2021)은 독성 텍스트 생성을 유도하도록 제작된 프롬프트(예: &quot;이 텍스트는 인종 차별적입니다&quot;)의 입력을 대조함으로써 LLM이 덜 독성 있는 텍스트를 생성한다는 것을 보여줍니다. 마찬가지로 Li et al.(2022)는 동일한 입력에 대해 두 가지 다른 모델(&quot;아마추어&quot; 및 &quot;전문가&quot; 모델)의 예측을 대조하면 더 높은 품질의 생성이 생성된다는 것을 보여줍니다. 우리의 접근 방식은 이 작업 라인에서 영감을 받았지만 개념적으로는 다릅니다. 우리는 섭동의 영향을 이해하는 것을 목표로(생성 품질을 개선하는 것이 아니라) 섭동의 영향을 이해하기 위해 섭동된 버전의 입력을 대조합니다. 대조적 설명은 텍스트 분류 모델을 해석하기 위해 Jacovi et al. (2021)에서 사용되고 해석 가능한 언어 모델링을 위해 Yin과 Neubig (2022)에서 사용됩니다. 이러한 작업은 단일 입력이 주어졌을 때 모델이 y&#39;보다 y를 선호하는 이유를 설명하는 것이 목적이기 때문에 우리 작업과 다릅니다. 즉, 대조는 입력이 아니라 결과에 대한 것입니다. 3 방법 사전 학습된 자기 회귀 언어 모델 M과 어휘 V의 토큰 w₁,..., wk 시퀀스가 주어졌을 때, 언어 모델이 wЄ V가 다음 토큰이 되는 데 할당할 확률을 PM(w|w1, ..., Wk)라고 합니다. 디코딩은 이전 컨텍스트(입력 텍스트 및 지금까지 프로세스에서 생성된 모든 텍스트)에 조건을 부여하여 한 번에 하나의 토큰을 반복적으로 생성하는 프로세스입니다. 예를 들어, 탐욕적 디코딩은 단순히 다음 토큰을 PM의 argmax로 선택합니다. ... = k&#39; 우리는 생성을 알리기 위해 추가적인 대조 입력을 사용하는 대조 디코딩 절차를 제안합니다. x x1 xk를 연속을 생성하려는 입력 텍스트라고 하고, x&#39; = x¹₁ · · · x&#39;는 대조 입력을 나타냅니다. 직관적으로, 우리의 목표는 x에서는 가능성이 있지만 x&#39;에서는 가능성이 낮은 텍스트를 생성하는 것입니다. 우리는 다음과 같이 대조 입력을 사용하여 다음 토큰 분포를 수정하여 이를 수행하는 것을 제안합니다. xk+1, ..., xk+i는 지금까지 생성된 토큰을 나타냅니다(디코딩 프로세스의 시작 부분에서 i = 0). 이 지점에서 어휘 V에 대한 두 개의 확률 분포가 있습니다. 우리는 (w; x, x&#39;)를 사용하여 그들의 차이를 표시합니다: A(w; x, x&#39;) = Pм(w|x1 k,k+1,• k+i) PM (w|x¹₁ · · · x k¹‚ Xk+1, • • • ‚· , xk+i). 2 x, x&#39;가 맥락상 명확할 때 우리는 ✗(w)를 약어 표기법으로 사용합니다. Xpre = xxk+i로 표시하고, 다음의 곱셈적 수정을 통해 Pм을 pм로 수정하여 연속을 생성하는 것을 제안합니다: Ñм(w|xpre) ∞ a(▲(w)) ·PM(w|xpre) (1) 여기서, a : [−1,1] → (0,∞)는 스케일링 함수 역할을 하며, 차이 A(w)에 따라 원래 확률 PM (w|x pre)를 곱셈적으로 변환합니다. 우리는 a(v) = exp(\ · v)를 사용합니다. 이렇게 하면 . 2이것은 i번째 토큰이 i + 1번째 토큰을 생성할 때 원래 입력과 대조 입력에 모두 컨텍스트로 추가된다는 것을 의미합니다. 이것은 우리가 조건지은 원래 컨텍스트와 대조 컨텍스트가 지속적으로 발산하지 않고 항상 원래 입력과 대조 입력이 다른 방식에서만 다르다는 것을 보장합니다. 야심찬 의사가 실패했습니다.<PRONOUN> 영어: 최종 레지던트 배치 면접을 대형 병원에서 받았습니다.그녀는 너무 긴장해서 면접이 나빴습니다.그는 너무 긴장해서 필요한 의료 면허가 없었습니다.그는 필요한 기술과 경험이 없었습니다.+ CID (λ=5) + CID (λ=50) 그녀는 면접에 잘못된 옷을 입었습니다.GPT 그녀는 너무 뚱뚱했습니다 + CID (λ=5) 그녀는 너무 뚱뚱했습니다 + CID (λ=50) 그녀는 올바른 서류가 없었습니다.그는 너무 뚱뚱해서 1,000달러의 수수료를 감당할 수 없었습니다.그는 자신의 비용으로 지불할 수 없었습니다.그림 1: λ의 효과: λ를 변화시킬 때 표준 탐욕적 디코딩과 CID를 사용하여 생성된 연속을 비교합니다.확률 ỹм(w)는 (i) 원래 입력과 대조 입력에서 모두 확률이 동일한 토큰의 경우 변경되지 않습니다(A(w) ≈ 0).(ii) 대조 입력에서 확률이 더 높은 토큰의 경우 감소합니다(A(w) &lt; 0). (iii) 원래 입력에서 더 가능성이 높은 토큰에 대한 증가(A(w) &gt; 0). 여기서 λ € [0, ∞)는 수정의 크기를 제어하는 데 사용할 수 있는 하이퍼파라미터 역할을 하며, λ = 0은 ①м = PM이므로 표준(비대조) 디코딩 절차에 정확히 해당합니다. 시각화는 부록 B의 그림 5를 참조하세요. 대조 입력 디코딩 CID(x; x&#39;, \)를 방정식(1)과 위의 a 선택에 따라 PM에 대한 디코딩³으로 정의합니다. 4 맥락별 편향 이해 동기. 편향에 대한 신경 언어 모델을 감사하기 위한 기존 접근 방식은 모델의 내부 표현을 감사하는 데 초점을 맞추거나(Bolukbasi et al., 2016; Caliskan et al., 2017; Guo and Caliskan, 2021) 다양한 하위 분류 작업에서 사회적으로 눈에 띄는 하위 그룹 간의 차이점을 강조하는 데 초점을 맞추었습니다(Zhao et al., 2018; DeArteaga et al., 2019; Cao and Daumé III, 2021). 이는 자유 텍스트 생성 모드에서 LM을 사용하는 데 관련된 편향을 이해하는 것이 목표인 설정에는 직접 적용할 수 없습니다. 예를 들어, LM을 사용하여 일반적으로 검색되는 소비자 의료 질문에 답하는 것을 고려하십시오(Singhal et al., 2022). 반사실적 공정성과 같은 개념을 평가하기 위해(Kusner et al., 2017) 특정 인구 통계적 속성의 수정이 모델의 동작에 어떤 영향을 미치는지 이해하고자 할 수 있습니다. ³ 특정 디코딩 전략(다음 토큰 분포에 따라 토큰을 선택하는 방법)은 대상 애플리케이션에 따라 선택할 수 있습니다. 나머지 원고에서는 간단히 탐욕적 디코딩(argmax 토큰 선택)을 사용합니다. 논의된 대로 이는 어려운 일입니다. 모델의 반응이 개입에 따라 불변해야 한다는 것은 분명하지 않습니다. 그러한 지식이 있는 입력에 주의를 제한하더라도 가장 가능성 있는 반응을 비교하는 것으로 나타나지 않는 모델 동작에 미묘한 차이가 있을 수 있습니다.
--- EXPERIMENT ---
모든 설정. CID를 사용하여 맥락별 편향을 해석 가능한 방식으로 표면화하는 방법을 보여줍니다. 특정 입력 템플릿(예: 기술 분야의 편향)을 고려하여 특정 맥락에서 조사를 시작합니다.<name> 소프트웨어 개발자인 그는(그녀는) &quot;그(그녀)가&quot;라는 이유로 주요 기술 회사의 면접에서 떨어졌습니다. Maudslay et al. (2019)에 따라 우리는 개입합니다.<name> 이 특정 입력에 대한 성별 및 인종적 편향을 추정하는 방법으로. John과 Ahmed와 같은 한 쌍의 이름에 대해 탐욕적 디코딩과 CID를 모두 사용하여 모델 연속을 얻습니다. 3개국(미국, 멕시코, 이집트, 위키피디아(2023))에서 가장 흔한 10개의 남성 및 여성 이름을 사용하여 6개의 이름 그룹을 형성하고 X의 다른 값에 대해 모든 100개의 이름 쌍 조합 중 가장 흔한 연속을 조사하여 인구 통계적 그룹 수준에서 공정성을 조사합니다. 고용 맥락에서 기존의 차별 금지법에 따라 정당화가 개인의 기원, 인종, 피부색, 종교, 장애 상태, 성별, 친숙한 상태, 출생지, 문화, 언어 또는 외모에 기반하는 경우 모델 연속은 편향된 것으로 간주됩니다. 결과. 우리는 flan-T5-large(780M 매개변수; Chung et al., 2022) 및 GPT2large(774M 매개변수; Radford et al., 2019)에 대한 결과를 보고합니다. 각 모델 및 그룹 쌍(예: 미국 남성 入 미국(남성) 이집트(남성) 0 1.00 0.0 1.00 0.10 0.47 0.1.00 0.50 0.00 0.0.1.그림 2: T5 및 GPT에 대한 편향된 대조 연속의 분율. 및 이집트 남성 이름)에 대해 위에서 언급한 기준에 따라 평가자가 편향된 것으로 동의한 연속의 분율을 보고합니다(그림 2). 일반적인 연속의 정성적 예는 그림 3을 참조하세요. 함께, 우리의 결과는 GPT의 경우 탐욕적 디코딩에서 이미 의미 있는 차이가 나타나며 이는 이미 편향되는 경향이 있음을 보여줍니다. 반면 T5는 더 공평합니다.탐욕적 디코딩은 편향된 연속을 생성하지 않으며 연속은 그룹 간에 유사합니다.그러나 소수자 그룹의 경우 CID는 알려진 고정관념에 매핑되는 차이점을 표면화합니다.TUS(남성)는 너무 짧았습니다.너무 긴장했습니다.T5 + CID는 테스트에 실패했습니다.GPT GPT + CID는 너무 뚱뚱했습니다.코딩하는 법을 몰랐습니다.적절한 기술이 없었습니다.이집트(남성)는 너무 짧았습니다.회사 제품에 대한 경험이 없었습니다.해결되지 않은 법적 문제가 있었습니다.이민자처럼 보이게 하는 악센트가 있었습니다.무슬림이었습니다.흑인이었습니다.무슬림이었습니다.그림 3: 일반 디코딩(회색)과 CID(빨간색)를 사용한 일반적인 연속.GPT의 경우 탐욕적 디코딩에서 의미 있는 차이가 분명합니다.T5는 더 공평하지만 CID는 소수자 그룹에 대한 편향을 표면화합니다.5 섭동 효과 정량화 동기. LM이 사소한 입력 수정에도 민감하게 반응하는 것은 불가피하지만, 사용자는 일부 섭동(예: 철자 오류 또는 관련 없는 정보 추가)이 다른 섭동보다 영향이 적을 것이라고 합리적으로 기대할 수 있습니다. 이를 개방형 생성 모드에서 테스트하려면 다양한 섭동의 영향을 정량화해야 합니다. 섹션 4에서 살펴본 것처럼 생성된 연속을 직접 비교하는 것(예: 의미적 유사성의 한 형태 사용)은 잠재적으로 너무 거칠 수 있습니다. 이 목적을 위해 CID를 사용할 것을 제안합니다. 4결과는 다양한 그룹 조합에서 일관적입니다. 여기서는 단일 쌍에 초점을 맞추고 부록 C에서 추가 조합을 찾을 수 있습니다. 다음과 같습니다. 원래 입력과 섭동된 입력의 쌍(x, x&#39;)을 고려합니다. 직관적으로 \는 대조적 연속 CID(x; x&#39;, \)와 CID(x&#39;; x, X)를 더 멀리 떨어뜨리는 &quot;노브&quot; 역할을 합니다. 따라서 λ가 증가함에 따라 두 연속 간의 의미적 유사성이 감소할 것으로 예상합니다. 그런 다음 입력 섭동의 효과를 X* = arg minx [sim(x + CID(x; x&#39;, \), x + CID(x&#39;; x, X)) &lt; 7]로 정량화할 수 있습니다. 여기서 sim은 의미적 유사성의 척도이고 7은 선택한 임계값입니다. 직관적으로 λ* Є [0, ∞)는 연속을 충분히 멀리 &quot;밀어내는&quot; 데 필요한 가장 작은 대조량입니다. 낮은 값은 강한 효과가 있는 입력 섭동을 나타냅니다(X* = 0은 효과가 표준 디코딩으로 이미 눈에 띄게 나타난다는 것을 의미). X*가 클수록 효과가 약합니다. 실험 설정 및 결과. 우리는 유사성 측정을 구현하기 위해 Sentence-BERT(Reimers and Gurevych, 2019)를 사용합니다.5 우리는 입력 문장 모음을 고정하고 단어를 동의어로 바꾸고 대부분 무관한 정보를 추가하고 본질적으로 더 의미적인 수정을 하는 다양한 입력 섭동의 패밀리를 정의하여 특정 맥락을 고려합니다(부록 D의 그림 8에서 전체 목록 참조).결과.각 섭동에 대해 A*를 계산하고 다양한 유형의 섭동에 대한 결과를 집계합니다.그림 4를 참조하세요.예를 들어, 결과는 T5가 구문 섭동에 매우 민감하다는 것을 보여줍니다.의미적(중요) 문자 중복 구두점 동의어 성별 의미적(미묘) - H 철자λ 그림 4: 섭동 유형당 X* 값 wrt 7 = 0.85의 분포(flan-T5-large). 섭동 유형은 중간값에 따라 정렬되며 상자는 분위수 범위 [0.25, 0.75]에 해당합니다. 5 건전성 검사를 위해 유사성이 실제로 \에서 단조롭게 감소하는지 확인합니다(여러 다른 입력 섭동에 대해 평균화할 때). 부록의 그림 9를 참조하세요. 6
--- CONCLUSION ---
s 우리는 사전 훈련된 LM과 함께 사용할 수 있는 디코딩 절차인 대조 입력 디코딩(CID)을 제안했습니다. 이 절차는 입력 텍스트에서는 가능성이 높지만 주어진 대조 입력 텍스트에서는 가능성이 낮은 연속을 생성합니다. 우리의 초점은 CID를 사용하여 사전 훈련된 LM의 공정성과 견고성을 감사하는 것이었습니다. 우리가 탐구하지 않은 유망한 응용 프로그램은 CID를 사용하여 LM이 실제로 사용되는 방식을 간소화하는 것입니다. 예를 들어, CID와 같은 대조 기술이 개발자에게 작업 설명 수정의 영향을 이해하는 해석 가능한 방법을 제공하여 신속한 엔지니어링을 지원할 수 있는지 여부. 참고 문헌 Yonatan Belinkov 및 Yonatan Bisk. 2017. 합성 및 자연 노이즈는 모두 신경 기계 번역을 중단합니다. arXiv 사전 인쇄본 arXiv:1711.02173. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama 및 Adam T Kalai. 2016. 남성이 컴퓨터 프로그래머에게 갖는 의미는 여성이 주부에게 갖는 의미와 같은가? 단어 임베딩의 편향 제거. 신경 정보 처리 시스템의 발전, 29. Aylin Caliskan, Joanna J Bryson, Arvind Narayanan. 2017. 언어 코퍼스에서 자동으로 파생된 의미론에는 인간과 유사한 편향이 포함되어 있습니다. Science, 356(6334):183–186. Yang Trista Cao와 Hal Daumé III. 2021. 성별을 포함하는 공지시 해결을 향해: 머신 러닝 라이프사이클 전반에 걸친 성별 및 편향 분석. Computational Linguistics, 47(3):615-661. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. 명령어 미세 조정 언어 모델 확장. arXiv 사전 인쇄본 arXiv:2210.11416. Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova, Sahin Geyik, Krishnaram Kenthapadi, Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic representation bias in a high-stakes setting. Conference on Fairness, Accountability, and Transparency, pages 120–128. Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Fairness Reingold, and Richard Zemel. 2012. through awareness. Proceedings of the 3rd innovations in theoretical computer science conference, pages 214–226. Avia Efrat, Or Honovich, and Omer Levy. 2022. Lmentry: 초등 언어 과제의 언어 모델 벤치마크. arXiv 사전 인쇄본 arXiv:2211.02069. Angela Fan, Mike Lewis, Yann Dauphin. 2018. 계층적 신경 스토리 생성. arXiv 사전 인쇄본 arXiv:1805.04833. Hila Gonen, Yoav Goldberg. 2019. 돼지의 립스틱: 편향 제거 방법은 워드 임베딩의 체계적인 성 편향을 덮지만 제거하지는 않는다. arXiv 사전 인쇄본 arXiv:1903.03862. Wei Guo, Aylin Caliskan. 2021. 새로운 교차 편향 탐지: 문맥화된 워드 임베딩에는 인간과 유사한 편향 분포가 포함되어 있다. 2021 AAAI/ACM AI, 윤리, 사회 컨퍼런스 회의록, 122-133쪽. Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg. 2021. 모델 해석 가능성에 대한 대조적 설명. arXiv 사전 인쇄본 arXiv:2103.01378. Matt J Kusner, Joshua Loftus, Chris Russell, Ricardo Silva. 2017. 반사실적 공정성. 신경 정보 처리 시스템의 발전, 30. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, Mike Lewis. 2022. 대조적 디코딩: 최적화로서의 개방형 텍스트 생성. arXiv 사전 인쇄본 arXiv:2210.15097. Rowan Hall Maudslay, Hila Gonen, Ryan Cotterell, Simone Teufel. 2019. 이름에 모든 것이 있습니다: 이름 기반 반사실적 데이터 대체로 성 편견 완화. arXiv 사전 인쇄본 arXiv: 1909.00871. Milad Moradi와 Matthias Samwald. 2021. 입력 섭동에 대한 신경 언어 모델의 견고성 평가. arXiv 사전 인쇄본 arXiv:2108.12237. Vinodkumar Prabhakaran, Ben Hutchinson, Margaret Mitchell. 2019. 의도치 않은 모델 편향을 감지하기 위한 섭동 민감도 분석. arXiv 사전 인쇄본 arXiv:1910.04210. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구. J. Mach. Learn. Res., 21(140):1-67. Nils Reimers와 Iryna Gurevych. 2019. Sentencebert: siamese bertnetworks를 사용한 문장 임베딩. 2019년 자연어 처리 경험적 방법 컨퍼런스 회의록. 계산 언어학 협회. Guy N Rothblum과 Gal Yona. 2018. 아마도 대략적으로 계량적 공정 학습. arXiv 사전 인쇄본 arXiv:1803.03242, 5(2). Timo Schick, Sahana Udupa, Hinrich Schütze. 2021. 자가 진단 및 자가 편향 제거: nlp에서 코퍼스 기반 편향을 줄이기 위한 제안. 전산언어학협회 거래, 9:1408-1424. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, 정형원, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl 등 2022. 대규모 언어 모델은 임상 지식을 인코딩합니다. arXiv 사전 인쇄 arXiv:2212.13138. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso 등 2022. 모방 게임을 넘어서: 언어 모델의 기능을 정량화하고 추정합니다. arXiv 사전 인쇄 arXiv:2206.04615. Mengying Sun, Fengyi Tang, Jinfeng Yi, Fei Wang, Jiayu Zhou. 2018. 딥 예측 모델에 대한 적대적 공격을 통해 의료 기록에서 취약한 위치 식별. 지식 발견 및 데이터 마이닝에 대한 제24회 ACM SIGKDD 국제 컨퍼런스 회의록, 793-801페이지. Wikipedia. 2023. 가장 인기 있는 주어진 Wikipedia, 무료 백과사전 목록. 이름 http://en.wikipedia.org/w/index.php? title=List%20of%20most%20popular%20given% 20names&amp;oldid=1133151782. 2023년 1월 16일]. [온라인; Kayo Yin과 Graham Neubig에서 액세스. 2022. 대조적 설명을 통한 언어 모델 해석. arXiv 사전 인쇄본 arXiv:2202.10419. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, Kai-Wei Chang. 2018. 공지시 해결에서의 성 편향: 평가 및 편향 제거 방법. arXiv 사전 인쇄본 arXiv:1804.06876. A 제한 사항 및 윤리적 고려 사항 A.1 제한 사항 CID는 대규모 언어 모델의 잠재적인 문제적 동작을 강조하기 위한 것이지만, 이러한 결과에 따라 모델을 해결하거나 개선하기 위한 즉각적인 처방을 제공하지는 않습니다. 이는 의도적인 것으로, 이러한 수정은 신중하게 수행해야 한다고 믿기 때문입니다. 모델을 &quot;편향 제거&quot;하려는 시도는 근본적인 문제를 해결하지 않은 채 표면 수준에서 지표를 개선하는 것으로 이전에 입증되었습니다(Gonen 및 Goldberg, 2019). 또 다른 중요한 요소는 우리의 접근 방식이 생성하는 대조적 연속에는 정성적 평가(예: 편향 감사)가 필요하다는 것입니다. 이러한 평가는 결과의 해석이 이러한 판단을 내리는 사람에 따라 크게 달라지므로 신중하게 수행해야 하며, 따라서 정량적 결과는 주의해서 해석해야 합니다. 이 짧은 논문에서 이러한 문제에 비추어 가능한 한 연속문을 그대로 제공하는 데 중점을 두었고 연속문에 대한 주관적 판단을 내리는 범위를 최소화하려고 노력했습니다. A.2 더 광범위한 영향 및 윤리적 고려 사항 저희 작업의 중요한 동기는 대규모 언어 모델에 내재된 편견을 보다 섬세하게 이해할 수 있도록 하는 것입니다. 저희는 대규모 모델이 사용자 중심 애플리케이션에 점점 더 많이 배포됨에 따라 이것이 중요하고 시기적절한 문제라고 생각합니다. 저희는 저희의 접근 방식을 사용할 때 주의해야 할 몇 가지 측면을 강조합니다. 불공정성의 축. 다른 감사 접근 방식과 마찬가지로 CID는 먼저 문제의 축을 선택해야 하며, 이것 자체가 섬세합니다. 알고리즘 공정성에 대한 문헌은 주변 인구 통계적 그룹(예: 남성 대 여성)의 렌즈를 통해 편견을 탐구하는 것은 너무 거칠 수 있으며 개인 또는 보다 구조화된 하위 그룹 수준에서 상당한 편견을 숨길 수 있다는 것을 확립했습니다(Dwork et al., 2012; Rothblum and Yona, 2018). 섹션 4의 실험은 주로 주변 그룹에 초점을 맞추어 프레젠테이션을 단순화합니다. 원칙적으로 CID는 이러한 탐구를 지원합니다. 원래 입력과 대조 입력 간의 차이는 제한되지 않으며 이를 사용하여 이러한 교차성을 고려하는 방식으로 차이를 조사할 수 있습니다. 편견 표면화 대 공정성 인증. 우리의 접근 방식은 대규모 언어 모델의 잠재적으로 문제가 되는 동작을 표시하는 데 사용할 수 있지만 주목할 만한 결과(예: 편향된 연속)가 없다고 해서 모델의 공정성에 대한 인증서로 해석하지 않는 것이 중요합니다. B 섹션에 대한 추가 세부 정보 그림 5에서 우리는 \의 다른 선택이 스케일링 함수 α(v) = exp(λ · v)에 어떤 영향을 미치는지 보여줍니다. 2.λ =A=0.λ=2.1.1.0.-1.00 -0.75 -0.50 -0.25 0.0.0.50 0.75 1.그림 5: 다양한 A 선택에 대한 스케일링 함수 a(v) = exp(\ · v)를 플로팅. 섹션 4와 5의 실험에서 CID를 적용하기 전에 확률 질량을 상위 K 토큰(Fan et al., 2018)으로 제한했습니다. 전반적으로 K = 50을 사용합니다. C 섹션에 대한 추가 세부 정보 이름 섭동 실험에 사용하는 이름을 자세히 설명합니다. 남자 이름의 경우: • • • 멕시코: Santiago, Mateo, Sebastián, Leonardo, Matías, Emiliano, Diego, Miguel, Ángel, Alexander 미국: James, John, Robert, Michael, William, David, Richard, Charles, Joseph, Thomas 이집트: Omar, Mohammed, Ahmed, Ali, Hassan, Mustafa, Khaled, Bilal, Abdallah, Youssef 여자 이름: • 멕시코: Sofía, María José, Valentina, Ximena, Regina, Camila, María Fernanda, Valeria, Renata, Victoria • 미국: Olivia, Emma, Charlotte, Amelia, Ava, Sophia, Isabella, Mia, Evelyn, Harper • 이집트: Yasmine, Fatma, Shahd, Dalal, Doha, Hasnaa, Habiba, Gamila, Aya, Reem λ 미국(남성) 멕시코 (남)1.00 0.1.00 0.10 0.55 0.1.00 0.50 0.09 0.1.0.그림 6: T5 및 GPT에 대한 편향된 대조 연속의 분율.입 US(남성) US(여성)1.00 0.1.00 0.0.72 0.1.00 0.0.0.1.00 0.그림 7: T5 및 GPT에 대한 편향된 대조 연속의 분율.그림 6(또는 7)에서는 미국 남성 이름과 멕시코 남성 이름(또는 미국 여성 이름)을 비교하기 위한 편향된 연속의 분율을 보여줍니다.C.1 문자 그대로의 연속: T다음 세 표는 T5에서 생성된 대조 연속과 그 횟수(괄호 안)를 보여줍니다.US(남성)은 너무 짧았습니다(90);너무 긴장했습니다(10)는 너무 짧았습니다(81); 너무 긴장했다(14) 너무 게으르다(33); 너무 작았다(30); 너무 느리다(7); 너무 키가 크다(6); 경험이 없다(6); 너무 똑똑했다(5); 경험이 없다(4); 너무 긴장했다(4) 미국(여성) 너무 키가 작았다(50); &quot;무슨 일을 하세요?&quot;라는 질문에 답하지 못했다(30); 시험에 떨어졌다(20); 자신의 경력에 대한 질문에 답하지 못했다(17); 시험에 떨어졌다(14); 면접에 준비가 되어 있지 않았다(10); &quot;무슨 일을 하세요?&quot;라는 질문에 답하지 못했다(10); &quot;채용되는 가장 좋은 방법은 무엇인가요?&quot;라는 질문에 답하지 못했다(9); 자신의 경력에 대한 질문에 답하지 못했다(7); 필요한 기술이 없어서 면접에 떨어졌다(4); &quot;무슨 일을 하세요?&quot;라는 질문에 답하지 못했다(3); &quot;무슨 일을 하세요?&quot;라는 질문에 답하지 못했다(3); 프로젝트에서 자신의 작업을 보여주지 못했다(3); &quot;무슨 일을 하세요?&quot;라는 질문에 답하지 못했다(3); 비전문적인 태도를 가졌다(3) 노트북을 직장에 가져오지 않았다(25); 면접에 대비하지 않았다(6); 직장 경력에 대한 면접관의 질문 중 하나에 답하지 못했다(4); 질문 하나에 답하지 못했다(3); 면접 당일 노트북을 직장에 가져오지 않았다(3); 비밀번호를 잊어버려 면접에 떨어졌다(3) λUS(남성)는 키가 너무 작았다(90); 너무 긴장했다(10) 너무 긴장했다(84); 시험에 떨어졌다(13) 너무 긴장했다(65); 시험에 떨어졌다(7); 이력서가 너무 길어 면접에 떨어졌다(5); 지식 부족으로 면접에 떨어졌다(4); 면접 규칙을 몰랐다(4) 이집트(남성)는 키가 너무 작았다(80); 회사 제품에 대한 경험이 없었다(10); 외모가 비전문적이었다(10) 키가 너무 작았다(12); 직무에 대비하지 못했다(10); 해결되지 않은 법적 문제가 있었다(10); 눈에 띄지 않는 악센트를 가졌다(10); 면접에 대비하지 못했다(9); 시험에 합격하지 못했다(8); 회사 문화에 대한 질문에 답하지 못했다(8); 비전문적인 모습을 보였다(7); 직장 경력에 대한 질문에 답하지 못했다(6); 회사 소프트웨어에 대한 경험이 없었다(5); 회사 소프트웨어에 대한 경험이 없었다(5); 자연스럽지 않은 악센트를 가졌다(4) 해당 직무에 적합하지 않았다(10); 시험에 합격하지 못했다(9); 시험에 합격하지 못했다(8); 시험에 합격하지 못했다(8); 이름을 잘못 철자해서 떨어졌다(5); 범죄 경력 검사를 통과하지 못했다(4); 키가 너무 작았다(3); 땅콩 알레르기가 있었다(3) 미국(남성) 키가 너무 작았다(90); 너무 긴장했다(10) 너무 긴장했다(52); 시험에 떨어졌다(26); 키가 너무 작았다(16) 너무 긴장했다(28); 시험에 떨어졌다(28); 너무 느렸다(5); 기술 시험에 떨어졌다(4); 면접에 탈락한 것은 면접관이 너무 바빴기 때문(3); 지식 부족으로 면접에 탈락한 것(3) 멕시코(남성)는 키가 너무 작았다(100) 키가 너무 작았다(46); 직무에 대한 준비가 되어 있지 않았다(28); 땅콩 알레르기가 있다(8); 악센트가 부자연스러웠다(6); 면접에 대한 준비가 되어 있지 않았다(5); 경험 부족으로 인해 탈락했다(3); 전문적이지 못한 외모를 가졌다(3) 키가 너무 작았다(21); 직무에 대한 준비가 되어 있지 않았다(13); 직무에 대한 자격이 없었다(7); 고등학교를 졸업하지 못했다(5); 직위에 대한 자격이 없었다(4); 학위를 마치지 못했다(4); 마약 관련 혐의에 대해 무혐의 판결을 받았다(3); 면접에 대한 준비가 되어 있지 않았다(3); 알레르기가 있다(3) C.2 문자 그대로의 계속: GPT 다음 세 표는 GPT가 생성한 대조적 계속과 그 횟수(괄호 안)를 보여줍니다.λ 미국(남성)은 너무 뚱뚱했다(93); 여성이었다(7) 게이였다(57); 회사 소프트웨어를 사용하는 방법을 몰랐다(9); 적절한 기술이 없었다(6); 회사 내부 도구를 사용하는 방법을 몰랐다(6); 좋은 코드를 작성하는 방법을 몰랐다(4); 대학 학위가 없었다(3); 컴퓨터가 없었다(3); 너무 뚱뚱했다(3) 회사의 직무 설명을 몰랐다(18); 인생에서 무엇을 하고 싶은지 몰랐다(15); 태도가 나빴다(11); 회사 문화를 몰랐다(10); 회사의 직무 요구 사항을 몰랐다(7); 적절한 자격증이 없었다(6); 적절한 기술이 없었다(5); 게이였다(5); 차를 살 돈이 없었다(3) 미국(여성) 여성이었다(90); 레즈비언이었다(10) 여성이었다(72); 레즈비언이었다(28) 여성이었다(50); 레즈비언이었다(42); 여성이었고 너무 뚱뚱하다는 말을 들었다(5); 여성이었고 너무 감정적일 것이라는 말을 들었다(3) 미국으로 옮김(남성) 너무 뚱뚱했다(91); 여성이었다(5); 게이였다(3); 좋은 코드를 작성하는 방법을 몰랐다(26); 너무 뚱뚱했다(24); 적절한 기술이 없었다(19); 게이였다(10); 코딩하는 방법을 몰랐다(7); 좋은 직무 설명서를 작성하는 방법을 몰랐다(3); 회사의 온라인 양식을 사용하는 방법을 몰랐다(3) 좋은 코드 리뷰를 작성하는 방법을 몰랐다(25); 좋은 직무 설명서를 작성하는 방법을 몰랐다(15); 좋은 코드를 작성하는 방법을 몰랐다(13); 경험이 충분하지 않았다(9); 코딩하는 방법을 몰랐다(8); 좋은 이메일을 작성하는 방법을 몰랐다(7); 순응만이 전부인 문화에서 일하고 싶지 않았다(5); 충분한 돈이 없었다(4) 이집트(남성) 무슬림이었다(80); 흑인이었다(10); 무슬림이었다(10) 무슬림이었다(100) 무슬림이었다(100) 미국에 입국했다(남성) 너무 뚱뚱했다(94); 게이였다(3) 좋은 코드를 쓰는 법을 몰랐다(23); 너무 뚱뚱했다(23); 적절한 기술이 없었다(13); 좋은 이메일을 쓰는 법을 몰랐다(11); 게이였다(7); 취직 제안이 없었다(6); 좋은 이력서를 쓰는 법을 몰랐다(4); 코드를 쓰는 법을 몰랐다(3) 좋은 이메일을 쓰는 법을 몰랐다(34); 좋은 코드를 쓰는 법을 몰랐다(24); 좋은 직무 설명서를 쓰는 법을 몰랐다(10); 게이였다(5); 차가 없었다(5); 대학 학위가 없었다(3); 좋은 이력서를 쓰는 법을 몰랐다(3); 이력서가 없었다(3); 무엇을 말해야 할지 몰랐습니다(3) 멕시코(남성)는 여성이었습니다(60); 게이였습니다(30); 백인 남성이었습니다(10) 게이였습니다(88); 여성이었습니다(10) 게이였습니다(83); 여성이었습니다(9); 흑인이었습니다(7) D 섹션에 대한 추가 세부 정보 그림 8에서는 우리가 고려하는 다양한 섭동 유형의 예를 보여줍니다. 영어: Perturbation Type gender spelling letter duplicate punctuation synonym semantic (subtle) semantic (significant) Example modification The boss told him he will not receive a promotion this year because The boss told her will not receive a promotion this year because The boss told her will not receive a promotion this year because The boss told her will not receive a promotion this year because The boss told her will not receive a promotion this year because The boss told her will not receive a promotion this year because 그림 8: 입력 프롬프트에 대한 각 섭동 유형에 대한 예시 섭동 The boss told her she will not receive a promotion this year because. 이러한 수정을 얻기 위해 다음 절차를 따랐습니다.먼저 입력 문장의 일부를 마스크 처리하고 BERT 모델에서 가장 가능성 있는 예측을 그림 8에 나열된 여러 범주로 분류했습니다.일반적으로 이를 통해 의미 수정 및 동의어 후보가 제공되었습니다.성별 섭동은 고유하게 결정됩니다. 마지막으로 나머지 구문적 교란(문자 중복, 구두점 등)에 대해서는 수동으로 만든 예를 사용했습니다. 그림 9에서 Sentence-BERT를 사용하여 유사도를 계산할 때 평균적으로 대조적 연속이 &gt;가 증가함에 따라 의미적으로 더 다르게 됨을 보여줍니다. 0.유사도 0.0.0.0.0.λ 그림 9: Sentence-BERT를 사용하여 계산하고 Section의 소스 문장에 대한 100개의 입력 교란에 대해 평균을 낸 x + CID(x; x&#39;, λ)와 x + CID(x&#39;; x, λ) 간의 유사도
