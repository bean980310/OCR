--- INTRODUCTION ---
최근 몇 년 동안 우리는 사회와 산업 모두에서 캡처된 장면 표현의 이미지를 사실적으로 렌더링하는 데 의존하는 디지털 애플리케이션의 중요성이 엄청나게 증가하는 것을 보았습니다. 특히, 컴퓨터 게임과 영화 제작에서 가상 또는 증강 현실의 설정에 이르기까지 다양한 그래픽 애플리케이션에서 역동적인 인간 얼굴과 머리에 대한 새로운 뷰의 합성이 주목의 중심이 되었습니다. 여기서 핵심 작업은 다음과 같습니다. 얼굴 표정을 보이거나 말하는 인간 배우의 녹음이 주어지면 시간적으로 일관된 3D 표현을 재구성합니다. 이 표현은 임의의 관점과 시간 단계에서 인간 얼굴의 사실적인 재렌더링을 합성할 수 있어야 합니다. 그러나 사실적인 새로운 관점 렌더링이 가능한 3D 표현을 재구성하는 것은 동적 객체의 경우 특히 어렵습니다. 여기서 우리는 사람의 정적 상태를 재구성해야 할 뿐만 아니라 시간에 따른 동작의 모습을 동시에 캡처하고 컴팩트한 장면 표현으로 인코딩해야 합니다. 작업은 인간 얼굴의 맥락에서 훨씬 더 어려워집니다.하위 애플리케이션에서는 미세하고 충실도가 높은 세부 정보가 필요하기 때문입니다.시각적 아티팩트에 대한 허용 오차가 일반적으로 매우 낮습니다.2. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter 및 Matthias Nießner.특히 인간의 머리는 머리카락의 복잡성, 반사율 속성의 차이, 심하게 비강성적인 변형과 미세한 주름을 만드는 인간 피부의 탄력성과 같이 새로운 뷰 합성(NVS)을 극도로 어렵게 만드는 여러 가지 속성을 보입니다.정적 장면의 맥락에서 신경 광도장 표현(NeRF)[Mildenhall et al. 2020]이 설득력 있는 NVS 결과를 얻는 것을 보았습니다.이 선구적 연구의 핵심 아이디어는 체적 렌더링 공식을 재구성 손실로 활용하고 결과 광도장을 신경장 기반 표현으로 인코딩하는 것입니다.최근에는 NeRF를 확장하여 동적 장면을 표현하는 데 대한 상당한 연구 관심이 있었습니다. 일부 접근 방식은 동적으로 변화하는 장면 콘텐츠를 모델링하기 위해 변형 필드에 의존하는 반면 [Park et al. 2021a, b], 다른 접근 방식은 시간 조건화된 잠재 코드[Li et al. 2022b]를 선호하여 변형 필드를 대체하는 것을 제안합니다. 이러한 방법은 제한된 동작이 있는 짧은 시퀀스에서 설득력 있는 결과를 보여주었습니다. 그러나 복잡한 동작이 있는 인간 머리를 충실하게 재구성하는 것은 여전히 어려운 일입니다. 이 연구에서는 새롭게 설계된 다중 뷰 캡처 설정의 맥락에서 이러한 과제를 해결하는 데 중점을 두고 변형 필드의 강점과 유연한 잠재 조건화를 결합하여 동적 인간 머리의 모습을 표현하는 새로운 방법인 NeRSemble을 제안합니다. 이 접근 방식의 핵심 아이디어는 Instant NGP[Müller et al. 2022]와 유사한 다중 해상도 해시 그리드 앙상블에 잠재 피처를 저장하는 것입니다. 이는 주어진 시간 단계를 설명하기 위해 혼합됩니다. 중요한 점은 해시 그리드에서 피처를 쿼리하기 전에 변형 필드를 활용한다는 것입니다. 결과적으로 변형 필드는 장면의 모든 거친 역학을 나타내고 해시 그리드의 좌표계를 정렬하여 미세한 세부 사항과 복잡한 움직임을 모델링하는 역할을 합니다. 방법을 훈련하고 평가하기 위해 16개의 머신 비전 카메라로 73fps에서 7.1MP 비디오를 녹화하는 새로운 멀티뷰 캡처 설정을 설계합니다. 이 설정을 통해 총 3,170만 개의 개별 프레임이 있는 222개의 인간 머리 시퀀스의 새로운 데이터 세트를 캡처합니다. 이 새로 도입된 데이터 세트에서 방법을 평가하고 기존의 동적 NeRF 재구성 접근 방식보다 상당히 우수하다는 것을 보여줍니다. 데이터 세트는 해상도와 초당 프레임 수 측면에서 모든 비교 가능한 데이터 세트를 크게 능가하며 공개적으로 제공될 것입니다. 또한 인간 머리의 동적 NVS에 대한 공개 벤치마크를 호스팅하여 분야를 발전시키고 방법 간 비교 가능성을 높이는 데 도움이 될 것입니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 변형 필드와 다중 해상도 해시 인코딩 앙상블을 결합하는 NeRF 표현을 기반으로 하는 동적 머리 재구성 방법. 이를 통해 희소 카메라 어레이에서 고충실도 NVS가 가능해지고 복잡한 동작이 있는 장면을 자세히 표현할 수 있습니다. • 220명 이상의 피사체에 대한 4700개 이상의 시퀀스가 포함된 다양한 인간 머리에 대한 고 프레임 속도 및 고해상도 다중 뷰 비디오 데이터 세트. 이 데이터 세트는 공개될 예정이며 인간 머리에 대한 동적 NVS의 새로운 벤치마크를 포함합니다. 표 1. 인간 얼굴에 대한 기존의 다중 뷰 비디오 데이터 세트. 각 데이터 세트에서 공개적으로 액세스 가능한 녹화만 계산합니다. 데이터 세트 D3DFACS [2011] BP4D-Spontaneous [2014] #Subj. #Cam. 해상도 Fps1280 x1392 x인터디지털 라이트필드 [2017]2048 x4DFAB [2018]1600 xVOCASET [2019]1600 xMEAD [2020]1920 x멀티페이스 [2022]2048 x3208 xOurs 2
--- RELATED WORK ---
인간의 얼굴을 모델링하고 렌더링하는 것은 그래픽의 핵심 주제이며 컴퓨터 게임, 소셜 미디어, 통신, 가상 현실과 같은 많은 응용 프로그램에서 중요한 역할을 합니다.2.1 3D 변형 가능 모델 3D 변형 가능 모델(3DMM)은 지난 20년 동안 주요 접근 방식이었습니다.통합 메시 토폴로지를 사용하면 간단한 통계 도구를 사용하여 정체성과 표현을 표현할 수 있습니다[Blanz 및 Vetter 1999; Li et al. 2017].텍스처를 추가로 사용하면 이미 매력적인 렌더링을 생성할 수 있지만[Blanz 및 Vetter 1999; Paysan et al. 2009], 메시 기반 3DMM은 머리카락이나 세부적인 정체성별 세부 사항을 모델링하는 데 본질적으로 제한적입니다.최근에는 신경장을 사용[Xie et al. 2022]하여 위상적으로 균일한 메시에서 작업하는 제약이 완화되었습니다.이러한 모델은 머리카락을 포함하여 완전한 인간 머리를 모델링할 수 있습니다[Yenamandra et al. 2021] 및 세부 사항[Giebenhain et al. 2022]. 또 다른 작업 라인에서 Zheng et al. [2022]은 신경장과 고전적인 3DMM의 아이디어를 결합하여 단안 비디오에 맞춥니다. 2.2 신경 광휘장 저희의 작업은 세부적인 헤어스타일과 복잡한 변형을 포함하여 비디오의 매우 사실적인 렌더링을 달성하기 위해 노력합니다. 따라서 3DMM에서 일반적으로 가정하는 것과 달리 단일 다중 뷰 비디오 시퀀스를 가능한 가장 높은 수준의 세부 사항에 맞추는 데 중점을 둡니다. 신경 광휘장(NeRF)[Mildenhall et al. 2020]은 최근 NVS에서 최첨단이 되었습니다. 최초의 NeRF는 일반적으로 단일 장면에서 몇 시간 또는 며칠 동안 학습되었지만 최근 연구 발전으로 학습 시간이 몇 분으로 단축되었습니다. 예를 들어 그리드 기반 최적화[Fridovich-Keil and Yu et al. 2022; Karnewar et al. 2022; Sun et al. 2022], 텐서 분해 [Chen et al. 2022] 또는 Instant NGP [Müller et al. 2022]의 다중 해상도 폭셀 해싱. 2.3 동적 NeRF NeRF를 시간에 따라 변하는 비강체 콘텐츠로 확장하는 것은 빠른 진전을 보인 또 다른 중심 연구 주제입니다. Pumarola et al. [2020] 및 Park et al. [2021a; 2021b]는 표준 공간에서 단일 NeRF를 모델링하고 관찰된 프레임에서 역방향 변형을 명시적으로 모델링하여 장면의 비강체 콘텐츠를 설명합니다. OLD: 지침에 따라 마이크로폰 16개 카메라 7.1MP @ 73fps 글로벌 셔터 &lt; 1μs 시간 동기화 NeRSemble: 인간 머리의 다중 뷰 광도장 재구성 그림 2. 왼쪽: 맞춤형 다중 뷰 비디오 캡처 설정. 오른쪽: 녹화에서 얻은 16개 관점과 얼굴 세부 정보. 반면에 Li et al. [2022b]는 명시적 변형을 사용하지 않고 대신 NeRF를 직접 조건화하는 잠재 벡터에서 장면 상태를 인코딩합니다.Wang et al. [2022b]는 그리드 피처의 푸리에 기반 압축을 활용하여 4D 광도장을 나타냅니다.Lombardi et al. [2019]는 변형장과 함께 이미지-볼륨 생성기를 사용합니다.우리의 작업과 동시에 Song et al. [2023]은 빠른 NeRF 백본(예: TensoRF 또는 Instant NGP)을 슬라이딩 윈도우 접근 방식과 결합하여 시간적 변화를 설명합니다.Attal et al. [2023]은 4D 텐서 분해를 학습된 샘플링과 결합합니다.
--- METHOD ---
복잡한 동작에서 인간 머리에 대한 매우 사실적인 새로운 관점을 합성할 수 있습니다. 보이지 않는 관점(오른쪽)에서 렌더링한 결과는 매우 비강성적인 변형을 겪는 정적 장면 부분과 영역을 충실하게 표현합니다. 이 방법과 함께 총 222명의 피사체에서 얻은 3,170만 개의 프레임으로 구성된 고품질 다중 시점 비디오 캡처 데이터를 게시합니다. 우리는 인간 머리의 고충실도 광도 필드를 재구성하고, 시간에 따른 애니메이션을 캡처하고, 임의의 시간 단계에서 새로운 관점에서 다시 렌더링을 합성하는 데 중점을 둡니다. 이를 위해 7.1MP 해상도와 초당 73프레임으로 시간 동기화된 이미지를 기록하는 16개의 보정된 머신 비전 카메라로 구성된 새로운 다중 시점 캡처 설정을 제안합니다. 이 설정을 통해 220개 이상의 인간 머리에 대한 4,700개 이상의 고해상도, 고프레임 속도 시퀀스의 새로운 데이터 세트를 수집하여 새로운 인간 머리 재구성 벤치마크¹를 도입합니다. 기록된 시퀀스는 머리 움직임, 자연스러운 표정, 감정, 구어를 포함한 광범위한 얼굴 역학을 포괄합니다. 충실도 높은 인간의 머리를 재구성하기 위해 우리는 해시 앙상블(NeRSemble)을 사용한 동적 신경 광도 필드를 제안합니다. 우리는 변형 필드와 3D 다중 해상도 해시 인코딩 앙상블을 결합하여 장면 역학을 표현합니다. 변형 필드는 간단한 장면 움직임의 정확한 모델링을 허용하는 반면, 해시 인코딩 앙상블은 복잡한 역학을 표현하는 데 도움이 됩니다. 결과적으로 우리는 시간에 따른 동작을 포착하고 임의의 새로운 관점을 다시 렌더링하는 것을 용이하게 하는 인간 머리의 광도 필드 표현을 얻습니다. 일련의
--- EXPERIMENT ---
s에서 우리는 우리 방법의 설계 선택을 탐구하고 우리의 접근 방식이 최첨단 동적 복사장 접근 방식보다 상당한 마진으로 성능이 우수함을 보여줍니다.¹우리는 이 분야의 추가 연구를 지원하기 위해 새로운 공개 벤치마크와 함께 모든 4734개 녹음 및 기준 코드를 포함하여 수집된 모든 데이터를 공개할 것입니다.웹사이트: https://tobias-kirschstein.github.io/nersemble 저자 주소: Tobias Kirschstein, 뮌헨 공과대학교, 독일, tobias.kirschstein@tum.de; Shenhan Qian, 뮌헨 공과대학교, 독일, shenhan.qian@tum.de; Simon Giebenhain, 뮌헨 공과대학교, 독일, simon.giebenhain@tum.de; Tim Walter, 뮌헨 공과대학교, 독일, tim.michelbach@hotmail.com; Matthias Nießner, 뮌헨 공과대학교, 독일, niessner@tum.de. • CCS 개념: 컴퓨팅 방법론 체적 모델; 재구성. → 렌더링; 3D 이미징; 추가 키워드 및 구문: 신경 복사장, 동적 장면 표현, 새로운 뷰 합성, 멀티 뷰 비디오 데이터 세트, 인간 머리 서론 최근 몇 년 동안 우리는 사회와 산업 모두에서 캡처된 장면 표현의 이미지에 대한 사실적인 렌더링에 의존하는 디지털 애플리케이션의 중요성이 엄청나게 증가하는 것을 보았습니다. 특히, 컴퓨터 게임과 영화 제작에서 가상 또는 증강 현실의 설정에 이르기까지 많은 그래픽 애플리케이션에서 동적 인간 얼굴과 머리에 대한 새로운 뷰의 합성이 주목의 중심이 되었습니다. 여기서 핵심 작업은 다음과 같습니다. 얼굴 표정을 보이거나 말하는 인간 배우의 녹음이 주어지면 시간적으로 일관된 3D 표현을 재구성합니다. 이 표현은 임의의 관점과 시간 단계에서 인간 얼굴에 대한 사실적인 재렌더링을 합성할 수 있어야 합니다. 그러나 사실적인 새로운 관점 렌더링이 가능한 3D 표현을 재구성하는 것은 동적 객체에 특히 어렵습니다. 여기에서 우리는 사람의 정적 상태를 재구성해야 할 뿐만 아니라 시간에 따른 동작의 모습을 동시에 포착하여 컴팩트한 장면 표현으로 인코딩해야 합니다. 이 작업은 인간 얼굴의 맥락에서 훨씬 더 어려워집니다. 왜냐하면 다운스트림 애플리케이션에서는 미세하고 충실도가 높은 세부 정보가 필요하기 때문입니다. 여기서는 시각적 아티팩트에 대한 허용 오차가 일반적으로 매우 낮습니다. 2. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter 및 Matthias Nießner. 특히 인간의 머리는 머리카락의 복잡성, 반사율 속성의 차이, 심하게 비강성적인 변형과 미세한 주름을 만드는 인간 피부의 탄력성과 같이 새로운 관점 합성(NVS)을 극도로 어렵게 만드는 여러 가지 속성을 보입니다. 정적 장면의 맥락에서 우리는 신경 광도장 표현(NeRF) [Mildenhall et al. 2020]이 설득력 있는 NVS 결과를 얻는 것을 보았습니다. 이 획기적인 연구의 핵심 아이디어는 체적 렌더링 공식을 재구성 손실로 활용하고 신경장 기반 표현에서 결과적인 광도장을 인코딩하는 것입니다. 최근 NeRF를 확장하여 동적 장면을 표현하는 데 대한 연구 관심이 상당했습니다. 일부 접근 방식은 동적으로 변화하는 장면 콘텐츠를 모델링하기 위해 변형 필드에 의존하는 반면 [Park et al. 2021a, b], 다른 접근 방식은 시간 조건 잠재 코드를 선호하여 변형 필드를 대체하는 것을 제안합니다 [Li et al. 2022b]. 이러한 방법은 제한된 동작이 있는 짧은 시퀀스에서 설득력 있는 결과를 보여주었습니다. 그러나 복잡한 동작이 있는 인간 머리를 충실하게 재구성하는 것은 여전히 어려운 일입니다. 이 연구에서는 새롭게 설계된 다중 뷰 캡처 설정의 맥락에서 이러한 과제를 해결하는 데 중점을 두고 변형 필드의 장점과 유연한 잠재 조건화를 결합하여 동적 인간 머리의 모양을 표현하는 새로운 방법인 NeRSemble을 제안합니다. 이 접근 방식의 핵심 아이디어는 Instant NGP와 유사한 다중 해상도 해시 그리드 앙상블에 잠재 피처를 저장하는 것입니다 [Müller et al. 2022], 주어진 시간 단계를 설명하기 위해 혼합됩니다. 중요한 점은 해시 그리드에서 피처를 쿼리하기 전에 변형 필드를 활용한다는 것입니다. 결과적으로 변형 필드는 장면의 모든 거친 역학을 나타내고 해시 그리드의 좌표계를 정렬하여 미세한 세부 사항과 복잡한 움직임을 모델링하는 역할을 합니다. 방법을 훈련하고 평가하기 위해 16개의 머신 비전 카메라로 73fps에서 7.1MP 비디오를 녹화하는 새로운 다중 뷰 캡처 설정을 설계합니다. 이 설정을 통해 총 3,170만 개의 개별 프레임이 있는 222개의 인간 머리 시퀀스의 새로운 데이터 세트를 캡처합니다. 이 새로 도입된 데이터 세트에서 방법을 평가하고 기존의 동적 NeRF 재구성 접근 방식보다 상당히 우수하다는 것을 보여줍니다. 저희 데이터 세트는 해상도와 초당 프레임 수 측면에서 모든 비교 가능한 데이터 세트를 크게 능가하며 공개적으로 제공될 것입니다. 또한 인간 머리의 동적 NVS에 대한 공개 벤치마크를 호스팅하여 분야를 발전시키고 방법 간 비교성을 높이는 데 도움이 될 것입니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 변형 필드와 다중 해상도 해시 인코딩 앙상블을 결합하는 NeRF 표현을 기반으로 하는 동적 머리 재구성 방법. 이를 통해 희소 카메라 어레이에서 고충실도 NVS가 가능해지고 복잡한 동작이 있는 장면을 자세히 표현할 수 있습니다. • 220명 이상의 피사체에 대한 4700개 이상의 시퀀스가 포함된 다양한 인간 머리에 대한 고 프레임 속도 및 고해상도 다중 뷰 비디오 데이터 세트. 이 데이터 세트는 공개될 예정이며 인간 머리에 대한 동적 NVS의 새로운 벤치마크를 포함합니다. 표 1. 인간 얼굴에 대한 기존의 다중 뷰 비디오 데이터 세트. 각 데이터 세트에서 공개적으로 접근 가능한 녹화만 계산합니다. 데이터 세트 D3DFACS [2011] BP4D-Spontaneous [2014] #Subj. #Cam. 해상도 Fps1280 x1392 xInterdigital Light-Field [2017]2048 x4DFAB [2018]1600 xVOCASET [2019]1600 xMEAD [2020]1920 xMultiFace [2022]2048 x3208 xOurs 2 관련 작업 인간 얼굴 모델링 및 렌더링은 그래픽의 핵심 주제이며 컴퓨터 게임, 소셜 미디어, 통신, 가상 현실과 같은 많은 응용 프로그램에서 중요한 역할을 합니다.2.1 3D 변형 가능 모델 3D 변형 가능 모델(3DMM)은 지난 20년 동안 기본적인 접근 방식이었습니다.통합 메시 토폴로지를 사용하면 간단한 통계 도구를 사용하여 정체성과 표현을 표현할 수 있습니다[Blanz 및 Vetter 1999; Li et al. 2017].텍스처를 추가로 사용하면 이미 매력적인 렌더링을 제작할 수 있습니다[Blanz 및 Vetter 1999; 영어: Paysan et al. 2009], 하지만 메시 기반 3DMM은 본질적으로 머리카락이나 정교한 신원별 세부 사항을 모델링하는 데 제한이 있습니다.최근에는 신경장[Xie et al. 2022]을 사용하여 위상적으로 균일한 메시 작업의 제약이 완화되었습니다.이러한 모델은 머리카락[Yenamandra et al. 2021]과 정교한 세부 사항[Giebenhain et al. 2022]을 포함한 완전한 인간 머리를 모델링할 수 있습니다.또 다른 작업 라인에서 Zheng et al.[2022]은 신경장과 기존 3DMM의 아이디어를 결합하여 단안 비디오에 맞춥니다.2.2 신경 광채장 우리의 작업은 세부적인 헤어스타일과 복잡한 변형을 포함하여 비디오의 매우 사실적인 렌더링을 달성하기 위해 노력합니다.따라서 우리는 3DMM에서 만든 일반적인 가정에서 벗어나 가능한 가장 높은 수준의 세부 사항에 단일 다중 뷰 비디오 시퀀스를 맞추는 데 중점을 둡니다.신경 광채장(NeRF)[Mildenhall et al. 2020]은 최근 NVS에서 최첨단 기술이 되었습니다. 최초의 NeRF는 일반적으로 단일 장면에서 몇 시간 또는 며칠 동안 학습되었지만 최근 연구 발전으로 학습 시간이 몇 분으로 단축되었습니다. 예를 들어, 이는 그리드 기반 최적화[Fridovich-Keil 및 Yu 등 2022; Karnewar 등 2022; Sun 등 2022], 텐서 분해[Chen 등 2022] 또는 Instant NGP의[Müller 등 2022] 다중 해상도 폭셀 해싱을 통해 달성할 수 있습니다. 2.3 동적 NeRF NeRF를 시간에 따라 변하는 비강체 콘텐츠로 확장하는 것은 빠른 진전을 보인 또 다른 핵심 연구 주제입니다. Pumarola 등 [2020] 및 Park 등 [2021a; 2021b]는 표준 공간에서 단일 NeRF를 모델링하고 관찰된 프레임에서 역방향 변형을 명시적으로 모델링하여 장면의 비강체 콘텐츠를 설명합니다.이전: 지침 마이크 16개 카메라 7.1MP @ 73fps 글로벌 셔터 &lt; 1μs 시간 동기화 NeRSemble: 인간 머리의 다중 시점 광도장 재구성그림 2. 왼쪽: 맞춤형 다중 시점 비디오 캡처 설정.오른쪽: 녹음에서 얻은 16개 시점과 얼굴 세부 정보.반면에 Li et al.[2022b]는 명시적 변형을 사용하지 않고 대신 NeRF에 직접 조건을 부여하는 잠재 벡터로 장면 상태를 인코딩합니다.Wang et al.[2022b]는 그리드 피처의 푸리에 기반 압축을 활용하여 4D 광도장을 나타냅니다.Lombardi et al.[2019]는 변형장과 함께 이미지-볼륨 생성기를 사용합니다.우리의 작업과 동시에 Song et al. [2023]은 시간적 변화를 설명하기 위해 빠른 NeRF 백본, 즉 TensoRF 또는 Instant NGP를 슬라이딩 윈도우 접근 방식과 결합합니다.Attal et al. [2023]은 빠른 동적 NVS를 위해 4D 텐서 분해를 학습된 샘플링 방법과 결합합니다.이러한 작업과 달리 우리는 변형 필드와 함께 해시 기반 분해를 제안합니다.2.4 비디오 뷰 합성 NeRF 외에도 광도 필드 백본에 의존하지 않는 비디오 뷰 합성을 위한 다른 방법도 있습니다.초기 작업에서 Zitnick et al. [2004]은 기하학 지원 이미지 기반 렌더링을 사용하여 동적 장면의 새로운 뷰를 렌더링합니다.최근에 Broxton et al. [2020]은 빠른 렌더링 및 스트리밍을 위해 계층화된 메시 표현으로 변환된 다중 구 이미지를 구성하여 자유 시점 비디오를 얻습니다.Collet et al. [2015]은 다른 접근 방식을 추구하며 다중 뷰 스테레오 시스템으로 동적 성능의 추적된 메시를 얻습니다. 이러한 메시 기반 방법은 더 큰 장면에 대해 매력적인 비디오 뷰 합성을 생성하는 반면, NeRF의 강점은 머리카락과 같은 섬세하고 복잡한 세부 사항의 사진처럼 사실적인 재구성에 있습니다.2.5 얼굴을 위한 NeRF 여러 연구에서 인간의 머리 영역에 특화된 방법을 제안합니다.특히, Gafni et al.[2021]은 NeRF를 조건화하기 위해 피팅된 3DMM 매개변수를 사용하고 Athar et al.[2022]은 이 접근 방식을 확장하여 3DMM의 기하학에서 파생된 명시적 변형을 모델링합니다.더 최근에 Zielonka et al.[2023]은 Instant NGP와 함께 추적된 3DMM을 활용하여 재구성 속도와 실시간 렌더링에 초점을 맞춘 유사한 접근 방식을 제안합니다.Wang et al.[2022a]은 신원 매개변수를 제어하는 생성 NeRF를 제안합니다.Hong et al.[2022]은 추가 표현 매개변수를 사용하여 유사한 접근 방식을 추구합니다.Lombardi et al. [2021]은 3DMM 표면에 느슨하게 조작된 폭셀 그리드에 색상 방출 값을 명시적으로 저장하여 신경 렌더링에 대한 고도로 최적화된 접근 방식을 제안합니다. 이 작업에서 우리는 FLAME과 같은 거친 기하 프록시를 사용하여 픽셀 정확도의 새로운 뷰 합성을 달성하는 것이 어렵다고 주장하기 때문에 템플릿 없는 접근 방식을 제안합니다 [Li et al. 2017]. 우리의 방법과 유사하게 Gao et al. [2022]은 최근 여러 해시 그리드의 피처를 혼합하는 것을 제안했습니다. 그들의 접근 방식은 추적된 3DMM의 매개변수를 사용하는 반면, NeRSemble은 혼합 가중치와 나머지 모델 매개변수를 공동으로 최적화합니다. 또한 해시 그리드를 혼합하기 전에 변형 필드를 포함하면 상당한 개선이 있음을 보여줍니다. 인간 얼굴의 다중 뷰 비디오 데이터 세트 16개의 머신 비전 카메라로 촬영한 222명의 피험자에 대한 4734개의 다중 뷰 비디오 녹화로 구성된 새로운 데이터 세트를 소개합니다. 우리의 앞을 향한 캡처 장비는 좌우로 93°, 상하로 32°의 시야를 커버합니다. 인간의 얼굴 움직임은 복잡하고 지각되는 감정은 미묘한 차이에 크게 영향을 받을 수 있기 때문에 그림 2에서와 같이 개별 머리카락 가닥과 주름 수준까지 전체 얼굴을 포괄하는 7.1메가픽셀의 고해상도를 사용합니다. 또한 초당 73프레임으로 녹화하여 미묘한 움직임이 놓치지 않도록 합니다. 종합하면, 우리의 데이터 세트는 많은 피사체의 고해상도, 고프레임 속도 녹화의 고유한 조합으로, 현재 다른 어떤 데이터 세트와도 비교할 수 없습니다(표 1 참조). 3.1 수집 각 녹화 세션은 25개의 짧은 시퀀스로 구성되어 사람당 약 3분 분량의 다중 시점 비디오 영상이 생성됩니다. 참가자들에게 다양한 얼굴 표정을 4. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, Matthias Nießner 표 2. 다중 시점 비디오 얼굴 데이터 세트의 통계. #참가자 222명(157m 65f) #시퀀스 #프레임 3,170만 개 총 시간 7시간 30분 디스크 공간 203TB #참가자 연령 및 성별 분포 남성 여성16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 인종별 연령 분포75 100 125 #참가자 175 200 백인(65.5%) 라틴계(2.2%) 중동계(8.5%) 아시아인(17.0%) 인도인(6.3%) 흑인(0.4%) 그림 3. 데이터 세트의 참가자 통계. 기록된 시퀀스에는 두 성별 모두에서 다양한 연령대와 인종이 포함되어 있어 동작의 다양성을 극대화합니다. 구체적으로, 우리의 캡처 스크립트는 다양한 얼굴 근육 그룹을 포함하는 9개의 표정 시퀀스, 빠른 움직임이 있는 1개의 헤어 시퀀스, 4개의 감정 시퀀스, 오디오가 있는 10개의 문장, 그리고 피사체가 임의의 얼굴 변형과 머리 움직임을 자유롭게 수행할 수 있는 1개의 긴 시퀀스로 구성되어 있습니다. 고품질 비디오 녹화를 얻기 위해 3ms의 셔터 속도를 사용하여 모션 블러를 피하면서 빠른 움직임을 캡처할 수 있습니다. 또한 작은 렌즈 조리개를 사용하여 얼굴 영역 전체에서 선명한 이미지를 얻습니다. 이 조합은 고품질 캡처를 제공하지만 카메라 센서에서 입사광의 양을 줄여 8개의 강력한 LED 조명 패널로 장면을 조명해야 합니다. 또한 조명에 확산판을 사용하여 피부의 반사광을 줄입니다. 또한 카메라는 정확한 시간 동기화를 위해 정밀 시간 프로토콜(PTP)을 사용합니다. 동기화된 시계는 마이크로초 미만의 정확도를 가지므로 효과적으로 동시에 캡처되는 비디오 프레임이 생성됩니다. 마지막으로 색상 검사기를 사용하여 각 카메라의 화이트 밸런싱 요소와 감마 매개변수를 보정합니다. 결과 비디오 녹화는 관점 전체에서 일관된 색상을 가지며 그림 2에 표시된 것처럼 미세한 디테일을 포착합니다.3. 처리 우리는 번들 조정 최적화 절차와 함께 미세 체커보드를 사용하여 개별 외부 및 공유 내부 카메라 매트릭스를 추정합니다.이를 통해 정확한 추정 카메라 포즈가 도출되며, 이는 합성 설정에서 밀리미터 미만 범위에 있음을 확인했습니다.또한 녹화 배경은 녹화 전에 캡처한 흰색 벽입니다.이러한 빈 배경에서 나중에 BackgroundMatting v2 [Lin et al. 2021]를 사용하여 이미지 매팅 방법을 통해 각 프레임에 대한 고품질 전경 분할 맵을 얻을 수 있습니다.그림 4. 데이터 세트의 구조.모든 참가자에게 동일한 일련의 표정을 수행하도록 요청합니다.3. 벤치마크 우리의 데이터 세트를 사용하면 다중 뷰 비디오에서 사실적인 인간 머리 재구성을 연구할 수 있으며, 이것이 이 작업의 목표입니다. 또한, 캡처된 데이터는 인간의 머리에 대한 일반화, 몰입형 화상 회의, VR 지원 아바타 렌더링, 미세 표정 연구, 재연, 애니메이션 등과 같이 NVS를 훨씬 넘어서는 사용 사례를 허용합니다.따라서 우리는 전체 데이터 세트를 학계에 공개할 계획입니다.또한, 우리는 데이터 세트에서 대표적인 녹음을 선택하여 인간 얼굴에 대한 NVS 벤치마크를 컴파일할 것입니다.이러한 노력이 방법 간 비교를 촉진하고 궁극적으로 고충실도 인간 머리 재구성에 대한 연구를 발전시키기를 바랍니다.3.데이터 개인 정보 보호 캡처된 데이터의 민감성으로 인해 데이터 세트의 모든 참여자는 GDPR 요구 사항을 준수하는 계약서에 서명했습니다.GDPR 준수에는 모든 참여자가 자신의 데이터를 적시에 삭제하도록 요청할 권리가 포함됩니다.우리는 데이터 세트 배포 시 이러한 권리를 시행할 것입니다.4 해시 앙상블을 사용한 동적 너프 우리의 목표는 복잡한 비강체 변형을 겪는 인간 머리에 대한 매우 사실적인 NVS를 허용하는 시공간적 표현을 찾는 것입니다. 이를 위해 변형 필드와 4D 장면 볼륨을 시간 차원에 따라 3D 특징 볼륨 앙상블로 분해하여 장면의 역학을 재구성하는 방법을 제안합니다(그림 5 참조).x ωτ NeRSemble: 인간 머리의 다중 시점 광도장 재구성 Σ MLP base f base ft d (c) 특징 블렌딩 D x&#39; ☐ 8888 HH₁ (d) 광도장H☐ ☐ 日☐ ☐ ☐ MLP 색상 C HN Π ར。 FHEB(a) 변형 필드 (b) 해시 앙상블 그림 5. 방법 개요.NeRSemble은 볼륨 렌더링(왼쪽)을 사용하여 동적 NVS에 대한 시공간 광도장을 나타냅니다.오른쪽에서는 NeRSemble이 시간 t에서 레이의 점 x에 대한 밀도 σ(x)와 색상 값 c(x, d)를 얻는 방법을 보여줍니다. (a) x 지점에서 변형 코드가 주어지면 표준 공간에서 x&#39; = D(x, wt)로 워핑됩니다.(b) 결과 지점은 앙상블의 i번째 해시 그리드에서 피처 H₂(x&#39;)를 쿼리하는 데 사용됩니다.(c) 결과 피처는 가중치 ẞt를 사용하여 블렌딩됩니다. wt와 ẞt는 모두 시간적 변화를 설명하는 데 기여합니다.(d) 두 개의 작은 MLP로 구성된 효율적인 렌더링 헤드를 사용하여 블렌딩된 피처에서 밀도 σ(x)와 뷰 종속 색상 c(x, d)를 예측합니다.4. 예비 단계: 신경 광도 필드 우리의 작업은 밀도 필드 σ(x)와 뷰 종속 색상 필드 c(x, d)를 통한 볼륨 렌더링을 활용하는 신경 광도 필드(NeRF) [Mildenhall et al. 2020]의 최근 성공을 바탕으로 구축됩니다. 광선 r(t) = 0 + rd가 주어지면 색상 값 Tf C(r) = S&quot; T(t)o(r(t)) c(r(7), d) dɩ, W(T) (1)은 광선을 따라 가까운 평면 ™에서 먼 평면 Tf로 적분하여 얻습니다. 여기서 T(T) = exp (−√ σ(r(s)) ds)는 T까지의 누적 투과율을 나타냅니다. 최적화의 목표는 결과 광도장을 인코딩하는 다층 퍼셉트론(MLP)의 최적 매개변수를 구하는 것입니다. 최근 순수한 복셀 그리드[Fridovich-Keil 및 Yu et al. 2022]와 MLP가 있는 명시적 그리드의 조합[Müller et al. 2022]이 광도장 표현을 위한 효과적인 대안임이 입증되었습니다. 인스턴트 NGP. 우리의 방법은 인스턴트 NGP의 복셀 해싱 방식에 의존합니다. [Müller et al. 2022]는 두 개의 작은 MLP와 함께 다중 해상도 기능 f(x)를 사용하여 NeRF의 3D 필드를 표현합니다. [σ(x), fbase(x)] = MLP base(f(x)) c(x, d) = MLP color(fbase(x), d). (2) (3) 중요한 점은 기능이 다중 해상도 해시 그리드 H, st f(x) = H(x)에 저장된다는 것입니다. 해시 그리드 H는 3D 장면 볼륨을 인코딩하는 메모리 효율적인 방법을 제공하여 작은 MLP가 가장 복잡한 장면도 표현할 수 있을 만큼 강력합니다. 4. 다중 해상도 해시 앙상블 N i=1&#39; 동적 장면에 대한 우리의 표현은 고전적인 블렌드 모양에서 영감을 받았습니다. [Blanz et al. 2003]. 시간 t에서 장면의 모든 상태는 다중 해상도 해시 그리드 {H;}\\~₁에서 추출한 피처 벡터의 조합으로 표현될 수 있다고 가정합니다.이를 해시 그리드 앙상블이라고 합니다.시간 t에서 혼합된 광도장을 얻기 위해 해시 앙상블, 공유 MLP 기반 및 MLP 색상과 함께 최적화된 혼합 가중치 ẞ를 사용하여 해당 피처 N ft(x) = Σbt,iHi (x), i=(4)의 선형 조합으로 공식화합니다.이 혼합 작업은 혼합이 피처 공간에서 발생하므로 모델이 복잡한 움직임을 표현할 수 있게 합니다.그런 다음 혼합된 피처는 MLPb 및 MLP 색상으로 디코딩됩니다.4.3 피처 기반 공간 정렬 해시 그리드 피처의 혼합은 앙상블의 모든 개별 요소가 공유 좌표계에서 작동하는 경우 가장 효과적입니다.예를 들어, 기존 혼합 모양은 메시 토폴로지의 정점 순서에 의해 주어진 완벽한 대응 관계에서 작동합니다. 이러한 구조가 없는 피처를 혼합하기 때문에 Park et al. [2021a]에 따라 좌표 기반 MLP로 표현되는 SE(3) 필드를 사용하여 변형을 명시적으로 모델링합니다.더 구체적으로, 우리의 변형 필드 D: R³× Rddef → R³, (x, wt) → x&#39; (5)는 현재 표현식을 설명하는 조건 코드 wt가 주어진 경우 관찰 공간의 점 x를 표준 공간에서 해당 점 x&#39;로 매핑합니다.그런 다음 변형 필드는 시간 단계에 걸쳐 해당 점을 찾아 공유 표준 공간에 매핑합니다.6. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter 및 Matthias Nießner 이러한 학습된 대응 관계를 사용하여 표준 공간에서 작동하도록 방정식 4를 수정합니다.N f(t) = Σ þt,iH; (D (x, wt)) .i= 표면 앞의 빈 공간을 조각합니다.여기서 e는 URF에서와 같이 훈련하는 동안 지수적으로 감소합니다. 둘째, (6) Lnear = Er~Rd 이렇게 하면 두 개의 다른 시간 단계에서 관찰된 동일한 이동 지점의 특징을 혼합하기가 더 쉬워집니다.4.4 워밍업 단계 이 조합을 사용하면 해시 앙상블과 변형이 얼굴의 역학을 설명하기 위해 경쟁합니다.따라서 최적화는 D가 의미 있는 변형을 제공하지 않는 국소 최소값으로 이어질 가능성이 높습니다.따라서 D가 관찰된 공간과 표준 공간 간의 의미 있는 대응 관계를 학습하도록 장려하기 위해 최적화 절차에서 워밍업 단계를 제안합니다.최적화의 첫 번째 Einit 에포크 동안 모델이 본질적으로 변형 가능한 NeRF를 모방하도록 해시 그리드를 하나만 제외하고 모두 비활성화합니다.이 단계에서 변형 코드 wt와 함께 변형 필드 D는 동적 동작을 설명하는 유일한 수단입니다.따라서 D는 방해받지 않고 의미 있는 변형을 학습할 수 있으며, 이는 나중에 해시 테이블 특징을 효과적으로 혼합하는 데 필수적입니다.워밍업 후, 변형 필드와 함께 첫 번째 해시 테이블은 장면의 저주파 역학을 설명할 수 있습니다. 우리는 다음 Etrans 에포크 동안 남은 모든 해시 테이블을 최적화에 계속 추가합니다. 이렇게 연속적으로 삽입된 테이블을 통해 D로 표현할 수 없는 미세한 동작과 세부 사항을 표현할 수 있습니다. 원활한 전환을 보장하기 위해 블렌드 가중치(7)를 조정합니다. 여기서 i는 해시 앙상블을 인덱싱하고, a;(s)는 Park et al. [2021a]에서 도입한 윈도잉 함수이며, s는 에포크 Einit과 Etrans+Einit 사이에서 1에서 N으로 선형적으로 증가하도록 예약되어 있습니다. 가장 중요한 것은 전체 최적화에서 α1(s) = 1이므로 첫 번째 해시 테이블이 항상 활성화됩니다. Bt,i(s)* = αi(s)ẞt,i (Vi Є {1, ..., N}), 4.5 깊이 감독 다중 뷰 데이터 세트는 기존 방법을 통해 깊이 맵을 계산하는 기능을 제공하므로 이 작업에서 추가 깊이 감독의 유용성도 연구합니다. 광선의 깊이 zgt(r)이 주어지면 깊이 손실을 Ldepth = Er-Rd |로 계산합니다. [(z(r) — 28° (r)²]. . 여기서 광선 r의 예상 깊이는 z(r) = √rw(t) · τdг입니다. 깊이 관측은 실제로 불완전하므로 깊이 손실은 깊이가 알려진 광선 r = Rd에 대해서만 계산됩니다. 또한 Urban Radiance Fields(URF) [Rematas et al. 2022]의 두 개의 시선 사전 확률을 채택하여 깊이 제약 조건을 더욱 활용합니다. 먼저 Lempty = Er~Rd z(r)-e | √ ²(r) * w (r)²dr (9) Tn z(r)+e -N| | Score (w (7) — N (T | (r), (3) ³))² dr] (z(r)-e (10)은 깊이 관측 주변 지역의 체적 밀도가 좁아지는 가우시안 분포를 따르도록 합니다. 깊이와 함께 이 세 개의 사전 확률은 다음과 같은 깊이 감독을 형성합니다. 재구성의 기하학적 충실도를 개선하는 것을 목표로 함.4. 배경 제거 우리는 모델이 배경의 일부를 재구성하지 못하도록 연속 값의 알파 맵 M(r)을 사용합니다.우리는 배경 픽셀에 부딪히는 광선의 밀도에 페널티를 주는 희소성 강제 L1 손실을 사용합니다: Lmask = Br-Rb |||(1 - T(ty)) – M(r)|||] · (11) 여기서 T(Tf)는 광선 r의 총 투과율이고 M(r)은 미리 계산된 알파 맵의 해당 알파 값입니다.4. 최적화 목표 최종 손실은 다음 항으로 구성됩니다: L = Lrgb + Lmask + Ldepth + Lnear + Lempty + Ldist 깊이 감독 (12) 여기서 Lrgb는 전경 광선에서만 계산하는 표준 MSE 색상 손실입니다.또한 낮은 밀도의 고립된 섬에 페널티를 주는 왜곡 손실 Ldist를 추가합니다[Barron et al. 2022]. 우리의 장면은 대략 볼록한 인간의 머리로만 구성되어 있으므로, 우리는 중심을 가리키는 무작위 광선에 대해 Ldist를 추가로 계산합니다. 이것은 종종 우리 시나리오에서 가려지는 머리 뒤의 공간으로 항의 정규화 효과를 확장합니다. 4. 마지막으로, 우리는 각 손실 항에 해당 가중치를 부여합니다: Adepth, Adist, Anear, empty = 1e-4 및 mask = 1e-2. 텐서 분해와의 동적 장면 표현 관계에 대한 논의. 방정식 4는 TensoRF에 도입된 벡터 행렬 분해와 유사한 4D 텐서 분해의 특수한 경우로 해석될 수 있습니다[Chen et al. 2022]. 시공간 텐서 T = RDTxDxxDyxDz가 동적 장면을 나타내면 4개의 벡터 텐서 외적의 합으로 분해될 수 있습니다.T≈ N ΣΣ ο Μαία), aЄA i=(13) 여기서 o는 벡터 텐서 외적을 나타내고, A = {X, Y, Z, T}는 축 인덱스 집합이고, va € RƊa는 벡터이고 M^\{a}는 3D 텐서입니다(예: MAª\{X} ЄRDTXDYxDz). 우리 방법의 방정식 4는 방정식 13의 특수한 경우로 볼 수 있으며, 여기서는 a = T에 대한 항만 사용됩니다. A\{a} 피처를 밀집 그리드 M에 저장하는 대신 메모리 효율적인 해시 테이블 표현 M^A^\\ {T} = H를 사용하고 벡터 v.는 블렌드 가중치 v₁t = ẞt,i에 해당합니다. NeRSemble: 인간 머리의 다중 뷰 광도장 재구성 최종 방법은 Hi를 쿼리하기 전에 변형장 D를 사용하여 이 텐서 분해 관점에서 벗어납니다(식 6 참조). 이는 변형장으로 동작의 일부를 설명함으로써 해시 테이블의 공간적 특징을 시간 단계에 걸쳐 효과적으로 정렬합니다. 4D 텐서 분해를 달성하는 또 다른 방법은 Attal et al. [2023]; Cao and Johnson [2023]; Fridovich-Keil et al. [2023]의 동시 작업에서 제시되며, 이들은 식 13과 같이 1D와 3D 텐서 간의 4개의 외적 대신 6개의 가능한 2D 특징 평면의 특징을 결합합니다. HyperNeRF와의 관계. HyperNeRF [Park et al. 2021b]는 변형장으로 모델링할 수 없는 위상 문제를 해결하기 위해 표준 공간 NeRF에 소위 주변 차원을 추가합니다. 연속적인 주변 차원을 추가하는 대신, 우리 방법은 여러 해시 그리드로 표준 공간을 모델링하여 본질적으로 비슷한 목적을 달성하는 불연속적인 주변 차원을 도입합니다.실험 결과 우리는 얼굴과 머리 움직임의 다양한 측면에 초점을 맞춘 데이터 세트의 10가지 다양한 시퀀스에 대한 다중 뷰 비디오 녹화에서 새로운 뷰 합성(NVS) 작업에서 우리 방법을 평가합니다.검증 시퀀스에는 머리 회전, 웃음, 눈 깜빡임, 말하기, 머리카락 흔들기, 다양한 입 움직임과 하나의 자유로운 표정 시퀀스가 포함됩니다.모든 비디오는 73fps에서 300-500프레임으로 구성됩니다.사용 가능한 16개 관점 중 12개를 입력으로 선택하고 나머지 4개에서 NVS 작업을 평가합니다.선택된 평가 뷰는 카메라 설정에 균등하게 분산되어 극단적인 시야각이 존재하기 때문에 어려운 평가 프로토콜이 발생합니다(그림 7 참조).5.1 데이터 준비실험을 실행하기 전에 데이터 세트의 제어된 특성을 활용하여 이미지 입력에서 동적 3D 장면을 재구성합니다. 구체적으로, 우리는 다음의 전처리 단계를 수행합니다: 깊이 맵 생성. 우리는 표준 COLMAP 파이프라인을 사용하여 12개의 트레이닝 뷰 각각에 대한 깊이 맵을 얻습니다[Schönberger and Frahm 2016; Schönberger et al. 2016]. 노이즈가 있는 깊이 측정을 제거하기 위해 카메라보다 적은 수의 카메라에서 관찰된 깊이 값을 버립니다. 배경 매팅. 우리는 Background Matting v2[Lin et al. 2021]를 사용하여 캡처된 프레임과 해당 배경 이미지가 주어졌을 때 알파 맵을 얻습니다. 최상의 품질을 보장하기 위해 ResNet101[He et al. 2016] 버전을 사용하고 정제 단계에서 오류 임계값을 0.01로 설정합니다. 이미지 다운샘플링. 모든 실험에서 우리는 이미지를 2배로 다운샘플링하여 모든 방법에 충분한 1604 × 1100픽셀로 만듭니다. 일시적으로 우리는 다운샘플링하지 않고 모든 실험을 전체 73fps에서 수행합니다. 색상 보정. 카메라의 색상 보정에도 불구하고 뷰 간에 밝기에 약간의 차이가 있을 수 있습니다.이를 해결하기 위해 먼저 얼굴 분할 마스크[Yu et al. 2018]를 사용하여 얼굴, 몸통 및 머리카락 영역에서 픽셀 값을 샘플링합니다.그런 다음 최적 전송을 사용하여 아핀 색상 변환 행렬을 풀어서 얻은 색상 분포를 뷰 간에 정렬합니다[Flamary et al. 2021].5.플로터 제거 그리드 기반 장면 표현은 일반적으로 순수 MLP 아키텍처의 유도된 부드러움 사전이 부족합니다.결과적으로 다시 렌더링할 때 시각적 품질을 손상시키는 작은 플로터를 생성하는 경향이 있습니다.해시 앙상블은 Instant NGP를 기반으로 하기 때문에 이러한 경향을 물려받습니다.이를 해결하기 위해 각 시퀀스에 대해 꼭 맞는 축 정렬 경계 상자를 지정하고 그 안에서 광도 필드만 재구성합니다. NeRSemble은 모든 베이스라인에 사용할 수 있는 타이트한 장면 상자 피팅 외에도 다음 두 가지 기술을 사용하여 플로터를 억제합니다.이는 섹션 5.7에서 제거됩니다.뷰 프러스텀 컬링. 2개 미만의 열차 카메라에서 볼 수 있고 따라서 플로터를 생성할 가능성이 특히 높은 공간의 영역을 제외합니다.이러한 영역은 학습이나 추론 중에 쿼리되지 않습니다.점유 그리드 필터링.추론 전에 Instant NGP 백본이 학습 중에 추적하는 밀도 그리드에 저역 통과 필터를 적용하고 가장 큰 연결 구성 요소 내에서만 렌더링하여 밀도의 작은 고립된 섬을 효과적으로 버립니다.5.3 구현 세부 정보 우리는 Nerfstudio [Tancik* et al. 2022] 프레임워크 내에서 PyTorch [Paszke et al. 2019]에서 방법을 구현합니다.이 프레임워크는 Instant NGP의 NerfAcc [Li et al. 2022c] 구현을 사용합니다. 우리는 Einit = Etrans = 40k의 워밍업 일정을 사용하여 모든 모델을 300k 반복으로 학습하는데, 이는 단일 Nvidia RTX A6000에서 약 1일이 걸립니다. 1604 × 1100픽셀의 해상도에서 단일 프레임을 추론하는 데 약 25초가 걸립니다. 우리는 모든 모델 구성 요소에 대해 le¯³의 학습률을 사용하는데, 이는 20k 반복마다 0.8배로 감소합니다. 변형 필드 D의 경우, 워밍업 단계 이후에 학습률이 충분히 낮아지도록 대신 0.5의 계수를 사용합니다. 또한, Müller et al. [2022]의 기본 하이퍼파라미터로 구성된 N = 32 해시 테이블을 사용합니다. 변형 필드 D의 경우 [Park et al. 2021a]의 SE(3) 필드의 기본 구성과 학습 가능한 변형 코드 wt에 대해 128차원을 사용합니다. 우리의 블렌드 가중치 B = RN은 해시 테이블당 하나의 가중치를 갖습니다.5. 기준선 E 우리는 우리의 방법을 동적 장면의 NVS를 위한 여러 최첨단 방법과 비교합니다.특히, 우리는 다음 방법과 비교합니다: 5.4.1 동적 NeRF.Nerfies[Park et al. 2021a]는 변형 가능한 NeRF를 대표하는 역할을 합니다.우리는 HyperNeRF와 동일한 구현을 사용하지만 주변 차원은 없습니다.HyperNeRF[Park et al. 2021b]는 위상 문제를 해결하기 위해 Nerfies를 확장합니다.공식 구현의 메모리 문제로 인해, 우리는 그들의 코드를 Nerfstudio 프레임워크로 이식하고 신중하게 8. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, Matthias Nießner NeRFies HyperNeRF DyNeRF ด Instant NGP Ours 그림 6. 정성적 결과.우리의 방법은 어려운 표현에 대해서도 고품질의 디테일을 재구성합니다. GT NeRSemble: 인간 머리의 다중 뷰 광도장 재구성표 3. 양적 평가. 두 개의 비시간적 기준선과 세 가지 동적 재구성 방법에 대한 비교를 수행합니다. 데이터 세트에서 10개의 다양한 시퀀스에 대한 보이지 않는 검증 뷰를 평가합니다. 저희 방법은 세 가지 지표 모두에서 기준선보다 성능이 우수합니다. 아래 두 행은 핵심 건축 구성 요소와 학습 절차에 대한 저희 방법의 절제를 보여줍니다. 그림 7. 카메라 설정의 공간 레이아웃. 빨간색으로 표시된 것은 평가에 사용된 4개의 뷰입니다. 포인트 클라우드 재구성은 COLMAP을 통해 얻어지고 추가 깊이 감독에 사용됩니다. 공식 구현의 성능과 일치하도록 하이퍼파라미터를 사용합니다. 반면 DyNeRF[Li et al. 2022b]는 변형 필드를 사용하여 동적 콘텐츠를 표현하도록 제한되지 않지만 시간 종속 잠복 코드에 NERF를 직접 조건화합니다. 공개 코드를 사용할 수 없으므로 Nerfstudio에서 DyNeRF를 구현하고 데이터 분포에 맞게 미세 조정합니다. 5.4.2 시간에 독립적인 방법. 나아가, 데이터 세트의 다중 뷰 특성은 프레임 단위로 3D 재구성을 허용합니다. 따라서 시간을 고려하지 않는 두 가지 추가 기준선 방법을 고려합니다. 첫째, COLMAP 포인트 클라우드에 Poisson Surface Reconstruction(PSR) [Kazhdan et al. 2006]을 적용합니다. 둘째, 각 프레임에서 별도로 공식 Instant NGP [Müller et al. 2022]를 실행합니다. 5.4.3 얼굴에 특정한 방법. 또한, 얼굴 특정 동적 재구성 방법의 대표로 Neural Head Avatars(NHA) [Grassal et al. 2022] 및 NeRFace [Gafni et al. 2021]와 비교합니다. 둘 다 추적된 통계 메시 모델이 제공하는 기하학적 사전에 의존합니다. NHA는 FLAME 위에 정점 오프셋을 최적화하고 뷰 및 표현에 따라 달라지는 텍스처를 예측하는 메시 기반 방법입니다. NeRFace는 3DMM 매개변수를 직접 사용하여 NeRF를 조절하므로 DyNeRF와 유사합니다. 두 방법 모두 원래 단안 사용 사례를 위해 설계되었으므로 사용자 지정 다중 뷰 FLAME 추적기를 사용하고 훈련 중에 12개 뷰를 모두 제공하여 다중 뷰 시나리오로 확장합니다. 3DMM에 대한 의존성은 두 방법 모두 어느 정도의 재생성 기능을 제공하지만 충분히 밀도가 높은 관찰이 제공되면 렌더링 품질이 저하될 수 있습니다. 5.4.4 배경 모델링. 공정한 비교를 위해 모든 NeRF 기반 베이스라인이 모든 나머지 투과율을 흰색으로 채색하여 밀도 없이 배경을 나타내도록 권장합니다. 이를 위해 알파 마스크를 사용하여 기준 진실 이미지의 모든 배경 픽셀도 흰색으로 설정합니다. 경험상 이 간단한 기술을 사용하면 모든 베이스라인이 전경에 있는 사람에 대한 좋은 재구성을 학습할 수 있습니다. 5. 정적 부품 제거 방법 PSNR ↑ SSIM↑ LPIPS↓↓ PSR 12.0.0. 인스턴트 NGP 28.0.0. Nerfies 29.0.0. HyperNeRF 29.0.0. DyNeRF 30.0.0. Ours 31.0.0. NGP + Def. 30.0.0. 해시 앙상블 30.0.0. 깊이 없음 31.0.0. 워밍업 없음 31.0.0. 테이블 16개만 있음 31.0.0. 평가 프로토콜 모든 방법을 4개의 보류된 카메라 관점에서 평가합니다. 그림은 평가 카메라의 공간적 배열을 보여줍니다. 또한 계산 시간을 고려하여 각 평가 카메라에서 균등하게 분포된 15개의 타임스텝에 대해서만 예측을 평가합니다. 우리는 전체 시퀀스 대신 15개 타임스텝만 평가할 때 사용된 모든 이미지 메트릭이 최대 0.02포인트 차이가 나는 것을 여러 시퀀스에서 확인했습니다.메트릭. 우리는 개별 재구성 프레임의 시각적 품질을 평가하기 위해 세 가지 이미지 메트릭을 보고합니다.최대 신호 대 잡음비(PSNR), 구조적 유사성(SSIM)[Wang et al. 2004], 학습된 지각적 이미지 패치 유사성(LPIPS)[Zhang et al. 2018]. 모든 메트릭은 얼굴 영역에 초점을 맞추기 위해 알파 마스크와 예측을 혼합한 후 프레임별로 평가됩니다.또한 [Li et al. 2022b]에서 사용하는 JOD 메트릭[Mantiuk et al. 2021]을 계산하여 참조 비디오와의 지각적 차이를 나타냅니다.5.6 최신 기술과의 비교 표 3은 NeRSemble이 모든 이미지 메트릭에서 모든 기준선보다 정량적으로 성능이 우수함을 보여줍니다. 특히, 우리의 방법은 고주파 세부 정보에 민감한 SSIM 및 LPIPS 메트릭에서 강력한 개선을 보여줍니다. 이러한 관찰은 그림 6의 정성적 비교와 일치하며, 여기서 우리의 방법은 더 나은 얼굴 세부 정보를 재구성합니다. 독자는 우리 방법에 대한 보다 심층적인 시각적 분석을 위해 보충 비디오를 시청할 것을 권장합니다. 시간적 일관성 평가. PSNR, SSIM 및 LPIPS와 같은 프레임당 메트릭은 깜빡임과 같은 시간적 아티팩트를 고려하지 않습니다. 따라서 우리는 지각적 비디오 메트릭 JOD [Mantiuk et al. 2021]를 사용하여 렌더링된 비디오와 실제 비디오의 시각적 유사성을 측정합니다. 모든 주요 기준선에 대해 우리는 훈련 프레임 속도의 1/3인 24.3fps로 비디오를 렌더링하고 JOD의 평균을 냅니다. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, Matthias Nieẞner 표 4. 지각적 품질 측정 기준 Just-Objectionable-Difference(JOD)를 사용한 시간적 일관성 평가[Mantiuk et al. 2021]. 숫자가 높을수록 시간적 깜빡임이 적고 실제 영상과 더 유사함을 나타냅니다. 방법 기관 NGP Nerfies HyperNeRF DyNeRF JOD ↑ 6. 저희 7.7.7.7.표 5. 얼굴별 방법에 대한 평가. NeRSemble은 Neural Head Avatars[Grassal et al. 2022] 및 NeRFace[Gafni et al. 2021]와 유리하게 비교됩니다. 방법 PSNR SSIM ↑ LPIPS↓ 신경 머리 아바타 NeRFace 31.0.0.35.0.0.Ours 37.0.0.Ours 인스턴트 NGP t=t=t =표 6. 신경 3D 비디오 데이터 세트에 대한 정량적 비교. NeRSemble의 기능은 얼굴 블렌드 셰이프에서 영감을 받았지만 일반적인 동적 장면도 합리적으로 모델링할 수 있습니다. 방법 PSNR ↑ 30. NeRFPlayer NeRFPlayer (인스턴트 NGP) (TensoRF) 30. HyperReel Ours 31.29. 모든 검증 뷰와 모든 10개 검증 시퀀스에 대한 점수. 이 시간적 평가의 결과는 표 4에 나와 있습니다. 인스턴트 NGP 기준선은 완전히 시간에 독립적이므로 비디오 렌더링에 상당한 깜빡임 아티팩트가 발생합니다. 그림 8은 이러한 아티팩트의 예를 보여줍니다. 이와 대조적으로 NeRSemble은 부드러운 시간적 경험을 제공합니다. 인스턴트 NGP와의 비교. Instant NGP 기준선은 그림 6에서 볼 수 있듯이 프레임별로 매력적인 이미지를 생성합니다. 그러나 카메라 설정의 희소성으로 인해 플로터와 산란된 표면을 생성하는 경향이 강합니다. 반면 NeRSemble은 여러 시간 단계에 걸쳐 공간을 제한하여 플로터를 제거하는 데 크게 기여합니다. 이는 안티 플로터 전략이나 추가 손실 없이 학습된 NeRSemble에도 해당합니다. 이러한 기본 버전의 모델은 여전히 Instant NGP보다 성능이 뛰어납니다(표 7의 맨 위 행 참조). 이는 희소 설정에서 각 프레임을 독립적으로 모델링하여 표현력을 높이는 것(예: Instant NGP 기준선은 모델보다 1015배 더 많은 매개변수를 가짐)이 더 나은 재구성으로 이어지지 않는다는 것을 보여줍니다. 얼굴별 방법과의 비교. NHA 및 NeRFace와 비교하기 위해 데이터 세트에서 7개 시퀀스를 평가하고, NHA의 전처리 파이프라인이 얼굴 랜드마크, 분할 마스크 및 법선을 예측하지 못한 더 복잡한 동작이 있는 3개 시퀀스를 제외했습니다. 또한 NHA는 몸통 없이 머리만 합성합니다.따라서 공정한 비교를 위해 얼굴 영역만 평가합니다.표 5는 NeRSemble이 얼굴을 위해 특별히 설계되었음에도 불구하고 두 베이스라인 모두보다 성능이 우수함을 보여줍니다.t1-&gt; tt2-&gt; t그림 8. 시간적 일관성.새로운 뷰(왼쪽)에 대한 재렌더링과 시간적 차이 이미지를 보여줍니다.오른쪽에서는 세 개의 인접한 프레임 사이에서 눈썹이 줄어들고 커지는 Instant NGP 베이스라인의 깜빡이는 아티팩트를 보여줍니다.이에 비해 NeRSemble은 더 많은 시간적 일관성을 제공합니다.신경망 3D 비디오 데이터 세트에서의 비교[Li et al. 2022b].NeRSemble은 동적 장면의 내용에 대해 강력한 가정을 하지 않으므로 보다 일반적인 시나리오에 적용할 수 있습니다.NeRSemble의 일반성을 보여주기 위해 신경망 3D 비디오 데이터 세트의 공개적으로 사용 가능한 6개 시퀀스에서 평가합니다[Li et al. 2022b]. 우리는 NeRFPlayer [Song et al. 2023]와 HyperReel [Attal et al. 2023]의 평가 프로토콜을 따르는데, 이는 녹음을 1352 x 1014픽셀 해상도로 다운샘플링하고, 평가를 위해 상단 중앙 뷰를 유지하고, 모든 6개 시퀀스에 대해 평균화된 메트릭을 보고합니다. 또한 데이터 세트와 함께 제공된 포즈가 약간 틀리기 때문에 COLMAP [Schönberger et al. 2016]으로 포즈를 다시 계산합니다. 표는 정량적 결과를 보여줍니다. 원래 DyNeRF [Li et al. 2022b]와 StreamRF [Li et al. 2022a]는 평가에서 제외했습니다. 그 이유는 해당 숫자가 사용 가능한 6개 시퀀스 중 1개에서만 계산되었고 따라서 NeRFPlayer와 HyperReel의 결과와 비교할 수 없기 때문입니다. 평가 결과 NeRSemble은 얼굴 블렌드 셰이프에서 영감을 받은 기능에도 불구하고 일반적인 동적 장면을 합리적으로 모델링할 수 있음을 보여줍니다. 그러나 저희 방법은 Instant NGP에 의존하기 때문에 일부 약점도 물려받았습니다. 특히 HyperReel처럼 빛의 굴절을 모델링하지 않습니다. 그 결과 NeRSemble은 Neural 3D Video 데이터 세트에서 흔히 볼 수 있는 창문 유리창과 유리병의 효과를 완벽하게 포착할 수 없습니다. 5. Ablations 기준선과의 비교 외에도 저희는 여러 실험을 수행하여 설계 선택의 타당성을 검증하고 NeRSemble의 내부 작동 방식을 이해했습니다. (a) NGP + Def. (b) Hash Ensemble (c) 워밍업 없음 NeRSemble: 인간 머리의 다중 뷰 Radiance Field 재구성 (d) 16개 해시 테이블 (e) 저희의 것 (f) GT .그림 9. 저희 모델 구성 요소의 Ablation. Instant NGP를 변형 필드와 결합하면 (a) 몸통과 같이 장면의 딱딱하게 움직이는 영역에서 선명한 디테일을 생성하지만 입 움직임과 같은 더 어려운 동작에는 어려움을 겪습니다. 반면, 해시 인코딩 앙상블(b)을 사용하면 복잡한 동작을 더 잘 처리할 수 있지만 일반적으로 더 흐릿한 재구성이 생성됩니다. 두 구성 요소(c)를 결합하면 두 아키텍처의 강점을 활용하지만 변형 필드가 있는 인스턴트 NGP와 같이 딱딱하게 움직이는 영역에서 동일한 세부 정보를 생성하지는 않습니다. 워밍업 단계를 사용하면 16개 테이블을 사용할 때 이미 선명한 세부 정보가 반환되며(d) 해시 인코딩 수를 늘리면 더욱 개선할 수 있습니다(e). 아키텍처 구성 요소의 기여. 해시 앙상블과 변형 필드를 사용하는 효과를 제거합니다. 표 3은 인스턴트 NGP 백본(NGP + Def.)이 있는 변형 필드나 일반 해시 앙상블이 최종 모델의 성능과 일치하지 않는다는 것을 보여줍니다. 그러나 두 아키텍처 모두 그 자체로 강력한 기준선입니다. 그림 9에서 정성적 결과를 제시하는데, 이는 변형 기반 접근 방식이 일반적으로 더 선명한 재구성을 생성하지만 변형으로 모델링하기 어려운 더 어려운 동작에는 어려움을 겪는다는 것을 보여줍니다. 반면, 해시 앙상블은 피처 블렌딩을 통해 모든 동적 장면을 모델링할 수 있는 표현력이 있지만 변형 필드의 사전 지식이 없기 때문에 일반적으로 간단한 움직임에 대해 더 흐릿한 결과를 생성합니다.표 3의 정량적 결과는 이러한 결과를 확인하며, 해시 앙상블은 높은 PSNR을 기록했지만 LPIPS 값은 더 나쁩니다.해시 테이블 수.해시 테이블이 16개인 NeRSemble은 해시 테이블이 32개인 경우에 비해 무시할 수 있는 수준만 손상됩니다.이는 프레임 수와 해시 테이블 간의 비율이 잘 조절되고 정보가 테이블 간에 효과적으로 공유됨을 확인합니다.워밍업 단계의 효과.워밍업 없이 학습하면 지속적으로 성능이 떨어집니다.이는 모델에 모든 해시 그리드에 대한 액세스 권한을 바로 부여하면 변형 필드와의 대응 관계를 학습하지 못하기 때문입니다.결과적으로 학습된 해시 인코딩은 정렬이 덜 잘 되고 효과적으로 블렌딩할 수 없습니다.시각적으로 이는 약간 더 흐릿한 렌더링으로 나타납니다. 이러한 통찰력은 HyperNeRF에서 처음에 주변 차원 사용을 비활성화하려는 제안과 일치합니다.표 7. 플로터 제거 기술의 소거.뷰 프러스텀 컬링(VFC)과 점유 그리드 필터링(OGF)은 모두 평가 프로토콜에서 생략된 영역의 플로터를 대부분 제거하기 때문에 메트릭에 미치는 영향이 무시할 만합니다.추가 손실(L)이나 플로터 제거 기술이 없는 일반 버전의 NeRSemble은 이미 표 3의 모든 기준선과 비교하여 경쟁력 있는 성능을 보입니다.☑ L VFC OGF PSNR ↑ SSIM ↑ LPIPS↓ 30.0.0.31.0.0.☑ ☐ 31.0.0.☐ ☑ 31.0.0.☑ 31.0.0.☐ ☐ &gt; &gt; &gt; ☐ &gt; &gt; &gt; ☑ ☑ ☑ ☑ ☑ 깊이 감독의 효과. 깊이 감독을 제거해도 성능이 약간만 저하되므로 12개 입력 뷰의 RGB 정보가 이미 지오메트리를 충분히 감독하고 있다고 가정합니다. 그러나 피팅된 3DMM이나 훈련된 깊이 예측 네트워크와 같은 직교 채널에서 깊이 감독을 활용하면 RGB 비디오 프레임 이외의 소스에서 데이터 사전을 통합하므로 여전히 유익할 수 있습니다.플로터 제거 기술.플로터를 억제하기 위한 세 가지 전략의 효과를 제거합니다.먼저 마스크 손실, 깊이 감독 및 왜곡 손실과 같은 모든 추가 손실의 효과를 분리하고 표 7에서 성능에 미치는 상당한 영향을 확인합니다.반면에 뷰 프러스텀 컬링 및 점유 그리드 필터링은 보고된 메트릭에 영향을 미치지 않지만 새로운 카메라 궤적을 렌더링할 때 시각적 품질을 개선합니다.• Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter 및 Matthias Nieẞner ẞt₁ = ££££ Helen 그림 10. 블렌드 가중치. 첫 번째 해시 그리드의 내용을 ẞt₁,i = 0(i &gt; 1)으로 설정하여 조사하면 첫 번째 해시 그리드가 일종의 평균 표현(왼쪽)을 저장한다는 것을 알 수 있습니다. 오른쪽에서 i = {2, 3, 4}에 대해 ẞt1,i = 0을 차례로 설정합니다. 각 표에는 변형 네트워크의 표현 용량을 초과하는 추가 세부 정보가 저장됩니다. 표시된 모든 예에 Wt1을 사용하고 t₁는 첫 번째 프레임을 나타냅니다. 개별 해시 그리드의 내용. 그림 10에서 개별 해시 그리드 H;의 내용을 분석합니다. 이를 위해 시퀀스의 첫 번째 프레임 t₁에 대해 학습된 블렌드 가중치 ßt₁i(i &gt; 1)를 수정하고 ẞt 1,1과 변형 코드 @t₁는 고정합니다. 이 실험은 변형 필드 D가 장면의 고정된 움직임을 설명한다는 것을 보여줍니다.왜냐하면 ẞt₁를 수정하면 머리가 고정된 상태에서 잘 정렬된 모양이 변경되기 때문입니다.또한 H₁는 사람의 평균 얼굴과 비교할 수 있는 표현을 저장하는 것으로 보입니다.나머지 해시 그리드는 역동적이고 체적적인 텍스처와 유사하게 동작하여 그렇지 않으면 설명할 수 없는 세부 정보(예: 위상적으로 복잡한 변형, 표정에 따른 주름 또는 조명 변경)를 장면에 추가합니다.첫 번째 해시 그리드 H₁의 특별한 상태는 훈련 중에 항상 활성화되어 있는 반면 다른 모든 해시 그리드는 워밍업 단계에서 점진적으로 도입된다는 사실에 기인합니다.5. 제한 사항 실험에서 희소한 다중 뷰 녹화 세트로 설득력 있는 결과를 얻을 수 있음을 보여줍니다.그러나 다양한 제한 사항이 남아 있습니다.NeRSemble은 변형 필드를 통해 시간 단계에 걸쳐 명확한 대응 관계를 모델링하기 때문에 빠른 머리카락 움직임을 완벽하게 포착할 수 없습니다(그림 11c 참조). 이를 해결하기 위해 광학 흐름 또는 미분 물리학을 통한 운동 사전 확률을 통합하는 것은 향후 연구에 흥미로운 분야가 될 수 있습니다. 게다가 현재 저희 방법은 동적 광도장 표현을 최적화하여 특정 시퀀스의 유사성과 동작을 복구하는 데 중점을 두고 있습니다. 그 결과 저희 방법은 시퀀스 전체에 걸쳐 일반화되는 사전 확률을 학습할 수 없습니다. 여기서 저희는 신원과 얼굴 표정 모두에 대해 일반화되는 동적 NeRF에 대한 향후 연구에 큰 잠재력을 봅니다. 사실적인 4D 아바타의 분포에 대한 학습된 사전 확률은 최적화 절차를 더욱 제한하는 데 도움이 될 수 있습니다. 이는 단안 입력이나 대부분의 기록 시간 동안 가려져 있고 따라서 재구성 품질이 떨어질 수 있는 입 내부와 같은 얼굴 영역을 캡처하는 데 특히 중요합니다(그림 11 참조). (a) (b) (c) 그림 11. 실패 사례. 입 내부의 높은 수준의 폐쇄는 때때로 입 뒤쪽에서 치아가 잘못 재구성되는 움푹 들어간 얼굴 환상을 일으킬 수 있습니다(a). 눈의 광원의 반사는 드문 눈 아티팩트를 일으킬 수 있습니다(b). 변형 필드는 매우 빠른 머리카락 움직임을 모델링하지 못할 수 있으며, 이는 표준 해시 그리드가 일부 프레임에 대해 선명한 결과를 합성하는 것을 방해합니다(c).
--- CONCLUSION ---
이 작업에서 우리는 다중 시점 비디오 입력에서 애니메이션 인간 머리의 광도장 재구성에 초점을 맞춘 새로운 방법과 데이터 세트를 제안했습니다. 이를 위해 우리는 4700개의 시퀀스가 있는 220개 이상의 아이덴티티를 포함하는 다양한 인간 머리의 새로운 다중 시점 비디오 벤치마크를 도입했습니다. 우리는 또한 임의의 시점과 시간 단계의 사진처럼 사실적인 재렌더링을 생성하는 새로운 방법을 제안했으며, 우리의 데이터 세트와 수반되는 벤치마크가 커뮤니티에 중요한 기여가 되고 디지털 인간에 대한 향후 작업을 용이하게 하기를 바랍니다. 우리가 제안한 시공간적 NeRF에 대한 새로운 표현은 변형 필드를 사용하여 거친 움직임을 요인 제거하고 해시 그리드 인코딩의 앙상블을 사용하여 미세한 변형을 모델링하고 모델의 시간적 용량을 늘립니다. 우리의 실험은 NeRSemble이 다중 시점 비디오 입력에서 시간적으로 일관되고 매우 자세한 체적 재구성을 달성하여 특히 시퀀스에 복잡한 동작이 포함된 경우 기존 기준선보다 상당한 마진으로 성능이 우수함을 보여줍니다. 감사의 글 이 연구는 ERC Starting Grant Scan2CAD(804724), 독일 연구 재단(DFG) Grant &quot;Making Machine Learning on Static and Dynamic 3D Data Practical&quot;, 독일 연구 재단(DFG) Research Unit &quot;Learning and Simulation in Visual Computing&quot;의 지원을 받았습니다. 또한 데이터 수집에 도움을 준 Maximilian Knörl과 비디오 음성 해설을 해준 Angela Dai에게 감사드립니다. 참고문헌 Shahrukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shechtman, Zhixin Shu. 2022. RigNeRF: Fully Controllable Neural 3D Portraits. IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR)의 회의록. 20364-20373. Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O&#39;Toole, and Changil Kim. 2023. HyperReel: Ray-Conditioned Sampling을 사용한 고충실도 6-DoF 비디오. CVPR (2023). Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. 2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. CVPR (2022). V. Blanz, C. Basso, T. Poggio, and T. Vetter. 2003. Reanimating Faces in Images and Video. (2003), 641-650. Volker Blanz and Thomas Vetter. 1999. A morphable Model for the Synthesis of 3D Faces. 제26회 컴퓨터 그래픽 및 대화형 기술 연례 컨퍼런스(SIGGRAPH &#39;99)의 회의록. ACM Press/Addison-Wesley Publishing Co., USA, 187-194. https://doi.org/10.1145/311535.NeRSemble: 인간 머리의 다중 시점 광도 필드 재구성 •Michael Broxton, John Flynn, Ryan Overbeck, Daniel Erickson, Peter Hedman, Matthew Duvall, Jason Dourgarian, Jay Busch, Matt Whalen, Paul Debevec. 2020. 계층화된 메시 표현을 사용한 몰입형 광 필드 비디오. ACM Transactions on Graphics(TOG) 39, 4(2020), 86–1. Ang Cao와 Justin Johnson. 2023. HexPlane: 동적 장면을 위한 빠른 표현. CVPR(2023). Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, Hao Su. 2022. TensoRF: Tensorial Radiance Fields. 유럽 컴퓨터 비전 컨퍼런스(ECCV)에서. Shiyang Cheng, Irene Kotsia, Maja Pantic, Stefanos Zafeiriou. 2018. 4dfab: 얼굴 표정 분석 및 생체 인식 애플리케이션을 위한 대규모 4d 데이터베이스. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서. 5117-5126. Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk, Steve Sullivan. 2015. 고품질 스트리밍 가능한 자유 시점 비디오. ACM 그래픽스 트랜잭션(ToG) 34, 4(2015), 1-13. Darren Cosker, Eva Krumhuber, Adrian Hilton. 2011. 3D 동적 모핑 얼굴 모델링에 응용되는 FACS 유효 3D 동적 액션 유닛 데이터베이스. 2011년 컴퓨터 비전 국제 컨퍼런스. IEEE, 2296-2303. Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag Ranjan, Michael Black. 2019. 3D 스피킹 스타일의 캡처, 학습 및 합성. IEEE Conf. on Computer Vision and Pattern Recognition(CVPR) 회의록. 10101-10111. Rémi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Léo Gautheron, Nathalie TH Gayraud, Hicham Janati, Alain Rakotomamonjy, levgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong 및 Titouan Vayer. 2021. POT: Python 최적 전송. 기계학습연구 22, 78(2021), 1-8. http://jmlr.org/papers/v22/20-451.html Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo Kanazawa. 2023. K-Planes: Explicit Radiance Fields in Space, Time, and Appearance. CVPR에 실림. Fridovich-Keil and Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. CVPR에 실림. Guy Gafni, Justus Thies, Michael Zollhöfer, and Matthias Nießner. 2021. Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)의 회의록에 실림. 8649-8658. Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, Juyong Zhang. 2022. 단안 비디오에서 개인화된 의미적 얼굴 NeRF 모델 재구성. ACM Transactions on Graphics(SIGGRAPH Asia 논문집) 41, 6(2022). https://doi.org/10.1145/3550454.Simon Giebenhain, Tobias Kirschstein, Markos Georgopoulos, Martin Rünz, Lourdes Agapito, Matthias Nießner. 2022. 신경 매개 변수 머리 모델 학습. Philip-William Grassal, Malte Prinzler, Titus Leistner, Carsten Rother, Matthias Nieẞner, Justus Thies. 2022. 단안 RGB 비디오에서 신경 머리 아바타. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 사항. 18653-18664. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 2016. 이미지 인식을 위한 심층적 잔여 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 사항. 770-778. Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, Juyong Zhang. 2022. HeadNeRF: 실시간 NeRF 기반 매개변수 헤드 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)에서. Animesh Karnewar, Tobias Ritschel, Oliver Wang, Niloy Mitra. 2022. Relu 필드: 할 수 있는 작은 비선형성. ACM SIGGRAPH 2022 컨퍼런스 진행 사항. 1-9. Michael M. Kazhdan, Matthew Bolitho, Hugues Hoppe. 2006. Poisson Surface Reconstruction. 제4회 Eurographics Symposium on Geometry Processing(Cagliari, Sardinia, Italy)(SGP &#39;06, Vol. 256)의 회의록에서 Alla Sheffer와 Konrad Polthier(편집자). Eurographics Association, Aire-la-Ville, Switzerland, 61-70. http://dl.acm.org/citation.cfm?id=1281957.Lingzhi Li, Zhen Shen, Li Shen, Ping Tan, et al. 2022a. 3D 비디오 합성을 위한 스트리밍 복사장. 신경 정보 처리 시스템의 발전에서. Ruilong Li, Matthew Tancik, Angjoo Kanazawa. 2022c. NerfAcc: 일반적인 NeRF 가속 도구 상자. arXiv 사전 인쇄본 arXiv:2210.04847 (2022). Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. 2017. 4D 스캔에서 얼굴 모양과 표정 모델 학습. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia) 36, 6 (2017), 194:1-194:17. https://doi.org/10.1145/3130800.Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. 2022b. 다중 시점 비디오에서 신경 3D 비디오 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 회의록. 5521-5531. Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian L Curless, Steven M Seitz, Ira Kemelmacher-Shlizerman. 2021. 실시간 고해상도 배경 매팅. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 8762-8771. Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, Yaser Sheikh. 2019. Neural Volumes: 이미지에서 동적 렌더링 가능 볼륨 학습. ACM Trans. Graph. 38, 4, Article 65(2019년 7월), 14페이지. Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, Jason Saragih. 2021. 효율적인 신경 렌더링을 위한 체적 기본 요소의 혼합. ACM 그래픽스 트랜잭션(TOG) 40, 4(2021), 1-13. Rafał K. Mantiuk, Gyorgy Denes, Alexandre Chapiro, Anton Kaplanyan, Gizem Rufo, Romain Bachy, Trisha Lian, Anjul Patney. 2021. FovVideoVDP: 광시야 비디오에 대한 눈에 보이는 차이 예측기. ACM Trans. Graph. 40, 4, Article 49(2021년 7월), 19페이지. https://doi.org/10.1145/3450626. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng. 2020. NeRF: 뷰 합성을 위한 신경 광도장으로 장면 표현. ECCV에서. Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 2022. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽스 프리미티브. ACM Trans. Graph. 41, 4, Article 102(2022년 7월), 15페이지. https://doi.org/10.1145/3528223.Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla. 2021a. Nerfies: 변형 가능한 신경 광도장. ICCV(2021). Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz. 2021b. HyperNeRF: 위상적으로 변하는 신경 복사장에 대한 고차원 표현. ACM Trans. Graph. 40, 6, Article 238(2021년 12월). Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala. 2019. PyTorch: 필수적 스타일, 고성능 딥 러닝 라이브러리. 신경 정보 처리 시스템의 발전 32. Curran Associates, Inc., 8024-8035. http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-highperformance-deep-learning-library.pdf Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, Thomas Vetter. 2009. 포즈 및 조명 불변 얼굴 인식을 위한 3D 얼굴 모델. 제6회 IEEE 고급 비디오 및 신호 기반 감시 국제 컨퍼런스. IEEE, 296-301. Albert Pumarola, Enric Corona, Gerard Pons-Moll, Francesc Moreno-Noguer. 2020. D-NeRF: 동적 장면을 위한 신경 광도 필드. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. Konstantinos Rematas, Andrew Liu, Pratul P. Srinivasan, Jonathan T. Barron, Andrea Tagliasacchi, Tom Funkhouser, Vittorio Ferrari. 2022. Urban Radiance Fields. CVPR(2022). Neus Sabater, Guillaume Boisson, Benoit Vandame, Paul Kerbiriou, Frederic Babon, Matthieu Hog, Tristan Langlois, Remy Gendrot, Olivier Bureller, Arno Schubert, Valerie Allie. 2017. 다중 뷰 라이트 필드 비디오를 위한 데이터 세트 및 파이프라인. CVPR 워크숍에서. Johannes Lutz Schönberger와 Jan-Michael Frahm. 2016. Structure-from-Motion Revisited. 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)에서. Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys, Jan-Michael Frahm. 2016. 비정형 멀티뷰 스테레오를 위한 픽셀별 뷰 선택. 유럽 컴퓨터 비전 컨퍼런스(ECCV)에서. Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, Andreas Geiger. 2023. NeRFPlayer: 분해된 신경 복사장을 사용한 스트리밍 가능한 동적 장면 표현. IEEE 시각화 및 컴퓨터 그래픽 트랜잭션 29, 5(2023), 2732-2742. Cheng Sun, Min Sun, Hwann-Tzong Chen. 2022. 직접 복셀 그리드 최적화: 복사장 재구성을 위한 초고속 수렴. CVPR에서. Matthew Tancik*, Ethan Weber*, Evonne Ng*, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, Angjoo Kanazawa. 2022. Nerfstudio: 신경 광도장 개발을 위한 프레임워크. https://github.com/nerfstudio-project/nerfstudio Daoye Wang, Prashanth Chandran, Gaspard Zoss, Derek Bradley, Paulo Gotardo. 2022a. MoRF: 다중 뷰 신경 헤드 모델링을 위한 변형 가능한 광도장. ACM SIGGRAPH 2022 컨퍼런스 회의록(캐나다 브리티시 컬럼비아주 밴쿠버)(SIGGRAPH &#39;22). Association for Computing Machinery, 뉴욕, 뉴욕, 미국, 제55조, 9페이지. 한국어: https://doi.org/10.1145/3528233.Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change Loy. 2020. MEAD: 감정적 대화 얼굴 생성을 위한 대규모 오디오비주얼 데이터 세트. ECCV에서. Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and Lan Xu. 2022b. 실시간으로 동적 광채 필드 렌더링을 위한 Fourier PlenOctrees. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록에서. 13524–13534. Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. IEEE 이미지 처리 거래 13, 4(2004), 600-612. Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, Matthias Nieẞner Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson, Rohan Bali, Danielle Belko, Eric Brockmeyer, Lucas Evans, Timothy Godisart, Hyowon Ha, Alexander Hypes, Taylor Koska, Steven Krenn, Stephen Lombardi, Xiaomin Luo, Kevyn McPhail, Laura Millerschoen, Michal Perdoch, Mark Pitts, Alexander Richard, Jason Saragih, Junko Saragih, Takaaki Shiratori, Tomas Simon, Matt Stewart, Autumn Trimble, Xinshuo Weng, David Whitewolf, Chenglei Wu, Shoou-I Yu, Yaser Sheikh. 2022. Multiface: 신경 얼굴 렌더링을 위한 데이터 세트. arXiv에서. https://doi.org/10.48550/ ARXIV.2207.Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, Srinath Sridhar. 2022. 시각 컴퓨팅 및 그 너머의 신경 필드. 컴퓨터 그래픽 포럼(2022). https://doi.org/10.1111/cgf.Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel, Mohamed Elgharib, Daniel Cremers, Christian Theobalt. 2021. i3DMM: 인간 머리의 심층 암묵적 3D 변형 가능 모델. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 12803-12813. Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, Nong Sang. 2018. Bisenet: 실시간 의미 분할을 위한 양측 분할 네트워크. 유럽 컴퓨터 비전 컨퍼런스(ECCV)의 진행 중. 325-341. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. 2018. 지각적 지표로서 딥 피처의 비합리적 효과. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스의 진행 중. 586-595. Xing Zhang, Lijun Yin, Jeffrey F Cohn, Shaun Canavan, Michael Reale, Andy Horowitz, Peng Liu, Jeffrey M Girard. 2014. Bp4d-spontaneous: 고해상도 자발적 3D 동적 얼굴 표정 데이터베이스. Image and Vision Computing 32,(2014), 692-706. Yufeng Zheng, Victoria Fernández Abrevaya, Marcel C. Bühler, Xu Chen, Michael J. Black, Otmar Hilliges. 2022. IM 아바타: 비디오에서 암묵적으로 변형 가능한 머리 아바타. 컴퓨터 비전 및 패턴 인식(CVPR)에서. Wojciech Zielonka, Timo Bolkart, Justus Thies. 2023. 인스턴트 체적 머리 아바타. CVPR(2023). C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele, Simon Winder, Richard Szeliski. 2004. 계층화된 표현을 사용한 고품질 비디오 뷰 보간. ACM 그래픽 거래(TOG) 23, 3(2004), 600-608.
