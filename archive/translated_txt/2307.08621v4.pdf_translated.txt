--- ABSTRACT ---
이 연구에서 우리는 대규모 언어 모델을 위한 기반 아키텍처로 Retentive Network(RETNET)를 제안하여 동시에 학습 병렬성, 저비용 추론 및 우수한 성능을 달성합니다. 우리는 이론적으로 재귀와 주의 사이의 연결을 도출합니다. 그런 다음 세 가지 계산 패러다임, 즉 병렬, 순환 및 청크별 순환을 지원하는 시퀀스 모델링을 위한 보존 메커니즘을 제안합니다. 구체적으로 병렬 표현은 학습 병렬성을 허용합니다. 순환 표현은 저비용 O(1) 추론을 가능하게 하여 성능을 희생하지 않고 디코딩 처리량, 대기 시간 및 GPU 메모리를 개선합니다. 청크별 순환 표현은 선형 복잡도로 효율적인 긴 시퀀스 모델링을 용이하게 하며, 각 청크는 병렬로 인코딩되는 동시에 청크를 반복적으로 요약합니다. 언어 모델링에 대한 실험 결과는 RETNET이 유리한 스케일링 결과, 병렬 학습, 저비용 배포 및 효율적인 추론을 달성한다는 것을 보여줍니다. 흥미로운 속성은 RETNET을 대규모 언어 모델을 위한 Transformer의 강력한 후속 제품으로 만듭니다. 코드는 https://aka.ms/retnet에서 제공됩니다. 추론 비용 8.4X3.4X15.6XGPU 메모리 처리량↑(GB) (wps) 지연 시간↓(ms) Transformer RetNet 스케일링 곡선 모델 크기(B) 그림 1: 유지 네트워크(RetNet)는 Transformer에 비해 저비용 추론(즉, GPU 메모리, 처리량 및 지연 시간), 학습 병렬성 및 유리한 스케일링 곡선을 달성합니다. 추론 비용의 결과는 입력 길이로 8k로 보고됩니다. 그림 6은 다른 시퀀스 길이에 대한 더 많은 결과를 보여줍니다. 동등한 기여. ◇ 책임 저자. ❝ 가능성의 한계를 발견하는 유일한 방법은 불가능한 것으로 나아가는 것입니다. Arthur C. Clarke ,, 1
--- METHOD ---
인기. 두 번째 가닥은 효율적인 추론을 위해 반복 모델로 돌아가지만 훈련 병렬성은 희생합니다. 해결책으로 요소별 연산자 [PAA+23]가 가속에 사용되지만 표현 용량과 성능이 손상됩니다. 세 번째 연구 라인은 주의를 S4 [GGR21] 및 그 변형 [DFS+22, PMN+23]과 같은 다른 메커니즘으로 대체하는 것을 탐구합니다. 이전 작업 중 어느 것도 불가능한 삼각형을 돌파할 수 없어 Transformers와 비교할 때 명확한 승자가 없습니다. 그림 2: RetNet은 &quot;불가능한 삼각형&quot;을 가능하게 하여 동시에 학습 병렬성, 우수한 성능, 낮은 추론 비용을 달성합니다. 이 작업에서 우리는 저비용 추론, 효율적인 롱시퀀스 모델링, Transformer와 유사한 성능, 병렬 모델 학습을 동시에 달성하는 유지 네트워크(RetNet)를 제안합니다. 구체적으로, 우리는 세 가지 계산 패러다임, 즉 병렬, 재귀 및 청크별 재귀 표현을 갖는 멀티헤드 주의를 대체하기 위한 멀티스케일 유지 메커니즘을 소개합니다. 첫째, 병렬 표현은 GPU 장치를 완전히 활용할 수 있는 학습 병렬성을 제공합니다. 둘째, 재귀 표현은 메모리와 계산 측면에서 효율적인 O(1) 추론을 가능하게 합니다. 배포 비용과 대기 시간을 크게 줄일 수 있습니다. 게다가 키-값 캐시 트릭 없이 구현이 크게 간소화됩니다. 셋째, 청크별 재귀 표현은 효율적인 롱시퀀스 모델링을 수행할 수 있습니다. 우리는 계산 속도를 위해 각 로컬 블록을 병렬로 인코딩하는 동시에 GPU 메모리를 절약하기 위해 글로벌 블록을 재귀적으로 인코딩합니다. 우리는 광범위한
--- EXPERIMENT ---
언어 모델링에 대한 모든 결과는 RETNET이 유리한 스케일링 결과, 병렬 학습, 저비용 배포 및 효율적인 추론을 달성한다는 것을 보여줍니다. 흥미로운 속성으로 인해 RETNET은 대규모 언어 모델을 위한 Transformer의 강력한 후속 모델이 되었습니다. 코드는 https://aka.ms/retnet에서 제공됩니다. 추론 비용 8.4X3.4X15.6X GPU 메모리 처리량↑(GB) (wps) 대기 시간↓(ms) Transformer RetNet 스케일링 곡선 모델 크기(B) 그림 1: 유지 네트워크(RetNet)는 Transformer에 비해 저비용 추론(즉, GPU 메모리, 처리량 및 대기 시간), 학습 병렬 처리 및 유리한 스케일링 곡선을 달성합니다. 추론 비용의 결과는 입력 길이로 8k로 보고됩니다. 그림 6은 다른 시퀀스 길이에 대한 더 많은 결과를 보여줍니다. 동등한 기여. ◇ 책임 저자. ❝ 가능성의 한계를 발견하는 유일한 방법은 불가능한 것으로 넘어가는 것입니다. Arthur C. Clarke,, 1 서론 선형 변압기 병렬성 학습 저비용 추론 RetNet 변압기 순환 네트워크 강력한 성능 변압기 [VSP+17]는 원래 순환 모델의 순차적 학습 문제를 극복하기 위해 제안된 대규모 언어 모델 [BMR+20]의 사실상의 아키텍처가 되었습니다 [HS97]. 그러나 변압기의 학습 병렬성은 단계당 O(N) 복잡도와 메모리 바인딩 키-값 캐시 [Sha19]로 인해 비효율적인 추론을 희생해야 하며, 이로 인해 변압기는 배포에 적합하지 않습니다. 시퀀스 길이가 늘어나면 GPU 메모리 소비와 대기 시간이 증가하고 추론 속도가 느려집니다. 효율적인 O(1) 추론을 하는 동시에 변압기로서의 학습 병렬성과 경쟁력 있는 성능을 유지하기 위해 차세대 아키텍처를 개발하기 위한 수많은 노력이 계속되었습니다. 위의 목표를 동시에 달성하는 것은 어렵습니다. 즉, 그림 2에 표시된 소위 &quot;불가능한 삼각형&quot;입니다. 세 가지 주요 연구 분야가 있습니다. 첫째, 선형화된 주의[KVPF20]는 표준 주의 점수 exp(q. k)를 커널(q)(k)로 근사하므로 자기 회귀 추론을 재귀 형태로 다시 쓸 수 있습니다. 그러나 모델링 능력과 성능이 Transformers보다 나빠서 이 방법의 인기를 방해합니다. 두 번째 가닥은 효율적인 추론을 위해 재귀 모델로 돌아가지만 학습 병렬성을 희생합니다. 해결책으로 요소별 연산자[PAA+23]를 가속에 사용하지만 표현 용량과 성능이 손상됩니다. 세 번째 연구 라인은 S4[GGR21] 및 그 변형[DFS+22, PMN+23]과 같은 다른 메커니즘으로 주의를 대체하는 것을 탐구합니다. 이전 작업 중 어느 것도 불가능한 삼각형을 돌파할 수 없어 Transformers와 비교할 때 명확한 승자가 없습니다. 그림 2: RetNet은 &quot;불가능한 삼각형&quot;을 가능하게 하여 동시에 학습 병렬성, 우수한 성능, 낮은 추론 비용을 달성합니다. 이 작업에서 우리는 저비용 추론, 효율적인 롱시퀀스 모델링, Transformer와 유사한 성능, 병렬 모델 학습을 동시에 달성하는 유지 네트워크(RetNet)를 제안합니다. 구체적으로, 우리는 세 가지 계산 패러다임, 즉 병렬, 재귀 및 청크별 재귀 표현을 갖는 멀티헤드 주의를 대체하기 위한 멀티스케일 유지 메커니즘을 소개합니다. 첫째, 병렬 표현은 GPU 장치를 최대한 활용하기 위한 학습 병렬성을 강화합니다. 둘째, 재귀 표현은 메모리와 계산 측면에서 효율적인 O(1) 추론을 가능하게 합니다. 배포 비용과 지연 시간을 크게 줄일 수 있습니다. 게다가 키-값 캐시 트릭 없이 구현이 크게 간소화됩니다. 셋째, 청크별 재귀 표현은 효율적인 롱시퀀스 모델링을 수행할 수 있습니다. 우리는 계산 속도를 위해 각 로컬 블록을 병렬로 인코딩하는 동시에 GPU 메모리를 절약하기 위해 글로벌 블록을 재귀적으로 인코딩합니다. 우리는 RetNet을 Transformer 및 그 변형과 비교하기 위해 광범위한 실험을 수행합니다. 언어 모델링에 대한 실험 결과는 RetNet이 스케일링 곡선과 컨텍스트 내 학습 측면에서 지속적으로 경쟁력이 있음을 보여줍니다.게다가 RetNet의 추론 비용은 길이 불변입니다.7B 모델과 8k 시퀀스 길이의 경우 RetNet은 키-값 캐시가 있는 Transformer보다 8.4배 더 빠르게 디코딩하고 메모리를 70% 절약합니다.RetNet은 학습하는 동안 표준 Transformer보다 25-50%의 메모리 절약과 7배의 가속을 달성하며 고도로 최적화된 FlashAttention[DFE+22]에 대한 이점을 제공합니다.게다가 RetNet의 추론 지연 시간은 배치 크기에 영향을 받지 않아 엄청난 처리량이 가능합니다.이러한 흥미로운 속성은 RetNet을 대규모 언어 모델에 대한 Transformer의 강력한 후속 제품으로 만듭니다.2 유지 네트워크 유지 네트워크(RetNet)는 L개의 동일한 블록으로 쌓여 있으며 Transformer[VSP+17]와 유사한 레이아웃(예: 잔여 연결 및 사전 LayerNorm)을 따릅니다. 각 RetNet 블록에는 두 개의 모듈이 포함되어 있습니다. 다중 스케일 유지(MSR) 모듈과 피드 포워드 네트워크(FFN) 모듈입니다. 다음 섹션에서 MSR 모듈을 소개합니다. 입력 시퀀스 x = x1 · X|x|가 주어지면 RetNet은 시퀀스를 자기 회귀 방식으로 인코딩합니다. 입력 벡터 {x;}!21이 먼저 Xo, x|x|] = R |x|xdmodel에 패킹됩니다. 여기서 dmodel은 은닉 차원입니다. 그런 다음 문맥화된 벡터 표현 X₁ = RetNet, (X¹−¹), 1 € [1, L]. = [x1, 2.1 Retention ... x 이 섹션에서는 재발과 병렬성의 이중 형태를 갖는 유지 메커니즘을 소개합니다. 따라서 추론을 반복적으로 수행하는 동안 병렬 방식으로 모델을 학습할 수 있습니다. 입력 X € R|x|xdmodel이 주어지면 이를 1차원 함수 v(n) = Xn • wy로 투영합니다. v(n) → o(n)을 상태 sn에 매핑하는 시퀀스 모델링 문제를 고려합니다. 단순화를 위해 Un, On을 v(n), o(n)으로 표시합니다. 재귀적인 방식으로 매핑을 공식화합니다. Sn = Asn−1+Kjn, On = n A Є Rdxd, Kn Є R¹×d Um, Qn Є R1xd Qnsn = QnAn- Km vm=(1) 여기서 Un을 상태 벡터 sm에 매핑한 다음 선형 변환을 구현하여 시퀀스 정보를 재귀적으로 인코딩합니다. 다음으로 투영 Qn, Kn을 콘텐츠 인식으로 만듭니다. Q XWQ, K = XWK 여기서 WQ, WK Є Rdxd는 학습 가능한 행렬입니다. 행렬 A = Aye) A-1을 대각화합니다. 여기서 Y, 0 € Rd. 그러면 An-m A(ven-mA-1을 얻습니다. A를 WQ와 WK에 흡수시키면 방정식(1)을 다음과 같이 다시 쓸 수 있습니다. n On = n(e)nm Kmum m=n = = (Qn (ve²)&quot;) (Km (ve²º)¯m)Tum m=(2) (3) 여기서 Qn (yen, Km (ye²)-m은 xPos [SDP+22]로 알려져 있습니다. 즉, Transformer에 대해 제안된 상대 위치 임베딩입니다. 스칼라로 더 단순화하면 방정식(3)은 다음과 같습니다. n On = Σγη-m(@neino °) (Kme imo) tv, vm + m=(4) 여기서 켤레 전치입니다. 이 공식은 학습 인스턴스 내에서 쉽게 병렬화할 수 있습니다. 요약하면 방정식(1)에 표시된 대로 순환 모델링으로 시작한 다음 방정식(4)에서 병렬 공식을 도출합니다. 원래 매핑 v(n) → o(n)을 벡터로 간주하고 다음을 얻습니다. 보존 메커니즘은 다음과 같습니다. 보존의 병렬 표현 그림 3a에 표시된 대로 보존 층은 다음과 같이 정의됩니다. Q = (XW₁) © ☹, K = (XWк)○ē, V = XWv Ꮎ, = eine Dnm Syn-m, n&gt;m 10, n <m (5) Retention (X) = (QKT © D)V where is the complex conjugate of O, and D = R××× combines causal masking and exponential decay along relative distance as one matrix. Similar to self-attention, the parallel representation enables us to train the models with GPUs efficiently.Q GN Sn-Recurrent γ (QKT O D)V State Κη Qn GN Sn On Output X (a) Parallel representation. Xn Input (b) Recurrent representation. Figure 3: Dual form of RetNet. “GN” is short for GroupNorm. The Recurrent Representation of Retention As shown in Figure 3b, the proposed mechanism can also be written as recurrent neural networks (RNNs), which is favorable for inference. For the n-th timestep, we recurrently obtain the output as: Sn = YSn_1+ KV Retention (X) = QnSn, n = 1, ∙∙∙‚|x| where Q, K, V, are the same as in Equation (5). (6) The Chunkwise Recurrent Representation of Retention A hybrid form of parallel representation and recurrent representation is available to accelerate training, especially for long sequences. We divide the input sequences into chunks. Within each chunk, we follow the parallel representation (Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following the recurrent representation (Equation (6)). Specifically, let B denote the chunk length. We compute the retention output of the i-th chunk via: Q[i]=QBi:B(i+1), K[i] = KBi:B(i+1), V[i] = VBi:B(i+1) B R₁ = K₁₁ (V] C) + √³ Ri-1, Sij = √B-i-Retention (X]) = (QK+] © D)V[i] + (Q[] Ri−1) ©§, _{ij = y²+[i] Inner-Chunk Cross-Chunk where [i] indicates the i-th chunk, i.e., x[i] = [X(i−1)B+1, · · ·‚ X¿B]· 2.2 Gated Multi-Scale Retention (7) We use h=dmodel/d retention heads in each layer, where d is the head dimension. The heads use different parameter matrices WQ, WK, Wy Є Rdxd. Moreover, multi-scale retention (MSR) assigns different for each head. For simplicity, we sety identical among different layers and keep them fixed. In addition, we add a swish gate [HG16, RZL17] to increase the non-linearity of retention layers. Formally, given input X, we define the layer as: Υ =headi Y = -5-arange(0,h)ЄRh Retention (X, Yi) GroupNorm (Concat(head1, ..., head)) Y)Wo MSR(X) = (swish(XWG) (8) where WG, WO ER dmodel X dmodel are learnable parameters, and GroupNorm [WH18] normalizes the output of each head, following SubLN proposed in [SPP+19]. Notice that the heads use multiple scales, which results in different variance statistics. So we normalize the head outputs separately. The pseudocode of retention is summarized in Figure 4.def Parallel Retention ( q, # bsz * num_head * len * qk_dim k, #bsz✶ num_head * len * qk_dim v, #23 bsz * num_head * len * v_dim decay_mask # num_head * len * len ): retention = q @ k.transpose(-1, -2) retention = retention * decay_mask output = retention @ v output group_norm (output) return output def Chunkwise Retention ( def Recurrent Retention ( q, k, v, #23 bsz * num_head * len * qkv_dim past_kv, #3 bsz * num_head * qk_dim✶ v_dim decay # num_head * 1 *): current_kv = decay past_kv + k.unsqueeze (-1) v.unsqueeze (-2) output = torch. sum (q. unsqueeze (-1) * current_kv, dim=-2) output group_norm (output) return output, current_kv q, k, v, # bsz * num_head * chunk_size * qkv_dim past_kv, # bsz * num_head * qk_dim * v_dim decay_mask, # num_head * chunk_size* chunk_size chunk_decay, # num_head * 1 *inner_decay, # num_head * chunk_size ): retention = q @ k.transpose(-1, -2) retention = retention * decay_mask inner retention = retention @ v cross_retention = (q past_kv) * inner_decay retention = inner_retention + cross_retention output = group_norm (retention) current kv = chunk_decay * past_kv + k.transpose(-1, -2) @ v return output, current_kv Figure 4: Pseudocode for the three computation paradigms of retention. = Retention Score Normalization We utilize the scale-invariant nature of Group Norm to improve the numerical precision of retention layers. Specifically, multiplying a scalar value within GroupNorm does not affect outputs and backward gradients, i.e., GroupNorm(a * head;) GroupNorm (head;). We implement three normalization factors in Equation (5). First, we normalize QKT as QK√d. Second, we replace D with Dnm Dnm/Dni. Third, let R denote the retention scores R QKT D, we normalize it as Rnm Rnm/max(Rni],1). Then the retention output becomes Retention (X) = RV. The above tricks do not affect the final results while stabilizing the numerical flow of both forward and backward passes, because of the scale-invariant property. = 2.3 Overall Architecture of Retention Networks = For an L-layer retention network, we stack multi-scale retention (MSR) and feed-forward network (FFN) to build the model. Formally, the input sequence {x;} ! 1 is transformed to vectors by a word embedding layer. We use the packed embeddings Xº = [x₁, · · ·, x|x|] = R|x|×dmodel as the input and compute the model output XL: Y² = MSR(LN(X²)) + X¹ X¹+¹ = FFN(LN(Y¹)) + Y¹ (9) where LN() is LayerNorm [BKH16]. The FFN part is computed as FFN(X) = gelu(XW1)W2, where W1, W2 are parameter matrices. Training We use the parallel (Equation (5)) and chunkwise recurrent (Equation (7)) representations during the training process. The parallelization within sequences or chunks efficiently utilizes GPUs to accelerate computation. More favorably, chunkwise recurrence is especially useful for long-sequence training, which is efficient in terms of both FLOPs and memory consumption.Architectures Transformer Linear Transformer Recurrent NN RWKV H3/SHyena RetNet Training Parallelization Inference Cost Long-Sequence Memory Complexity Performance O(N) O(N2) ✓✓ O(1) O(N) ✓ O(1) O(N) O(1) O(N) O(1) O(N log N) O(N) O(1) O(N log N) O(N) Table 1: Model comparison from various perspectives. RetNet achieves training parallelization, constant inference cost, linear long-sequence memory complexity, and good performance. Inference The recurrent representation (Equation (6)) is employed during the inference, which nicely fits autoregressive decoding. The O(1) complexity reduces memory and inference latency while achieving equivalent results. 2.4 Relation to and Differences from Previous Methods Table 1 compares RetNet with previous methods from various perspectives. The comparison results echo the "impossible triangle" presented in Figure 2. Moreover, RetNet has linear memory complexity for long sequences due to the chunkwise recurrent representation. We also summarize the comparisons with specific methods as follows. Transformer The parallel representation of retention shares similar spirits as Transformers [VSP+17]. The most related Transformer variant is Lex Transformer [SDP+22] which implements xPos as position embeddings. As described in Equation (3), the derivation of retention aligns with xPos. In comparison with attention, retention removes softmax and enables recurrent formulation, which significantly benefits inference. S4 Unlike Equation (2), if Qn and K are content-unaware, the formulation can be degenerated to S4 [GGR21], where O = (QKT, QAKT,.., QA|×|−¹ K¹) * V. Linear Attention The variants typically use various kernels (9;)ø(k;)/21 (9¿)ó̟(kn) to replace the softmax function. However, linear attention struggles to effectively encode position information, rendering the models less performant. Besides, we reexamine sequence modeling from scratch, rather than aiming at approximating softmax. AFT/RWKV Attention Free Transformer (AFT) simplifies dot-product attention to element-wise operations and moves softmax to key vectors. RWKV replaces AFT's position embeddings with exponential decay and runs the models recurrently for training and inference. In comparison, retention preserves high-dimensional states to encode sequence information, which contributes to expressive ability and better performance. xPos/ROPE Compared with relative position embedding methods proposed for Transformers, Equation (3) presents a similar formulation as xPos [SDP+22] and ROPE [SLP+21]. Sub-Layer Norm As shown in Equation (8), the retention layer uses Sub-LayerNorm [WMH+22] to normalize outputs. Because the multi-scale modeling leads to different variances for the heads, we replace the original LayerNorm with GroupNorm. 3 Experiments We conduct experiments on language modeling to evaluate RetNet. We evaluate the proposed architecture with various benchmarks, i.e., language modeling performance, and zero-/few-shot learning on downstream tasks. Moreover, for training and inference, we compare speed, memory consumption, and latency.Size Hidden Dim. #Layers Batch Size # Tokens Learning Rate 1.3B2.7B6.7B4M 100B 6 x-4M 100B 3 × 10-4M 100B 3 × 10-Table 2: Sizes, and learning hyper-parameters of the models in language modeling experiments. 15.14.14.13.13.012.1.3B 2.7B Model Size RetNet Transformer 6.7B Figure 5: Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B. 3.1 Setup Parameter Allocation We re-allocate the parameters in MSR and FFN for fair comparisons. Let d denote dmodel for simplicity here. In Transformers, there are about 4d² parameters in self-attention where WQ, WK, WV, Wo Є Rdxd, and 8d² parameters in FFN where the intermediate dimension is 4d. In comparison, RetNet has 8d² parameters in retention, where WQ, Wк Є Rdxd, WG, Wv € Rdx2d, WoЄ R2dxd. Notice that the head dimension of V is twice Q, K. The widened dimension is projected back to d by Wo. In order to keep the parameter number the same as Transformer, the FFN intermediate dimension in RetNet is 2d. Meanwhile, we set the head dimension to 256 in our experiments, i.e., 256 for queries and keys, and 512 for values. For fair comparison, we keep y identical among different model sizes, where x = 1 - elinspace (log 1/32,log 1/512, h) ER instead of the default value in Equation (8). Language Model Training As shown in Table 2, we train language models with various sizes (i.e., 1.3B, 2.7B, and 6.7B) from scratch. The training corpus is a curated compilation of The Pile [GBB+20], C4 [DMI+21], and The Stack [KLBA+22]. We append the <bos>토큰은 시퀀스²의 시작을 나타냅니다. 학습 배치 크기는 최대 길이가 2048인 4M 토큰입니다. 100B 토큰, 즉 25k 단계로 모델을 학습합니다. βι가 0.9, 62 = 0.98인 AdamW [LH19] 옵티마이저를 사용하고 가중치 감소는 0.05로 설정합니다. 워밍업 단계 수는 선형 학습률 감소로 375입니다. 매개변수는 학습 안정성을 보장하기 위해 DeepNet [WMD+22]에 따라 초기화됩니다. 구현은 TorchScale [MWH+22]을 기반으로 합니다. 512개의 AMD MI200 GPU로 모델을 학습합니다. = 3.2 Transformer 언어 모델링과의 비교 그림 5에서 볼 수 있듯이 Transformer 및 RetNet 기반 언어 모델의 검증 세트에 대한 복잡도를 보고합니다. 1.3B, 2.7B 및 6.7B의 세 가지 모델 크기로 스케일링 곡선을 제시합니다. RetNet은 Transformers와 비슷한 결과를 달성합니다. 더 중요한 것은 RetNet이 크기 조정 측면에서 유리하다는 것을 결과가 나타냅니다. 성능 외에도 RetNet 훈련은 실험에서 매우 안정적입니다. 실험 결과에 따르면 RetNet은 대규모 언어 모델에서 Transformer의 강력한 경쟁자입니다. 경험적으로 모델 크기가 2B보다 클 때 RetNet이 Transformer보다 성능이 뛰어나기 시작한다는 것을 알게 되었습니다. 또한 부록 B에서 다른 컨텍스트 길이의 언어 모델링 결과를 요약합니다. 2 다음을 추가하면<bos> 토큰을 처음에 사용하면 훈련 안정성과 성능에 도움이 됩니다.HS BoolQ COPA PIQA Winograd Winogrande SC Avg Zero-Shot Transformer 55.RetNet 60.7 62.4-Shot Transformer 55.62.69.74.69.56.77.75.77.58.75.0 66.76.0 69.58.71.75.71.57.75.4 66.RetNet 60.5 60.78.76.77.59.75.9 69.표 3: Transformer와 RetNet을 사용한 Zero-shot 및 few-shot 학습. 모델 크기는 6.7B입니다. 메모리(GB)↓↓ 처리량(wps) ↑ 모델 크기 Trm Trm+FlashAttn RetNet Trm Trm+FlashAttn RetNet 1.3B 74.38.34.5 10832.63965.73344.2.7B 69.42.42.5186.34990.38921.6.7B 69.51.48.2754.16230.17458.13B 61.46.45.1208.7945.8642.표 4: Transformer(Trm), FlashAttention이 있는 Transformer(Trm+FlashAttn) 및 RetNet의 학습 비용. 메모리 소비량과 학습 처리량(초당 단어 수, wps)을 보고합니다. 다운스트림 작업에서의 제로샷 및 퓨샷 평가 또한 광범위한 다운스트림 작업에서 언어 모델을 비교합니다. 6.7B 모델을 사용하여 제로샷 및 4샷 학습을 평가합니다. 표 3에서 볼 수 있듯이 데이터 세트에는 HellaSwag(HS) [ZHB+19], BoolQ [CLC+19], COPA [WPN+19], PIQA [BZB+20], Winograd, Winogrande [LDM12], StoryCloze(SC) [MRL+17]가 포함됩니다. 정확도 수치는 그림 5에 제시된 언어 모델링 복잡도와 일치합니다. RetNet은 제로샷 및 컨텍스트 내 학습 설정에서 Transformer와 비슷한 성능을 달성합니다. 3.3 훈련 비용 표 4에서 볼 수 있듯이 훈련 시퀀스 길이가 8192인 Transformer와 RetNet의 훈련 속도와 메모리 소비를 비교합니다. 또한 재계산과 커널 퓨전을 통해 속도를 개선하고 GPU 메모리 IO를 줄이는 FlashAttention [DFE+22]와 비교합니다. 이에 비해 Vanilla PyTorch 코드를 사용하여 RetNet을 구현하고 커널 퓨전이나 FlashAttention과 유사한 가속은 향후 작업으로 남겨둡니다. 식 (7)에 설명된 대로 보존의 청크별 재귀 표현을 사용합니다. 청크 크기는 512로 설정됩니다. FlashAttention은 A100에 최적화되어 있기 때문에 8개의 Nvidia A100-80GB GPU로 결과를 평가합니다. 6.7B 및 13B 모델에 대해 텐서 병렬 처리가 활성화됩니다. 실험 결과에 따르면 RetNet은 훈련 중에 Transformer보다 메모리 효율성이 높고 처리량이 높습니다. FlashAttention과 비교해도 RetNet은 속도와 메모리 비용 면에서 여전히 경쟁력이 있습니다. 게다가 특정 커널에 의존하지 않고도 다른 플랫폼에서 RetNet을 효율적으로 훈련하는 것이 쉽습니다.예를 들어, 우리는 적절한 처리량으로 AMD MIcluster에서 RetNet 모델을 훈련합니다.RetNet은 커널 퓨전과 같은 고급 구현을 통해 비용을 더욱 줄일 수 있는 잠재력이 있다는 점이 주목할 만합니다.3.4 추론 비용 그림 6에서 볼 수 있듯이 추론 중 Transformer와 RetNet의 메모리 비용, 처리량 및 대기 시간을 비교합니다.Transformer는 이전에 디코딩된 토큰의 KV 캐시를 재사용합니다.RetNet은 식 (6)에 설명된 대로 재귀 표현을 사용합니다.실험에서 A100-80GB GPU에서 6.7B 모델을 평가합니다.그림 6은 RetNet이 추론 비용 측면에서 Transformer보다 성능이 우수함을 보여줍니다.메모리 그림 6a에서 볼 수 있듯이 Transformer의 메모리 비용은 KV 캐시로 인해 선형적으로 증가합니다. 대조적으로 RetNet의 메모리 소비는 긴 시퀀스의 경우에도 일관되게 유지됩니다.GPU 메모리(GB) 모델 가중치 RetNet Transformer 2048 3072 4096 5120 6144 7168 시퀀스 길이(a) Transformer 및 RetNet의 GPU 메모리 비용. 처리량(wps) 300-200-RetNet -- Transformer 2048 3072 4096 5120 6144 7168 시퀀스 길이(b) Transformer 및 RetNet의 처리량. --Transformer(1024) 대기 시간(ms) 350Transformer(2048) 300Transformer(4096) Transformer(8192)RetNet(8192) 200-་་배치 크기(c) 다양한 배치 크기에 따른 추론 대기 시간. 그림 6: 모델 크기가 6.7B인 Transformer와 RetNet의 추론 비용. RetNet은 메모리 소비, 처리량, 대기 시간 측면에서 Transformer보다 성능이 우수합니다. RetNet을 호스팅하는 데 필요한 GPU 메모리가 훨씬 적습니다. RetNet의 추가 메모리 소비는 거의 무시할 수 있는 수준(즉, 약 3%)인 반면 모델 가중치는 97%를 차지합니다. 처리량 그림 6b에 표시된 대로 Transformer의 처리량은 디코딩 길이가 길어짐에 따라 떨어집니다. 이에 비해 RetNet은 보존의 반복적 표현을 활용하여 디코딩 중에 더 높고 길이에 관계없이 처리량을 제공합니다. 대기 시간 대기 시간은 배포에서 중요한 지표로, 사용자 경험에 큰 영향을 미칩니다. 그림 6c에서 디코딩 대기 시간을 보고합니다. 실험 결과에 따르면 배치 크기를 늘리면 Transformer의 대기 시간이 더 길어집니다. 게다가 Transformer의 대기 시간은 입력이 길수록 더 빨리 증가합니다. 대기 시간을 허용할 수 있는 수준으로 만들기 위해 배치 크기를 제한해야 하며, 이는 Transformer의 전체 추론 처리량에 해를 끼칩니다. 대조적으로 RetNet의 디코딩 지연 시간은 Transformers보다 우수하며 다양한 배치 크기와 입력 길이에서 거의 동일하게 유지됩니다.3.5 Transformer 변형과의 비교 Transformer 외에도 RetNet을 Linear Transformer [KVPF20], RWKV [PAA+23], H3 [DFS+22], Hyena [PMN+23]를 포함한 다양한 효율적인 Transformer 변형과 비교합니다.모든 모델은 16개 레이어와 1024의 숨겨진 차원을 가진 200M 매개변수를 갖습니다.H3의 경우 헤드 차원을 8로 설정합니다.RWKV의 경우 TimeMix 모듈을 사용하여 공정한 비교를 위해 FFN 레이어를 다른 모델과 일관되게 유지하면서 셀프 어텐션 레이어를 대체합니다.배치 크기가 0.5M 토큰인 10k 단계로 모델을 학습합니다.대부분의 하이퍼 매개변수와 학습 코퍼스는 섹션 3.1과 동일하게 유지됩니다. 표 5는 도메인 내 검증 세트와 기타 도메인 외 코퍼스에 대한 복잡도 숫자를 보고합니다.예: Project Gutenberg 2019-2022(PG22) [SDP+22], QMSum [ZYY+21], GovRe-Method In-Domain PG22 QMSum GovReport SummScreen RWKV 30.51.28.19.25.H29.49.24.19.25.Hyena 32.52.75 28.20.26.Linear Transformer 40.RetNet 26.63.45.27 21.28.25.32.16.22.표 5: 언어 모델링에 대한 복잡도 결과. RetNet은 도메인 내 평가 세트와 다양한 도메인 외 코퍼스에서 다른 아키텍처보다 성능이 우수합니다. 방법 RetNet - 스위시 게이트 ― GroupNorm Y 붕괴 - 다중 스케일 붕괴 헤드 차원 감소 도메인 내 PG22 QMSum GovReport SummScreen 26.45.21.16.22.27.49.22.17.23.27.46.95 22.17.23.27.47.85 21.17.23.27.47.22.17.23.27.47.23.17.23.표 6: 도메인 내 및 도메인 외 코퍼스에 대한 절제 결과. port [HCP+21], SummScreen [CCWG21, SSI+22]. 전반적으로 RetNet은 다양한 데이터 세트에서 이전 방법보다 성능이 뛰어납니다. RetNet은 도메인 내 코퍼스에서 더 나은 평가 결과를 얻을 뿐만 아니라 여러 도메인 외 데이터 세트에서 더 낮은 복잡도를 얻습니다. RetNet은 상당한 비용 절감의 이점 외에도 유리한 성능으로 Transformer의 강력한 후속 모델이 되었습니다(섹션 3.3 및 3.4). 또한 비교된 방법의 학습 및 추론 효율성에 대해 논의합니다. d는 숨겨진 차원을 나타내고 n은 시퀀스 길이를 나타냅니다. 학습을 위해 RWKV의 토큰 혼합 복잡도는 O(dn)인 반면 Hyena의 토큰 혼합 복잡도는 고속 푸리에 변환 가속을 사용하여 O(dn log n)입니다. 위의 두 방법은 요소별 연산자를 사용하여 모델링 용량을 절충하여 학습 FLOPS를 줄입니다. 보존과 비교하여 청크별 재귀 표현은 O(dn(b + h))이며, 여기서 b는 청크 크기, h는 헤드 차원이며 일반적으로 b = 512, h는 256으로 설정합니다. 큰 모델 크기(즉, 더 큰 d) 또는 시퀀스 길이의 경우 추가 b + h는 무시할 수 있는 효과를 갖습니다. 따라서 RetNet 학습은 모델링 성능을 희생하지 않고도 매우 효율적입니다. 추론을 위해 비교된 효율적 아키텍처 중 Hyena는 Transformer와 동일한 복잡도(즉, 단계당 O(n))를 갖는 반면 다른 아키텍처는 O(1) 디코딩을 수행할 수 있습니다.3.6 Ablation Studies = RetNet의 다양한 디자인 선택을 ablation하고 언어 모델링 결과를 표 6에 보고합니다.평가 설정과 메트릭은 섹션 3.5와 동일합니다.아키텍처 식 (8)에서 설명한 대로 swish gate와 GroupNorm을 ablation합니다.표는 위의 두 구성 요소가 최종 성능을 개선한다는 것을 보여줍니다.첫째, 게이팅 모듈은 비선형성을 강화하고 모델 기능을 개선하는 데 필수적입니다.게이트를 제거한 후 Transformer와 동일한 매개변수 할당을 사용한다는 점에 유의하세요.둘째, 보존의 그룹 정규화는 다중 헤드 출력의 분산을 균형 있게 조정하여 학습 안정성과 언어 모델링 결과를 개선합니다.66-다중 스케일 감소 식 (8)은 보존 헤드의 감소 속도로 다른 &amp;를 사용한다는 것을 보여줍니다. 절제 연구에서는 y 붕괴(즉, &quot;- y 붕괴&quot;)를 제거하고 헤드 전체에 동일한 붕괴율(즉, 다중 스케일 붕괴)을 적용하는 것을 살펴봅니다. 구체적으로 붕괴를 절제하는 것은 γ = 1과 같습니다. 두 번째 설정에서 모든 헤드에 대해 y = 127/128로 설정합니다. 표 6은 붕괴 메커니즘과 다중 붕괴율 사용 모두 언어 모델링 성능을 개선할 수 있음을 나타냅니다. 헤드 차원 방정식(1)의 재귀적 관점에서 헤드 차원은 숨겨진 상태의 메모리 용량을 의미합니다. 절제 연구에서 기본 헤드 차원을 256에서 64로 줄입니다. 즉, 쿼리와 키의 경우 64, 값의 경우 128입니다. 헤드 수가 증가하도록 숨겨진 차원 dmodel을 동일하게 유지합니다. 표 6의 실험 결과는 헤드 차원이 클수록 성능이 더 좋아짐을 보여줍니다. 4
--- CONCLUSION ---
이 연구에서 우리는 시퀀스 모델링을 위한 보존 네트워크(RetNet)를 제안하는데, 이는 병렬, 순환, 청크별 순환 등 다양한 표현을 가능하게 합니다. RetNet은 Transformers에 비해 상당히 더 나은 추론 효율성(메모리, 속도, 지연 시간 측면에서), 유리한 학습 병렬화, 경쟁력 있는 성능을 달성합니다. 위의 장점들은 RetNet을 대규모 언어 모델을 위한 Transformers의 이상적인 후속 모델로 만듭니다. 특히 O(1) 추론 복잡도가 가져온 배포 이점을 고려할 때 더욱 그렇습니다. 앞으로 우리는 모델 크기[CDH+22]와 학습 단계 측면에서 RetNet을 확장하고 싶습니다. 게다가 보존은 장기 메모리를 압축함으로써 구조화된 프롬프트[HSD+22b]와 효율적으로 작동할 수 있습니다. 또한 우리는 RetNet을 백본 아키텍처로 사용하여 멀티모달 대규모 언어 모델[HSD+22a, HDW+23, PWD+23]을 학습할 것입니다. 또한 우리는 모바일 폰과 같은 다양한 에지 디바이스에 RetNet 모델을 배포하는 데 관심이 있습니다. 감사의 말 MSRA System Group의 Jiayu Ding, Songlin Yang 및 동료들에게 유익한 토론에 대한 감사를 표합니다. 참고문헌 [BKH16] Jimmy Lei Ba, Jamie Ryan Kiros 및 Geoffrey E Hinton. 레이어 정규화. arXiv 사전 인쇄본 arXiv:1607.06450, 2016. [BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc., 2020. [BZB+20] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, Yejin Choi. Piqa: 자연어에서 물리적 상식에 대한 추론. 제34회 AAAI 인공지능 컨퍼런스, 2020. [CCWG21] Mingda Chen, Zewei Chu, Sam Wiseman, Kevin Gimpel. Summscreen: 추상적 시나리오 요약을 위한 데이터 세트. arXiv 사전 인쇄본 arXiv:2104.07091, 2021. [CDH+22] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, Furu Wei. 전문가의 희소한 혼합의 표현 붕괴에 관하여. 신경 정보 처리 시스템의 발전, 2022. [CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova. BoolQ: 자연스러운 예/아니요 질문의 놀라운 어려움 탐구. 2019년 북미 컴퓨터 언어학회 학술대회 회의록, 2924-2936쪽, 2019. [DFE+22] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré. Flashattention: io-awareness를 갖춘 빠르고 메모리 효율적인 정확한 주의. 신경 정보 처리 시스템의 발전, 35:16344-16359, 2022. [DFS+22] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, Christopher Ré. 배고픈 하마: 상태 공간 모델을 이용한 언어 모델링을 향하여. arXiv 사전 인쇄본 arXiv:2212.14052, 2022.[DMI+21] Jesse Dodge, Ana Marasović, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 대용량 웹텍스트 코퍼스 문서화: 거대한 클린 크롤링 코퍼스에 대한 사례 연구. 자연어 처리 경험적 방법에 대한 컨퍼런스, 2021. [GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: 언어 모델링을 위한 다양한 텍스트의 800GB 데이터 세트. arXiv 사전 인쇄본 arXiv:2101.00027, 2020. [GGR21] Albert Gu, Karan Goel, and Christopher Ré. 구조화된 상태 공간을 사용하여 긴 시퀀스를 효율적으로 모델링합니다. arXiv 사전 인쇄본 arXiv:2111.00396, 2021. [HCP+21] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 긴 문서 요약을 위한 효율적인 주의. arXiv 사전 인쇄본 arXiv:2104.02112, 2021. [HDW+23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. 언어만 필요한 것은 아닙니다. 언어 모델에 맞춰 인식하기. ArXiv, abs/2302.14045, 2023. [HG16] Dan Hendrycks 및 Kevin Gimpel. 가우스 오차 선형 단위(GELU). arXiv: 학습, 2016. [HS97] Sepp Hochreiter 및 Jurgen Schmidhuber. 장단기 기억. 신경 계산, 9:1735-1780, 1997년 11월. [HSD+22a] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma 및 Furu Wei. 언어 모델은 범용 인터페이스입니다. ArXiv, abs/2206.06336, 2022. [HSD+22b] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu 및 Furu Wei. 구조화된 프롬프트: 1,000개 예제로 컨텍스트 내 학습 확장. ArXiv, abs/2212.06713, 2022. [KLBA+22] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, Harm de Vries. 스택: 3TB의 허가된 소스 코드. 사전 인쇄본, 2022. [KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret. Transformers are rnns: 선형적 주의가 있는 빠른 자기 회귀 변환기. 기계 학습 국제 컨퍼런스, 5156-5165페이지. PMLR, 2020. [LDM12] Hector Levesque, Ernest Davis, Leora Morgenstern. Winograd 스키마 챌린지. 2012년 지식 표현 및 추론 원리에 관한 제13회 국제 컨퍼런스에서. [LH19] Ilya Loshchilov, Frank Hutter. 분리된 가중치 감소 정규화. 2019년 학습 표현에 관한 국제 컨퍼런스에서. [MRL+17] Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, James Allen. Lsdsem 2017 공유 과제: 스토리 빈칸 채우기 테스트. 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics의 회의록, 46-51쪽, 2017년. [MWH+22] Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong, Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, Furu Wei. TorchScale: 규모에 따른 변압기. CoRR, abs/2211.13184, 2022. [OSG+23] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, Soham De. 긴 시퀀스에 대한 순환 신경망 부활. ArXiv, abs/2303.06349, 2023.[PAA+23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu 및 Rui-Jie Zhu. Rwkv: 트랜스포머 시대를 위한 rnns 재창조, 2023. [PMN+23] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré. 하이에나 계층: 더 큰 합성 언어 모델을 향해. arXiv 사전 인쇄본 arXiv:2302.10866, 2023. [PWD+23] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei. Kosmos-2: 다중 모드 대규모 언어 모델을 세계에 접지. ArXiv, abs/2306.14824, 2023. [RZL17] Prajit Ramachandran, Barret Zoph, Quoc V. Le. Swish: 자체 게이트 활성화 함수. arXiv: Neural and Evolutionary Computing, 2017. [SDP+22] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, Furu Wei. 길이 외삽 가능한 변압기. arXiv 사전 인쇄본 arXiv:2212.10554, 2022. [Sha19] Noam M. Shazeer. 빠른 변압기 디코딩: 쓰기 헤드 하나만 있으면 됩니다. ArXiv, abs/1911.02150, 2019. [SLP+21] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu. Roformer: 회전 위치 임베딩이 있는 향상된 변압기. arXiv 사전 인쇄본 arXiv:2104.09864, 2021. [SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM: 모델 병렬성을 사용하여 수십억 개의 매개변수 언어 모델 학습. arXiv 사전 인쇄본 arXiv:1909.08053, 2019. [SSI+22] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: 긴 언어 시퀀스에 대한 표준화된 비교. arXiv 사전 인쇄본 arXiv:2201.03533, 2022. [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 주의만 기울이면 됩니다. 신경 정보 처리 시스템의 발전 30: 신경 정보 처리 시스템 연례 컨퍼런스 2017, 2017년 12월 4-9일, 미국 캘리포니아주 롱비치, 60006010페이지, 2017. [WH18] Yuxin Wu, Kaiming He. 그룹 정규화. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 3-19페이지, 2018. [WMD+22] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei. DeepNet: Transformers를 1,000개 레이어로 확장. ArXiv, abs/2203.00555, 2022. [WMH+22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers. arXiv 사전 인쇄본 arXiv:2210.06423, 2022. [WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman. SuperGLUE: 범용 언어 이해 시스템을 위한 더 끈적끈적한 벤치마크. arXiv 사전 인쇄 arXiv:1905.00537, 2019. [ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi 및 최예진. Hellaswag: 기계가 정말로 당신의 문장을 완성할 수 있나요? 전산 언어학 협회 제57차 연례 회의 진행, 2019. [ZYY+21] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: 쿼리 기반 다중 도메인 회의 요약을 위한 새로운 벤치마크입니다. arXiv 사전 인쇄본 arXiv:2104.05938, 2021.A 하이퍼파라미터 하이퍼파라미터 1.3B 2.7B 6.7B 레이어 숨겨진 크기 FFN 크기 헤드 학습률 6 x3 x 10 3 × 10-LR 스케줄러 워밍업 단계 배치당 토큰 Adam B 학습 단계 그래디언트 클리핑 드롭아웃 가중치 감소 다항식 감소 4M (0.9, 0.98) 25,2.0.0.표 7: 섹션 3의 모델에 사용된 하이퍼파라미터.B 다른 컨텍스트 길이의 그룹화된 결과 표 8에서 볼 수 있듯이, 다른 컨텍스트 길이에 대한 언어 모델링 결과를 보고합니다. 숫자를 비교하기 위해 2048개의 텍스트 청크를 평가 데이터로 사용하고 마지막 128개 토큰에 대해서만 복잡도를 계산합니다. 실험 결과에 따르면 RetNet은 다른 컨텍스트 길이에서 Transformer보다 성능이 뛰어납니다. 게다가 RetNet은 더 나은 결과를 위해 더 긴 컨텍스트를 활용할 수 있습니다. 모델 512 1024Transformer 13.55 12.56 12.RetNet 13.09 12.14 11.표 8: 컨텍스트 길이가 다른 RetNet과 Transformer의 언어 모델링 복잡도. 결과는 RetNet이 시퀀스 길이에 걸쳐 일관된 이점을 가지고 있음을 보여줍니다.
