--- ABSTRACT ---
3진 및 2진 신경망은 곱셈이 필요 없는 계산을 가능하게 하며, 특수 하드웨어에 구현하는 경우 전체 정밀도 네트워크보다 여러 배의 효율성 향상을 약속합니다. 그러나 매개변수와 출력 공간이 모두 고도로 이산화되어 있기 때문에 이러한 네트워크는 최적화하기가 매우 어려운 것으로 입증되었습니다. 주의 연산이 양자화에 민감하고 고차수 출력 공간에서 자기 회귀 디코딩의 노이즈 합성 효과로 인해 변환기 텍스트 생성 모델 클래스의 어려움이 가중됩니다. 우리는 가중치에 대한 통계 기반 양자화와 활성화의 탄성 양자화를 혼합하여 문제에 접근하고 요약 및 기계 번역의 다운스트림 작업에 대한 최초의 3진 및 2진 변환기 모델을 보여줍니다. 우리의 3진 BART 기반은 CNN/DailyMail 벤치마크에서 R1 점수를 달성하는데, 이는 전체 모델보다 불과 3.9포인트 뒤떨어져 있지만 효율성은 16배 더 높습니다. 우리의 2진 모델은 정확도는 떨어지지만 35.6이라는 매우 사소한 점수를 달성합니다. 기계 번역의 경우, 우리는 WMT16 En-Ro 벤치마크에서 BLEU 점수 21.7과 17.6을 달성했고, 이는 전체 정밀도 mBART 모델 점수 26.8과 비교됩니다. 또한 8비트 활성화 설정에서 우리의 접근 방식을 비교했는데, 여기서 우리의 3진 및 2진 가중치 모델은 문헌에서 가장 우수한 기존 8비트 가중치 모델과 일치하거나 더 나은 성능을 낼 수 있습니다. 우리의 코드와 모델은 다음에서 제공됩니다: https://github.com/facebookresearch/ Ternary_Binary_Transformer. 1
--- INTRODUCTION ---
생성적 사전 훈련된 변환기(Brown et al., 2020; Lewis et al., 2020; Radford et al., 2018)는 강력하고 일반적인 도구로 등장하여 언어 이해뿐만 아니라 일반적으로 AI 분야에서 획기적인 발전을 이루었습니다. 이러한 모델은 주로 점점 더 큰 데이터와 모델 크기에 맞게 확장할 수 있는 겉보기에 무한한 능력에 성공했습니다. 안타깝게도 이러한 확장은 대규모 계산 요구 사항의 대가로 이루어지며, 리소스가 가장 풍부한 기관을 제외한 모든 기관에서 광범위하게 대규모 생성 변환기를 사용할 수 없게 됩니다. 중간 크기의 사전 훈련된 변환기조차도 크기와 계산 비용으로 인해 적용 범위가 제한적입니다. 생성 변환기를 보다 효율적으로 만드는 것은 더 많은 장치와 실용적인 응용 프로그램으로 사용을 확대하는 데 필수적입니다. 이 작업에서 우리는 가중치와 활성화의 양자화를 통해 생성적 사전 훈련된 변환기를 보다 효율적으로 만드는 방법을 탐구합니다. 신경망의 가중치를 양자화하는 것은 압축에 유용하며 모델을 보다 효율적으로 저장할 수 있게 합니다. 그러나 압축만으로는 네트워크의 활성화를 완전한 정밀도로 계산해야 하기 때문에 계산 비용을 줄일 수 없습니다. 가중치와 활성화를 모두 양자화하면 더 낮은 정밀도로 계산을 수행할 수 있어 양자화 수준과 하드웨어 구현에 따라 상당한 효율성 향상으로 이어질 수 있습니다. 신경망 양자화는 오랜 역사를 가지고 있으며, 여러 연구에서 다양한 양자화 수준에서 사전 학습된 변환기를 양자화하려고 시도했습니다(Shen et al., 2020; Zhang et al., 2020; Liu et al., 2022; Qin et al., 2021). 이 작업의 대부분은 문장 및 토큰 분류 작업을 위한 인코더 전용 모델(주로 BERT)에 초점을 맞춥니다. 텍스트 생성 모델을 양자화하는 것은 일반적으로 출력 어휘가 크고 순차적 디코딩이 필요하기 때문에 더 어려운 작업으로 간주되었습니다(Behnke et al., 2021; Tao et al., 2022). 최근 연구에서는 이 문제를 다루었지만, 경미한 양자화 수준(8비트 활성화까지)에 한해 성공 여부가 엇갈렸습니다. 반면, 저희는 3진법 및 2진법 가중치와 활성화까지 매우 낮은 비트 양자화에 관심이 있습니다. 이를 달성하기 위해 저희는 가중치 및 활성화 양자화에 대한 모범 사례를 결합하고 통합하고 가중치에 대한 그래디언트 매칭 양자화와 활성화에 대한 탄력적 양자화를 사용하는 프레임워크를 제시합니다. 저희는 저희 방법을 자연어 생성 작업에 적용하고, 처음으로 경쟁력 있는 정확도의 낮은 비트 생성 변환기를 시연합니다. 저희의 3진법(가중치 및 활성화) 모델은 XSUM 요약 데이터 세트에서 ROUGE에서 전체 정밀도 BART(Lewis et al., 2020) 모델보다 4포인트만 뒤처집니다. 반면, 3진법 가중치와 8비트 활성화를 사용한 저희 모델은 1포인트 이내에 들어오고 8비트 가중치를 사용한 최신 모델보다 성능이 더 좋습니다. 또한 완전한 이진(가중치 및 활성화) 모델을 시연합니다. 경쟁력은 떨어지지만 31.7이라는 매우 비자명한 ROUGE-1 점수를 달성할 수 있습니다. 저희의 결과는 기계 번역 모델에도 적용됩니다. WMT16 En-Ro 벤치마크에서 mBART 모델을 양자화하여 3진 가중치 8비트 활성화 SoTA를 1.2포인트 확장하는 동시에 처음으로 완전한 3진 및 완전한 이진 번역 모델을 시연합니다. 저희의 기여를 요약하면 다음과 같습니다. • 통계 기반 가중치 양자화와 학습 기반 활성화 양자화의 새로운 조합을 제안하여 트랜스포머 인코더-디코더 모델을 완전히 3진/2진 설정으로 수렴하도록 안정적으로 학습할 수 있는데, 이는 이전에는 불가능했습니다. • 8비트 활성화 및 3진/2진 가중치 설정에서 최첨단 텍스트 생성 모델을 크게 개선하는 동시에 완전한 3진 및 완전한 이진 설정에 대한 최초의 비자명한 기준선을 설정합니다. 2 방법 이 섹션에서는 먼저 이진화 및 삼진화의 이전 관행을 소개합니다. 그런 다음 그래디언트 불일치 문제를 완화하고 양자화된 가중치 엔트로피를 향상시킬 수 있는 통합된 통계 기반 가중치 이진화/삼진화 방법을 소개합니다. 마지막으로 가중치 양자화와 활성화 양자화의 차이를 분석하고 활성화를 위한 탄력적 삼진화 방법을 제안합니다. 이 방법을 TBT(Ternary/Binary Transformer)로 축약합니다. 2.1 예비 2.1.1 삼진화 실수 값이 3단계로 양자화된 삼진 신경망은 (Li et al., 2016)에서 처음 소개되었습니다. 따라서 이러한 값은 2비트로 표현될 수 있어 크기와 계산량이 16배 감소합니다. 게다가 계산은 곱셈 없이 계산될 수 있어 적합한 하드웨어에서 더욱 많은 계산 이득을 얻을 수 있습니다. 최근 연구에서는 자연어 모델에서 3진화 알고리즘을 통합하여 분류 작업에서 가중치와 활성화를 양자화하고(Zhang et al., 2020) 생성 모델에서 가중치(8비트 활성화 사용)를 3진화합니다(Li et al., 2022; Tao et al., 2022). 3진화에 대한 일반 공식(Li et al., 2016)은 다음과 같습니다. X &lt; -A인 경우 X 0, - A≤ X ≤AR인 경우 (1) XR &gt; A인 경우 0.7 ||XR||Δ (2) NXR αT .ΣXR 1XR&gt;A Σ1XR&gt;A 여기서 XT는 3진 가중치/활성화를 나타내고 XR은 실수 값 대응 항목을 나타냅니다. NXR은 텐서의 총 요소 수를 나타냅니다. A는 3진 임계값이고 α는 XT와 XR 사이의 12-손실을 최소화하는 스케일링 인수입니다. 2.1. 이진화 신경망 이진화는 가중치 및/또는 활성화를 2단계 값으로 표현하는 것을 의미합니다. 이는 BNN(Courbariaux et al., 2016)에서 처음 제안되었으며 후속 작업(Rastegari et al., 2016; Liu et al., 2018)에서 발전했습니다. Rastegari et al. (2016)은 이진화를 다음과 같이 공식화합니다. X = α Sign(X) (4) -αB&quot; XR &lt;+α인 경우, X≥R ||XR| |(5) ав NXR 여기서 XB는 이진 가중치 또는 이진 활성화를 나타낼 수 있습니다. a는 XR과 α Sign(XR) 사이의 12 손실을 최소화하는 스케일링 인수를 나타냅니다. 3진/이진 신경망의 가속 및 압축 효과는 중요합니다. 가중치와 활성화를 {-1,0,1}로 표현함으로써 네트워크는 32비트 부동 소수점 대응 제품에 비해 약 16배의 메모리를 절약할 수 있습니다. 가중치와 활성화를 1비트(즉, {−1, 1})로만 추가로 이진화하면 모델 크기가 최대 32배 감소하고 CPU 속도가 58배 향상됩니다(Rastegari et al., 2016). 여기서 행렬 곱셈 연산은 가벼운 비트 단위 XNOR 연산으로 대체됩니다. 매력적인 특성에도 불구하고 순진하게 자연어 생성을 위해 변환기 모델을 이진화 또는 삼진화하면 정확도가 여러 번 떨어지거나 학습이 완전히 실패할 수 있습니다. 변환기 네트워크의 어텐션 계층은 낮은 비트로 양자화하기 어렵다는 것이 관찰되었습니다. 또한, 자기 회귀 디코딩은 양자화로 인해 오류가 축적되는 경향이 있습니다. 고정밀 출력이 필요한 생성 언어 네트워크의 특성을 감안할 때 이러한 모델의 활성화와 가중치를 모두 극단적인 비트 값으로 양자화하는 것은 사소한 일이 아니며 이전에는 탐구되지 않았습니다. 2.2 통계 기반 최대 엔트로피 등척성 가중치 양자화 가중치 이진화/삼진화를 위한 통계 기반 방법을 제안합니다. 특히, 이 새로운 양자화 방법은 양자화된 가중치의 엔트로피를 최대화하고 역방향 패스에서 그래디언트 불일치를 줄이는 것을 고려합니다. 이전 연구(Courbariaux 등, 2016; Bai 등, 2021b; Zhang 등, 2020)는 주로 양자화된 가중치와 실수 값 가중치 사이의 12 손실을 최소화하여 최적의 양자화 방식 a* (6) arg min ||aŴQ WR||여기서 ŴQ는 이진/삼진 가중치를 나타내고 a*는 계산된 최적의 스케일링 인수를 나타냅니다. 고전적 양자화 방식의 광범위한 적용과 큰 성공에도 불구하고, 우리는 단순히 12 손실을 최소화하는 것이 초저비트 가중치 양자화에서 몇 가지 중요하면서도 난제를 무시한다는 것을 발견했습니다.(1) 양자화된 가중치의 정보 엔트로피는 고려되지 않습니다. Eq. 1과 Eq. 4는 실수 값 가중치까지의 거리를 최소화하기 위해 양자화된 가중치를 계산하는데, 이는 양자화된 가중치 분포의 불균형을 초래하고 양자화된 가중치 표현 용량을 손상시킬 수 있습니다. (2) 양자화 함수 Eq. 1과 Eq. 4는 등거리가 아니며, 즉 양자화된 가중치와 실수 값 가중치 간의 크기 일관성을 고려하지 않지만, 우리는 크기 일관성이 정확한 기울기 추정에 상당히 기여한다는 것을 발견했습니다. 이전 솔루션의 위의 두 가지 한계를 고려하여 정보 엔트로피를 향상시키고 기울기 불일치를 줄이는 새로운 양자화 함수를 설계하고자 합니다. 정보 이론에서 가중치 표현 기능을 높이기 위해 양자화된 가중치에 더 높은 엔트로피가 포함되어 있을 때 더 많은 정보가 보존됩니다. N = −pi log(pi), st Pi max H = Pi i== (7) 여기서 på는 전체 N 수준에서 실수 값 가중치가 i번째 양자화 수준으로 양자화되는 비율을 나타냅니다. Eq. 7은 라그랑주 승수와 최적의 p ¼, i = {1, 2, ……., N}로 쉽게 풀 수 있으며, 이는 최대 정보 엔트로피를 보존하는 가장 좋은 양자화 계획이 모든 양자화 레벨에서 실수 값 가중치를 가능한 한 균등하게 분배하는 것임을 시사합니다. 이전 이진화 작업(Liu et al., 2020b)에서 제안한 대로 그래디언트 불일치를 줄이기 위해 양자화된 가중치와 실수 값 가중치 간의 크기 차이는 그래디언트 스케일에 큰 영향을 미치고 크기의 불일치는 역전파에서 증폭되어 학습 중에 그래디언트 소멸 또는 폭발을 유발합니다. 따라서 실수 값 가중치와 양자화된 가중치의 크기가 일관되도록 하는 것이 중요합니다. 위에서 논의한 두 가지 요구 사항을 결합하여 최대 엔트로피 등척성 가중치 양자화를 제안했습니다. 3진화에서는 다음과 같이 공식화됩니다.여기서 μT WT = W α Clip(R-μT 1)] ατ WR (8) 4 ||WRμT||NWR αT = 여기서 WT와 WR은 각각 3진 가중치와 실수 값 가중치를 나타냅니다.반올림 함수 [.]와 Clip() 함수는 가중치를 {-1,0,1}로 양자화합니다.μ는 실수 값 가중치의 평균이며 이제 가중치 행렬의 가중치 수를 나타냅니다.스케일링 계수 a는 가중치 통계에서 계산되고 엔트로피 규칙에 따라 실수 값 가중치 WR을 양자화 수준에서 균등하게 분포되도록 스케일링합니다.3진 경우 가중치는 {-α, 0, α}로 양자화됩니다. 실수 값 가중치가 균일하고 대칭적으로 분포되도록 초기화되면(He et al., 2015; Glorot and Bengio, 2010), 스케일링 인자는 αT를 [-1.5, 1.5]로 분포시켜 출력 3진 가중치 Wi аt R 통계 기반 가중치 양자화 Q 양자화된 임베딩 학습 기반 대칭 활성화 양자화 통계 기반 가중치 양자화 K 학습 기반 학습 기반 대칭 대칭 활성화 양자화 활성화 양자화 소프트맥스 학습 기반 비대칭 활성화 양자화 {0,1,...} 학습 기반 대칭 활성화 양자화 통계 기반 가중치 양자화 V 학습 기반 대칭 활성화 양자화 통계 기반 가중치 양자화 FC ReLU 학습 기반 비대칭 활성화 양자화 {0,1,...} 학습 기반 대칭 활성화 양자화 통계 기반 가중치 → 양자화 FC 통계 기반 가중치 양자화 FC 피드포워드 네트워크 셀프 어텐션 트랜스포워드 블록 출력 그림 1: TBT 개요. 트랜스포머 블록은 멀티헤드 셀프 어텐션 및 피드포워드 네트워크를 포함합니다. 가중치 3진화/2진화를 위한 통계 기반 양자화 방법을 제안하고 ReLU/Softmax 출력(X = R2)에서 활성화를 위한 학습 기반 비대칭 양자화 방법과 다른 레이어에서 양수 및 음수 값을 모두 포함하는 활성화(X = R^)에 대한 학습 기반 비대칭 양자화 방법을 채택합니다. αT는 3개의 3진 수준에서 거의 균일한 분포를 갖습니다. 한편, Eq. 8은 실수 값 가중치가 1로 조정되어 [-1, 1]에 가까워지고 양자화 후 시간 α가 축소되는 등거리 사상입니다. 이런 식으로 크기가 유지됩니다. 이에 따라 2진의 경우 다음이 있습니다. WB = QB Sign(여기서 μB ав . = WR - μB) WR, ав ||WR-B|| Ꭱ NWR (9) 여기서 WB는 이진 가중치를 나타내며, 평균 μ를 빼면 이진화 전에 실수 값 가중치가 0 중심이 되어 이진화된 가중치가 균등하게 분포됩니다.그러면 스케일링 계수 QB가 실수 값 가중치와 이진 가중치 사이의 크기와 일치합니다.특히, Eq. 9에서 W QB Sign(. . (WB) = α Sign (WR - HB), aB는 이진화 함수를 등각 투영으로 유지하기 위해 분모에 α를 명시적으로 포함하고 가중치에 대한 그래디언트는 다음과 같이 간단히 계산할 수 있습니다.OW STE ≈ᎧᎳ . R wi R -HB aB (10) |
--- RELATED WORK ---
양자화는 오랫동안 신경망을 더 효율적으로 만들기 위해 연구되어 왔습니다(조사는 (Hubara et al., 2017) 참조). BERT의 인기로 인해 수많은 연구에서 변환기 모델에 대한 양자화를 연구했으며, 8비트 양자화(Zafrir et al., 2019; Fan et al., 2020)로 시작하여 4비트(Shen et al., 2020; Zadeh et al., 2020), 3진(Zhang et al., 2020) 및 2진(Bai et al. (2021b); Qin et al. (2021); Liu et al. (2022)으로 진행되었습니다. 이러한 모든 연구는 인코더 전용 설정에 초점을 맞추었습니다. 생성 설정에서 Prato et al. (2019); Behnke et al. (2021)은 기계 번역을 위한 양자화된 모델을 시연했으며 Fan et al. (2020); Bai et al. (2021a) 언어 모델링을 위한 것이지만, 적당한 양자화 수준(4-8비트)에만 해당합니다. 가장 최근에 Tao et al. (2022)과 Li et al. (2022)은 가중치 양자화를 2비트(8비트 활성화 양자화 포함)로 낮추고 언어 모델링과 요약에 대해 평가했습니다. 그러나 우리의
--- METHOD ---
자연어 생성 작업에 적용하고, 처음으로 경쟁력 있는 정확도의 저비트 생성 변환기를 보여줍니다. 저희의 3진(가중치 및 활성화) 모델은 XSUM 요약 데이터 세트의 ROUGE에서 전체 정밀도 BART(Lewis et al., 2020) 모델보다 4포인트만 뒤처집니다. 반면, 3진 가중치와 8비트 활성화를 사용한 저희 모델은 1포인트 이내에 들어오고 8비트 가중치를 사용한 최신 모델보다 성능이 뛰어납니다. 저희는 또한 완전 2진(가중치 및 활성화) 모델을 보여줍니다. 경쟁력은 떨어지지만, 31.7이라는 매우 사소하지 않은 ROUGE-1 점수를 달성할 수 있습니다. 저희의 결과는 기계 번역 모델에도 적용됩니다. WMT16 En-Ro 벤치마크에서 mBART 모델을 양자화하여 3진 가중치 8비트 활성화 SoTA를 1.2포인트 확장하는 동시에 처음으로 완전 3진 및 완전 2진 번역 모델을 보여줍니다. 우리는 다음과 같이 우리의 기여를 요약합니다. • 우리는 통계 기반 가중치 양자화와 학습 기반 활성화 양자화의 새로운 조합을 제안합니다. 이를 통해 변압기 인코더-디코더 모델이 이전에는 불가능했던 완전한 3진/2진 설정으로 수렴하도록 안정적으로 학습할 수 있습니다. • 우리는 완전한 3진 및 완전한 2진 설정에 대한 최초의 비자명한 기준선을 설정하는 동안 8비트 활성화 및 3진/2진 가중치 설정에서 최첨단 텍스트 생성 모델을 크게 개선합니다. 2 방법 이 섹션에서는 먼저 이진화 및 3진화의 이전 관행을 소개합니다. 그런 다음 그래디언트 불일치 문제를 완화하고 양자화된 가중치 엔트로피를 향상시킬 수 있는 통합된 통계 기반 가중치 이진화/3진화 방법을 소개합니다. 마지막으로 우리는 가중치 양자화와 활성화 양자화의 차이를 분석하고 활성화를 위한 탄력적 3진화 방법을 제안합니다. 우리는 우리의 방법을 TBT, 즉 &quot;Ternary / Binary Transformer&quot;로 약칭합니다. 2.1 예비 2.1.1 3진화 실수 값을 3단계로 양자화하는 3진 신경망은 (Li et al., 2016)에서 처음 소개되었습니다. 따라서 이러한 값을 2비트로 표현할 수 있어 크기와 계산량이 16배 감소합니다. 게다가 곱셈 없이 계산할 수 있어 적합한 하드웨어에서 계산량을 더욱 늘릴 수 있습니다. 최근 연구에서는 분류 작업에서 가중치와 활성화를 양자화하기 위해(Zhang et al., 2020) 자연어 모델에서 3진화 알고리즘을 통합하고 생성 모델에서 가중치(8비트 활성화 사용)를 3진화합니다(Li et al., 2022; Tao et al., 2022). 3진화에 대한 일반 공식(Li et al., 2016)은 다음과 같습니다. X &lt; -A인 경우 - A≤ X ≤AR인 경우 (1) XR &gt; A인 경우 0.7 ||XR||Δ (2) NXR αT .ΣXR 1XR&gt;A Σ1XR&gt;A여기서 XT는 3진 가중치/활성화를 나타내고 XR은 실수 값을 갖는 대응 항목을 나타냅니다. NXR은 텐서의 총 요소 수를 나타냅니다. A는 3진 임계값이고 α는 XT와 XR 사이의 12-손실을 최소화하는 스케일링 인수입니다. 2.1. 이진화 신경망 이진화는 가중치 및/또는 활성화를 2진 값 값으로 나타내는 것을 나타냅니다. 이는 BNN(Courbariaux et al., 2016)에서 처음 제안되었으며 후속 작업(Rastegari et al., 2016; Liu et al., 2018)에서 발전되었습니다. Rastegari et al. (2016)은 이진화를 다음과 같이 공식화합니다. X = α Sign(X) (4) -αB&quot; XR &lt;+α인 경우, X≥R ||XR| |(5) ав NXR 여기서 XB는 이진 가중치 또는 이진 활성화를 나타낼 수 있습니다. a는 XR과 α Sign(XR) 사이의 12 손실을 최소화하는 스케일링 인수를 나타냅니다. 3진/이진 신경망의 가속 및 압축 효과는 중요합니다. 가중치와 활성화를 {-1,0,1}로 표현함으로써 네트워크는 32비트 부동 소수점 대응 제품에 비해 약 16배의 메모리를 절약할 수 있습니다. 가중치와 활성화를 1비트(즉, {−1, 1})로만 추가로 이진화하면 모델 크기가 최대 32배 감소하고 CPU 속도가 58배 향상됩니다(Rastegari et al., 2016). 여기서 행렬 곱셈 연산은 가벼운 비트 단위 XNOR 연산으로 대체됩니다. 매력적인 특성에도 불구하고 순진하게 자연어 생성을 위해 변환기 모델을 이진화 또는 삼진화하면 정확도가 여러 번 떨어지거나 학습이 완전히 실패할 수 있습니다. 변환기 네트워크의 어텐션 계층은 낮은 비트로 양자화하기 어렵다는 것이 관찰되었습니다. 또한, 자기 회귀 디코딩은 양자화로 인해 오류가 축적되는 경향이 있습니다. 고정밀 출력이 필요한 생성 언어 네트워크의 특성을 감안할 때 이러한 모델의 활성화와 가중치를 모두 극단적인 비트 값으로 양자화하는 것은 사소한 일이 아니며 이전에는 탐구되지 않았습니다. 2.2 통계 기반 최대 엔트로피 등척성 가중치 양자화 가중치 이진화/삼진화를 위한 통계 기반 방법을 제안합니다. 특히, 이 새로운 양자화 방법은 양자화된 가중치의 엔트로피를 최대화하고 역방향 패스에서 그래디언트 불일치를 줄이는 것을 고려합니다. 이전 연구(Courbariaux 등, 2016; Bai 등, 2021b; Zhang 등, 2020)는 주로 양자화된 가중치와 실수 값 가중치 사이의 12 손실을 최소화하여 최적의 양자화 방식 a* (6) arg min ||aŴQ WR||여기서 ŴQ는 이진/삼진 가중치를 나타내고 a*는 계산된 최적의 스케일링 인수를 나타냅니다. 고전적 양자화 방식의 광범위한 적용과 큰 성공에도 불구하고, 우리는 단순히 12 손실을 최소화하는 것이 초저비트 가중치 양자화에서 몇 가지 중요하면서도 난제를 무시한다는 것을 발견했습니다.(1) 양자화된 가중치의 정보 엔트로피는 고려되지 않습니다. Eq. 1과 Eq. 4는 실수 값 가중치까지의 거리를 최소화하기 위해 양자화된 가중치를 계산하는데, 이는 양자화된 가중치 분포의 불균형을 초래하고 양자화된 가중치 표현 용량을 손상시킬 수 있습니다. (2) 양자화 함수 Eq. 1과 Eq. 4는 등거리가 아니며, 즉 양자화된 가중치와 실수 값 가중치 간의 크기 일관성을 고려하지 않지만, 우리는 크기 일관성이 정확한 기울기 추정에 상당히 기여한다는 것을 발견했습니다. 이전 솔루션의 위의 두 가지 한계를 고려하여 정보 엔트로피를 향상시키고 기울기 불일치를 줄이는 새로운 양자화 함수를 설계하고자 합니다. 정보 이론에서 가중치 표현 기능을 높이기 위해 양자화된 가중치에 더 높은 엔트로피가 포함되어 있을 때 더 많은 정보가 보존됩니다. N = −pi log(pi), st Pi max H = Pi i== (7) 여기서 på는 전체 N 수준에서 실수 값 가중치가 i번째 양자화 수준으로 양자화되는 비율을 나타냅니다. Eq. 7은 라그랑주 승수와 최적의 p ¼, i = {1, 2, ……., N}로 쉽게 풀 수 있으며, 이는 최대 정보 엔트로피를 보존하는 가장 좋은 양자화 계획이 모든 양자화 레벨에서 실수 값 가중치를 가능한 한 균등하게 분배하는 것임을 시사합니다. 이전 이진화 작업(Liu et al., 2020b)에서 제안한 대로 그래디언트 불일치를 줄이기 위해 양자화된 가중치와 실수 값 가중치 간의 크기 차이는 그래디언트 스케일에 큰 영향을 미치고 크기의 불일치는 역전파에서 증폭되어 학습 중에 그래디언트 소멸 또는 폭발을 유발합니다. 따라서 실수 값 가중치와 양자화된 가중치의 크기가 일관되도록 하는 것이 중요합니다. 위에서 논의한 두 가지 요구 사항을 결합하여 최대 엔트로피 등척성 가중치 양자화를 제안했습니다. 3진화에서는 다음과 같이 공식화됩니다.여기서 μT WT = W α Clip(R-μT 1)] ατ WR (8) 4 ||WRμT||NWR αT = 여기서 WT와 WR은 각각 3진 가중치와 실수 값 가중치를 나타냅니다.반올림 함수 [.]와 Clip() 함수는 가중치를 {-1,0,1}로 양자화합니다.μ는 실수 값 가중치의 평균이며 이제 가중치 행렬의 가중치 수를 나타냅니다.스케일링 계수 a는 가중치 통계에서 계산되고 엔트로피 규칙에 따라 실수 값 가중치 WR을 양자화 수준에서 균등하게 분포되도록 스케일링합니다.3진 경우 가중치는 {-α, 0, α}로 양자화됩니다. 실수 값 가중치가 균일하고 대칭적으로 분포되도록 초기화되면(He et al., 2015; Glorot and Bengio, 2010), 스케일링 인자는 αT를 [-1.5, 1.5]로 분포시켜 출력 3진 가중치 Wi аt R 통계 기반 가중치 양자화 Q 양자화된 임베딩 학습 기반 대칭 활성화 양자화 통계 기반 가중치 양자화 K 학습 기반 학습 기반 대칭 대칭 활성화 양자화 활성화 양자화 소프트맥스 학습 기반 비대칭 활성화 양자화 {0,1,...} 학습 기반 대칭 활성화 양자화 통계 기반 가중치 양자화 V 학습 기반 대칭 활성화 양자화 통계 기반 가중치 양자화 FC ReLU 학습 기반 비대칭 활성화 양자화 {0,1,...} 학습 기반 대칭 활성화 양자화 통계 기반 가중치 → 양자화 FC 통계 기반 가중치 양자화 FC 피드포워드 네트워크 셀프 어텐션 트랜스포워드 블록 출력 그림 1: TBT 개요. 트랜스포머 블록은 멀티헤드 셀프 어텐션 및 피드포워드 네트워크를 포함합니다. 가중치 3진화/2진화를 위한 통계 기반 양자화 방법을 제안하고 ReLU/Softmax 출력(X = R2)에서 활성화를 위한 학습 기반 비대칭 양자화 방법과 다른 레이어에서 양수 및 음수 값을 모두 포함하는 활성화(X = R^)에 대한 학습 기반 비대칭 양자화 방법을 채택합니다. αT는 3개의 3진 수준에서 거의 균일한 분포를 갖습니다. 한편, Eq. 8은 실수 값 가중치가 1로 조정되어 [-1, 1]에 가까워지고 양자화 후 시간 α가 축소되는 등거리 사상입니다. 이런 식으로 크기가 유지됩니다. 이에 따라 2진의 경우 다음이 있습니다. WB = QB Sign(여기서 μB ав . = WR - μB) WR, ав ||WR-B|| Ꭱ NWR (9) 여기서 WB는 이진 가중치를 나타내며, 평균 μ를 빼면 이진화 전에 실수 값 가중치가 0 중심이 되어 이진화된 가중치가 균등하게 분포됩니다.그러면 스케일링 계수 QB가 실수 값 가중치와 이진 가중치 사이의 크기와 일치합니다.특히, Eq. 9에서 W QB Sign(. . (WB) = α Sign (WR - HB), aB는 이진화 함수를 등각 투영으로 유지하기 위해 분모에 α를 명시적으로 포함하고 가중치에 대한 그래디언트는 다음과 같이 간단히 계산할 수 있습니다.OW STE ≈ᎧᎳ . R wi R -HB aB (10) |
--- EXPERIMENT ---
s 이 섹션에서는 자연어 생성 모델을 위한 저비트 양자화 체계의 텍스트 요약 벤치마크인 CNN/DailyMail(Nallapati et al., 2016) 및 XSUM(Narayan et al., 2018)에 대한 효과를 평가합니다. 또한 WMT16 영어-루마니아어(En-Ro) 데이터 세트(Bojar et al., 2016a)에서 mBART를 사용한 기계 번역 작업에 대해 실험합니다. 3. 실험 설정 우리는 최근 연구(Li et al., 2022)를 따르며, 완전 정밀도 사전 학습된 모델에서 초기화 및 지식 증류를 사용하여 양자화된 네트워크를 학습합니다. 구체적으로, 우리는 요약 작업을 위한 완전 정밀도 기준으로 BARTbase(Lewis et al., 2019)를 사용하고 번역 작업에는 mBARTlarge(Liu et al., 2020a)를 사용합니다. 우리는 8비트 활성화 모델의 경우 128의 배치 크기와 2.5e-4의 학습 속도, 이진 및 삼진 활성화 모델의 경우 5e-4의 GPU에서 20개 에포크 동안 양자화된 모델을 훈련합니다. 3.2 요약 요약 작업의 경우 다음 벤치마크를 채택합니다. XSUM 데이터 세트(Narayan et al., 2018)는 BBC의 온라인 뉴스 웹사이트에서 샘플링한 226,000개 문서와 짧은 한 문장 요약으로 구성되어 있습니다. 요약이 매우 짧기 때문에 이 데이터 세트에서는 추상화된 방법이 더 나은 성과를 거두는 경향이 있습니다. 표 1: XSUM 및 CNN/DailyMail 벤치마크에서 텍스트 요약을 위한 양자화 방법 비교. 우리는 임베딩, 가중치 및 활성화의 비트 수를 나타내는 &quot;EWA (#bits)&quot; 표기법을 사용합니다(특히 1은 이진, 2는 삼진을 나타냄). QuantBart, DQ-BART 및 BlockPruning의 결과는 해당 논문에서 인용되었습니다. 또한 BinaryBert, BiBert 및 Ternary Bert에서 개발한 알고리즘을 BART 모델에 구현하고 결과를 보고하며, *로 표시합니다. 평가 지표로 rouge-{1,2,L}을 사용합니다. 방법 BART QuantBart (Tao et al., 2022) DQ-BART (Li et al., 2022) #Bits Size (EWA) (MB) 32-32-532.8-8-138.XSUM CNN/DailyMail RFLOPS R2 RL R1 R2 RL 1x 43.84 20.79 35.71 44.90 22.25 42.40.25 17.78 32.8-8-138.42.51 19.61 34.61 44.66 21.92 41.Ternary Baseline(TWN)(Li 외, 2016) 2-2-39.QuantBart(Tao 외, 2022) 2-2-39.DQ-BART (Li 등, 2022) 2-2-39.TBT 2-2-39.Baseline (TWN) (Li 등, 2016) 2-2-TernaryBert* (Zhang 등, 2020) 2-2-TBT 2-2-39.39.39.0.25x 0.25x 0.25x 40.06 17.34 32.46 42.94 20.07 40.0.25x 42.40 19.54 34.51 43.46 20.52 40.0.0625× 12.80 1.21 11.4 12.92 0.32 12.0.0625× 14.03 2.23 11.79 10.95 0.52 8.0.0625× 36.21 14.38 29.07 41.03 18.18 38.39.99 17.13 31.99 42.99 20.05 40.39.15 16.72 31.이진 기준선(BWN) (Courbariaux 등, 2016) 1-1-BinaryBert*(Bai 등, 2021b) 1-1-BlockPruning(Lagunas 등, 2021) TBT 1-1-Baseline(BWN)(Courbariaux 등, 2016) BinaryBert*(Bai 등, 2021b) 1-1-1-1-BiBert*(Qin et al., 2021) TBT 1-1-1-1-23.23.23.23.23.23.23.0.125x 1.90 0.01 1.78 2.78 0.08 2.0.125× 39.76 17.05 31.99 40.66 18.52 28.41.4 18.7 38.0.125 40.96 18.37 33.30 42.66 19.72 39.0.0156× 1.90 0.01 1.78 2.78 0.08 2.0.0156× 8.13 0.12 7.69 9.80 0.15 8.0.0156× 7.58 0.06 7.54 14.22 0.13 10.0.0156× 31.68 11.19 25.29 35.56 11.71 33. CNN/DailyMail(Nallapati et al., 2016)은 또 다른 뉴스 요약 벤치마크로, 더 긴 문서(~30개 문장)와 더 길고 여러 문장으로 된 요약을 제공합니다. 이 데이터 세트에는 약 300,000개의 문서-요약 쌍이 포함되어 있습니다. 1억 4,000만 개의 매개변수를 갖춘 영어 전용 인코더-디코더 변환기인 BART 기반 모델(Lewis et al., 2019)을 사용합니다. 이 작업에 표준 ROUGE-{1,2,1} 메트릭을 사용하여 비교합니다. 3진 가중치와 8비트 활성화 설정의 경우, 우리는 두 가지 최신 방법인 QuantBart(Tao et al., 2022)와 DQ-BART(Li et al., 2022)와 비교합니다. 완전한 3진 설정과 2진 양자화 실험의 경우, 선행 기술은 없습니다. 따라서 우리는 이전 연구(Li et al., 2016; Courbariaux et al., 2016)에서 널리 사용된 구현을 사용하여 순진한 양자화 기준선을 제공하고, BERT 모델(Bai et al., 2021b; Qin et al., 2021; Zhang et al., 2020)에 제안된 2진 및 3진 방법을 BART에 적용합니다. 우리의 주요 결과는 표 1에 요약되어 있습니다. 3진 가중치와 8비트 활성화 설정에서 TBT는 XSUM에서 ROUGE 점수에서 이전 SoTA를 최대 2.3포인트, CNN/DailyMail에서 최대 0.5포인트까지 향상시킵니다. 두 가지 개선 모두 중요합니다. 활성화를 8비트로 유지하면서 가중치를 이진으로 추가로 양자화하면 XSUM에서 ROUGE-L 점수 33.3을 달성할 수 있는데, 이는 이전 3진 SOTA(DQBART)보다 0.포인트 높고 CNN/DailyMail에서는 비슷합니다. 이는 우리가 아는 한 경쟁력 있는 정확도의 이진 가중치 생성 변환기 모델을 처음으로 시연하는 것입니다. 또한 TBT 이진 가중치 BART 모델은 동일한 압축 모델 크기를 가진 SOTA 가지치기 방법과 비교하여 CNN에서 1.2포인트 더 높은 ROUGE 점수를 달성합니다. 3진 및 이진 활성화로 넘어가면 선행 기술은 없으며 이전 구현은 의미 있는 결과를 생성하지 못합니다. 반면에 우리의 방법은 완전한 3진 설정에서 XSUM과 CNN/DailyMail에 대해 각각 6.6포인트와 3.포인트 뒤처진 ROUGE-L 점수 29.1과 38.3을 달성했습니다. 우리의 완전한 2진(가중치와 활성화) 모델은 10.4포인트와 8.9포인트로 더 넓은 격차를 보이지만, XSUM과 CNN/DailyMail에 대해 각각 25.3포인트와 33.2포인트의 ROUGE-L 점수에서 매우 사소하지 않은 출력을 생성하는 데 성공했습니다. 3.3 기계 번역 우리는 또한 기계 번역에 대한 모델을 평가합니다. 우리는 32-32-2.26.8-8-0.61 25의 En-Ro 벤치마크를 채택했습니다. 표 2: WMT16 En-Ro에서의 번역을 위한 mBART-대형 모델에서의 양자화 방법 비교. 방법 mBART(Liu et al., 2020a) DQ-BART(Li et al., 2022) 비트 수(EWA) 크기(GB) BLEU 표 4: 기준 방법과 우리 방법 간의 생성된 평균 시퀀스 길이 비교. CNN/DailyMail 방법 #Bits(EWA) XSUM BART-base 32-32-30.99.Baseline 2-2-28.93.DQ-BART(Li et al., 2022) 2-2-0.31 23.TBT 2-2-32.95.TBT 2-2-0.31 24.Baseline 2-2-48.14.TBT 2-2-0.31 21.TBT 2-2-30.88.TBT 1-1-TBT 1 - 1 -0.16 24.17.0.Baseline 1-1-62.128.TBT 1-1-31.97.Baseline TBT 1-1-62.128.1 - 1 -29.67.표 3: 제안된 방법의 효과에 대한 절제 연구 XSUM과 CNN/DailyMail 벤치마크에 대한 학습 기반 활성화 양자화 방법과 통계 기반 가중치 양자화 방법. 방법 1 기준선(TWN)+활성화(학습 기반)+둘 다 3 가중치(통계 기반) 5 기준선(BWN) 6+활성화(학습 기반) 7+가중치(통계 기반)+둘 다 9 기준선(TWN) XSUM 비트 수(EWA) R1 R2 RL 2-2-12.80 1.21 11.2-2-2 15.05 1.38 12.2-2-2 13.79 0.87 12.2-2-2 36.21 14.38 29.1-1-1 - 1 -1-1-1-1-1.90 0.01 1.1.90 0.01 1.10.10.96 0.31.68 11.19 25.CNN/DailyMail R1 R2 RL 2-2-2 12.92 0.32 12.10+ 활성화(학습 기반) 2-2-2 13.34 0.99 12.11+ 가중치(통계 기반) 12 + 둘 다 2-2-2 19.34 0.42 18.2-2-2 41.03 18.18 38.13 기준선(BWN) 1-1-1 2.78 0.08 2.14+ 활성화(학습 기반) 1-1-1 2.78 0.08 2.15+ 가중치(통계 기반) 1-1-1 15.05 0.35 14.16+ 둘 다 1-1-1 35.56 11.71 33.WMT&#39;16 공유 과제(Bojar et al., 2016b) 이전 작업과 호환되도록 합니다. 기본 모델은 25개 언어로 사전 학습된 백만 매개변수 다국어 인코더-디코더 변환기인 mBART-대형 모델(Liu et al., 2020a)입니다. 표 2는 결과를 보여줍니다. 8비트 활성화를 사용한 3진 가중치 설정에서 이전 SoTA를 1.2포인트 향상시켜 24.63 BLEU를 달성했습니다. 놀랍게도 2진 가중치 모델도 이전 3진 가중치 SOTA보다 거의 1포인트 더 뛰어납니다. 24.3 BLEU를 기록했는데, 전체 mBART 모델보다 1.5포인트 뒤처진 반면 크기는 16배 작습니다. 이전 방법이 수렴하지 못한 완전한 3진 및 2진 설정에서 TBT 모델은 실용적인 수준의 성능에 도달할 수 있으며 3진 TBT mBART는 21.7 BLEU를 달성하고 TBT 2진 mBART는 17.59를 달성했습니다. 3.4 절제 앞서 언급했듯이, 제안된 주요 모델링 개선 사항은 두 가지 방법을 결합한 것입니다. 가중치에 대한 통계 기반 양자화와 활성화에 대한 학습 기반 양자화입니다. 이러한 방법의 기여도를 절제하고 결과를 표 3에 제시합니다. 결과는 각 방법이 기준선에 비해 자체적으로 적당한 이득을 제공할 수 있지만 이러한 개선 사항만으로는 의미 있는 결과를 생성하기에 충분하지 않다는 것을 명확하게 보여줍니다. 절제된 모델 중 어느 것도 1.5 이상의 R2 점수를 달성할 수 없습니다. 두 가지를 결합해야만 학습이 안정화되고 완전한 3진 및 2진 모델에 대한 양호한 수렴이 이루어집니다. 3.5 시퀀스 길이 분석 언어 생성 작업에서 재귀적 디코더 생성 프로세스의 오류 합성 문제는 양자화 오류를 크게 증폭시키거나 심지어 결과가 달라지기 때문에 양자화 방법의 견고성을 테스트하기에 가혹한 요소입니다. 생성된 평균 시퀀스 길이는 양자화된 모델이 합성 오류를 극복하고 합리적인 길이의 텍스트를 생성할 수 있는지 여부를 나타냅니다. 표 4에서 제안된 방법과 기준 방법(즉, 3진법의 경우 TWN(Li et al., 2016), 2진법의 경우 BWN(Courbariaux et al., 2016)) 간에 생성된 시퀀스 길이를 비교합니다. 가중치와 활성화가 모두 2진화된 경우에도 저희 방법은 XSUM 벤치마크에서 전체 정밀도 모델과 비슷한 길이의 요약을 성공적으로 생성합니다. 문서가 단 하나의 문장으로 요약되는 XSUM 데이터 세트와 비교할 때, CNN/DailyMail은 더 긴 요약을 허용하기 때문에 더 어렵습니다. 8비트 활성화 모델로 생성된 텍스트는 전체 정밀도 BART 모델과 거의 비슷한 평균 길이를 유지할 수 있는 반면, 2진법 및 3진법 활성화 모델은 약간 벗어납니다. 이와 대조적으로 기준선 방법은 3진 가중치(WT)만 도출할 수 있습니다.가중치 활성화 실수 값 활성화 첫 번째 행의 WT 3진 활성화 기준선 (TWN) (XT) -0.0.0.-0.0.0.(b) -0.0.0.0.0.(c) 우리의 것 (TBT) (XR) 0.10 0.(d) -0.1 0.0 0.-0.0.0.0.0.0.00 0.05 0.10 0.(e) (f) (g) (h) 그림 2: CNN/DailyMail 벤치마크에서 BART 모델을 3진화하기 위한 기준선 TWN 방법과 TBT 방법 간의 가중치 및 활성화 히스토그램 비교.가중치는 디코더의 첫 번째 셀프 어텐션 블록에 있는 가치 행렬의 완전 연결 계층에서 가져오고 활성화는 동일한 계층의 어텐션 출력입니다. 영어: 2비트 가중치 8비트 활성화를 사용한 합리적인 요약이며 낮은 비트 폭에서 실패하여 언어 생성 작업의 어려운 본성을 보여줍니다.3.6 시각화 제안된 방법의 효과를 더 잘 이해하기 위해 그림 2에서 기준선 방법과 제안된 방법으로 3진화된 BART 모델에서 가중치 및 활성화 히스토그램을 시각화합니다.기준선 방법과 우리 방법 모두 행당 가중치 3진화를 사용하므로 텐서 텐서는 #행의 스케일링 인수를 갖습니다.그림 2(b) 및 (g)에서 볼 수 있듯이 제안된 방법은 가중치를 3진화 수준에서 더 균등하게 분산시킬 수 있으므로 2.2절에서 설명한 대로 양자화된 가중치에서 더 높은 정보 엔트로피를 허용할 수 있습니다. 또한, BART 기반 모델에서 96개의 완전 연결 층에서 양자화된 가중치 분포 엔트로피(즉, Eq. 7)를 계산하고 제안된 TBT 방법이 모든 층에서 기준선 방법보다 양자화된 가중치에서 일관되게 더 높은 엔트로피를 달성한다는 것을 발견했습니다. 나아가, 그림 2(a)(e)에서 볼 수 있는 흥미로운 현상은 기준선 모델의 3진 가중치가 가우시안 분포에 매우 가까운 반면, TBT로 3진화된 가중치는 더 정교한 분포를 포착한다는 것입니다. 이 현상은 제안된 방법이 가중치가 더 많은 정보 패턴을 학습하고 따라서 언어 생성 작업에 대한 높은 수요를 더 잘 충족시키는 데 도움이 된다는 것을 의미합니다. 활성화 양자화의 경우, 어텐션 층과 SoftMax 출력에는 양의 활성화만 포함되어 있음이 분명합니다(✗R = R+). 단순히 {-a, 0, a}로 3진화하는 경우, 3진 활성화는 하나의 대표적 수준을 낭비하고(그림 2(d)) 따라서 정확도가 낮아집니다. 대신 제안된 방법은 음이 아닌 활성화 계층(XR Є R+)을 {0, a, 2a)로 3진화하고 기본 실수 값 분포에 더 잘 맞도록 스케일링 인자 a를 학습하는 2세트 3진화 방법을 사용합니다. 이 3진화 방법은 정보 손실을 크게 줄이고 최종 정확도를 향상시킵니다. 4 관련 연구 양자화는 신경망을 더 효율적으로 만들기 위해 오랫동안 연구되어 왔습니다(조사는 (Hubara et al., 2017) 참조). BERT의 인기로 인해 많은 연구에서 변환기 모델에 대한 양자화를 연구했으며, 8비트 양자화(Zafrir et al., 2019; Fan et al., 2020)로 시작하여 4비트(Shen et al., 2020; Zadeh et al., 2020), 3진(Zhang et al., 2020) 및 2진(Bai et al., 2021b); Qin et al. (2021); Liu et al. (2022). 이 모든 연구는 인코더 전용 설정에 초점을 맞추었습니다. 생성 설정에서 Prato et al. (2019); Behnke et al. (2021)은 기계 번역을 위한 양자화된 모델을 시연했고, Fan et al. (2020); Bai et al. (2021a)은 언어 모델링을 위한 양자화된 모델을 시연했지만, 적당한 양자화 수준(4-8비트)에 한했습니다. 가장 최근에 Tao et al. (2022)과 Li et al. (2022)은 가중치 양자화를 2비트(8비트 활성화 양자화 포함)로 낮추고 언어 모델링과 요약에서 평가했습니다. 그러나 저희의 방법은 이러한 작업보다 상당히 우수한 성능을 보이는 동시에 가중치와 활성화를 모두 2비트, 심지어는 1비트로 양자화한 정확한 생성 변환기를 처음으로 시연했습니다. 5
--- CONCLUSION ---
우리는 사전 훈련된 트랜스포머 인코더-디코더 백본을 기반으로 높은 정확도의 3진 및 2진 자연어 생성 모델을 시연했습니다. 네트워크의 가중치와 활성화를 양자화하면 이러한 모델은 곱셈 모듈이 필요하지 않은 2진 및 3진 산술을 사용하여 특수 목적의 하드웨어에서 실행할 수 있습니다. 따라서 우리의 결과는 이러한 모델을 실행하는 동안 효율성에서 여러 자릿수 증가를 약속하며, 이러한 모델의 사용 사례를 하이엔드 GPU 서버를 넘어 크게 확장할 수 있습니다. 우리는 GPT-3(Brown et al., 2020)과 같은 대규모 텍스트 생성 모델에 대한 결과의 의미에 대해 특히 기대하고 있습니다. 이러한 모델은 인상적인 기능을 입증했지만 엄청난 확장 및 계산적 과제를 제시했습니다. 저비트 양자화는 이러한 문제 중 일부를 완화하는 유망한 접근 방식입니다. 우리의 접근 방식이 이러한 모델에 확장될지 여부는 미지수 문제이며 흥미로운 미래 연구 방향입니다. 6 제한 사항 우리는 유한한 문장 길이의 공개 데이터 세트에 대한 실험을 수행하지만 매우 긴 시퀀스 또는 스트리밍 데이터로의 일반화 가능성은 검증되지 않았습니다. 또한 제안된 양자화 방법을 컴퓨터 비전이나 음성 인식을 포함한 다른 작업에 일반화하는 것은 아직 테스트가 필요합니다. 또한 이진화와 삼진화에는 실제 메모리 절약과 실시간 가속을 위한 전용 하드웨어 지원을 위해 비트 패킹이 필요한데, 이는 하드웨어 구현 측면에 가깝고 이 논문에서는 연구하지 않습니다. 7 윤리 성명 우리는 사회에 기여하고 피해를 피하며 정직하고 신뢰할 수 있음을 확인합니다. 우리는 이전 작업을 존중하고 사용하는 방법과 데이터 세트를 적절히 인용합니다. 우리가 사용하는 모든 데이터는 공개되며 비공개 데이터는 포함되지 않습니다. 번역 기술이 제3자에 의해 악의적으로 사용되는 경우 잠재적인 위험이 있으므로 우리는 개발한 압축 기술과 올바르게 사용되는 일반적인 요약/기계 번역 기술을 어떠한 형태의 차별도 없이 유지하기 위해 최선을 다합니다. 참고문헌 Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, Michael R Lyu. 2021a. 사전 훈련된 언어 모델의 효율적인 사후 훈련 양자화를 향해. arXiv 사전 인쇄본 arXiv:2109.15082. Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael R Lyu, Irwin King. 2021b. Binarybert: bert 양자화의 한계를 넓히다. ACL/IJCNLP(1)에서. Maximiliana Behnke, Nikolay Bogoychev, Alham Fikri Aji, Kenneth Heafield, Graeme Nail, Qianqian Zhu, Svetlana Tchistiakova, Jelmer Van der Linde, Pinzhen Chen, Sidharth Kashyap, et al. 2021. 모델 가지치기 및 양자화를 사용한 효율적인 기계 번역. 제6회 기계 번역 회의록, 775-780쪽. Yoshua Bengio, Nicholas Léonard, Aaron Courville. 2013. 조건부 계산을 위한 확률적 뉴런을 통한 그래디언트 추정 또는 전파. arXiv 사전 인쇄본 arXiv:1308.3432. Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, Marcos Zampieri. 2016a. 기계 번역에 대한 2016년 컨퍼런스 결과. 제1회 기계 번역 컨퍼런스 회의록: 제2권, 공유 과제 논문, 131~198쪽, 독일 베를린. 계산 언어학 협회. Ondřej Bojar, Yvette Graham, Amir Kamran, Miloš Stanojević. 2016b. wmt16 메트릭 공유 작업의 결과. 제1회 기계 번역 컨퍼런스 회의록: 제2권, 공유 작업 논문, 199-231페이지. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901. Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio. 2016. 이진화된 신경망: 가중치와 활성화가 +1 또는 -1로 제한된 딥 신경망 학습. arXiv 사전 인쇄본 arXiv:1602.02830. Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, Dharmendra S Modha. 2019. 학습된 스텝 크기 양자화. 국제 학습 표현 컨퍼런스에서. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Herve Jegou, Armand Joulin. 2020. 극단적인 모델 압축을 위한 양자화 노이즈로 학습. arXiv 사전 인쇄본 arXiv:2004.07320. Xavier Glorot과 Yoshua Bengio. 2010. 딥 피드포워드 신경망 학습의 어려움 이해. 제13회 인공지능 및 통계 국제 컨퍼런스 논문집, 249-256쪽. JMLR 워크숍 및 컨퍼런스 논문집. 카이밍 허, 샹위 장, 샤오칭 렌, 지안 선. 2015. 정류기 심층 탐구: 이미지넷 분류에서 인간 수준의 성능을 능가. IEEE 컴퓨터 비전 국제 컨퍼런스 논문집, 1026-1034쪽. 이타이 후바라, 마티유 쿠르바리악스, 다니엘 수드리, 란 엘-야니브, 요슈아 벤지오. 2017. 양자화된 신경망: 낮은 정밀도 가중치 및 활성화를 사용한 신경망 학습. 머신 러닝 연구 저널, 18(1):6869-6898. 프랑수아 라구나스, 엘라 샤를레, 빅터 산, 알렉산더 M 러시. 2021. 더 빠른 변압기를 위한 블록 가지치기. arXiv 사전 인쇄본 arXiv:2109.04838. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer. 2019. Bart: 자연어 생성, 번역 및 이해를 위한 시퀀스 간 사전 학습의 노이즈 제거. arXiv 사전 인쇄본 arXiv:1910.13461. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer. 2020. Bart: 자연어 생성, 번역 및 이해를 위한 시퀀스 간 사전 학습의 노이즈 제거. 58회 연례 총회 의사록, 7871-7880쪽. Fengfu Li, Bo Zhang, Bin Liu. 2016. Ternary Weight Networks. arXiv 사전 인쇄본 arXiv:1605.04711. Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew Arnold, Bing Xiang, Dan Roth. 2022. Dq-bart: 공동 증류 및 양자화를 통한 효율적인 시퀀스-투-시퀀스 모델. 60회 연례 총회 의사록, 2권: 단편 논문, 203-211쪽. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer. 2020a. 신경망 기계 번역을 위한 다국어 노이즈 제거 사전 학습. Association for Computational Linguistics의 거래, 8:726-742. Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang, Wei Liu, Kwang-Ting Cheng. 2020b. Bi-real net: real-network 성능을 향한 딥 네트워크 이진화. International Journal of Computer Vision, 128(1):202-219. Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, Yashar Mehdad. 2022. Bit: 견고하게 이진화된 다중 증류 변환기. arXiv 사전 인쇄본 arXiv:2205.13016. Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, Kwang-Ting Cheng. 2018. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capabilities and advanced training algorithm. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 722-737쪽. Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et al. 2016. Abstractive text summarization using sequence-to-sequence rns and beyond. arXiv 사전 인쇄본 arXiv:1602.06023. Shashi Narayan, Shay B. Cohen, Mirella Lapata. 2018. 자세한 내용은 말하지 말고 요약만 말해! 극단적인 요약을 위한 주제 인식 합성 신경망. 자연어 처리 경험적 방법에 대한 컨퍼런스의 회의록, 1797-1807페이지, 벨기에 브뤼셀. 계산 언어학 협회. Gabriele Prato, Ella Charlaix, Mehdi Rezagholizadeh. 2019. 기계 번역을 위한 완전 양자화 변환기. arXiv 사전 인쇄본 arXiv:1910.10485. Haotong Qin, Yifu Ding, Mingyuan Zhang, YAN Qinghua, Aishan Liu, Qingqing Dang, Ziwei Liu, Xianglong Liu. 2021. Bibert: 정확한 완전 이진화된 bert. 국제 학습 표현 컨퍼런스에서. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. 2018. 생성적 사전 학습을 통한 언어 이해 향상. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016. Xnor-net: 이진 합성 신경망을 사용한 Imagenet 분류. 유럽 컴퓨터 비전 컨퍼런스, 525-542쪽. Springer. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: bert의 헤시안 기반 초저정밀 양자화. AAAI 인공지능 컨퍼런스 회의록, 34권, 8815-8821쪽. Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. 2022. 양자화를 통한 생성적 사전 학습 언어 모델의 압축. 제60회 연례 총회 의사록(제1권: 장문 논문), 4821-4836쪽. Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, Andreas Moshovos. 2020. Gobo: 낮은 지연 시간과 에너지 효율적인 추론을 위한 주의 기반 NLP 모델 양자화. 2020년 제53회 IEEE/ACM 국제 마이크로아키텍처 심포지엄(MICRO), 811-824쪽. IEEE. Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat. 2019. Q8bert: 양자화된 8비트 bert. 2019년 제5회 에너지 효율적인 머신 러닝 및 인지 컴퓨팅 워크숍-NeurIPS 에디션(EMC2-NIPS), 36-39쪽. IEEE. Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang 및 Qun Liu. 2020. Ternarybert: 증류 인식 초저비트 BERT. EMNLP에서. Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen 및 Yuheng Zou. 2016. Dorefa-net: 낮은 비트폭 경사도를 사용하여 낮은 비트폭 컨벌루션 신경망을 훈련합니다. arXiv 사전 인쇄 arXiv:1606.06160.
