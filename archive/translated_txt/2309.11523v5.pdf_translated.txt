--- ABSTRACT ---
Vision Transformer(ViT)는 최근 몇 년 동안 컴퓨터 비전 커뮤니티에서 점점 더 주목을 받고 있습니다. 그러나 ViT의 핵심 구성 요소인 Self-Attention은 명시적 공간 사전이 부족하고 2차 계산 복잡도를 지녀 ViT의 적용성이 제한됩니다. 이러한 문제를 완화하기 위해 NLP 분야의 최근 Retentive Network(RetNet)에서 영감을 얻어 일반적인 목적을 위한 명시적 공간 사전을 갖춘 강력한 비전 백본인 RMT를 제안합니다. 구체적으로 RetNet의 시간적 감쇠 메커니즘을 공간 영역으로 확장하고 맨해튼 거리를 기반으로 하는 공간 감쇠 행렬을 제안하여 Self-Attention에 명시적 공간 사전을 도입합니다. 또한 명시적 공간 사전을 능숙하게 적용하는 어텐션 분해 형식을 제안하여 공간 감쇠 행렬을 방해하지 않고 전역 정보를 모델링하는 계산 부담을 줄이는 것을 목표로 합니다. 공간 감쇠 행렬과 어텐션 분해 형식을 기반으로 선형 복잡도로 명시적 공간 사전을 비전 백본에 유연하게 통합할 수 있습니다. 광범위한 실험에서 RMT가 다양한 비전 작업에서 뛰어난 성능을 보인다는 것이 입증되었습니다. 구체적으로, 추가 학습 데이터 없이 RMT는 ImageNet-1k에서 27M/4.5GFLOPs 및 96M/18.2GFLOPs로 84.8% 및 86.1%의 top-1 acc를 달성합니다. 다운스트림 작업의 경우 RMT는 COCO 감지 작업에서 54.5 박스 AP 및 47.2 마스크 AP, ADE20K 의미 분할 작업에서 52.8 mloU를 달성합니다. 코드는 https://github.com/qhfan/RMT 1에서 제공됩니다.
--- INTRODUCTION ---
Vision Transformer (ViT) [12]는 연구자들에게 매우 선호되는 뛰어난 시각적 아키텍처입니다. 그러나 ViT의 핵심 모듈인 Self-Attention의 내재적 구조는 *Ran He가 해당 저자입니다. Top-1 Acc(%)RMT(Ours) SMTBiFormer MaxViTModel #Params Top1 Acc. MaxVIT-T [31] 31M 83.SMT-S [34] 20M 83.BiFormer-S [75] 26M 83.RMT-S(당사) 27M 84.RMT-S*(당사) 27M 84.BiFormer-B [75] 57M 84.MaxViT-S [29] 69M 84.RMT-B(당사) 54M 85.RMT-B*(당사) 55M 85.SMT-L [34] 81M 84.MaxVIT-B [51] 120M 84.RMT-L(당사) RMT-L*(당사) 95M 85.96M 86.FLOPS(G)그림 1. 224를 사용한 ImageNet-1K에서의 FLOPS 대 Top-1 정확도 224 해상도. &quot;*&quot;는 토큰 레이블링[27]으로 학습된 모델을 나타냅니다. 명시적 공간 사전 확률. 게다가, 셀프 어텐션의 이차 복잡도는 글로벌 정보를 모델링할 때 상당한 계산 비용으로 이어집니다. 이러한 문제는 ViT의 적용을 제한합니다. 많은 연구에서 이전에 이러한 문제를 완화하려고 시도했습니다[13, 16, 30, 35, 50, 57, 61]. 예를 들어, Swin Transformer[35]에서 저자는 윈도잉 연산을 적용하여 셀프 어텐션에 사용되는 토큰을 분할합니다. 이 연산은 셀프 어텐션의 계산 비용을 줄일 뿐만 아니라 윈도우와 상대적 위치 인코딩을 사용하여 모델에 공간 사전 확률을 도입합니다. 이에 더하여 NAT[19]는 셀프 어텐션의 수용 필드를 컨볼루션 모양과 일치하도록 변경하여 계산 비용을 줄이는 동시에 모델이 수용 필드 모양을 통해 공간 사전 확률을 인식할 수 있도록 합니다. 이전 방법과 달리, 우리는 NLP 분야에서 최근 성공을 거둔 Retentive Network(RetNet) [46]에서 영감을 얻었습니다.RetNet은 거리 종속적 시간적 감소 행렬을 활용하여 1차원 및 단방향 텍스트 데이터에 대한 명시적 시간적 사전을 제공합니다.: 쿼리: 수용 필드 (a) 바닐라 자기 주의 (b) 윈도우 자기 주의 (c) 이웃 자기 주의 (d) 맨해튼 자기 주의 그림 2. 다양한 자기 주의 메커니즘 간 비교.MaSA에서 어두운 색상은 더 작은 공간적 감소율을 나타내는 반면, 밝은 색상은 더 큰 공간적 감소율을 나타냅니다.거리에 따라 변하는 공간적 감소율은 모델에 풍부한 공간적 사전을 제공합니다.RetNet 이전의 ALiBi [41]도 비슷한 접근 방식을 적용하여 NLP 작업에서 성공했습니다.우리는 이 시간적 감소 행렬을 공간 영역으로 확장하여 토큰 간 맨해튼 거리를 기반으로 하는 2차원 양방향 공간 감소 행렬을 개발했습니다. 우리의 공간 감소 행렬에서 대상 토큰의 경우 주변 토큰이 멀수록 주의 점수의 감소 정도가 커집니다. 이 속성은 대상 토큰이 전역 정보를 인식하는 동시에 다양한 거리에 있는 토큰에 다른 수준의 주의를 할당할 수 있게 합니다. 이 공간 감소 행렬을 사용하여 비전 백본에 명시적 공간 사전을 도입합니다. RetNet에서 영감을 받아 맨해튼 거리를 명시적 공간 사전으로 통합한 이 Self-Attention 메커니즘을 Manhattan Self-Attention(MaSA)이라고 합니다. 명시적 공간 사전 외에도 Self-Attention을 사용한 전역 모델링으로 인해 발생하는 또 다른 문제는 엄청난 계산 부담입니다. 이전의 희소 주의 메커니즘[11, 35, 53, 63, 75]과 RetNet[46]에서 보존이 분해되는 방식은 대부분 공간 감소 행렬을 방해하여 MaSA에 적합하지 않습니다. 공간 감소 행렬을 손상시키지 않고 전역 정보를 희소하게 모델링하기 위해 이미지의 두 축을 따라 SelfAttention을 분해하는 방법을 제안합니다. 이 분해 방법은 사전 정보의 손실 없이 Self-Attention과 공간적 감쇠 행렬을 분해합니다.분해된 MaSA는 선형 복잡도로 전역 정보를 모델링하고 원래 MASA와 동일한 수용 필드 모양을 갖습니다.그림 2에서 MaSA를 다른 Self-Attention 메커니즘과 비교합니다.우리의 MaSA가 다른 메커니즘보다 모델에 더 풍부한 공간적 사전을 도입하는 것을 볼 수 있습니다.MaSA를 기반으로 RMT라는 강력한 비전 백본을 구성합니다.우리는 광범위한 실험을 통해 제안된 방법의 효과를 보여줍니다.그림 1에서 볼 수 있듯이, 우리의 RMT는 이미지 분류 작업에서 최첨단(SOTA) 모델보다 성능이 뛰어납니다.또한, 우리의 모델은 객체 감지, 인스턴스 분할, 의미적 분할과 같은 작업에서 다른 모델에 비해 더 두드러진 이점을 보여줍니다.우리의 기여는 다음과 같이 요약할 수 있습니다.• 우리는 Self-Attention을 증강하기 위해 Manhattan Distance에 기반한 공간적 감쇠 행렬을 제안하여 명시적 공간적 사전을 가진 Manhattan Self-Attention(MaSA)을 만듭니다. • MaSA에 대한 분해 형태를 제안하여 공간적 붕괴 행렬을 방해하지 않고 글로벌 정보 모델링에 대한 선형적 복잡성을 가능하게 합니다. • MaSA를 활용하여 일반적인 목적을 위한 강력한 비전 백본인 RMT를 구성합니다. RMT는 추가 학습 데이터 없이도 ImageNet-1k에서 이미지 분류에서 높은 최고 정확도를 달성하며 객체 감지, 인스턴스 분할 및 의미 분할과 같은 작업에서 탁월합니다. 2.
--- RELATED WORK ---
영어: Transformer. Transformer 아키텍처는 [52]에서 처음 제안되어 순환 모델의 학습 한계를 해결한 다음 많은 NLP 작업에서 엄청난 성공을 거두었습니다. 이미지를 작고 겹치지 않는 패치 시퀀스로 분할함으로써 Vision Transformer(ViTs) [12]도 큰 주목을 받았고 비전 작업에서 널리 사용되었습니다 [5, 14, 18, 39, 58, 66]. 과거 RNN과 CNN이 각각 NLP 및 CV 분야를 지배했던 것과 달리 Transformer 아키텍처는 다양한 모달리티와 분야에서 빛을 발했습니다 [26, 37, 42, 60]. 컴퓨터 비전 커뮤니티에서 많은 연구가 ViT에 공간적 사전 확률을 도입하여 학습에 필요한 데이터 요구 사항을 줄이려고 시도하고 있습니다 [6, 19, 49]. 동시에 다양한 희소 어텐션 메커니즘이 셀프 어텐션의 계산 비용을 줄이기 위해 제안되었습니다 [13, 53, 54, 57]. Transformer의 사전 지식. Transformer 모델의 성능을 향상시키기 위해 사전 지식을 통합하려는 시도가 많이 있었습니다. 원래 Transformers[12, 52]는 삼각법 위치 인코딩을 사용하여 각 토큰에 대한 위치 정보를 제공합니다. 비전 작업에서 [35]는 원래 절대 위치 인코딩을 대체하기 위해 상대 위치 인코딩을 사용할 것을 제안합니다. [6]은 합성곱 계층의 제로 패딩이 ViT에 대한 위치 인식을 제공할 수도 있고 이 위치 인코딩이
--- METHOD ---
s, 우리는 NLP 분야에서 최근 성공을 거둔 Retentive Network(RetNet) [46]에서 영감을 얻었습니다.RetNet은 거리 종속적 시간 감소 행렬을 활용하여 1차원 및 단방향 텍스트 데이터에 대한 명시적 시간적 사전을 제공합니다.: 쿼리: 수용 필드 (a) 바닐라 자기 주의 (b) 윈도우 자기 주의 (c) 이웃 자기 주의 (d) 맨해튼 자기 주의 그림 2. 다양한 자기 주의 메커니즘 간 비교.MaSA에서 어두운 색상은 더 작은 공간적 감소율을 나타내는 반면 밝은 색상은 더 큰 감소율을 나타냅니다.거리에 따라 변하는 공간적 감소율은 모델에 풍부한 공간적 사전을 제공합니다.RetNet 이전의 ALiBi [41]도 비슷한 접근 방식을 적용하여 NLP 작업에서 성공했습니다.이 시간적 감소 행렬을 공간 영역으로 확장하여 토큰 간 맨해튼 거리를 기반으로 하는 2차원 양방향 공간 감소 행렬을 개발했습니다. 우리의 공간 감소 행렬에서 대상 토큰의 경우 주변 토큰이 멀수록 주의 점수의 감소 정도가 커집니다. 이 속성은 대상 토큰이 전역 정보를 인식하는 동시에 다양한 거리에 있는 토큰에 다른 수준의 주의를 할당할 수 있게 합니다. 이 공간 감소 행렬을 사용하여 비전 백본에 명시적 공간 사전을 도입합니다. RetNet에서 영감을 받아 맨해튼 거리를 명시적 공간 사전으로 통합한 이 Self-Attention 메커니즘을 Manhattan Self-Attention(MaSA)이라고 합니다. 명시적 공간 사전 외에도 Self-Attention을 사용한 전역 모델링으로 인해 발생하는 또 다른 문제는 엄청난 계산 부담입니다. 이전의 희소 주의 메커니즘[11, 35, 53, 63, 75]과 RetNet[46]에서 보존이 분해되는 방식은 대부분 공간 감소 행렬을 방해하여 MaSA에 적합하지 않습니다. 공간 감소 행렬을 손상시키지 않고 전역 정보를 희소하게 모델링하기 위해 이미지의 두 축을 따라 SelfAttention을 분해하는 방법을 제안합니다. 이 분해 방법은 사전 정보의 손실 없이 Self-Attention과 공간적 붕괴 행렬을 분해합니다. 분해된 MaSA는 선형 복잡도로 전역 정보를 모델링하고 원래 MASA와 동일한 수용 필드 모양을 갖습니다. 그림 2에서 MaSA를 다른 Self-Attention 메커니즘과 비교합니다. 우리의 MaSA가 다른 메커니즘보다 모델에 더 풍부한 공간적 사전 정보를 도입한다는 것을 알 수 있습니다. MaSA를 기반으로 RMT라는 강력한 비전 백본을 구성합니다. 우리는 광범위한
--- EXPERIMENT ---
s는 RMT가 다양한 비전 작업에서 뛰어난 성능을 보인다는 것을 보여줍니다. 구체적으로, 추가 학습 데이터 없이 RMT는 27M/4.5GFLOPs와 96M/18.2GFLOPs로 ImageNet-1k에서 84.8%와 86.1%의 상위 1 정확도를 달성합니다. 다운스트림 작업의 경우 RMT는 COCO 감지 작업에서 54.5 박스 AP와 47.2 마스크 AP를 달성하고 ADE20K 의미 분할 작업에서 52.8 mloU를 달성합니다. 코드는 https://github.com/qhfan/RMT에서 사용할 수 있습니다. 1. 소개 Vision Transformer(ViT)[12]는 연구자들에게 매우 선호되는 뛰어난 시각적 아키텍처입니다. 그러나 ViT의 핵심 모듈인 Self-Attention의 내재적 구조는 *Ran He가 해당 저자입니다. 상위 1 정확도(%)RMT(저자) SMTBiFormer MaxViTModel #Params 상위 1 정확도. MaxVIT-T [31] 31M 83.SMT-S [34] 20M 83.BiFormer-S [75] 26M 83.RMT-S(당사) 27M 84.RMT-S*(당사) 27M 84.BiFormer-B [75] 57M 84.MaxViT-S [29] 69M 84.RMT-B(당사) 54M 85.RMT-B*(당사) 55M 85.SMT-L [34] 81M 84.MaxVIT-B [51] 120M 84.RMT-L(당사) RMT-L*(당사) 95M 85.96M 86.FLOPS(G)그림 1. 224를 사용한 ImageNet-1K에서의 FLOPS 대 Top-1 정확도 224 해상도. &quot;*&quot;는 토큰 레이블링[27]으로 학습된 모델을 나타냅니다. 명시적 공간 사전 확률. 게다가, 셀프 어텐션의 이차 복잡도는 글로벌 정보를 모델링할 때 상당한 계산 비용으로 이어집니다. 이러한 문제는 ViT의 적용을 제한합니다. 많은 연구에서 이전에 이러한 문제를 완화하려고 시도했습니다[13, 16, 30, 35, 50, 57, 61]. 예를 들어, Swin Transformer[35]에서 저자는 윈도잉 연산을 적용하여 셀프 어텐션에 사용되는 토큰을 분할합니다. 이 연산은 셀프 어텐션의 계산 비용을 줄일 뿐만 아니라 윈도우와 상대적 위치 인코딩을 사용하여 모델에 공간 사전 확률을 도입합니다. 이에 더하여 NAT[19]는 셀프 어텐션의 수용 필드를 컨볼루션 모양과 일치하도록 변경하여 계산 비용을 줄이는 동시에 모델이 수용 필드 모양을 통해 공간 사전 확률을 인식할 수 있도록 합니다. 이전 방법과 달리, 우리는 NLP 분야에서 최근 성공을 거둔 Retentive Network(RetNet) [46]에서 영감을 얻었습니다.RetNet은 거리 종속적 시간적 감소 행렬을 활용하여 1차원 및 단방향 텍스트 데이터에 대한 명시적 시간적 사전을 제공합니다.: 쿼리: 수용 필드 (a) 바닐라 자기 주의 (b) 윈도우 자기 주의 (c) 이웃 자기 주의 (d) 맨해튼 자기 주의 그림 2. 다양한 자기 주의 메커니즘 간 비교.MaSA에서 어두운 색상은 더 작은 공간적 감소율을 나타내는 반면, 밝은 색상은 더 큰 공간적 감소율을 나타냅니다.거리에 따라 변하는 공간적 감소율은 모델에 풍부한 공간적 사전을 제공합니다.RetNet 이전의 ALiBi [41]도 비슷한 접근 방식을 적용하여 NLP 작업에서 성공했습니다.우리는 이 시간적 감소 행렬을 공간 영역으로 확장하여 토큰 간 맨해튼 거리를 기반으로 하는 2차원 양방향 공간 감소 행렬을 개발했습니다. 우리의 공간 감소 행렬에서 대상 토큰의 경우 주변 토큰이 멀수록 주의 점수의 감소 정도가 커집니다. 이 속성은 대상 토큰이 전역 정보를 인식하는 동시에 다양한 거리에 있는 토큰에 다른 수준의 주의를 할당할 수 있게 합니다. 이 공간 감소 행렬을 사용하여 비전 백본에 명시적 공간 사전을 도입합니다. RetNet에서 영감을 받아 맨해튼 거리를 명시적 공간 사전으로 통합한 이 Self-Attention 메커니즘을 Manhattan Self-Attention(MaSA)이라고 합니다. 명시적 공간 사전 외에도 Self-Attention을 사용한 전역 모델링으로 인해 발생하는 또 다른 문제는 엄청난 계산 부담입니다. 이전의 희소 주의 메커니즘[11, 35, 53, 63, 75]과 RetNet[46]에서 보존이 분해되는 방식은 대부분 공간 감소 행렬을 방해하여 MaSA에 적합하지 않습니다. 공간 감소 행렬을 손상시키지 않고 전역 정보를 희소하게 모델링하기 위해 이미지의 두 축을 따라 SelfAttention을 분해하는 방법을 제안합니다. 이 분해 방법은 사전 정보의 손실 없이 Self-Attention과 공간적 감쇠 행렬을 분해합니다.분해된 MaSA는 선형 복잡도로 전역 정보를 모델링하고 원래 MASA와 동일한 수용 필드 모양을 갖습니다.그림 2에서 MaSA를 다른 Self-Attention 메커니즘과 비교합니다.우리의 MaSA가 다른 메커니즘보다 모델에 더 풍부한 공간적 사전을 도입하는 것을 볼 수 있습니다.MaSA를 기반으로 RMT라는 강력한 비전 백본을 구성합니다.우리는 광범위한 실험을 통해 제안된 방법의 효과를 보여줍니다.그림 1에서 볼 수 있듯이, 우리의 RMT는 이미지 분류 작업에서 최첨단(SOTA) 모델보다 성능이 뛰어납니다.또한, 우리의 모델은 객체 감지, 인스턴스 분할, 의미적 분할과 같은 작업에서 다른 모델에 비해 더 두드러진 이점을 보여줍니다.우리의 기여는 다음과 같이 요약할 수 있습니다.• 우리는 Self-Attention을 증강하기 위해 Manhattan Distance에 기반한 공간적 감쇠 행렬을 제안하여 명시적 공간적 사전을 가진 Manhattan Self-Attention(MaSA)을 만듭니다. • 우리는 MaSA에 대한 분해 형태를 제안하여 공간적 감쇠 행렬을 방해하지 않고 전역 정보 모델링에 대한 선형적 복잡도를 가능하게 합니다.• MaSA를 활용하여 일반적인 목적을 위한 강력한 비전 백본인 RMT를 구성합니다.RMT는 추가 학습 데이터 없이 이미지 분류에서 ImageNet-1k에서 높은 정확도를 달성하고 객체 감지, 인스턴스 분할 및 의미 분할과 같은 작업에서 탁월합니다.2. 관련 작업 Transformer. Transformer 아키텍처는 [52]에서 처음 제안되어 순환 모델의 학습 제한을 해결한 다음 많은 NLP 작업에서 엄청난 성공을 거두었습니다.이미지를 작고 겹치지 않는 패치 시퀀스로 분할함으로써 Vision Transformer(ViT) [12]도 큰 주목을 받았고 비전 작업에서 널리 사용되었습니다 [5, 14, 18, 39, 58, 66]. 과거 RNN과 CNN이 각각 NLP와 CV 분야를 지배했던 것과 달리, 트랜스포머 아키텍처는 다양한 모달리티와 분야에서 빛을 발했습니다[26, 37, 42, 60]. 컴퓨터 비전 커뮤니티에서 많은 연구가 ViT에 공간적 사전 지식을 도입하여 학습에 필요한 데이터 요구 사항을 줄이려고 시도하고 있습니다[6, 19, 49]. 동시에 다양한 희소한 어텐션 메커니즘이 셀프 어텐션의 계산 비용을 줄이기 위해 제안되었습니다[13, 53, 54, 57]. 트랜스포머의 사전 지식. 성능을 향상시키기 위해 트랜스포머 모델에 사전 지식을 통합하려는 시도가 많이 있었습니다. 원래 트랜스포머[12, 52]는 삼각 위치 인코딩을 사용하여 각 토큰에 대한 위치 정보를 제공합니다. 비전 작업에서[35]는 원래 절대 위치 인코딩을 대체하기 위해 상대 위치 인코딩을 사용할 것을 제안합니다. [6]은 합성곱 계층의 제로 패딩이 ViT에 대한 위치 인식을 제공할 수도 있으며, 이 위치 인코딩 방법이 매우 효율적이라고 지적합니다. 많은 연구에서 FFN의 합성곱 [13, 16, 54]은 ViT의 위치 정보를 더욱 풍부하게 하기 위해 비전 모델에 사용되었습니다. NLP 작업의 경우 최근 Retentive Network [46]에서 시간 감쇠 행렬을 도입하여 거리 변화에 따른 사전 지식을 모델에 제공했습니다. RetNet 이전에 ALiBi [41]도 유사한 시간 감쇠 행렬을 사용했습니다. 3. 방법론 3.1. RetNet의 예비 시간 감쇠. Retentive Network(RetNet)는 언어 모델을 위한 강력한 아키텍처입니다. 이 작업은 시퀀스 모델링을 위한 보존 메커니즘을 제안합니다. 보존은 Transformers에 없는 시간 감쇠를 언어 모델에 가져옵니다. 보존은 먼저 순환 방식으로 시퀀스 모델링 문제를 고려합니다. 다음과 같이 쓸 수 있습니다. 1: n On = Σyn-m(Qneine)(Kmeimo) + vm m=병렬 훈련 과정의 경우, Eq. 1은 다음과 같이 표현됩니다: (1) Q = (XWQ), K = (XWк) ©ē, V = XWv On = eine Dnm = [yn-m, n&gt;mn
--- CONCLUSION ---
이 작업에서 우리는 명시적 공간 사전을 가진 비전 백본인 RMT를 제안합니다. RMT는 NLP에서 인과 모델링에 사용되는 시간적 감쇠를 공간 수준까지 확장하고 맨해튼 거리를 기반으로 하는 공간 감쇠 행렬을 도입합니다. 이 행렬은 명시적 공간 사전을 셀프 어텐션에 통합합니다. 또한 RMT는 공간 감쇠 행렬을 방해하지 않고 전역 정보를 희소하게 모델링할 수 있는 셀프 어텐션 분해 형태를 활용합니다. 공간 감쇠 행렬과 어텐션 분해 형태의 조합을 통해 RMT는 명시적 공간 사전 및 선형 복잡도를 가질 수 있습니다. 이미지 분류, 객체 감지, 인스턴스 분할 및 의미 분할에 대한 광범위한 실험을 통해 RMT의 우수성이 검증되었습니다. A. 아키텍처 세부 정보 아키텍처는 표 10에 설명되어 있습니다. 합성곱 스템의 경우 5개의 3 × 3 합성곱을 적용하여 이미지를 56 × 56 토큰에 임베드합니다. GELU 및 배치 정규화는 마지막 합성곱을 제외한 각 합성곱 후에 사용되며, 마지막 합성곱은 배치 정규화만 따릅니다. 단계 사이에 스트라이드 2의 3 × 3 합성곱을 사용하여 피처 맵의 해상도를 줄입니다.CPE에서는 3 × 3 깊이별 합성곱을 채택합니다.또한 LCE에서는 5 × 5 깊이별 합성곱을 채택합니다.RMT-DeiT-S, RMT-Swin-T 및 RMT-Swin-S는 절제 실험에 사용한 모델입니다.이들의 구조는 합성곱 스템, CPE 및 기타 기술을 사용하지 않고도 DeiT[49] 및 Swin-Transformer[35]의 구조와 긴밀하게 일치합니다.B. 실험 설정 ImageNet 이미지 분류.유일한 감독이 분류 손실인 DeiT[49]와 동일한 학습 전략을 채택합니다.특히, 모델은 300에포크 동안 처음부터 학습됩니다.코사인 감쇠 학습률 스케줄러와 5에포크의 선형 워밍업이 있는 Adam W 최적화 프로그램을 사용합니다. 초기 학습률, 가중치 감소 및 배치 크기는 각각 0.001, 0.05 및 1024로 설정됩니다. 증강 설정은 RandAugment[8](randm9-mstd0.5-inc1), Mixup[70](prob=0.8), CutMix[69](probe=1.0), Random Erasing[73](prob=0.25) 및 지수 이동 평균(EMA)[40]입니다. 확률적 깊이 증가의 최대 비율[24]은 각각 RMT-T/S/B/L에 대해 0.1/0.15/0.4/0.5로 설정됩니다. 보다 포괄적인 비교를 위해 두 가지 버전의 모델을 훈련합니다. 첫 번째 버전은 분류 손실만을 감독으로 사용하는 반면, 두 번째 버전은 분류 손실 외에도 추가 감독을 위해 [27]에서 도입한 토큰 레이블링을 통합합니다. 토큰 레이블링을 사용하는 모델은 &quot;*&quot;로 표시됩니다. COCO 객체 감지 및 인스턴스 분할. 실험을 수행하기 위해 RetinaNet[32], Mask-RCNN[22] 및 Cascaded Mask-CNN[2]을 감지 프레임워크로 적용합니다. MMDetection[4]을 기반으로 구현합니다. 모든 모델은 두 가지 공통 설정인 &quot;1×&quot;(학습을 위한 12에포크) 및 &quot;3×+MS&quot;(학습을 위한 다중 스케일 증가가 있는 에포크)에서 학습합니다. &quot;1×&quot; 설정의 경우 이미지가 픽셀의 짧은 쪽으로 크기가 조정됩니다. &quot;3×+MS&quot;의 경우 다중 스케일 학습 전략을 사용하고 짧은 쪽을 무작위로 800픽셀 사이의 크기로 조정합니다. 초기 학습 속도가 1e-4인 AdamW 최적화기를 적용합니다. RetinaNet의 경우 RetinaNet에 대해 1e-4의 가중치 감소를 사용하는 반면 Mask-RCNN 및 Cascaded Mask-RCNN의 경우 5e-2로 설정합니다. 모든 설정에 대해 우리는 이전 연구 [35, 63, 64] ADE20K 의미 분할을 따르는 16의 배치 크기를 사용합니다. MMSegmentation [7]을 기반으로 UperNet [59]과 SemanticFPN [28]을 구현하여 모델을 검증합니다. UperNet의 경우 우리는 Swin-Transformer [35]의 이전 설정을 따르고 512x 512의 입력 크기로 160k 반복에 대해 모델을 학습합니다. SemanticFPN의 경우 우리는 또한 512 x 512의 입력 해상도를 사용하지만 80k 반복에 대해 모델을 학습합니다. C. 효율성 비교 우리는 표 11에서 볼 수 있듯이 RMT의 추론 속도를 다른 백본과 비교합니다. 우리 모델은 많은 경쟁자들 중에서 속도와 정확도 사이에서 가장 좋은 균형을 이룹니다. D. 명시적 붕괴의 세부 사항 Υ 다중 헤드 ReSA의 각 헤드에 대해 다른 것을 사용하여 각 헤드의 수용 필드를 제어하여 ReSA가 다중 스케일 정보를 인식할 수 있도록 합니다. ReSA 헤드의 모든 y를 특정 범위 내에 유지합니다. 특정 ReSA 모듈의 주어진 수용 필드 제어 간격을 [a, b]라고 가정합니다. 여기서 a와 b는 모두 양의 실수입니다. 그리고 ReSA 모듈 헤드의 총 수는 N입니다. i번째 헤드의 Y는 다음과 같이 쓸 수 있습니다. Eq. 8: Vi = 1a(ba)i (8) 다른 백본의 다른 단계에 대해 a와 b의 다른 값을 사용하며 자세한 내용은 표에 나와 있습니다. 12. 모델 블록 채널 헤드 비율 매개변수(M) FLOPS(G) RMT-T [2, 2, 8, 2] RMT-S [3, 4, 18, 4] RMT-B [4, 8, 25, 8] RMT-L [4, 8, 25, 8] RMT-DeiT-S [12] RMT-Swin-T [2, 2, 6, 2] RMT-Swin-S [2, 2, 18, 2] [64, 128, 256, 512] [4, 4, 8, 16] [3, 3, 3, 3]2.[64, 128, 256, 512] [4, 4, 8, 16] [4, 4, 3, 3]4.[80, 160, 320, 512] [112, 224, 448, 640] [5, 5, 10, 16] [4, 4, 3, 3]9.[7, 7, 14, 20] [4, 4, 3, 3]18.[384] [6] [4]4.[96, 192, 384, 768] [96, 192, 384, 768] [3, 6, 12, 24] [4, 4, 4, 4]4.[3, 6, 12, 24] [4, 4, 4, 4]9.표 10. 모델의 세부 아키텍처. 매개변수 FLOPS 모델(M) (G) 처리량 Top(imgs/s) (%) 매개변수 모델(M) FLOPS 처리량 Top(G) (imgs/s) (%) MPVIT-XS [29]2.80.Focal-S [63]9.83.Swin-T [35] BiFormer-T [75] GC-VIT-XT [20] SMT-T [34] RMT-T Focal-T [63] CSWin-T [11] Eff-B4 [47] MPVIT-S [29] Swin-S [35] SGFormer-S [15] iFormer-S [45]4.81.Eff-B5 [47]9.83.2.81.SGFormer-M [15]7.84.2.82.SMT-B [34]7.84.2.82.BiFormer-B [75]9.84.2.82.RMT-Swin-S9.84.Max ViT-S [51]11.84.4.82.CMT-B [16]9.84.4.82.iFormer-B [45]9.84.4.82.RMT-B9.85.4.83.8.83.Swin-B [35]15.83.4.83.Eff-B6 [47]19.84.4.83.Focal-B [63]16.84.CMT-S [16]4.83.CSWin-B [11]15.84.RMT-Swin-T4.83.MPVIT-B [29]16.84.CSwin-S [11]6.83.SMT-L [34]17.84.Max ViT-T [51]5.83.SGFormer-B [15]15.84.SMT-S [34]4.83.iFormer-L [45]14.84.BiFormer-S [75]4.83.Max ViT-B [51]23.84.RMT-S4.84.RMT-L18.85.표 11. 추론 속도 비교. 모델 ab RMT-T [2, 2, 2, 2] [6, 6, 8, 8] RMT-S [2, 2, 2, 2] [6, 6, 8, 8] RMT-B [2, 2, 2, 2] [7, 7, 8, 8] RMT-L RMT-DeiT-S RMT-Swin-T RMT-Swin-S [2, 2, 2, 2] [2] [2, 2, 2, 2] [2, 2, 2, 2] [8, 8, 8, 8] [8, 8, 8, 8] [8, 8, 8, 8] 표 12. 붕괴에 대한 세부 정보. Υ 참고문헌 [1] Moab Arar, Ariel Shamir 및 Amit H. Bermano. 효율적인 로컬 주의를 위한 학습된 쿼리. CVPR, 2022.[2] Zhaowei Cai 및 Nuno Vasconcelos. Cascade r-cnn: 고품질 객체 감지 탐구. CVPR, 2018. 5,[3] Chun-Fu (Richard) Chen, Rameswar Panda 및 Quanfu Fan. Region ViT: 비전 변환기를 위한 지역-지역 주의. ICLR, 2022. 5,[4] Kai Chen, Jiaqi Wang, Jiangmiao Pang, et al. MMDetection: Open mmlab 감지 도구 상자 및 벤치마크. arXiv 사전 인쇄본 arXiv:1906.07155, 2019. 5,[5] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia 및 Chunhua Shen. Twins: 비전 변환기에서 공간 주의 설계 재검토. NeurIPS, 2021.[6] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Chunhua Shen. 비전 변환기를 위한 조건부 위치 인코딩. ICLR, 2023. 2, 3,[7] MMSegmentation 기여자. Mmsegmentation, 오픈 소스 의미 분할 도구 상자, 2020. 7,[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, et al. Randaugment: 검색 공간이 줄어든 실용적 자동 데이터 증강. CVPRW, 2020. 4,[9] Jia Deng, Wei Dong, Richard Socher, et al. Imagenet: 대규모 계층적 이미지 데이터베이스. CVPR, 2009.[10] Mingyu Ding, Bin Xiao, Noel Codella, et al. Davit: 이중 주의 비전 변환기. ECCV, 2022.[11] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, et al. Cswin transformer: 십자가 모양 창이 있는 일반 비전 변환기 백본. CVPR, 2022. 2, 3, 5, 6, 7,[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, et al. 이미지는 16x16 단어의 가치가 있습니다: 규모에 따른 이미지 인식을 위한 변환기. ICLR, 2021. 1,[13] Qihang Fan, Huaibo Huang, Jiyang Guan, Ran He. 경량 비전 변환기에서 로컬 인식 재고, 2023. 1, 2,[14] Li Gao, Dong Nie, Bo Li, Xiaofeng Ren. 이중 융합 vit: 로컬 표현과 비전 변환기의 정보를 이중으로 융합. ECCV, 2022.[15] SG-Former: 진화하는 토큰 재할당을 갖춘 자체 유도 변환기. 수청런, 싱이양, 류쑹화, 왕신차오. ICCV, 2023. 5,[16] Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, Yunhe Wang. Cmt: 컨벌루션 신경망이 비전 변환기를 만납니다. CVPR, 2022. 1, 3, 4, 5, 6, 8,[17] Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng 및 Shi-Min Hu. 시각적 주의 네트워크. arXiv 사전 인쇄 arXiv:2202.09741, 2022. 5,[18] Kai Han, An Xiao, Enhua Wu, et al. 변압기 속의 변압기. NeurIPS, 2021.[19] Ali Hassani, Steven Walton, Jiachen Li, Shen Li 및 Humphrey Shi. 이웃주의 변압기. CVPR, 2023. 1, 2, 3, 5, 6,[20] Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz 및 Pavlo Molchanov. 글로벌 컨텍스트 비전 변환기. ICML, 2023. 5, 6, 7,[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren 및 Sun Jian. 이미지 인식을 위한 심층 잔여 학습. CVPR, 2016. 6,[22] Kaiming He, Georgia Gkioxari, Piotr Dollár 및 Ross B. Girshick. 마스크 r-cnn. ICCV, 2017. 5,[23] Qibin Hou, Cheng-Ze Lu, Ming-Ming Cheng, Jiashi Feng. Conv2former: 시각적 인식을 위한 간단한 변환기 스타일 convnet. arXiv 사전 인쇄본 arXiv:2211.11943, 2022.[24] Gao Huang, Yu Sun, Zhuang Liu. 확률적 깊이를 갖춘 딥 네트워크. ECCV에서, 2016. 4,[25] Huaibo Huang, Xiaoqiang Zhou, Ran He. 직교 변환기: 토큰 직교화를 갖춘 효율적인 비전 변환기 백본. NeurIPS에서, 2022. 5,[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, et al. 노이즈가 많은 텍스트 감독을 통한 시각 및 시각 언어 표현 학습 확장. ICML에서, 2021.[27] Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, Jiashi Feng. 모든 토큰은 중요합니다: 더 나은 비전 변환기를 훈련하기 위한 토큰 라벨링. NeurIPS, 2021. 1, 5,[28] Alexander Kirillov, Ross Girshick, Kaiming He 및 Piotr Dollar. 파노라마 피처 피라미드 네트워크. CVPR, 2019. 7,[29] Youngwan Lee, Jonghee Kim, Jeffrey Willette 및 Sung Ju Hwang. Mpvit: 밀집 예측을 위한 다중 경로 비전 변환기. CVPR, 2022. 1, 5, 6, 7,[30] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng Li 및 Yu Qiao. Uniformer: 효율적인 시공간 표현 학습을 위한 통합 변환기, 2022. 1, 4,5,6,[31] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong, Jitendra Malik, Christoph Feichtenhofer. Mvitv2: 분류 및 감지를 위한 개선된 다중 스케일 비전 변환기. CVPR에서, 2022. 1,[32] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He 및 Piotr Dollar. 밀집 객체 감지를 위한 초점 손실. ICCV에서, 2017. 5,[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, et al. Microsoft coco: 컨텍스트 내 공통 객체. ECCV에서, 2014.[34] Weifeng Lin, Ziheng Wu, Jiayu Chen, Jun Huang, Lianwen Jin. 스케일 인식 변조가 변압기를 만납니다.ICCV, 2023. 1, 5, 6, 7, 8,[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. Swin 변압기: 이동된 창을 사용하는 계층적 비전 변압기.ICCV, 2021. 1, 2, 3, 4, 5, 6, 7, 8, 9,[36] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, et al. 2020년대를 위한 convnet.CVPR, 2022. 5,[37] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi. Unified-io: 비전, 언어 및 다중 모달 작업을 위한 통합 모델. ICLR에서, 2023.[38] Junting Pan, Adrian Bulat, Fuwen Tan, et al. Edgevits: 비전 변환기를 사용한 모바일 기기의 경쟁 경량 CNN. ECCV에서, 2022.[39] Zizheng Pan, Jianfei Cai, and Bohan Zhuang. hilo 어텐션을 사용한 빠른 비전 변환기. NeurIPS에서, 2022. 2,[40] Boris T Polyak 및 Anatoli B Juditsky. 평균화를 통한 확률적 근사 가속화. arXiv 사전 인쇄본 arXiv:1906.07155, 2019.[41] Ofir Press, Noah Smith, and Mike Lewis. 짧게 학습하고 길게 테스트: 선형 편향이 있는 어텐션은 입력 길이 외삽을 가능하게 함. ICLR에서, 2022. 2,[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, et al. 자연어 감독을 통해 전달 가능한 시각적 모델을 학습합니다. ICML, 2021.[43] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, Ser-Lam Lim 및 Jiwen Lu. Hornet: 재귀 게이트 컨볼루션을 사용한 효율적인 고차 공간 상호 작용. NeurIPS, 2022. 6,[44] Sucheng Ren, Daquan Zhou, Shengfeng He, Jiashi Feng 및 Xinchao Wang. 다중 규모 토큰 집계를 통해 self-attention을 차단했습니다. CVPR, 2022.[45] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao Wang 및 Shuicheng YAN. 인셉션 트랜스포머. NeurIPS, 2022. 5,[46] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei. Retentive network: 대규모 언어 모델을 위한 Transformer의 후속 모델. Arxiv, abs/2307.08621, 2023. 1, 2, 3,[47] Mingxing Tan 및 Quoc Le. Efficientnet: 합성곱 신경망을 위한 모델 스케일링 재고. ICML, 2019.[48] Shitao Tang, Jiahui Zhang, Siyu Zhu, et al. 비전 변환기를 위한 Quadtree attention. ICLR, 2022.[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, et al. 학습 데이터 효율적인 이미지 변환기 및 주의를 통한 증류. ICML, 2021. 2, 4, 5, 7, 8,[50] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, Hervé Jégou. 이미지 변환기를 더 깊이 파고들다. ICCV, 2021. 1,[51] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li. Maxvit: 다축 비전 변환기. ECCV, 2022. 1, 5, 8,[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 주의만 있으면 됩니다. NeurIPS, 2017.[53] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo 및 Ling Shao. 피라미드 비전 변환기: 컨볼루션 없이 조밀한 예측을 위한 다용도 백본입니다. ICCV, 2021. 2, 3, 4, 6,[54] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo 및 Ling Shao. Pvtv2: 피라미드 비전 변환기로 기준선을 개선했습니다. 전산 영상 미디어, 8(3):1–10, 2022. 2, 3, 4, 5, 6,[55] Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He 및 Wei Liu. Crossformer: 교차 규모의 관심을 끄는 다용도 비전 변환기입니다. ICLR, 2022. 5,6,[56] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: 변형 가능한 컨볼루션을 갖춘 대규모 비전 기반 모델을 탐색합니다. CVPR, 2023. 5, 6,[57] Haiping Wu, Bin Xiao, Noel Codella, Xiyang Dai, Lu Yuan 및 Lei Zhang. 비전 변환기에 컨볼루션을 적용합니다. arXiv:2103.15808, 2021. 1, Mengchen Liu, Cvt: IntroducarXiv 사전 인쇄 [58] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li 및 Gao Huang. 변형 가능한 주의력을 갖춘 비전 트랜스포머. CVPR, 2022. 2, 6,[59] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang 및 Jian Sun. 장면 이해를 위한 통합 지각 분석. ECCV, 2018. 7,[60] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang 및 Jingren Zhou. mplug-2: 텍스트, 이미지, 비디오 전반에 걸쳐 모듈화된 다중 모드 기반 모델입니다. ICML에서는 2023년.[61] Chenglin Yang, Yilin Wang, Jianming Zhang 등. 향상된 Self-Attention을 갖춘 Lite 비전 변환기. CVPR에서는 2022.[62] Chenglin Yang, Siyuan Qiao, Qihang Yu 등. 해자(Moat): 모바일 컨볼루션과 어텐션을 번갈아 사용하면 강력한 비전 모델이 제공됩니다. ICLR, 2023.[63] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan 및 Jianfeng Gao. 비전 변환기의 로컬-글로벌 상호 작용을 위한 초점 selfattention입니다. NeurIPS, 2021. 2, 3, 5, 6, 9,[64] Jianwei Yang, Chunyuan Li, Xiyang Dai 및 Jianfeng Gao. 초점 변조 네트워크. NeurIPS, 2022. 5,[65] Rui Yang, Hailong Ma, Jie Wu, Yansong Tang, Xuefeng Xiao, Min Zheng 및 Xiu Li. Scalablevit: 비전 변환기의 상황 중심 일반화를 다시 생각합니다. ECCV, 2022. 5,[66] Ting Yao, Yingwei Pan, Yehao Li, Chong-Wah Ngo 및 Tao Mei. Wave-vit: 시각적 표현 학습을 위한 웨이블릿 및 변환기 통합. 유럽 컴퓨터 비전 컨퍼런스(ECCV) 회의록, 2022. 2,[67] Ting Yao, Yehao Li, Yingwei Pan, Yu Wang, Xiao-Ping Zhang 및 Tao Mei. 듀얼 비전 변환기. TPAMI, 2023.[68] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng 및 Shuicheng Yan. Volo: 시각적 인식을 위한 비전 아웃룩커. TPAMI, 2022.[69] Sangdoo Yun, Dongyoon Han, Seong Joon Oh 및 et al. Cutmix: 지역화 가능한 특징을 가진 강력한 분류기를 훈련하기 위한 정규화 전략. ICCV, 2019. 4,[70] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, et al. 혼동: 경험적 위험 최소화를 넘어서는 것입니다. ICLR, 2018. 4,[71] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei Zhang 및 Jianfeng Gao. Multi-scale Vision Longformer: 고해상도 이미지 인코딩을 위한 새로운 비전 변환기입니다. ICCV, 2021.[72] Qiming Zhang, Yufei Xu, Jing Zhang, Dacheng Tao. Vsa: 비전 변환기에서 다양한 크기의 창 주의를 학습합니다. ECCV, 2022.[73] Zhun Zhong, Liang Zheng, Guoliang Kang 등. 무작위 삭제 데이터 증대. AAAI, 2020. 4,[74] Bolei Zhou, Hang Zhao, Xavier Puig, et al. ade20k 데이터 세트를 통한 장면 구문 분석. CVPR, 2017.[75] Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, Rynson Lau. Biformer: 2단계 라우팅 어텐션을 갖춘 비전 변환기. CVPR, 2023. 1, 2, 4, 5, 6, 8,
