--- ABSTRACT ---
텍스트 전용 코퍼스를 사용한 도메인 적응은 종단간(E2E) 음성 인식에서 어렵습니다. TTS를 통해 텍스트에서 오디오를 합성하여 적응하는 것은 리소스를 많이 소모합니다. 텍스트 전용 코퍼스를 사용하여 빠른 도메인 적응을 가능하게 하기 위해 USTR-CT(Unified Speech-Text Representation in Conformer Transducer)를 학습하는 방법을 제시합니다. 이전 텍스토그램 방법과 달리 텍스트 표현을 학습하기 위해 추가 텍스트 인코더를 우리 작업에 도입하고 추론 중에 제거하므로 온라인 배포에 대한 수정이 없습니다. 적응의 효율성을 개선하기 위해 단일 단계 및 다단계 적응도 탐구합니다. LibriSpeech를 SPGISpeech에 적응시키는 실험은 제안하는 방법이 대상 도메인에서 단어 오류율(WER)을 비교적 44% 줄이는 것을 보여주며, 이는 TTS 방법 및 텍스토그램 방법보다 우수합니다. 또한 제안하는 방법을 내부 언어 모델 추정(ILME)과 결합하여 성능을 더욱 개선할 수 있음을 보여줍니다. 색인 용어: 자동 음성 인식, 텍스트 전용, 도메인 적응, 적합 변환기 1.
--- INTRODUCTION ---
최근 몇 년 동안 E2E 모델은 자동 음성 인식(ASR)에서 상당한 개선을 이루었습니다[1, 2, 3]. 음향, 발음 및 언어 모델(LM)이 별도로 구축되고 최적화된 하이브리드 모델과 비교할 때, E2E 모델은 음성 특징을 단어 시퀀스에 직접 매핑하여 유망한 성능을 달성했습니다. 연결주의 시간 분류[4, 5], 순환 신경망 변환기(RNNT)[2, 6, 7], 주의 기반 인코더-디코더[8, 9, 10]를 포함한 몇 가지 인기 있는 E2E 모델이 있습니다. 그러나 텍스트 전용 데이터를 사용하여 LM을 학습하여 새로운 도메인에 적응할 수 있는 하이브리드 모델과 달리 E2E 모델은 텍스트 전용 데이터를 사용하여 도메인 적응에 어려움이 있습니다. 게다가 E2E 모델은 쌍을 이룬 음성-텍스트 데이터로 학습되므로 다양한 콘텐츠에 대한 일반화 능력이 제한적이며 소스 도메인과 타겟 도메인 간에 불일치가 있으면 성능이 저하됩니다. 이를 극복하기 위해 가장 유망한 접근 방식은 텍스트 전용 데이터를 사용하여 E2E 모델을 조정하는 것입니다.대상 도메인에서 쌍을 이룬 음성-텍스트 데이터보다 텍스트 전용 데이터를 수집하는 것이 훨씬 쉽기 때문입니다.E2E 모델을 새로운 도메인에 조정하기 위해 여러 가지 방법이 제안되었습니다.가장 일반적인 솔루션은 대상 도메인에서 텍스트 코퍼스를 사용하여 외부 LM을 학습하고 추론 중에 E2E 모델에 통합하는 것입니다.여기에는 얕은 융합[11], 밀도 비율 융합[12], 딥 융합[13], 콜드 융합[14] 및 내부 LM(ILM) 추정 기반 융합[15, * Equal contribution. 16]이 있습니다.그럼에도 불구하고 이러한 모든 방법은 추론 중에 외부 LM을 포함하고 디코딩의 계산 비용이 증가합니다.다른 방법은 디코딩을 변경하지 않기 위해 E2E 모델을 직접 업데이트하려고 시도합니다.TTS를 사용하여 쌍을 이룬 음성-텍스트 데이터를 합성하는 것은 일반적인 솔루션[17, 18]이지만 프로세스가 복잡하고 저장 및 계산 비용도 증가합니다. 최근, TTS를 대체하기 위해 E2E 모델[19]에서 텍스트-멜-스펙트로그램 생성기가 제안되었습니다. [20]에서 제안된 방법은 예측 네트워크에 임시 LM 계층을 삽입하고 LM 손실은 텍스트 전용 데이터로 적응하는 데 사용됩니다. 내부 LM 적응(ILMA)[21]은 추가 LM 손실로 내부 LM의 매개변수를 미세 조정하기 위해 제안되었습니다. 대안적인 접근 방식은 음성과 텍스트의 두 가지 모달리티에 대한 공동 학습을 통해 공유 임베딩 공간을 만드는 데 중점을 두고 있으며 모델 매개변수나 디코딩 복잡성을 증가시키지 않고도 ASR 성능을 개선하는 것으로 보고되었습니다[22, 23]. 최근 연구[24, 25]는 텍스트 전용 적응 작업에서 텍스트와 음성 기능 간에 일관된 표현을 만드는 이 아이디어를 포함합니다. [22, 24, 25]에서 영감을 얻어 빠른 텍스트 전용 도메인 적응을 위해 Conformer Transducer(USTR-CT)에서 통합 음성-텍스트 표현을 학습하는 방법을 제안했습니다. 분리된 인코더를 채택하여 각각 음성 및 텍스트 특징에 대한 공유 표현을 학습하고, 공유 인코더를 사용하여 음성 및 텍스트 표현을 융합합니다. 동시에, 다양한 표현 단위가 탐색되고 음소 표현이 대상 도메인에서 가장 잘 수행됩니다. 적응의 효율성을 개선하기 위해 단일 단계 및 다중 단계 적응도 탐색합니다. 마지막으로, 페어링되지 않은 텍스트 데이터로 대상 도메인에서 44%의 상대적 WER 감소를 관찰합니다. 또한 ILME와 결합하여 제안된 방법은 추가 이득을 얻을 수 있습니다. 논문은 다음과 같이 구성됩니다. 섹션 2에서는 다음에 대한 간략한 소개를 제공합니다.
--- RELATED WORK ---
. 제안된 USTR-CT는 섹션 3에서 논의되고, 그 뒤를 이어 섹션 4에서 실험과 논의가 이어진다. 2. 관련 연구 2.1. ASR을 위한 음성-텍스트 공동 훈련 여러
--- METHOD ---
텍스트 전용 코퍼스를 사용하여 빠른 도메인 적응을 가능하게 하기 위해 USTR-CT(Unified Speech-Text Representation in Conformer Transducer)를 학습합니다. 이전 텍스토그램 방법과 달리, 텍스트 표현을 학습하기 위해 추가 텍스트 인코더가 우리 작업에 도입되었으며 추론 중에 제거되므로 온라인 배포에 대한 수정이 없습니다. 적응의 효율성을 개선하기 위해 단일 단계 및 다중 단계 적응도 탐색합니다.
--- EXPERIMENT ---
영어: LibriSpeech를 SPGISpeech에 적용하는 s는 제안된 방법이 대상 도메인에서 단어 오류율(WER)을 비교적 44% 줄이는 것을 보여주며, 이는 TTS 방법 및 텍스토그램 방법보다 우수합니다. 또한, 제안된 방법을 내부 언어 모델 추정(ILME)과 결합하여 성능을 더욱 향상시킬 수 있음을 보여줍니다. 색인 용어: 자동 음성 인식, 텍스트 전용, 도메인 적응, 컨포머 변환기 1. 서론 최근 몇 년 동안 E2E 모델은 자동 음성 인식(ASR)에서 상당한 개선을 이루었습니다[1, 2, 3]. 음향, 발음 및 언어 모델(LM)을 별도로 구축하고 최적화하는 하이브리드 모델과 비교할 때, E2E 모델은 음성 특징을 단어 시퀀스에 직접 매핑하여 유망한 성능을 달성했습니다. 연결주의 시간 분류[4, 5], 순환 신경망 변환기(RNNT)[2, 6, 7], 어텐션 기반 인코더-디코더[8, 9, 10]를 포함한 몇 가지 인기 있는 E2E 모델이 있습니다. 그러나 텍스트 전용 데이터를 사용하여 LM을 훈련하여 새로운 도메인에 적응할 수 있는 하이브리드 모델과 달리 E2E 모델은 텍스트 전용 데이터를 사용하여 도메인 적응에 어려움이 있습니다.게다가 E2E 모델은 쌍을 이룬 음성-텍스트 데이터로 훈련되므로 다른 콘텐츠에 대한 일반화 능력이 제한되고 소스 도메인과 타겟 도메인 간에 불일치가 있으면 성능이 저하됩니다.이를 극복하기 위해 가장 유망한 접근 방식은 텍스트 전용 데이터를 사용하여 E2E 모델을 적응시키는 것입니다.대상 도메인에서 쌍을 이룬 음성-텍스트 데이터보다 텍스트 전용 데이터를 수집하는 것이 훨씬 쉽기 때문입니다.E2E 모델을 새로운 도메인에 적응시키기 위해 여러 가지 방법이 제안되었습니다.가장 일반적인 솔루션은 대상 도메인에서 텍스트 코퍼스를 사용하여 외부 LM을 훈련하고 추론 중에 E2E 모델에 통합하는 것입니다.이에는 얕은 융합[11], 밀도 비율 융합[12], 딥 융합[13], 콜드 융합[14] 및 내부 LM(ILM) 추정 기반 융합[15, * 동등 기여.16]이 있습니다. 그럼에도 불구하고, 이러한 모든 방법은 추론 중에 외부 LM을 포함하고 디코딩의 계산 비용이 증가합니다. 다른 방법은 디코딩을 변경하지 않기 위해 E2E 모델을 직접 업데이트하려고 시도합니다. TTS를 사용하여 쌍을 이룬 음성-텍스트 데이터를 합성하는 것은 일반적인 솔루션[17, 18]이지만 프로세스가 복잡하고 저장 및 계산 비용도 증가합니다. 최근 TTS를 대체하기 위해 E2E 모델[19]에서 텍스트-멜-스펙트로그램 생성기가 제안되었습니다. [20]에서 제안된 방법은 예측 네트워크에 임시 LM 계층을 삽입하고 LM 손실을 사용하여 텍스트 전용 데이터로 적응합니다. 내부 LM 적응(ILMA)[21]은 추가 LM 손실로 내부 LM의 매개변수를 미세 조정하기 위해 제안되었습니다. 대안적인 접근 방식은 음성과 텍스트의 두 가지 모달리티에 대한 공동 학습을 통해 공유 임베딩 공간을 만드는 데 중점을 두고 있으며 모델 매개변수나 디코딩 복잡성을 증가시키지 않고도 ASR 성능을 개선하는 것으로 보고되었습니다[22, 23]. 최근 연구[24, 25]는 텍스트 전용 적응 작업에서 텍스트와 음성 특징 간에 일관된 표현을 만드는 이 아이디어를 포함합니다. [22, 24, 25]에서 영감을 얻어 빠른 텍스트 전용 도메인 적응을 위해 Conformer Transducer(USTR-CT)에서 통합된 음성-텍스트 표현을 학습하는 방법을 제안했습니다. 분리된 인코더를 채택하여 음성 및 텍스트 특징에 대한 공유 표현을 각각 학습하고 공유 인코더를 사용하여 음성 및 텍스트 표현을 융합합니다. 동시에 다양한 표현 단위가 탐색되고 음소 표현이 대상 도메인에서 가장 좋은 성능을 보입니다. 적응의 효율성을 높이기 위해 단일 단계 및 다단계 적응도 탐색합니다. 마지막으로 페어링되지 않은 텍스트 데이터로 대상 도메인에서 44%의 상대적 WER 감소를 관찰합니다. 또한 ILME와 결합하여 제안된 방법은 추가 이득을 얻을 수 있습니다. 논문은 다음과 같이 구성됩니다. 섹션 2에서는 관련 작업에 대한 간략한 소개를 제공합니다. 제안된 USTR-CT는 섹션 3에서 논의되고, 섹션 4에서는 실험과 논의가 이어진다. 2. 관련 연구 2.1. ASR을 위한 음성-텍스트 공동 학습 음성 및 텍스트 모달리티를 사용하여 E2E 모델을 학습하는 여러 방법이 제안되었다[22, 23, 26, 27, 28, 29, 30, 31]. 최근 JOIST[23]는 캐스케이드 인코더 기반 스트리밍 ASR 프레임워크[32]에서 감독된 쌍을 이룬 음성 텍스트 데이터와 쌍을 이루지 않은 텍스트 데이터에서 계산된 손실을 조합하여 공동 학습을 탐구한다. 입력된 쌍을 이루지 않은 텍스트 표현은 간단하고 매개변수가 없는 지속 시간 모델로 업샘플링된 다음 마스킹 후 텍스트 인코더에 입력된다. 텍스트 인코더의 출력은 1차 통과 디코더 또는 공유 인코더와 2차 통과 디코더에 입력될 수 있다. 그러나 실험은 다중 도메인 코퍼스에서 수행되었고 롱테일 희귀 단어의 성능이 평가되었다. 이 작업에서 우리는 주로 도메인 적응, 즉 텍스트 전용 데이터가 있는 대상 도메인으로 모델을 전송하는 데 중점을 둡니다. 음성 및 텍스트 기능에서 학습한 표현이 정렬되도록 하기 위해 MAESTRO[22]는 일관성 손실을 도입하여 쌍 데이터를 사용하여 두 가지 유형의 표현을 정렬합니다. 이 방법은 ASR 및 음성 번역 작업에서 상당한 개선을 이루었습니다. 우리 작업과 달리 도메인 적응보다는 더 나은 사전 학습된 모델을 얻기 위해 자체 감독 학습에 중점을 둡니다. 게다가, 추가 지속 시간 모델은 학습의 복잡성을 증가시킵니다. 2.2. 텍스트 전용 도메인 적응 이 하위 섹션에서는 외부 LM을 사용하지 않고 텍스트 전용 도메인 적응 방법을 간략하게 소개합니다. TTS 적응은 미세 조정을 위해 대상 도메인 텍스트에서 쌍을 이룬 음성-텍스트 데이터를 생성합니다[17, 18]. 그러나 TTS의 경우 화자 수가 제한되어 있으며 신뢰할 수 있는 다중 화자 TTS 모델을 학습하는 데 시간이 많이 걸리고 계산 비용이 많이 듭니다. 게다가 합성된 데이터를 저장하면 저장 비용도 증가합니다. 게다가 합성 오디오와 실제 오디오의 불일치로 인해 추가적인 문제가 발생할 수도 있습니다. 위의 문제를 완화하기 위해 최근 텍스트-멜-스펙트로그램 생성기로 구성된 텍스트-스펙트로그램 프런트엔드가 ASR 모델에 도입되었습니다[19]. 이런 방식으로 텍스트 전용 데이터에서 즉석으로 스펙트로그램을 생성하는 것이 가능하며, 이전처럼 실제 오디오와 합성 오디오의 불일치가 명확하지 않습니다. 그러나 여전히 학습하는 동안 생성기의 스펙트로그램 증강기의 품질에 주의를 기울여야 합니다. ILMA는 텍스트 전용 데이터로 ILM을 미세 조정하여 E2E 모델[21]을 적용하고, 과적합을 방지하기 위해 매개변수 정규화를 추가할 것을 제안합니다. 또한 ILM이 독립형 LM처럼 동작하도록 하기 위해 ILMA 전에 ASR 손실 외에도 ILM 학습(ILMT)[33]을 수행하는 것이 필수적입니다. 조인터 네트워크의 마지막 선형 계층만 업데이트되므로 대상 도메인에서 ILMA의 성능도 제한됩니다. Textogram은 [24]에서 제안되었는데, 여기서 텍스트 표현은 고정된 횟수만큼 텍스트 토큰의 원핫 임베딩을 반복하여 생성됩니다. Textogram 특징은 표준 음성 특징과 함께 쌓입니다. 텍스트 전용 데이터를 사용하여 학습하는 경우 음성 특징은 0으로 설정됩니다. 그리고 쌍을 이룬 음성-텍스트 데이터를 사용하여 학습하는 경우 Textogram 특징은 0으로 설정됩니다. 음성 특징과 Textogram이 연결되기 때문에 추론 중에 0 Textogram 특징을 음성 특징과 연결해야 합니다. 그러나 분리된 인코더를 사용하는 저희 작업에서는 학습 중에 입력이 텍스트 특징이거나 음성 특징이며, 아무런 수정 없이 직접 추론을 수행할 수 있습니다. 게다가 Textogram에서는 조인터만 업데이트하는 것이 가장 좋은 성능을 내는 반면, 조인터, 예측기, 심지어 인코더도 더 나은 성능으로 대상 도메인에 맞게 조정할 수 있습니다. 저희 작업에서는 문자소 표현 외에도 하위 단어와 음소 표현도 탐구합니다. 3. 학습 및 적응 방법 3.1. 모델 아키텍처 표준 RNN-T의 경우 음성-텍스트 쌍을 사용하여 모델을 학습합니다. 오디오 특징이 Fbank이고, 이전 토큰이 yo:u-Speech이고, 프레임 t에서 RNN-T의 출력과 단계 예측자 Yo:uP(yu) 조인터 공유 인코더 인코더 오디오 인코더 텍스트 인코더 음성 1:T 1.페어된 음성과 텍스트 말하지 않은 텍스트 그림 1: USTR-RNN-T의 모델 구조. u는 다음에 의해 계산됩니다. h1:T hpred = Predictor (yo:u-1), enc Encoder (xh), speech (1) (2) (3) joint = Joiner (따라서 pred). 영어: jointer t, 그런 다음 시간과 forwardbackward 알고리즘[7]에 Softmax 계층을 사용하여 모든 가능한 정렬 경로 π의 합 확률 P(yx)인 변환기 손실은 다음과 같이 학습 목적 함수 Linn-t == - log Σ P(7|x speech), ་ ་ ΠΕΠ(y) 1:T (4)로 계산됩니다. 여기서 y = (y1,..., yu)는 레이블 시퀀스이고 U는 대상 토큰의 수이며 II(y)는 정렬 경로 세트입니다. 학습 중에 텍스트 전용 코퍼스를 포함시키기 위해 RNNT 인코더는 AudioEncoder와 Shared Encoder라는 두 부분으로 나뉘고 추가 TextEncoder가 텍스트 피처 X1N을 모델화하는 텍스트에 도입됩니다. 이는 그림 1에 USTR-RNN-T로 설명되어 있습니다. Transducer 손실은 쌍을 이룬 음성-텍스트 코퍼스와 동일한 방식으로 계산할 수 있습니다. enc,text 1:N = SharedEncoder(TextEncoder(X/Y/N)), (5) joint,text hn, u text enc,text = Jointer(h &quot;, hored), (6) 여기서 xx는 문자소/하위 단어/음소 표현일 수 있습니다. 추론 중에 추가 TextEncoder를 제거할 수 있으므로 제안된 방법은 온라인 배포를 위해 수정할 필요가 없습니다. 실험에서 SharedEncoder는 12개의 비스트리밍 Conformer[34] 계층으로 구성되므로 기준선은 Conformer Transducer(CT)로 표시됩니다. AudioEncoder에는 스트라이드가 2인 두 개의 Conv2d 계층과 선형 투영 계층이 있어 시간이 4만큼 단축됩니다. TextEncoder에는 임베딩 계층과 Transformer 계층이 포함됩니다. CT의 예측 네트워크의 경우 2계층 LSTM이 채택되었고 RNN-T의 조인터 네트워크는 피드포워드 계층입니다. 3.2. 학습 SharedEncoder의 공동 임베딩 공간에서 음성-텍스트 모달리티 매칭을 적용하려면 AudioEncoder와 TextEncoder를 학습하기 위해 쌍으로 된 음성-텍스트 샘플이 필요합니다.TTS 쌍으로 된 음성 및 텍스트 말하지 않은 대상 도메인 텍스트 TTS 오디오 Dp Du Dp Dtts Dp+ Dtts CT Init 모델 단일 단계 USTR-CT Dp + Du Dp 다중 단계 USTR-CT Dp + Du USTR-CT TTS 적응 적응형 CT 그림 2: TTS, 다중 단계 및 단일 단계 USTR-CT의 적응 프로세스.TextEncoder가 학습에 도입되면 학습 코퍼스의 쌍으로 된 음성-텍스트가 오디오 특징 대신 텍스트 특징을 사용하여 무작위 확률 p로 말하지 않은 텍스트로 사용됩니다.이 작업에서는 세 가지 유형의 텍스트 특징을 고려합니다.첫 번째는 [24]의 텍스토그램과 유사한 문자소 특징입니다. 두 번째는 CT의 출력 어휘와 동일한 하위 단어 특징이며, subword-nmt[35]¹를 사용하여 생성됩니다. 마지막은 Grapheme-to-Phoneme 시스템으로 생성된 음소 특징입니다. 이 작업의 영어에 대해 g2pE²를 사용했습니다. 음성 특징의 지속 시간을 시뮬레이션하기 위해 텍스트 특징을 고정된 횟수만큼 반복하는데, 이는 [24,23]의 경우와 동일합니다. 또한 텍스트 특징은 텍스트 인코더가 자소/하위 단어 시퀀스를 맹목적으로 기억하지 못하도록 마스크 처리됩니다[24, 23]. 그러나 마스킹 방법은 반복되는 텍스트 특징에 적용하여 [24, 23]의 방법과 다릅니다. 반복하기 전에 마스킹하면 성능이 더 좋아지는 것으로 나타났습니다. 학습하는 동안 텍스트와 음성 특징을 모두 포함하는 미니 배치가 모델에 입력됩니다. 또한 ILMT 손실은 선택적 보조 손실로 선택되고 전체 손실은 L=Lrnn-t Lilm ilmt인 경우이고, (7)은 모든 실험에서 0.2로 설정된 ILMT 손실에 해당하는 가중치입니다.3.3. 적응 텍스트 전용 코퍼스를 사용하여 CT 모델을 새 도메인에 적응시키는 경우 이 작업에서는 그림 2에서와 같이 두 가지 적응 전략을 조사합니다.3.3.1. 다단계 적응 그림 2의 아랫부분에 나와 있듯이 USTR-CT를 사용하는 다단계 적응에는 두 단계가 포함됩니다.첫 번째 단계에서는 페어링된 음성-텍스트 데이터를 사용하여 USTR-CT를 훈련하고, 각 샘플은 오디오 기능 대신 텍스트 기능을 사용하여 TextEncoder에 공급되어 무작위 확률 p로 TextEncoder를 훈련합니다.실험에서 확률 p는 0.15로 설정됩니다. 두 번째 단계, 즉 적응 단계에서는 쌍을 이룬 음성 텍스트 데이터와 말하지 않은 텍스트가 모두 각 미니 배치에서 1:1의 비율로 사용됩니다. 이 비율은 대상 도메인에서 더 나은 성능을 얻기 위해 추가로 조정할 수 있으며, 이는 향후 논의를 위해 남겨둡니다. 쌍을 이룬 음성-텍스트 데이터는 소스 도메인에서 https://github.com/rsennrich/subword-nmt 2https://github.com/Kyubyong/g2p 성능을 유지하는 데 사용됩니다. 이 단계에서는 AudioEncoder와 Shared Encoder(즉, CT의 인코더)의 매개변수가 일정하게 유지되는 반면, Jointer와 Predictor는 새로운 도메인에 적응하도록 훈련됩니다. 첫 번째 단계 이후에 USTRCT 모델이 존재하기 때문에 다중 도메인 시나리오가 있는 경우 다른 도메인에 적응하는 것이 더 편리합니다. 3.3.2. 단일 단계 적응 그림 2의 중간 부분에서 볼 수 있듯이 단일 단계 USTR-CT는 무작위 초기화에서 직접 적응된 CT 모델을 훈련합니다. 다단계 USTR-CT와 유사하게, 쌍으로 된 음성-텍스트 데이터도 확률 p = 0.15로 텍스트 인코더에 입력됩니다. 또한, 쌍으로 된 음성-텍스트 데이터와 말하지 않은 텍스트 간의 비율은 다단계 적응과 일관성을 유지하기 위해 여전히 1:1입니다. 4. 실험 및 결과 4.1. 실험 설정 실험은 LibriSpeech[36] 및 SPGISpeech[37] 코퍼스에서 수행됩니다. SPGISpeech에는 5,000시간 분량의 금융 오디오가 포함되어 있습니다. 이 작업에서는 170만 개의 발화가 있는 SPGISpeech의 필사된 텍스트만 텍스트 전용 도메인 적응에 사용됩니다. 실험에서는 Large(L) 및 Small(S)로 표시된 두 가지 버전의 텍스트를 생성하는데, 전자는 전체 170만 개의 발화를 포함하고 후자는 Librispeech 발화 수와 거의 같은 280,000개의 발화 하위 집합을 포함합니다. 또한 TTS 오디오는 TTS가 리소스를 많이 소모한다는 것을 나타내기 위해 사내 엔진을 사용하여 Small 하위 집합에서 합성됩니다. 오디오 특징의 경우 80-dim 필터 뱅크(Fbank)가 사용되고 Spec-Augment[38]가 Fbank 특징에 적용됩니다. 텍스트 특징은 반복되기 전에 0.15의 확률로 마스크됩니다. 모델의 구조는 섹션 3과 같으며 RNN-T의 출력은 4,048개의 하위 단어 단위입니다. 모든 모델은 PyTorch[39]로 훈련되었습니다. WER은 Librispeech test-clean/test-other 세트와 SPGISpeech val 세트에서 평가되어 소스 및 대상 도메인에서 ASR 성능을 측정합니다. 4.2. 기준 시스템 CT 모델은 SPGISpeech val 세트에서 23.55%의 WER을 달성하는 LibriSpeech에서 훈련되었습니다. 그림 2의 상단에 표시된 것처럼 TTS 기반 적응을 사용하면 WER이 상대적으로 36.35% 감소하여 14.99%로 줄어듭니다. 또한 이 작업에서는 23.94%의 WER을 달성하는 텍스토그램[24] 방법도 평가합니다. 인코더와 조인터를 일정하게 유지하는 텍스토그램 기반 적응은 텍스트 전용 코퍼스로 학습합니다. 그리고 적응 후 S/L 하위 집합을 사용할 때 WER이 각각 상대적으로 33.25%/37.80% 감소합니다. 제안된 다단계 USTR-CT는 먼저 0.15의 비율로 마스킹하고 4번 반복하여 그래핌 표현으로 학습합니다. 표 1에서 볼 수 있듯이 제안된 USTR-CT는 적응 전에 SPGISpeech val set에서 22.72%의 WER을 달성하여 텍스토그램보다 우수합니다. 적응 후, 제안된 방법은 대상 도메인에서 더 나은 성능을 보일 뿐만 아니라 LibriSpeech 테스트 세트에서도 최상의 성능을 달성하는데, 이는 적응하는 동안 소스 도메인의 성능을 유지하기 위해 쌍을 이룬 음성-텍스트 데이터가 사용되기 때문이다. 결과에 따르면 USTR-CT의 추가 텍스트 인코더와 두 번째 단계의 조인터 적응이 유익하다. 또한, 제안된 방법은 소스 도메인과 대상 도메인 모두에서 TTS 기반 적응보다 성능이 뛰어나며, 상대적 WER 감소율은 2.54% 5.45%이다. SPGISpeech val 표 1: LibriSpeech 테스트 세트와 SPGISpeech val 세트에서 다양한 시스템의 WER(%). 텍스트 적응 L/S는 SPGISpeech의 필사된 텍스트의 Large/Small 하위 세트에 해당한다. 모델 LibriSpeech 테스트 clean/other 표 3: 다중/단일 단계 USTR-CT의 WER(%). 모델 LibriSpeech 테스트 clean/다른 SPGISpeech val CT + TTS 적응 3.85/8.14.CT 다단계 USTR-CT 3.82/8.22.3.99/8.23.+ 텍스트 적응(L) 3.64/7.13.+ TTS 적응 3.85/8.14.단일 단계 USTR-CT(L) | 3.07/7.13.textogram 기준선 [24] 4.18/8.23.+ 텍스트 적응(S) 5.12/10.15.표 4: ILME가 있는 다양한 모델의 WER(%). + 텍스트 적응(L) 4.43/8.14.다단계 USTR-CT 3.76/8.22.model SPGISpeech val + 텍스트 적응(S) 3.66/8.14.+ 텍스트 적응(L) 3.64/7.14.CT 23.+ ILME 13.CTTTS 적응 14.4.3. 표현 단위 표 2: 텍스트 특징에 대해 다양한 표현 단위를 사용하는 다단계 USTR-CT의 WER(%). LibriSpeech 테스트 SPGISpeech + ILME 11.다단계 USTR-CT 13.+ ILME 10.단일 단계 USTR-CT 13.+ ILME 10.model clean/other val grapheme repeat+text adaptation(L) phoneme repeat+text adaptation(L) 3.76/8.3.64/7.22.14.4.10/8.23.3.80/7.13.phoneme repeat3.82/8.22.+ text adaptation(L) 3.64/7.13.phoneme repeat3.82/8.22.+ text adaptation(L) 3.71/7.13.subword repeat4.20/8.3.80/8.23.15.+ text adaptation(L) 다단계 USTR-CT의 경우 다양한 표현 단위가 탐색되며, 여기서 음소 표현의 반복 횟수도 조사됩니다. 표 2에 나와 있듯이 반복되는 숫자가 4일 때 음소 표현은 타겟 도메인에서 가장 좋은 성능을 보인다(WER 13.38% 대 14.61%/15.00%). 이는 음소 표현이 Fbank 특징과 더 관련이 있기 때문일 수 있으며, 따라서 통합 음성-텍스트 표현의 학습이 더 쉬울 수 있다. 게다가 반복되는 숫자를 4에서 3/5로 변경했지만 더 이상의 이득은 관찰되지 않았다. 4.4. 다단계 대 단일 단계 단일 단계 USTR-CT를 다단계 USTR-CT 및 TTS 기반 적응과 비교했으며 그 결과는 표 3에 나와 있다. 단일 단계 USTR-CT가 소스 도메인과 타겟 도메인에서 모두 가장 좋은 성능을 보이는 것으로 나타났다. 공유 인코더도 타겟 도메인에 적응되므로 단일 단계 USTR-CT가 다단계 USTR-CT보다 성능이 더 좋다. 게다가 SPGISpeech의 추가 텍스트도 소스 도메인에 도움이 된다. 4.5. ILME과의 조합 대상 도메인의 텍스트가 이미 CT 모델의 학습에 포함되어 있으므로 외부 LM의 이점이 감소할 수 있습니다.우리는 LSTM LM을 학습하고 다양한 CT 모델을 사용하여 ILME의 성능을 평가했습니다.표 4에서 볼 수 있듯이 ILME은 기준 CT 및 TTS 기반 모델에서 WER을 41.27%/24.35% 감소시킵니다.다중/단일 단계 USTR-CT의 경우 ILME도 WER을 각각 24.89%/18.49% 감소시킵니다.이는 제안된 USTR이 ILME과 결합하여 대상 도메인의 성능을 더욱 개선할 수 있음을 나타냅니다.다중 단계 USTR-CT는 ILME과 결합할 때 단일 단계 USTR-CT보다 성능이 더 뛰어나며, 이는 섹션 4.4에서 ILME 없이 수행한 결과와 다릅니다.인코더가 학습 중에 대상 도메인의 언어 정보도 캡처하기 때문에 단일 단계 USTR-CT의 ILM 점수는 정확하지 않다고 가정합니다. 게다가, 인코더는 2단계에서 동결되므로, 다단계 USTR-CT는 대규모 텍스트 코퍼스를 사용하여 학습하는 데 더 적합합니다.
--- CONCLUSION ---
s 이 작업에서는 텍스트 전용 도메인 적응을 위해 추가 텍스트 인코더를 도입했으며, 이는 음소 표현을 사용할 때 TTS 적응보다 상대적으로 11.61% 더 우수한 성능을 보입니다. TTS 적응과 비교할 때, 제안된 USTR-CT는 빠른 도메인 적응을 위해 효율적이고 리소스를 절약합니다. 게다가, USTR-CT는 단일 단계 학습으로 대상 도메인에 적응하고 ILME과 결합하여 추가 이득을 얻을 수 있습니다. 이 작업에서는 스트리밍이 아닌 모델에서 실험을 수행했지만, 이 방법은 스트리밍 ASR에도 여전히 적용할 수 있습니다. 분리된 음성 및 텍스트 인코더를 사용하면 음성 및 텍스트 모달리티 간의 유사성을 고려하여 성능을 더욱 향상시킬 수 있으며, 이는 향후 작업에서 논의하기로 합니다. 6. 참고문헌 [1] C.-C. Chiu, TN Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, RJ Weiss, K. Rao, E. Gonina 외, &quot;시퀀스 간 모델을 사용한 최첨단 음성 인식&quot;, ICASSP, 2018, pp. 4774-4778. [2] TN Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier, S.-Y. Chang, W. Li, R. Alvarez, Z. Chen 외, &quot;서버측 기존 모델 품질 및 대기 시간을 능가하는 스트리밍 온디바이스 종단 간 모델&quot;, ICASSP, 2020, pp. 6059-6063. [3] J. Li et al., &quot;종단간 자동 음성 인식의 최근 발전&quot;, APSIPA 신호 및 정보 처리 저널, 제11권, 제1호, 2022년. [4] A. Graves, S. Fernández, F. Gomez, J. Schmidhuber, &quot;연결주의 시간 분류: 순환 신경망을 사용한 분할되지 않은 시퀀스 데이터 레이블링&quot;, ICML, 2006년, 369-376쪽. [5] J. Li, G. Ye, A. Das, R. Zhao, Y. Gong, &quot;음향-단어 ctc 모델 발전&quot;, ICASSP, 2018년, 5794-5798쪽. [6] J. Li, R. Zhao, Z. Meng, Y. Liu, W. Wei, S. Parthasarathy, V. Mazalov, Z. Wang, L. He, S. Zhao 등, &quot;맞춤형 기능을 갖춘 고성능 하이브리드 모델을 능가하는 rnnt 모델 개발&quot;, Interspeech, 2020, pp. 3590–3594. [7] A. Graves, &quot;순환 신경망을 이용한 시퀀스 변환&quot;, ICML Representation Learning Workshop, 2012. [8] JK Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, &quot;음성 인식을 위한 주의 기반 모델&quot;, NeurIPS, vol. 28, 2015. [9] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, NEY Soplin, R. Yamamoto, X. Wang et al., &quot;음성 애플리케이션에서 transformer와 rnn의 비교 연구&quot;, ASRU 2019, 2019, pp. 449–456. [10] J. Li, Y. Wu, Y. Gaur, C. Wang, R. Zhao, and S. Liu, &quot;대규모 음성 인식을 위한 인기 있는 엔드투엔드 모델 비교에 관하여 &quot;규모 음성 인식&quot;, Interspeech, 2020, 1-5쪽. [11] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates 외, &quot;심층 음성: 종단 간 음성 인식 확장&quot;, arXiv 사전 인쇄 arXiv:1412.5567, 2014. [12] E. McDermott, H. Sak 및 E. Variani, &quot;종단 간 자동 음성 인식의 언어 모델 융합에 대한 밀도 비율 접근 방식&quot;, ASRU 2019, 2019, pp. 434–441. [13] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin, F. Bougares, H. Schwenk 및 Y. 벤지오, “단일언어 사용에 대하여” 영어: 신경망 기계 번역의 코퍼스,&quot; arXiv 사전 인쇄본 arXiv:1503.03535, 2015. [14] A. Sriram, H. Jun, S. Satheesh, and A. Coates, &quot;Cold fusion: Training seq2seq models together with language models,&quot; Interspeech, 2018, pp. 387–391. [15] E. Variani, D. Rybach, C. Allauzen, and M. Riley, &quot;Hybrid autoregressive transducer (hat),&quot; ICASSP, 2020, pp. 6139-6143. [16] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu, X. Chen, R. Zhao, J. Li, and Y. Gong, &quot;도메인 적응형 내부 언어 모델 추정&quot; 2021년 SLT에서 &quot;종단간 음성 인식&quot;, 243-250쪽. [17] Y. Huang, J. Li, L. He, W. Wei, W. Gale, Y. Gong, &quot;개인화된 음성 합성 및 신경 언어 생성기를 사용한 신속한 rnn-t 적응.&quot; Interspeech에서, 2020년, 1256-1260쪽. [18] C. Peyser, S. Mavandadi, TN Sainath, J. Apfel, R. Pang, S. Kumar, &quot;대용량 텍스트 코퍼스를 사용하여 심의 e2e asr 모델의 테일 성능 개선&quot;, Interspeech, 2020, 4921-4925쪽. [19] V. Bataev, R. Korostik, E. Shabalin, V. Lavrukhin, B. Ginsburg, &quot;통합 텍스트-멜 스펙트로그램 생성기를 사용한 엔드투엔드 asr에 대한 텍스트 전용 도메인 적응&quot;, arXiv 사전 인쇄본 arXiv:2302.14036, 2023. [20] J. Pylkkönen, A. Ukkonen, J. Kilpikoski, S. Tamminen, H. Heikinheimo, &quot;RNNTransducer 예측의 빠른 텍스트 전용 도메인 적응 영어: Network,&quot; in Interspeech, 2021, pp. 18821886. [21] Z. Meng, Y. Gaur, N. Kanda, J. Li, X. Chen, Y. Wu, 및 Y. Gong, &quot;End-to-End 음성 인식을 위한 텍스트 전용 데이터를 사용한 내부 언어 모델 적응,&quot; Interspeech, 2022, pp. 26082612. [22] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, PJ Moreno, A. Bapna, 및 H. Zen, &quot;MAESTRO: 모달리티 매칭을 통한 매칭된 음성 텍스트 표현,&quot; Interspeech, 2022, pp. 4093-4097. [23] TN Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen, B. Li, W. Wang 및 T. Strohman, “Joist: ASR을 위한 공동 음성 및 텍스트 스트리밍 모델”, SLT 2022, 2023, 52-59쪽. [24] S. Thomas, B. Kingsbury, G. Saon, 및 H.-KJ Kuo, &quot;rnn 트랜스듀서 asr 모델의 훈련 및 적응을 위한 텍스트 입력 통합&quot;, ICASSP, 2022, 8127-8131쪽. [25] H. Sato, T. Komori, T. Mishima, Y. Kawai, T. Mochizuki, S. Sato, 및 T. Ogawa, &quot;중간 ctc 기반 텍스트 전용 도메인 적응&quot;, Interspeech, 2022, 2208-2212쪽. [26] A. Bapna, Y.-a. Chung, N. Wu, A. Gulati, Y. Jia, JH Clark, M. Johnson, J. Riesa, A. Conneau, 및 Y. Zhang, &quot;Slam: 음성-텍스트 공동 사전 훈련을 통한 음성 및 언어 모델링을 위한 통합 인코더&quot;, arXiv 사전 인쇄본 arXiv:2110.10329, 2021. [27] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, A. Conneau, &quot;mslam: 음성 및 텍스트를 위한 대규모 다국어 공동 사전 훈련,&quot; arXiv 사전 인쇄본 arXiv:2202.01374, 2022. [28] Y. Tang, H. Gong, N. Dong, C. Wang, W.-N. Hsu, J. Gu, A. Baevski, X. Li, A. Mohamed, M. Auli 등, &quot;음성 번역 및 인식을 위한 통합 음성 텍스트 사전 훈련,&quot; ACL, 2022, 1488-1499쪽. [29] S. Thomas, H.-KJ Kuo, B. Kingsbury 및 G. Saon, &quot;말하기 언어 이해 시스템을 구축하기 위한 음성 훈련 데이터의 필요성 감소를 향해&quot;, ICASSP, 2022, 7932-7936쪽. [30] Y.-A. Chung, C. Zhu 및 M. Zeng, &quot;Splat: 구어 이해를 위한 음성-언어 공동 사전 훈련,&quot; 2021년 북미 컴퓨터 언어학 협회: 인간 언어 기술 회의록, 2021, pp. 1897-1907. [31] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko, Q. Li, Y. Zhang 등, &quot;Speecht5: 구어 처리를 위한 통합 모달 인코더-디코더 사전 훈련,&quot; ACL, 2022, pp. 5723-5738. [32] A. Narayanan, TN Sainath, R. Pang, J. Yu, C.-C. 영어: Chiu, R. Prabhavalkar, E. Variani 및 T. Strohman, &quot;스트리밍 및 비스트리밍 ASR을 통합하기 위한 계단식 인코더&quot;, ICASSP, 2021, 5629-5633쪽. [33] Z. Meng, N. Kanda, Y. Gaur, S. Parthasarathy, E. Sun, L. Lu, X. Chen, J. Li 및 Y. Gong, &quot;도메인 적응형 엔드투엔드 음성 인식을 위한 내부 언어 모델 학습&quot;, ICASSP, 2021, 7338-7342쪽. [34] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu 등, &quot;Conformer: 음성 인식을 위한 합성 증강 변환기,&quot; Interspeech, 2020. [35] R. Sennrich, B. Haddow, A. Birch, &quot;서브워드 단위가 있는 희귀 단어의 신경망 기계 번역,&quot; ACL, 2016, pp. 1715– 1725. [36] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, &quot;Librispeech: 퍼블릭 도메인 오디오 북을 기반으로 한 ASR 코퍼스,&quot; ICASSP, 2015, pp. 5206-5210. [37] PK O&#39;Neill, V. Lavrukhin, S. Majumdar, V. Noroozi, Y. Zhang, O. Kuchaiev, J. Balam, Y. Dovzhenko, K. Freyberg, MD Shulman, B. Ginsburg, S. Watanabe, G. Kucsko, &quot;Spgispeech: 완전히 포맷된 종단간 음성 인식을 위한 5,000시간 분량의 재무 오디오 필사본&quot;, Interspeech, 2021, 1434-1438쪽. [38] DS Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, ED Cubuk, 및 QV Le, &quot;Specaugment: 자동 음성 인식을 위한 간단한 데이터 증강 방법&quot;, Interspeech, 2019, 2613-2617쪽. [39] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., &quot;Pytorch: 명령형 스타일의 고성능 딥 러닝 라이브러리&quot;, NeurIPS, vol. 32, 2019.
