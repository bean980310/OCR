--- ABSTRACT ---
생성적 AI 혁명은 최근 비디오로 확대되었습니다. 그럼에도 불구하고, 현재의 최첨단 비디오 모델은 시각적 품질과 생성된 콘텐츠에 대한 사용자 제어 측면에서 여전히 이미지 모델에 뒤처져 있습니다. 이 연구에서는 텍스트 기반 비디오 편집 작업을 위해 텍스트-이미지 확산 모델의 힘을 활용하는 프레임워크를 제시합니다. 구체적으로, 소스 비디오와 대상 텍스트 프롬프트가 주어지면, 우리의 방법은 입력 비디오의 공간적 레이아웃과 동작을 보존하면서 대상 텍스트를 준수하는 고품질 비디오를 생성합니다. 우리의 방법은 편집된 비디오의 일관성은 확산 특징 공간에서 일관성을 적용함으로써 얻을 수 있다는 주요 관찰에 기초합니다. 우리는 모델에서 쉽게 사용할 수 있는 프레임 간 대응 관계를 기반으로 확산 특징을 명시적으로 전파하여 이를 달성합니다. 따라서 우리의 프레임워크는 어떠한 훈련이나 미세 조정도 필요하지 않으며, 모든 기성형 텍스트-이미지 편집 방법과 함께 작동할 수 있습니다. 우리는 다양한 실제 비디오에서 최첨단 편집 결과를 보여줍니다.
--- INTRODUCTION ---
텍스트-이미지 모델의 진화는 최근 이미지 편집 및 콘텐츠 생성의 발전을 촉진하여 사용자가 생성된 이미지와 실제 이미지의 다양한 속성을 제어할 수 있게 했습니다. 그럼에도 불구하고 이 흥미로운 진전을 비디오로 확장하는 것은 여전히 뒤처져 있습니다. 대규모 텍스트-비디오 생성 모델의 급증은 텍스트 설명만으로 클립을 생성하는 데 인상적인 결과를 보여주었습니다. 그러나 이 분야에서 이루어진 진전에도 불구하고 기존 비디오 모델은 해상도, 비디오 길이 또는 표현할 수 있는 비디오 역학의 복잡성에 제한이 있어 아직 초기 단계에 있습니다. 이 논문에서는 자연스러운 비디오의 텍스트 기반 편집 작업을 위해 최첨단 사전 학습된 텍스트-이미지 모델의 힘을 활용합니다. 구체적으로, 우리의 목표는 원본 비디오의 공간적 레이아웃과 동작을 보존하면서 입력 텍스트 프롬프트에서 표현된 대상 편집을 준수하는 고품질 비디오를 생성하는 것입니다. 비디오 편집을 위해 이미지 확산 모델을 활용하는 데 있어 가장 큰 과제는 편집된 콘텐츠가 모든 비디오 프레임에서 일관성을 유지하도록 하는 것입니다. 이상적으로는 3D 세계의 각 물리적 지점이 시간에 따라 일관된 수정을 거칩니다. 이미지 확산 모델을 기반으로 하는 기존 및 동시 비디오 편집 방법은 셀프 어텐션 모듈을 여러 프레임을 포함하도록 확장하여 편집된 프레임에서 전역적 모양 일관성을 달성할 수 있음을 보여주었습니다(Wu et al., 2022; Khachatryan et al., 2023b; Ceylan et al., 2023; Qi et al., 2023). 그럼에도 불구하고 이 접근 방식은 비디오의 동작이 어텐션 모듈을 통해서만 암묵적으로 보존되기 때문에 원하는 수준의 시간적 일관성을 달성하기에 충분하지 않습니다. 결과적으로 전문가 또는 준전문가 사용자는 종종 추가적인 수동 작업이 필요한 정교한 비디오 편집 파이프라인을 사용합니다. 이 작업에서 우리는 편집에서 원래 프레임 간 대응 관계를 명시적으로 적용하여 이 과제를 해결하는 프레임워크를 제안합니다. 직관적으로 자연스러운 비디오는 프레임 간에 중복된 정보를 포함합니다. 예를 들어, 유사한 모양과 공유되는 시각적 요소를 묘사합니다. 우리의 주요 관찰은 확산 모델에서 비디오의 내부 표현이 유사한 속성을 보인다는 것입니다. 즉, RGB 공간과 확산 특징 공간에서 프레임의 중복성 수준과 시간적 일관성은 긴밀하게 상관 관계가 있습니다. 이 관찰을 바탕으로, 우리 접근 방식의 핵심은 편집된 비디오의 특징이 프레임 간에 일관되도록 보장하여 일관된 편집을 달성하는 것입니다. 구체적으로, 편집된 특징이 원래 비디오 특징과 동일한 프레임 간 대응 관계와 중복성을 전달하도록 합니다. 이를 위해 모델에서 쉽게 사용할 수 있는 원래 프레임 간 특징 대응 관계를 활용합니다. 이를 통해 원래 비디오 역학을 기반으로 편집된 확산 특징을 직접 전파하는 효과적인 방법이 도출됩니다. 이 접근 방식을 사용하면 추가 학습이나 미세 조정 없이 최첨단 이미지 확산 모델의 생성 사전을 활용할 수 있으며 기성형 확산 기반 이미지 편집 방법(예: Meng et al. (2022); Hertz et al. (2022); Zhang &amp; Agrawala (2023); Tumanyan et al. (2023))과 함께 작동할 수 있습니다. 요약하자면, 다음과 같은 주요 기여를 합니다.• 프레임 전체에 걸쳐 확산 특징의 의미적 대응을 적용하는 TokenFlow라는 기술로, 텍스트-이미지 확산 모델에서 생성된 비디오에서 시간적 일관성을 크게 높일 수 있습니다.• 비디오 전체에서 확산 특징의 특성을 연구하는 새로운 경험적 분석.• 복잡한 동작을 묘사하는 다양한 비디오에 대한 최첨단 편집 결과.
--- RELATED WORK ---
텍스트 기반 이미지 및 비디오 합성 Seminal work는 텍스트 임베딩을 조건으로 이미지를 합성하기 위해 GAN 아키텍처를 설계했습니다(Reed et al., 2016; Zhang et al., 2016). 시각 언어 데이터 세트와 사전 학습 전략의 규모가 계속 커짐에 따라(Radford et al., 2021; Schuhmann et al., 2022), 텍스트 기반 이미지 생성 기능에서 놀라운 진전이 있었습니다. 사용자는 간단한 텍스트 프롬프트를 사용하여 고품질 시각적 콘텐츠를 합성할 수 있습니다. 이러한 진전의 대부분은 확산 모델(Sohl-Dickstein 등, 2015; Croitoru 등, 2022; Dhariwal &amp; Nichol, 2021; Ho 등, 2020; Nichol &amp; Dhariwal, 2021) 덕분이기도 하며, 이는 최첨단 텍스트-이미지 생성기(Nichol 등, 2021; Saharia 등, 2022; Ramesh 등, 2022; Rombach 등, 2022; Sheynin 등, 2022; Bar-Tal 등, 2023)로 자리 잡았습니다. 이러한 모델은 2D 아키텍처를 시간 차원으로 확장(예: 시간적 주의 Ho et al. (2022b) 사용)하고 비디오 데이터 세트에 대한 대규모 학습을 수행(Ho et al., 2022a; Blattmann et al., 2023; Singer et al., 2022)하여 텍스트-비디오 생성을 위해 확장되었습니다. 최근 Gen-1(Esser et al., 2023)은 네트워크를 구조/모양 표현에 따라 조절하여 비디오 편집 작업에 대한 확산 모델 아키텍처를 맞춤화했습니다. 그럼에도 불구하고 광범위한 계산 및 메모리 요구 사항으로 인해 기존 비디오 확산 모델은 아직 초기 단계에 있으며 짧은 클립으로 크게 제한되거나 이미지 모델에 비해 시각적 품질이 낮습니다. 스펙트럼의 반대편에서 최근 유망한 연구 동향은 추가 학습 없이 비디오 합성 작업을 위해 사전 학습된 이미지 확산 모델을 활용하는 것입니다(Fridman et al., 2023; Wu et al., 2022; Lee et al., 2023a; Qi et al., 2023). 저희의 연구는 이 범주에 속하며, 어떠한 학습이나 미세 조정 없이 비디오 편집 작업을 위해 사전 학습된 텍스트-이미지 확산 모델을 사용합니다. 일관된 비디오 스타일 지정 비디오 스타일 지정을 위한 일반적인 접근 방식은 프레임별로 이미지 편집 기술(예: 스타일 전송)을 적용한 다음, 편집된 비디오의 시간적 불일치를 해결하기 위한 후처리 단계를 거치는 것입니다(Lai et al. (2018b); Lei et al. (2020; 2023)). 이러한
--- METHOD ---
영어: 원본 장면의 의미적 레이아웃과 동작을 보존하면서 대상 텍스트 프롬프트(가운데 및 아래쪽 행)에 따라 편집합니다.초록 생성 AI 혁명은 최근 비디오로 확장되었습니다.그럼에도 불구하고 최신 비디오 모델은 시각적 품질과 생성된 콘텐츠에 대한 사용자 제어 측면에서 여전히 이미지 모델에 뒤처져 있습니다.이 연구에서는 텍스트 기반 비디오 편집 작업을 위해 텍스트-이미지 확산 모델의 힘을 활용하는 프레임워크를 제시합니다.특히 소스 비디오와 대상 텍스트 프롬프트가 주어지면 우리 방법은 입력 비디오의 공간적 레이아웃과 동작을 보존하면서 대상 텍스트를 준수하는 고품질 비디오를 생성합니다.우리 방법은 편집된 비디오의 일관성은 확산 피처 공간에서 일관성을 적용하여 얻을 수 있다는 주요 관찰에 기반합니다.우리는 모델에서 쉽게 사용할 수 있는 프레임 간 대응 관계를 기반으로 확산 피처를 명시적으로 전파하여 이를 달성합니다. 따라서 저희 프레임워크는 어떠한 훈련이나 미세 조정도 필요하지 않으며, 모든 기성형 텍스트-이미지 편집 방법과 함께 작동할 수 있습니다. 저희는 다양한 실제 비디오에서 최첨단 편집 결과를 보여줍니다. 서론 텍스트-이미지 모델의 진화는 최근 이미지 편집 및 콘텐츠 생성의 발전을 촉진하여 사용자가 생성된 이미지와 실제 이미지 모두의 다양한 속성을 제어할 수 있게 했습니다. 그럼에도 불구하고 이 흥미로운 진전을 비디오로 확장하는 것은 여전히 뒤처져 있습니다. 대규모 텍스트-비디오 생성 모델의 급증은 텍스트 설명만으로 클립을 생성하는 데 인상적인 결과를 보여주었습니다. 그러나 이 분야에서 이루어진 진전에도 불구하고 기존 비디오 모델은 해상도, 비디오 길이 또는 표현할 수 있는 비디오 역학의 복잡성이 제한되어 아직 초기 단계에 있습니다. 이 논문에서는 자연스러운 비디오의 텍스트 기반 편집 작업을 위해 최첨단 사전 훈련된 텍스트-이미지 모델의 힘을 활용합니다. 구체적으로, 우리의 목표는 입력 텍스트 프롬프트에 의해 표현된 타겟 편집을 준수하는 동시에 원본 비디오의 공간적 레이아웃과 동작을 보존하는 고품질 비디오를 생성하는 것입니다.비디오 편집을 위해 이미지 확산 모델을 활용하는 데 있어서 가장 큰 과제는 편집된 콘텐츠가 모든 비디오 프레임에서 일관성을 유지하도록 하는 것입니다.이상적으로는 3D 세계의 각 물리적 지점이 시간에 따라 일관된 수정을 거칩니다.이미지 확산 모델을 기반으로 하는 기존 및 동시 비디오 편집 방법은 셀프 어텐션 모듈을 여러 프레임을 포함하도록 확장하여 편집된 프레임 전체에서 글로벌 모양 일관성을 달성할 수 있음을 보여주었습니다(Wu et al., 2022; Khachatryan et al., 2023b; Ceylan et al., 2023; Qi et al., 2023).그럼에도 불구하고, 이 접근 방식은 비디오의 동작이 어텐션 모듈을 통해서만 암묵적으로 보존되기 때문에 원하는 수준의 시간적 일관성을 달성하기에 충분하지 않습니다. 따라서 전문가 또는 준전문가 사용자는 종종 추가적인 수동 작업이 필요한 정교한 비디오 편집 파이프라인을 사용합니다. 이 작업에서 우리는 편집에서 원래의 프레임 간 대응 관계를 명시적으로 적용하여 이러한 과제를 해결하는 프레임워크를 제안합니다. 직관적으로 자연스러운 비디오는 프레임 간에 중복된 정보를 포함합니다. 예를 들어, 유사한 모양과 공유되는 시각적 요소를 묘사합니다. 우리의 주요 관찰 결과는 확산 모델에서 비디오의 내부 표현이 유사한 속성을 보인다는 것입니다. 즉, RGB 공간과 확산 특징 공간에서 프레임의 중복성 수준과 시간적 일관성은 긴밀하게 상관 관계가 있습니다. 이 관찰 결과에 따라 우리 접근 방식의 핵심은 편집된 비디오의 특징이 프레임 간에 일관되도록 보장하여 일관된 편집을 달성하는 것입니다. 구체적으로, 우리는 편집된 특징이 원래 비디오 특징과 동일한 프레임 간 대응 관계와 중복성을 전달하도록 적용합니다. 이를 위해 우리는 모델에서 쉽게 사용할 수 있는 원래의 프레임 간 특징 대응 관계를 활용합니다. 이는 원본 비디오 역학을 기반으로 편집된 확산 특징을 직접 전파하는 효과적인 방법으로 이어집니다. 이 접근 방식을 사용하면 추가 훈련이나 미세 조정 없이 최첨단 이미지 확산 모델의 생성 사전을 활용할 수 있으며 기성형 확산 기반 이미지 편집 방법(예: Meng et al. (2022); Hertz et al. (2022); Zhang &amp; Agrawala (2023); Tumanyan et al. (2023))과 함께 작동할 수 있습니다. 요약하자면, 다음과 같은 주요 기여를 합니다. • TokenFlow라는 기술은 프레임 전체에서 확산 특징의 의미적 대응 관계를 적용하여 텍스트-이미지 확산 모델에서 생성된 비디오에서 시간적 일관성을 크게 높일 수 있습니다. • 비디오 전체에서 확산 특징의 특성을 연구하는 새로운 경험적 분석. • 복잡한 동작을 묘사하는 다양한 비디오에서 최첨단 편집 결과. 관련 작업 텍스트 기반 이미지 및 비디오 합성 획기적인 연구는 텍스트 임베딩을 조건으로 이미지를 합성하기 위해 GAN 아키텍처를 설계했습니다(Reed et al., 2016; Zhang et al., 2016). 시각 언어 데이터 세트와 사전 학습 전략의 규모가 계속 커짐에 따라(Radford et al., 2021; Schuhmann et al., 2022), 텍스트 기반 이미지 생성 기능에서 놀라운 진전이 있었습니다. 사용자는 간단한 텍스트 프롬프트를 사용하여 고품질 시각적 콘텐츠를 합성할 수 있습니다. 이러한 진전의 대부분은 확산 모델(Sohl-Dickstein 등, 2015; Croitoru 등, 2022; Dhariwal &amp; Nichol, 2021; Ho 등, 2020; Nichol &amp; Dhariwal, 2021) 덕분이기도 하며, 이는 최첨단 텍스트-이미지 생성기(Nichol 등, 2021; Saharia 등, 2022; Ramesh 등, 2022; Rombach 등, 2022; Sheynin 등, 2022; Bar-Tal 등, 2023)로 자리 잡았습니다. 이러한 모델은 2D 아키텍처를 시간 차원으로 확장(예: 시간적 주의 Ho et al. (2022b) 사용)하고 비디오 데이터 세트에 대한 대규모 학습을 수행(Ho et al., 2022a; Blattmann et al., 2023; Singer et al., 2022)하여 텍스트-비디오 생성을 위해 확장되었습니다. 최근 Gen-1(Esser et al., 2023)은 네트워크를 구조/모양 표현에 따라 조절하여 비디오 편집 작업에 대한 확산 모델 아키텍처를 맞춤화했습니다. 그럼에도 불구하고 광범위한 계산 및 메모리 요구 사항으로 인해 기존 비디오 확산 모델은 아직 초기 단계에 있으며 짧은 클립으로 크게 제한되거나 이미지 모델에 비해 시각적 품질이 낮습니다. 스펙트럼의 반대편에서 최근의 유망한 연구 동향은 추가 학습 없이 비디오 합성 작업에 사전 학습된 이미지 확산 모델을 활용하는 것입니다(Fridman et al., 2023; Wu et al., 2022; Lee et al., 2023a; Qi et al., 2023). 저희의 연구는 이 범주에 속하며, 어떠한 학습이나 미세 조정 없이 비디오 편집 작업에 사전 학습된 텍스트-이미지 확산 모델을 사용합니다. 일관된 비디오 스타일 지정 비디오 스타일 지정을 위한 일반적인 접근 방식은 프레임별로 이미지 편집 기술(예: 스타일 전송)을 적용한 다음, 편집된 비디오의 시간적 불일치를 해결하기 위한 후처리 단계를 거치는 것입니다(Lai et al. (2018b); Lei et al. (2020; 2023)). 이러한 방법은 고주파 시간적 깜빡임을 효과적으로 줄이지만, 텍스트 기반 이미지 편집 기술을 적용할 때 종종 발생하는 내용에 상당한 변화가 나타나는 프레임을 처리하도록 설계되지 않았습니다(Qi et al., 2023). Kasten et al.(2021)은 비디오를 2D 아틀라스 세트로 분해하는 것을 제안합니다. 각각은 비디오 전체의 배경 또는 전경 객체에 대한 통합된 표현을 제공합니다. 2D 아틀라스에 적용된 편집 내용은 자동으로 비디오에 다시 매핑되므로 최소한의 노력으로 시간적 일관성을 얻을 수 있습니다. Bar-Tal et al.(2022); Lee et al.(2023b)은 이 표현을 활용하여 텍스트 기반 편집을 수행합니다. 그러나 아틀라스 표현은 간단한 동작이 있는 비디오로 제한되고 긴 훈련이 필요하므로 이 기술과 이를 기반으로 하는 방법의 적용성이 제한됩니다. 영어: 저희의 연구는 또한 자연스러운 비디오의 작은 패치가 여러 프레임에 걸쳐 광범위하게 반복된다는 것을 보여준 고전적 연구와 관련이 있으며(Shahar et al., 2011; Cheung et al., 2005), 따라서 일관된 편집은 다음을 편집하여 간소화될 수 있습니다.x-t 슬라이스 특징(PCA) 샘플 프레임 X 원본 프레임당 편집 저희 XX 그림 3: 시간에 따른 확산 특징.왼쪽: 입력 비디오(위쪽 행)가 주어지면 각 프레임에 DDIM 역전을 적용하고 €0에서 가장 높은 해상도 디코더 계층에서 특징을 추출합니다.모든 프레임에서 추출한 특징(즉, 셀프 어텐션 모듈의 출력 토큰)에 PCA를 적용하고 처음 세 구성 요소를 시각화합니다(두 번째 행).RGB와 특징(아래쪽 행) 모두에 대해 xt 슬라이스(원래 프레임에서 빨간색으로 표시)를 추가로 시각화합니다.특징 표현은 시간에 따라 일관됩니다.해당 영역은 비디오 전체에서 유사한 특징으로 인코딩됩니다.가운데: 각 프레임에 이미지 편집 방법(Tumanyan et al. (2023))을 적용하여 얻은 편집된 비디오의 프레임과 특징 시각화. RGB의 일관되지 않은 패턴은 피처 공간에서도 분명하게 드러납니다(예: 개의 몸). 오른쪽: 저희의 방법은 편집된 비디오가 원본 비디오와 동일한 수준의 피처 일관성을 전달하도록 강제하여 RGB 공간에서 일관되고 고품질의 편집으로 변환됩니다. 키 프레임의 하위 집합을 사용하고 수작업 피처와 광학 흐름을 사용하여 패치 대응을 설정하거나 패치 기반 GAN을 학습하여(Ruder et al., 2016; Jamriška et al., 2019) 비디오 전체에 편집을 전파합니다(Texler et al., 2020). 그럼에도 불구하고 이러한 전파 방법은 조명이 바뀌거나 복잡한 역학이 있는 비디오를 처리하는 데 어려움을 겪습니다. 중요한 것은 사용자가 제공한 키 프레임의 일관된 편집에 의존한다는 점인데, 이는 아직 자동화되지 않은 노동 집약적인 작업으로 남아 있습니다. Yang et al.(2023)은 키 프레임 편집을 Jamriška et al.(2019)의 전파 방법과 결합합니다. 그들은 텍스트-이미지 확산 모델을 사용하여 키프레임을 편집하는 동시에 편집된 키프레임에 광학 흐름 제약을 적용합니다. 그러나 먼 프레임 간의 광학 흐름 추정은 신뢰할 수 없기 때문에 그들의 방법은 멀리 떨어진 키프레임을 일관되게 편집하지 못하고(보충 자료 - SM에서 볼 수 있듯이) 결과적으로 대부분의 비디오를 일관되게 편집하지 못합니다. 저희의 작업은 자연스러운 비디오의 시간적 중복성으로부터 이점을 얻는 이 접근 방식과 유사한 동기를 공유합니다. 저희는 이러한 중복성이 텍스트-이미지 확산 모델의 특징 공간에도 존재한다는 것을 보여주고 이 속성을 활용하여 일관성을 달성합니다. 확산 특징 조작을 통한 제어된 생성 최근, 많은 연구에서 텍스트-이미지 확산 모델이 확산 네트워크의 중간 특징 표현에 대한 간단한 연산을 수행함으로써 다양한 편집 및 생성 작업에 쉽게 적용될 수 있음을 보여주었습니다(Chefer et al., 2023; Hong et al., 2022; Ma et al., 2023; Tumanyan et al., 2023; Hertz et al., 2022; Patashnik et al., 2023; Cao et al., 2023). Luo et al.(2023); Zhang et al.(2023)은 확산 특징 대응 관계를 사용하여 의미적 모양 스와핑을 시연했습니다. Hertz et al.(2022)은 교차 주의 계층을 조작함으로써 이미지의 공간적 레이아웃과 텍스트의 각 단어 간의 관계를 제어할 수 있음을 관찰했습니다. 플러그 앤 플레이 확산(PnP, Tumanyan et al. (2023))은 공간적 특징과 자기 주의 소스 대상 II 대상을 분석했습니다.(a) 재구성된 대상 (b) 워핑된 소스 대상 (c) 최근접 이웃 필드 그림 2: 세분화된 특징 대응 관계. 소스 프레임에서 추출된 특징(즉, 자기 주의 모듈의 출력 토큰)은 인근 프레임을 재구성하는 데 사용됩니다.이는 다음을 통해 수행됩니다.(a) 모든 계층 및 모든 생성 시간 단계에서 소스의 가장 가까운 특징으로 대상의 각 특징을 교환하고,(b) 가장 높은 해상도 디코더 계층에서 추출된 소스 및 대상 특징 사이에서 계산된 최근접 이웃 필드(c)를 사용하여 RGB 공간에서 단순 워핑합니다. 대상은 충실하게 재구성되어, 피처 간에 높은 수준의 공간적 세분성과 공유된 콘텐츠를 보여줍니다.입력 비디오 I DDIM 역전 €토큰 추출 NN 필드 계산 (II) TokenFlow 전파 잡음 제거 비디오 J-잡음 비디오 J, (I) 공동 편집 샘플링된 키 프레임 €Tbase Fr €확장된 주의 &quot;다채로운 그림&quot; &#39;다채로운 그림&quot; 그림 4: TokenFlow 파이프라인.위: 입력 비디오 Z가 주어지면, 각 프레임을 DDIM 역전하고, 토큰, 즉 셀프 어텐션 모듈의 출력 피처를 각 시간 단계와 계층에서 추출하고, 최근접 이웃(NN) 검색을 사용하여 프레임 간 피처 대응 관계를 계산합니다.아래: 편집된 비디오는 다음과 같이 생성됩니다.각 잡음 제거 단계 t에서, (I) 잡음 비디오 Jt에서 키 프레임을 샘플링하고 확장된 어텐션 블록을 사용하여 공동 편집합니다.편집된 결과 토큰 세트는 Tbase입니다.(II) 원본 비디오 기능의 사전 계산된 대응 관계에 따라 비디오 전체에서 편집된 토큰.Jt의 노이즈를 제거하기 위해 각 프레임을 네트워크에 공급하고 생성된 토큰을 전파 단계(II)에서 얻은 토큰으로 대체합니다.맵을 만들고 높은 공간적 세분성으로 의미 정보를 캡처한다는 것을 발견했습니다.Tune-A-Video(Wu et al., 2022)는 셀프 어텐션 모듈을 여러 프레임에서 작동하도록 확장함으로써 공통된 글로벌 모양을 공유하는 프레임을 생성할 수 있음을 관찰했습니다.Qi et al.(2023);Ceylan et al.(2023);Khachatryan et al.(2023a);Shin et al.(2023);Liu et al.(2023)은 이 속성을 활용하여 글로벌하게 일관된 비디오 편집을 달성합니다.그럼에도 불구하고 Sec. 5에서 보여준 것처럼 셀프 어텐션 모듈을 팽창시키는 것만으로는 세밀한 시간적 일관성을 달성하기에 충분하지 않습니다. 이전 및 동시 작업은 시각적 품질을 손상시키거나 제한된 시간적 일관성을 보입니다.이 작업에서 우리는 또한 사전 훈련된 텍스트-이미지 모델의 피처 공간에서 간단한 작업을 통해 비디오 편집을 수행하고 TokenFlow를 통해 모델의 피처가 시간적으로 일관성을 갖도록 명시적으로 장려합니다.예비 과정 확산 모델 확산 확률적 모델(DPM)(Sohl-Dickstein 등, 2015; Croitoru 등, 2022; Dhariwal &amp; Nichol, 2021; Ho 등, 2020; Nichol &amp; Dhariwal, 2021)은 점진적인 노이즈 제거 프로세스를 통해 데이터 분포 q를 근사하는 것을 목표로 하는 생성 모델 클래스입니다.가우스 iid 노이즈 이미지 xT ~N(0, 1)에서 시작하여 확산 모델 eo는 대상 분포 q에서 추출한 깨끗한 이미지 xo에 도달할 때까지 점차적으로 노이즈를 제거합니다. DPM은 텍스트 컨디셔닝과 같은 추가 안내 신호를 통합하여 조건부 분포를 학습할 수 있습니다.Song et al. (2020)은 초기 노이즈 xT가 주어진 결정적 샘플링 알고리즘인 DDIM을 도출했습니다.이 알고리즘을 깨끗한 xo에서 시작하여 역순으로 적용하면(일명 DDIM 역전) 생성하는 데 사용된 중간 노이즈 이미지 {x;}\/\₁을 얻을 수 있습니다.T t=안정 확산 안정 확산(SD)(Rombach et al., 2022)은 잠재 이미지 공간에서 작동하는 뛰어난 텍스트-이미지 확산 모델입니다.사전 학습된 인코더는 RGB 이미지를 이 공간에 매핑하고 디코더는 잠재 이미지를 고해상도 이미지로 다시 디코딩합니다.더 자세히 말하면 SD는 잔여, 자기 주의 및 교차 주의 블록으로 구성된 U-Net 아키텍처(Ronneberger et al., 2015)를 기반으로 합니다. 잔여 블록은 이전 계층의 활성화를 합성하는 반면, 교차 주의는 텍스트 프롬프트에 따라 기능을 조작합니다. 셀프 어텐션 블록에서 피처는 쿼리 Q, 키 K 및 값 V로 투사됩니다.어텐션 연산(Vaswani et al., 2017)은 d차원 투영 Q, K 간의 친화도를 계산하여 다음 계층의 출력을 생성합니다.A.V 여기서 A = 어텐션(Q; K)이고 어텐션(Q; K) = 소프트맥스((QKT) (1)입력 비디오 R &quot;반 고흐 초상화&quot; 입력 비디오 &quot;자동차의 얼음 조각&quot; &quot;대리석 조각&quot; &quot;해변에 있는 자동차의 모래 조각&quot; 입력 비디오 입력 비디오 &quot;로봇 늑대&quot; &quot;모아나 무비의 마우이&quot; &quot;다채로운 다각형 일러스트&quot; &quot;픽사 애니메이션&quot; 그림 5: 결과. 방법의 샘플 결과. 더 많은 예와 전체 비디오 결과는 웹페이지와 SM에서 확인할 수 있습니다. 4 방법 입력 비디오 I = [I¹, ,In]과 대상 편집을 설명하는 텍스트 프롬프트 P가 주어지면 편집된 비디오 J = [J¹, ..., Jn]은 텍스트 P를 고수하면서 I의 원래 동작과 의미적 레이아웃을 보존합니다. 이를 달성하기 위해 프레임워크는 사전 학습되고 고정된 텍스트-이미지 확산 모델 €0을 활용합니다. 각 프레임에 독립적으로 이미지 편집 방법을 적용하여 비디오 편집에 ε를 순진하게 활용하면(예: Hertz et al. (2022); Tumanyan et al. (2023); Meng et al. (2022); Zhang &amp; Agrawala (2023)) 프레임 간에 콘텐츠 불일치가 발생합니다(예: 그림 3 가운데 열). 주요 발견 사항은 편집 프로세스 중에 프레임 간에 내부 확산 기능 간의 일관성을 적용하여 이러한 불일치를 완화할 수 있다는 것입니다. 자연스러운 비디오는 일반적으로 시간에 따라 일관되고 공유되는 콘텐츠를 묘사합니다. ε에서 자연스러운 비디오의 내부 표현이 유사한 속성을 가지고 있음을 관찰합니다. 이는 그림 3에서 설명되며, 주어진 비디오(첫 번째 열)에서 추출한 기능을 시각화합니다. 보시다시피, 특징은 프레임 간에 공유되고 일관된 표현을 나타냅니다.즉, 해당 영역은 유사한 표현을 보입니다.또한 원래 비디오 특징이 간단한 최근접 이웃 검색(그림 2)을 사용하여 프레임 간에 세밀한 대응 관계를 제공한다는 것을 관찰합니다.또한 이러한 대응 특징은 확산 모델에 대해 상호 교환 가능함을 보여줍니다.즉, 근처 프레임의 해당 특징으로 특징을 바꿔서 한 프레임을 충실하게 합성할 수 있습니다(그림 2(a)).그럼에도 불구하고 각 프레임에 개별적으로 편집을 적용하면 특징의 일관성이 깨집니다(그림 3 가운데 열).이는 RGB 공간에서의 일관성 수준이 프레임의 내부 특징의 일관성과 상관 관계가 있음을 의미합니다.따라서 핵심 아이디어는 편집된 비디오의 특징을 조작하여 원래 비디오 특징의 일관성 수준과 프레임 간 대응 관계를 유지하는 것입니다.그림 4에서 볼 수 있듯이 TokenFlow라는 프레임워크는 각 생성 시간 단계에서 두 가지 주요 구성 요소(i)를 번갈아 가며 수행합니다.키 프레임 세트를 샘플링하고 P에 따라 공동 편집합니다. 이 단계에서는 키 프레임 전체에 걸쳐 공유된 글로벌 모양이 생성되고, (ii) 키 프레임의 기능을 원본에서 제공한 대응 관계에 따라 모든 프레임으로 전파합니다.Input TAV PNP Gen-TextVideo FateZero 비디오 렌더링 Ours &quot;무지개 질감의 개&quot; &quot;반짝이는 금속 조각품&quot; &quot;황새 종이접기&quot; 그림 6: 비교. 프레임당 적용되는 Tune-A-Video(TAV, Wu et al. (2022)), PnPDiffusion(Tumanyan et al., 2023), Gen-1(Esser et al., 2023), Text2Video-Zero(Khachatryan et al., 2023a) 및 Fate-Zero(Qi et al., 2023)와 방법을 비교합니다. 전체 비디오 비교는 보충 자료를 참조하세요. 비디오 기능; 이 단계에서는 원본 비디오 기능의 일관성과 세분화된 공유 표현을 명시적으로 보존합니다. 두 단계 모두 이미지 편집 기술 €(예: Tumanyan et al. (2023))과 함께 수행됩니다. 직관적으로 키 프레임 편집과 전파를 번갈아 하는 것의 이점은 두 가지입니다. 첫째, 각 생성 단계에서 무작위 키 프레임을 샘플링하면 특정 선택에 대한 견고성이 높아집니다. 둘째, 각 생성 단계에서 더 일관된 기능이 생성되므로 다음 단계에서 샘플링된 키 프레임이 더 일관되게 편집됩니다. 전처리: 확산 기능 추출. 입력 비디오 I가 주어지면 각 프레임 I²에 DDIM 역전(3절 참조)을 적용하여 잠재 객체 [×1, ..., x]의 시퀀스를 생성합니다. 각 세대 타임스텝 t에 대해 각 프레임 i Є [n]의 잠재 객체 x½을 모델에 입력하고 네트워크의 모든 계층의 셀프 어텐션 모듈에서 토큰(x)을 추출합니다 €(그림 4, 위). 나중에 이러한 토큰을 사용하여 확산 기능 간의 프레임 간 대응 관계를 설정합니다. 4. 키프레임 샘플링 및 공동 편집 우리의 관찰에 따르면 단일 편집 프레임의 특징이 주어지면 해당 특징을 해당 위치로 전파하여 다음 프레임을 생성할 수 있습니다. 그러나 대부분의 비디오는 단일 키프레임으로 표현할 수 없습니다. 이를 설명하기 위해 여러 키프레임을 고려하여 나중에 전체 비디오로 전파될 특징(토큰) 집합 Tbase를 얻습니다. 구체적으로 각 생성 단계에서 고정된 프레임 간격으로 키프레임 집합 {Ji}iɛk을 무작위로 샘플링합니다(자세한 내용은 SM 참조). 셀프 어텐션 블록을 확장하여 동시에 처리하여 키프레임을 공동 편집합니다(Wu et al., 2022). 따라서 글로벌 모양을 공유하도록 장려합니다. 더 자세히 설명하면, 수정된 블록의 입력은 모든 키 프레임 {Q}iЄk, {Ki}iɛk, {Vi}iɛk의 셀프 어텐션 특징입니다. 여기서 Qi, K₁, Vi는 프레임 i EK의 쿼리, 키 및 값이며 K = {1,...}입니다. 모든 프레임의 키는 연결되고 확장된 주의는 다음과 같습니다.ik T ExtAttn (Q&#39;; [K¹¹, &#39; ,... Kik]) .K²k]™ QiKi = Softmax K“]&quot;) √d (2) 프레임 i에 대한 블록의 출력은 다음과 같습니다.V₁k] 여기서 Â = ExtAttn(Q²; [K¹¹‚.... Kik]) (3) ¢(Ƒ²) = Â · [V²¹ 직관적으로 각 키 프레임은 다른 모든 키 프레임을 쿼리하고 해당 키 프레임에서 정보를 집계합니다.이로 인해 편집된 프레임에서 대략적으로 통합된 모양이 나타납니다(Wu et al., 2022; Khachatryan et al., 2023b; Ceylan et al., 2023; Qi et al., 2023). 네트워크의 각 계층에 대해 Tbase = {(J²)}iЄk로 정의합니다(그림 4 하단 가운데). 4.2 토큰 플로우를 통한 편집 전파 Tbase가 주어지면 원본 비디오에서 추출한 토큰 대응 관계를 기반으로 비디오 전체에 전파합니다. 각 생성 단계 t에서 각 원본 프레임 토큰의 최근접 이웃(NN)인 (x½)과 두 인접 키 프레임 토큰인 (x-(+), (x-(¯¯)를 계산합니다. 여기서 가장 가까운 미래 키 프레임의 인덱스이고 가장 가까운 과거 키 프레임의 인덱스입니다. 결과 NN 필드 y²+, yi-를 다음과 같이 표시합니다. ²± [p] 计 = arg min D ((x²) [P], (x²±) [q]) q (4) 여기서 p, q는 토큰 피처 맵의 공간 위치이고 D는 코사인 거리입니다. 단순화를 위해 생성 시간 단계 t를 생략합니다. 우리의 방법은 모든 시간 단계와 셀프 어텐션 계층에 적용됩니다. ±를 얻으면 이를 사용하여 편집된 프레임의 토큰 Tbase를 나머지 비디오로 전파합니다.이는 각 공간 위치 p와 프레임 i에 해당하는 Tbase의 토큰을 선형적으로 결합하여 이루어집니다.Fy(Tbase, i, p) = w; • ¢(Ƒ²+)[y²+ [p]] + (1 − w;) ⋅ þ(ɲ²¯)[v²¯¯|[p]] (5) 여기서 (Ji±)Є Tbase이고 wiЄ (0, 1)은 프레임 i와 인접한 키 프레임 사이의 거리에 비례하는 스칼라입니다(SM 참조).이는 원활한 전환을 보장합니다.F가 샘플링된 키 프레임의 토큰도 수정한다는 점에 유의하세요.즉, 셀프 어텐션 블록을 수정하여 키 프레임을 포함한 모든 프레임에 대해 Tbase의 토큰의 선형 조합을 원래 비디오 토큰 대응에 따라 출력합니다. 알고리즘 1 TokenFlow 편집 입력: I = P = [I¹, ..., In] ▷ 입력 비디오 ▷ 대상 텍스트 프롬프트 ▷ 확산 기반 이미지 편집 기술 {x}}}=1, {(xi)}}=±1 ← DDIM-Inv[I²] \i € [n], t€ [T] J½, ., №← x, . . ., x Fort T,..., 1 do 전체 알고리즘 알고리즘 1에서 비디오 편집 알고리즘을 요약합니다. 먼저 입력 비디오 I에 DDIM 역전을 수행하고 모든 프레임 i Є [n]에 대한 노이즈 잠복 시퀀스 {x}{}\\₁를 추출합니다(그림 4, 위). 그런 다음 키 프레임 편집과 TokenFlow 전파를 번갈아가며 비디오의 노이즈를 제거합니다. 각 세대 단계 t에서 kn 키 프레임 인덱스를 무작위로 지정하고 이미지 편집 기술(예: Tumanyan et al. (2023); Meng et al. (2022); Zhang &amp; Agrawala (2023))을 확장된 주의(Eq. 3, Fig. 4 (I))와 결합하여 노이즈를 제거합니다. 그런 다음 네트워크의 모든 계층에서 모든 셀프 어텐션 블록에서 이미지 편집 기술과 TokenFlow(Eq. 5, Fig. 4 (II))를 결합하여 전체 비디오 Jt의 노이즈를 제거합니다. 각 계층에는 셀프 어텐션 블록의 입력과 출력 사이에 잔여 연결이 포함되므로 각 계층에서 TokenFlow를 수행하는 것이 필요합니다. 5 결과 K = {i1, … ….‚ ik} ← 샘플 키 프레임 인덱스 Fy± Vi Є [n]_NN 필드 계산 {Ƒ/{-1}j€K ← €0[{J{}jek; ExtAttn] Tbase ({-1}jek) 키 프레임의 토큰 추출 Jt−1 ← ćo [Jt; TokenFlow (Fy (Tbase))] 출력: J = [J], ……., Jn] DAVIS 비디오(Pont-Tuset et al., 2017)와 동물, 음식, 인간 및 다양한 물체가 움직이는 모습을 묘사한 인터넷 비디오에서 방법을 평가합니다. 비디오의 공간 해상도는 384×672 또는 512x512픽셀이며 40~200개의 프레임으로 구성됩니다. 각 비디오에 다양한 텍스트 프롬프트를 사용하여 다양한 편집 결과를 얻습니다. 평가 데이터 세트는 61개의 텍스트-비디오 쌍으로 구성됩니다. 우리는 프레임 편집 방법으로 PnP-Diffusion(Tumanyan et al., 2023)을 활용하고, 모든 결과에 동일한 하이퍼 매개변수를 사용합니다.PnP-Diffusion은 부정확한 DDIM 반전으로 인해 각 프레임의 구조를 정확하게 보존하지 못할 수 있습니다(그림 3, 가운데 열, 오른쪽 프레임: 개의 머리가 왜곡됨).우리의 방법은 여러 프레임이 비디오의 각 프레임 생성에 기여하므로 이에 대한 견고성을 향상시킵니다.우리의 프레임워크는 이미지의 구조를 정확하게 보존하는 모든 확산 기반 이미지 편집 기술과 결합될 수 있습니다.다른 이미지 편집 기술(예: Meng et al. (2022); Zhang &amp; Agrawala (2023))을 사용한 결과는 SM에서 사용할 수 있습니다.그림 5와 1은 편집된 비디오의 샘플 프레임을 보여줍니다.우리의 편집은 시간적으로 일관되고 편집 프롬프트를 따릅니다.남자의 머리는 반 고흐 또는 대리석으로 변경됩니다(왼쪽 상단); 중요한 점은 남자의 신원과 장면의 배경이 비디오 전체에서 일관되게 나타난다는 것입니다. 다각형 늑대(왼쪽 아래)의 패턴은 시간에 따라 동일합니다. 몸은 일관되게 주황색이고 가슴은 파란색입니다. 구현 세부 정보 및 비디오 결과는 SM에서 확인할 수 있습니다. 기준선. 저희의 방법을 최신 및 동시 작업과 비교합니다. (i) Fate-Zero(Qi et al., 2023) 및 (ii) Text2Video-Zero(Khachatryan et al., 2023b)는 셀프 어텐션 인플레이션을 사용하여 비디오 편집을 위한 텍스트-이미지 모델을 활용합니다. (iii) 이미지 모델의 셀프 어텐션 인플레이션에 광학 흐름 최적화를 추가하여 키 프레임을 편집한 다음, 기성품 전파 방법을 사용하여 키 프레임에서 편집 내용을 나머지 비디오로 전파하는 비디오 다시 렌더링(Yang et al., 2023). (iv) 주어진 테스트 비디오에서 텍스트-이미지 모델을 미세 조정하는 Tune-a-Video(Wu et al., 2022). (v) 대규모 이미지 및 비디오 데이터 세트에서 학습된 비디오 확산 모델인 Gen-1(Esser et al., 2023). (vi) 프레임당 확산 기반 이미지 편집 기준선, PnP-Diffusion(Tumanyan et al., 2023). 또한 다음 두 가지 기준선을 고려합니다. (i) 계층화된 비디오 표현(NLA)(Kasten et al., 2021)을 활용하고 CLIP 손실을 사용하여 테스트 시간 학습을 수행하는 Text2LIVE(Bar-Tal et al., 2022). NLA는 전경/배경 분리 마스크가 필요하고 학습하는 데 약 10시간이 걸립니다. (ii) Jamriška et al.(2019)을 사용하여 단일 키 프레임에 PnP-Diffusion을 적용하고 편집 내용을 전체 비디오에 전파합니다. 5. 정성적 평가 그림 6은 우리 방법과 주요 기준선의 정성적 비교를 제공합니다. 전체 비디오는 SM을 참조하십시오. 우리 방법(아래쪽 행)은 결과 편집된 비디오의 시간적 일관성을 유지하면서 편집 프롬프트를 더 잘 준수하는 비디오를 출력하는 반면, 다른 방법은 이 두 가지 목표를 모두 충족하는 데 어려움을 겪습니다. Tune-A-Video(두 번째 행)는 2D 이미지 모델을 비디오 모델로 팽창시키고 비디오의 동작에 과적합하도록 미세 조정합니다. 따라서 짧은 클립에 적합합니다. 긴 비디오의 경우 동작을 포착하는 데 어려움을 겪어 무의미한 편집(예: 반짝이는 금속 조각품)이 발생합니다. 각 프레임에 독립적으로 PnP를 적용하면(세 번째 행) 편집 프롬프트를 준수하는 정교한 편집이 생성되지만 예상대로 시간적 일관성이 없습니다. Gen-1(네 번째 행)의 결과도 일부 시간적 불일치(오리지미 황새의 부리 색이 변함)로 어려움을 겪습니다. 더욱이, 이들의 프레임 품질은 텍스트-이미지 확산 모델보다 상당히 나쁘다.Text2 Video-Zero와 Fate-Zero(다섯 번째와 여섯 번째 행)의 편집은 이러한 방법이 암묵적으로 일관성을 장려하기 위해 확장된 주의 메커니즘에 크게 의존하기 때문에 심각한 지터링으로 어려움을 겪는다.Rerender-a-Video의 결과는 먼 프레임(예: 키 프레임)에 대한 광학 흐름 추정에 주로 의존하기 때문에 발생하는 주목할 만한 장거리 불일치와 아티팩트를 보이는데, 이는 최적이 아닌 것으로 알려져 있다(SM에서 비디오 결과를 참조. 늑대가 머리를 돌리면 코 색깔이 바뀐다).Text2LIVE와 SM의 RGB 전파 기준선과의 정성적 비교를 제공한다.5. 정량적 평가 표 1: 워프 오류를 계산하고 사용자 연구를 수행하여 시간적 일관성 측면에서 방법을 평가하고, CLIP 유사성을 사용하여 대상 텍스트 프롬프트에 대한 충실도 측면에서 방법을 평가한다.자세한 내용은 5절을 참조한다.LDM 재구성. PnP-Diffusion Text2Video-Zero |Warp-err ↓ 사용자 선호도 CLIP (×10-³) 방법 점수 ↑ 2.0.11.94% 0.12.78% 0.Tune-a-Video Fate-Zero Gen30.82% 0.6.71% 0.70% 0.1.71% 0.Ours w joint attention Ours w/o rand keyframes 5.90% 0.3.0.3.0.우리는 다음의 측면에서 방법을 평가합니다: (i) 각 편집된 프레임의 CLIP 임베딩(Radford et al., 2021)과 대상 텍스트 프롬프트 간의 평균 유사도를 계산하여 측정한 편집 충실도; (ii) 시간적 일관성. Ceylan et al. (2023); Lai et al.에 따라 (2018a), 시간적 일관성은 (a) Teed &amp; Deng(2020)을 사용하여 원본 비디오의 광학 흐름을 계산하고, 이에 따라 편집된 프레임을 워핑하고, 워핑 오류를 측정하고, (b) 사용자 연구를 통해 측정합니다. 우리는 Kolkin et al.(2019)에서 제안한 Two-alternative Forced Choice(2AFC) 프로토콜을 채택합니다. Park et al. 우리의 것(2020)에서는 참가자에게 입력 비디오, 우리의 것, 기준 결과를 보여주고 어느 비디오가 시간적으로 더 일관되고 원본 비디오의 동작을 더 잘 보존하는지 결정하도록 요청합니다. 설문 조사는 Amazon Mechanical Turk를 사용하여 얻은 기준당 2000~3000개의 판단으로 구성됩니다. Gen1의 제품 플랫폼이 동일한 수의 입력 프레임을 출력하지 않기 때문에 워핑 오류를 측정할 수 없습니다. 표 1은 우리의 방법을 기준과 비교합니다. 우리의 방법은 가장 높은 CLIP Rerender-a-Video &quot;A tractor&quot; 그림 7: 한계를 달성합니다. 우리의 방법은 원본 비디오의 기능 대응성에 따라 비디오를 편집하므로 구조적 편차가 필요한 편집은 처리할 수 없습니다. 편집된 비디오와 입력 안내 프롬프트 사이에 좋은 적합성을 보여주는 점수입니다. 또한, 우리의 방법은 워핑 오류가 낮아 시간적으로 일관된 결과를 나타냅니다. Re-rendera-Video는 워핑 오류를 최적화하고 광학 흐름을 사용하여 편집을 전파하므로 워핑 오류가 가장 낮습니다. 그러나 광학 흐름에 대한 이러한 의존성은 종종 워핑 오류에 반영되지 않는 아티팩트와 장거리 불일치를 생성합니다. 그럼에도 불구하고 이는 사용자 연구에서 명백하게 나타나며, 사용자는 시간적 일관성 측면에서 모든 기준선보다 우리의 방법을 상당히 선호했습니다. 또한 편집을 수행하지 않고 원본 비디오를 LDM 자동 인코더로 전달하는 참조 기준선(LDM 재구성)을 고려합니다. 이 기준선은 LDM 자동 인코더가 달성할 수 있는 시간적 일관성에 대한 상한을 제공합니다. 예상대로 이 기준선의 CLIP 유사성은 편집을 포함하지 않으므로 좋지 않습니다. 그러나 이 기준선은 LDM 자동 인코더의 불완전한 재구성으로 인해 워프 오류가 0이 되지 않으며, 이는 고주파 정보를 환각합니다. TokenFlow를 사용하여 비디오 재구성의 정확도를 측정하여 대응 관계와 비디오 표현을 추가로 평가합니다. 구체적으로, 키 프레임 편집 부분만 제거하여 편집 방법과 동일한 파이프라인을 사용하여 비디오를 재구성합니다. 표 2는 바닐라 DDIM 재구성과 비교하여 이 재구성의 PSNR 및 LPIPS 거리를 보고합니다. 보시다시피 TokenFlow 재구성은 DDIM 역전을 약간 개선하여 견고한 프레임 표현을 보여줍니다. 이러한 개선은 키 프레임 무작위화에 기인할 수 있습니다. 생성 중에 각 프레임이 여러 다른 프레임에서 재구성되므로 어려운 프레임에 대한 견고성이 높아집니다. 주목할 점은 평가가 RGB 프레임 대응 관계 평가보다는 생성 중에 피처 공간 내의 정확한 대응 관계에 초점을 맞추는데, 이는 방법에 필수적이지 않습니다. 5. ABLATION 연구 표 2: 키 프레임 편집을 제외한 TokenFlow 파이프라인을 사용하여 비디오를 재구성합니다. 우리는 PSNR 및 LPIPS 메트릭을 사용하여 TokenFlow 표현을 평가합니다. 우리의 재구성은 바닐라 DDIM 역산을 개선하여 TokenFlow 표현의 견고성을 강조합니다. 첫째, 시간적 일관성을 강제하기 위해 TokenFlow, Sec. 4.2의 사용을 중단합니다. 이
