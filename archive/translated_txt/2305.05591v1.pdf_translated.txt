--- ABSTRACT ---
최근의 여러 연구에서 객체 중심 아키텍처가 비전 도메인에서 비지도 장면 분해에 적합한 것으로 나타났습니다. 이러한 방법에서 영감을 얻어 오디오 도메인에서 블라인드 소스 분리를 위한 슬롯 중심 생성 모델인 AudioSlots를 제시합니다. AudioSlots는 순열 등가 인코더 및 디코더 네트워크를 사용하여 구축됩니다. Transformer 아키텍처를 기반으로 하는 인코더 네트워크는 혼합 오디오 스펙트로그램을 순서 없는 독립 소스 임베딩 세트에 매핑하는 방법을 학습합니다. 공간 브로드캐스트 디코더 네트워크는 소스 임베딩에서 소스 스펙트로그램을 생성하는 방법을 학습합니다. 순열 불변 손실 함수를 사용하여 종단 간 방식으로 모델을 학습합니다. Libri2Mix 음성 분리에 대한 결과는 이 접근 방식이 유망하다는 개념 증명을 구성합니다. 접근 방식의 결과와 한계를 자세히 논의하고 한계를 극복하고 향후 작업 방향을 더 설명합니다. 색인 용어 음성 분리, 객체 중심 표현 1.
--- INTRODUCTION ---
최근, 집합 구조화된 데이터에서 작동하는 신경망 기반 아키텍처와 비구조화된 입력에서 집합 구조화된 출력 공간으로 매핑하는 방법을 학습하는 아키텍처에 대한 많은 연구가 있었습니다. 특히, 비전 도메인에서 슬롯 중심 또는 객체 중심 아키텍처는 객체 감지 [1] 및 비지도 객체 검색 [2, 3]의 최근 발전을 뒷받침합니다. 이러한 객체 중심 아키텍처는 순열 동치성의 내장된 귀납적 편향을 가지고 있어 오디오 분리 작업에 자연스럽게 적합합니다. 이 논문에서 우리는 이러한 아키텍처의 핵심 아이디어를 사운드 분리 문제에 적용합니다. 즉, 소스나 믹싱 프로세스에 대한 특권 지식에 액세스하지 않고 혼합된 오디오 신호에서 오디오 소스를 분리하는 작업입니다. 사운드 분리는 소스의 순서가 임의적이기 때문에 본질적으로 집합 기반 문제입니다. 우리는 사운드 분리를 순열 불변 조건 생성 모델링 문제로 구성합니다. 우리는 혼합된 오디오 스펙트로그램에서 순서 없는 독립적인 소스 스펙트로그램 집합으로의 매핑을 학습합니다. 영어: 저희의 방법인 AudioSlots는 오디오를 소스당 개별 잠재 변수로 분리한 다음 이를 개별 소스 스펙트로그램으로 디코딩합니다. Transformer 아키텍처[4]를 기반으로 하는 순열 등가 인코더 및 디코더 함수를 사용하여 구축되었으며 따라서 소스 잠재 변수(&quot;슬롯&quot;)의 순서에 불변합니다. 이러한 아키텍처의 가능성을 평가하기 위해, 저희는 혼합된 오디오 신호에서 별도의 소스를 생성하기 위해 매칭 기반 손실을 사용하여 AudioSlots를 학습합니다. 저희는 Libri2Mix[5]의 간단한 두 화자 음성 분리 작업에서 저희 방법을 시연합니다. *Google에서 인턴으로 일하는 동안 수행한 작업. 저희의 결과는 주로 이 아이디어에 대한 개념 증명을 구성하지만, 슬롯 중심 생성 모델을 사용한 사운드 분리는 가능성을 보여주지만, 특정한 과제가 따른다는 것을 알게 되었습니다. 제시된 모델 버전은 고주파 세부 정보를 생성하는 데 어려움을 겪고, 독립적으로 예측된 오디오 청크를 스티칭하기 위해 휴리스틱에 의존하며, 여전히 학습을 위해 기준 진실 참조 오디오 소스가 필요합니다. 우리는 이러한 과제가 향후 작업을 통해 극복될 수 있을 것이라 낙관하며, 이 논문에서는 이에 대한 가능한 방향을 설명합니다. 2.
--- RELATED WORK ---
저희의 연구는 사운드 분리를 위한 새로운 세트 기반 생성 모델링 접근 방식을 탐구합니다. 다음에서 사운드 분리를 위한 최근의 사전 학습 기반 접근 방식과 다른 도메인에서 사용되는 세트 기반(또는 슬롯 중심) 신경망 아키텍처에 대한 간략한 개요를 제공합니다. 사운드 분리. 다양한 신경망
--- METHOD ---
s 우리는 오디오 도메인에서 블라인드 소스 분리를 위한 슬롯 중심 생성 모델인 AudioSlots를 제시합니다. AudioSlots는 순열 등가 인코더 및 디코더 네트워크를 사용하여 구축됩니다. Transformer 아키텍처를 기반으로 하는 인코더 네트워크는 혼합 오디오 스펙트로그램을 순서 없는 독립 소스 임베딩 세트에 매핑하는 방법을 학습합니다. 공간 브로드캐스트 디코더 네트워크는 소스 임베딩에서 소스 스펙트로그램을 생성하는 방법을 학습합니다. 우리는 순열 불변 손실 함수를 사용하여 종단 간 방식으로 모델을 학습합니다. Libri2Mix 음성 분리에 대한 결과는 이 접근 방식이 유망하다는 개념 증명을 구성합니다. 우리는 우리 접근 방식의 결과와 한계를 자세히 논의하고, 한계를 극복하고 향후 작업 방향을 더 자세히 설명합니다. 색인 용어 음성 분리, 객체 중심 표현 1. 서론 최근 집합 구조화된 데이터에서 작동하는 신경망 기반 아키텍처와 비구조화된 입력에서 집합 구조화된 출력 공간으로 매핑하는 방법을 학습하는 아키텍처에 대한 많은 연구가 있었습니다. 특히, 비전 도메인에서 슬롯 중심 또는 객체 중심 아키텍처는 객체 감지 [1] 및 비지도 객체 발견 [2, 3]의 최근 진전을 뒷받침합니다. 이러한 객체 중심 아키텍처는 순열 동치성의 내장된 귀납적 편향을 가지고 있어 오디오 분리 작업에 자연스럽게 적합합니다. 이 논문에서 우리는 이러한 아키텍처의 핵심 아이디어를 사운드 분리 문제에 적용합니다. 즉, 소스나 믹싱 프로세스에 대한 특권 지식에 액세스하지 않고 혼합 오디오 신호에서 오디오 소스를 분리하는 작업입니다. 사운드 분리는 소스의 순서가 임의적이므로 본질적으로 집합 기반 문제입니다. 우리는 사운드 분리를 순열 불변 조건 생성 모델링 문제로 구성합니다. 우리는 혼합 오디오 스펙트로그램에서 순서 없는 독립 소스 스펙트로그램 집합으로의 매핑을 학습합니다. 우리의 방법인 AudioSlots는 오디오를 소스당 개별 잠재 변수로 분리한 다음 이를 개별 소스 스펙트로그램으로 디코딩합니다. 이는 Transformer 아키텍처[4]를 기반으로 하는 순열 등가 인코더 및 디코더 함수를 사용하여 구축되었으며 따라서 소스 잠재 변수(&quot;슬롯&quot;)의 순서에 불변합니다. 이러한 아키텍처의 가능성을 평가하기 위해, 우리는 혼합된 오디오 신호에서 별도의 소스를 생성하기 위해 매칭 기반 손실을 사용하여 AudioSlots를 훈련합니다. 우리는 Libri2Mix[5]의 간단한 두 화자 음성 분리 작업에 대한 방법을 시연합니다. *Google에서 인턴으로 일하는 동안 수행한 작업. 우리의 결과는 주로 이 아이디어에 대한 개념 증명을 구성하지만, 슬롯 중심 생성 모델을 사용한 사운드 분리는 가능성을 보여주지만, 특정한 과제가 따른다는 것을 알게 되었습니다. 제시된 버전의 모델은 고주파 세부 정보를 생성하는 데 어려움을 겪고, 독립적으로 예측된 오디오 청크를 스티칭하기 위해 휴리스틱에 의존하며, 여전히 훈련을 위해 기준 진실 참조 오디오 소스가 필요합니다. 우리는 이러한 과제가 향후 작업에서 극복될 수 있을 것이라고 낙관하며, 이 논문에서 가능한 방향을 설명합니다. 2. 관련 작업 우리의 작업은 사운드 분리를 위한 새로운 세트 기반 생성 모델링 접근 방식을 탐구합니다. 다음에서 우리는 사운드 분리를 위한 최근의 사전 학습 기반 접근 방식과 다른 도메인에서 사용되는 세트 기반(또는 슬롯 중심) 신경망 아키텍처에 대한 간략한 개요를 제공합니다.사운드 분리.다양한 신경망 방법이 지도 사운드 분리를 위해 제안되었으며, 사운드 재구성 메커니즘과 전체 아키텍처 측면에서 다릅니다.마스크 기반 방법은 입력 오디오 신호의 분석/합성 기반 표현(예: STFT 또는 학습된 기반)에 적용되는 분리 마스크를 예측하여 사운드를 재구성합니다(예: [6, 7, 8, 9, 10, 11, 12, 13]).또는 직접 재구성 방법은 마스크를 명시적으로 추정하지 않고 소스 신호 또는 해당 스펙트럼을 추정합니다[14, 15].순환 네트워크[16], 합성곱 네트워크[9], U-nets[14], 어텐션 네트워크[17] 및 이들의 조합을 포함하여 사운드 분리를 위해 많은 일반 아키텍처가 제안되었습니다. 이는 훈련 중에 순열 불변 손실을 사용하여 출력 소스의 임의 순열을 처리합니다[7, 8]. 일부 방법은 각 소스에 해당하는 순서 없는 표현을 생성하여 아키텍처 수준에서 순열 불변성을 처리하기 위해 더 나아갔습니다. 딥 클러스터링 및 딥 어트랙터 네트워크[6, 18, 7]는 각 시간-주파수 빈의 임베딩에 대해 어텐션과 유사한 방식으로 작동하는 순열-등가 아키텍처를 사용합니다. 저희의 접근 방식은 슬롯 기반 어텐션 메커니즘을 사용하여 각 소스에 대한 임베딩을 생성하고 NeRF와 유사한[19] 방법을 사용하여 소스 스펙트로그램을 직접 추정하기 위해 이를 디코딩합니다. 이는 직접 예측을 사용하는 마스크 기반 방법과 다르며 NeRF 아키텍처를 사용하는 직접 예측 방법은 참신합니다. 슬롯 기반 어텐션 메커니즘은 사운드 분리를 위한 이전의 어텐션 네트워크와 다르며 딥 클러스터링과 더 밀접하게 관련되어 있습니다. 그러나 저희의 어텐션 방법은 개별 시간-주파수 빈이 아닌 상위 레벨 스펙트로그램 영역에서 작동하며 간단한 친화성 기반 방법 대신 범용 어텐션 메커니즘을 사용합니다. 영어: MixIT(혼합 불변 학습) [20]과 같은 최근의 비지도 접근 방식은 학습을 위해 오디오 혼합만을 사용합니다. 저희 작업에서는 기준 진실 분리된 참조 소스를 사용하여 지도된 사운드 분리만 탐구하지만 MixIT 사전 처리 인코더 CNN 변환기 디코더 -매칭 공간 방송 디코더 그림 1: 아키텍처 개요와 같은 학습 설정이 필요합니다. 입력 파형은 먼저 잘라내어 스펙트로그램으로 변환합니다. 그런 다음 신경망은 스펙트로그램을 순열 불변 소스 임베딩 $1...n 세트로 인코딩하고 이러한 임베딩은 디코딩되어 개별 소스 스펙트로그램 세트를 생성합니다. 전체 파이프라인은 매칭 기반 순열 불변 손실 함수를 사용하여 기준 진실 소스 스펙트로그램으로 지도됩니다. 저희 접근 방식과 직교하며 향후 작업에서 탐구하는 것이 흥미로울 것입니다. 슬롯 중심 신경망. 순서 없는 기능 세트에서 작동하는 신경망은 얼마 동안 연구되었습니다[21, 22, 23]. 우리 연구와 가장 밀접하게 관련된 것은 일부 입력 데이터[24, 25, 1]에 따라 조건화된 무순서 출력 집합을 생성하는 접근 방식과 시각적 장면의 객체와 같은 입력의 순열 불변 측면[2, 3]을 모델링하기 위해 잠재 변수 집합(&quot;슬롯&quot;)을 사용하는 방법입니다.후자를 슬롯 중심 신경망이라고 합니다.시각의 맥락에서 이 모델 클래스는 객체 감지[1], 파노라마 분할[26], 비지도 객체 발견[2, 3, 27]을 포함한 많은 현대적 장면 이해 접근 방식의 기초를 형성합니다.우리의 연구에서 슬롯 중심 생성 모델이 오디오 처리에서 구성적이고 순열 불변적인 작업, 특히 혼합물에서 개별 오디오 소스를 분리하는 데 유망함을 보여줍니다.3. 방법 우리는 감독 사운드 분리를 위한 일반화된 순열 불변 학습 프레임워크를 제시합니다.이전 방법과 달리 우리는 생성적 관점에서 소스 분리 작업에 접근합니다. 우리 방법의 주요 목적은 입력 오디오를 각각 입력의 다른 소스를 나타내는 임베딩 세트로 투사하는 것입니다.그런 다음 이러한 임베딩을 사용하여 순열 불변 방식으로 개별 소스의 크기 스펙트로그램을 생성합니다.전체 파이프라인은 순열 불변 손실을 사용하여 기준 진실 소스 스펙트로그램으로 감독됩니다.이 섹션의 나머지 부분에서는 학습 파이프라인의 다른 단계에 대해 설명합니다.전처리: 학습 중에 혼합 파형이 주어지면 먼저 0.5초 오디오 클립을 무작위로 자릅니다.그런 다음 [28]에 따라 창 크기가 512이고 홉 크기가 125인 단시간 푸리에 변환을 사용하여 클립을 스펙트로그램으로 변환합니다.그런 다음 복소 스펙트로그램의 절대값은 낮은 값을 강조하기 위해 0.3의 거듭제곱으로 지수화하여 비선형적으로 조정됩니다.이 조정된 절대값은 다음 단계의 입력으로 전달됩니다. 인코딩: 소스 임베딩을 추론하기 위해 입력 스펙트로그램은 먼저 ResNet-34 네트워크[29]를 사용하여 32 × 8 그리드의 인코딩된 &quot;이미지&quot; 피처 z로 인코딩됩니다. ResNet 루트 블록에서 축소된 스트라이드를 사용하여 더 높은 공간 해상도를 유지합니다. 다음으로 4개 레이어가 있는 변환기는 z를 소스 임베딩 $1...n에 매핑합니다. 여기서 n은 소스 수이고 si Є Rd입니다. 원래 공식[4]과 달리 각 변환기 레이어에서 먼저 쿼리 벡터 q 간에 셀프 어텐션 연산을 수행한 다음 셀프 어텐션 단계와 z의 출력 간에 교차 어텐션을 수행합니다. 셀프 어텐션과 교차 어텐션 단계에서 키와 값으로 동일한 변수를 사용합니다. 차원이 4096인 초기 쿼리 q는 DETR[1]과 유사하게 역전파를 통해 학습됩니다. 디코딩: 공간 브로드캐스트 디코더[30]를 사용하여 s₁...n에서 개별 소스 스펙트로그램을 생성합니다. 먼저 각 임베딩 si를 2D 그리드에 복사하여 모양이 F × Txd인 텐서 g₁를 생성합니다.여기서 F, T는 출력 스펙트로그램의 주파수 빈과 타임스텝입니다.그런 다음 푸리에 피처[31]가 있는 위치 임베딩을 gi에 추가하여 모양이 F x T x (d + e)가 됩니다.여기서 e는 각 푸리에 피처 임베딩의 크기입니다.그런 다음 공유 매개변수가 있는 완전히 연결된 네트워크를 gi의 모든 벡터에 적용하여 각각 모양이 F × T인 스펙트로그램 집합에 도달합니다.이 디코더는 NeRF 모델[19]과 유사하며, 이 모델도 위치 코드를 입력으로 받고 완전히 연결된 네트워크를 학습하여 좌표에 따라 달라지는 출력을 생성합니다.우리의 경우 2D 이미지 그리드에 배열된 스펙트로그램 값을 출력으로 직접 생성하고 생성된 각 스펙트로그램에 대해 해당 잠재 소스 임베딩 s에 따라 네트워크를 추가로 조절합니다. 목적: 소스 분리 문제가 순열 불변이기 때문에 예측 스펙트로그램은 미리 정해진 순서 없이 생성됩니다. 따라서 추정 스펙트로그램을 실제 스펙트로그램과 매치하여 네트워크 손실을 계산해야 합니다. 실제 스펙트로그램과 추정 스펙트로그램 간의 모든 가능한 매치 중에서 최소 재구성 오류로 최적의 할당을 찾으려고 합니다. 속도, 정확도 및 많은 수의 항목을 효율적으로 처리할 수 있는 능력 때문에(비록 소스 수가 적은 데이터 세트만 사용하지만) 이 할당 문제를 해결하기 위해 헝가리 매칭 알고리즘을 사용할 것을 제안합니다.
--- EXPERIMENT ---
s). 각 가능한 쌍에 비용 또는 가중치를 할당한 다음, 이와 관련된 총 비용 또는 가중치를 최소화하는 쌍을 선택하여 작동합니다. 마지막으로, 매칭된 실제 값과 추정된 스펙트로그램 간의 평균 제곱 오차는 네트워크 매개변수를 최적화하기 위한 학습 목표로 최소화됩니다. 소스 분리: 테스트하는 동안 입력 혼합 파형이 주어지면 먼저 길이가 0.5초인 여러 개의 겹치지 않는 파형으로 나눕니다. 입력 파형 길이가 0.5초의 청크로 균등하게 나누어지지 않는 경우 끝에 0 패딩을 추가합니다. 이러한 파형은 위에서 언급한 대로 전처리되어 네트워크에 입력으로 전달되고, 위의 파이프라인을 사용하여 개별 소스의 스펙트로그램의 절대값을 추정하도록 학습됩니다. 추정된 스펙트로그램은 먼저 전처리 중에 수행된 스케일링을 반전하기 위해 1/0.3의 거듭제곱으로 거듭제곱하여 스케일을 조정합니다. 이러한 스케일 조정된 추정치는 오라클 방법에서 마스크를 계산하여 입력에서 개별 소스의 복잡한 스펙트로그램 추정치를 만드는 데 사용됩니다. 입력 스펙트로그램 I가 주어지면 출력 소스 스펙트로그램은 mi * I로 계산됩니다. 여기서 mi는 스펙트로그램을 사용하여 추정된 i번째 소스에 해당하는 마스크입니다. 표 1: 결과. 추정된 절대 스펙트로그램을 입력 복소 스펙트로그램에 대한 마스크로 사용하여 개별 소스의 복소 스펙트로그램을 생성합니다[33]. 아래 표에서는 IBM 및 Wienerlike 마스킹을 사용하여 SI-SNR[dB] 및 SI-SNRi[dB] 값을 제시합니다. 높을수록 좋습니다. Autoencoder와 AudioSlots 간에 성능 저하가 최소한으로 나타나는데, 이는 파이프라인이 음성을 잘 분리하는 법을 배울 수 있음을 보여줍니다. SI-SNRi SI-SNR SI-SNRi IBM Wiener Wiener 12.SI-SNR Masking Type IBM Oracle 12.12.12.Autoencoder 10.10.10.10.10.AudioSlots 09.09.09.09.09.13.13.09.10.13.10.Oracle (1초) AudioSlots (1초) 09.신경망에 의해 예측됩니다.이러한 복잡한 스펙트로그램은 역단시간 푸리에 변환(STFT)을 사용하여 파형으로 반전된 다음 함께 꿰매어 단순화를 위해 기준 진실 신호와 가장 잘 일치하는 항목을 사용하여 일치를 해결합니다.학습: 배치 크기 64, 학습 속도 2e-4, 워밍업 단계 2500개 및 코사인 감쇠 일정으로 300k 단계에 대해 Adam [32]을 사용하여 학습합니다. 4. 실험 Libri2Mix [5] 데이터 세트를 사용하여 음성 분리에 대한 방법의 성능을 평가합니다. 무반향 버전의 데이터 세트를 사용합니다. 데이터 세트의 각 인스턴스는 16kHz, 10초 길이로 샘플링됩니다. Libri2Mix에는 LibriSpeech [34]에서 가져온 남성 및 여성 화자의 발화가 포함됩니다. 데이터 세트의 train360-clean 분할에는 364시간 분량의 혼합물이 포함되어 있으며 소스는 중복 없이 추출됩니다. 위에서 언급했듯이 입력 스펙트로그램과 네트워크 예측을 사용하여 개별 소스의 복소 스펙트로그램을 추정하기 위해 마스킹을 사용합니다. 사용할 수 있는 다양한 마스킹 함수가 있습니다 [33]. 실험에서는 이상적 바이너리 마스크(IBM) [35]와 Wiener 필터 유사 마스크를 마스크 함수로 사용하며 이는 다음과 같이 정의됩니다.IBM: mi = i = argmax(m) 그렇지 않으면 &quot;Wiener 유사&quot;: mi = (mi)Σ-1 (m)메트릭: SI-SNR(스케일 불변 신호 대 잡음비) [36]과 SI-SNR 개선(SI-SNRi)을 사용하여 분리 성능을 측정합니다.y가 목표를 나타내고 ŷ가 우리 방법으로 얻은 추정치를 나타냅니다.그런 다음 SI-SNR은 목표를 재스케일링하여 임의의 스케일 내에서 y와 ŷ 사이의 충실도를 측정합니다.SI-SNR(y, ŷ) = 10 log||ay||&#39; ||ay — ŷ||² 여기서 a = argmin ||ay ŷ||² = y¹ŷ/||y||². SI-SNRi는 처리 후 각 소스 추정치의 SI-SNR과 각 소스에 대한 추정치로 입력 혼합물을 사용하여 얻은 SI-SNR의 차이입니다.평가하는 동안 먼저 SI-SNR을 최대화하기 위해 대상과 추정치를 일치시킨 다음 결과 SI-SNR 및 SI-SNRi 점수의 평균을 냅니다.결과: 표 1에서 우리는 우리의 성능을 우리 방법의 자동 인코더 변형(입력으로 기준 진실 참조 소스를 받음)과 (전처리된) 기준 진실 신호를 사용하여 얻은 분리 성능과 비교합니다.기준 진실 신호를 사용하여 계산된 메트릭은 (손실이 있는) 전처리로 얻을 수 있는 최대값을 나타냅니다.자동 인코더 변형에서 우리는 n = 1인 개별 소스 신호를 재구성하도록 네트워크를 훈련합니다.그런 다음 개별 추정치를 혼합물의 복합 스펙트로그램에 대한 마스크로 사용합니다.스펙트로그램에는 고주파 기능이 포함되어 있으므로 이를 통해 아키텍처가 이러한 기능을 충실하게 표현할 수 있는 능력을 이해하는 데 도움이 됩니다. 또한 전처리 단계에서 자르기 길이를 1초로 늘려서 절제를 제시합니다. 자동 인코더 변형과 분리 모델 간의 성능 차이는 0.18±0.01에 불과합니다. 이는 AudioSlots라는 방법이 음성 분리를 잘 학습하여 완전히 분리된 소스를 입력으로 받는 기준선의 성능에 근접한다는 것을 나타냅니다. 그래도 모델과 전체 파이프라인 측면에서 개선의 여지가 상당히 있습니다. 이전의 마스킹 기반 접근 방식[11, 12, 13]은 이미 Libri2Mix 스피커 분리를 인상적인 수준으로 해결하여 여기서 보고하는 것보다 훨씬 높은 SISNR 값을 달성했습니다. 아주 최근에 확산 기반 접근 방식도 경쟁력 있는 성능을 보였습니다[37]. 이 격차는 부분적으로 손실이 많은 전처리 파이프라인 때문입니다. 예를 들어, 사전 청크 오디오에서 STFT를 계산하면(단순화를 위해 여기서 수행) 경계 아티팩트가 발생하여 다른 모델이 달성할 수 있는 것보다 기준 진실 SI-SNR 점수가 낮아집니다. 우리는 단순성을 위해 모든 오디오 신호를 동일한 길이로 0 패딩합니다. 또한 생성된 스펙트로그램에서 마스크를 생성하기 때문에 마스크 기반 방법의 한계, 즉 마스크된 스펙트럼 콘텐츠를 재생할 수 없다는 제약도 따릅니다. 한계: 실험적 비교는 재구성 충실도라는 방법의 주요 한계를 강조합니다. 자동 인코더 기준선과 AudioSlots 모델은 모두 잠재적 병목 현상 표현을 사용하여 신호를 인코딩하고 특정 고주파 세부 정보(기타)를 버리는 경향이 있습니다. 이는 그림 2에서도 정성적으로 볼 수 있습니다. 비교는 자르기 길이가 분리 성능에 영향을 미친다는 것을 추가로 보여줍니다. 이는 단순한 자르기 접근 방식으로 인한 경계 아티팩트로 부분적으로 설명할 수 있지만, AudioSlot이 청크 길이에서 오디오 스펙트로그램을 분리하고 재구성하는 기능의 민감성을 암시하기도 합니다. 토론: 우리의 결과는 소스별 잠재적 변수 집합을 사용하여 오디오를 표현하는 슬롯 중심 생성 모델을 사용하여 오디오 분리를 해결하는 데 유망함을 보여줍니다. 이는 예를 들어 마스킹 기반 접근 방식[20]을 사용하여 입력 오디오에서 직접 작동하는 이전 방식과 크게 다릅니다. 소스별 잠재 변수를 학습하는 것은 이러한 분해된 잠재 변수를 생성뿐만 아니라 인식 작업에도 사용할 수 있다는 이점이 있습니다. 이는 객체 중심 컴퓨터 비전 모델의 슬롯이 객체 감지 및 분할의 기반으로 사용되는 방식과 유사합니다. 우리는 현재 접근 방식의 한계가 향후 작업에서 극복될 수 있다고 낙관합니다. • 재구성 충실도(고주파 특징에 대한 흐릿한 재구성) 문제를 해결하기 위해 AudioLM[38]과 같이 결정론적 피드포워드 디코더에서 자기 회귀 디코딩 접근 방식이나 [39]와 같이 반복적 확산 기반 디코더로 이동하면 고충실도 생성과의 격차를 메울 수 있습니다. • 현재 AudioSlots는 기준 진실 소스의 형태로 감독을 가정합니다. 영어: 원시, Autoencoder Reconstruction Reconstruction Ours Groundtruth Input에 대한 완전 비지도 학습에 대한 확장 그림 2: Groundtruth, Autoencoder 추정치 및 AudioSlots(Ours) 추정치의 개별 소스 스펙트로그램의 절대값 비교.입력 스펙트로그램(위)은 혼합이고 나머지 행은 개별 소스의 스펙트로그램을 보여줍니다.입력 및 Groundtruth 스펙트로그램은 Sec 3에서 언급한 단계를 사용하여 전처리됩니다.우리의 방법은 고조파를 상당히 잘 재구성할 수 있지만 고주파 기능(강조된 예제 영역 참조)을 추정하는 데 어려움을 겪습니다.혼합 오디오가 바람직할 것입니다.이를 위해 AudioSlots의 Transformer를 Slot Attention[3] 모듈로 대체하는 것을 살펴보았습니다.이 모듈은 분해에 대한 귀납적 편향이 있어 시각적 장면 분해의 맥락에서 비지도 학습이 가능합니다. 그러나 초기 실험에서 우리는 이 귀납적 편향이 완전한 비지도 방식으로 오디오 스펙트로그램을 분해하기에 충분하지 않을 수 있다는 것을 발견했습니다.그러나 Slot Attention 모듈이 있는 AudioSlots의 지도 버전은 초기 실험에서 Transformer 모듈이 있는 버전과 유사한 성능을 보였으며, 이는 향후 작업에 대한 추가 탐색이 여전히 유망하다는 것을 강조했습니다.• 사후 스티칭이 필요한 개별 청크를 격리하여 처리하는 데 따른 한계는 Slot Attention for Video [40]에서와 같이 이전 시간 단계의 슬롯이 다음 시간 단계의 초기화로 사용되는 모델의 순차적 확장을 사용하여 극복할 수 있다고 생각합니다.이는 향후 작업으로 남겨둡니다.5.
--- CONCLUSION ---
우리는 오디오 스펙트로그램을 위한 슬롯 중심 생성 아키텍처인 AudioSlots를 제시합니다. 우리는 AudioSlots가 구조화된 생성 모델을 사용하여 오디오 소스 분리 작업을 처리하는 데 유망하다는 개념 증명을 보여줍니다. AudioSlots에 대한 현재 구현에는 고주파 기능에 대한 낮은 재구성 충실도와 감독으로 분리된 오디오 소스가 필요하다는 것을 포함하여 여러 가지 제한이 있지만, 우리는 이러한 제한이 극복될 수 있다고 낙관하고 향후 작업에 대한 몇 가지 가능한 방향을 설명합니다. 6. 참고문헌 [1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, &quot;변압기를 사용한 종단 간 객체 감지&quot;, ECCV, 2020. [2] Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, Alexander Lerchner, &quot;반복적 변분 추론을 사용한 다중 객체 표현 학습&quot;, ICML, 2019. [3] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, Thomas Kipf, &quot;슬롯 어텐션을 사용한 객체 중심 학습&quot;, NeurIPS, 2020. [4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin, &quot;Attention is all you need,&quot; NeurIPS, 2017. [5] Joris Cosentino, Manuel Pariente, Samuele Cornell, Antoine Deleforge 및 Emmanuel Vincent, &quot;Librimix: 일반화 가능한 음성 분리를 위한 오픈 소스 데이터 세트&quot; arXiv preprint arXiv:2005.11262, 2020. [6] John R Hershey, Zhuo Chen, Jonathan Le Roux 및 Shinji Watanabe, &quot;심층 클러스터링: 분할 및 분리를 위한 차별적 임베딩&quot;, IEEE 국제 음향, 음성 및 신호 처리 회의(ICASSP), 2016. [7] 유수프 이식, 조나단 르 Roux, Zhuo Chen, Shinji Watanabe, and John R Hershey, &quot;심층 클러스터링을 사용한 단일 채널 다중 화자 분리,&quot; arXiv 사전 인쇄본 arXiv:1607.02173, 2016. [8] Morten Kolbæk, Dong Yu, Zheng-Hua Tan, and Jesper Jensen, &quot;심층 순환 신경망의 발화 수준 순열 불변 훈련을 통한 다중 화자 음성 분리,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 저널, 2017. [9] Yi Luo and Nima Mesgarani, &quot;Conv-tasnet: 음성 분리를 위한 이상적인 시간-주파수 크기 마스킹을 능가,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 저널, 2019. [10] Ethan Manilow, Gordon Wichern, Prem Seetharaman, and Jonathan Le Roux, &quot;Slakh의 음악 소스 분리를 줄이기: 훈련의 영향을 연구하는 데이터 세트 데이터 품질 및 양, Proc. IEEE 오디오 및 음향에 대한 신호 처리 응용 워크숍(WASPAA), 2019. #[11] Yi Luo, Zhuo Chen, Takuya Yoshioka, &quot;Dual-path rnn: efficient long sequence modeling for time-domain singlechannel speech separation,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2020. [12] Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, Jianyuan Zhong, &quot;Attention is all you need in speech separation,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2021. [13] Kai Li, Runxuan Yang, Xiaolin Hu, &quot;음성 분리를 위한 탑다운 어텐션을 갖춘 효율적인 인코더디코더 아키텍처,&quot; arXiv 사전 인쇄본 arXiv:2209.15200, 2022. [14] Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas 및 Dominik Roblek, &quot;SEANet: 다중 모드 음성 향상 네트워크&quot;, Proc. Interspeech, 2020. [15] Zhong-Qiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeong-Yeol Kim, Shinji Watanabe, “Tf-gridnet: 모노럴 스피커 분리를 위해 시간-주파수 영역 모델을 다시 훌륭하게 만들기,&quot; arXiv 사전 인쇄본 arXiv:2209.03952, 2022. [16] Felix Weninger, John R Hershey, Jonathan Le Roux, Björn Schuller, &quot;단일 채널 음성 분리를 위한 차별적으로 훈련된 순환 신경망,&quot; 2014 IEEE 신호 및 정보 처리 글로벌 컨퍼런스(GlobalSIP). IEEE, 2014, 577-581쪽. [17] Yuma Koizumi, Shigeki Karita, Scott Wisdom, Hakan Erdogan, John R Hershey, Llion Jones, Michiel Bacchiani, &quot;Df-conformer: 음성 향상을 위한 선형 복잡도 자체 주의를 사용하는 conv-tasnet 및 conformer의 통합 아키텍처&quot;, 2021 IEEE 오디오 및 음향에 대한 신호 처리 응용 워크숍(WASPAA). IEEE, 2021, 161-165쪽. [18] Zhuo Chen, Yi Luo 및 Nima Mesgarani, &quot;단일 마이크 스피커 분리를 위한 딥 어트랙터 네트워크&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2017. [19] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng, &quot;Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현&quot;, ECCV, 2020. [20] Scott Wisdom, Efthymios Tzinis, Hakan Erdogan, Ron Weiss, Kevin Wilson, John Hershey, &quot;혼합 불변 훈련을 사용한 무감독 사운드 분리&quot;, NeurIPS, 2020. [21] Oriol Vinyals, Samy Bengio, Manjunath Kudlur, &quot;순서가 중요합니다: 세트의 시퀀스에서 시퀀스로&quot;, arXiv 사전 인쇄본 arXiv:1511.06391, 2015. [22] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, 및 Alexander J Smola, &quot;Deep sets,&quot; NeurIPS, 2017. [23] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, 및 Yee Whye Teh, &quot;Set transformer: A framework for attention-based permutation-invariant neural networks,&quot; ICML, 2019. [24] Yan Zhang, Jonathon Hare, 및 Adam Prugel-Bennett, &quot;Deep set prediction networks,&quot; NeurIPS, 2019. [25] Adam R Kosiorek, Hyunjik Kim, 및 Danilo J Rezende, &quot;Conditional set generation with transformers,&quot; arXiv 사전 인쇄본 arXiv:2006.16841, 2020. [26] Yi Zhou, Hui Zhang, Hana Lee, Shuyang Sun, Pingjun Li, Yangguang Zhu, ByungIn Yoo, Xiaojuan Qi, 및 Jae-Joon Han, &quot;Slot-vps: 비디오 파노라마 분할을 위한 객체 중심 표현 학습&quot;, CVPR, 2022. [27] Pradyumna Reddy, Paul Guerrero 및 Niloy J Mitra, &quot;개념 검색: 직접 최적화를 사용하여 시각적 개념 발견&quot;, arXiv 사전 인쇄본 arXiv:2210.14808, 2022. [28] Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, Christoph Feichtenhofer, et al., &quot;수신하는 마스크 자동 인코더&quot;, arXiv 사전 인쇄본 arXiv:2207.06405, 2022. [29] Kaiming He, Xiangyu Zhang, Shaoqing Ren 및 Jian Sun, &quot;이미지 인식을 위한 심층 잔여 학습&quot;, CVPR, 2016. [30] Nicholas Watters, Loic Matthey, Christopher P Burgess 및 Alexander Lerchner, &quot;공간 브로드캐스트 디코더: VAES에서 얽힘이 풀린 표현을 학습하기 위한 간단한 아키텍처&quot;, arXiv 사전 인쇄본 arXiv:1901.07017, 2019. [31] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron 및 Ren Ng, &quot;푸리에 피처를 통해 네트워크가 저차원 도메인에서 고주파 함수를 학습할 수 있음&quot;, NeurIPS, 2020. [32] Diederik P Kingma 및 Jimmy Ba, &quot;Adam: 확률적 최적화 방법&quot;, arXiv 사전 인쇄본 arXiv:1412.6980, 2014. [33] Hakan Erdogan, John R. Hershey, Shinji Watanabe 및 Jonathan Le Roux, &quot;딥 순환 신경망을 사용한 위상 감지 및 인식 향상 음성 분리&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2015. [34] Vassil Panayotov, Guoguo Chen, Daniel Povey 및 Sanjeev Khudanpur, &quot;Librispeech: 퍼블릭 도메인 오디오 북을 기반으로 한 ASR 코퍼스&quot;, IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2015. [35] Yipeng Li 및 DeLiang Wang, &quot;이상적인 이진 시간-주파수 마스크의 최적성에 관하여&quot;, 음성 커뮤니케이션, 2009. [36] Jonathan Le Roux, Scott Wisdom, Hakan Erdogan 및 John R Hershey, &quot;Sdr-half-baked or well done?,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 2019. [37] Shahar Lutati, Eliya Nachmani, Lior Wolf, &quot;분리 및 확산: 소스 분리 개선을 위해 사전 훈련된 확산 모델 사용&quot;, arXiv 사전 인쇄 arXiv:2301.10752, 2023. [38] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour, &quot;Audiolm: 오디오 생성에 대한 언어 모델링 접근 방식&quot;, arXiv 사전 인쇄 arXiv:2209.03143, 2022. [39] Curtis Hawthorne, Ian Simon, Adam Roberts, Neil Zeghidour, Josh Gardner, Ethan Manilow 및 Jesse Engel, &quot;스펙트로그램 확산을 사용한 다중 악기 음악 합성,&quot; arXiv 사전 인쇄 arXiv:2206.05408, 2022. [40] Thomas Kipf, Gamaleldin F. Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy 및 Klaus Greff, &quot;비디오를 통한 조건부 개체 중심 학습&quot;, ICLR, 2022.
