--- ABSTRACT ---
Transformer 아키텍처에는 두 가지 주요 비임베딩 구성 요소가 있습니다. 주의와 피드 포워드 네트워크(FFN)입니다. 주의는 위치에 관계없이 단어 간의 상호 의존성을 포착하는 반면, FFN은 각 입력 토큰을 독립적으로 비선형적으로 변환합니다. 이 작업에서 우리는 FFN의 역할을 탐구하고, 모델 매개변수의 상당 부분을 차지함에도 불구하고 매우 중복적이라는 것을 발견했습니다. 구체적으로, 디코더 계층에서 FFN을 제거하고 인코더에서 단일 FFN을 공유함으로써 정확도가 약간만 떨어지면서 매개변수 수를 크게 줄일 수 있습니다. 마지막으로 공유 FFN의 숨겨진 차원을 늘려 이 아키텍처를 원래 크기로 축소하여 원래 Transformer Big.1에 비해 정확도와 지연 시간에서 상당한 이득을 얻었습니다.
--- INTRODUCTION ---
Transformer 아키텍처(Vaswani et al., 2017)는 기계 번역(MT)을 포함한 많은 자연어 처리(NLP) 작업에서 사실상의 패러다임이 되었습니다. 여러 연구에 따르면 Transformer는 인상적인 스케일링 법칙 속성을 보이는 것으로 나타났습니다(Gordon et al., 2021; Bansal et al., 2022; Ghorbani et al., 2022). 여기서 모델 매개변수의 수를 늘리면 정확도가 더욱 향상됩니다. 이 아키텍처의 인상적인 매개변수 수 스케일링(Chowdhery et al., 2022)과 병행하여 실제 배포를 위한 모델 풋프린트를 줄여 대기 시간 요구 사항과 메모리 및 디스크 공간 제한과 같은 실질적인 제약을 충족하려는 추세가 커지고 있습니다. 이에 따라 연구자들은 Transformer compo*Equal 기여의 차원을 줄이는 매개변수 공유(Ge et al., 2022; Takase and Kiyono, 2023; Lou et al., 2022)를 적극적으로 탐구하고 있습니다. Apple에서 수행한 작업. nents, 그리고 어텐션 헤드와 같은 가지치기 구성 요소(Voita 등, 2019; Michel 등, 2019). 토큰 간의 쌍별 종속성을 학습하는 데 있어 어텐션의 역할은 비교적 잘 이해되고 있지만(Voita 등, 2019; Clark 등, 2019; Vig and Belinkov, 2019), 피드 포워드 네트워크(FFN)의 역할은 여전히 탐구되지 않은 상태입니다. 최근 Geva 등(2021)은 FFN이 학습 가능한 키-값 쌍에 해당하며 FFN의 첫 번째 계층의 가중치는 키에 해당하고 두 번째 계층의 가중치는 값에 해당한다고 가정하여 FFN과 어텐션 간의 연결을 확립했습니다. 그들은 키가 각 계층에서 눈에 띄는 텍스트 패턴을 포착할 수 있음을 발견했으며 패턴 클래스가 인접한 계층 사이에서 겹치는 경향이 있어 표현의 중복성을 나타낸다는 것을 알아챘습니다. 이러한 관찰은 레이어당 개별 FFN을 할당하는 기존 관행을 다시 살펴보는 작업의 동기가 됩니다. MT 모델에서 여러 레이어에 걸쳐 FFN을 공유하고 삭제하는 효과를 조사합니다. 리소스가 부족한 언어 쌍과 다국어를 포함하여 다양한 언어 쌍에 걸쳐 Transformer의 다양한 구성으로 철저한 실험을 수행합니다. 또한 디코더 전용 Transformer 기반 모델에서 FFN의 효과를 조사합니다. 인코더와 디코더 FFN 사이에 상당한 수준의 중복성이 있음을 발견했습니다. 그 결과 디코더 FFN을 제거하고 모델의 정확도를 크게 떨어뜨리지 않고 인코더에서 단일 FFN을 공유할 수 있습니다. 이 단계는 상당한 매개변수 절감으로 이어질 뿐만 아니라 추가 개선의 기회도 열어줍니다. 또한 디코더의 FFN을 삭제하는 동안 인코더에서 더 넓은 FFN을 사용하여 비슷한 크기의 모델을 만들지만 정확도가 향상되고 지연 시간이 감소하는 것을 제안합니다. 마지막으로 레이어당 하나의 독립 FFN을 사용하여 원래 모델과 공유 FFN이 있는 다양한 모델 간의 표현적 유사성에 대한 세부 분석을 수행합니다. 우리의 결과는 FFN을 공유할 때 모델 정확도와 Transformer 블록의 내부 표현이 모두 안정적으로 유지됨을 보여줍니다.2 배경 및 방법론 2. Transformer Transformer 아키텍처에는 두 가지 주요 구성 요소인 주의와 FFN이 있으며, 이는 잔여 연결(He et al., 2016)과 계층 정규화(Ba et al., 2016)를 통해 연결됩니다.인코더-디코더 모델에는 자기 주의와 교차 주의라는 두 가지 유형의 주의가 있습니다.자기 주의는 인코더와 디코더 모두에서 사용되어 모델이 동일한 시퀀스 내에서 관련 정보에 집중할 수 있도록 합니다.교차 주의는 디코더에만 적용되며 인코더의 출력에 주의를 기울일 수 있도록 합니다.주의는 쿼리, 키, 값 집합을 입력으로 받으며, 4개의 Rdmodel xdmodel 행렬(쿼리, 키, 값 및 최종 출력용 행렬 하나)을 사용하여 투영합니다.여기서 dmodel은 모델의 숨겨진 차원입니다.그런 다음 SOFTMAX 함수를 적용하여 가장 관련성 있는 값에 집중할 수 있습니다. FFN은 인코더와 디코더 모두에서 어텐션 후에 적용되며 다음과 같은 2계층 선형 변환으로 구성됩니다.FFN(x) = max(0, xW1 + b₁) W2+ b2, (1) 여기서 RELU 비선형성이 입력 시퀀스(x)의 변환에 적용됩니다.각 계층에서 FFN은 두 행렬 W₁ Є Rdmodelxdff 및 W₂ € Rdf×dmodel로 매개변수화됩니다.여기서 dff는 FFN 차원이며 일반적으로 4× dmodel로 설정됩니다(Vaswani et al., 2017).최근 연구에서 어텐션과 FFN(Geva et al., 2021) 사이에 상당한 연관성이 도출되었는데, 여기서 W₁과 W2는 입력(x)이 쿼리 역할을 하는 비정규화된 어텐션에 대한 키와 값과 유사한 역할을 합니다. 일반적인 주의와 달리 FFN은 RELU를 사용하여 여러 키가 최종 출력에 크게 기여할 수 있도록 합니다(Geva et al., 2021). 또한 이러한 키는 훈련 데이터에서 학습한 눈에 띄는 패턴의 인벤토리에 해당합니다.Geva et al.(2021)은 FFN이 하위 계층에서 얕은 구문 패턴을 학습하고 더 깊은 계층에서 점진적으로 깊은 의미 패턴을 학습한다고 제안합니다.또한 저자는 인접 계층에서 캡처한 패턴 사이에 상당한 중복이 있음을 발견하여 FFN에 중복이 있음을 나타내며 이러한 매개변수를 더 잘 할당하면 성능에 도움이 될 수 있음을 시사합니다.2.2 FFN 공유 및 확장 바닐라 트랜스포머는 인코더와 디코더의 각 계층, 즉 각각 FFence 또는 FFNdec에 대해 하나의 FFN을 할당합니다.임베딩 매개변수를 제외하고 이러한 FFN은 매개변수 예산의 약 2/3를 차지하는 반면 주의는 나머지 1/3을 차지합니다¹. 이전 연구에서는 디코더 FFN의 매개변수화를 제한해도 정확도가 저하되지 않는다는 것을 발견했습니다(Ge et al., 2022). 이 연구에서는 FFN의 매개변수를 계층 및/또는 인코더와 디코더 간에 공유하여 FFN 간 중복성을 최소화합니다. Nenc, Ndec를 각각 인코더 계층과 디코더 계층의 수로 합니다. 우리는 매개변수 공유를 위해 다음과 같이 여러 구성을 고려합니다. • 전체 인코더에 대한 하나의 FFNene: 모두 연결됨 FFNene (.) FFNenc(), Vi: 1 ≤ i ≤ Nenc 모두 • 전체 디코더에 대한 하나의 FFNder: FFNdec (.) 모두 연결됨 모두 FFNdec (.), Vj: 1 ≤ j ≤ N 모두 감소 • 인코더와 디코더 모두에 대한 하나의 FFNenedec: 연결됨 FFNene (.) 연결됨 FFNdec (.) FFNencdec (.), Vi, j1i 모두 Nenc, 1 ≤ j ≤ Ndec 또한 공유 FFN의 차원을 수정하는 것을 살펴보겠습니다. 이를 def로 표시합니다. dff &gt; dff로 설정하면 공유 FFN이 넓어지고 dff&#39;
--- RELATED WORK ---
가중치 가지치기와 매개변수 공유는 모델의 발자국을 줄이는 잘 알려진 기술입니다. 최신 모델의 규모를 감안할 때(Chowdhery et al., 2022), 다양한 자동 기반 뉴런을 가지치기 위한 여러 가지 노력이 있었습니다.
--- METHOD ---
ology 2. Transformer Transformer 아키텍처에는 두 가지 주요 구성 요소인 어텐션과 FFN이 있으며, 이는 잔여 연결(He et al., 2016)과 레이어 정규화(Ba et al., 2016)를 통해 연결됩니다. 인코더디코더 모델에는 셀프 어텐션과 크로스 어텐션이라는 두 가지 유형의 어텐션이 있습니다. 셀프 어텐션은 인코더와 디코더 모두에서 사용되어 모델이 동일한 시퀀스 내에서 관련 정보에 집중할 수 있도록 합니다. 크로스 어텐션은 디코더에만 적용되며 디코더가 인코더의 출력에 주의를 기울일 수 있도록 합니다. 어텐션은 4개의 Rdmodel xdmodel 행렬(쿼리, 키, 값 및 최종 출력용 행렬 하나)을 사용하여 투영된 쿼리, 키 및 값 집합을 입력으로 받습니다. 여기서 dmodel은 모델의 숨겨진 차원입니다. 그런 다음 SOFTMAX 함수를 적용하여 가장 관련성 있는 값에 집중할 수 있습니다. FFN은 인코더와 디코더 모두에서 어텐션 후에 적용되며 다음과 같은 2계층 선형 변환으로 구성됩니다.FFN(x) = max(0, xW1 + b₁) W2+ b2, (1) 여기서 RELU 비선형성이 입력 시퀀스(x)의 변환에 적용됩니다.각 계층에서 FFN은 두 행렬 W₁ Є Rdmodelxdff 및 W₂ € Rdf×dmodel로 매개변수화됩니다.여기서 dff는 FFN 차원이며 일반적으로 4× dmodel로 설정됩니다(Vaswani et al., 2017).최근 연구에서 어텐션과 FFN(Geva et al., 2021) 사이에 상당한 연관성이 도출되었는데, 여기서 W₁과 W2는 입력(x)이 쿼리 역할을 하는 비정규화된 어텐션에 대한 키와 값과 유사한 역할을 합니다. 일반적인 주의와 달리 FFN은 RELU를 사용하여 여러 키가 최종 출력에 크게 기여할 수 있도록 합니다(Geva et al., 2021). 또한 이러한 키는 훈련 데이터에서 학습한 눈에 띄는 패턴의 인벤토리에 해당합니다.Geva et al.(2021)은 FFN이 하위 계층에서 얕은 구문 패턴을 학습하고 더 깊은 계층에서 점진적으로 깊은 의미 패턴을 학습한다고 제안합니다.또한 저자는 인접 계층에서 캡처한 패턴 사이에 상당한 중복이 있음을 발견하여 FFN에 중복이 있음을 나타내며 이러한 매개변수를 더 잘 할당하면 성능에 도움이 될 수 있음을 시사합니다.2.2 FFN 공유 및 확장 바닐라 트랜스포머는 인코더와 디코더의 각 계층, 즉 각각 FFence 또는 FFNdec에 대해 하나의 FFN을 할당합니다.임베딩 매개변수를 제외하고 이러한 FFN은 매개변수 예산의 약 2/3를 차지하는 반면 주의는 나머지 1/3을 차지합니다¹. 이전 연구에서는 디코더 FFN의 매개변수화를 제한해도 정확도가 저하되지 않는다는 것을 발견했습니다(Ge et al., 2022). 이 연구에서는 FFN의 매개변수를 계층 및/또는 인코더와 디코더 간에 공유하여 FFN 간 중복성을 최소화합니다. Nenc, Ndec를 각각 인코더 계층과 디코더 계층의 수로 합니다. 우리는 매개변수 공유를 위해 다음과 같이 여러 구성을 고려합니다. • 전체 인코더에 대한 하나의 FFNene: 모두 연결됨 FFNene (.) FFNenc(), Vi: 1 ≤ i ≤ Nenc 모두 • 전체 디코더에 대한 하나의 FFNder: FFNdec (.) 모두 연결됨 모두 FFNdec (.), Vj: 1 ≤ j ≤ N 모두 감소 • 인코더와 디코더 모두에 대한 하나의 FFNenedec: 연결됨 FFNene (.) 연결됨 FFNdec (.) FFNencdec (.), Vi, j1i 모두 Nenc, 1 ≤ j ≤ Ndec 또한 공유 FFN의 차원을 수정하는 것을 살펴보겠습니다. 이를 def로 표시합니다. dff &gt; dff로 설정하면 공유 FFN이 넓어지고 dff&#39;
--- EXPERIMENT ---
Transformer의 다른 구성, 낮은 리소스 언어 쌍 및 다국어를 포함한 다른 언어 쌍에 대해 s를 사용합니다. 또한 디코더 전용 Transformer 기반 모델에서 FFN의 효과를 조사합니다. 인코더와 디코더 FFN 사이에 상당한 수준의 중복성이 있음을 발견했습니다. 결과적으로 디코더 FFN을 제거하고 모델의 정확도를 크게 떨어뜨리지 않고 인코더에서 단일 FFN을 공유할 수 있습니다. 이 단계는 상당한 매개변수 절감으로 이어질 뿐만 아니라 추가 개선의 기회도 제공합니다. 또한 디코더의 FFN을 삭제하는 동안 인코더에서 더 넓은 FFN을 사용하여 비슷한 크기의 모델이지만 정확도가 향상되고 지연 시간이 감소하는 것을 제안합니다. 마지막으로 레이어당 하나의 독립 FFN을 사용하여 원래 모델과 공유 FFN이 있는 다양한 모델 간의 표현적 유사성에 대한 세부 분석을 수행합니다. 결과에 따르면 FFN을 공유할 때 모델 정확도와 Transformer 블록의 내부 표현이 모두 안정적으로 유지됩니다. 2 배경 및 방법론 2. Transformer Transformer 아키텍처에는 두 가지 주요 구성 요소인 어텐션과 FFN이 있으며, 이는 잔여 연결(He et al., 2016)과 레이어 정규화(Ba et al., 2016)를 통해 연결됩니다. 인코더디코더 모델에는 셀프 어텐션과 크로스 어텐션이라는 두 가지 유형의 어텐션이 있습니다. 셀프 어텐션은 인코더와 디코더 모두에서 사용되어 모델이 동일한 시퀀스 내에서 관련 정보에 집중할 수 있도록 합니다. 크로스 어텐션은 디코더에만 적용되며 디코더가 인코더의 출력에 주의를 기울일 수 있도록 합니다. 어텐션은 4개의 Rdmodel xdmodel 행렬(쿼리, 키, 값 및 최종 출력을 위한 행렬)을 사용하여 투영된 쿼리, 키 및 값 세트를 입력으로 받습니다. 여기서 dmodel은 모델의 숨겨진 차원입니다. 그런 다음 SOFTMAX 함수를 적용하여 가장 관련성 있는 값에 집중할 수 있습니다. FFN은 인코더와 디코더 모두에서 어텐션 후에 적용되며 다음과 같은 2계층 선형 변환으로 구성됩니다.FFN(x) = max(0, xW1 + b₁) W2+ b2, (1) 여기서 RELU 비선형성이 입력 시퀀스(x)의 변환에 적용됩니다.각 계층에서 FFN은 두 행렬 W₁ Є Rdmodelxdff 및 W₂ € Rdf×dmodel로 매개변수화됩니다.여기서 dff는 FFN 차원이며 일반적으로 4× dmodel로 설정됩니다(Vaswani et al., 2017).최근 연구에서 어텐션과 FFN(Geva et al., 2021) 사이에 상당한 연관성이 도출되었는데, 여기서 W₁과 W2는 입력(x)이 쿼리 역할을 하는 비정규화된 어텐션에 대한 키와 값과 유사한 역할을 합니다. 일반적인 주의와 달리 FFN은 RELU를 사용하여 여러 키가 최종 출력에 크게 기여할 수 있도록 합니다(Geva et al., 2021). 또한 이러한 키는 훈련 데이터에서 학습한 눈에 띄는 패턴의 인벤토리에 해당합니다.Geva et al.(2021)은 FFN이 하위 계층에서 얕은 구문 패턴을 학습하고 더 깊은 계층에서 점진적으로 깊은 의미 패턴을 학습한다고 제안합니다.또한 저자는 인접 계층에서 캡처한 패턴 사이에 상당한 중복이 있음을 발견하여 FFN에 중복이 있음을 나타내며 이러한 매개변수를 더 잘 할당하면 성능에 도움이 될 수 있음을 시사합니다.2.2 FFN 공유 및 확장 바닐라 트랜스포머는 인코더와 디코더의 각 계층, 즉 각각 FFence 또는 FFNdec에 대해 하나의 FFN을 할당합니다.임베딩 매개변수를 제외하고 이러한 FFN은 매개변수 예산의 약 2/3를 차지하는 반면 주의는 나머지 1/3을 차지합니다¹. 이전 연구에서는 디코더 FFN의 매개변수화를 제한해도 정확도가 저하되지 않는다는 것을 발견했습니다(Ge et al., 2022). 이 연구에서는 FFN의 매개변수를 계층 및/또는 인코더와 디코더 간에 공유하여 FFN 간 중복성을 최소화합니다. Nenc, Ndec를 각각 인코더 계층과 디코더 계층의 수로 합니다. 우리는 매개변수 공유를 위해 다음과 같이 여러 구성을 고려합니다. • 전체 인코더에 대한 하나의 FFNene: 모두 연결됨 FFNene (.) FFNenc(), Vi: 1 ≤ i ≤ Nenc 모두 • 전체 디코더에 대한 하나의 FFNder: FFNdec (.) 모두 연결됨 모두 FFNdec (.), Vj: 1 ≤ j ≤ N 모두 감소 • 인코더와 디코더 모두에 대한 하나의 FFNenedec: 연결됨 FFNene (.) 연결됨 FFNdec (.) FFNencdec (.), Vi, j1i 모두 Nenc, 1 ≤ j ≤ Ndec 또한 공유 FFN의 차원을 수정하는 것을 살펴보겠습니다. 이를 def로 표시합니다. dff &gt; dff로 설정하면 공유 FFN이 넓어지고 dff&#39;
--- CONCLUSION ---
이 작업에서 우리는 Transformer 모델에서 FFN의 중요성을 연구했습니다. 레이어 간에 FFN을 제거 및/또는 공유하는 것의 영향을 분석한 결과, 이 구성 요소의 중복성으로 인해 기계 번역의 정확도에 거의 영향을 미치지 않고 모델 크기를 상당히 줄일 수 있음을 발견했습니다. 특히, 모든 인코더 레이어에서 FFN을 공유하면서 더 크게 만들고 디코더 레이어에서 제거하면 추론에서 더 정확하고 빠른 모델이 생성됨을 발견했습니다. 우리의 연구 결과는 디코더 전용 모델과 다국어 모델을 포함한 여러 설정에 적용할 수 있습니다. 리소스가 부족한 설정에서는 결과가 적당하지만 우리의 접근 방식은 더 빠른 추론으로 기준선의 성능을 여전히 회복할 수 있습니다. 마지막으로, vanilla Transformer와 제안된 아키텍처 간에 철저한 유사성 분석을 수행한 결과, 후자의 내부 표현이 덜 중복된다는 점을 제외하면 전자와 크게 다르지 않음을 발견했습니다. 제한 사항 이 작업에서 우리의 초점은 기계 번역이었습니다. 결과가 다른 시퀀스 대 시퀀스 작업으로 일반화될 것으로 예상하지만 추가 실험이 필요하므로 향후 작업으로 미루겠습니다. 윤리적 성명 중요한 고려 사항 중 하나는 모델 학습에 대한 에너지 소비로, 이는 온실 가스 배출을 초래합니다(Strubell et al., 2019). 저희의 작업은 기존 데이터 세트를 사용하며, 개인정보 유출(Carlini et al., 2021) 및 성적 편견(Cho et al., 2019)과 같은 일부 위험을 물려받습니다. Vanmassenhove et al.(2018)과 같은 완화 전략이 필요할 수 있습니다. 감사의 말 이 작업을 검토하는 데 피드백과 지원을 해주신 Robin Schmidt, Matthias Sperber, Stephan Peitz에게 감사드립니다. 참고문헌 Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton. 2016. Layer normalization. arXiv 사전 인쇄본 arXiv:1607.06450. Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Colin Cherry, Behnam Neyshabur, and Orhan Firat. 2022. NMT의 데이터 스케일링 법칙: 노이즈와 아키텍처의 효과. 제39회 기계 학습 국제 컨퍼런스 논문집, 기계 학습 연구 논문집 162권, 1466-1482쪽. PMLR. Angie Boggust, Brandon Carter, and Arvind Satyanarayan. 2022. 임베딩 비교자: 소규모 다중을 통한 글로벌 구조와 지역적 이웃의 차이점 시각화. 제27회 지능형 사용자 인터페이스 국제 컨퍼런스, 746766쪽, 헬싱키 핀란드. ACM. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), 2633-2650쪽. USENIX Association. Won Ik Cho, Ji Won Kim, Seok Min Kim, and Nam Soo Kim. 2019. On Measuring gender bias in translation of gender-neutral pronouns. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing, 173-181쪽, Florence, Italy. Association for Computational Linguistics. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason 웨이, 캐시 마이어-헬스턴, 더글러스 에크, 제프 딘, 슬라브 페트로프, 노아 피델. 2022. Palm: 경로를 통한 언어 모델링 확장. 케빈 클라크, 우르바시 칸델왈, 오머 레비, 크리스토퍼 D. 매닝. 2019. BERT는 무엇을 보나요? BERT의 관심 분석. 2019년 ACL 워크숍 BlackboxNLP: NLP를 위한 신경망 분석 및 해석, 276-286쪽, 이탈리아 피렌체. Association for Computational Linguistics. Fahim Dalvi, Hassan Sajjad, Nadir Durrani, Yonatan Belinkov. 2020. 사전 학습된 변환기 모델에서 중복성 분석. 2020년 자연어 처리 경험적 방법(EMNLP) 컨퍼런스의 회의록, 4908-4926쪽, 온라인. Association for Computational Linguistics. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser. 2019. 범용 변환기. International Conference on Learning Representations. Tao Ge, Si-Qing Chen, Furu Wei. 2022. EdgeFormer: 온디바이스 seq2seq 생성을 위한 매개변수 효율적 변환기. 2022 자연어 처리 경험적 방법 컨퍼런스의 회의록, 10786-10798페이지, 아랍에미리트 아부다비. 전산 언어학 협회. Mor Geva, Roei Schuster, Jonathan Berant, Omer Levy. 2021. 변환기 피드포워드 레이어는 키값 메모리입니다. 2021 자연어 처리 경험적 방법 컨퍼런스의 회의록, 5484-5495페이지, 온라인 및 도미니카 공화국 푼타카나. 전산 언어학 협회. Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, Colin Cherry. 2022. 신경 기계 번역을 위한 스케일링 법칙. 국제 학습 표현 컨퍼런스에서. Mitchell A Gordon, Kevin Duh, Jared Kaplan. 2021. 신경 기계 번역을 위한 데이터 및 매개변수 스케일링 법칙. 2021 자연어 처리 경험적 방법 컨퍼런스 회의록, 5915-5922쪽, 온라인 및 도미니카 공화국 푼타카나. 계산 언어학 협회. Arthur Gretton, Olivier Bousquet, Alex Smola, Bernhard Schölkopf. 2005. 힐버트-슈미트 규범을 사용한 통계적 종속성 측정. 알고리즘 학습 이론 국제 컨퍼런스에서, 63-77쪽. Springer. William L. Hamilton, Jure Leskovec, Dan Jurafsky. 2016. 문화적 변화 또는 언어적 표류? 의미적 변화에 대한 두 가지 계산적 측정 비교. 2016년 자연어 처리 경험적 방법 컨퍼런스 회의록, 2116-2121페이지, 텍사스 오스틴. Association for Computational Linguistics. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 2016. 이미지 인식을 위한 심층 잔여 학습. 2016년 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 770-778페이지. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2022. LORA: 대규모 언어 모델의 저순위 적응. International Conference on Learning Representations에서. Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, Noah A. Smith. 2021. 심층 인코더, 얕은 디코더: 비자기 회귀 기계 번역 재평가. 9th International Conference on Learning Representations, ICLR, virtual. OpenReview.net. Diederik P. Kingma와 Jimmy Ba. 2015. Adam: 확률적 최적화를 위한 방법. 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, 2015년 5월 7-9일, Conference Track Proceedings. Simon Kornblith, Mohammad Norouzi, Honglak Lee, Geoffrey Hinton. 2019. Similarity of neural network representations revisited. 36th International Conference on Machine Learning의 회의록, Proceedings of Machine Learning Research의 97권, 3519-3529쪽. PMLR. Taku Kudo와 John Richardson. 2018. SentencePiece: 신경 텍스트 처리를 위한 간단하고 언어에 독립적인 하위 단어 토크나이저 및 디토크나이저. 2018년 자연어 처리 경험적 방법에 대한 컨퍼런스의 회의록: 시스템 시범, 66-71페이지, 벨기에 브뤼셀. 계산 언어학 협회. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. 2020. Albert: 언어 표현의 자기 지도 학습을 위한 가벼운 bert. 국제 학습 표현 컨퍼런스에서. Qian Lou, Ting Hua, Yen-Chang Hsu, Yilin Shen, Hongxia Jin. 2022. Dictformer: 공유 사전이 있는 작은 변환기. 국제 학습 표현 컨퍼런스에서. Paul Michel, Omer Levy, Graham Neubig. 2019. 열여섯 개의 머리가 정말로 하나보다 나을까요? Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: 시퀀스 모델링을 위한 빠르고 확장 가능한 툴킷. 2019년 북미 컴퓨터 언어학 협회(Demonstrations) 컨퍼런스 회의록, 48-53쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학 협회. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. 신경 기계 번역 확장. 제3차 기계 번역 컨퍼런스 회의록: 연구 논문, 1-9쪽, 벨기에 브뤼셀. 컴퓨터 언어학 협회. Telmo Pires, Robin Schmidt, Yi-Hsiu Liao, Stephan Peitz. 2023. 다국어 기계 번역을 위한 언어별 레이어 학습. Association for Computational Linguistics의 제61회 연례 회의록(제1권: 장문 논문), 14767-14783쪽, 캐나다 토론토. Association for Computational Linguistics. Ofir Press와 Lior Wolf. 2017. 언어 모델을 개선하기 위한 출력 임베딩 사용. Association for Computational Linguistics의 유럽 지부 제15차 회의록: 제2권, 단편 논문, 157-163쪽, 스페인 발렌시아. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구. Journal of Machine Learning Research, 21(140):1-67. Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo. 2021. Subformer: 생성 변환기에서 매개변수 효율성을 위한 가중치 공유 탐구. Findings of the Association for Computational Linguistics: EMNLP 2021, 4081-4090페이지, 푼타카나, 도미니카 공화국. Association for Computational Linguistics. Robin Schmidt, Telmo Pires, Stephan Peitz, Jonas Lööf. 2022. 비자기회귀 신경 기계 번역: 명확성에 대한 요구. 2022 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 2785-2799페이지, 아부다비, 아랍에미리트. Association for Computational Linguistics. Rico Sennrich, Barry Haddow, Alexandra Birch. 2016. 하위 단어 단위가 있는 희귀 단어의 신경 기계 번역. Association for Computational Linguistics 제54회 연례 회의록(제1권: 장문 논문), 1715-1725쪽, 베를린, 독일. Association for Computational Linguistics. Emma Strubell, Ananya Ganesh, Andrew McCallum. 2019. NLP에서 딥 러닝을 위한 에너지 및 정책 고려 사항. Association for Computational Linguistics 제57회 연례 회의록, 3645-3650쪽, 피렌체, 이탈리아. Association for Computational Linguistics. Sho Takase, Shun Kiyono. 2023. 변압기에서 계층 간 매개변수 공유에 대한 수업. The Fourth Workshop on Simple and Efficient Natural Language Processing(SustaiNLP)의 회의록, 78-90페이지, 토론토, 캐나다(하이브리드). Association for Computational Linguistics. Eva Vanmassenhove, Christian Hardmeier, and Andy Way. 2018. Getting gender right in neural machine translation. 2018 Conference on Empirical Methods in Natural Language Processing의 회의록, 3003-3008페이지, 브뤼셀, 벨기에. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Jesse Vig and Yonatan Belinkov. 2019. 트랜스포머 언어 모델에서 어텐션 구조 분석. 2019 ACL 워크숍 BlackboxNLP의 회의록: NLP를 위한 신경망 분석 및 해석, 63-76페이지, 이탈리아 피렌체. 계산 언어학 협회. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov. 2019. 다중 헤드 자기 어텐션 분석: 전문화된 헤드가 힘든 일을 하고 나머지는 정리할 수 있음. 계산 언어학 협회의 제57회 연례 회의록, 5797-5808페이지, 이탈리아 피렌체. 계산 언어학 협회. 여러 FFN의 사용자 지정 공유 N개 레이어의 모듈 내에서 MN FFN을 공유하는 방법은 조합적인 수가 있습니다. 이것이 금지되어 있으므로 Takase와 Kiyono(2023)의 다음 전략을 조사합니다.• 순서: M/N 연속 레이어마다 하나의 FFN을 지정하여 블록 패턴을 형성합니다.tie FFN¿(•) FFN seqm (•), Vi: 1 ≤ i ≤ N, m = [(i − 1)/(N/M)] • 주기: M개의 FFN을 동일한 순서로 쌓아 반복적인 체커보드 패턴을 형성합니다.• tie FFN¿(•) FFN cycm (·), Vi: 1 ≤ i ≤ N, m = (i-1) 모듈로 M 주기(Rev): M개의 FFN을 역순으로 쌓아 반복적인 팰린드롬 급수를 형성합니다.tie FFNC cycrevm (•), Vi FFN¿(•) N,m N/Mi = : 1 ≤ i ≤ N이 짝수이고 N으로 나누어 떨어진다고 가정합니다.주기(Rev)는 M = N/2에만 유효합니다. EdgeFormer(Ge et al., 2022)는 인코더 FFN에 대해 M = 2인 Cycle을 채택합니다. 표 7은 인코더에 적용된 이러한 전략의 결과를 보여줍니다. 참고로, 표 2에서 Transformer Big 및 ShareEnc의 결과를 복사합니다. ShareEnc의 정확도는 Takase 및 Kiyono(2023)의 전략과 유사할 뿐만 아니라 매개변수가 적고 확장하기가 더 쉽습니다. 아키텍처 Transformer Big + SharedEnc(M=1) BLEU || (%) 35.6 228M(100) 35.4 186M(82) + 시퀀스 M=35.2 194M(85) + 시퀀스 M=+ 사이클 M=35.3 202M(88) 35.2 194M(85) + 사이클 M=35.5 202M(88) + 사이클 회전 M=+ 사이클 회전 M=35.2 194M(85) 35.5 202M(88) 표 7: WMT22에서 다양한 FFN 공유 전략의 정확도 EN → DE. B 주의 공유 또는 삭제 표 8에서는 계층 간에 주의 모듈(자기, 교차 또는 둘 다)을 공유한 결과를 보고합니다.FFN과 대조적으로 주의는 모델의 성능에서 더 중요한 역할을 하는 것으로 보이는데, 인코더와 디코더 모두에서 다른 주의 메커니즘을 공유하면 디코더의 교차 주의와 인코더의 자기 주의를 공유하는 경우를 제외하고 모든 설정에서 정확도가 크게 떨어지기 때문입니다.디코더 인코더 자기 주의 자기 주의 교차 주의 BLEU || (%) 변압기 큰 공유 공유 공유 공유 공유 개별 공유 개별 개별 35.6 228M(100) 27.5 165M(72) 27.6 186M(82) 35.5 207M(91) 개별 공유 개별 26.5 207M(91) 개별 공유 공유 개별 공유 25.7 186M(82) 35.5 207M(91) 표 8: 인코더와 디코더(셀프 및 크로스)의 어텐션을 공유할 때 WMT 22 EN →DE에서 BLEU 점수. 명명법은 섹션 3을 따르지만 셀프 어텐션과 크로스 어텐션을 각각 인코더/디코더의 셀프 어텐션과 크로스 어텐션(디코더)으로 사용합니다. 이 논문에서 평가하는 모든 모델에서 유사한 패턴을 관찰합니다. C 내부 표현 분석에 대한 세부 정보 C.1 벤치마킹을 위한 원시 유사도 점수 서로 다른 랜덤 시드로 학습된 동일한 모델과 기준 트랜스포머 빅을 비교하여 두 메트릭의 예상 유사도에 대한 벤치마크 점수를 설정합니다. 표는 표 5에 제시된 정규화된 점수를 계산하는 데 사용된 원시 유사도 점수를 나타냅니다. 표시된 대로 Encoder Decoder SN&#39;T 0.0.0.0.40.30.20.Model Shared Encoder + Shared Decoder Transformer Big Benchmark Architecture 0.CKA LNS CKA LNS TransformerBig Seed..61 .94 .TransformerBig Seed..62 .95 .Layer.Module (a) Encoder SharedEnc .94 .58 .95 .Model Shared Encoder + Shared Decoder Transformer Big Benchmark SharedDec .97 .62 .93 .0.SharedEncSharedDec .95 .59 .94 .0.SharedEncDec .94 .57 .93 .NoEnc .87 .43.95 .0.NoDec 간의 유사도 .96 .60 .90 .0.ShareEncNoDec SN&#39;T .94 .59 .92 .ShareEncNoDecd=41952 .94 .51 .89 .표 9: 다양한 아키텍처의 해당 계층 모듈 표현의 원시 유사도 대 WMT22 EN →DE에 대한 Transformer Big. NoDec 구성의 경우 서로 다른 하위 모듈이 있으므로 Transformer 계층의 최종 출력을 전체적으로 비교합니다. 공유 및 삭제된 FFN의 열은 각각 회색과 파란색으로 강조 표시됩니다. C.2 계층별 분석 표 5에서는 Transformer 인코더 및 디코더의 모든 계층에서 집계된 유사도 점수를 보고합니다. 여기서는 주로 집계된 점수의 신뢰성을 보여주기 위해 보다 세부적인 계층별 유사도 점수를 보고합니다. 그림 2에서는 계층별 LNS를 플로팅하여 각 계층에서 캡처된 의미 정보가 모든 계층의 기준 모델과 얼마나 유사한지 연구합니다. LNS 점수가 높으면 네트워크는 평가 세트의 각 문장에 대해 유사한 로컬 이웃을 생성합니다. 특히, 우리는 벤치마크 LNS 점수와 각 계층에서 SharedEncSharedDec의 점수를 비교하는 데 관심이 있습니다. 표시된 대로 Shared EncSharedDec의 계층별 LNS 점수는 거의 모든 계층에서 기준 점수를 추적하여 집계된 점수의 신뢰성을 확인합니다. 그림 0.0.0.0.0.cross-attnLayer.Module (b) 디코더 2: SharedEncSharedDec과 Transformer Big 간의 계층별 LNS(파란색 막대). 서로 다른 임의 초기화에서 학습된 두 버전의 Transformer Big 간의 LNS는 비교의 근거를 위해 회색 막대로 표시됩니다. FFN 공유는 각 계층에서 생성된 활성화를 크게 변경하지 않습니다. D 디코딩 속도에 대한 배치 크기의 영향 섹션 4.3에서 배치 크기가 1인 One Wide FFN 모델과 Transformer Big의 디코딩 속도를 비교했습니다. 표 10에서 배치 크기가 증가함에 따라 디코딩 속도가 어떻게 변화하는지 살펴봅니다. 표시된 대로 One Wide FFN 모델은 작은 배치 크기에서는 더 빠르지만 배치 크기가 커질수록 이점이 줄어들어 큰 배치 크기에서는 Transformer Big보다 느립니다. 이 속도 저하가 |Batch | Transformer Big One Wide FFN 속도 향상(%) 배치 수 110.8±1.137.5±1.2,221.7±14.260.9±6.1,397.4±8.448.9±2.718.3±8.748.7±10.1,220.7±56.1,226.9±17.1,958.5±112.1, 837.6±15.-64 1,319.1±36.1,259.0±70.1,925.1±64.256 2,312.1±67.1,705.0±62.-1,976.5±123.-2,512.050.1,957.9±32.-표 10: 디코딩 속도에 대한 배치 크기의 영향 (토큰/초) Transformer Big 및 One Wide FFN(dff = 49, 152)의 경우. A는 추론 속도의 백분율 변화이고, # 배치는 평가에 사용된 배치 수입니다. 배치 크기가 큰 경우 배치가 적습니다(데이터 세트 크기가 고정되어 있기 때문). 이는 측정의 분산이 더 커지게 됩니다. FFN 크기가 클수록 피크 메모리가 더 많이 필요하므로 이 모델에 더 큰 크기는 최적이 아닙니다.
