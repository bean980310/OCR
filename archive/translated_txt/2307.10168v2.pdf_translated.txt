--- ABSTRACT ---
LLM은 이전에는 인간에게만 해당되는 것으로 여겨졌던 크라우드소싱 작업에서 인간과 유사한 행동을 복제하는 데 유망함을 보였습니다. 그러나 현재의 노력은 주로 간단한 원자 작업에 집중합니다. LLM이 더 복잡한 크라우드소싱 파이프라인을 복제할 수 있는지 살펴보겠습니다. 우리는 현대 LLM이 이러한 &quot;인간 계산 알고리즘&quot;에서 크라우드워커의 일부 능력을 시뮬레이션할 수 있다는 것을 발견했지만, 성공 수준은 가변적이며 요청자의 LLM 역량에 대한 이해, 하위 작업에 필요한 특정 기술, 이러한 하위 작업을 수행하기 위한 최적의 상호 작용 양식에 따라 영향을 받습니다. 우리는 인간과 LLM의 지시에 대한 민감도의 차이를 반영하고, LLM에 대한 인간 대면 보호 장치를 활성화하는 것의 중요성을 강조하며, 보완적인 기술 세트로 인간과 LLM을 교육하는 잠재력에 대해 논의합니다. 중요한 점은 크라우드소싱 파이프라인을 복제하면 (1) 하위 작업에 대한 성과를 교차 비교하여 다양한 작업에 대한 LLM의 상대적 강점과 (2) 복잡한 작업에서 LLM의 잠재력을 조사할 수 있는 귀중한 플랫폼을 제공한다는 것을 보여줍니다. 여기서 LLM은 작업의 일부를 완료하고 다른 작업은 인간에게 맡길 수 있습니다. 1
--- INTRODUCTION ---
AI 시스템의 급속한 발전은 기계의 기능에 대한 우리의 이해에 혁명을 일으켰습니다. 이 분야에서 주목할 만한 획기적인 진전 중 하나는 대규모 언어 모델(LLM, 예: ChatGPT)의 등장입니다. 광범위한 사전 학습(Brown et al., 2020)과 * 이 작업은 CMU 05499/899: 인간 중심 NLP의 과제를 기반으로 합니다. Tongshuang Wu는 Haiyi Zhu와의 논의를 기반으로 과제를 설계하고 대부분의 논문을 쓴 과정 강사입니다. 나머지 저자는 알파벳순으로 나열된 학생들입니다. 참여 연구의 원칙에 따라 모든 학생에게 연구에 대해 알리고 참여를 선택했으며 초기 실험과 논문 초안 검토 후 참여/저작을 중단할 수 있는 기회가 주어졌습니다. 크라우드 파이프라인 LLM 체인 그림 1: LLM을 사용하여 크라우드소싱 파이프라인을 복제하고 특정 고급 &quot;인간-계산 프로세스&quot;에서 인간 근로자를 대체할 수 있는지 연구합니다. 지침 튜닝(Stiennon 등, 2020; Wu 등, 2023; Ziegler 등, 2019) 덕분에 LLM은 이제 방대한 양의 세계 지식을 보유할 뿐만 아니라 지침을 따르기만 하면 이 지식을 효과적으로 활용하여 다양한 작업을 수행할 수 있습니다. 다양한 연구에 따르면 이러한 모델은 어느 정도 인간과 유사한 행동을 복제할 수 있으며, 이는 AI 모델 훈련의 핵심 목표입니다(Wang 등, 2022a; Bubeck 등, 2023). 특히 이러한 연구의 상당 부분은 크라우드소싱 작업을 복제하는 데 LLM을 사용해 왔는데, 이는 이전에는 인간의 계산 능력에만 국한되었다고 여겨졌던 광범위한 작업을 나타내기 때문일 수 있습니다(Bernstein, 2013). 예를 들어, LLM은 크라우드워커 또는 전문가에 비해 더 낮은 비용으로 더 높은 품질의 주석을 생성할 수 있으며(Gilardi et al., 2023; Törnberg, 2023), 주관적 작업에서 인간의 의견을 근사화하여 크라우드소싱 설문지와 인터뷰에 대한 시뮬레이션된 인간 응답을 허용할 수 있습니다(Hämäläinen et al., 2023; Argyle et al., 2022). 이러한 관찰 결과는 LLM이 상당한 사회적 및 경제적 영향을 미쳐 특정 인간 작업을 대체함으로써 인력을 재편할 가능성이 있음을 시사합니다(Eloundou et al., 2023). 사실, 일부 연구에 따르면 크라우드워커는 이제 텍스트 생산 작업을 완료하기 위해 LLM에 의존하는 경향이 있습니다(Veselovsky et al., 2023). 그러나 대부분의 기존 노력은 간단하고 자립적이며 단 한 명의 크라우드워커가 짧은 시간 내에 완료하기 쉬운 원자적 작업에 초점을 맞추는 경향이 있습니다. 이는 인간의 계산 능력의 가장 기본적인 버전입니다. 이러한 노력은 또한 다양한 작업과 도메인에 분산되어 있어 LLM이 어떤 작업에서 뛰어나거나 못 미치는지, 그리고 특정 작업에서 인간을 어느 정도 시뮬레이션, 대체 또는 증강할 수 있는지 체계적으로 비교하고 이해하기 어렵게 만듭니다. 이러한 강조는 LLM 재현성이 얼마나 일반화되는지에 대한 질문을 던집니다. 그것들은 &quot;인간 계산&quot;의 더 진보된 형식에서 유용할까요? 우리는 LLM이 인간 계산을 활용하는 데 있어 더 정교한 접근 방식을 나타내는 크라우드소싱 파이프라인을 복제하는 데 사용될 수 있는지에 특히 관심이 있습니다(Little et al., 2010). 일반적인 파이프라인에서 복잡한 작업은 독립적으로 수행될 수 있는 부분(하위 작업)으로 나뉜 다음 나중에 결합됩니다(Chilton et al., 2013; Kim et al., 2017; Law and Zhang, 2011; Retelny et al., 2017). 이 방법은 크라우드소싱의 유용성을 확장하는 데 널리 사용되어 제한된 수준의 헌신과 알려지지 않은 전문 지식을 가진 개별 크라우드워커에게 너무 어려운 작업(예: 긴 소설 요약, 소프트웨어 개발 또는 심하게 흐릿한 텍스트 해독; Kittur et al., 2011)을 처리할 수 있습니다. 흥미롭게도 LLMS에 대한 연구에서는 체이닝을 통해 더 복잡한 작업에 대한 기능을 확장하는 것도 탐구했습니다. 이름은 다르지만 LLM 체인과 크라우드소싱 파이프라인은 유사한 동기를 공유합니다. 그리고 LLM 유틸리티를 확장하는 전략. 이전 연구에서는 두 가지를 연결하여 서로 다른 문제를 해결하기 위해 작업을 분해한다는 점을 지적했습니다(Wu et al., 2022b): 크라우드소싱 파이프라인은 인지 부하 및 작업 기간과 같은 인간 근로자 성과에 영향을 미치는 요인에 초점을 맞추는 반면, LLM 체인은 신속한 효과성의 높은 분산과 같은 LLM의 고유한 한계를 해결합니다. 그러나 LLM은 이제 지시를 따르고 복잡한 맥락을 처리하는 데 있어 인간과 더 잘 일치하도록 훈련되었기 때문에(Ouyang et al., 2022), 인간과 LLM 근로자가 동일한 작업 분할 전략을 채택할 수 있습니다. 이 연구에서 우리는 LLM이 고급 인간 계산 프로세스에서 인간 근로자를 대체할 수 있는 잠재력을 조사합니다. 이를 달성하기 위해 우리는 카네기 멜론 대학교의 인간 중심 NLP라는 특별 주제 과정에 대한 과정 과제를 설계했습니다. 이 과제에서 20명의 학생은 이전 작업에 묘사된 크라우드소싱 파이프라인 중 하나(7개 중)를 선택하고 LLM을 사용하여 각 하위 작업을 처리하여 이를 복제하는 과제를 받았습니다. 복제 연구는 또한 흥미로운 보너스 분석 포인트를 제공합니다. 체인의 LLM 모듈은 고유한 하위 작업을 수행하지만 모든 하위 작업은 동일한 애플리케이션 도메인에서 발생합니다(예: 동일한 문서를 다른 방식으로 처리). 따라서 다른 하위 작업에서 LLM의 성과를 비교하고 상대적인 강점과 약점을 발견하는 것이 더 공정합니다. LM이 크라우드소싱 파이프라인을 복제할 수 있는 것처럼 보이지만, 어떤 부분에서 잘 수행하는 경향이 있는지/인간에게 기대하는 방식으로 수행하는 경향이 있는지에 큰 차이가 있습니다(표 2b의 주요 결과). 이러한 차이점은 두 가지 주요 이유에서 나타납니다. 첫째, LLM과 인간은 지시에 다르게 반응합니다. LLM은 &quot;더 나은&quot; 또는 &quot;더 다양한&quot;과 같은 형용사와 비교 기반 지침에 더 잘 반응하는 반면, 인간은 상쇄 기준과 관련된 지침을 더 잘 처리합니다. 둘째, 인간은 의견 불일치 해결 메커니즘과 인터페이스 강화 상호 작용을 통해 더 많은 스캐폴드를 받아 LLM에서 사용할 수 없는 출력 품질과 구조에 대한 가드레일을 사용할 수 있습니다. 이러한 관찰 결과는 모호하거나 불완전한 지침을 더 잘 처리하기 위해 LLM 지침 튜닝을 개선해야 할 필요성과 LLM 미세 조정 또는 실제 사용 중에 비텍스트 &quot;지침&quot;을 어떻게 사용할 수 있는지 고려해야 할 필요성을 강조합니다. 게다가 복제된 LLM 체인의 효과는 LLM 강점에 대한 학생의 인식에 따라 달라지므로 보조적 프롬프트에 대한 추가 조사가 필요합니다. LLM과 크라우드워커 간의 차이점에 대한 즉각적인 통찰력을 제공하는 것 외에도, 우리의 연구는 크라우드소싱 파이프라인을 복제하는 것이 더 광범위한 작업에서 LLM의 부분적 효과에 대한 향후 조사를 위한 귀중한 플랫폼 역할을 한다는 것을 보여줍니다. LLM이 복잡한 작업 전체를 다루기를 기대하는 대신, LLM이 지속적으로 인간과 동등한 성과를 내는 특정 하위 작업을 평가하고 식별할 수 있습니다. 그런 다음 이 증거를 활용하여 LLM과 인간 근로자 간에 하위 작업을 분배하여 책임 할당을 최적화할 수 있습니다. https://github.com/tongshuangwu/11m-crowdsourcing-pipeline에서 프롬프트 체인, 출력 및 평가를 오픈소스로 제공합니다. 2 배경 및
--- RELATED WORK ---
크라우드소싱은 인간의 입력이 필요한 문제를 규모에 맞게 해결하는 데 도움이 됩니다(Howe et al., 2006). 특히 AI 역량이 제한적이었던 초기에 크라우드소싱은 인간이 가진 고유한 계산 능력을 활용하고 향상시키는 유망한 접근 방식으로 여겨졌습니다. 크라우드소싱 연구의 주요 초점은 점점 더 복잡해지는 크라우드소싱 목표를 해결하기 위한 파이프라인 개발이었습니다(Kittur et al., 2011). 신중한 작업 분해를 통해 크라우드소싱 파이프라인은 인간 근로자로부터 전략적으로 입력을 수집하여 강점을 활용하고 한계를 완화합니다. 이러한 업적은 기존 크라우드소싱 디자인에서는 달성하기 어렵거나 불가능합니다. 예를 들어 Bernstein et al.(Bernstein et al., 2010)은 크라우드워커의 노력의 분산을 줄이기 위해 하위 작업의 범위를 조절하는 Find-Fix-Verify 워크플로를 통해 텍스트 편집 품질을 보장했습니다. 한편, 컨텍스트 트리(Verroios 및 Bernstein, 2014)는 압도적인 글로벌 컨텍스트를 계층적으로 요약하고 트리밍하여 단일 작업자가 소화할 수 있을 만큼 컴팩트하게 만듭니다. 정교한 설계로 인해 크라우드소싱 파이프라인은 종종 인간 계산 알고리즘 또는 크라우드 알고리즘(Howe et al., 2006; Law 및 Zhang, 2011; Kittur et al., 2011; Little et al., 2010)이라고 합니다. 완전히 별개의 분야(NLP)에서 등장했지만 LLM 체인은 한 번에 수행하기 어려운 복잡한 작업을 완료하기 위해 크라우드소싱 파이프라인과 유사한 목표를 공유합니다. 이 분해는 명시적 또는 암묵적 형태를 취할 수 있습니다. 예를 들어, Chain-ofThought(Kojima et al., 2022; Wei et al., 2022)는 &quot;이것을 단계별로 고려해 보자&quot;와 같은 프롬프트를 사용하여 LLM이 미리 정의되지 않은 하위 작업을 해결하도록 하는 반면, AI Chains(Wu et al., 2022b)와 Decomposed Prompting(Khot et al., 2022)은 하위 작업을 명시적으로 정의하고 각 하위 작업에 대해 별도의 프롬프트를 사용합니다. 최근에는 LangChain(Chase)과 같은 오픈소스 라이브러리와 PromptChainer(Wu et al., 2022a; noa)와 같은 서비스를 통해 실무자는 복잡한 구성성을 포함하는 작업을 처리하기 위한 LLM 체인을 만들 수 있게 되었습니다. 섹션 1에서 검토한 대로, Wu et al.(2022b)은 LLM 체이닝과 크라우드소싱 파이프라인 사이에 명확한 연결을 그렸습니다. 유사한 동기 외에도 이 두 가지
--- METHOD ---
크라우드소싱 유용성을 확장하는 데 널리 사용되어 제한된 수준의 헌신과 알려지지 않은 전문 지식을 가진 개별 크라우드워커에게는 너무 어려운 작업(예: 긴 소설 요약, 소프트웨어 개발 또는 심하게 흐릿한 텍스트 해독; Kittur et al., 2011)을 처리할 수 있습니다. 흥미롭게도 LLMS에 대한 연구에서는 체이닝을 통해 더 복잡한 작업에 대한 기능을 확장하는 방법도 탐구했습니다. LLM 체인과 크라우드소싱 파이프라인은 이름이 다르지만 LLM 유용성을 확장하려는 유사한 동기와 전략을 공유합니다. 이전 연구에서는 두 가지를 연결하여 작업을 분해하여 서로 다른 문제를 해결한다고 언급했습니다(Wu et al., 2022b): 크라우드소싱 파이프라인은 인지 부하 및 작업 기간과 같은 인간 근로자 성과에 영향을 미치는 요인에 초점을 맞추는 반면 LLM 체인은 신속한 효과의 높은 분산과 같은 LLM의 고유한 한계를 해결합니다. 그러나 LLM은 이제 지시를 따르고 복잡한 맥락을 처리하는 데 있어 인간과 더 잘 일치하도록 훈련되었기 때문에(Ouyang et al., 2022) 인간과 LLM 근로자가 동일한 작업 분할 전략을 채택할 수 있습니다. 이 연구에서 우리는 LLM이 고급 인간 계산 프로세스에서 인간 근로자를 대체할 수 있는 잠재력을 조사합니다. 이를 달성하기 위해 우리는 카네기 멜론 대학교에서 인간 중심 NLP라는 특별 주제 과정을 위한 과정 과제를 설계했습니다. 이 과제에서 20명의 학생은 이전 작업에 묘사된 크라우드소싱 파이프라인 중 하나(7개 중)를 선택하고 LLM을 사용하여 각 하위 작업을 처리하여 이를 복제하는 과제를 받았습니다. 복제 연구는 또한 흥미로운 보너스 분석 포인트를 제공합니다. 체인의 LLM 모듈은 고유한 하위 작업을 수행하지만 모든 하위 작업은 동일한 애플리케이션 도메인에서 발생합니다(예: 동일한 문서를 다른 방식으로 처리). 따라서 다른 하위 작업에서 LLM의 성과를 비교하고 상대적인 강점과 약점을 발견하는 것이 더 공정합니다. LM이 크라우드소싱 파이프라인을 복제할 수 있는 것처럼 보이지만, 어떤 부분에서는 잘 수행하거나 인간에게 기대하는 방식으로 수행하는 경향이 있는지에 큰 차이가 있습니다(표 2b의 주요 결과). 이러한 차이는 두 가지 주요 이유에서 비롯됩니다. 첫째, LLM과 인간은 지시에 다르게 반응합니다. LLM은 &quot;더 나은&quot; 또는 &quot;더 다양한&quot;과 같은 형용사와 비교 기반 지시에 더 잘 반응하는 반면, 인간은 상쇄 기준이 포함된 지시를 더 잘 처리합니다. 둘째, 인간은 의견 불일치 해결 메커니즘과 인터페이스 강화 상호 작용을 통해 더 많은 스캐폴드를 받아 LLM이 사용할 수 없는 출력 품질과 구조에 대한 보호책을 마련합니다. 이러한 관찰 결과는 모호하거나 불완전한 지시를 더 잘 처리하기 위해 LLM 지시 튜닝을 개선해야 할 필요성과 LLM 미세 조정 또는 실제 사용 중에 비텍스트 &quot;지시&quot;를 어떻게 사용할 수 있는지 고려해야 할 필요성을 강조합니다. 더욱이 복제된 LLM 체인의 효과는 학생의 LLM 강점에 대한 인식에 따라 달라지므로 보조적 프롬프트에 대한 추가 조사가 필요합니다. LLM과 크라우드워커의 차이점에 대한 즉각적인 통찰력을 제공하는 것 외에도, 저희의 연구는 크라우드소싱 파이프라인을 복제하는 것이 더 광범위한 작업에 걸쳐 LLM의 부분적 효과성에 대한 향후 조사를 위한 귀중한 플랫폼 역할을 한다는 것을 보여줍니다. LLM이 복잡한 작업 전체를 처리할 것으로 기대하는 대신, LLM이 지속적으로 인간과 동등한 성과를 내는 특정 하위 작업을 평가하고 식별할 수 있습니다. 그런 다음 이 증거를 활용하여 LLM과 인간 작업자 간에 하위 작업을 분배하여 책임 할당을 최적화할 수 있습니다. 저희는 https://github.com/tongshuangwu/11m-crowdsourcing-pipeline에서 프롬프트 체인, 출력 및 평가를 오픈소스로 제공합니다. 2 배경 및 관련 연구 크라우드소싱은 대규모로 인간의 입력이 필요한 문제를 해결하는 데 도움이 됩니다(Howe et al., 2006). 특히 AI 역량이 제한적이었던 초기에는 크라우드소싱이 인간이 가진 고유한 계산 능력을 활용하고 향상시키는 유망한 접근 방식으로 여겨졌습니다. 크라우드소싱 연구의 주요 초점은 점점 더 복잡해지는 크라우드소싱 목표를 해결하기 위한 파이프라인 개발이었습니다(Kittur et al., 2011). 신중한 작업 분해를 통해 크라우드소싱 파이프라인은 인간 근로자로부터 전략적으로 입력을 수집하여 그들의 강점을 활용하고 한계를 완화합니다. 이러한 업적은 기존 크라우드소싱 디자인에서는 달성하기 어렵거나 불가능합니다. 예를 들어 Bernstein et al.(Bernstein et al., 2010)은 크라우드워커의 노력의 분산을 줄이기 위해 하위 작업의 범위를 조절하는 Find-Fix-Verify 워크플로를 통해 텍스트 편집 품질을 보장했습니다. 한편, 컨텍스트 트리(Verroios and Bernstein, 2014)는 압도적인 글로벌 컨텍스트를 계층적으로 요약하고 트리밍하여 단일 근로자가 소화할 수 있을 만큼 압축합니다. 정교한 설계로 인해 크라우드소싱 파이프라인은 종종 인간 계산 알고리즘 또는 크라우드 알고리즘(Howe et al., 2006; Law and Zhang, 2011; Kittur et al., 2011; Little et al., 2010)이라고 불립니다. 완전히 별개의 분야(NLP)에서 출현했지만 LLM 체인은 한 번에 수행하기 어려운 복잡한 작업을 완료하기 위해 크라우드소싱 파이프라인과 유사한 목표를 공유합니다. 이 분해는 명시적 또는 암묵적 형태를 취할 수 있습니다. 예를 들어, Chain-ofThought(Kojima et al., 2022; Wei et al., 2022)는 &quot;단계별로 고려해 보자&quot;와 같은 프롬프트를 사용하여 LLM이 미리 정의되지 않은 하위 작업을 해결하도록 하는 반면, AI Chains(Wu et al., 2022b)와 Decomposed Prompting(Khot et al., 2022)은 하위 작업을 명시적으로 정의하고 각 하위 작업에 대해 별도의 프롬프트를 사용합니다. 최근에는 LangChain(Chase)과 같은 오픈소스 라이브러리와 PromptChainer(Wu et al., 2022a; noa)와 같은 서비스를 통해 실무자는 복잡한 구성성을 포함하는 작업을 처리하기 위한 LLM 체인을 만들 수 있게 되었습니다. 섹션 1에서 검토한 대로, Wu et al.(2022b)은 LLM 체이닝과 크라우드소싱 파이프라인 간에 명확한 연결을 도출했습니다. 유사한 동기 외에도 이 두 방법은 유사한 과제를 공유합니다. 예를 들어, 연쇄 오류 처리 후기 단계에 영향을 미치는 것(Kittur et al., 2011)이나 작업자의 일관되지 않은 기여를 종합하는 것(Kittur et al., 2011; Bernstein et al., 2010)이 있지만, 이러한 과제는 Al 주입 시스템의 투명성과 디버깅 가능성을 향상시키는 데 활용할 수 있습니다. 더 중요한 것은 Wu et al.(2022b)이 두 가지 접근 방식에 대한 작업 분해 목표를 구분했다는 것입니다. 즉, 인간과 LLM 작업자의 서로 다른 한계를 해결하기 위한 것입니다. 이론적으로는 이 주장이 사실이지만 실제로는 인간과 LLM 작업자 간의 차이가 모호해지는 듯합니다. LLM이 더 긴 맥락을 처리하도록 진화(OpenAI, 2023), 지침을 더 면밀히 따르도록(Ouyang et al., 2022), 추론 능력이 향상됨에 따라(Bubeck et al., 2023) 일부 한계가 인간의 한계와 겹치기 시작했습니다. 다양한 최근 연구도 이러한 관찰을 증명합니다.명확하게 체이닝으로 분류되지는 않았지만, 여러 연구에서 LLM이 여러 번 실행하여 자체 개선되도록 하는 전략을 채택했습니다.이러한 전략에는 자체 질문(Press et al., 2022), 자체 반성(Shinn et al., 2023), 자체 일관성(Wang et al., 2022b)이 있으며, 이 중 일부는 크라우드소싱 파이프라인과 유사합니다.이러한 LLM의 최근 발전과 크라우드소싱 파이프라인의 성공은 인간 계산 알고리즘의 아이디어를 AI로 직접 전환할 수 있는지 재평가하도록 촉구합니다.3 연구 설계 연구 절차 이 연구에서는 참가자(학생)가 LLM이 다양한 마이크로 작업을 완료하도록 지시하는 여러 프롬프트를 작성하여 크라우드소싱 파이프라인을 복제하도록 요구했습니다.이를 달성하기 위해 학생들은 복제를 위해 크라우드소싱 파이프라인 논문을 철저히 읽었습니다. 복제된 파이프라인의 효과를 입증하기 위해, 그들은 또한 적절한 테스트 과제를 결정하고, 입력과 이상적인 출력의 쌍으로 구성된 최소 3개의 테스트 사례를 만들고, 파이프라인 출력을 평가하기 위한 과제 종속적 지표 세트(예: 유창성, 창의성, 일관성)를 자체 제안하라는 요청을 받았습니다. 그런 다음, 그들은 두 가지 솔루션을 구현하라는 지시를 받았습니다. (1) 하나의 LLM 모듈이 전체 과제를 완료하도록 촉구하는 기준 솔루션(기준선), (2) 선택한 크라우드소싱 파이프라인의 복제본(LLM 체인). 그들은 지정된 테스트 사례와 지표를 사용하여 두 LLM 솔루션을 비교하여 평가의 이유를 제공했습니다. 마지막으로, 그들은 LLM 체인 복제가 성공했거나 실패한 이유를 성찰하고 미래에 체인을 개선할 수 있는 가능한 방법을 브레인스토밍하여 과제를 마무리했습니다. 학생들이 과제를 제출한 후, 그들은 동료 평가 과정을 거쳤습니다. 이 과정에서 각 학생의 제출물은 다음 세 가지 파이프라인 설명에 따라 평가되었습니다.맵-리듀스 분할 작업을 개별 하위 작업으로 매핑, 하위 작업을 매핑(Kittur et al., 2011) | 작업자, 결과를 단일 출력으로 축소/병합 입력 에세이 주제 미국 학교에 미친 종교적 영향의 역사 유머 도구(Chilton et al., 2016) 입력 뉴스 헤드라인 FDA가 과다 복용을 줄이기 위해 최초의 일반 의약품인 나르칸 승인 분할 개요 종교 교육의 기원 블레인 개정안과 그 영향 ... 미국 종교 교육의 미래 Мар 사실 최초의 교육 기관은 ... 1600년대에 종교 단체에 의해 설립되었습니다... ...개신교 교파는 대학을 설립하고 도덕적 가치를 가르쳤습니다. 20세기...종교 수업을 제공하지만 범위는... 주마다 크게 달랐습니다. | 비전문가도 직관적으로 이해할 수 있는 일련의 질문에 대한 답변으로 의미적 역할을 정의합니다. 과다복용 헤드라인의 측면 연관성 관련 개념 약물 금단 증상 보충제 약국 처방전 없이 구입 가능한 약물 줄이기 샘플 과제 에세이 쓰기 문단 복제 평가 전체 | 고유 | 정확 | 효과적 | 41 미국 종교 교육의 기원은 1600년대 초의 가장 초기 교육 기관으로 거슬러 올라갈 수 있는데, 이는 종교 단체가 학생들에게 종교적 교육과 도덕적 지침을 제공하기 위해 설립한 것입니다. 19세기에 개신교 교파는 종교 교육과 도덕적 가치 교육에 중점을 둔 단과대학과 대학을 설립했습니다. 이로 인해 미국에서 종교 교육이 독특한 학문 분야로 발전하게 되었습니다. 20세기에 일부 주에서는 공립학교에서 종교 수업을 제공하도록 허용하기 시작했지만, 이러한 수업의 범위와 내용은 주마다 크게 달랐습니다. 이를 통해 종교 교육이 미국 학생들에게 더 널리 제공될 수 있었습니다... 풍자 만들기 | 4 2 예상 이유 믿음 측면과 연관성을 사용하여 헤드라인에 농담을 합니다. 다음에 식료품점에 갈 때 뭘 사야 할지 알 것 같아... 나르칸과 칩, 누구? 와, 이제 나르칸을 처방전 없이 살 수 있어? 이제 최신 파티 약물을 손에 넣는 데 처방전도 필요 없지!반복적 프로세스 | 한 가지 생성 작업의 결과를 다음 작업에 공급하여 (Little et al., 2010) | 작업자는 이전 작업자가 생성한 콘텐츠를 볼 수 있습니다.입력 쿼리 반복 5개 제목 1. A Woof of a Tale 2. Barking Up the Right Tree 말하는 개에 대한 단편 소설 제목 3. The Adventures of the Chatty Canines 4. The Dog Who Could Speak 5. The Journey of the Talking Dogs 브레인스토밍 반복 1 &quot;더 좋고 창의적인&quot; 제목 1. A Ruff and Ready Conversation 2. The Speak of the Dogs 3. A Lively Pack of Pups 4. Tales From the Bark Side 사용하지 않은 단계 예상 반응 위반1. 피도의 혀 펄럭임 Iter 2 &quot;더 좋고 창의적인&quot; 제목 2. 개들의 비밀 대화 3. 지혜의 으르렁거림 4. 개들의 대화 5. 다른 품종의 짖는 소리 5. 혀가 묶인 테리어 마이크로태스킹 정렬 작업을 위한 구체적 마이크로태스킹: 구현 정렬(Cheng et al., 2015) | 인력 퀵정렬 입력 숫자 목록 쌍 쌍으로 결합. #홀수 비교 정렬 54321123451234512345찾기-수정-검증 | 쓰기 및 편집: 문제 찾기, 식별된 문제 수정 |텍스트 줄이기(Bernstein et al., 2010) 문제, 이 편집 확인 입력 텍스트를 줄여야 합니다. 인쇄 출판사는 Apple의 새로운 iPad에 대해 흥분하고 있습니다. 마침내 디지털 에디션에 대한 요금을 청구할 수 있기를 바라기 때문입니다. 하지만 사람들이 잡지와 신문 앱을 구매하도록 하려면 신문 가판대나 오픈 웹에서 독자들이 얻을 수 없는 다른 것을 제공해야 합니다. 자세한 부분을 찾아서 마침내... 디지털 에디션을 통해 사람들이... 앱이 오픈 웹에서 무언가를 제공하도록 하기를 바랍니다. 자세한 부분을 줄이고 맥락에 맞게 다시 배치하여 사람들이 앱에 비용을 지불하도록 하기를 바랍니다. 다른 곳에서는 볼 수 없는 독특한 것을 제공합니다. 인쇄 출판사는 Apple의 새로운 iPad에 대해 당황하고 있습니다. 디지털 에디션에 비용을 지불하도록 하기를 바라기 때문입니다. 하지만 사람들이 앱에 비용을 지불하도록 하려면 다른 곳에서는 볼 수 없는 독특한 것을 제공해야 합니다. 가격 분할 해결 작업자는 복잡한 단계를 재귀적으로 나누어 적절하게 간단한 수준에 도달한 다음 해결합니다. 에세이 쓰기(Kulkarni et al., 2012) 입력 주제 카네기 멜론 대학교의 마스코트에 대한 1단락 설명 작성 분할 하위 과제 찾기 마스코트 조사 단락 작성 교정 및 단축 완료 하위 과제 완료(또는 추가 분할) 마스코트는... 타탄 스코트... 만화 캐릭터... 우리 마스코트는... 우리 학교의 교육 우수성에 대한 헌신을 나타냅니다... .↳ 카네기 멜론 대학교의 마스코트는... 과제 의역 | 비전문가도 직관적으로 이해할 수 있는 일련의 (He et al., 2015) | 질문에 대한 답변으로 의미적 역할을 정의합니다. 입력 레이블 없는 텍스트 금요일에 Clark은 Facebook에 자신의 결정을 설명하는 글을 올렸습니다. &quot;저는 보통 정치적 항목을 게시하지 않지만 오늘은 다릅니다.&quot; 동사 &quot;게시&quot;에 대한 질문 누가 무언가를 게시했습니까? SRL 레이블 답변 Clark 왜 누군가가 무언가를 게시했습니까? 언제 누군가가 무언가를 게시했습니까? 금요일에 문법 문제 수정 인쇄 출판사는 디지털 에디션에 요금을 부과하기를 바라기 때문에 Apple의 새로운 iPad에 대해 흥분하고 있습니다. 하지만 사람들이 앱에 비용을 지불하게 하려면 다른 곳에서는 볼 수 없는 독특한 것을 제공해야 합니다. 글로벌 솔루션 카네기 멜론 대학교의 마스코트는 학교의 스코틀랜드 유산을 나타내는 쾌활한 만화 캐릭터인 Tartan Scot입니다. 이 마스코트는... 교육에 대한 학교의 우수성에 대한 헌신을 구현하고 있습니다... 1 1 1 라벨링 SRL ARGO에 대한 답변 맵: Clark ARGM-PRP: 자신의 결정을 설명하려면 ARGM-TMP: 금요일에 표 1: 복제된 크라우드소싱 파이프라인과 학생이 복제한 LLM 체인의 예시 출력. 이중 맹검 방식으로 동료들과 평가했습니다. 동료들은 복제 정확성, 철저함, 구상한 LLM 체인 개선의 포괄성을 기준으로 제출물을 평가했습니다. 그들은 모든 기준을 5단계 리커트 척도로 평가하고 평가에 대한 자세한 이유를 제공했습니다. 강사는 채점을 주의 깊게 검토하고 사려 깊은 반성이 부족하거나 제출물을 오해한 것으로 보이는 평가를 제외했습니다.전체 과제 지침, 동료 채점 양식 및 학생 제출물은 모두 https://github.com/tongshuangwu/11m-crowdsourcing-pipeline에서 확인할 수 있습니다.참가자 21명의 학생(여 13명, 남 8명)이 2023년 봄 과정 05-499/899: HumanCentered NLP의 과제 중 하나로 과제를 완료했습니다.이 과정에는 사회학, 학습 과학, 인간-컴퓨터 상호 작용 또는 자연어 처리를 전문으로 하는 학부생 6명, 석사생 10명, 박사생 5명이 포함되었습니다.이 논문은 1명의 학생이 부분 학점을 위해 비프로그래밍 방식을 선택했기 때문에 20명의 학생의 제출물에서 얻은 결과를 제시합니다. 크라우드소싱 논문 우리는 세 가지 기준에 따라 크라우드소싱 논문을 선택했습니다.(1) 다양성: 논문은 다양한 파이프라인 설계(반복적, 병렬적), 중간 단계(질의응답, 비교, 편집) 및 작업(창의적 작업, 주석 작업, 편집 작업 등)을 나타내야 합니다.(2) 재현성: 논문은 각 하위 단계에 대한 명확한 정의와 구체적인 샘플 테스트 사례를 제공해야 합니다. LLM에 대한 강조점을 고려하여 텍스트 입력 및 출력이 있는 작업을 설명하는 논문만 고려했습니다.(3) 비동기화: 설정을 쉽게 하기 위해 논문은 (LLM) 작업자가 동기화된 토론이 필요 없이 독립적으로 마이크로 작업을 완료할 수 있도록 해야 합니다. 강사는 이러한 기준을 충족하는 6개의 논문(표 1의 처음 6개)을 미리 선택했으며 학생은 승인을 위해 추가 논문을 제안할 수 있습니다(표 1의 작업 의역). 최대 4명의 학생이 선착순 방식으로 동일한 파이프라인을 복제하기 위해 등록할 수 있습니다. LLM 버전 학생들은 최종 구현 및 테스트를 위해 text-davinci-003³을 사용해야 하며, &#39;http://www.cs.cmu.edu/~sherryw/courses/2023s-hcnlp.html 2&#39;를 사용하는 가장 유능한 모델을 사용해야 합니다. 두 개의 LLM API에 토론을 지시하면 동기화된 토론을 쉽게 설정할 수 있다는 점에 유의하십시오. ³https://platform.openai.com/docs/models 과제 설계 시 자동 완성 인터페이스를 사용합니다. 그러나 처음에는
--- EXPERIMENT ---
s 및 논문 초안을 검토한 후. 크라우드 파이프라인 LLM 체인 그림 1: LLM을 사용하여 크라우드소싱 파이프라인을 복제하고 특정 고급 &quot;인간-계산 프로세스&quot;에서 인간 근로자를 대체할 수 있는지 연구합니다. 지침 튜닝(Stiennon et al., 2020; Wu et al., 2023; Ziegler et al., 2019)에 따라 LLM은 이제 방대한 양의 세계 지식을 소유할 뿐만 아니라 지침을 따르는 것만으로 다양한 작업을 수행하는 데 이 지식을 효과적으로 활용할 수 있습니다. 다양한 연구에 따르면 이러한 모델은 어느 정도 인간과 유사한 행동을 복제할 수 있으며, 이는 AI 모델 훈련의 핵심 목표입니다(Wang et al., 2022a; Bubeck et al., 2023). 특히 이러한 연구의 상당 부분은 크라우드소싱 작업을 복제하는 데 LLM을 사용해 왔는데, 이는 이전에 인간의 계산 능력에만 국한된 것으로 간주되었던 광범위한 작업을 나타내기 때문일 수 있습니다(Bernstein, 2013). 예를 들어, LLM은 크라우드워커 또는 전문가에 비해 더 낮은 비용으로 더 높은 품질의 주석을 생성할 수 있으며(Gilardi et al., 2023; Törnberg, 2023), 주관적 작업에서 인간의 의견을 근사화하여 크라우드소싱 설문지와 인터뷰에 대한 시뮬레이션된 인간 응답을 허용할 수 있습니다(Hämäläinen et al., 2023; Argyle et al., 2022). 이러한 관찰 결과는 LLM이 상당한 사회적 및 경제적 영향을 미쳐 특정 인간 작업을 대체함으로써 인력을 재편할 가능성이 있음을 시사합니다(Eloundou et al., 2023). 사실, 일부 연구에 따르면 크라우드워커는 이제 텍스트 생산 작업을 완료하기 위해 LLM에 의존하는 경향이 있습니다(Veselovsky et al., 2023). 그러나 대부분의 기존 노력은 간단하고 자립적이며 단 한 명의 크라우드워커가 짧은 시간 내에 완료하기 쉬운 원자적 작업에 초점을 맞추는 경향이 있습니다. 이는 인간의 계산 능력의 가장 기본적인 버전입니다. 이러한 노력은 또한 다양한 작업과 도메인에 분산되어 있어 LLM이 어떤 작업에서 뛰어나거나 못 미치는지, 그리고 특정 작업에서 인간을 어느 정도 시뮬레이션, 대체 또는 증강할 수 있는지 체계적으로 비교하고 이해하기 어렵게 만듭니다. 이러한 강조는 LLM 재현성이 얼마나 일반화되는지에 대한 질문을 던집니다. 그것들은 &quot;인간 계산&quot;의 더 진보된 형식에서 유용할까요? 우리는 LLM이 인간 계산을 활용하는 데 있어 더 정교한 접근 방식을 나타내는 크라우드소싱 파이프라인을 복제하는 데 사용될 수 있는지에 특히 관심이 있습니다(Little et al., 2010). 일반적인 파이프라인에서 복잡한 작업은 독립적으로 수행될 수 있는 부분(하위 작업)으로 나뉜 다음 나중에 결합됩니다(Chilton et al., 2013; Kim et al., 2017; Law and Zhang, 2011; Retelny et al., 2017). 이 방법은 크라우드소싱의 유용성을 확장하는 데 널리 사용되어 제한된 수준의 헌신과 알려지지 않은 전문 지식을 가진 개별 크라우드워커에게 너무 어려운 작업(예: 긴 소설 요약, 소프트웨어 개발 또는 심하게 흐릿한 텍스트 해독; Kittur et al., 2011)을 처리할 수 있습니다. 흥미롭게도 LLMS에 대한 연구에서는 체이닝을 통해 더 복잡한 작업에 대한 기능을 확장하는 것도 탐구했습니다. 이름은 다르지만 LLM 체인과 크라우드소싱 파이프라인은 유사한 동기를 공유합니다. 그리고 LLM 유틸리티를 확장하는 전략. 이전 연구에서는 두 가지를 연결하여 서로 다른 문제를 해결하기 위해 작업을 분해한다는 점을 지적했습니다(Wu et al., 2022b): 크라우드소싱 파이프라인은 인지 부하 및 작업 기간과 같은 인간 근로자 성과에 영향을 미치는 요인에 초점을 맞추는 반면, LLM 체인은 신속한 효과성의 높은 분산과 같은 LLM의 고유한 한계를 해결합니다. 그러나 LLM은 이제 지시를 따르고 복잡한 맥락을 처리하는 데 있어 인간과 더 잘 일치하도록 훈련되었기 때문에(Ouyang et al., 2022), 인간과 LLM 근로자가 동일한 작업 분할 전략을 채택할 수 있습니다. 이 연구에서 우리는 LLM이 고급 인간 계산 프로세스에서 인간 근로자를 대체할 수 있는 잠재력을 조사합니다. 이를 달성하기 위해 우리는 카네기 멜론 대학교의 인간 중심 NLP라는 특별 주제 과정에 대한 과정 과제를 설계했습니다. 이 과제에서 20명의 학생은 이전 작업에 묘사된 크라우드소싱 파이프라인 중 하나(7개 중)를 선택하고 LLM을 사용하여 각 하위 작업을 처리하여 이를 복제하는 과제를 받았습니다. 복제 연구는 또한 흥미로운 보너스 분석 포인트를 제공합니다. 체인의 LLM 모듈은 고유한 하위 작업을 수행하지만 모든 하위 작업은 동일한 애플리케이션 도메인에서 발생합니다(예: 동일한 문서를 다른 방식으로 처리). 따라서 다른 하위 작업에서 LLM의 성과를 비교하고 상대적인 강점과 약점을 발견하는 것이 더 공정합니다. LM이 크라우드소싱 파이프라인을 복제할 수 있는 것처럼 보이지만, 어떤 부분에서 잘 수행하는 경향이 있는지/인간에게 기대하는 방식으로 수행하는 경향이 있는지에 큰 차이가 있습니다(표 2b의 주요 결과). 이러한 차이점은 두 가지 주요 이유에서 나타납니다. 첫째, LLM과 인간은 지시에 다르게 반응합니다. LLM은 &quot;더 나은&quot; 또는 &quot;더 다양한&quot;과 같은 형용사와 비교 기반 지침에 더 잘 반응하는 반면, 인간은 상쇄 기준과 관련된 지침을 더 잘 처리합니다. 둘째, 인간은 의견 불일치 해결 메커니즘과 인터페이스 강화 상호 작용을 통해 더 많은 스캐폴드를 받아 LLM에서 사용할 수 없는 출력 품질과 구조에 대한 가드레일을 사용할 수 있습니다. 이러한 관찰 결과는 모호하거나 불완전한 지침을 더 잘 처리하기 위해 LLM 지침 튜닝을 개선해야 할 필요성과 LLM 미세 조정 또는 실제 사용 중에 비텍스트 &quot;지침&quot;을 어떻게 사용할 수 있는지 고려해야 할 필요성을 강조합니다. 게다가 복제된 LLM 체인의 효과는 LLM 강점에 대한 학생의 인식에 따라 달라지므로 보조적 프롬프트에 대한 추가 조사가 필요합니다. LLM과 크라우드워커 간의 차이점에 대한 즉각적인 통찰력을 제공하는 것 외에도, 우리의 연구는 크라우드소싱 파이프라인을 복제하는 것이 더 광범위한 작업에서 LLM의 부분적 효과에 대한 향후 조사를 위한 귀중한 플랫폼 역할을 한다는 것을 보여줍니다. LLM이 복잡한 작업 전체를 다루기를 기대하기보다는 LLM이 인간과 동등하게 지속적으로 수행하는 특정 하위 작업을 평가하고 식별할 수 있습니다. 그런 다음 이 증거를 활용하여 LLM과 인간 근로자 간에 하위 작업을 분배하여 책임 할당을 최적화할 수 있습니다. https://github.com/tongshuangwu/11m-crowdsourcing-pipeline에서 프롬프트 체인, 출력 및 평가를 오픈소스로 제공합니다. 2 배경 및 관련 작업 크라우드소싱은 대규모로 인간의 입력이 필요한 문제를 해결하는 데 도움이 됩니다(Howe et al., 2006). 특히 AI 역량이 제한되었던 초기에 크라우드소싱은 인간이 가진 고유한 계산 능력을 활용하고 향상시키는 유망한 접근 방식으로 여겨졌습니다. 크라우드소싱 연구의 주요 초점은 점점 더 복잡해지는 크라우드소싱 목표를 해결하기 위한 파이프라인 개발이었습니다(Kittur et al., 2011). 신중한 작업 분해를 통해 크라우드소싱 파이프라인은 인간 근로자로부터 전략적으로 입력을 수집하여 그들의 강점을 활용하고 한계를 완화합니다. 이러한 업적은 기존 크라우드소싱 디자인에서는 달성하기 어렵거나 불가능합니다. 예를 들어, Bernstein 등(Bernstein 등, 2010)은 크라우드워커의 노력의 분산을 줄이기 위해 하위 작업의 범위를 조절하는 Find-Fix-Verify 워크플로를 통해 텍스트 편집 품질을 보장했습니다. 한편, Context Trees(Verroios 및 Bernstein, 2014)는 압도적인 글로벌 컨텍스트를 계층적으로 요약하고 트리밍하여 단일 작업자가 소화할 수 있을 만큼 압축합니다. 정교한 디자인 때문에 크라우드소싱 파이프라인은 종종 인간 계산 알고리즘 또는 크라우드 알고리즘(Howe 등, 2006; Law 및 Zhang, 2011; Kittur 등, 2011; Little 등, 2010)이라고 합니다. 완전히 별도의 분야(NLP)에서 등장했지만 LLM 체인은 한 번에 수행하기 어려운 복잡한 작업을 완료하기 위해 크라우드소싱 파이프라인과 유사한 목표를 공유합니다. 이 분해는 명시적 또는 암묵적 형태를 취할 수 있습니다. 예를 들어, Chain-ofThought(Kojima et al., 2022; Wei et al., 2022)는 &quot;단계별로 고려해 보자&quot;와 같은 프롬프트를 사용하여 LLM이 미리 정의되지 않은 하위 작업을 해결하도록 하는 반면, AI Chains(Wu et al., 2022b)와 Decomposed Prompting(Khot et al., 2022)은 하위 작업을 명시적으로 정의하고 각 하위 작업에 대해 별도의 프롬프트를 사용합니다. 최근에는 LangChain(Chase)과 같은 오픈소스 라이브러리와 PromptChainer(Wu et al., 2022a; noa)와 같은 서비스를 통해 실무자는 복잡한 구성성을 포함하는 작업을 처리하기 위한 LLM 체인을 만들 수 있게 되었습니다. 섹션 1에서 검토한 대로, Wu et al.(2022b)은 LLM 체이닝과 크라우드소싱 파이프라인 간에 명확한 연결을 도출했습니다. 유사한 동기 외에도 이 두 방법은 유사한 과제를 공유합니다. 예를 들어, 연쇄 오류 처리 후기 단계에 영향을 미치는 것(Kittur et al., 2011)이나 작업자의 일관되지 않은 기여를 종합하는 것(Kittur et al., 2011; Bernstein et al., 2010)이 있지만, 이러한 과제는 Al 주입 시스템의 투명성과 디버깅 가능성을 향상시키는 데 활용할 수 있습니다. 더 중요한 것은 Wu et al.(2022b)이 두 가지 접근 방식에 대한 작업 분해 목표를 구분했다는 것입니다. 즉, 인간과 LLM 작업자의 서로 다른 한계를 해결하기 위한 것입니다. 이론적으로는 이 주장이 사실이지만 실제로는 인간과 LLM 작업자 간의 차이가 모호해지는 듯합니다. LLM이 더 긴 맥락을 처리하도록 진화(OpenAI, 2023), 지침을 더 면밀히 따르도록(Ouyang et al., 2022), 추론 능력이 향상됨에 따라(Bubeck et al., 2023) 일부 한계가 인간의 한계와 겹치기 시작했습니다. 다양한 최근 연구도 이러한 관찰을 증명합니다.명확하게 체이닝으로 분류되지는 않았지만, 여러 연구에서 LLM이 여러 번 실행하여 자체 개선되도록 하는 전략을 채택했습니다.이러한 전략에는 자체 질문(Press et al., 2022), 자체 반성(Shinn et al., 2023), 자체 일관성(Wang et al., 2022b)이 있으며, 이 중 일부는 크라우드소싱 파이프라인과 유사합니다.이러한 LLM의 최근 발전과 크라우드소싱 파이프라인의 성공은 인간 계산 알고리즘의 아이디어를 AI로 직접 전환할 수 있는지 재평가하도록 촉구합니다.3 연구 설계 연구 절차 이 연구에서는 참가자(학생)가 LLM이 다양한 마이크로 작업을 완료하도록 지시하는 여러 프롬프트를 작성하여 크라우드소싱 파이프라인을 복제하도록 요구했습니다.이를 달성하기 위해 학생들은 복제를 위해 크라우드소싱 파이프라인 논문을 철저히 읽었습니다. 복제된 파이프라인의 효과를 입증하기 위해, 그들은 또한 적절한 테스트 과제를 결정하고, 입력과 이상적인 출력의 쌍으로 구성된 최소 3개의 테스트 사례를 만들고, 파이프라인 출력을 평가하기 위한 과제 종속적 지표 세트(예: 유창성, 창의성, 일관성)를 자체 제안하라는 요청을 받았습니다. 그런 다음, 그들은 두 가지 솔루션을 구현하라는 지시를 받았습니다. (1) 하나의 LLM 모듈이 전체 과제를 완료하도록 촉구하는 기준 솔루션(기준선), (2) 선택한 크라우드소싱 파이프라인의 복제본(LLM 체인). 그들은 지정된 테스트 사례와 지표를 사용하여 두 LLM 솔루션을 비교하여 평가의 이유를 제공했습니다. 마지막으로, 그들은 LLM 체인 복제가 성공했거나 실패한 이유를 성찰하고 미래에 체인을 개선할 수 있는 가능한 방법을 브레인스토밍하여 과제를 마무리했습니다. 학생들이 과제를 제출한 후, 그들은 동료 평가 과정을 거쳤습니다. 이 과정에서 각 학생의 제출물은 다음 세 가지 파이프라인 설명에 따라 평가되었습니다.맵-리듀스 분할 작업을 개별 하위 작업으로 매핑, 하위 작업을 매핑(Kittur et al., 2011) | 작업자, 결과를 단일 출력으로 축소/병합 입력 에세이 주제 미국 학교에 미친 종교적 영향의 역사 유머 도구(Chilton et al., 2016) 입력 뉴스 헤드라인 FDA가 과다 복용을 줄이기 위해 최초의 일반 의약품인 나르칸 승인 분할 개요 종교 교육의 기원 블레인 개정안과 그 영향 ... 미국 종교 교육의 미래 Мар 사실 최초의 교육 기관은 ... 1600년대에 종교 단체에 의해 설립되었습니다... ...개신교 교파는 대학을 설립하고 도덕적 가치를 가르쳤습니다. 20세기...종교 수업을 제공하지만 범위는... 주마다 크게 달랐습니다. | 비전문가도 직관적으로 이해할 수 있는 일련의 질문에 대한 답변으로 의미적 역할을 정의합니다. 과다복용 헤드라인의 측면 연관성 관련 개념 약물 금단 증상 보충제 약국 처방전 없이 구입 가능한 약물 줄이기 샘플 과제 에세이 쓰기 문단 복제 평가 전체 | 고유 | 정확 | 효과적 | 41 미국 종교 교육의 기원은 1600년대 초의 가장 초기 교육 기관으로 거슬러 올라갈 수 있는데, 이는 종교 단체가 학생들에게 종교 교육과 도덕적 지침을 제공하기 위해 설립한 것입니다. 19세기에 개신교 교파는 종교 교육과 도덕적 가치 교육에 중점을 둔 단과대학과 대학을 설립했습니다. 이로 인해 미국에서 종교 교육이 독특한 학문 분야로 발전하게 되었습니다. 20세기에 일부 주에서는 공립학교에서 종교 수업을 제공하도록 허용하기 시작했지만, 이러한 수업의 범위와 내용은 주마다 크게 달랐습니다. 이를 통해 종교 교육이 미국 학생들에게 더 널리 제공될 수 있었습니다... 풍자 만들기 | 4 2 예상 이유 믿음 측면과 연관성을 사용하여 헤드라인에 농담을 합니다. 다음에 식료품점에 갈 때 뭘 사야 할지 알 것 같아... 나르칸과 칩, 누구? 와, 이제 나르칸을 처방전 없이 살 수 있어? 이제 최신 파티 약물을 손에 넣는 데 처방전도 필요 없지!반복적 프로세스 | 한 가지 생성 작업의 결과를 다음 작업에 공급하여 (Little et al., 2010) | 작업자는 이전 작업자가 생성한 콘텐츠를 볼 수 있습니다.입력 쿼리 반복 5개 제목 1. A Woof of a Tale 2. Barking Up the Right Tree 말하는 개에 대한 단편 소설 제목 3. The Adventures of the Chatty Canines 4. The Dog Who Could Speak 5. The Journey of the Talking Dogs 브레인스토밍 반복 1 &quot;더 좋고 창의적인&quot; 제목 1. A Ruff and Ready Conversation 2. The Speak of the Dogs 3. A Lively Pack of Pups 4. Tales From the Bark Side 사용하지 않은 단계 예상 반응 위반1. 피도의 혀 펄럭임 Iter 2 &quot;더 좋고 창의적인&quot; 제목 2. 개들의 비밀 대화 3. 지혜의 으르렁거림 4. 개들의 대화 5. 다른 품종의 짖는 소리 5. 혀가 묶인 테리어 마이크로태스킹 정렬 작업을 위한 구체적 마이크로태스킹: 구현 정렬(Cheng et al., 2015) | 인력 퀵정렬 입력 숫자 목록 쌍 쌍으로 결합. #홀수 비교 정렬 54321123451234512345찾기-수정-검증 | 쓰기 및 편집: 문제 찾기, 식별된 문제 수정 |텍스트 줄이기(Bernstein et al., 2010) 문제, 이 편집 확인 입력 텍스트를 줄여야 합니다. 인쇄 출판사는 Apple의 새로운 iPad에 대해 흥분하고 있습니다. 마침내 디지털 에디션에 대한 요금을 청구할 수 있기를 바라기 때문입니다. 하지만 사람들이 잡지와 신문 앱을 구매하도록 하려면 신문 가판대나 오픈 웹에서 독자들이 얻을 수 없는 다른 것을 제공해야 합니다. 자세한 부분을 찾아서 마침내... 디지털 에디션을 통해 사람들이... 앱이 오픈 웹에서 무언가를 제공하도록 하기를 바랍니다. 자세한 부분을 줄이고 맥락에 맞게 다시 배치하여 사람들이 앱에 비용을 지불하도록 하기를 바랍니다. 다른 곳에서는 볼 수 없는 독특한 것을 제공합니다. 인쇄 출판사는 Apple의 새로운 iPad에 대해 당황하고 있습니다. 디지털 에디션에 비용을 지불하도록 하기를 바라기 때문입니다. 하지만 사람들이 앱에 비용을 지불하도록 하려면 다른 곳에서는 볼 수 없는 독특한 것을 제공해야 합니다. 가격 분할 해결 작업자는 복잡한 단계를 재귀적으로 나누어 적절하게 간단한 수준에 도달한 다음 해결합니다. 에세이 쓰기(Kulkarni et al., 2012) 입력 주제 카네기 멜론 대학교의 마스코트에 대한 1단락 설명 작성 분할 하위 과제 찾기 마스코트 조사 단락 작성 교정 및 단축 완료 하위 과제 완료(또는 추가 분할) 마스코트는... 타탄 스코트... 만화 캐릭터... 우리 마스코트는... 우리 학교의 교육 우수성에 대한 헌신을 나타냅니다... .↳ 카네기 멜론 대학교의 마스코트는... 과제 의역 | 비전문가도 직관적으로 이해할 수 있는 일련의 (He et al., 2015) | 질문에 대한 답변으로 의미적 역할을 정의합니다. 입력 레이블 없는 텍스트 금요일에 Clark은 Facebook에 자신의 결정을 설명하는 글을 올렸습니다. &quot;저는 보통 정치적 항목을 게시하지 않지만 오늘은 다릅니다.&quot; 동사 &quot;게시&quot;에 대한 질문 누가 무언가를 게시했습니까? SRL 레이블 답변 Clark 왜 누군가가 무언가를 게시했습니까? 언제 누군가가 무언가를 게시했습니까? 금요일에 문법 문제 수정 인쇄 출판사는 디지털 에디션에 요금을 부과하기를 바라기 때문에 Apple의 새로운 iPad에 대해 흥분하고 있습니다. 하지만 사람들이 앱에 비용을 지불하게 하려면 다른 곳에서는 볼 수 없는 독특한 것을 제공해야 합니다. 글로벌 솔루션 카네기 멜론 대학교의 마스코트는 학교의 스코틀랜드 유산을 나타내는 쾌활한 만화 캐릭터인 Tartan Scot입니다. 이 마스코트는... 교육에 대한 학교의 우수성에 대한 헌신을 구현하고 있습니다... 1 1 1 라벨링 SRL ARGO에 대한 답변 맵: Clark ARGM-PRP: 자신의 결정을 설명하려면 ARGM-TMP: 금요일에 표 1: 복제된 크라우드소싱 파이프라인과 학생이 복제한 LLM 체인의 예시 출력. 이중 맹검 방식으로 동료들과 평가했습니다. 동료들은 복제 정확성, 철저함, 구상한 LLM 체인 개선의 포괄성을 기준으로 제출물을 평가했습니다. 그들은 모든 기준을 5단계 리커트 척도로 평가하고 평가에 대한 자세한 이유를 제공했습니다. 강사는 채점을 주의 깊게 검토하고 사려 깊은 반성이 부족하거나 제출물을 오해한 것으로 보이는 평가를 제외했습니다.전체 과제 지침, 동료 채점 양식 및 학생 제출물은 모두 https://github.com/tongshuangwu/11m-crowdsourcing-pipeline에서 확인할 수 있습니다.참가자 21명의 학생(여 13명, 남 8명)이 2023년 봄 과정 05-499/899: HumanCentered NLP의 과제 중 하나로 과제를 완료했습니다.이 과정에는 사회학, 학습 과학, 인간-컴퓨터 상호 작용 또는 자연어 처리를 전문으로 하는 학부생 6명, 석사생 10명, 박사생 5명이 포함되었습니다.이 논문은 1명의 학생이 부분 학점을 위해 비프로그래밍 방식을 선택했기 때문에 20명의 학생의 제출물에서 얻은 결과를 제시합니다. 크라우드소싱 논문 우리는 세 가지 기준에 따라 크라우드소싱 논문을 선택했습니다.(1) 다양성: 논문은 다양한 파이프라인 설계(반복적, 병렬적), 중간 단계(질의응답, 비교, 편집) 및 작업(창의적 작업, 주석 작업, 편집 작업 등)을 나타내야 합니다.(2) 재현성: 논문은 각 하위 단계에 대한 명확한 정의와 구체적인 샘플 테스트 사례를 제공해야 합니다. LLM에 대한 강조점을 고려하여 텍스트 입력 및 출력이 있는 작업을 설명하는 논문만 고려했습니다.(3) 비동기화: 설정을 쉽게 하기 위해 논문은 (LLM) 작업자가 동기화된 토론이 필요 없이 독립적으로 마이크로 작업을 완료할 수 있도록 해야 합니다. 강사는 이러한 기준을 충족하는 6개의 논문(표 1의 처음 6개)을 미리 선택했으며 학생은 승인을 위해 추가 논문을 제안할 수 있습니다(표 1의 작업 의역). 최대 4명의 학생이 선착순 방식으로 동일한 파이프라인을 복제하기 위해 등록할 수 있습니다. LLM 버전 학생들은 최종 구현 및 테스트를 위해 text-davinci-003³을 사용해야 하며, 이는 &#39;http://www.cs.cmu.edu/~sherryw/courses/2023s-hcnlp.html 2&#39;를 사용하는 가장 유능한 모델입니다. 두 개의 LLM API에 토론을 지시하면 동기화된 토론을 쉽게 설정할 수 있을 것입니다. ³https://platform.openai.com/docs/models 과제 설계 시 자동 완성 인터페이스. 그러나 처음에는 더 비용 효율적인 모델(예: text-ada-001)을 사용하여 프롬프트를 실험하고 미세 조정하도록 권장되었습니다. 우리는 두 가지 차원에서 repliReplication 평가 체인을 평가합니다. 1. 복제 정확성: 피어 평가 결과를 사용하여 복제의 성공 여부를 측정합니다. 정확한 복제에 대한 피어 평균 점수가 3보다 크면 복제가 성공한 것으로 간주됩니다. 2. 체인 효과성: 학생 자신의 평가를 사용하여 복제된 체인이 기준선보다 효과적인지 평가합니다. 학생들이 테스트한 입력의 대부분에서 복제된 체인이 기준선보다 성능이 우수하다고 표시하면(최소 3개의 입력을 테스트해야 했다는 점을 기억하세요) 파이프라인이 효과적인 것으로 간주됩니다. 여러 학생이 동일한 파이프라인을 복제했기 때문에 성공적인 복제를 위한 핵심 요소를 밝히기 위해 동일한 파이프라인의 복제본을 비교하는 것도 흥미롭습니다. 학생들의 복제 전략을 살펴보고 (3)개의 고유한 복제본 수를 보고합니다. 구체적으로, LLM 프롬프트에 문구 차이가 있더라도 본질적으로 동일한 의도된 기능을 제공하는 단계가 포함된 경우 두 개의 체인을 동일한 것으로 간주하여 관련된 마이크로 작업을 기준으로 학생들의 LLM 체인을 수동으로 그룹화했습니다. 4 결과 및 반성 4.1 복제 개요: 부분적 성공 표 1에서 볼 수 있듯이 모든 파이프라인은 LLM으로 복제할 수 있습니다. 각 파이프라인에는 적어도 하나의 올바른 복제와 하나의 효과적인 복제가 있습니다. 성공을 나타내기 위해 학생들이 선호한다고 생각한 학생들의 LLM 체인 복제를 사용하여 생성된 실제 입출력 시퀀스 하나를 보여줍니다. 이러한 결과는 LLM이 이전에는 인간에게만 가능하다고 여겨졌던 작업의 하위 집합을 이제 수행할 수 있다는 것을 반복합니다(Bubeck et al., 2023). 여러 학생이 실험한 여러 파이프라인을 기록했는데, 이는 파이프라인/체인이 신속한 프로토타입 제작을 가능하게 한다는 이전의 관찰과 일치합니다(Wu et al., 2022a). 일부 탐구는 단일 단계에 초점을 맞추었고(예: Find-Fix-Verify의 P7에서 &quot;조각&quot;, &quot;절&quot;, &quot;하위 문자열&quot; 등 중에서 다른 표현을 선택), 다른 일부 학생들은 전역적으로 재설계하는 것을 실험했습니다.파이프라인 파이프라인 단계/작업당 차원 관찰 아이디어 둘 다: 복잡한 작업을 독립적으로 수행한 다음 결합할 수 있는 부분으로 분해합니다.제한 사항 | 둘 다: 연쇄 오류, 병렬 경로 간의 충돌 등.이득 최적 설계 둘 다: 그렇지 않으면 어려운 작업에 맞게 확장하고, 상호 작용이 더 구조화되고, 방해에 더 강합니다.LLM 체인: 설명을 위해 연쇄 효과 및 병렬 경로를 활용할 수 있습니다.크라우드.파이프라인: 단일 작업자의 함정 해결: 높은 작업 분산, 제한된 인지 부하 등.LLM 체인: 단일 LLM의 함정 해결: 제한된 추론 능력 등.(a) 이전 작업(예: Wu et al., 2022b). 차원 실용적 설계(§4.2) 지침에 대한 민감성(§4.3) 관찰 둘 다: 유사한 파이프라인 설계에서 이점을 얻을 수 있음(LLM은 지침에 따라 미세 조정됨). LLM 체인: LLM의 강점과 약점에 대한 학생의 신념에 따라 다름. 군중: 지침에서 무의식적으로 균형을 맞출 수 있음 vs. LLM은 명확한 우선순위 지정이 필요함. LLM: 추상적 지침(&quot;더 다양한 제목&quot;)에 반응함 vs. 군중 작업자는 앵커링 편향에 직면함. 출력 품질 군중: 노이즈 및 의견 불일치 해결 스캐폴드(§4.2) |LLM: 없음; LLM 비결정론이 간과됨. 군중: 다중 모드 &quot;지침&quot;(예: 텍스트 출력 구조 설명, 인터페이스 규정). 스캐폴드(§4.4) LLMS: 텍스트 지침만. 반성 및 기회 실무자가 프롬프트 세부 사항을 조정하여 LLM 유용성에 대한 인식을 적응시킬 수 있는 프레임워크를 개발합니다. LLM 지침 조정(예: 형용사에 대한 민감성, 단일 요구 사항에 대한 강조)의 효과를 평가합니다. 더 모호한 지침을 따르도록 LLM을 조정합니다. LLM 강점을 보완하는 기술을 식별하고 개발하도록 인간을 교육합니다. 동일한 프롬프트를 사용하는 다른 LLM 세대를 여러 LLM 근로자의 투표로 취급합니다. 인간-LLM 정렬을 확장하여 최적의 지침 양식도 고려합니다. LLM 시뮬레이션 인간에 대한 관찰을 실제 인간에 매핑하는 것을 탐구합니다. (b) 크라우드소싱 파이프라인에서 학생들의 복제에 대한 관찰 및 반성의 개요. 특정 파이프라인 연결(예: 반복적 프로세스의 P11은 이전 결과를 다음 단계로 전달하는 방법을 변경함)을 사용합니다. 흥미롭게도, 학생들의 최종 제출물과 그들 자신의 성찰을 살펴보면 (학생들이 믿는 바에 따르면) 특정 파이프라인은 조정이 필요하고(예: 마이크로태스킹, 찾기-수정-검증), 다른 파이프라인은 더 문자 그대로 복제할 수 있다는 것이 분명해집니다(예: 맵-리듀스). 그렇긴 하지만, 대부분의 파이프라인은 100% 성공이나 효과를 달성하지 못했습니다. 학생들은 복제 실패의 원인을 대체로 &quot;파이프라인을 LLM으로 변환하는 데 많은 작업이 필요했습니다(올바른 프롬프트를 파악하고 파이프라인의 한 단계에서 다음 단계로 이동하는 데 필요한 사전 처리 측면에서) 크라우드소싱 작업자와 함께 구현했을 때와 비교했습니다&quot;(P14, 찾기-수정-검증). 그러나 우리는 이러한 프롬프트 어려움의 근저에 더 미묘한 이유가 있다고 생각합니다. 다음 섹션에서는 복제 관행에서 나온 여러 가지 정성적 관찰을 탐구하고 그 의미를 성찰합니다(관찰과 기회에 대한 개요는 표 2b에 나와 있습니다). 4.2 복제 분산: LLM 역량에 대한 학생들의 인식에 영향을 받음 결과에서 드러난 흥미로운 측면 중 하나는 일부 파이프라인이 다른 파이프라인보다 복제 분산이 더 크다는 것입니다(즉, 동일한 파이프라인에 대한 다른 학생들의 복제가 서로 상당히 다름). 예를 들어, 두 논문 모두 복제에 대한 충분한 세부 정보를 제공했지만, 반복적 프로세스를 복제한 세 참가자는 비슷한 체인에 도달했습니다. 유일한 차이점은 P11이 후속 작업자를 보여주기 위해 이전 결과 상위를 선택하는 또 다른 단계를 도입한 것입니다. 즉, 원래 단계는 변경되지 않았습니다. 그러나 FindFix-Verify를 복제한 세 학생은 상당히 다른 버전을 구현했습니다(그림 2): P14는 대부분 Bernstein et al.(2010)의 설명(예: 인적 오류를 줄이기 위해 Verify 단계에 투표 메커니즘 포함)을 따랐지만, Find 단계를 확장하여 훨씬 더 많은 유형의 쓰기 문제를 포함했습니다. 또한 LLM이 &quot;컴퓨터 코드에 대한 배경 지식을 가지고 있기&quot; 때문에 &quot;컴퓨터가 자연어보다 쉽게 이해할 수 있는 데이터 구조를 사용하여&quot; 프롬프트를 설계했습니다. 반면 P7은 단축할 수 있는 구문을 찾는 데만 Find 단계를 전담했고, 대신 앞의 단축 단계에서 발생한 문법적 오류를 수정하기 위해 Verify 단계를 구현했습니다. 그들은 &quot;LLM에는 [인간의 노력과 오류의 높은 분산]이라는 문제가 없다&quot;고 믿었기 때문에 의도적으로 디자인을 재구성했다고 설명했습니다. 그러나 이러한 믿음은 부정확할 수 있습니다. Just Initiation Iterative Little et al. (2010): 이전 직원이 생성한 직원 콘텐츠를 보여주세요 회사 세부 정보: {설명} 회사에 대한 새로운 회사 이름 아이디어 5개를 제공하세요: {새로운 이름 5개} P13: 이전 직원이 생성한 직원 콘텐츠를 보여주세요 다음 설명을 기반으로 회사에 대한 가능한 이름 5개를 제안하세요: {설명} {새로운 이름 5개} 회사 세부 정보: {설명} 지금까지 제안된 이름: {이전 이름} 회사에 대한 새로운 회사 이름 아이디어 5개를 제공하세요: {새로운 이름 5개} 이전 직원 {설명} 이전에 제안된 이름 5개: {이전 이름} 새로 생성된 이름 5개: {새로운 이름 5개} P3: 이전 직원이 생성한 직원 콘텐츠를 보여주세요(&quot;더 나은&quot; 이름 요청) {개념}에 대한 단편 소설의 잠재적인 제목 5개를 제공하세요. {새로운 제목 5개} {개념}에 대한 단편 소설의 잠재적인 제목 5개는 다음과 같습니다. {이전 제목} 더 좋고 창의적인 제목을 5개 더 제공하세요. {5개의 새로운 제목} P11: 이전 작업자가 생성한 작업자 콘텐츠 표시, 프로젝트 설명: {project_description} 목표: {need}에 대한 브레인스토밍. {n}개의 새로운 {need} 아이디어 목록: {n개의 새로운 아이디어} 프로젝트 설명: {project_description} 목표: {need}에 대한 브레인스토밍. 다음 아이디어는 품질이 낮은 예이므로 다음과 같은 일반적인 함정을 피하세요. {이전 아이디어} {n}개의 새로운 {need} 아이디어 목록: {n개의 새로운 아이디어} 반복 프로세스 프로젝트 설명: {project_description}. 다음 후보 {need} 아이디어 목록이 주어지면 최상의 {n}개 아이디어를 순위를 매기세요. 아이디어 목록: {idea_list} 상위 {n}개 아이디어: {상위 n개 새로운 아이디어} 찾기 수정 검증 B Bernstein et al. (2010): 단축할 세그먼트 찾기, 단축하여 수정, 오류가 있는 재작성 식별하여 검증. 문단의 의미를 변경하지 않고 단축할 수 있는 영역을 하나 이상 식별합니다. 강조 표시된 섹션을 편집하여 문단의 의미를 변경하지 않고 길이를 줄입니다. 찾기-수정-검증 중대한 스타일 오류가 있는 최소한 하나의 재작성을 선택하십시오. 문장의 의미를 상당히 변경하는 최소한 하나의 재작성을 선택하십시오. P14: 모든 유형의 오류가 있는 세그먼트를 찾고, 이러한 구문을 수정하여 수정하고, 수정 사항을 정확성에 따라 순위를 매겨 확인합니다. 입력은 텍스트입니다. 출력은 다음 형식의 텍스트 오류이며, 여기서 키는 오류 유형(철자, 문법, 단어 수, 어휘)이고 값은 해당 오류 목록입니다. 예: 찾기: I lvie in Charlotte I pley golf. 출력 &quot;spelling&quot;: [&#39;lvie&#39;, &#39;pley&#39;], &quot;punctuation&quot;: [&#39;period missing after Charlotte&#39;], &quot;wordiness&quot;: [&#39;I live in Charlotte and golf. &#39;]}. 다음 텍스트에서 이 찾기를 사용합니다. {input text} {output dict} 입력은 find의 사전과 문단입니다. 출력은 다음 형식의 텍스트 오류입니다.여기서 키는 오류 유형이고 값은 오류 문자, 해당 오류 및 수정 사항이 포함된 span 목록입니다.예: 입력 {&quot;spelling&quot;: [&#39;lvie&#39;]}, I lvie in Charlotte.수정: 출력 = {&quot;spelling&quot;: [{&#39;span&#39; (2,5), &#39;value&#39;: &#39;lvie&#39;, &#39;correction&#39;: &#39;ive&#39; 이 수정 사항을 다음 사전에서 사용한 다음 텍스트: {dict and text} {output dict} 입력은 이전 문장을 수정된 문장에 매핑하는 사전입니다.출력은 키가 원래 문장이고 값이 수정된 문장의 목록과 원래 문장과 수정된 문장의 문장 유사도인 사전입니다.예: {&#39;I lvie in Charlotte&#39;: [&#39;I live in Charlotte&#39;, 4.5/5], I pley golf: [&#39;I play golf&#39;, 4.5/5]}. {입력 문장} {출력 순위} P7: 줄일 세그먼트 찾기, 이 구문을 줄여서 수정하기, 문법적 오류를 수정하여 확인하기. 다음 텍스트에서 줄일 수 있는 세 개의 세그먼트를 찾으세요. 이 세그먼트는 텍스트에 있어야 합니다. 텍스트: {입력 텍스트} 세그먼트: 1. {출력 세그먼트} 의미를 변경하지 않고 다음 텍스트를 줄이세요. 텍스트: {세그먼트} 단축된 텍스트: {줄인 세그먼트} P10: 줄일 세그먼트 찾기, 이 구문을 줄여서 수정하기, (확인 안 함) 주어진 문장의 다음 부분은 불필요합니다: {텍스트} {불필요한 세그먼트} 주어진 문장과 줄일 내용이 주어졌을 때, 이 문장을 줄이세요: {세그먼트} 줄이기: {줄인 세그먼트} 다음 텍스트의 문법을 교정하세요. 텍스트: {수정된 텍스트} 수정된 텍스트: {문법적 텍스트} 그림 2: (A) 반복적 프로세스(Little 등, 2010) 및 (B) 찾기-수정-검증(Bernstein 등, 2010)에 대한 원래 파이프라인과 LLM 복제. P11만이 이전 결과를 어떻게 순위를 매기고 후속 단계에서 사용해야 하는지에 대한 조건을 추가하여 원래 반복적 프로세스 파이프라인에서 벗어났지만, 찾기-수정-검증을 복제하는 학생들은 모두 서로 다른 검증 단계를 가졌습니다(빨간색 상자로 표시). 가독성을 위해 체인을 약간 단순화했습니다. 인간의 노이즈와 마찬가지로 LLM의 비결정적 특성은 동일한 프롬프트가 여러 번 실행되는 경우(여러 작업자가 동일한 하위 작업을 완료하는 것과 유사) 다양한 결과로 이어질 수도 있습니다. 사실, 이전 작업에서는 LLM 노이즈를 제거하기 위해 Find-Fix-Verify에서 Verification과 유사한 다수결 투표를 적용했습니다(Wang et al., 2022b; Yao et al., 2023). 이는 이 단계가 정확히 동일한 문제를 해결하는 데 여전히 유용할 것임을 나타냅니다. 즉, 문제가 있는 다시 쓰기(이제 LLM으로 생성됨)를 제거하는 것입니다. P10도 유사한 추론으로 인해 Verify 단계를 제거했습니다. 반성: LLM을 사용하기 위한 작업별 및 파이프라인별 모범 사례를 수립합니다. LLM을 사용한 크라우드소싱 파이프라인의 다양한 구현은 학생들이 특정 작업을 완료하는 데 있어 이러한 모델의 성능과 필요한 지침의 양에 대해 다양한 가정을 가지고 있음을 보여줍니다. 실제로 LLM과 프롬프트 기술의 급속한 발전으로 LLM의 기능과 한계를 파악하고 특정 사용 사례에 어떻게 적용할 수 있는지 파악하는 것이 어렵습니다. 지속적으로 진화하는 LLM에 대한 일반적인 정신 모델을 형성하려고 하는 대신 실무자는 특정 사용 사례의 맥락에 따라 LLM 유용성에 대한 이해를 동적으로 조정하는 것이 더 유익할 수 있습니다. 이를 달성하기 위해 실무자는 LLM을 &quot;모든 분야의 잭이지만, 어느 분야나 몇몇 분야에는 능숙하지 않다&quot;(Kocoń et al., 2023)고 보는 사고방식을 채택하고, 체계적인 접근 방식을 사용하여 지침을 지정하고, 희소한 것에서 세부적인 것으로 점진적으로 이동할 수 있습니다. 실무자는 LLM이 모호한 요청을 해석할 수 있는 충분한 세계 지식을 보유하고 있다는 가정 하에 일반적이고 지정되지 않은 프롬프트를 사용하여 기준선을 설정하는 것으로 시작할 수 있습니다. Find-Fix-Verify의 맥락에서 오류 유형을 지정하지 않고 &quot;텍스트의 모든 오류를 출력하세요&quot;와 같은 상위 명령으로 Find 단계를 구현하는 것으로 충분할 수 있습니다. 그런 다음 전담 프롬프트 테스트(Ribeiro, 2023)에서 일반 프롬프트가 부족한 경우가 발견되면 실무자는 코너 케이스에 대한 텍스트 지침과 같은 보다 구체적인 지침을 통합하거나 프롬프트 앙상블 기법을 채택하도록 프롬프트를 조정할 수 있습니다(Pitis et al., 2023). 반면에 학생들은 자신의 경험과 과정 지침을 통해 이를 알고 있음에도 불구하고 LLM이 복제 연습 중에 확률적 생성을 수행한다는 사실을 간과한 것으로 보입니다. LLM의 비결정적 특성이 특히 체이닝 맥락에서 사용될 때 무시되는 경향이 있는 것을 관찰하는 것은 흥미롭습니다. 이러한 간과 사항은 프로토타입 체인 구조를 만드는 것과 각 하위 작업에 대한 개별 프롬프트를 미세 조정하는 것 사이의 균형에서 비롯될 수 있습니다(Wu et al., 2022a): LLM의 비결정론은 일반적으로 모델 신뢰도 또는 생성된 출력과 관련된 확률을 사용하여 표현되며, 학생들이 다음 하위 과제에 하나의 출력만 전달할 수 있는 경우 이는 2차적인 고려 사항이 될 수 있습니다. 이를 해결하기 위해 LLM 비결정론을 &quot;여러 LLM 작업자의 투표를 통해 노출된 노이즈&quot;로 도입하면 필요한 투표/주석 수에 대한 적응적 의사 결정과 같은 의견 불일치 완화 기술을 통합할 수 있습니다(Lin et al., 2014; Nie et al., 2020). 4.3 복제 효과: LLM 대 인간의 강점의 영향 그렇다면 LLM의 실제 강점과 약점은 무엇이며 복제된 LLM 체인에 어떤 영향을 미칩니까? 학생들의 구현 효과에 대한 성찰과 개선 아이디어를 탐구합니다. 놀랍지 않게도 효과적인 것으로 입증된 크라우드소싱 파이프라인은 여전히 인간의 역량과 다른 LLM의 고유한 역량을 수용하기 위해 약간의 재설계가 필요할 수 있습니다. 이러한 관찰은 이전 연구(Wu et al., 2022b; Webson et al., 2023)에서의 논의와 일치하지만 복제를 통해 작업 공간을 포괄적으로 탐색하면서 이제 두 가지 중요한 패턴이 더욱 분명해졌습니다. LLM은 명시적 정보 수집이 필요함 여러 크라우드소싱 파이프라인에는 암묵적 정보 선택 및 통합이 필요합니다. 예를 들어, Map-Reduce에서 Reduce 단계를 수행하는 작업자는 최종 문단을 일관성 있게 만들기 위해 불필요한 정보를 제거해야 했습니다. 필요성에도 불구하고 선택을 위한 그러한 명시적 하위 작업을 포함하는 파이프라인은 거의 없습니다. 이는 인간이 암묵적 정보 필터링, 재순위 지정 및 선택이 가능하기 때문일 수 있습니다(Pirolli 및 Card, 1999; Sperber 및 Wilson, 1986; Marchionini, 1995). 특정 부분이 품질이 낮거나, 제자리에 없거나, 중복된다는 것이 분명하면 인간은 합리적인 인지 부하를 유지하기 위해 불필요한 부분을 사전에 제거합니다. 반면에 LLM은 정보 수집에 어려움을 겪고 지속적으로 맥락을 축적하고 혼합된 품질의 출력을 생성하는 경향이 있습니다. 학생들은 세 가지 수준에서 이러한 결함을 관찰하고 가능한 변경 사항을 제안했습니다. • 품질이 낮은 중간 결과를 완화하지 못함. 예를 들어, Price-Divide-Solve를 사용하여 문단을 작성할 때 P4는 서로 다른 하위 작업에서 상충되는 정보조차도 최종 작성물에 통합되어 불일치가 발생한다는 것을 발견했습니다(예: 대학 마스코트를 스코틀랜드인과 올빼미로 주장). 여러 학생은 &quot;모델의 예측 불가능성을 줄이기 위해&quot; 중간 품질 관리가 필요하다고 강조했습니다(P13, 반복적 프로세스). • 하위 작업의 하위 집합을 선택적으로 수행하지 못함. 이는 HumorTool에서 가장 잘 드러납니다. 원래 설계에서는 작업자가 하위 작업의 하위 집합(총 8개)을 스스로 선택하고 정렬하여 효과적인 흐름을 만들어야 했습니다. 이를 복제한 4명의 학생 중에서 P17만이 하위 작업에 &quot;이러한 마이크로 작업의 실행 순서에 명확한 구조가 없다&quot;는 것을 알아차리고 4개의 하위 작업 체인을 성공적으로 구현했습니다. 다른 학생들은 8개의 하위 작업이 너무 많은 정보를 모았다는 데 동의했고, P18은 나중에 &quot;단계가 그렇게 엄격한 순서가 아니어야 한다&quot;고 생각했습니다. • 하나의 하위 작업에서 여러 요구 사항의 균형을 맞추지 못함. 하나의 LLM 프롬프트에 과도한 요구 사항이 있으면 충돌이 발생할 수도 있습니다. 앞서 언급한 유머 도구 사례에서 너무 많은 하위 작업의 결과를 통합하면 특정 옵션이 다른 옵션을 압도할 수 있습니다. 예를 들어, LLM은 &quot;농담을 냉소적으로 바꾸는 데 집중하여 농담에서 유머를 없앨 수 있습니다&quot;(P5). 마찬가지로 P14(찾기-수정-확인)는 여러 문제를 동시에 검색하기 위해 찾기 단계(그림 2)를 구현했는데, 이로 인해 LLM은 철자 오류와 단어 오류 문제를 우선시했습니다. 전반적으로 LLM에게 가장 중요한 기준을 명시적으로 명시하는 것이 중요해 보입니다. LLM은 인간보다 비교 기반에 더 민감합니다. 이전 연구에서 관찰했듯이 LLM은 여전히 사소한 의역에 민감합니다(예: 찾기-수정-확인의 P7은 프롬프트에서 &quot;단편&quot;, &quot;절&quot;, &quot;하위 문자열&quot; 등에서 다른 단어를 프로토타입화했습니다). 그러나 반면에 LLM은 비교 기반 지침에 상당히 반응합니다. 예시로 반복 프로세스를 사용하겠습니다. Little et al.(2010)은 원래 설계에서 앵커링 편향이 파이프라인의 고유한 한계라고 보고했습니다. &quot;아마도 크라우드워커가 기존 아이디어를 반복하고 개선하기 때문에 분산이 낮을 것입니다.&quot; 이 파이프라인을 복제한 세 학생 모두 비슷한 관찰을 했지만 이러한 편향은 간단한 지침만으로도 완화될 수 있다는 것을 발견했습니다. 예를 들어, P11은 처음에 파이프라인이 &quot;특정 주제로 수렴하는 경향이 있다&quot;고 관찰했지만 간단한 프롬프트로 모델을 리디렉션할 수 있었습니다. &quot;다음 아이디어는 품질이 낮은 예입니다. 이러한 일반적인 함정을 피하십시오.&quot; 마찬가지로 P3은 단순히 &quot;초기 세트와 다른 출력을 요청하는 것&quot;이 얼마나 효과적인지에 즐겁게 놀랐습니다. &quot;처음에는 예를 제공하면 모델이 동일한 형식의 예만 생성하도록 &#39;프라이밍&#39;될까 걱정했지만 실제로는 문제가 아닌 것 같습니다.&quot; 이처럼 간단한 지침은 개인적 편견에 갇힌 군중 작업자들에게는 효과가 없을 가능성이 높습니다(Wu et al., 2021). — &quot;다른&quot; 및 &quot;다양한&quot;과 같은 형용사에 대한 이러한 민감성은 추가 탐색을 보증합니다. 한 동료 평가자는 &quot;제안을 할 수 있다면 더 행복하고, 더 둔하고, 더 재미있는 제목을 요청할 수 있으며, 이는 전통적인 크라우드소싱 방법을 넘어섭니다.&quot;라고 제안하여 이를 강조했습니다. 이 제안은 LLM이 특정 차원에 초점을 맞춘 개선된 버전을 생성하기 위해 자신의 출력을 비판하는 Self-Refine(Madaan et al., 2023)과 같은 기존의 프롬프트 기술과 일치합니다. 반성: 지시 조정의 효과를 조사하고 보완성을 위해 인간을 훈련합니다. 인간과 LLM 간의 차이점은 예상되지만 이러한 차이점 중 일부가 LLM이 인간 행동을 모방하도록 훈련하는 목표에서 발생하는 방식은 흥미롭습니다. 예를 들어, 인간 피드백을 통한 강화 학습(RLHF Ouyang et al., 2022)과 같은 방법은 인간의 선호도를 사용하여 LLM이 지시를 따르는 능력을 향상시킵니다. 이를 통해 LLM은 추상적 비교 명령에 따라 콘텐츠를 반복할 수 있었을 것입니다. 인간은 종종 인지적 편향에 갇히거나 모호하거나 애매한 지시에 어려움을 겪습니다(Gershman et al., 2015). 그렇긴 하지만, 이러한 모델도 훈련에 의해 편향되고 양극화된 입장을 가질 수 있기 때문에 LLM 세대가 항상 이런 경우에 더 나은지는 불분명합니다(Jiang et al., 2022; Santurkar et al., 2023). 이 관찰에서 분기하여 LLM 훈련 스키마의 잠재적인 &quot;부작용&quot;을 탐구하는 것이 흥미로울 것입니다. 이전 연구에서는 few-shot 대 zero-shot 기능 간의 균형과 다면적인 인간 피드백으로 LLM을 훈련해야 할 필요성을 강조했습니다(Wu et al., 2023). LLM이 명시적인 정보 수집을 필요로 한다는 점을 고려할 때, 또 다른 가치 있는 조사 방향은 지시의 완전성과 명확성일 것입니다. 대부분의 기존 교육 튜닝 데이터 세트는 고품질의 정확한 교육을 우선시하기 때문에(Longpre et al., 2023), LLM이 관련 없는 정보가 포함된 잘 정의되지 않은 프롬프트나 교육에 어떻게 반응할지 불분명합니다. 인간이 설계 요구 사항을 이끌어내기 위해 사용하는 왕복 대화와 유사한 &quot;교육 체인 설명&quot; 접근 방식을 사용하여 LLM을 어떻게 훈련할 수 있는지 살펴보는 것이 흥미로울 수 있습니다. 예를 들어, 인간이 최상위 기준을 명확히 하는 하위 작업을 통합하면 LLM이 여러 요구 사항을 효과적으로 처리하는 능력을 향상시킬 수 있습니다. 강점의 분할은 또한 인간-LLM 상호 보완성을 요구합니다. 인간이나 LLM이 모든 하위 작업을 완료하는 대신, 다양한 &quot;근로자&quot;의 혼합 간에 효과적인 작업 위임이 유용할 수 있습니다. 예를 들어, HumorTool의 P15는 LLM 체인의 부분적인 효과를 알아챘습니다. &quot;뉴스 헤드라인의 관련 속성을 추출하고 관련 개념을 브레인스토밍하는&quot; 데는 뛰어났지만 실제 농담으로 변환하는 데는 실패했습니다. 따라서 LLM 강점을 보완하는 기술을 식별하고 개발하도록 인간을 명시적으로 훈련하는 것은 추구하기에 흥미로운 방향이 될 수 있습니다(Bansal et al., 2021; Ma et al., 2023; Liu et al., 2023). 이러한 보완성은 인간과 다양한 LLM 간에 발생할 수 있습니다. 예를 들어, 반복적 프로세스의 P3에서는 약한 모델을 단독으로 또는 파이프라인에서 사용하면 성능이 떨어지는 반면, &quot;이전 예제로 [약한 모델이 반복할] 더 강력한 모델의 예제를 제공했을 때 성능이 극적으로 향상되었습니다.&quot; 이 관찰은 적절한 작업이 주어지면 덜 최신 모델이라도 효과적인 팀원이 될 수 있음을 반영합니다. &quot;모든 모델은 틀렸지만 일부는 유용합니다.&quot; (Box, 1976). 4.4 복제 과제: 다중 모달 규정 대 텍스트 지침 LLM 복제의 과제를 되돌아볼 때, 4명의 학생이 구조화된 입력/출력 형식을 만드는 것이 어렵다고 언급했습니다. 예를 들어, P(Find-Fix-Verify 복제)는 프롬프트에 제약 조건을 포함하는 것을 설명했습니다. &quot;이 세그먼트는 텍스트에 있어야 합니다.&quot; 그들은 반성에서 그 중요성을 강조했습니다. &quot;이 프롬프트가 없으면 반환된 세그먼트는 종종 원래 텍스트를 기반으로 극적으로 재구성된 문장이 되어 수정 단계 후에 원래 텍스트에 다시 삽입하기 어렵습니다.&quot; 마찬가지로, 과제 의역의 P6은 &quot;이러한 프롬프트의 가장 큰 약점은 특히 파이프라인 모델의 경우 구조화된 정보를 추출하는 데 어려움이 있다는 것입니다.&quot;라고 말했습니다. LLM만큼(아니면 그보다 더) &quot;생성적&quot;인 인간 근로자가 구조화된 입력과 출력을 생성할 수 있는 이유를 고려해 볼 가치가 있습니다. 본질적으로 크라우드소싱 파이프라인의 모든 LLM 복제는 부분적입니다. 할당은 크라우드소싱 파이프라인의 지침 복제에만 초점을 맞추고 크라우드소싱의 다른 구성 요소는 무시합니다. 구체적으로 거의 모든 크라우드소싱 파이프라인에는 본질적으로 사용자 인터페이스에서 도입된 제약 조건이 포함됩니다. 예를 들어, 찾기-수정-확인에서 찾기 단계는 크라우드워커에게 텍스트에서 마우스 선택을 통해 약어 영역을 식별하도록 촉구하여 세그먼트가 원본 문서에서 정확하게 추출되도록 보장합니다. 마찬가지로 He et al. (2015)은 주석자가 제한된 답변 길이와 미리 정해진 질문 옵션이 있는 스프레드시트 인터페이스에서 질문과 답변에 레이블을 지정하도록 요구했습니다. 이를 통해 모든 답변이 예측 가능한 질문에 대한 짧은 문구가 될 수 있습니다. 한편, LLM 모듈/작업자는 텍스트 지침에 의해서만 구동되므로 UI 제한이 없는 것을 보완하기 위한 추가 규정이 필요합니다. 일부 학생들은 구문적 제약의 텍스트 버전을 제시했습니다. 예를 들어, &quot;[MASK] 토큰 사용과 같은 훨씬 더 엄격한 템플릿을 허용하는 프롬프팅 시스템은 크라우드워크 스타일의 파이프라인을 훨씬 더 쉽게 만들 것입니다.&quot;(P11, 반복적 프로세스) 다른 방법도 가능할 수 있습니다. 예를 들어, 생성 작업을 객관식 작업으로 변환하여 LLM이 단일 선택만 출력하도록 할 수 있습니다. 반성: 지시 양식의 정렬과 인간 시뮬레이션에서의 역할. 다중 모달 기반 모델(OpenAI, 2023; Ramesh et al., 2022)이 등장하면서 지시를 따르는 측면에서 인간과 모델 간의 정렬을 고려할 뿐만 아니라 인간의 직관과 일치하는 최적의 지시 양식을 탐색하는 것도 중요해졌습니다. 예를 들어, LLM은 시각화를 통한 일부 상호 작용을 자동화했지만, 이전 연구에서는 사용자가 자연어 명령에서 모호한 참조를 해결하기 위해 마우스 이벤트가 필요하다는 것을 발견했습니다(&quot;이 막대를 파란색으로 만들어라&quot;(Wang et al., 2022c; Kumar et al., 2017)). 이러한 동작을 텍스트 지침으로 변환하는 대신 시각적 주석을 활용하는 방향으로 전환하는 것이 더 유리할 것입니다. 이러한 과제는 LLM의 실제 적용에도 영향을 미칩니다. LLM이 인간을 충실하게 시뮬레이션할 수 있는지에 대한 지속적인 논의에서 연구자들은 LLM을 연구 지침과 설계를 효율적으로 개선하기 위한 파일럿 연구 사용자로 사용하는 것의 타당성을 조사하기 시작했습니다(Hämäläinen et al., 2023). 실제로 이 방향은 가치가 있습니다. 그림 2와 마찬가지로 인간과 LLM 모두 작업을 완료하기 위해 &quot;프롬프트&quot;가 필요합니다. 그럼에도 불구하고, 우리의 연구 결과는 이러한 전환이 간단하지 않을 수 있음을 나타냅니다. 한편으로 LLM은 텍스트 지침에만 응답하기 때문에 LLM 지침을 인간을 위한 다중 모달 제약으로 매핑하기 위한 중요한 후처리 단계가 필요할 수 있습니다. 예를 들어, &quot;정확한 문장 추출&quot;이라는 지침은 특정 구문을 선택하는 인터페이스 디자인에 매핑해야 할 수 있으며, &quot;주요 아이디어를 의역&quot;은 직접 반복을 막고 사용자가 직접 입력하도록 장려하기 위해 텍스트에서 복사-붙여넣기를 비활성화해야 합니다. 반면, 섹션 4.3에서 언급했듯이 LLM과 인간은 동일한 지침에 다르게 반응할 수 있습니다. 이러한 불일치로 인해 LLM은 지침에만 기반한 작업에 대한 인간의 반응을 시뮬레이션하는 데에도 신뢰할 수 없습니다. LLM은 연구 설계자가 고수준 요구 사항(예: 수집할 인간 반응 유형 결정)을 반영하는 데 도움이 될 수 있다고 생각하지만, 문자 그대로의 지침을 재설계해야 합니다. LLM을 사용하여 사용자 연구 설계의 어떤 부분을 프로토타입으로 만들 수 있는지 탐구하는 것은 흥미로운 미래 방향인 듯합니다. 5 토론 및
--- CONCLUSION ---
이 연구에서 우리는 LLM을 사용하여 과정 과제를 통해 크라우드소싱 파이프라인을 복제할 수 있는지 연구합니다. 우리는 현대 모델이 실제로 이러한 고급 &quot;인간 계산 알고리즘&quot;에서 인간 주석을 시뮬레이션하는 데 사용될 수 있음을 보여주지만, 복제의 성공과 효과는 하위 작업의 특성에 따라 크게 다릅니다. 또한 LLM의 성능과 실패 모드는 직관적이지 않을 수 있으며, 인간 근로자가 데이터에 안정적으로 주석을 달 수 있도록 하는 다중 모드 신호를 활용할 수 있는 능력이 부족합니다. 우리의 정성적 결과는 두 가지 중요한 점을 나타냅니다. 첫째, 기존 파이프라인이나 워크플로 내에서 LLM을 검토하면 다양한 파이프라인 구성 요소에 요구 사항이 다르기 때문에 강점과 약점을 보다 직접적으로 이해할 수 있습니다. 둘째, LLM을 사용하여 인간 계산을 시뮬레이션할 때 인간과 LLM 출력 간의 내재적 일치에 초점을 맞출 뿐만 아니라 추가 스캐폴드를 정렬하는 것도 고려하는 것이 유리합니다. 여기에는 인간의 지시에 대한 오해, 인간 응답의 노이즈, 인간에 대한 다중 모드 제약을 통합해야 할 필요성과 같은 과제를 해결하는 기존 기술을 적용하는 것이 포함됩니다. 그래도 과정 과제 설정으로 인해 LLM 체인 품질은 학생들의 노력과 전문성에 따라 크게 달랐습니다. 또한 제한된 표본 크기를 감안하면 양적 분석은 제한된 의미를 도출했을 것입니다. 향후 연구에서는 크라우드소싱 파이프라인의 어떤 구성 요소가 LLM 주석을 사용하여 이점을 얻을 수 있는지, 그리고 인간이 계속해서 주석을 달아야 하는지에 대한 보다 체계적인 조사를 살펴볼 수 있습니다. 교육 관점에서, 학생들이 LLM과 상호 작용하도록 하는 것이 실제로 이러한 모델에 대한 자신감을 교정하는 데 도움이 되었다는 것을 발견했습니다. 많은 학생들이 LLM이 예상했던 만큼 효과적이고 안정적으로 수행되지 않을 때 좌절감을 표했습니다. 이 작업이 학생들이 LLM과 상호 작용하고 이러한 모델의 실수를 인식하도록 하여 건설적인 학습 과정을 촉진하고 LLM에 대한 과도한 의존을 방지할 수 있는 향후 탐색에 영감을 줄 수 있기를 바랍니다. 과제 설계와 학생 응답은 https://github.com/tongshuangwu/11m-crowdsourcing-pipeline에서 오픈 소스로 제공합니다. 참고문헌 PromptChainer: 시각적 흐름 빌더를 사용하여 복잡한 AI 기반 흐름을 쉽게 만듭니다. https:// promptchainer.io/. 액세스: 2023-7-2. Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua Gubler, Christopher Rytting, David Wingate. 2022. Out of one, many: 언어 모델을 사용하여 인간 샘플 시뮬레이션. ArXiv 사전 인쇄본, abs/2209.06899. Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, Daniel Weld. 2021. 전체가 부분을 초과하는가? 보완적 팀 성과에 대한 AI 설명의 효과. 2021년 CHI 컴퓨팅 시스템의 인간 요인 컨퍼런스 회의록, CHI &#39;21, 뉴욕, 뉴욕, 미국. 컴퓨팅 기계 협회. Michael S Bernstein. 2013. 군중 구동 시스템. KI-Künstliche Intelligenz, 27:69–73. Michael S Bernstein, Greg Little, Robert C Miller, Björn Hartmann, Mark S Ackerman, David R Karger, David Crowell, Katrina Panovich. 2010. Soylent: 군중이 있는 워드 프로세서. 사용자 인터페이스 소프트웨어 및 기술에 대한 제23회 연례 ACM 심포지엄 회의록, 313–322쪽. George EP Box. 1976. 과학 및 통계. Journal of the American Statistical Association, 71(356):791– 799. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상. Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. ArXiv 사전 인쇄본, abs/2303.12712. Harrison Chase. langchain: 구성 가능성을 통해 LLM으로 애플리케이션 구축. Justin Cheng, Jaime Teevan, Shamsi T. Iqbal, and Michael S. Bernstein. 2015. Break it down: A comparison of macro- and microtasks. 2015년 4월 18일-23일, 대한민국 서울에서 열린 제33회 ACM 컴퓨팅 시스템의 인적 요소에 관한 연례 컨퍼런스의 진행 내용, CHI 2015, 4061-4064쪽. ACM. Lydia B Chilton, James A Landay, Daniel S Weld. 2016. Humortools: 뉴스 풍자 쓰기를 위한 마이크로태스크 워크플로. 텍사스 엘파소: ACM. Lydia B. Chilton, Greg Little, Darren Edge, Daniel S. Weld, James A. Landay. 2013. Cascade: 크라우드소싱 분류법 생성. 2013년 ACM SIGCHI 컴퓨팅 시스템의 인적 요소에 관한 컨퍼런스, CHI &#39;13, 프랑스 파리, 2013년 4월 27일-5월 2일, 1999-2008쪽. ACM. Tyna Eloundou, Sam Manning, Pamela Mishkin, Daniel Rock. 2023. Gpts는 gpts입니다: 대규모 언어 모델의 노동 시장 영향 잠재력에 대한 초기 고찰. ArXiv 사전 인쇄본, abs/2303.10130. Samuel J Gershman, Eric J Horvitz, Joshua B Tenenbaum. 2015. Computational Reasonity: A Converging Paradigm for Intelligence in Brains, Minds, and Machines. Science, 349(6245):273–278. Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt는 텍스트 주석 작업에서 군중 작업자보다 성능이 뛰어납니다. ArXiv 사전 인쇄본, abs/2303.15056. Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023. 합성 HCI 연구 데이터 생성에서 대규모 언어 모델 평가: 사례 연구. 2023년 CHI 컴퓨팅 시스템의 인적 요소에 대한 컨퍼런스 회의록, 1-19페이지. Luheng He, Mike Lewis, Luke Zettlemoyer. 2015. 질문-답변 기반 의미적 역할 라벨링: 자연어를 사용하여 자연어에 주석을 달기. 2015년 자연어 처리의 경험적 방법에 대한 컨퍼런스 회의록, 643-653페이지, 포르투갈 리스본. 계산언어학 협회. Jeff Howe et al. 2006. 크라우드소싱의 부상. Wired 매거진, 14(6):176-183. Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, Yixin Zhu. 2022. 사전 훈련된 언어 모델에서 성격 평가 및 유도. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal. 2022. 분해된 프롬핑: 복잡한 작업 해결을 위한 모듈식 접근법. ArXiv 사전 인쇄본, abs/2210.02406. Joy Kim, Sarah Sterman, Allegra Argent Beal Cohen, Michael S Bernstein. 2017. 기계 소설: 반성과 수정을 통한 복잡한 작업의 크라우드소싱. 컴퓨터 지원 협동 작업 및 소셜 컴퓨팅에 대한 2017년 ACM 컨퍼런스 회의록, 233~245페이지. Aniket Kittur, Boris Smus, Susheel Khamkar, Robert E Kraut. 2011. Crowdforge: 복잡한 작업의 크라우드소싱. 사용자 인터페이스 소프트웨어 및 기술에 대한 제24회 연례 ACM 심포지엄 회의록, 43~52페이지. Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz 등 2023. Chatgpt: 모든 거래의 잭, 아무 것도 마스터하지 못합니다. Information Fusion, 101861페이지. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo 및 Yusuke Iwasawa. 2022. 대규모 언어 모델은 제로샷 추론기입니다. 신경 정보 처리 시스템의 발전, 35:2219922213. 아난드 쿨카르니(Anand Kulkarni), 매튜 캔(Matthew Can), 비욘 하트만(Björn Hartmann). 2012. turkomatic을 사용한 공동 크라우드소싱 워크플로. 컴퓨터 지원 협동 작업에 대한 acm 컨퍼런스의 진행 사항, 1003-1012페이지. Abhinav Kumar, Barbara Di Eugenio, Jillian Aurisano, Andrew Johnson, Abeer Alsaiari, Nigel Flowers, Alberto Gonzalez, Jason Leigh. 2017. 탐색적 데이터 시각화 대화를 위한 다중 모드 공참조 해결을 향해: 컨텍스트 기반 주석 및 제스처 식별. 대화의 의미론 및 실용성에 대한 21번째 워크숍(SemDial 2017-SaarDial)(2017년 8월), 48권. Edith Law와 Haoqi Zhang. 2011. 대규모 협업 계획을 향해: 인간 계산을 사용하여 고수준 검색 쿼리에 답변. 2011년 8월 7-11일 미국 캘리포니아주 샌프란시스코에서 열린 제25회 AAAI 인공지능 컨퍼런스 논문집에서. AAAI 출판사. Christopher Lin, Daniel Weld 외. 2014. To re (label), or not to re (label). AAAI 인간 컴퓨팅 및 크라우드소싱 컨퍼런스 논문집, 2권, 151-158쪽. Greg Little, Lydia B Chilton, Max Goldman, Robert C Miller. 2010. Exploring iterative and parallel human computing procedures. ACM SIGKDD 워크숍 인간 컴퓨팅 논문집, 68-76쪽. Michael Xieyang Liu, Advait Sarkar, Carina Negreanu, Benjamin Zorn, Jack Williams, Neil Toronto, Andrew D Gordon. 2023. &quot;내가 말하길 원하는 것&quot;: 최종 사용자 프로그래머와 코드를 생성하는 대규모 언어 모델 간의 추상화 격차 해소. 2023년 CHI 컴퓨팅 시스템의 인적 요소에 대한 컨퍼런스 회의록, 1-31페이지. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. 플랜 컬렉션: 효과적인 지침 조정을 위한 데이터 및 방법 설계. ArXiv 사전 인쇄본, abs/2301.13688. Qianou Ma, Tongshuang Wu, and Kenneth Koedinger. 2023. AI가 더 나은 프로그래밍 파트너인가? humanhuman 페어 프로그래밍 대 human-AI 페어 프로그래밍. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. 자체 개선: 자체 피드백을 통한 반복적 개선. ArXiv 사전 인쇄본, abs/2303.17651. Gary Marchionini. 1995. 전자 환경에서의 정보 탐색. 9. 케임브리지 대학 출판부. Yixin Nie, Xiang Zhou, Mohit Bansal. 2020. 자연어 추론 데이터에 대한 집단적 인간 의견에서 무엇을 배울 수 있을까? 2020 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 9131~9143쪽, 온라인. Association for Computational Linguistics. OpenAI. 2023. Gpt-4 기술 보고서. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35:27730-27744. Peter Pirolli와 Stuart Card. 1999. 정보 수집. 심리학 리뷰, 106(4):643. Silviu Pitis, Michael R Zhang, Andrew Wang, Jimmy Ba. 2023. 대규모 언어 모델을 위한 부스트 프롬프트 앙상블. ArXiv 사전 인쇄본, abs/2304.05970. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis. 2022. 언어 모델의 구성성 갭 측정 및 좁히기. ArXiv 사전 인쇄본, abs/2210.03350. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. ArXiv 사전 인쇄본, abs/2204.06125. Daniela Retelny, Michael S Bernstein, Melissa A Valentine. 2017. 워크플로우가 충분할 수는 없다: 크라우드소싱 워크플로우가 복잡한 작업을 제약하는 방식. ACM 인간-컴퓨터 상호 작용 회의록, 1(CSCW):1–23. Marco Tulio Ribeiro. 2023. 소프트웨어를 테스트하는 것처럼 언어 모델(및 프롬프트) 테스트. Medium. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto. 2023. 언어 모델은 누구의 의견을 반영하는가? ArXiv 사전 인쇄본, abs/2303.17548. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao. 2023. Reflexion: 언어적 강화 학습을 통한 언어 에이전트. Dan Sperber와 Deirdre Wilson. 1986. Relevance: Communication and cognition, 142권. Citeseer. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul F. Christiano. 2020. 인간 피드백을 통한 요약 학습. 신경 정보 처리 시스템의 발전 33: 신경 정보 처리 시스템 연례 컨퍼런스 2020, NeurIPS 2020, 2020년 12월 6-12일, 가상. Petter Törnberg. 2023. Chatgpt-4는 제로샷 학습으로 정치적 트위터 메시지에 주석을 달 때 전문가와 군중 작업자보다 성능이 뛰어납니다. ArXiv 사전 인쇄본, abs/2304.06588. Vasilis Verroios와 Michael S Bernstein. 2014. 컨텍스트 트리: 로컬 뷰에서 글로벌 이해의 군중 소싱. 인간 계산 및 군중 소싱에 대한 두 번째 AAAI 컨퍼런스에서. Veniamin Veselovsky, Manoel Horta Ribeiro, Robert West. 2023. 인공 지능: 군중 작업자는 텍스트 생성 작업에 대규모 언어 모델을 널리 사용합니다. ArXiv 사전 인쇄본, abs/2306.07899. Xuezhi Wang, Haohan Wang, Diyi Yang. 2022a. NLP 모델의 견고성 측정 및 개선: 설문 조사. 2022년 북미 컴퓨터 언어학회 학술대회 논문집: 인간 언어 기술, 4569-4586쪽, 미국 시애틀. 컴퓨터 언어학회. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou. 2022b. 자기 일관성은 언어 모델에서 사고의 사슬 추론을 개선합니다. ArXiv 사전 인쇄본, abs/2203.11171. Yun Wang, Zhitao Hou, Leixian Shen, Tongshuang Wu, Jiaqi Wang, He Huang, Haidong Zhang, Dongmei Zhang. 2022c. 자연어 기반 시각화 저술을 향해. IEEE 시각화 및 컴퓨터 그래픽스 저널, 29(1):1222–1232. Albert Webson, Alyssa Marie Loo, Qinan Yu, and Ellie Pavlick. 2023. 언어 모델이 프롬프트를 따르는 데 있어 인간보다 나쁠까? 복잡하다. ArXiv 사전 인쇄본, abs/2301.07085. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837. Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J Cai. 2022a. Promptchainer: 시각적 프로그래밍을 통해 대규모 언어 모델 프롬프트 체인화. CHI 컴퓨팅 시스템의 인적 요소에 대한 컨퍼런스에서 확장된 초록, 1-10페이지. Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel Weld. 2021. Polyjuice: 모델을 설명하고, 평가하고, 개선하기 위한 반사실적 생성. 제59회 계산 언어학 협회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스(제1권: 장문 논문), 6707-6723페이지, 온라인. 계산 언어학 협회. Tongshuang Wu, Michael Terry, Carrie Jun Cai. 2022b. Ai 체인: 대규모 언어 모델 프롬프트를 체인으로 연결하여 투명하고 제어 가능한 인간-AI 상호 작용. 2022년 CHI 컴퓨팅 시스템의 인적 요소에 대한 컨퍼런스에서 진행된 회의록, 1-22페이지. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, Hannaneh Hajishirzi. 2023. 세분화된 인간 피드백은 언어 모델 학습에 더 나은 보상을 제공합니다. ArXiv 사전 인쇄본, abs/2306.01693. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan. 2023. 생각의 나무: 대규모 언어 모델을 사용한 의도적인 문제 해결. ArXiv 사전 인쇄본, abs/2305.10601. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, Geoffrey Irving. 2019. 인간 선호도에서 언어 모델 미세 조정. ArXiv 사전 인쇄본, abs/1909.08593.
