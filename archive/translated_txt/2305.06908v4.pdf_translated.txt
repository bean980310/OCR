--- ABSTRACT ---
잡음 제거 확산 확률 모델(DDPM)은 음성 합성에 유망한 성능을 보였습니다. 그러나 높은 샘플 품질을 달성하기 위해 많은 반복 단계가 필요하여 추론 속도가 제한됩니다. 샘플링 속도를 높이는 동시에 샘플 품질을 유지하는 것은 어려운 과제가 되었습니다. 이 논문에서는 단일 확산 샘플링 단계를 통해 음성 합성을 달성하는 동시에 높은 오디오 품질을 달성하는 일관성 모델 기반 음성 합성 방법인 CoMoSpeech를 제안합니다. 일관성 제약 조건을 적용하여 잘 설계된 확산 기반 교사 모델에서 일관성 모델을 추출하여 궁극적으로 추출된 CoMoSpeech에서 우수한 성능을 얻습니다. 실험 결과 단일 샘플링 단계로 오디오 녹음을 생성함으로써 CoMoSpeech는 단일 NVIDIA A100 GPU에서 실시간보다 150배 이상 빠른 추론 속도를 달성하는데, 이는 FastSpeech2와 비슷하여 확산 샘플링 기반 음성 합성을 진정으로 실용적으로 만듭니다. 한편, 텍스트-음성 합성과 노래 음성 합성에 대한 객관적 및 주관적 평가는 제안된 교사 모델이 최상의 오디오 품질을 제공하고, 1단계 샘플링 기반 CoMoSpeech가 다른 기존 다단계 확산 모델 기준선보다 더 좋거나 비슷한 오디오 품질로 최상의 추론 속도를 달성한다는 것을 보여줍니다. 오디오 샘플과 코드는 https://comospeech.github.io/에서 제공됩니다. 키워드: 텍스트-음성, 노래 음성 합성, 확산 모델, 일관성 모델
--- METHOD ---
, CoMoSpeech는 단일 확산 샘플링 단계를 통해 음성 합성을 달성하는 동시에 높은 오디오 품질을 달성합니다. 일관성 제약 조건은 잘 설계된 확산 기반 교사 모델에서 일관성 모델을 추출하는 데 적용되며, 궁극적으로 추출된 CoMoSpeech에서 우수한 성능을 제공합니다. 우리의
--- EXPERIMENT ---
s는 단일 샘플링 단계로 오디오 녹음을 생성함으로써 CoMoSpeech가 단일 NVIDIA A100 GPU에서 실시간보다 150배 이상 빠른 추론 속도를 달성한다는 것을 보여줍니다. 이는 FastSpeech2와 비슷하여 확산 샘플링 기반 음성 합성을 진정으로 실용적으로 만듭니다. 한편, 텍스트 음성 합성과 노래 음성 합성에 대한 객관적 및 주관적 평가는 제안된 교사 모델이 가장 좋은 오디오 품질을 제공하고 1단계 샘플링 기반 CoMoSpeech가 다른 기존 다단계 확산 모델 베이스라인보다 더 좋거나 비슷한 오디오 품질로 가장 좋은 추론 속도를 달성한다는 것을 보여줍니다. 오디오 샘플과 코드는 https://comospeech.github.io/에서 사용할 수 있습니다. 키워드: 텍스트 음성 합성, 노래 음성 합성, 확산 모델, 일관성 모델 소개 음성 합성 Tan et al. [2021]은 인간의 사실적인 오디오를 생성하는 것을 목표로 하며 텍스트 음성(TTS) Taylor [2009], Shen et al. [2023] 및 노래 음성 합성(SVS) Nishimura et al. [2016] 인간-기계 상호작용 및 엔터테인먼트 분야에서 응용 프로그램이 증가함에 따라 작업이 증가했습니다. 음성 합성의 주류는 딥 신경망(DNN) 기반 방법인 Wang et al. [2017] Kim et al. [2021]에 의해 주도되었으며, 일반적으로 2단계 파이프라인이 채택되었습니다 Ren et al. [2019] Lu et al. [2020], 여기서 음향 모델은 먼저 텍스트 및 기타 제어 정보를 음향 특징(예: 멜 스펙트로그램)으로 변환한 다음 보코더가 음향 특징을 가청 파형으로 추가로 변환합니다. 2단계 파이프라인은 프레임으로 표현되는 음향 특징이 짧은 텍스트를 높은 샘플링 주파수로 긴 오디오로 변환하는 일대다 매핑 문제(잘못된 위치 또는 잘못된 조건 문제) Bertero et al. [1988]를 완화하는 &quot;릴레이&quot; 역할을 효과적으로 하기 때문에 상당한 성공을 거두었습니다. 음향 모델(일반적으로 멜 스펙트로그램)에서 생성된 음향 특징의 품질은 합성된 음성의 품질에 결정적인 영향을 미칩니다.업계에서 널리 사용되는 접근 방식, 예: **연락 저자: Wei Xue {weixue@ust.hk}, Yike Guo {yikeguo@ust.hk} ACM MM 2023 수락.5.4.Better 4.4.CoMoSpeech 4.FastSpeechDiffGAN-TTS MOS(오디오 품질) 3.3.☐ ● ProDiff 3.Our teacher Grad-TTS DiffSpeech 3.NFE(추론 속도) Faster 그림 1: 다양한 TTS 시스템의 오디오 품질 및 추론 속도 비교.자세한 내용은 표 1에 나와 있으며 SVS의 경우에도 유사한 결과가 얻어졌습니다.Tacotron Wang 등 [2017], DurIAN Yu 등 [2020], FastSpeech Ren 등 [2019]는 일반적으로 제어 요인으로부터 멜로 스펙트로그램을 예측하기 위해 합성곱 신경망(CNN)과 Transformers를 채택합니다. 확산 모델 방법은 고품질 샘플을 생성할 수 있는 잠재력이 잘 알려져 있기 때문에 많은 주목을 받았습니다. 점수 기반 모델이라고도 하는 Ho et al. [2020]의 확산 모델은 데이터를 점차적으로 노이즈로 교란하는 확산 프로세스와 노이즈를 점진적으로 다시 데이터로 변환하는 역 프로세스의 두 가지 프로세스를 기반으로 합니다. 확산 모델의 중요한 단점인 Song et al. [2021a] Yang et al. [2022]은 생성에 많은 반복이 필요하다는 것입니다. 음성 합성에서 음향 모델링을 위해 확산 모델을 기반으로 하는 여러 방법이 제안되었습니다. 이러한 작업의 대부분은 여전히 생성 속도가 느리다는 문제가 있습니다. ~ Grad-TTS Popov et al. [2021] 음향 모델링을 위한 확산 모델을 적용하여 확률적 미분 방정식(SDE) Anderson [1982]을 공식화하여 잡음을 멜 스펙트로그램으로 점진적으로 변환하고 수치적 ODE 솔버를 사용하여 역 SDE를 풀었습니다(Song et al. [2021b]). 높은 오디오 품질을 제공하지만 역 프로세스에서 반복 횟수(10×1000단계)가 많아 추론 속도가 느립니다. Prodiff Huang et al. [2022]는 샘플링 단계를 줄이기 위해 점진적 증류 Salimans 및 Ho [2022]를 사용하도록 추가로 개발되었습니다. Liu et al. [2022b]에서 DiffGAN-TTS는 효율적인 음성 합성을 위해 적대적으로 훈련된 모델을 채택하여 잡음 제거 함수를 근사했습니다. Chen et al. [2022]에서 ResGrad는 확산 모델을 사용하여 사전 훈련된 FastSpeech2 Ren et al. 간의 예측 잔차를 추정합니다. [2020] 및 실제 결과. 일반적인 말하는 음성 외에도 최근 연구는 피치, 타이밍 및 표현에 더 복잡한 변화가 있는 음성에 초점을 맞춥니다. 예를 들어, Diffsinger Liu et al. [2022a]는 잘 설계된 확산 모델이 100단계의 반복을 통해 합성된 노래 음성에서 높은 품질을 달성할 수 있음을 보여줍니다. 위의 논의에서 음성 합성의 목표는 세 가지입니다. • 높은 오디오 품질: 생성 모델은 합성 오디오의 자연스러움과 표현력에 기여하는 말하는 음성의 뉘앙스를 정확하게 표현해야 합니다. 또한 생성된 오디오의 아티팩트와 왜곡도 피해야 합니다. • 빠른 추론 속도: 커뮤니케이션, 대화형 음성 및 음악 시스템을 포함한 실시간 애플리케이션에는 오디오의 빠른 생성 속도가 필요합니다. 통합 시스템에서 다른 알고리즘을 위한 시간을 만드는 것을 고려할 때 실시간보다 빠르다는 것만으로는 음성 합성에 충분하지 않습니다. • 음성 이상: 일반적인 말하는 음성 대신 피치, 표현, 리듬, 위반 제어 및 음색에 대한 더 복잡한 음성 모델링이 필요합니다(예: 노래 음성). 많은 노력이 있었지만 샘플링을 수행할 때의 노이즈 제거 확산 프로세스 메커니즘으로 인해 합성된 오디오 품질, 모델 성능 및 추론 속도 간의 상충 문제가 TTS에 여전히 존재하며 특히 SVS에서 두드러집니다. 기존 방법은 근본적으로 해결하기보다는 느린 추론 문제를 완화하려고 하며, 그 속도는 여전히 FastSpeech2 Ren et al. [2020]와 같은 확산 모델에 의존하지 않는 기존 방법과 비교할 수 없습니다. 최근 샘플링 프로세스를 설명하는 확률적 미분 방정식(SDE)을 상미분 방정식(ODE)으로 표현하고 ODE 궤적의 일관성 제약 조건을 더욱 강화하여 일관성 모델 Song et al. [2023]이 개발되어 샘플링 단계가 하나뿐인 고품질 이미지를 생성했습니다. 그러나 이미지 합성에서 이처럼 성공했음에도 불구하고 일관성 모델을 기반으로 한 음성 합성 모델은 지금까지 알려져 있지 않습니다. 이는 고품질 합성과 빠른 추론 속도를 모두 달성하기 위해 일관성 모델 기반 음성 합성 방법을 설계할 수 있는 잠재력을 나타냅니다. 이 논문에서 우리는 음성 합성을 위한 일관성 모델 기반 방법인 CoMoSpeech를 제안하는데, 이는 빠르고 고품질의 오디오 생성을 달성합니다. 우리의 CoMoSpeech는 사전 훈련된 교사 모델에서 추출되었습니다. 더 구체적으로, 우리의 교사 모델은 SDE를 활용하여 멜 스펙트로그램을 가우시안 노이즈 분포로 원활하게 변환하고 해당 스코어 함수를 학습합니다. 훈련 후, 우리는 해당 수치적 ODE 솔버를 활용하여 교사 노이즈 제거 함수를 구성하는데, 이는 추가적인 일관성 증류에 사용됩니다. 일관성 증류를 통해 우리의 CoMoSpeech가 얻어집니다. 궁극적으로, 우리의 CoMoSpeech는 단일 단계 샘플링으로 고품질 오디오를 생성할 수 있습니다. 우리는 TTS와 SVS 모두에 대한 실험을 수행하였고, 그 결과 CoMoSpeech는 실시간보다 150배 이상 빠른 단일 샘플링 단계로 음성을 생성할 수 있음을 보여줍니다. 오디오 품질 평가는 또한 CoMoSpeech가 수십에서 수백 번의 반복을 포함하는 다른 확산 모델 방법(그림 1에 시각화됨)보다 더 좋거나 비슷한 오디오 품질을 달성한다는 것을 보여줍니다. 이를 통해 확산 모델에 기반한 음성 합성이 처음으로 진정으로 실용적이 되었습니다. 일관성 모델의 배경 이제 일관성 모델을 간략하게 소개합니다. 데이터 분포가 Pdata(x)라고 가정합니다. 확산 모델은 가우시안 노이즈를 점진적으로 확산 데이터에 추가한 다음 역방향 노이즈 제거 프로세스를 채택하여 노이즈에서 샘플을 생성합니다. 확산 프로세스에서 po(x) = Pdata(x)이고 PT(x)가 가우시안 분포에 무한히 가깝고 T가 시간 상수인 노이즈 데이터 {x}\/0의 경우 전방 확산 프로세스는 SDE Song et al. [2021b] dx = f(x, t)dt + g(t)dw, t=(1) 여기서 w는 표준 위너 프로세스이고, f(·, ·) 및 g(.)는 각각 드리프트 및 확산 계수입니다. f(x, t)는 이전 작업(VP, VE, EDM) Song et al. [2021b], Karras et al. [2022]에서 f(x, t) = f(t)x로 작용하므로 dx = f(t)xdt g(t)dw입니다. 위 SDE의 주목할 만한 속성은 시간 t에서 SDE의 샘플링 궤적 분포를 나타내는 확률 흐름 ODE에 해당한다는 것입니다. Song et al. [2021b], Karras et al. [2022], dx = [ƒ(t)x — —±9(t)² V log p:(x)] dt, (3) 여기서 Vlog Pt(x)는 pt(x)의 점수 함수입니다. Hyvärinen 및 Dayan [2005]. 확률 흐름 ODE는 확률적 w를 제거하여 결정론적 샘플링 궤적을 생성합니다. Vlog Pt (x) = (D(x, t) — x+)/0, 점수 함수 V log pt(x)가 알려져 있는 한 (3)의 확률 흐름 ODE를 샘플링에 사용할 수 있습니다. D(x, t)가 t 단계에서 샘플 x+의 잡음을 제거하는 &quot;잡음 제거기&quot;라고 가정하면, 잡음 제거 오류 ||D(x, t) – x||2 Karras et al. [2022]를 최소화하여 점수 함수를 얻을 수 있습니다. (4) 여기서 σ = g(t)²dt. 또한 확률 흐름 ODE 기반 샘플링은 먼저 노이즈 분포에서 샘플링한 다음 Euler 및 Heun 솔버 Song et al. [2021b] Karras et al. [2022]와 같은 수치 ODE 솔버를 사용하여 실제 샘플로 노이즈를 제거하여 수행할 수 있습니다. 그러나 ODE 솔버는 여전히 많은 반복을 포함하므로 샘플링 속도가 느립니다. 샘플링을 가속화하려면 Song et al. [2023] 또는 샘플링 드리프트를 최소화하려면 Daras et al. [2023], 일관성 속성은 다음 두 가지를 모두 부과하기 위해 확산 모델에 제안되었습니다.D(x,t) = D(x+&#39;, t&#39;) (5) 텍스트 길이 예측기 [2, 1, 3, 2] 선택 입력: 악보 인코더 gtmel X 길이 조절기 ODE 솔버 ... tN-일관성 증류 디노이저 D(XN, TN, cond) tN 1단계 합성 priormel μ 조건부 입력 노이즈 mel XN~N(μ, I) 교사 CoMoSpeech 그림 2: CoMoSpeech의 그림. 당사의 CoMoSpeech는 일관성 제약 조건을 활용하여 교사 모델의 다단계 샘플링을 하나의 단계로 증류합니다. 모든 t 및 t&#39;에 대해 D(X0, 0) = x0. (6) 이런 식으로 일관성 모델을 얻을 수 있고, 확률 흐름 ODE의 샘플링 궤적에 있는 모든 지점이 궤적의 원점에 직접 연결되어 있기 때문에 1단계 샘플링 D(XT, T)를 얻을 수 있습니다. po(x). 일관성 모델은 격리된 상태에서 또는 사전 학습된 확산 기반 교사 모델에서 증류하여 학습할 수 있으며, 후자의 접근 방식이 일반적으로 더 나은 성능을 냅니다. 자세한 논의는 Song et al. [2023]을 참조할 수 있습니다. 저희 작업에서는 CoMoSpeech라고 하는 음성 합성을 위한 증류 기반 일관성 모델을 아래에 제안합니다. 3 CoMoSpeech 이 섹션에서는 1단계 음성 합성 모델인 제안된 CoMoSpeech를 소개합니다. 제안된 방법의 프레임워크는 그림 2에 나와 있으며, 두 가지 주요 단계가 있습니다. 첫 번째 단계는 텍스트(TTS 및 SVS의 경우) 및 악보 입력(SVS의 경우)에 따라 오디오를 생성하도록 확산 기반 교사 모델을 학습합니다. 그런 다음 두 번째 단계에서는 일관성 속성을 강제하여 교사 모델의 증류에서 CoMoSpeech를 얻어 최종적으로 조건부 입력을 기준으로 한 단계 추론을 달성합니다.교사 모델을 설계하고 일관성 증류를 수행하고 훈련과 추론을 구현하는 방법에 대해 설명합니다.3.1 교사 모델 생성 모델의 꽃이 만발한 클래스로서 많은 음성 합성 시스템은 확산 모델을 적용하고 고품질 오디오를 생성합니다.그러나 교사 모델이 되려면 특정 기준을 충족해야 합니다.먼저 모델은 이론적 요구 사항을 충족해야 합니다.섹션 2에서 언급했듯이 우리는 잡음 제거기를 채택하여 단계 생성을 구현하는 것을 목표로 합니다.즉, 이 기능은 잡음 대신 깨끗한 데이터를 가리켜야 합니다.즉, Huang et al. [2022]의 용어에 따라 교사 모델은 그래디언트 기반 방법이 아닌 생성기 기반이어야 합니다.이 제한으로 인해 최신 모델인 Grad-TTS Popov et al. [2021]을 교사 모델로 수정해야 합니다. 우리는 훈련 설정과 주요 아키텍처를 계승했습니다. 또한, 우리는 또한 EDMKarras et al. [2022]을 확산 모델에 대한 설계 선택으로 채택하여 추가적인 일관성 증류 Song et al. [2023]를 보장합니다. 구체적으로, 우리는 (2)에서 mel-spectrogram을 x로 설정하고 EDM Karras et al. [2022]의 스케줄 σ(t)와 스케일링 계수를 각각 t와 1로 설정합니다. (4)와 결합하면, 우리의 ODE는 dxt = [(xt — Do(xt, t, cond))/t]dt, (7)로 공식화될 수 있습니다.여기서 cond는 다음 섹션에서 소개할 조건부 입력이고, De(xt, t, cond)는 t-종속 스킵 연결을 사용하여 신경망을 사전 조건화하도록 설계되었습니다.De(xt, t, cond) = Cskip (t)x+ + Cout (t) F (Xt, t, cond) (8) 여기서 Fo는 아키텍처를 유연하게 선택할 수 있는 학습 대상 네트워크입니다.예를 들어, WaveNet Liu et al. [2022a] Oord et al. [2016] 또는 U-Net Popov et al. [2021] Ronneberger et al. [2015]의 아키텍처를 선택하여 Fe를 구성할 수 있습니다. Cskip(t)와 Cout(t)는 스킵 연결을 조절하고 Fo의 크기를 조절하는 데 사용되며 이는 Song et al.에 의해 주어질 수 있습니다. [2023] Cskip(t) data = 2 &quot; data Cout(t) = σdata(t - €) √√o data + t²&#39; (9) 여기서 σdata = 0.5는 Cskip과 Cout 간의 비율을 균형 잡는 데 사용되고 € = 0.002는 샘플링 중 가장 작은 시간 순간입니다. 위의 공식을 선택한 첫 번째 이유는 Cskip(e) = 1이고 Cout(€) = 0이므로 (6)을 충족할 수 있기 때문입니다. 두 번째 이유는 두 스케일링 계수 모두 Fo의 예측 결과가 단위 분산으로 스케일링되도록 도울 수 있기 때문에 다른 노이즈 수준에서 그래디언트 크기의 큰 변동을 피할 수 있기 때문입니다. De를 훈련하기 위해 손실 함수는 ― Со ||De(xt, t, cond) – xo||², (10)로 공식화할 수 있습니다. 이는 예측된 mel-spectrogram predmel과 실제 mel-spectrogram gtmel 간의 가중 L2 손실이며 손실 함수의 가중치도 다시 지정합니다. 영어: 다른 t의 경우 EDM Karras et al. [2022]과 동일합니다.마지막으로, 교사 모델을 학습시키고, 합성된 mel-spectrogram을 알고리즘 1로 샘플링할 수 있습니다.교사 모델에 대한 추론 동안, 먼저 N(µ, I)에서 x를 샘플링한 다음, N Euler 단계에 대해 수치 ODE 솔버를 반복합니다.알고리즘 1 제안된 교사 모델의 샘플링 절차 입력: 잡음 제거 함수 Do;사전 mel-spectrogram μ;시간 지점 집합 tiε {0,...,N} 1: 샘플 XN ~ N (µ‚ I) 2: XN = NXN 3: i = 4인 경우: di 5: N에서 1인 경우 do (xi Do(xi, ti, µ))/ti Xi−1 ←xi + (ti — ti−1)di 6: 끝 7인 경우: xx출력: x 알고리즘 2 제안된 방법의 샘플링 절차 입력: 잡음 제거 함수 Do; 사전 mel-spectrogram μ; 시간 지점 집합 tie {0,...,N} 1: 샘플 XN ~ N(μ, I) 2: XN = tNXN 3: xD(XN, TN, μ) 4: 1단계 합성인 경우 5: 출력: x 6: 그렇지 않은 경우 i = N에 대한 다단계 합성 1 to 1 do _ 샘플 z ~ ~ N(µ, I) 7: 8: 9: 10: 11: end for x← Do(xi, ti, μ) 출력: x(XN, TN) 노이즈 mel (Xi+1, ti+1) ODE 궤적 일관성 제약 조건 (xi, ti) (xo, to) gt mel 그림 3: 일관성 속성의 예. 일관성 속성이 있는 함수는 ODE 궤적의 모든 지점을 원래 데이터에 매핑합니다. 3.2 일관성 증류 일관성 증류를 기반으로 한 교사 모델에서 1단계 확산 샘플링 기반 모델을 추가로 학습하여 제안된 CoMoSpeech를 얻었습니다. 이제 (5)와 (6)에 정의된 제약 조건을 다시 검토합니다. (9)에서 Cskip(t)와 Cout(t)를 선택한 경우 제안된 교사 모델의 잡음 제거기 De가 이미 (6)을 충족하므로 남은 학습 목표는 (5)의 속성을 충족하는 것입니다. Song et al. [2023]에서 영감을 얻어 모멘텀 기반 증류를 활용하여 제안된 CoMoSpeech를 학습합니다. 일관성 증류 손실은 Lo = ||Do(xi+1, ti+1, cond) — D₁- (x², ti, cond)||², (11)로 정의됩니다. 여기서 및 는 교사 모델에서 상속받은 CoMoSpeech의 초기화된 가중치이고, 는 섹션 3.1의 교사 모델의 고정 ODE 솔버이며, i는 N에서 1까지의 전체 ODE 단계에서 균일하게 샘플링된 단계 인덱스입니다. x는 x;+1 및 ODE 솔버 에서 추정됩니다. 학습하는 동안 가중치는 손실 함수에 의해 직접 최적화되고 0¯는 stopgrad(að¯ + (1 − a)0)에 의해 재귀적으로 업데이트됩니다. 여기서 a는 경험적으로 0.95로 설정된 운동량 계수입니다. (12) 증류 후 일관성 속성을 활용하여 원래 데이터 포인트 x를 그림 3에 표시된 대로 ODE 궤적의 모든 포인트 xt에서 변환할 수 있습니다. 따라서 다음과 같이 단계 tn에서 분포 XN에서 대상 샘플을 직접 생성할 수 있습니다. melpred = D(XN, TN, cond). (13) 따라서 1단계 멜 스펙트로그램 생성이 가능합니다. 또한 다른 확률적 샘플러와 마찬가지로 알고리즘 2를 사용하여 오디오 품질과 샘플링 속도 간의 균형을 유지하면서 다단계 합성을 수행할 수 있습니다. 3.3 조건부 입력 그림 2에 표시된 프레임워크에서 남은 문제는 알고리즘 설계 전반에 사용될 조건부 입력 cond를 얻는 방법입니다. 잘 설계된 음성 합성기는 읽기 음성 합성(TTS)뿐만 아니라 매우 동적인 멜로디를 추가로 생성하는 SVS와 같은 다른 더 복잡한 작업에서도 좋은 성능을 발휘할 것으로 예상됩니다. 조건부 입력을 생성할 때 TTS 및 SVS 작업을 모두 고려하여 제안된 프레임워크의 효과를 포괄적으로 조사합니다. 구체적으로 TTS 및 SVS의 기본 입력으로 음소를 채택합니다. 그런 다음 간단한 조회 테이블을 사용하여 음소 기능을 임베드합니다. 또한 SVS 작업의 경우 음소에 시간적으로 정렬된 음표 수준을 지정하는 악보를 추가합니다. 음표 기능 추출의 경우 범주형 기능 음표 피치와 슬러 표시기에 대한 임베딩 방법을 사용하고 연속 기능 음표 지속 시간에 선형 계층에 의존합니다. 모든 기능 시퀀스를 합산하여 FastSpeech Ren et al. [2019]의 인코더 구조와 분산 어댑터를 활용합니다. 구체적으로 N개의 피드포워드 변환기 블록(FFT 블록)을 쌓아 음소 숨김 시퀀스를 추출합니다. 지속 시간 예측기를 사용하여 각 음소 dpred의 지속 시간을 추정하고 해당 손실 함수는 Lduration = ||로 표현됩니다. log(dpred) - log(dgt)||여기서 dgt는 기준 진실 음소 지속 시간을 나타냅니다. (14) 또한 길이 조절기는 음소 숨김 시퀀스를 mel-spectrogram 도메인의 숨김 시퀀스로 투영하며, 음소 지속 시간은 숨김 mel로 표시됩니다. 그런 다음 사전 손실 함수가 Lprior ||μgtmel||2인 숨김 mel을 사용하여 사전 mel-spectrogram μ를 예측합니다. = (15) 우리는 Grad-TTS Popov et al. [2021]의 인코더 부분과 사전 mel-spectrogram 설정을 따릅니다. 그림 2의 왼쪽 아래 부분에서 볼 수 있듯이 숨김 mel에서 동일한 음소에 속하는 확장된 특징이 반복되기 때문에 예측된 사전 mel은 음소 시퀀스를 기반으로 gtmel의 시간-주파수 구조를 대략적으로만 근사할 수 있습니다. mel-spectrogram의 세부 사항은 확산 모델에 의해 모델링됩니다.신경망과 잡음 제거기의 조건부 입력의 경우, 우리는 다양한 조합을 조사했고 마지막으로 공정한 비교를 위해 이전 작업에서와 동일한 설정을 따랐습니다.a) Diffsinger: WaveNet 아키텍처Oord et al. [2016] 및 SVS에 대한 (13)의 특징 조건으로 hiddenmel 및 b) Grad-TTS: U-Net 아키텍처 Ronneberger et al. [2015] 및 TTS에 대한 조건으로 μmel.3.4 학습 절차 전체 프로세스는 교사 모델 학습과 일관성 증류의 두 단계로 요약할 수 있습니다.교사 모델 학습의 경우 손실 항은 지속 시간 손실((14)), 사전 손실((15)) 및 잡음 제거 손실((10))의 세 부분으로 구성될 수 있습니다.이 세 가지 손실은 추가 가중치 없이 함께 합산됩니다. 이 단계의 목적은 다단계 합성으로 고품질 오디오를 생성할 수 있고 추가적인 일관성 증류의 잠재력이 있는 음성 합성 시스템을 구축하는 것입니다.두 번째 단계는 일관성 증류입니다.(11)에 의해 정의된 손실 함수는 하나뿐이며, 이는 모델이 일관성 속성을 학습하는 데 도움이 됩니다.매개 변수는 교사 모델에서 초기화됩니다.학습하는 동안 인코더의 매개변수는 고정되므로 노이즈 제거기의 가중치만 업데이트됩니다.증류 후 1단계 합성(13)으로 고품질 녹음을 얻을 수 있습니다.4 실험 제안된 CoMoSpeech의 성능을 평가하기 위해 TTS와 SVS 모두에서 실험을 수행합니다.4.1 실험 설정 4.1.1 데이터 및 전처리 22.05kHz로 샘플링된 약 1시간 분량의 영어 여성 음성 녹음이 포함된 공개 LJSpeech Ito 및 Johnson [2017]을 TTS 데이터 세트로 채택합니다.Ren et al. [2019] Chen et al.과 유사합니다. [2022], 우리는 데이터 세트를 세 세트로 나누었습니다: 12, 228개의 훈련용 샘플, 349개의 샘플(METHOD NFE RTF (↓) FD (↓) MOS (1) GT 4.GT(Mel+HiFi-GAN) 0.4.FastSpeech 2 Ren et al. [2020]0.10.4.DiffGAN-TTS Liu et al. [2022b]0.8.3.ProDiff Huang et al. [2022]0.3.3.DiffSpeech Liu et al. [2022a]0.2.4.Grad-TTS Popov et al. [2021]0.1.4.Teacher0.0.4.CoMoSpeech0.0.4.표 1: TTS에 대한 LJSpeech의 평가 결과. METHOD NFE RTF (↓) FD (↓) MOS (†) GT 4.GT(Mel+HiFi-GAN) 0.4.FFTSinger Blaauw 및 Bonada [2020]0.7.2.HIFiSinger Chen 등 [2020]0.6.3.DiffSinger Liu 등 [2022a] A 버전0.3.3.DiffSinger Liu 등 [2022a] B 버전0.3.3.COMOSVS-teacher0.3.4.COMOSVS0.3.3.표 2: SVS에 대한 Opencpop의 평가 결과. 문서 제목 LJ003)을 검증용으로, 523개 샘플(문서 제목 LJ001 및 LJ002)을 테스트용으로 사용했습니다. Ren 등의 일반적인 관행을 따랐습니다.[2020]Huang 등 [2022] TTS의 경우 프레임 크기가 1024이고 홉 크기가 256인 80빈 멜 스펙트로그램을 추출합니다. SVS 작업의 경우 100개의 중국 팝송이 포함된 Wang et al. [2022]의 Opencpop 데이터 세트를 사용하며, 이는 총 지속 시간이 약 5.2시간인 3,756개의 발화로 분할됩니다. 모든 녹음은 한 명의 여성 가수의 녹음이며 정렬된 음소와 MIDI 피치 시퀀스로 레이블이 지정됩니다. Wang et al. [2022]의 공식 훈련/테스트 분할, 즉 각각 훈련 및 평가를 위한 95개의 노래와 5개의 노래를 따릅니다. Chen et al. [2020]Liu et al.의 설정과 동일합니다. [2022a] SVS의 경우, 녹음은 16비트 정밀도로 24kHz 속도로 리샘플링되고, 80빈 멜 스펙트로그램은 프레임 크기 512, 홉 크기 128로 추출됩니다. 4.1.2 구현 세부 정보 TTS의 경우 공정한 비교를 위해 인코더와 지속 시간 예측기는 Grad-TTS Popov et al. [2021]의 인코더와 지속 시간 예측기와 정확히 동일합니다. 인코더에는 6개의 피드 포워드 변환기(FFT) 블록 Ren et al. [2019]이 있으며, 숨겨진 채널은 192로 설정됩니다. 지속 시간 예측기는 예측을 위해 두 개의 합성곱 계층을 사용합니다. 교사 모델과 CoMoSpeech는 모두 배치 크기가 16인 단일 NVIDIA A100 GPU에서 170만 번의 반복으로 훈련됩니다. Adam 옵티마이저 Kingma 및 Ba [2015]가 학습 속도 1e-4로 채택됩니다. SVS의 경우, 우리는 다른 하이퍼파라미터를 사용하여 TTS와 거의 동일한 아키텍처를 채택합니다. 인코더는 4개의 FFT 블록을 채택하고, 인코더에서 숨겨진 채널을 256으로 설정합니다. 지속 시간 예측기는 지속 시간을 추정하기 위한 5개의 합성곱 계층으로 구성됩니다. SVS와 CoMoSpeech의 교사 모델은 AdamW Loshchilov 및 Hutter [2017] 최적화 도구를 사용하여 단일 GPU에서 250k 단계로 학습합니다. 초기 학습률은 1e-3이고, 50k 단계마다 0.5의 감소 요인을 갖는 지수 감소 전략이 채택됩니다. 4.1. 평가 지표 우리는 샘플 품질(MOS &amp; FD)과 모델 추론 속도(RTF &amp; NFE)를 측정하기 위해 객관적 및 주관적 평가를 모두 수행합니다.• MOS(평균 의견 점수) Chu 및 Peng [2006]은 합성 오디오의 지각된 품질을 측정하는 데 사용되며, 이는 10명의 청취자에게 테스트 세트를 제시하고 합성 오디오의 품질을 1~5점 척도로 평가하도록 요청하여 얻습니다.• FD(프레셰 거리)²는 이미지 생성에서 프레셰 개시 거리 Heusel et al. [2017]와 유사합니다.우리는 오디오에서 프레셰 거리 Liu et al. [2023]를 사용하여 대규모 사전 학습된 오디오 신경망 PANNS Kong et al. [2020b]를 활용하여 생성된 샘플과 대상 샘플 간의 유사성을 측정합니다.• RTF(실시간 요인)는 시스템이 실시간 애플리케이션에서 얼마나 빨리 오디오를 합성할 수 있는지 결정합니다. 이는 음성 시스템이 주어진 양의 오디오를 합성하는 데 걸리는 총 시간과 해당 오디오의 지속 시간 간의 비율로 정의됩니다.또한 RTF에 대한 모든 실험은 단일 NVIDIA A100 GPU에서 구현됩니다.• NFE(함수 평가 횟수)는 생성 프로세스 동안 노이즈 제거 기능이 평가되는 총 횟수를 나타내는 계산 비용을 측정합니다.4.2 텍스트 음성 변환 성능 교사 모델과 CoMoSpeech에서 생성한 샘플의 위의 네 가지 메트릭을 다음 시스템과 비교합니다.• GT, 기준 진실 녹음.• GT(Mel+HiFi-GAN), 기준 진실 멜 스펙트로그램을 사용하여 HiFi-GAN 보코더 Kong et al. [2020a]로 파형을 합성.• FastSpeech 2 Ren et al. [2020], FFT 블록과 분산 어댑터를 사용하여 빠른 속도로 고품질 음성을 합성.• DiffGAN-TTS Liu et al. [2022b] ³, 효율적인 음성 합성을 위해 잡음 제거 함수를 근사화하기 위해 적대적으로 훈련된 모델을 적용합니다.• ProDiff Huang et al. [2022] 4, 빠른 생성 속도를 위해 Salimans 및 Ho [2022]의 진행적 증류를 TTS에 직접 채택합니다.. DiffSpeech Liu et al. [2022a] 5, 보조 음향 모델을 사용하여 멜스펙트로그램을 생성하고 K 단계 잡음을 잡음이 있는 멜스펙트로그램에 주입합니다.그런 다음 DDPM을 통해 잡음이 있는 멜스펙트로그램에서 멜스펙트로그램을 반복적으로 생성합니다.• Grad-TTS Popov et al. [2021] 6, 멜스펙트로그램에 대한 확률적 미분 방정식 모델링을 사용하고 오디오 생성을 위해 해당 ODE 솔버를 사용합니다. TTS의 평가 결과는 표 1에 나와 있습니다. 오디오 품질의 경우 교사 모델은 가장 높은 MOS를 달성했고 Grad-TTS는 교사 모델이 Grad-TTS의 설계에 기반을 두고 있기 때문에 2위를 차지했지만 SDE에서 드리프트 및 확산 계수에 대한 더 나은 선택을 채택했습니다. 제안된 CoMoSpeech는 모든 방법 중에서 3위를 차지하지만 다른 고속 추론 방법인 ProDiff, DiffGAN-TTS 및 Fastspeech2보다 상당히 뛰어납니다. 이는 일관성 증류의 효과와 교사 모델 선택의 효과를 보여줍니다. 또한 교사 모델과 CoMoSpeech가 모든 방법 중에서 가장 좋은 주파수 거리 점수를 달성하는 것을 관찰하여 제안된 방법이 데이터 분포를 모델링하는 데 더 우수한 성능을 발휘한다는 것을 더욱 입증했습니다. 추론 속도와 관련하여 Fastspeech2가 분명히 최고를 달성하는 반면 CoMoSpeech도 매우 낮은 RTF를 생성하고 다른 모든 기준선보다 빠릅니다. DiffSpeech, Grad-TTS 및 교사 모델을 포함한 많은 반복을 포함하는 확산 기반 방법과 비교했을 때, 저희 방법은 비슷하거나 더 나은 오디오 품질로 약 50배 더 빠르게 달성합니다. 또한, 저희 CoMoSpeech는 확산 샘플링을 가속화하는 방법, 즉 DiffGAN-TTS 및 ProDiff보다 더 빠른 속도와 더 나은 품질을 달성합니다. 2https://github.com/haoheliu/audioldm_eval https://github.com/keonlee9420/DiffGAN-TTS https://github.com/Rongjiehuang/ProDiff Shttps://github.com/MoonInTheRiver/DiffSinger/blob/master/docs/README-TTS.md &quot;https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS(a) gtmel (b) 노이즈 mel (c) 일관성 증류 전 노이즈 제거기 De(XN, TN, cond) (d) 일관성 증류 후 노이즈 제거기 Do(XN, TN, cond) 그림 4: 일관성 증류의 효과: 일관성 증류 전 교사 모델의 노이즈 제거기와 비교했을 때, CoMoSpeech는 과도하게 평활화된 mel-spectrogram 대신 고품질 mel-spectrogram을 생성할 수 있습니다. 영어: mel-spectrogram은 denoiser 함수를 한 번만 호출하여 생성됩니다.4.3 노래 음성 합성 성능 방법의 모델링 기능을 추가로 조사하기 위해 제안된 SVS 버전 모델인 teacher-svs와 CoMoSpeech-svs를 SVS에 대한 여러 기준선과 비교합니다.기준선은 다음과 같습니다.• • . • GT, 기준 진실 녹음.GT(Mel+HiFi-GAN), HiFi-GAN Kong et al. [2020a] 보코더를 사용하여 노래 샘플을 합성하고 기준 진실 mel-spectrogram 입력을 사용합니다.FFTSinger Blaauw 및 Bonada [2020], FFT 블록을 채택하여 mel-spectrogram을 예측하고 HiFi-GAN 보코더를 사용하여 오디오를 합성합니다.HiFiSinger Chen et al. [2020], 새로운 하위 주파수 GAN(SF-GAN)을 사용하여 mel-spectrogram을 생성합니다. 우리의 목표는 음향 모델을 비교하는 것이므로 다른 방법과 동일한 HiFi-GAN으로 원래 보코더를 수정합니다.• DiffSinger Liu et al. [2022a], DDPM을 사용하여 노이즈가 있는 멜 스펙트로그램에서 멜 스펙트로그램을 생성합니다. 노이즈가 있는 멜 스펙트로그램을 생성하는 데는 두 가지 버전이 있는데, A 버전은 보조 음향 모델을 사용하여 멜 스펙트로그램을 생성하고 노이즈가 있는 멜 스펙트로그램에 K 단계 노이즈를 주입하고, B 버전은 가우시안 노이즈에서 노이즈가 있는 멜 스펙트로그램을 직접 생성합니다. SVS의 결과는 표 2에 나와 있습니다. 오디오 품질에 관해서는 CoMoSpeech 및 기타 확산 모델 기반 방법이 프레셰 거리 및 평균 의견 점수에서 FFTSinger 및 HIFiSinger를 포함한 모든 비반복적 방법을 크게 능가할 수 있음을 알 수 있습니다. 확산 모델 중에서 교사 모델이 가장 좋은 성능을 달성하고, 학생 모델 CoMoSpeech는 이에 가까운 성능을 보입니다. 추론 속도의 경우, 1단계 추론을 통해 제안된 CoMoSpeech는 비반복적 방법과 비슷한 속도를 유지하고 다른 확산 모델 기반 방법보다 상당히 우수한 성능을 낼 수 있습니다. 버전 A: https://github.com/MoonInTheRiver/DiffSinger/blob/master/docs/README-SVS-opencpopcascade.md 버전 B: https://github.com/MoonInTheRiver/DiffSinger/blob/master/docs/README-SVS-opencpope2e.md Frechet Distance (↓↓) NFE 교사 모델 CoMoSpeech7.0.4.0.2.0.1.0.0.0.표 3: TTS에 대한 다른 샘플링 단계를 사용한 CoMoSpeech와 교사 모델 간의 비교. Frechet Distance(↓) NFE Teacher model CoMoSpeech7.3.7.3.4.3.3.3.3.3.3.표 4: SVS를 위한 다른 샘플링 단계를 사용한 CoMoSpeech와 해당 Teacher 모델 간의 비교. 또한 말하는 음성과 노래하는 음성 합성 간의 결과도 비교합니다. 기본적으로 동일한 두 가지 방법인 DiffSinger와 DiffSpeech를 기반으로 노래하는 음성이 말하는 음성보다 FD가 더 큰 것을 관찰할 수 있으며, 이는 데이터를 모델링하기가 더 어렵다는 것을 나타냅니다. 그러나 제안된 Teacher 모델과 CoMoSpeech는 여전히 오디오 품질과 추론 속도에서 각각 가장 우수한 성능을 달성합니다. 이는 말하는 음성을 넘어 CoMoSpeech의 음성 합성 기능을 보여줍니다. 또한 SVS의 노이즈 제거 기능이 TTS의 U-Net 아키텍처보다 빠른 WaveNet 아키텍처를 따르기 때문에 CoMoSpeech-svs가 CoMoSpeech보다 빠르다는 것을 관찰할 수 있습니다. 이러한 관찰을 통해 FastSpeech2의 디코더보다 빠르게 실행되는 더 효율적인 노이즈 제거 기능을 설계할 수 있다면 향후 작업에서 CoMoSpeech를 비반복적 방법보다 훨씬 빠르게 만들 수 있다는 영감을 얻었습니다.4.4 일관성 증류에 대한 소거 연구 이 부분에서는 일관성 증류의 중요성을 보여드리겠습니다.그림 4에서 보듯이 일관성 증류 전후의 차이를 결과, 즉 교사 모델과 CoMoSpeech에서 시각화합니다.두 단계에서 증류 전 노이즈 제거 기능은 매끄러운 멜 스펙트로그램을 가리키며, 기준 진실 멜 스펙트로그램과 큰 차이가 있음을 나타냅니다.그러나 증류 후의 결과는 많은 세부 사항을 풍부하게 하여 자연스럽고 표현력이 풍부한 사운드를 생성하여 성능을 크게 향상시킵니다.표 3과 표 4에서 우리는 또한 프레셰 거리 메트릭을 사용하여 일관성 증류의 효과를 더욱 입증하는 실험을 수행합니다. TTS 및 SVS 작업 모두에 대한 교사 모델의 경우 반복 단계가 증가하면 프레셰 거리가 감소합니다. 추론 속도와 샘플 품질 간의 이러한 상충 관계는 다른 확산 모델 방법에서도 관찰되었습니다. 놀랍게도 CoMoSpeech가 한 단계에서 거의 최상의 성능을 달성할 수 있으며, TTS 및 SVS에서 각각 4단계와 10단계에서 최상의 성능을 달성할 수 있음을 알 수 있습니다. 그러나 상충 관계는 10단계 후에 사라지는 것으로 보입니다. 모델 성능이 몇 번의 샘플링 단계에서 개선된 다음 단계 수가 증가함에 따라 약간 감소하는 문제를 &quot;샘플링 드리프트&quot; 과제라고 합니다. Daras et al. [2023] Ji et al. [2023] Chen et al. [2023] Saxena et al. [2023]. 탐색은 향후 작업에 맡기겠습니다. 5
--- CONCLUSION ---
s 및 향후 작업 이 논문에서는 일관성 모델을 기반으로 하는 음성 합성을 위한 1단계 음향 모델인 CoMoSpeech를 제안합니다. 다양한 조건부 입력을 통해 CoMoSpeech는 단일 단계에서 노이즈 멜 스펙트로그램을 예측된 멜 스펙트로그램으로 변환하여 고품질의 음성이나 노래 음성을 생성할 수 있습니다. 그러나 여전히 이 방법에는 몇 가지 한계가 있습니다. CoMoSpeech는 더 나은 성능을 위해 교사 모델에서 추출해야 하므로 음성 합성 시스템을 구성하는 파이프라인이 더 복잡해집니다. 따라서 교사 모델을 추출하지 않고 CoMoSpeech를 직접 학습하는 방법이 다음 조사 단계입니다. 또한 SVS 작업에서 CoMoSpeech의 기능을 보여줍니다. CoMoSpeech가 모든 방법 중에서 가장 좋은 결과를 달성했지만 여전히 기준 진실 녹음과 상당한 차이가 있습니다. 감사의 말 이 연구는 홍콩 연구 지원 위원회의 주제 기반 연구 계획(T45-205/21-N)과 경력 초기 계획(ECS-HKUST22201322)의 지원을 받았습니다.참고문헌 Brian DO Anderson. 역시간 확산 방정식 모델.확률 과정 및 응용, 12(3):313–326, 1982.Mario Bertero, Tomaso A Poggio, Vincent Torre.초기 시각의 잘못 제기된 문제.IEEE 회의록, 76(8):869-889, 1988.Merlijn Blaauw와 Jordi Bonada.피드포워드 변압기를 사용한 시퀀스 간 노래 합성.In Proc. Intl. Conf. 음향, 음성 및 신호 처리(ICASSP), 2020. Jiawei Chen, Xu Tan, Jian Luan, Tao Qin, Tie-Yan Liu. Hifisinger: 고충실도 신경 노래 음성 합성을 향하여. arXiv 사전 인쇄본 arXiv:2009.01776, 2020. Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, Anru R Zhang. 샘플링은 점수를 배우는 것만큼 쉽습니다: 최소 데이터 가정을 사용한 확산 모델 이론. Proc. Intl. Conf. on Learning Representations(ICLR), 2023. Zehua Chen, Yihan Wu, Yichong Leng, Jiawei Chen, Haohe Liu, Xu Tan, Yang Cui, Ke Wang, Lei He, Sheng Zhao, et al. Resgrad: 텍스트 음성 변환을 위한 잔여 잡음 제거 확산 확률적 모델. arXiv 사전 인쇄본 arXiv:2212.14518, 2022. Min Chu와 Hu Peng. 합성 음성의 평균 의견 점수를 추정하기 위한 객관적 측정, 2006년 4월 4일. 미국 특허 7,024,362. Giannis Daras, Yuval Dagan, Alexandros G Dimakis, Constantinos Daskalakis. 일관된 확산 모델: 일관성을 학습하여 샘플링 드리프트 완화. arXiv 사전 인쇄본 arXiv:2302.09057, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter. 2시간 척도 업데이트 규칙으로 학습된 Gans는 로컬 내쉬 균형으로 수렴. 신경 정보 처리 시스템(NeurIPS)에 대한 Proc. Conf., 2017. Jonathan Ho, Ajay Jain, Pieter Abbeel. 노이즈 제거 확산 확률적 모델. 신경 정보 처리 시스템(NeurIPS)에 관한 Proc. Conf., 33권, 6840-6851페이지, 2020년. Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, Yi Ren. Prodiff: 고품질 텍스트 음성 변환을 위한 점진적 고속 확산 모델. 멀티미디어에 관한 ACM 국제 회의(ACM MM), 2595-2605페이지, 2022년. Aapo Hyvärinen과 Peter Dayan. 점수 매칭을 통한 비정규화된 통계적 모델 추정. 기계 학습 연구 저널, 6(4), 2005년. Keith Ito와 Linda Johnson. lj 음성 데이터 세트. LJ-Speech-Dataset/, 2017. https://keithito.com/ Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. DDP: 고밀도 시각적 예측을 위한 확산 모델. arXiv 사전 인쇄본 arXiv:2303.17559, 2023. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 확산 기반 생성 모델의 디자인 공간 설명. 신경 정보 처리 시스템(NeurIPS)에 대한 Proc. Conf.에서, 2022. Jaehyeon Kim, Jungil Kong, and Juhee Son. 엔드투엔드 텍스트-투-스피치를 위한 적대적 학습을 갖춘 조건부 변분 자동 인코더. Proc. Intl. Conf.에서. 기계 학습(ICML), 5530-5540쪽. PMLR, 2021. Diederik P Kingma와 Jimmy Ba. Adam: 확률적 최적화를 위한 방법. Proc. Intl. Conf. on Learning Representations(ICLR), 2015. Jungil Kong, Jaehyeon Kim, Jaekyoung Bae. Hifi-gan: 효율적이고 고충실도의 음성 합성을 위한 생성적 적대적 네트워크. 33권, 17022-17033쪽, 2020a. Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, Mark D Plumbley. PANNS: 오디오 패턴 인식을 위한 대규모 사전 학습된 오디오 신경망. IEEE/ACM Trans. Audio, Speech, Lang. Process., 28:2880–2894, 2020b. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D Plumbley. Audioldm: 잠재 확산 모델을 사용한 텍스트-오디오 생성. arXiv 사전 인쇄본 arXiv:2301.12503, 2023. Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, Zhou Zhao. Diffsinger: 얕은 확산 메커니즘을 통한 노래 음성 합성. Proc. AAAI Conf. on Artificial Intelligence, 36권, 11020-11028페이지, 2022a. Songxiang Liu, Dan Su, Dong Yu. Diffgan-tts: 잡음 제거 확산 gans를 사용한 고충실도 및 효율적인 텍스트-음성 변환. arXiv 사전 인쇄본 arXiv:2201.11972, 2022b. Ilya Loshchilov와 Frank Hutter. 분리된 가중치 감소 정규화. Proc. Intl. Conf. on Learning Representations (ICLR), 2017. Peiling Lu, Jie Wu, Jian Luan, Xu Tan, and Li Zhou. Xiaoicesing: 고품질의 통합된 노래 음성 합성 시스템. Proc. InterSpeech, 2020. Masanari Nishimura, Kei Hashimoto, Keiichiro Oura, Yoshihiko Nankaku, and Keiichi Tokuda. 딥 신경망을 기반으로 한 노래 음성 합성. Interspeech, 2478-2482쪽, 2016. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: 원시 오디오를 위한 생성 모델. Proc. Intl. Speech Commun. Assoc. (ISCA) Workshop on Speech Synthesis, 2016. Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov. Grad-TTS: 텍스트-음성을 위한 확산 확률적 모델. Proc. Intl. Conf. Machine Learning (ICML), 8599-8608쪽. PMLR, 2021. Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu. Fastspeech: 빠르고 견고하며 제어 가능한 텍스트-음성. Proc. Conf. on Neural Information Processing Systems (NeurIPS), 32권, 2019. Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu. Fastspeech 2: 빠르고 고품질의 엔드투엔드 텍스트-음성 변환. Proc. Intl. Conf. on Learning Representations(ICLR), 2020. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: 생물의학 이미지 분할을 위한 합성곱 네트워크. Proc. Conf. on Medical Image Computing and Computer Assisted Intervention(MICCAI), 234-241쪽, 2015. Tim Salimans and Jonathan Ho. 확산 모델의 빠른 샘플링을 위한 점진적 증류. Proc. Intl. Conf. on Learning Representations(ICLR), 2022. Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David J Fleet. 확산 모델을 사용한 단안 깊이 추정. arXiv 사전 인쇄본 arXiv:2302.14816, 2023. Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, Jiang Bian. Naturalspeech 2: 잠재 확산 모델은 자연스러운 제로샷 음성 및 노래 합성기입니다. arXiv 사전 인쇄본 arXiv:2304.09116, 2023. Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암시적 모델의 잡음 제거. Proc. Intl. Conf. on Learning Representations(ICLR), 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. 확률적 미분 방정식을 통한 점수 기반 생성 모델링. Proc. Intl. Conf. on Learning Representations(ICLR), 2021b. Yang Song, Prafulla Dhariwal, Mark Chen, Ilya Sutskever. 일관성 모델. arXiv 사전 인쇄본 arXiv:2303.01469, 2023. Xu Tan, Tao Qin, Frank Soong, Tie-Yan Liu. 신경 음성 합성에 대한 조사. arXiv 사전 인쇄본 arXiv:2106.15561, 2021. Paul Taylor. 텍스트 음성 합성. 케임브리지 대학 출판부, 2009. Yu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu, Hanzhao Li, Heyang Xue, Yongmao Zhang, Lei Xie, Mengxiao Bi. Opencpop: 노래 음성 합성을 위한 고품질 오픈 소스 중국어 인기 노래 코퍼스. Proc. InterSpeech, 2022. Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio 등 Tacotron: 엔드투엔드 음성 합성을 지향합니다. Proc에서 InterSpeech, 2017. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui 및 Ming-Hsuan Yang. 확산 모델: 방법 및 적용에 대한 포괄적인 조사입니다. arXiv 사전 인쇄 arXiv:2209.00796, 2022. Chengzhu Yu, Heng Lu, Na Hu, Meng Yu, Chao Weng, Kun Xu, Peng Liu, Deyi Tuo, Shiyin Kang, Guangzhi Lei, et al. 두리안: 멀티모달 합성을 위한 지속 시간 정보 주의 네트워크. Proc. InterSpeech, 2020.
