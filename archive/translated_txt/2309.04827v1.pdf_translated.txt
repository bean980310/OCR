--- ABSTRACT ---
우리는 단일 GPU에서 수행할 수 있는 가벼운 방식으로 대규모 언어 모델 패밀리를 분석합니다. 구체적으로, 우리는 125m에서 66b 매개변수 범위의 OPT 모델 패밀리에 초점을 맞추고 FFN 뉴런이 활성화되었는지 여부에만 의존합니다. 첫째, 우리는 네트워크의 초기 부분이 희소하고 많은 개별 기능을 나타낸다는 것을 발견했습니다. 여기서 많은 뉴런(66b 모델의 일부 계층에서 70% 이상)이 &quot;죽었습니다&quot;. 즉, 다양한 데이터의 대규모 컬렉션에서 결코 활성화되지 않습니다. 동시에, 살아있는 뉴런 중 많은 수가 개별 기능을 위해 예약되어 있으며 토큰 및 n-그램 감지기 역할을 합니다. 흥미롭게도, 해당 FFN 업데이트는 예상할 수 있듯이 다음 토큰 후보를 홍보할 뿐만 아니라 토큰을 트리거하는 정보, 즉 현재 입력을 제거하는 데 명시적으로 초점을 맞춥니다. 우리가 아는 한, 이것은 잔여 스트림에서 정보를 추가하지 않고 제거하는 데 특화된 메커니즘의 첫 번째 예입니다. 스케일이 커짐에 따라 모델은 더 많은 죽은 뉴런과 토큰 감지기를 갖게 되어 더 희소해집니다. 마지막으로, 일부 뉴런은 위치에 따라 활성화되거나 활성화되지 않습니다. 활성화 여부는 위치에 크게(또는 전적으로) 의존하고 텍스트 데이터에는 덜(또는 전혀) 의존합니다. 우리는 더 작은 모델이 위치 범위 지표 역할을 하는 뉴런 세트를 갖는 반면 더 큰 모델은 덜 명확한 방식으로 작동한다는 것을 발견했습니다. 1
--- INTRODUCTION ---
언어 모델의 기능 범위는 규모에 따라 확장되고 더 큰 규모에서는 모델이 매우 강력하고 다재다능해져서 단일 모델을 다양한 애플리케이션과 의사 결정 프로세스에 통합할 수 있습니다(Brown et al., 2020; Kaplan et al., 2020; Wei et al., 2022; Ouyang et al., 2022; OpenAI, 2023; Anil et al., 2023). 이로 인해 이러한 대규모 언어 모델(LLM)의 내부 작동 방식과 특히 규모에 따른 진화에 대한 이해에 대한 관심과 중요성이 커집니다. 안타깝게도 규모 확장은 대규모 모델을 다루는 데 많은 계산 리소스가 필요하기 때문에 해석 가능성 연구자의 진입 임계값도 높입니다. 이 작업에서 우리는 최대 66b 매개변수의 OPT 모델 패밀리를 분석하고 의도적으로 분석을 매우 가볍게 유지하여 단일 GPU를 사용하여 수행할 수 있도록 합니다. 우리는 FFN 내부의 뉴런, 즉 Transformer 피드포워드 블록(FFN)의 두 선형 층 사이의 표현에서 개별 활성화에 초점을 맞춥니다. 잔여 스트림의 뉴런과 달리 FFN 뉴런은 의미 있는 특징을 표현할 가능성이 더 높습니다. 요소별 비선형성은 이 표현의 회전 불변성을 깨고 특징이 기본 차원과 일치하도록 합니다(Elhage et al., 2021). 이러한 뉴런이 활성화되면 두 번째 FFN 층의 해당 행을 꺼내 잔여 스트림을 업데이트합니다. 활성화되지 않으면 잔여 스트림을 업데이트하지 않습니다(그림 6).¹ 따라서 이러한 FFN 뉴런의 기능을 두 가지 방법으로 해석할 수 있습니다. (i) 활성화되는 시점을 이해하고 (ii) 잔여 스트림에 제공되는 해당 업데이트를 해석합니다. 첫째, 네트워크의 전반부에서 많은 뉴런이 &quot;죽었다&quot;는 것을 발견했습니다. 즉, 다양한 데이터의 대규모 컬렉션에서 결코 활성화되지 않습니다. 더 큰 모델은 이런 의미에서 더 희소합니다.예를 들어, 66b 모델에서 일부 계층의 뉴런 중 70% 이상이 죽었습니다.동시에, 네트워크의 이 초기 부분에서 살아있는 뉴런 중 다수는 이산적인 특징을 위해 예약되어 있으며 토큰과 n-그램에 대한 지표 함수로 작동합니다.이들은 입력이 특정 토큰이거나 n-그램인 경우에만 활성화됩니다.이러한 토큰 감지기에서 잔여 스트림으로 오는 업데이트의 기능도 매우 ¹OPT 모델에는 ReLU 활성화 함수가 있으므로 &quot;활성화됨&quot; 또는 &quot;활성화되지 않음&quot;이라는 개념은 사소하고 0이 아닌 것과 0을 의미합니다.놀랍습니다.잠재적인 다음 토큰 후보와 관련된 개념을 홍보하는 동시에(Geva et al. (2021, 2022)에 따르면 예상되는 바와 같음) 현재 입력, 즉 트리거에 대한 정보를 제거하는 것을 명시적으로 목표로 합니다. 이는 현재 입력 토큰의 표현이 점진적으로 다음 토큰의 표현으로 변환되는 하향식 처리에서 현재 토큰 ID가 모델에 의해 명시적으로 제거된다는 것을 의미합니다(다음 토큰에 유용한 가산 업데이트의 결과로 암묵적으로 &quot;묻히게&quot; 되는 것이 아니라). 우리가 아는 한, 이는 잔여 스트림에서 정보를 추가하지 않고 제거하는 데 특화된 메커니즘의 첫 번째 예입니다. 마지막으로, 일부 뉴런은 텍스트 패턴에 관계없이 위치 정보를 인코딩하는 역할을 한다는 것을 알게 되었습니다. 토큰 및 n-gram 감지기와 마찬가지로 이러한 뉴런 중 다수는 위치 범위의 지표 함수로 작동합니다. 즉, 특정 범위 내의 위치에 대해 활성화되고 그렇지 않으면 활성화되지 않습니다. 흥미롭게도 이러한 뉴런은 종종 협업합니다. 예를 들어, 125m 모델의 두 번째 계층에는 표시된 위치 범위가 일치하는 10개의 위치 뉴런이 있습니다. 함께 모든 가능한 위치를 효율적으로 커버하고 어떤 뉴런도 중복되지 않습니다. 더 넓은 그림에서, 위치 뉴런은 FFN 계층의 키-값 메모리 뷰에 의문을 제기하며 &quot;각 키는 훈련 데이터의 텍스트 패턴과 상관 관계가 있고 각 값은 출력 어휘에 대한 분포를 유도한다&quot;(Geva et al., 2021, 2022)고 말합니다. 텍스트 패턴에 관계없이 위치에 의존하는 뉴런은 FFN 계층이 키-값 메모리 뷰에 맞지 않는 방식으로 모델에 의해 사용될 수 있음을 나타냅니다. 전반적으로, 우리는 이러한 계층이 수행하는 역할이 여전히 잘 이해되지 않았다고 주장합니다. 전반적으로, 우리는 다음과 같은 뉴런을 찾습니다. • &quot;죽은&quot;, 즉 대규모 다양한 데이터 모음에서 결코 활성화되지 않는 뉴런; • 다음 토큰 후보를 홍보하는 것 외에도 현재 토큰 정보를 명시적으로 제거하는 토큰 및 n-그램 감지기 역할을 하는 뉴런; • 텍스트 내용에 관계없이 위치를 인코딩하여 FFN 계층의 역할이 키-값 메모리 뷰를 넘어 확장됨을 나타냅니다. 스케일이 있으면 모델에 죽은 뉴런과 토큰 감지기가 더 많고 절대 위치에 덜 집중합니다. 2 데이터 및 설정 모델. 우리는 공개적으로 사용 가능한 디코더 전용 사전 학습된 변환기 모음인 OPT(Zhang et al., 2022)를 사용합니다. 우리는 125M에서 66B 매개변수 범위의 모델 크기를 사용하고 HuggingFace 모델 허브에서 모델 가중치를 가져옵니다. 데이터. 우리는 OPT 학습에 사용된 데이터 세트의 개발 분할과 여러 추가 데이터 세트를 포함하는 다양한 소스의 데이터를 사용합니다. 전반적으로 우리는 (i) Wikipedia, DM Mathematics, HackerNews를 포함한 Pile(Gao et al., 2020)의 검증 및 테스트 부분의 하위 집합, (ii) Reddit³(Baumgartner et al., 2020; Roller et al., 2021), (iii) Codeparrot*의 코드 데이터를 사용했습니다. 죽은 뉴런에 대해 이야기하는 섹션 3의 실험의 경우 몇 배 더 많은 데이터를 사용합니다. 구체적으로, 우리는 Wikipedia, DM Mathematics, Codeparrot에서 더 많은 데이터를 추가하고 Pile5: EuroParl, FreeLaw, PubMed 초록, Stackexchange에서 새로운 도메인을 추가합니다. 전반적으로 섹션 3에서 사용된 데이터는 2,000만 개가 넘는 토큰을 가지고 있고, 논문의 나머지 부분에서는 500만 개가 넘는 토큰을 가지고 있습니다. 단일 GPU 처리. 우리는 일부 데이터에 대해서만 뉴런 값의 집합을 사용합니다. 즉, 우리는 전체 모델이나 여러 개의 첫 번째 레이어의 순방향 패스만 실행합니다. 대규모 모델은 단일 GPU에 맞지 않기 때문에 나머지 레이어를 CPU에 유지하면서 한 번에 한 레이어씩 로드합니다. 이를 통해 대규모 모델에 대한 뉴런 활성화를 기록할 수 있습니다. 이 논문의 모든 주요 실험은 단일 GPU에서 수행되었습니다. 3 죽은 뉴런 뉴런 활성화 빈도(그림 1)와 같은 간단한 통계부터 시작해 보겠습니다. 많은 뉴런이 &quot;죽었습니다&quot;. 첫째, 우리는 많은 뉴런이 다양한 데이터에서 결코 활성화되지 않는다는 것을 발견했습니다. 즉, &quot;죽은&quot; 것으로 볼 수 있습니다. 그림 1a는 죽은 뉴런의 비율이 매우 상당하다는 것을 보여줍니다. 예를 들어, 66b 모델의 경우 일부 계층에서 죽은 뉴런의 비율이 70%를 넘습니다. 또한 더 큰 모델이 더 희소한 이유는 (i) 2https://huggingface.co/models 3 Pushshift.io Reddit 데이터 세트는 타사가 추출하여 얻은 기존 데이터 세트로, 소셜 네트워크 Reddit에 게시되고 pushshift.io에서 호스팅하는 사전 처리된 댓글이 포함되어 있습니다. https://huggingface.co/datasets/codeparrot/ codeparrot-clean Shttps://huggingface.co/datasets/EleutherAI/ pile 죽은 뉴런의 비율 평균 활성화 빈도(죽지 않은 뉴런에 대한 평균)모델 계층 0.• 125m0.⚫ 1.3b• 2.7b0.• 6.7b⚫ 13b0.• 30b• 66b0.0.0.0 0.0.0.0.0.Layer(네트워크 깊이에 대한 상대적) (a) 0.Layer(네트워크 깊이에 대한 상대적) (b) 뉴런 뉴런 뉴런 10k 20k 10k 30k 20k 모델: 66b모델: 30b모델: 13b 2k 1k वै लै लै लै모델: 2.7b 10k 5k모델: 1.3b 8k 4k 모델: 350m그림 1: (a) &quot;죽은&quot; 뉴런의 백분율; (b) 죽지 않은 뉴런 중 평균 뉴런 활성화 빈도. 죽은 뉴런이 더 많고 (ii) 살아있는 뉴런은 덜 자주 활성화됩니다(그림 1b). 모델의 전반부만 희소합니다. 다음으로, 이러한 종류의 희소성은 초기 계층에만 특정하다는 것을 알 수 있습니다. 이를 통해 네트워크의 전반부와 후반부가 명확하게 구분됩니다. 전반부는 상당한 비율의 죽은 뉴런을 포함하는 반면, 후반부는 완전히 &quot;살아 있습니다&quot;. 또한, 죽은 뉴런이 가장 많은 계층은 살아있는 뉴런이 가장 드물게 활성화되는 계층입니다. 뉴런에 개념 패킹. 계층 간 이러한 희소성의 차이는 초기 계층에서 상위 계층보다 &quot;개념 대 뉴런&quot; 비율이 훨씬 작기 때문에 설명될 수 있습니다. 직관적으로, 모델은 계층 개념에 인코딩된 집합을 사용 가능한 뉴런에 &quot;분산&quot;하여 표현해야 합니다. 초기 계층에서 인코딩된 개념은 대체로 얕고 이산적일 가능성이 높지만(예: 어휘적) 상위 계층에서 네트워크는 고수준 의미론과 추론을 학습합니다(Peters et al., 2018; Liu et al., 2019; Jawahar et al., 2019; Tenney et al., 2019; Geva et al., 2021). 가능한 얕은 패턴의 수가 많지 않고 잠재적으로 열거 가능하기 때문에 초기 계층에서 모델은 일부 기능에 전용 뉴런을 할당할 수 있습니다(나중에 볼 수 있듯이 할당합니다). 모델에 사용 가능한 뉴런이 많을수록 이를 수행하기가 더 쉽습니다. 이는 더 큰 모델이 더 희소하다는 것을 보여주는 그림 1의 결과와 일치합니다. 다르게 세분화된 의미론적 개념의 공간은 사용 가능한 뉴런의 수에 비해 너무 커서 예약하기 어렵습니다. 많은 전담 뉴런-개념 쌍.SO 죽은 뉴런은 완전히 죽었을까? 그림 1a의 결과는 두 가지 중 하나를 의미할 수 있습니다. 그러나 상위 계층에는 몇 개의 특수 뉴런이 있을 수 있습니다. 예를 들어, BERT에는 관계적 사실을 담당하는 뉴런이 있습니다(Dai et al., 2022). 뉴런 ནྡྲ ཎྜཾ ཨོཾ모델: 6.7b 유니그램 수계층 2k 1k모델: 125m 뉴런을 트리거하는 계층 1-5 6-10 11-20 21-50 51-200 201-1k 1k-2k 2k-50k 그림 2: 트리거할 수 있는 유니그램(즉, 토큰) 수에 따라 분류된 뉴런. 네트워크의 전반부는 살아있는 뉴런만. 사물: (i) 이러한 뉴런은 결코 활성화될 수 없습니다(즉, &quot;완전히 죽었습니다&quot;) 또는 (ii) 매우 드문 패턴에 해당하여 다양한 대규모 데이터 컬렉션에서 결코 발견하지 못했습니다. 후자는 가능하지만, 이는 희소성과 인코딩된 개념의 유형에 대한 위의 논의를 변경하지 않는다는 점에 유의하세요. 반대로: 이는 전담 뉴런을 특정 개념에 할당하는 모델에 대한 가설을 더욱 뒷받침합니다. 4 N-그램 감지 뉴런 이제 모델의 아래쪽 절반에 인코딩된 패턴을 자세히 살펴보고 위에서 관찰된 희소성의 특성을 이해해 보겠습니다. 구체적으로 뉴런 활성화가 입력 n-그램에 어떻게 의존하는지 분석합니다. 토큰 x1, x2, ..., xs가 있는 각 입력 텍스트에 대해 각 위치에서 뉴런 활성화를 기록하고 위치 k에서 뉴런이 활성화되면(즉, 0이 아님) n-그램(k_n+1,. ,xk)이 이 뉴런을 트리거했다고 말합니다. 4.1-4.4절에서는 유니그램(즉, 토큰)에 대해 이야기하고 4.5절에서는 더 큰 n-그램에 대해 다룹니다. 4.1 뉴런을 트리거하는 N-그램 수 먼저 각 뉴런을 트리거할 수 있는 n-그램의 수를 살펴보겠습니다. 각 뉴런에 대해 뉴런 활성화의 95% 이상을 커버하는 n-그램의 수를 평가합니다. 네트워크의 하단 절반의 경우 그림 2는 각 계층의 뉴런이 해당 계층을 덮는 n-그램의 수에 따라 어떻게 분류되는지 보여줍니다(여기서는 유니그램과 더 큰 단위를 표시함) 뉴런 수 토큰 감지기 덮인 토큰 수 덮인 토큰 수 덮인 토큰 수 모델 10k 모델 누적 1.3b 8k• 13b 모델 • 66b 2.7b 6.7b 6k 13b 4k 30b 66b 2k 0.0.0.1 0.2 0.3 0.4 0.0.0 0.0.2 0.3 0.4 0.0.0.1 0.2 0.3 0.0.0.계층(네트워크 깊이에 대한) (a) (b) 그림 3: (a) 토큰 감지 뉴런 수; (b) 토큰을 감지하는 토큰 수 영어: 감지하는 뉴런이 있음: 실선 - 계층당, 점선 - 계층 전체에 걸쳐 누적. 부록 A의 n-그램). 예상대로 더 큰 모델의 뉴런은 더 적은 n-그램으로 덮여 있음을 알 수 있습니다. 또한 가장 큰 모델은 1~5개 토큰에 의해 덮여 있는 뉴런의 상당 부분을 가지고 있습니다. 이는 이전 섹션의 가설과 일치합니다. 이 모델은 특정 전용 뉴런에 걸쳐 불연속적인 얕은 패턴을 확산합니다. 4.2 토큰 감지 뉴런 몇 개(예: 1~5개)의 토큰으로만 트리거될 수 있는 뉴런의 존재는 일부 뉴런이 토큰 감지기 역할을 할 가능성을 시사합니다. 즉, 이전 컨텍스트와 관계없이 입력이 해당 토큰 중 하나인 경우에만 활성화됩니다. 이러한 뉴런을 찾기 위해, 우리는 (1) 1-5개의 토큰에 의해서만 트리거될 수 있는 뉴런을 선택하고, (2) 이 뉴런에 의해 덮인 토큰을 수집합니다(뉴런이 토큰이 존재하는 시간의 최소 95%에서 활성화되는 경우), (3) 전체적으로 이러한 덮인 토큰이 뉴런 활성화의 최소 95%를 담당하는 경우입니다.그림 3a는 실제로 많은 토큰 감지 뉴런이 있음을 보여줍니다.예상대로, 더 큰 모델에는 이러한 뉴런이 더 많고 66b 모델은 전체적으로 5351개의 토큰 감지기를 갖습니다.각 토큰 감지기는 대부분의 경우 같은 단어의 변형(예: 대문자 사용, 단어 앞에 공백이 있는 특수 기호의 존재, 형태적 형태 등만 다름)인 여러 토큰 그룹을 담당한다는 점에 유의하세요.그림 5(위)는 토큰 감지 뉴런에 의해 감지된 토큰 그룹의 예를 보여줍니다. 흥미롭게도, 가장 큰 모델(매개변수의 13b에서 시작)의 동작은 나머지의 7 주의하세요 350m 모델은 나머지와 동일한 패턴을 따르지 않습니다. 이 모델에 대해서는 섹션 6에서 논의하겠습니다. 많은 뉴런의 경우 이 토큰이 대부분의 활성화를 담당하기 때문에 이러한 계산에서 문장 시작 토큰을 제외했습니다. 0. 계층(네트워크 깊이에 대한 상대적) 계층(네트워크 깊이에 대한 상대적) 새로운 전체 토큰 ☐ 이전 계층 토큰과 비교한 새로운 토큰 그림 4: 각 계층에서 다루는 토큰 수와 (i) 새로운 전체 토큰, (ii) 이전 계층 토큰과 비교한 새로운 토큰을 나타냅니다. 나머지의 경우 토큰 감지기 수가 증가하다가 감소하는 반면, 더 큰 모델은 세 가지 단조로운 단계에서 작동하고 첫 번째 계층부터 많은 토큰 감지 뉴런을 갖기 시작합니다(그림 3). 이는 이미 모델 간의 질적 차이를 보여줍니다. 용량이 클수록 더 큰 모델은 더 뚜렷한 단계로 더 복잡한 추론을 수행합니다. 4.3 계층의 앙상블 유사 동작 이제 &quot;감지된&quot; 토큰, 즉 특수 감지 뉴런이 있는 토큰을 살펴보겠습니다.그림 3b는 각 계층에서 감지된 토큰의 수와 계층 전체에서 누적된 감지된 토큰 수를 보여줍니다.예를 들어, 66b 모델은 각 계층에서 1.5k 토큰을 넘지 않지만 전체적으로 10k 토큰 이상에 초점을 맞춥니다.즉, 계층 전체에서 토큰 감지 뉴런은 크게 다른 토큰을 담당합니다.실제로 그림은 다음 각 계층에서 감지된 토큰이 아래 계층에서 다루는 모든 토큰과 대부분 다르다는 것을 보여줍니다.전반적으로 이는 계층의 앙상블 유사(순차적이 아닌) 동작을 나타냅니다.계층이 협력하여 토큰 감지 뉴런이 다른 계층의 서로 다른 토큰을 크게 다룹니다.이 분할 정복 스타일의 전략을 사용하면 더 큰 모델이 전체적으로 많은 토큰을 다루고 용량을 더 효과적으로 사용할 수 있습니다. 원래, 딥 레지듀얼 네트워크의 앙상블과 같은 행동은 컴퓨터 비전 모델에서 관찰되었습니다(Veit et al., 2016). 트랜스포머의 경우, 이전 증거에는 레이어를 삭제하거나 재정렬하는 것이 성능에 큰 영향을 미치지 않는다는 것을 보여주는 간단한 실험이 포함됩니다(Fan et al., 2020; Zhao et al., 2021). 4.4 토큰 감지기는 트리거를 억제합니다.이제 토큰 감지 뉴런의 역할을 토큰 감지 L=1,n=뉴런(66b)의 의미를 해석하여 모델에서 이해해 보겠습니다.감지된 토큰 Ġtitle, title, Ġtitles, Title L=5, n=Gweather, GWeather, weather, Weather L=10, n=Ghe, GHe, Ghim, He, GHim, him, Ghimself, ĠHimself L=10, n=Gschool, GSchool, Gschools, School, chool, GSchools, Gschooling 상위 승격 0.0.0.07 ĠShot 0.06 Ġsponsor holder 0.11 Gpatterns 0.09 Ġself 0.girl holders 0.10 Ġconditions 0.aps 0.boy 0.10 Ġpattern 0.08 Ġcondition ⠀ 0.: House 0.Ġdistrict 0.proof -0.Ghe ⠀ 가 잔여 ⠀ -0.category -0.He -0.school -0.-0.07 Gweather Title -0.him -0.GĠschools -0.06 Ġtitle -0.07 GWeather -0.Ġhis -0.13 Ġschool Top suppressed -0.07 Ġtitles -0.08 title -0.09 Ġtitle -0.07 Weather -0.GHIS -0.14 School -0.bart -0.15 Ġhis -0.Ġschools -0.antry -0.His -0.ĠSCHOOL -0.his -0.ĠSchool 그림 5: 토큰 감지 뉴런에 대한 상위 승격 및 억제 토큰의 예(Ġ is 단어 앞의 공백을 나타내는 특수 기호 - OPT 토크나이저에서 단어의 일부임); OPT-66b 모델. 0.12 스미스 0.10 광산 상위 홍보 어휘 프로젝트 -0.06 골드 -0.07 골드 OOOO + ● ● ● ●골드 ReLU(xW) x W₂ 활성화된 뉴런 X FFN 블록 OOOOH 상위 억제 그림 6: 개념 억제의 직관: 어휘에 대한 FFN 업데이트의 상위 투영뿐만 아니라 하위도 살펴봅니다. 음수 값으로 추가된 개념은 억제됩니다. 이들은 잔여 스트림을 업데이트합니다. 계층 전체에서 잔여 스트림의 토큰 표현은 현재 입력 토큰의 토큰 임베딩에서 다음 토큰의 분포를 인코딩하는 표현으로 변환됩니다. 이 변환은 각 계층의 어텐션 및 FFN 블록에서 나오는 가산 업데이트를 통해 발생합니다. FFN 뉴런이 활성화될 때마다 두 번째 FFN 계층의 해당 행(이 뉴런의 값에 곱함)이 잔여 스트림에 추가됩니다(그림 6의 그림 참조). 이 FFN 행을 어휘에 투영하면 잔여 스트림에 인코딩된 출력 분포에 미치는 영향 측면에서 이 업데이트(따라서 이 뉴런의 역할)에 대한 해석을 얻을 수 있습니다. &#39;OPT 모델의 경우 절대 위치 임베딩과 함께. 현재 토큰 억제: 암묵적 또는 명시적? 이전에는 이 영향이 최상위 투영, 즉 승격된 토큰(Geva et al., 2021, 2022)의 관점에서만 이해되었습니다. 이는 계층 전체에 걸쳐 현재 토큰 ID의 명시적 손실보다는 암묵적 손실을 뒷받침하는 기존 관점을 반영합니다. 즉, 현재 ID가 모델에서 명시적으로 제거되는 것이 아니라 다음 토큰에 유용한 업데이트의 결과로 &quot;묻힌&quot;다는 관점입니다. 대조적으로, 우리는 상단 투영뿐만 아니라 하단도 살펴봅니다. 이러한 투영이 음수이면 해당 토큰은 모델에 의해 억제됩니다(그림 6). 모델에서의 명시적 토큰 억제. 토큰 감지 뉴런이 종종 감지한 토큰을 의도적으로 억제하는 것을 발견했습니다. 그림 5는 상단에서 승격되고 억제된 개념과 함께 토큰 감지 뉴런의 여러 예를 보여줍니다. 상단에서 승격된 개념은 이전 연구와 일치하지만(Geva et al. (2021, 2022)과 일치하는 잠재적인 다음 토큰 후보임), 상단에서 억제된 개념은 다소 예상치 못한 것입니다. 바로 이 뉴런을 트리거하는 토큰입니다. 즉, 이러한 뉴런에 해당하는 벡터 업데이트는 뉴런을 트리거하는 토큰에서 멀어지는 것과 동시에 다음 토큰 후보의 방향을 가리킨다는 것을 의미합니다. 이러한 업데이트는 동시에 두 가지 매우 다른 역할을 하기 때문에 이는 사소한 일이 아니라는 점에 유의하세요. 전반적으로 토큰을 감지하는 뉴런의 80% 이상에서 해당 업데이트는 트리거 토큰에서 부정적인 방향을 가리킵니다(단, 트리거 토큰이 항상 그림 6의 예에서처럼 억제된 개념의 맨 위에 있는 것은 아닙니다). 전반적으로, 우리는 모델이 mechfrequency 활성화 진동을 가질 수 있다고 주장합니다.O와 1 활성화 주파수를 갖는 두 영역 둘 중 하나: O 활성화 주파수 또는기타 위치에 대한 강한 종속성 1.1.1.0.0.0.0.0.0.0.0.1.0.0.0.0.1.1.0.0.0.0.0.0.0.0.000.0.000.2k 1k 2k1k 2k1k 2k 1.1.1.1.1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.1k 2k1k 2k1k 2k 1k 1k 2k1k 2k 위치 위치 위치 위치 위치 그림 7: 위치 뉴런의 유형. 위쪽 행 - &quot;강한&quot; 패턴, 아래쪽 행 - &quot;약한&quot; 패턴. 잔여 스트림에서 정보를 제거하는 것을 목표로 하는 애니즘으로, 이는 향후 작업에서 더 자세히 살펴볼 수 있습니다. 4.5 유니그램 너머 부록 A에서 유니그램에 대한 관찰 결과를 반영하는 바이그램과 트라이그램에 대한 결과를 보여줍니다. (i) 더 큰 모델은 더 특화된 뉴런을 가지고 있습니다. (ii) 각 계층에서 모델은 대부분 새로운 n-그램을 다룹니다. 흥미롭게도, 더 큰 n-그램의 경우 더 큰 모델과 더 작은 모델 사이에 더 큰 격차가 있음을 알 수 있습니다. 5 위치 뉴런 죽은 뉴런을 분석할 때(섹션 3), 다양한 데이터에서 일관되게 몇 개의 첫 번째 토큰 위치를 제외하고는 결코 활성화되지 않는 뉴런도 발견했습니다. 이는 모델에서 위치가 어떻게 인코딩되는지, 특히 일부 뉴런이 위치 정보를 인코딩하는지 여부를 더 자세히 살펴보도록 동기를 부여합니다. 5.1 위치 뉴런 식별 직관적으로, 우리는 활성화 패턴이 토큰 위치에 의해 정의되거나 적어도 강하게 의존하는 뉴런을 찾고 싶습니다. 형식적으로, 우리는 활성화가 위치와 높은 상호 정보를 갖는 뉴런을 식별합니다. 각 뉴런에 대해 두 개의 확률 변수 간의 상호 정보를 평가합니다. • act – 뉴런이 활성화되었는지 여부({Y, N}), ‚ . . .‚T}). ⚫ pos – 토큰 위치({1, 2, ..., = 공식 설정. Wikipedia, DM Mathematics 및 Codeparrot의 전체 길이 데이터(즉, T 2048 토큰)에 대한 뉴런 활성화를 수집합니다. (pos) frn을 위치 pos에서 뉴런 n의 활성화 빈도로 하고 frn을 이 뉴런의 전체 활성화 빈도로 합니다. 그러면 원하는 상호 정보는 다음과 같습니다.TI(act, pos) ==Σfr (pos).log pos=(1 - fr (pos)) · log fr.(Pos) + 1- frn frn (pos).1 – frn 뉴런 선택. I(act, pos) &gt; 0.05인 뉴런을 선택합니다. 즉, 위치가 있는 높은 상호 정보입니다. 이렇게 하면 활성화 빈도가 콘텐츠가 아닌 위치에 따라 달라지는 뉴런이 생성됩니다. 실제로 예를 들어 뉴런이 데이터 도메인에 관계없이 특정 위치 범위 내에서 항상 활성화되는 경우 이 뉴런을 위치를 담당하는 것으로 처리할 수 있습니다. 적어도 어느 정도는 그렇습니다. 5.2 유형 위치 뉴런 위치 뉴런을 선택한 후 활성화 패턴, 즉 위치에 따른 활성화 빈도에 따라 분류합니다(그림 7). 진동형. 이러한 뉴런은 그림 7에서 보라색으로 표시됩니다. 이러한 패턴이 강하면(윗줄), 활성화 패턴은 위치 범위의 지표 함수입니다. 다시 말해, 이러한 뉴런은 위치가 특정 집합에 속하는 경우에만 활성화됩니다. 활성화 패턴은 데이터 도메인에서 변경되지 않으므로 위치에 의해서만 정의되고 어휘 또는 의미 정보의 존재에 의해 정의되지 않는다는 점에 유의하세요. 두 가지 유형의 활성화 극단. 이것들은 활성화 패턴이 진동적이지 않지만 활성화 빈도가 &quot;활성화 극단&quot;인 0(활성화되지 않음)과 1(항상 활성화됨)에 도달하는 구간이 있는 뉴런입니다. 이러한 뉴런은 가장 자주 어떤 값보다 작거나 큰 위치에서만 활성화되고 그렇지 않으면 활성화되지 않습니다. 진동 뉴런과 마찬가지로 10자세한 내용은 부록 B를 참조하세요.1.125m 1.3b2.7b ထာ6.7b13b30b2 N WATAoooooo 66b계층 그림 8: 각 모델의 위치 뉴런. 각 원은 단일 뉴런에 해당하고 색상과 강도는 그림 7에 표시된 패턴 유형에 해당합니다. 이러한 패턴은 강하고(그림 7, 위쪽 행) 또한(거의) 지표 함수입니다. 차이점이전 두 유형과 달리 이러한 뉴런의 활성화 패턴은 극단 값 0 또는 1 중 하나에만 도달할 수 있습니다(그림 7, 녹색). 이는 이들이 결코 지표 함수로 동작하지 않는다는 것을 의미하지만, 뉴런이 활성화되거나 활성화되지 않는 위치 범위가 토큰 위치에만 의존하는 경우가 있습니다. 기타. 마지막으로, 이들은 활성화 패턴이 위치에 크게 의존하지만 활성화 빈도가 0 또는 1로 유지되는 구간이 없는 뉴런입니다(그림 7, 노란색). 일반적으로 이러한 활성화 패턴은 이전 세 가지 유형보다 위치와의 상호 정보가 낮습니다. 강한 패턴 대 약한 패턴. 또한 각 유형의 &quot;강한&quot; 및 &quot;약한&quot; 버전을 구별하며, 색상 강도로 추가로 표시합니다(그림 7, 위쪽 대 아래쪽 행). 처음 세 가지 유형의 위치 뉴런의 경우, 강한 패턴과 약한 패턴의 차이는 해당 위치 범위에서 활성화 빈도가 0(또는 1)인지 또는 0(또는 1)에 가깝지만 같지는 않은지에 있습니다. 마지막 유형의 경우, 이 차이는 이웃 위치에 대한 이 값을 알고 있을 때 특정 위치에서 활성화 빈도를 얼마나 잘 예측할 수 있는지에 있습니다(비공식적으로 &quot;얇은&quot; 대 &quot;두꺼운&quot; 그래프). 5.3 모델 전체의 위치 뉴런 각 모델에 대해 그림 8은 계층 전체의 위치 뉴런을 보여줍니다. 작은 모델은 위치를 더 명확하게 인코딩합니다. 먼저, 작은 모델은 진동 뉴런에 크게 의존한다는 것을 알 수 있습니다. 이것은 매개변수가 6.7b보다 작은 모델에서 가장 빈번한 위치 뉴런 유형입니다. 더 넓은 위치 범위에 대한 지표 함수 역할을 하는 많은 &quot;빨간색&quot; 뉴런과 결합하여 모델은 토큰의 절대 위치를 다소 정확하게 도출할 수 있습니다. 흥미롭게도, 더 큰 모델은 진동 뉴런이 없고 빨간색과 초록색 원으로 표시된 더 일반적인 패턴에 의존합니다. 13b에서 66b까지 모델이 양면 빨간색 뉴런을 잃고 일방적 녹색 뉴런을 더 많이 사용하는 것을 볼 수도 있습니다. 이는 더 작은 모델과 더 큰 모델 간의 질적 차이 중 하나를 암시합니다. 전자가 절대 위치를 더 정확하게 인코딩하는 반면, 후자는 절대 위치보다 더 의미 있는 것에 의존할 가능성이 높습니다. 이는 추론 작업에서 절대 위치 인코딩이 길이 일반화에 해롭다는 것을 보여주는 최근 연구를 보완합니다(Kazemnejad et al., 2023). 동일한 모델 크기이지만 다양한 위치 인코딩을 사용한 실험과 달리, 우리는 규모에 따른 변화를 추적합니다. 모든 모델이 절대 위치 인코딩으로 훈련되었음에도 불구하고 더 강력한 모델은 절대 위치에서 추상화되는 경향이 있음을 알 수 있습니다. 위치 뉴런은 팀으로 작동합니다. 흥미롭게도, 위치 뉴런은 전체 위치 세트를 함께 다루기 위해 협력하는 것으로 보입니다. 예를 들어, 125m 모델의 두 번째 계층에 있는 10개의 강하게 진동하는 뉴런(그림 8에서 진한 보라색 원으로 표시)을 더 자세히 살펴보겠습니다. 이들은 지표 함수로 작용하므로 위치 범위 125m, 계층 2 강한 진동하는 뉴런을 다음과 같이 그릴 수 있습니다. 1114 422 850 1693 167 213 968 1948 1900 각 뉴런이 감지한 위치 범위 250 500 750 1000 1250 1500 1750 그림 9: 125m 모델의 두 번째 계층에 있는 강한 진동하는 뉴런이 나타내는 위치 범위. 이러한 뉴런 각각이 나타냅니다. 그림 9는 (i) 이러한 뉴런에 대한 표시된 위치 범위가 이동까지 유사하고, (ii) 이동이 &quot;완벽한&quot; 순서로 구성되어 있어 이 10개의 뉴런이 모두 모든 위치를 효율적으로 포괄하여 이러한 뉴런 중 어느 것도 중복되지 않는다는 것을 보여줍니다. 모델 내의 두 단계. 마지막으로, 그림 8은 모델 내의 위치 정보의 두 단계의 기복을 보여줍니다. 대략적으로 모델의 첫 번째 1/3과 나머지입니다. 흥미롭게도, 위치 패턴의 선호도도 단계 사이에서 변경됩니다. 예를 들어, &quot;빨간색&quot; 뉴런에 대한 선호도는 1.3b 및 2.7b 모델의 경우 진동하는 보라색 패턴으로 변경되고, &quot;빨간색&quot; 패턴은 13b 및 30b 모델의 상위 단계에서 덜 중요해집니다. 모델의 첫 번째 1/3은 죽은 뉴런과 n-그램 감지기가 있는 희소 단계에 해당합니다(섹션 3, 4). 따라서 이 두 단계에서 위치 정보는 먼저 얕은 패턴을 감지하는 데 로컬하게 사용된 다음, 더 전역적으로 사용되어 더 긴 맥락을 사용하고 의미 정보를 인코딩하는 데 도움이 된다고 가설을 세울 수 있습니다. 이전에는 Voita et al.(2019a)에서 언어 모델 내부에서 처리의 뚜렷한 하향식 단계가 관찰되었습니다. 저자는 표현이 계층 전체에서 정보를 얻고 잃는 방식이 훈련 목표에 의해 정의되며, 무엇보다도 위치 정보가 손실되어야 하는 이유(그리고 손실되는 이유)를 설명했습니다. 이는 이 작업에서 얻은 결과와 일치합니다. 두 번째 단계에는 많은 위치 패턴이 있지만 첫 번째 단계보다 약하다는 것을 알 수 있습니다. 5.4 위치 뉴런은 위치 인코딩 없이도 학습됩니다. 최근에 위치 인코딩 없이도 자기 회귀 언어 모델이 여전히 위치 정보를 학습한다는 것이 밝혀졌습니다(Haviv et al., 2022). 이러한 &quot;NoPos&quot; 모델이 위치를 인코딩하는 데 사용하는 메커니즘은 위치 뉴런이라고 가설을 세웁니다. 이를 확인하기 위해 위치 인코딩이 있는 125m 모델과 없는 125m 모델 두 가지 버전을 훈련하고 위치 뉴런의 유형을 비교합니다.설정.표준 OPT 설정을 사용하지만 더 작은 훈련 데이터 세트로 125m 모델을 훈련했습니다.GPT-2 훈련 데이터(Radford et al., 2019)의 오픈 클론인 OpenWebText 코퍼스(Gokaslan and Cohen, 2019)를 사용했습니다.이 데이터 세트에는 3B 토큰이 포함되어 있습니다(OPT의 경우 180B와 비교).위치 인코딩이 없는 위치 뉴런.그림 10은 위치 인코딩이 있는 125m 모델과 없는 125m 모델의 위치 뉴런을 보여줍니다.실제로 위치 인코딩이 없는 모델에도 많은 강력한 위치 패턴이 있음을 알 수 있습니다.그러나 NoPos 모델에는 다른 위치 뉴런과 결합하여 절대 위치를 비교적 정확하게 인코딩할 수 있는 진동 뉴런이 없습니다. 이는 NoPos 모델이 보다 일반적인 패턴, 예를 들어 위치가 어떤 값보다 크거나 작은지 여부를 인코딩하는 &quot;빨간색&quot; 뉴런에 의존한다는 것을 의미합니다.진동 뉴런은 더 긴 학습이 필요합니다.마지막으로, 진동 패턴은 긴 학습에서만 나타난다는 것을 발견했습니다.그림 11은 50k, 150k 및 300k 학습 배치로 학습된 기준 125m 모델이 학습한 위치 패턴을 보여줍니다.모든 모델이 매우 강력한 위치 패턴을 가지고 있지만 마지막 모델만 진동 뉴런을 가지고 있음을 알 수 있습니다.분명히 절대 위치를 학습하려면 더 긴 학습 시간이 필요합니다.5.5 키 값 메모리로서의 FFN 의심 현재 널리 퍼진 믿음은 변압기 기반 언어 모델의 피드포워드 계층이 키 값 메모리로 작동한다는 것입니다. 구체적으로, &quot;각 키는 훈련 예제의 텍스트 패턴과 상관 관계가 있으며, 각 값은 출력 어휘에 대한 분포를 유도합니다&quot;(Geva et al. (2021, 2022); Dai et al. (2022); Meng et al. (2022); Ferrando et al. (2023) 등). 섹션 4.4에서 이것이 일부 뉴런에 대해 사실임을 확인했지만, 이 섹션의 결과는 FFN 계층이 키-값 메모리 뷰에 맞지 않는 방식으로 모델에 의해 사용될 수 있음을 보여줍니다. 특히, 강력한 위치 뉴런의 활성화는 텍스트 내용과 관계없이 위치에 따라 정의되며, 해당 값은 어휘에 대한 의미 있는 분포를 인코딩하지 않는 것으로 보입니다. 즉, 이러한 뉴런의 역할은 텍스트 패턴을 다음 토큰 후보 세트에 일치시키는 것과 다릅니다. 더 넓은 맥락에서 이는 수행된 역할을 의미합니다00BaselineLayers NoPos: 위치 인코딩 없음 88888888888888888888........ oooooooooooooLayers 그림 10: 125m 모델의 위치 뉴런: 기준선 대 위치 인코딩이 없는 모델. 두 모델 모두 300k 배치로 학습되었습니다.50k 학습 배치 ၁ဝဝဝဝဝ150k 학습 배치Layers 300k 학습 배치그림 11: 50k, 150k 및 300k 배치로 학습된 기본 125m 모델의 위치 뉴런. Transformer 피드포워드 층은 여전히 잘 이해되지 않았습니다.6 350m 모델: 이상한 모델 위에서 언급했듯이 350m 모델은 나머지 모델과 동일한 패턴을 따르지 않습니다.특히, 죽은 뉴런이 없고(섹션 3) 뉴런 활성화는 그림 2의 다른 모든 모델에서 보았듯이 n-그램을 트리거하는 것과 관련하여 희소하지 않은 것으로 보입니다.-모델링 비트는 해석성에 영향을 미칩니다. 영어: 350m 모델이 나머지 모든 모델과 다르게 구현되었다는 점을 알아차리면 그다지 놀랍지 않습니다.이 모델은 어텐션과 피드포워드 블록 뒤에 LayerNorm을 적용하는 반면, 다른 모든 모델은 그 전에 적용합니다.분명히, 이러한 사소해 보이는 구현 세부 사항은 모델 구성 요소의 해석성에 상당히 큰 영향을 미칠 수 있습니다.사실, 이전 작업에서도 해석성을 장려하기 위해 특정 모델링 측면을 선택하려고 했습니다.이러한 작업의 예로는 해석 가능한 뉴런 수를 늘리기 위한 활성화 함수 선택(Elhage et al., 2022), 출력 분포 또는 어텐션을 더 해석 가능하게 만들기 위한 희소 소프트맥스 변형에 대한 방대한 작업(Martins and Astudillo(2016); Niculae and Blondel(2017); Peters et al. 11그러나 위치 뉴런도 있습니다.부록 B.2의 그림 16 참조).12https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py(2019); Correia et al. (2019); Martins et al. (2020) 등) 또는 명시적인 모듈식 구조를 사용하여 해석 가능한 구조를 구축하는 더 극단적인 접근 방식(Andreas et al. (2016); Hu et al. (2018); Kirsch et al. (2018); Khot et al. (2021) 등)이 있습니다. 직관적으로 OPT 모델에서 수행한 것처럼 ReLU 활성화 함수를 선택하는 것은 희소 소프트맥스 변형을 개발하는 것과 동일한 동기를 갖는 것으로 볼 수 있습니다. 모델의 정확한 0은 본질적으로 해석 가능합니다. 7 추가
--- RELATED WORK ---
역사적으로 뉴런은 분석의 기본 단위였습니다.초기 연구는 먼저 이미지(Krizhevsky et al., 2012)를 위한 합성곱 신경망에서 시작되었고 나중에 합성곱 텍스트 분류기(Jacovi et al., 2018)를 위한 것이었습니다.우리의 연구와 유사하게 Jacovi et al.(2018)도 n-gram 검출기를 찾았습니다.그러나 작은 합성곱 텍스트 분류기의 경우 이는 우리 연구와 같은 대규모 Transformer 기반 언어 모델에 비하면 거의 사소한 관찰입니다.순환 네트워크의 경우 해석 가능한 뉴런에는 줄 길이, 괄호 및 따옴표(Karpathy et al., 2015), 감정 뉴런(Radford et al., 2017)와 같은 간단한 패턴과 기계 번역 모델의 다양한 뉴런(예: 괄호, 따옴표 등 추적)과 동사 시제(Bau et al., 2019)와 같은 상위 수준 개념과 관련된 뉴런이 포함됩니다.Transformer 기반 BERT의 경우 Dai et al. (2022)는 피드포워드 블록 내부의 일부 뉴런이 사실적 지식을 저장하는 역할을 한다는 것을 발견했습니다. 더 큰 분석 단위에는 어텐션 블록(Voita 등(2018, 2019b); Clark 등(2019); Kovaleva 등(2019); Baan 등(2019); Correia 등(2019) 등), 피드포워드 레이어(Geva 등, 2021, 2022) 및 특정 작업을 담당하는 회로(Wang 등, 2022; Geva 등, 2023; Hanna 등, 2023)가 포함됩니다. 감사의 말 저자는 유익한 토론과 도움이 되는 피드백을 제공해준 Nicola Cancedda, Yihong Chen, Igor Tufanov 및 FAIR London 팀에 감사드립니다. 참고문헌 Jacob Andreas, Marcus Rohrbach, Trevor Darrell 및 Dan Klein. 2016. Neural module networks. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR) 회의록에서. 로한 아닐, 앤드류 M. 다이, 오르한 피라트, 멜빈 존슨, 드미트리 레피킨, 알렉상드르 파소스, 시아막 샤케리, 에마누엘 타로파, 페이지 베일리, 지펭 첸, 에릭 추, 조나단 H. 클라크, 로랑 엘 샤페이, 얀핑 황, 캐시 마이어-헬스턴, 가우라브 미슈라, 에리카 모레이라, 마크 오머닉, 케빈 로빈슨, 세바스찬 루더, 이 타이, 케판 샤오, 위안중 쉬, 유징 장, 구스타보 에르난데스 아브레고, 준완 안, 제이콥 오스틴, 폴 바람, 얀 보타, 제임스 브래드버리, 시다르타 브라흐마, 케빈 브룩스, 미셸 카타스타, 용 청, 콜린 체리, 크리스토퍼 A. 쇼켓-추, 아칸샤 초우데리, 클레멘트 크레피, 샤치 데이브, 모스타파 데흐가니, 수니파 Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, 임현택, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant 미스라, 메이삼 무살렘, 재커리 Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov 및 Yonghui Wu. 2023. Palm 2 기술 보고서. Joris Baan, Maartje ter Hoeve, Marlies van der Wees, Anne Schuth, Maarten de Rijke. 2019. 추상 요약에서 다중 헤드 주의 이해. Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, James Glass. 2019. 신경 기계 번역에서 중요한 뉴런 식별 및 제어. 국제 학습 표현 컨퍼런스, 뉴올리언스. Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, Jeremy Blackburn. 2020. pushshift reddit 데이터 세트. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc. Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning. 2019. BERT는 무엇을 보는가? BERT의 주의에 대한 분석. 2019 ACL 워크숍 BlackboxNLP의 진행 과정: NLP를 위한 신경망 분석 및 해석, 276-286쪽, 이탈리아 피렌체. Association for Computational Linguistics. Gonçalo M. Correia, Vlad Niculae, André FT Martins. 2019. 적응적 희소 변환기. 2019 경험적 언어학 컨퍼런스의 진행 과정
