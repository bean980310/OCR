--- ABSTRACT ---
출처를 명시하는 것은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org에서 허가를 요청하세요. © 2023 저작권은 소유자/저자가 보유합니다. 출판권은 ACM에 라이선스되었습니다. 0730-0301/2023/8-ART1 $15.https://doi.org/10.1145/생성 모션 매칭 모듈은 모션 매칭에 대한 생성 비용 함수로 양방향 시각적 유사성을 활용하고, 다단계 프레임워크에서 작동하여 모범 모션 매칭을 사용하여 무작위 추측을 점진적으로 개선합니다. 다양한 모션 생성 외에도 모션 완료, 키 프레임 가이드 생성, 무한 루핑, 모션 재조립을 포함하여 모션 매칭만으로는 불가능한 여러 시나리오로 확장하여 생성 프레임워크의 다재다능함을 보여줍니다. • CCS 개념: 컴퓨팅 방법론 → 모션 처리. 추가 키워드 및 구문: 모션 합성, 생성 모델, 모션 매칭 ACM 참조 형식: Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen. 2023. 생성 모션 매칭을 통한 예제 기반 모션 합성. ACM Trans. Graph. 42, 4, Article 1(2023년 8월), 12페이지. https://doi.org/10.1145/
--- INTRODUCTION ---
자연스럽고 다양하며 세부적인 동작을 생성하는 것은 컴퓨터 애니메이션의 핵심 문제입니다. 모션 캡처(모캡) 시스템을 통해 방대한 양의 동작 데이터를 수집하거나 정교한 애니메이션을 수동으로 작성하는 것은 비용이 많이 들고 지루한 것으로 알려져 있습니다. 따라서 동작 데이터 세트는 일반적으로 제한적이며, 특히 ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. 1:• Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen 스타일, 골격 구조 또는 생물 유형의 다양성으로 인해 기존 데이터 기반 동작 합성 방법의 효과가 저하됩니다. 따라서 제한된 예제 시퀀스에서 다양하고 광범위한 동작을 합성하는 생성 능력을 향상시키는 것이 중요한 연구 문제가 되었습니다. 최근 몇 년 동안 딥 러닝이 컴퓨터 애니메이션 분야를 강타했습니다. 딥 러닝 방법은 방대하고 포괄적인 데이터 세트에서 학습할 때 다양하고 자연스러운 동작을 합성하는 능력을 보여주었습니다. [Holden et al. 2016, 2017; Henter et al. 2020; Tevet et al. 2022a; Raab et al. 2023a; Tseng et al. 2022]. 더욱 고무적인 점은, 최근 매우 축소된 설정에서 성공이 재현되었다는 것입니다[Li et al. 2022]. 여기서는 학습을 위해 하나의 시퀀스만 제공되지만, 신경망은 샘플의 내부 분포를 학습하고 입력 예제 시퀀스의 다양한 변형을 합성하는 능력을 보여줍니다. 그럼에도 불구하고, 신경 동작 합성 방법은 실제로 적용 가능성을 제한하는 몇 가지 단점을 가지고 있습니다: (i) 긴 학습 시간이 필요합니다; (ii) 지터링이나 과도한 평활화와 같은 시각적 아티팩트가 발생하기 쉽습니다; (iii) 크고 복잡한 골격 구조에 잘 확장되지 않습니다. 이 논문에서는 문제에 대한 대안적인 접근 방식을 탐구합니다. 컴퓨터 애니메이션의 고전적인 아이디어인 동작 최근접 이웃[Lee et al. 2010년]은 딥 러닝 시대보다 훨씬 이전으로 거슬러 올라가며, 캐릭터 애니메이션을 위한 최첨단 산업 솔루션인 모션 매칭이 설립된 시기입니다. [Büttner and Clavet 2015], 매우 고품질의 모션 합성을 제공합니다. 모션 매칭은 자연스럽게 보이고 다양한 로컬 컨텍스트에 대응하는 캐릭터 애니메이션을 생성합니다. 전체 자연스러운 모션 공간의 로컬 근사치로 대규모 모캡 데이터베이스를 사용하여 모션 매칭은 주어진 로컬 컨텍스트에 가장 잘 맞는 모션 패치를 검색합니다. 그러나 대규모 데이터 세트에 대한 의존성은 우리의 목표와 맞지 않습니다. 우리는 가능한 한 많은 다양한 동작을 하나 또는 몇 개의 예제에서 &quot;마이닝&quot;하는 생성 모델을 추구합니다. Granot et al. [2022]의 이미지 합성 작업에서 영감을 받아, 우리는 모션 매칭을 생성 모델에 주입하고 생성 모션 매칭(GenMM, &quot;젬&quot;으로 발음)을 생성하기 위해 다음과 같은 통찰력을 취합니다. 첫째, 모션 매칭의 모션 품질을 유지하고 생성 기능을 주입하기 위해 [Simakov et al. 2008]에서 도입한 양방향 유사성을 모션 매칭을 위한 새로운 생성 비용 함수로 활용합니다. 양방향 유사성은 예제와 합성된 시퀀스 간의 패치 분포를 비교하는 목적을 갖습니다. 구체적으로, 합성된 시퀀스에는 예제 시퀀스의 모션 패치만 포함되도록 하고, 그 반대로 예제에는 합성의 모션 패치만 포함되도록 합니다. 결과적으로 합성에 아티팩트가 도입되지 않으며, 중요한 모션 패치도 손실되지 않습니다. 둘째, 우리는 다단계 프레임워크를 사용하여 예제와 패치 분포 차이가 최소인 동작 시퀀스를 점진적으로 합성하여 다양한 시간 해상도에서 패치 분포를 캡처합니다. 마지막으로, 우리는 GAN 기반 방법의 생성적 다양성이 주로 무조건 노이즈 입력에서 비롯된다는 관찰을 활용합니다[Granot et al. 2022]: 우리는 노이즈를 가장 거친 합성 단계에 입력하고 매우 다양한 합성 결과를 얻습니다. 우리는 GenMM이 소수의 입력 예제에서만 다양한 동작을 생성하는 데 매우 유능하다는 것을 보여줍니다. 특히 기존 작업에 비해 GenMM은 여러 가지 장점을 제공합니다. ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. • GenMM은 사전 학습 없이 매우 빠르게 실행됩니다. 동작 시퀀스는 1초도 채 걸리지 않고 합성할 수 있습니다. • GenMM은 동작 매칭의 매력적인 특성을 계승하여 고품질과 충실도의 동작을 생성합니다. • GenMM은 신경망이 어려움을 겪는 매우 복잡한 골격(그림 1에서 433개 관절이 있는 캐릭터 참조)으로 원활하게 확장됩니다[Li et al. 2022]. • GenMM을 여러 시퀀스가 있는 입력으로 확장하고 모든 예를 포함하도록 합성을 장려하는 것은 쉬운데, 이는 GAN 기반 방법에서는 사소한 일이 아닙니다[Li et al. 2022]. 다양한 동작 생성 외에도 동작 완료, 키 프레임 가이드 생성, 무한 루프, 동작 재조립과 같이 동작 매칭만으로는 달성할 수 없는 다양한 시나리오로 확장하여 생성 프레임워크의 다재다능함을 보여줍니다. 이는 모두 생성 동작 매칭의 공유 기반을 통해 가능합니다. 2
--- RELATED WORK ---
우리는 운동학 기반 모션 합성에 대한 가장 관련성 있는 연구를 검토합니다. 또한 특히 우리가 영감을 얻은 패치 기반 이미지 합성의 최근 발전 사항을 간략히 다룹니다. 모션 합성. 기존 모션을 다양화하여 새로운 모션을 생성하는 것은 Perlin과 Goldberg [1996]의 작업으로 거슬러 올라갈 수 있으며, Perlin 노이즈 [Perlin 1985]를 모션 클립에 추가하여 로컬 다양성이 있는 변형을 얻습니다. Pullen과 Bregler [2002]는 저주파 패치를 일치시키고 고주파 세부 정보를 혼합하여 모캡 데이터를 사용하여 거친 키 프레임 모션을 향상시킬 수 있음을 보여줍니다. Li et al. [2002]는 데이터 세트에서 유사한 패치를 일치시켜 그래프 모델을 구성하고 로컬 및 구조적 변형이 있는 임의 샘플을 생성하기 위한 확률적 모델을 만듭니다. 선형 동역학 모델을 사용하기 때문에 모델은 만족스러운 결과를 얻으려면 대규모 교육 데이터 세트가 필요하고 품질과 다양성의 딜레마에 직면합니다. 동시에 모션 그래프 [Kovar et al. 2002; Lee et al. 2002; Arikan 및 Forsyth 2002]는 결정론적 상태를 유지하면서 유사한 이산 그래프 모델, 즉 상태 머신을 사용하고 사용자 입력에 대화형으로 응답하는 문자를 보여줍니다. 그러나 이산 공간은 본질적으로 민첩성과 반응성을 제한합니다. 따라서 그 이후로 대규모 데이터 세트를 통계적 모델로 요약하려는 노력도 이루어졌습니다[Pullen 및 Bregler 2000; Brand 및 Hertzmann 2000; Bowden 2000; Grochow et al. 2004; Chai 및 Hodgins 2007; Wang et al. 2007]. 데이터 세트를 체계적이지만 이산적인 구조나 통계적 모델로 정렬하는 대신, 동작 최근접 이웃[Lee et al. 2010; Levine et al. 2012]은 연속적인 동작 필드에서 직접 작동하여 현재 포즈의 가장 가까운 이웃을 보간하는 제어 정책을 학습합니다. 그 후, [Büttner 및 Clavet 2015]는 Motion Matching을 소개합니다.
--- METHOD ---
s. 하나 또는 몇 개의 예가 주어지면, 매우 복잡한 골격 구조(중간)가 있더라도, 우리 프레임워크는 (a) 1초의 몇 분의 1 이내에 고품질의 새로운 동작을 합성할 수 있습니다. (b) 예제 동작 패치로 부분 동작(하체 동작)을 완료할 수 있습니다. (c) 희소한 키 프레임 세트(파란색 옷)에 의해 안내되는 일관된 시퀀스를 합성할 수 있습니다. (d) 지정된 포즈(파란색 옷)로 시작하고 끝나는 무한 루프 애니메이션을 생성할 수 있습니다. 우리는 하나 또는 몇 개의 예제 시퀀스에서 가능한 한 많은 다양한 동작을 &quot;마이닝&quot;하는 생성 모델인 GenMM을 제시합니다. 일반적으로 긴 오프라인 학습 시간이 필요하고 시각적 아티팩트가 발생하기 쉽고 크고 복잡한 골격에서 실패하는 경향이 있는 기존의 데이터 기반 방법과는 대조적으로, GenMM은 잘 알려진 동작 매칭 방법의 학습이 필요 없는 특성과 뛰어난 품질을 계승합니다. GenMM은 매우 복잡하고 큰 골격 구조가 있더라도 1초의 몇 분의 1 이내에 고품질 동작을 합성할 수 있습니다. 생성 프레임워크의 핵심은 &quot;공동 첫 번째 저자 *텐센트 AI 랩에서 인턴십 기간 동안 수행한 작업 *연락 저자 저자 주소: 웨이위 리, weiyuli.cn@gmail.com, 산둥 대학교, 중국; 쉬엘린 첸, Xuelin.chen.3d@gmail.com, 텐센트 AI 랩, 중국; 페이주오 리, peizhuo. li@inf.ethz.ch, 취리히 연방 공과대학교, 스위스; 올가 소르키네-호르눙, sorkine@inf.ethz.ch, 취리히 연방 공과대학교, 스위스; 바오취안 첸, baoquan@pku.edu.cn, 베이징 대학교, 중국. 이 작품의 일부 또는 전부를 개인 또는 교실에서 사용하기 위해 디지털 또는 하드 카피로 만드는 것은 비용 없이 허가되지만, 이 작품이 이익 또는 상업적 이익을 위해 만들어지거나 배포되지 않고, 이 고지 사항과 첫 페이지에 있는 전체 인용문이 있는 경우에 한합니다. 저자가 아닌 다른 사람이 소유한 이 작품의 구성 요소에 대한 저작권은 영광입니다. 출처를 명시한 초록은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org에서 허가를 요청하세요. © 2023 저작권은 소유자/저자가 보유합니다. 출판권은 ACM에 라이선스되었습니다. 0730-0301/2023/8-ART1 $15.https://doi.org/10.1145/생성 모션 매칭 모듈은 모션 매칭에 대한 생성 비용 함수로 양방향 시각적 유사성을 활용하고, 다단계 프레임워크에서 작동하여 모범 모션 매칭을 사용하여 무작위 추측을 점진적으로 개선합니다. 다양한 모션 생성 외에도 모션 완료, 키 프레임 가이드 생성, 무한 루핑, 모션 재조립을 포함하여 모션 매칭만으로는 불가능한 여러 시나리오로 확장하여 생성 프레임워크의 다양성을 보여줍니다. • CCS 개념: 컴퓨팅 방법론 → 모션 처리. 추가 키워드 및 구문: 모션 합성, 생성 모델, 모션 매칭 ACM 참조 형식: Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen. 2023. 생성 모션 매칭을 통한 예제 기반 모션 합성. ACM Trans. Graph. 42, 4, Article 1(2023년 8월), 12페이지. https://doi.org/10.1145/INTRODUCTION 자연스럽고 다양하며 세부적인 모션을 생성하는 것은 컴퓨터 애니메이션의 핵심 문제입니다. 모션 캡처(모캡) 시스템을 통해 방대한 양의 모션 데이터를 수집하거나 정교한 애니메이션을 수동으로 작성하는 것은 비용이 많이 들고 지루한 것으로 알려져 있습니다. 따라서 모션 데이터 세트는 일반적으로 제한적이며, 특히 ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. 1:• Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen은 기존 데이터 기반 동작 합성 방법의 효과를 방해하는 다양한 스타일, 골격 구조 또는 생물 유형을 지적했습니다. 따라서 제한된 예제 시퀀스에서 다양하고 광범위한 동작을 합성하는 생성 능력을 향상시키는 것이 중요한 연구 문제가 되었습니다. 최근 몇 년 동안 딥 러닝은 컴퓨터 애니메이션 분야를 강타했습니다. 딥 러닝 방법은 방대하고 포괄적인 데이터 세트에서 학습할 때 다양하고 자연스러운 동작을 합성하는 능력을 보여주었습니다[Holden et al. 2016, 2017; Henter et al. 2020; Tevet et al. 2022a; Raab et al. 2023a; Tseng et al. 2022]. 더욱 고무적인 점은 이러한 성공이 최근 극도로 축소된 환경에서 재현되었다는 것입니다[Li et al. 2022]에서 학습을 위해 하나의 시퀀스만 제공되지만 신경망은 샘플의 내부 분포를 학습하고 입력 예제 시퀀스의 다양한 변형을 합성하는 기능을 보여줍니다. 그럼에도 불구하고 신경 동작 합성 방법은 실제로 적용 가능성을 제한하는 몇 가지 단점이 있습니다. (i) 긴 학습 시간이 필요합니다. (ii) 지터링이나 과도한 평활화와 같은 시각적 아티팩트가 발생하기 쉽습니다. (iii) 크고 복잡한 골격 구조에 잘 확장되지 않습니다. 이 논문에서는 문제에 대한 대안적인 접근 방식을 탐구합니다. 컴퓨터 애니메이션의 고전적인 아이디어인 동작 최근접 이웃 [Lee et al. 2010]을 다시 살펴보겠습니다. 이는 딥 러닝 시대보다 오래 전부터 시작되었으며 캐릭터 애니메이션을 위한 최첨단 산업 솔루션인 동작 매칭이 설립된 기반입니다. [Büttner and Clavet 2015], 매우 고품질의 동작 합성을 제공합니다. 동작 매칭은 자연스럽게 보이고 다양한 로컬 컨텍스트에 대응하는 캐릭터 애니메이션을 생성합니다. 대규모 모캡 데이터베이스를 전체 자연스러운 동작 공간의 로컬 근사치로 사용하여, 동작 매칭은 주어진 로컬 맥락에 가장 잘 맞는 동작 패치를 간단히 검색합니다. 그러나 대규모 데이터 세트에 대한 의존성은 우리의 목표와 맞지 않습니다. 우리는 가능한 한 많은 다양한 동작을 하나 또는 몇 개의 예제에서 &quot;마이닝&quot;하는 생성 모델을 추구합니다. Granot et al. [2022]의 이미지 합성 작업에서 영감을 받아, 우리는 모션 매칭을 생성 모델에 주입하고 생성 모션 매칭(GenMM, &quot;젬&quot;으로 발음)을 생성하기 위해 다음과 같은 통찰력을 취합니다. 첫째, 모션 매칭의 모션 품질을 유지하고 생성 기능을 주입하기 위해 [Simakov et al. 2008]에서 도입한 양방향 유사성을 모션 매칭을 위한 새로운 생성 비용 함수로 활용합니다. 양방향 유사성은 예제와 합성된 시퀀스 간의 패치 분포를 비교하는 목적을 갖습니다. 구체적으로, 합성된 시퀀스에는 예제 시퀀스의 모션 패치만 포함되도록 하고, 그 반대로 예제에는 합성의 모션 패치만 포함되도록 합니다. 결과적으로 합성에 아티팩트가 도입되지 않으며, 중요한 모션 패치도 손실되지 않습니다. 둘째, 우리는 다단계 프레임워크를 사용하여 예제와 패치 분포 차이가 최소인 동작 시퀀스를 점진적으로 합성하여 다양한 시간 해상도에서 패치 분포를 캡처합니다. 마지막으로, 우리는 GAN 기반 방법의 생성적 다양성이 주로 무조건 노이즈 입력에서 비롯된다는 관찰을 활용합니다[Granot et al. 2022]: 우리는 노이즈를 가장 거친 합성 단계에 입력하고 매우 다양한 합성 결과를 얻습니다. 우리는 GenMM이 소수의 입력 예제에서만 다양한 동작을 생성하는 데 매우 유능하다는 것을 보여줍니다. 특히 기존 작업에 비해 GenMM은 여러 가지 장점을 제공합니다. ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. • GenMM은 사전 학습 없이 매우 빠르게 실행됩니다. 동작 시퀀스는 1초도 채 걸리지 않고 합성할 수 있습니다. • GenMM은 동작 매칭의 매력적인 특성을 계승하여 고품질과 충실도의 동작을 생성합니다. • GenMM은 신경망이 어려움을 겪는 매우 복잡한 골격(그림 1에서 433개 관절이 있는 캐릭터 참조)으로 원활하게 확장됩니다[Li et al. 2022]. • GenMM을 여러 시퀀스가 있는 입력으로 확장하고 모든 예를 포함하도록 합성을 장려하는 것은 쉬운데, 이는 GAN 기반 방법에서는 간단하지 않습니다[Li et al. 2022]. 다양한 동작 생성 외에도 동작 완료, 키 프레임 안내 생성, 무한 루핑, 동작 재조립과 같이 동작 매칭만으로는 달성할 수 없는 다양한 시나리오로 확장하여 생성 프레임워크의 다재다능함을 보여줍니다. 이는 모두 생성 동작 매칭의 공유 기반을 통해 가능합니다. 2 관련 작업 운동학 기반 동작 합성에 대한 가장 관련성 있는 작업을 검토합니다. 또한 영감을 얻은 패치 기반 합성을 비롯한 이미지 합성의 최근 발전 사항을 간략히 다룹니다. 동작 합성. 기존 동작을 다양화하여 새로운 동작을 생성하는 것은 Perlin과 Goldberg[1996]의 작업으로 거슬러 올라갈 수 있는데, 여기서 Perlin 노이즈[Perlin 1985]를 동작 클립에 추가하여 지역적 다양성이 있는 변형을 얻습니다.Pullen과 Bregler[2002]는 저주파 패치를 일치시키고 고주파 세부 정보를 혼합하여 모캡 데이터를 사용하여 거친 키 프레임 동작을 향상시킬 수 있음을 보여줍니다.Li et al.[2002]은 데이터 세트에서 유사한 패치를 일치시켜 그래프 모델을 구성하고 지역적 및 구조적 변형이 있는 임의 샘플을 생성하기 위한 확률적 모델을 만듭니다.선형 동역학 모델을 사용하기 때문에 이 모델은 만족스러운 결과를 얻으려면 대규모 학습 데이터 세트가 필요하고 품질과 다양성의 딜레마에 직면합니다.현재 동작 그래프[Kovar et al. 2002; Lee et al. 2002; Arikan 및 Forsyth 2002]는 결정론적 상태를 유지하면서 유사한 이산 그래프 모델, 즉 상태 머신을 사용하고 사용자 입력에 대화형으로 응답하는 문자를 보여줍니다.그러나 이산 공간은 본질적으로 민첩성과 반응성을 제한합니다.따라서 그 이후로 대규모 데이터 세트를 통계적 모델로 요약하려는 노력도 이루어졌습니다[Pullen 및 Bregler 2000; Brand 및 Hertzmann 2000; Bowden 2000; Grochow et al. 2004; Chai 및 Hodgins 2007; Wang et al. 2007].데이터 세트를 체계적이지만 이산적인 구조나 통계적 모델로 정렬하는 대신, 동작 최근접 이웃[Lee et al. 2010; Levine et al. 2012]은 연속적인 동작 필드에서 직접 작동하여 현재 포즈의 가장 가까운 이웃을 보간하는 제어 정책을 학습합니다. 그 후, [Büttner와 Clavet 2015]는 모션 매칭을 소개하는데, 이는 주어진 맥락에 가장 잘 맞는 애니메이션을 찾기 위해 대규모 애니메이션 데이터베이스를 검색하는 방법입니다. 이 방법은 단순성, 유연성, 제어 가능성 및 생성하는 모션의 품질로 인해 많은 스튜디오에서 빠르게 채택되었습니다[Harrower 2018; Buttner 2019]. 모션 매칭은 데이터베이스에 저장된 애니메이션 데이터를 그대로 재생하여 업계에서 사실상 최첨단 기술이 되었습니다. 그럼에도 불구하고 다양한 모션을 예제에서 합성하는 생성 모델을 목표로 하기 때문에 그 목표는 우리의 목표와 상당히 다릅니다. 입력 F 생성적 모션 매칭을 통한 예제 기반 모션 합성 • 1: 예제 모션 T₁ 단계 예제 모션 T₂ 단계 예제 모션 Ts 생성적 모션 매칭 합성된 F₁ 업샘플링 합성된 F... 생성적 모션 매칭 + 업샘플링 생성적 모션 매칭 합성된 Fs 단계 S 그림 2. 다단계 모션 합성. 가장 거친 단계부터 시작하여 각 단계 s의 생성적 모션 매칭은 이전 단계의 출력의 업샘플링된 버전을 초기 추측으로 취하고 예제 모션 Ts의 모션 패치로 정제하고 더 미세한 모션 시퀀스 Fs를 출력합니다. 가장 거친 단계는 입력이 단순히 가우시안 노이즈이기 때문에 순수하게 생성적이라는 점에 유의하세요. 딥 러닝의 최근 발전도 모션 합성 분야에 큰 영향을 미칩니다. 초기 시도[Holden et al. 2015, 2016]는 딥 신경망을 사용하여 애니메이션 데이터로부터 학습합니다. 딥 신경망은 대규모 데이터 세트[Rempe et al. 2021; He et al. 2022], 동작 예측[Fragkiadaki et al. 2015; Pavllo et al. 2018], 동작 중간화[Harvey et al. 2020; Duan et al. 2022; Qin et al. 2022], 동작 재조립[Jang et al. 2022; Lee et al. 2022], 텍스트 가이드 동작 합성[Tevet et al. 2022a,b; Zhao et al. 2023] 등을 포함한 많은 잘못된 생성 작업을 해결합니다. Holden et al. [2015, 2016]은 애니메이션 데이터에서 학습하기 위해 최신 딥 러닝 기술을 적용합니다. 한편, 동작 매칭을 딥 러닝과 결합하면 계산 비용이 덜 들고[Holden et al. 2020] 더 다재다능한[Habibie et al. 2022] 변형도 생겨났습니다. 특히, 이러한 모든 작업에는 학습을 위해 방대하고 포괄적인 데이터 세트가 필요합니다. 또 다른 주목할 만한 작업 라인은 작은 세트의 예제 동작으로 물리적으로 시뮬레이션된 캐릭터를 학습하기 위해 딥 강화 학습 기술을 적용하는 것입니다 [Peng et al. 2018, 2021]. 더 최근에는 우리와 가장 관련된 작업인 [Li et al. 2022]가 패치 GAN 기반 [Isola et al. 2017; Shaham et al. 2019] 접근 방식을 사용하여 단일 예제로 생성 모델을 학습하는 것을 제안합니다. 동시에 Raab et al. [2023b]은 다양한 출력을 생성하기 위해 단일 동작 클립의 내부 모티프를 학습하는 확산 기반 모델을 소개합니다. 그럼에도 불구하고 이 두 가지 방법은 날카로운 동작으로 결과를 생성하는 데 어려움을 겪고 불연속적인 밑줄이 있는 잠재 공간이 제공되기 때문에 여러 예제로 학습하기에 적합하지 않습니다. 우리는 섹션 4.1에서 심층적인 비교를 통해 GANimator보다 우리 방법이 우수함을 보여줍니다. 이미지 합성. 저희의 작업은 모션 합성과 비슷한 목표를 공유하는 텍스처 이미지 합성의 여러 알고리즘 설계를 채택합니다. 이 광범위한 본문에 대한 심층적인 조사는 독자들에게 조사 [Wei et al. 2009; Barnes and Zhang 2017]를 참조하세요. 딥 러닝에서 점진적 생성 [Karras et al. 2018]으로도 알려진 이미지 피라미드는 오래 전에 텍스처 합성에 사용되었습니다. Heeger and Bergen [1995]; De Bonet [1997]은 텍스처 합성에 라플라시안 피라미드 [Burt and Adelson 1987]를 사용하여 공간 주파수 영역에서 점진적 생성을 실현했습니다. Wei and Levoy [2000]는 비슷한 목적으로 가우시안 피라미드를 사용합니다. Han et al. [2008]은 세부 정보가 매우 풍부한 기가픽셀 크기의 이미지를 합성할 수 있는 새로운 차원으로 멀티 스케일 생성을 끌어올렸습니다. 또한 저희는 점진적 합성을 채택하여 생성 모션 매칭 모듈이 다양한 수준의 세부 정보를 캡처할 수 있도록 합니다. 영어: 오늘날의 딥 러닝 시대에 점진적 합성도 인기를 얻었으며, 무작위 노이즈를 점진적으로 정제하여 단일 자연 이미지와 유사한 이미지로 만드는 방법을 배우는 인상적인 생성 모델이 탄생했습니다.특히, 일련의 GANS[Goodfellow et al. 2014]가 다양한 스케일에서 예제의 패치 분포를 캡처하도록 훈련되었습니다.그 후, Granot et al.[2022]은 양방향 시각적 유사성[Simakov et al. 2008]이 예제와 합성된 이미지 간의 패치 분포 불일치를 측정하는 목적을 달성할 수 있음을 보여 GAN 기반 방법에 비해 훨씬 더 높은 품질과 빠른 합성의 다양한 이미지를 만들어냅니다.방법 우리는 주어진 예제와 유사한 고품질 동작을 대량으로 다양하게 합성할 수 있는 생성 프레임워크의 세부 사항을 설명합니다.우리의 방법은 여러 개의 예시 동작을 입력으로 사용할 수 있지만, 알고리즘에 대한 이해를 돕기 위해 우리의 적용 범위에서는 주로 단일 입력 설정에 대해 설명합니다. 더욱이 합성된 모션은 예제의 길이와 정확히 일치할 필요가 없으며 임의의 길이일 수 있습니다.3. 모션 표현 모션 시퀀스는 각각 루트 관절 변위 O € RTX3와 관절 회전 RE RTXJQ로 구성된 T 포즈의 시간적 집합으로 정의됩니다.여기서 J는 관절 수이고 Q는 회전 피처 수입니다.전역 루트 변위 O를 직접 사용하는 대신 O를 시간적으로 불변하고 두 연속 포즈의 차이로 계산되는 로컬 루트 변위 V로 변환합니다.관절 회전은 운동학 체인에서 부모의 좌표 프레임에서 정의되며 Zhou 등이 제안한 6D 회전 표현(즉, Q = 6)을 사용합니다.[2019].인간의 눈은 엔드 이펙터와 지면 사이의 비현실적인 상호 작용에 다소 민감하기 때문에 기존의 신경 기반 방법은 일반적으로 엔드 이펙터의 위치와 속도에 대한 기하학적 손실, 즉 발 접촉 손실을 확립합니다[Shi et al. 2020; Li et al. 2022; Tevet et al. 2022b], 반면 우리의 방법은 엔드 이펙터와 루트 모션 간의 높은 상관 관계와 같은 모범 모션 패치의 내부 구조가 합성에서 본질적으로 보존되기 때문에 그러한 설계를 요구하지 않습니다. 그렇기는 하지만 [Li et al. 2022]에서와 같이 발 접촉 레이블을 우리의 표현에 통합하여 드문 경우인 ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. 1:• Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen Qx11 + 3+ CFlatten P T¹ (with root joint) Flatten Qxjb + Cb 관절 회전 RP 국소 루트 변위 VT 접촉 레이블 L 초기 추측 Fs 예시 동작 Ts yb 패치 추출 ⠀ 패치 추출 정규화된 거리 행렬 관절 회전 R Бь 평균 투표 국소 루트 변위 V: 동작 패치 가장 가까운 패치 집계 접촉 레이블 L 골격 B 전신 동작 Ts 부분적 신체 동작 Ts 그림 3. 골격 인식 동작 패치 추출. 골격은 여러 개의 겹치는 골격 부분(왼쪽의 두 가지 색상)으로 분할되며, 이를 통해 전신 동작을 그에 따라 부분적 신체 동작 세트로 분할할 수 있습니다. 그런 다음 각 부분 신체 동작에서 p 프레임의 시간적 크기를 갖는 동작 패치(노란색 상자)를 추출할 수 있습니다.생성적 매칭 및 블렌딩 합성된 동작 Fs 그림 4. 생성적 매칭 및 블렌딩.초기 추측의 각 동작 패치는 정규화된 거리 행렬에 따라 예제 동작에서 가장 잘 일치하는 동작 패치를 찾습니다.그런 다음 겹치는 일치 패치를 블렌딩하여 새로운 부분 동작을 형성합니다.마지막으로 여러 결과 부분 동작을 블렌딩하여 최종 전체 신체 동작을 얻습니다(오른쪽 참조).큰 루트 동작 예제가 있는 합성에서 발이 미끄러질 수 있습니다.접촉 레이블을 사용하면 IK 사후 처리에서 발이 떠다니는 것을 방지할 수 있습니다.특히, 접촉 레이블 L은 속도 크기의 임계값을 설정하여 입력 동작에서 쉽게 검색할 수 있습니다.발 관절의 수가 C라고 가정하고 타임스탬프에서 각 발 관절에 대해 이진 벡터를 계산하여 프레임당 기능에 추가합니다(그림 3 및 4 참조). 편의상 Mµ = RH×(JQ+3+C)는 H 프레임의 연결된 동작 특징의 계량 공간, T = [R, V] € MT는 원래 입력 동작 특징, T¿ € MÃ;는 입력의 해당 다운샘플링 버전, F = MF는 F 프레임의 합성된 동작 특징, Fs는 F의 해당 다운샘플링 버전을 나타냅니다. 3.2 다단계 동작 합성 그림 2는 F 프레임의 동작을 점진적으로 합성하는 S 단계로 구성된 접근 방식의 전체 파이프라인을 보여줍니다. 구체적으로, 입력 모션이 주어지면 Ts = T가 원래 입력 시퀀스이고 Ts € MT가 r &gt; 1인 Ts+1 다운샘플링된 표본 피라미드 {T1, ..., Ts}를 구축합니다. 그런 다음 FS = MF가 F 프레임의 최종 합성 시퀀스이고 Fs € MF가 F의 중간 시퀀스인 합성 피라미드 {F1, ..., Fs}를 구축합니다. rs- 프레임은 가장 거친 단계에서 시작하여 가장 미세한 단계까지 거칠게-정밀하게 합성됩니다. 각 단계 s에서 생성 모션 매칭 모듈(섹션 3.3)은 이전 단계의 출력의 업샘플링된 버전인 Fs = Fs−1 ↑”을 초기 추측으로 받아들이고 Ts의 표본 모션 패치로 정제하여 더 미세한 모션 시퀀스 Fs를 출력합니다. 가장 거친 단계에서의 합성은 입력이 가우시안 분포에서 추출한 노이즈에 불과하므로 순수하게 생성적이라는 점에 유의하세요.즉, F₁ ~ N(µ, σ²) Є MF₁· 3. 생성적 모션 매칭 일반적으로 패치 기반 이미지 합성은 패치 추출, 최근접 이웃 매칭, 블렌딩의 세 단계로 구성되며, 이는 여러 반복에서 수렴하는 결과를 생성하기 위해 순차적으로 작동합니다.저희의 방법은 유사한 접근 방식을 따르지만 저희 작업에 특화된 알고리즘 설계를 따릅니다.특히, 각 단계 s에서 다음 단계는 E번의 반복 동안 순차적으로 호출됩니다.스켈레톤 인식 모션 패치 추출.모션 패치는 예제 시퀀스에서 p개의 연속 프레임의 하위 시퀀스로 간단히 정의할 수 있으며, 이는 모션 합성에서 일반적인 관행입니다[Li et al. 2022; [Büttner and Clavet 2015]. 우리의 접근 방식은 이 정의에 따라 간단히 작동할 수 있지만, 우리는 동작 시퀀스에서 골격 인식 동작 패치를 추출하여 골격을 전체로 취급하는 대신 여러 하위 집합, 즉 골격 부분으로 분해하고 결국 더 다양한 포즈로 이어지는 것을 제안합니다. 구체적으로, B가 전체 신체 동작 Ts에서 사용하는 골격 트리를 나타내며, 골격 부분 집합 {B1, ..., BB}을 나타내며, 여기서 Bb CB는 전체 골격의 하위 트리이고 Jb 관절이 있으며, 전체 신체 동작을 부분 신체 동작 집합 {T}, ..., T}으로 나누도록 정의할 수 있습니다. 여기서 보폭 크기가 1인 p 프레임의 하위 시퀀스를 동작 패치로 자릅니다(그림 3 참조). 일반적으로 서로 다른 애니메이션의 골격은 반드시 특정 규칙을 따르지 않으며, 예를 들어 두 발 동물 대 육각형 동물, 순수한 생물학적 골격 대 더 예술적인 관절이 있는 골격 등 매우 높은 가변성을 가질 수 있습니다.따라서 우리의 접근 방식은 사용자가 [Jang et al. 2022; Lee et al. 2022]에서와 같이 겹치는 관절이 있는 하위 부분으로 전체 골격 구조를 수동으로 나눌 수 있도록 합니다.생성적 매칭.X는 Fs에서 추출한 동작 패치 집합을 나타내고, Y는 예제 동작 Ts에서 추출한 동작 패치 집합을 나타냅니다.우리는 제곱-L2 거리를 사용하여 쌍별 패치 거리 행렬을 계산합니다.이는 각 예시 동작 패치와 각 합성 동작 패치 간의 유사성을 측정하기 위한 기초를 제공합니다. 패치 거리 행렬은 골격 부분별로 계산됩니다.D₁₁₂ = ||x − Y ||2, (1) 생성적 동작 매칭을 통한 예제 기반 동작 합성 • 1: 합성된 시퀀스433-관절 골격 합성된 시퀀스143-관절 골격 합성된 시퀀스합성된 시퀀스그림 5. 단일 예제에서 프레임워크는 1초 이내에 옷과 날개의 애니메이션을 포함하여 매우 복잡하고 큰 골격에 대해서도 다양한 동작 시퀀스를 생성합니다. 더 많은 애니메이션 결과는 첨부된 비디오를 참조하세요.여기서 Xb와 Yb는 해당 부분 신체 동작에서 추출된 동작 패치 세트를 나타냅니다.그런 다음 [Simakov et al. 2008; Granot et al. 2022]에서와 같이 양방향 유사성을 도입하여 모든 예시 동작 패치가 합성에 나타나고 합성의 모든 동작 패치가 예제에서 벗어나지 않도록 합니다.즉, 높은 완전성과 일관성을 갖습니다. 이는 예제별 패치 계수를 사용하여 거리 행렬을 정규화하여 달성됩니다.Do₁j (a + mine (D;)) (2) 여기서 a는 완전성의 정도를 제어하고 더 작은 a는 완전성을 장려합니다.a의 효과에 대한 심층적인 연구는 섹션 4.3에서 수행됩니다.블렌딩.Xb의 각 모션 패치에 대해 Yb에서 가장 가까운(방정식 2로 정의됨) 모션 패치를 찾은 다음 평균 투표를 사용하여 수집된 모션 패치의 값을 블렌딩하여 합성된 부분 신체 모션 Fb를 형성합니다.마지막으로 골격 부분 사이의 겹치는 관절에 대한 값의 평균을 내어 모든 합성된 부분 신체 모션을 최종 결과 Fs로 조립합니다(그림 4 참조).3. 더 많은 설정으로의 확장 이 방법은 다양한 설정으로 쉽게 확장할 수도 있습니다.모션 패치를 위한 골격 분할.위에 정의된 골격 인식 모션 패치 외에도 이 방법은 모션 패치의 기존 정의, 즉 골격을 전체로 처리한 다음 p개의 연속 포즈를 추출하는 방식으로 작동할 수도 있습니다.여러 예제. 앞서 언급했듯이, 저희의 방법은 단일 모션 입력에만 적용할 수 있는 것이 아니라, 프레임 수가 다른 여러 시퀀스에도 적용할 수 있습니다. 이는 모든 입력 모션에서 모션 패치를 추출하여 방정식 1에서 사용된 예시 패치 세트를 형성함으로써 간단히 달성할 수 있습니다. 이 설정에서 완전성 제어 노브 a는 모든 모션 패치가 출력에서 활용되도록 하는 데 중요한 역할을 합니다. 이기종 스켈레톤. 흥미롭게도, 여러 예제 설정에서 다른 모션의 스켈레톤은 반드시 같은 것을 공유하지 않습니다. 예를 들어, 괴물과 좀비의 모션 클립을 가져와 이 두 생물에서 추출한 다른 부분 신체 모션을 조화시켜 움직이는 프랑켄슈타인을 합성할 수 있습니다. 이를 달성하기 위해 사용자는 각 입력의 스켈레톤을 겹치게 분할하고 생성 모션 매칭 및 블렌딩에 사용할 관심 있는 스켈레탈 부분을 수동으로 지정할 수 있습니다. 겹치는 영역은 생성 프로세스 중에 다른 스켈레탈 부분을 연결하는 데 중요한 역할을 합니다. 그러면, 우리의 방법은 3.3절에서 논의한 대로 부분적 신체 동작을 결합하여 새로운 생물의 새로운 동작을 합성할 수 있습니다.
--- EXPERIMENT ---
S 우리는 예제 기반 동작 합성에서 우리 방법의 효과를 평가하고 다른 동작 생성 기술과 비교합니다.ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. 1:⋅ Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen 九州大太 예제 시퀀스 MotionTexture [2002] acRNN [2018] 보스 GANimator [2022] 우리 그림 6. 시각적 비교. MotionTexture [2002]는 부자연스러운 전환으로 동작을 생성합니다. acRNN [2018]은 노이즈가 많은 동작을 생성하거나 때로는 정적 포즈로 빠르게 수렴합니다. GANimator [2022]는 복잡한 골격을 처리하는 데 어려움을 겪어 지나치게 매끄러운 결과를 생성합니다. 우리의 방법은 다양하고 고품질의 결과로 이러한 방법보다 성능이 뛰어나며, 매우 동적인 동작이 잘 보존됩니다. 애니메이션 결과는 첨부된 비디오를 참조하세요. 다양한 설정과 응용 프로그램에 적용하여 다재다능함을 입증하세요. 독자 여러분께서 보다 정성적인 평가를 위해 첨부된 비디오를 참조하시기를 적극 권장합니다. 구현에 대한 이해를 돕고 향후 연구를 용이하게 하기 위해 코드와 데이터가 공개될 예정입니다. 데이터. Mixamo [2022]와 Truebones [2022]에서 다양한 동작 스타일과 매우 복잡하고 큰 골격 구조를 특징으로 하는 다양한 예제 애니메이션 세트를 수집했습니다. 실험한 동작 스타일에는 터지는 춤의 날카로운 동작, 날개를 펼치는 미묘한 동작 등이 있습니다. 일부 예는 그림 5에 시각화된 433-관절 및 143-관절 골격과 같이 매우 정교한 골격 구조로 작성되었습니다. 프레임 수는 30fps에서 140~1000프레임입니다. 구현 세부 정보. 프레임워크는 가볍고 어떠한 훈련도 필요하지 않습니다. 단순성과 효율성으로 인해 Python으로 간단히 메서드를 구현합니다. 또한 오픈소스 소프트웨어 Blender[Blender Online Community 2023]에서 사용자로부터 애니메이션을 가져와 다양하고 고품질의 변형을 합성할 준비가 된 애드온을 개발합니다.우리의 구현에서 약 1000개의 프레임으로 구성된 모션 샘플은 Apple M1 CPU를 사용하면 0.2초, 최신 GPU(NVIDIA V100)를 사용하면 ~0.05초에 생성할 수 있습니다.기본적으로 Apple M1 CPU를 사용하여 실험을 실행하지만 ~ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월.공평한 신경망 기반 방법과의 비교 실험을 위해 NVIDIA V100 GPU를 사용하여 비교 실험을 수행합니다.가장 거친 단계에서 T₁의 길이를 패치 크기 p의 K배로 설정합니다.따라서 수용 필드(이미지와 유사한 개념)는 항상 길이가 다른 예제 모션의 동일한 비율을 차지합니다. 그런 다음 T₁는 Ts의 최종 길이에 도달할 때까지 r 요소를 사용하여 점진적으로 업샘플링됩니다. 달리 지정하지 않는 한 패치 크기 p = 11, K = 4, 완전성 제어 노브 α = 0.01 및 반복 횟수 E = 5를 사용합니다. 이것들은 우리가 찾은 경험적으로 가장 좋은 하이퍼 매개변수입니다. 하이퍼 매개변수에 대한 자세한 내용은 보충 자료를 참조하십시오. 4. 새로운 동작 합성 먼저 새로운 동작 합성에서 프레임워크의 성능을 평가하고 이를 고전적인 통계 모델과 최근의 신경 기반 모델인 Motion Texture [Li et al. 2002], acRNN [Zhou et al. 2018] 및 GANimator [Li et al. 2022]와 비교합니다. 설정. 우리 방법은 여러 입력을 사용할 수 있지만 공정한 비교를 위해 단일 예제에서 다양한 동작 합성에 대한 평가를 수행하고 골격 인식 동작 패치 추출을 비활성화합니다. 우리는 평가를 위해 매우 역동적이고 민첩한 움직임을 포함하는 세 가지 예제 시퀀스를 사용합니다. 캐릭터는 생성적 모션 매칭을 통한 예제 기반 모션 합성으로 구성됩니다. • 1:↑ 골격 부분 * XXXXXXX 예제 시퀀스 X &amp; YLFAYLTTXIX 우리의 것(골격 인식 모션 패치 추출 없음) XX ↑ AADXXXX Å KA 우리의 것(골격 인식 모션 패치 추출 있음) 그림 7. 골격 인식 모션 패치 추출의 효과. 아티스트는 수동으로 골격을 세 개의 겹치는 부분으로 나눕니다(왼쪽 상단). 두 손을 동시에 흔드는 캐릭터의 예를 들어, 골격 인식 구성 요소 없이는 두 손이 흔드는 시퀀스만 합성할 수 있습니다. 그러나 골격 인식 모션 패치 추출을 사용하면 한 손으로만 흔드는 것(빨간색 상자)을 포함한 더 다양한 시퀀스를 생성할 수 있습니다. 65개 관절로 구성되며 각 시퀀스에는 약 500개의 프레임이 있습니다. 각 예제 시퀀스에 대해 모든 방법을 사용하여 예제의 길이를 두 배로 늘리는 새로운 시퀀스를 합성합니다. 정성적 비교. (i) 다양한 동작 합성을 달성하기 위해 Motion Texture는 훈련 동작에서 유사한 동작 패치를 텍스톤이라고 알려진 선형 동역학 모델로 구성하고 마르코프 체인을 사용하여 텍스톤 간 전환 확률을 모델링합니다. 그러나 특히 선형 동역학 모델을 선택했기 때문에 예제 시퀀스가 하나뿐인 경우 다양성과 품질의 균형을 맞추는 과제에 직면합니다. 우리는 [Li et al. 2022]에서 수행한 절차를 따라 단일 예제에 MotinTexture를 적용합니다. 결과적으로 MotionTexture는 텍스톤 간에 부자연스러운 전환을 생성합니다. (ii) acRNN은 RNN 기반 네트워크 구조를 사용합니다. 데이터가 부족하여 과적합되기 쉬운 모델이 생성되고 섭동이나 오류 누적에 강하지 않습니다. 결과적으로 acRNN은 제한된 수의 프레임만 안정적으로 생성할 수 있습니다. (iii) GANimator는 일련의 GAN을 사용하여 다양한 스케일에서 동작 패치의 분포를 캡처하여 입력과 매우 유사한 동작을 점진적으로 합성합니다. 영어: 표 3에서 보듯이 복잡하고 큰 골격을 사용한 실험에서 GANimator는 고품질 결과를 생성하는 데 어려움을 겪었으며, 종종 떨리거나 지나치게 부드러운 동작이 생성되었습니다.또한 일반적으로 몇 시간에서 하루까지 상당한 양의 훈련 시간이 필요합니다.반면에 저희 방법은 이러한 복잡한 골격 구조와 다양한 동작 스타일에 적응하고 그림 6에서 보듯이 다양하고 고품질의 변형을 합성할 수 있습니다.특히, 날카롭고 민첩한 동작과 같은 매우 동적인 동작은 합성 결과에서 잘 보존됩니다.더욱 정성적인 결과는 첨부된 비디오를 참조하십시오.표 1. 단일 예제 기반 생성에 대한 정량적 비교.Motion Texture [2002] Motion Texture (Single) acRNN [2018] GANimator Ours Coverage Set Global Local Div.Patch Dist.Patch Dist. 영어: 학습 추론 시간 시간 84.12 0.100.00 0.5.13 0.49.07 0.99.89 0.1.0.1.32.3초 0.03초 0.0.08초 0.07초 13.13.25시간 0.21초 2.2.6시간 0.12초 0.0.N/A 0.08초 정량적 비교. 생성된 결과의 품질을 몇 가지 예제와 비교하여 측정하는 것은 어려운 것으로 알려져 있습니다[Li et al. 2022]. 선구자 중 하나인 GANimator는 단일 메트릭으로는 전체 품질을 측정할 필요성이 충분하지 않기 때문에 적용 범위, 다양성 및 재구성 손실과 같은 확립된 메트릭을 결합하여 성능을 평가합니다. 그러나 재구성 손실은 신경망 기반 방법에 대한 품질 지표로만 적합합니다. 다양성은 생성된 모션 패치와 예제에서 가장 가까운 이웃 사이의 평균 거리로 측정되며, 다른 모션 패치 크기는 로컬 및 글로벌 다양성에 해당합니다. 결과적으로 생성된 모션이 부자연스러워지면 다양성이 증가하는 경향이 있으며 신경망에서 생성된 사소한 섭동이나 과도하게 매끈한 결과가 있는 결과를 선호합니다. 따라서 실험에서 다양성 지표에 대해 로컬 패치 거리 및 글로벌 패치 거리라는 중립적인 이름을 사용합니다. 이러한 지표에 대한 자세한 내용은 [Li et al. 2022]를 참조하세요. 또한 2D 이미지 합성에서 잘 확립된 지표[Shaham et al. 2019]에 따라 생성된 결과 간의 다양성을 측정하고 각 관절 과합성 모션의 회전의 평균 표준 편차로 계산하고 ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. 1:⋅ Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen 예제의 적용범위 예제의 적용범위 예제의 적용범위 GANimator [2022] 합성된 동작 예제의 적용범위 예제의 적용범위 예제의 적용범위 우리의 그림 8. 다중 예제 설정에서의 비교. GANimator는 모든 예제를 통합하는 데 어려움을 겪어 합성에서 예제 동작 패치의 상당 부분이 손실되고(회색으로 표시) 적용 범위 점수가 낮아집니다. 반면에 우리의 방법은 모든 예제(색상 표시)를 효과적으로 적용하여 높은 적용 범위 점수를 얻습니다. 입력 예제의 모든 조인트 회전. 이 메트릭도 노이즈가 있는 출력을 선호하지만, 우리는 주로 적용 범위와 집합 다양성을 결합하여 방법을 평가합니다. 양적 비교 결과는 표 1에 나와 있습니다. 주목할 점은, 우리의 방법은 충분히 다양한 결과를 보이는 동시에(높은 집합 다양성 점수로 입증됨) 상당히 높은 적용 범위 점수를 생성합니다. 품질을 보다 포괄적으로 비교하려면, 독자들에게 첨부된 비디오를 참조하세요. 또한, 표 1에 계산 시간을 보고했는데, 여기서 우리의 방법은 훈련이 필요 없고 추론 중에 매우 빠르기 때문에 매우 효율적이라는 것을 알 수 있습니다. 4.2 추가 생성 설정 위에서 사용한 기본 설정 외에도 다음과 같은 측면에서 우리의 방법을 추가로 평가합니다. 골격 인식 모션 패치. 우리의 방법은 시간 축 외에도 골격 축을 따라 예제에서 모션 패치를 추출할 수 있으므로 그림 7과 첨부된 비디오에서 볼 수 있듯이 공간 차원에서도 다양성을 얻을 수 있습니다. 여러 예제. 영어: 기존 방법은 완전성에 대한 명시적 장려가 부족하여 여러 예제 시퀀스가 제공되면 어려움을 겪는 반면, 본 방법은 방정식 2의 완전성 제어 노브로 여러 예제를 처리할 수 있습니다. 30fps에서 120-220프레임 범위의 5개 댄싱 시퀀스를 수집합니다. 표 2에서 더 많은 예제가 제공될 때 기존 방법은 낮은 적용 범위로 결과를 생성하는 반면 본 방법은 높은 적용 범위를 유지합니다. MotionTexture는 단일 예제 설정에서의 결과와 유사하게 부자연스러운 전환을 생성합니다. acRNN은 작업에 실패하고 다양하지만 부족한 ACM Trans. Graph., Vol. 42, No. 4, Article 1로 인해 노이즈가 많은 동작을 생성합니다. 출판일: 2023년 8월. 표 2. 다양한 개수의 예제 시퀀스에 대한 적용 범위 비율. 예제 시퀀스 수 모션 텍스처 [2002] acRNN [2018] GANimator [2022] 우리의 100 29.19 10.03 27.7.34 4.02 1.38 0.63.55 23.90 17.30 16.99.71 99.95 99.91 99.표 3. 다양한 복잡도의 골격에 대한 적용 범위.관절 수 GANimator [2022] 우리의 92.97.44.99.2.86.모션 데이터.GANimator는 각 시퀀스에 대해 미리 정의된 해당 잠재 변수가 필요합니다.그러나 주어진 시퀀스의 구조는 이러한 잠재 변수를 정의하는 데 고려되지 않아 네트워크가 다양하고 완전한 샘플을 생성하는 데 방해가 됩니다. 이와 대조적으로, 우리의 방법은 그림 8과 함께 제공되는 비디오에서 볼 수 있듯이 완전성 제어 노브를 적절히 설정하면 예제의 대부분을 포괄하는 동작을 생성합니다.복잡한 골격.우리의 방법은 복잡도가 높은 골격(그림 5 참조)에서 작동할 수 있으며, GAN 기반 방법인 GANimator는 함께 제공되는 비디오에서 볼 수 있듯이 합리적인 결과를 생성하지 못합니다.특히, 우리는 24, 65, 433개의 관절로 구성된 골격으로 실험합니다.표 3에서 볼 수 있듯이 GANimator는 24개 관절 골격에서는 정상적으로 수행되지만 복잡한 경우 성능이 급격히 떨어집니다.표 4. 하이퍼파라미터의 다양한 설정.생성적 동작 매칭을 통한 예제 기반 동작 합성 -시간(초) ■메모리(GB) .1:커버리지 세트 다양성 글로벌 패치 분포 로컬 패치 분포 우리의 (a를 사용하지 않음) 87.0.0.0.Ours (α = 5) 87.0.0.0.Ours (α = 0.5) 88.0.0.0.Ours (a 0.05) 93.0.0.0.Ours (a = 0.005) 99.0.0.0.Ours (α=0.0) 36.0.2.1.Ours (K = 20) 87.0.1.0.Ours (K = 15) 92.0.0.0.Ours (K = 10) 96.0.0.0.Ours (p = 23) 99.0.0.0.Ours (p = 17) 99.0.0.0.합성된 프레임 수 우리의 (p = 5) 99.0.0.0.Ours (r = 2) 99.0.0.0.Ours (r = 4) 99.0.0.0.Ours (r = 8) 97.0.0.0.그림 9. 생성된 프레임 수 증가에 따른 시간 및 메모리 소비. 스켈레톤. 반면, 저희 방법은 다른 스켈레톤에 대해 일관된 성능을 유지하는데, 이는 첨부된 비디오에서 스커트와 용 날개의 펄럭이는 효과에서 알 수 있습니다. 4. 하이퍼 매개변수의 효과 저희 프레임워크는 합성 프로세스 중에 여러 하이퍼 매개변수를 포함합니다. 이 섹션에서는 이러한 하이퍼 매개변수의 효과에 대해 설명합니다. 정량적 결과는 표 4에 나와 있습니다. a의 효과. 드물게 나타나는 패치는 최소 거리가 더 크므로 최소 거리가 있는 예제에서 추출한 패치의 초기 추측까지의 거리를 정규화하여 합성의 완전성을 촉진합니다. 따라서 방정식의 a는 합성된 결과에서 예시 패치의 완전성에 대한 제어 노브 역할을 합니다. 정규화 분모의 하한을 제한하기 때문에 더 작은 a 값은 합성에서 예시 콘텐츠를 더 많이 보존하도록 장려합니다. 표 4에서 볼 수 있듯이 a가 특정 수준으로 감소하면 더 높은 적용 범위 점수가 달성됩니다. 그러나 a 값이 너무 작으면 완전성에 대한 과도한 강조(특히 생성된 동작과의 거리가 거의 0인 패치의 경우)가 매칭 프로세스에 사용된 유사성 측정을 압도하여 불안정한 생성 및 낮은 품질의 동작(손상된 결과의 낮은 적용 범위 및 높은 패치 거리로 입증됨)이 발생할 수 있습니다.&quot; K의 효과. 가장 거친 단계에서 입력 예제 동작의 길이에 대한 패치 크기의 비율은 이미지 도메인의 개념과 유사하게 합성을 위한 수용 필드를 제어합니다. 더 큰 K는 더 작은 수용 필드를 발생시켜 더 다양한 결과를 가져옵니다. 특히, 큰 K는 미세한 수준의 동작만 캡처할 수 있고 일부 부자연스러운 전환으로 이어지는 반면, 작은 K는 원래 시퀀스의 과적합으로 이어집니다. 표 4는 K가 증가함에 따라 전역 및 로컬 패치 거리가 증가하는 것을 보여줍니다. 이는 수용 필드가 작을 때 생성된 결과가 입력 시퀀스에서 더 벗어나기 때문입니다. 패치 크기 p의 효과. 패치 크기 p는 생성 매칭 및 블렌딩에 사용되는 패치의 시간적 길이를 정의합니다. 패치 크기는 K와 공동으로 수용 필드를 제어하며, 더 작은 패치 크기는 더 작은 수용 필드를 초래하여 표 4에서 글로벌 및 로컬 패치 거리의 증가에서 볼 수 있듯이 덜 일관된 결과를 생성합니다.r의 효과.인자 r은 단계 간 전환의 단계 크기를 제어합니다.더 큰 단계 크기는 더 큰 것에 의해 제어되며 연속적인 스케일 사이에 큰 갭으로 인해 불안정한 생성을 초래할 수 있습니다.반면에, 작은 단계 크기는 불필요한 실행 시간을 발생시킵니다.4.시간 및 메모리 소비 섹션 3.3에서 설명한 거리 메트릭의 메모리 풋프린트는 생성된 프레임 수 F가 증가함에 따라 증가합니다.시간 및 메모리 소비를 더 자세히 조사하기 위해 EN이 65개 관절 캐릭터의 모션 프레임으로 구성되고 F를 1,000~100,000 범위로 설정하는 극한 조건에서 방법에 대한 스트레스 테스트를 실시합니다.이러한 테스트는 32GB 메모리가 장착된 NVIDIA VGPU를 사용하여 수행됩니다.그림 9는 시간과 메모리 소비가 생성된 프레임 수에 따라 선형적인 증가 패턴을 보인다는 것을 보여줍니다. GPU에서 거리 메트릭의 높은 병렬 계산 덕분에, 우리 방법은 100,000개의 프레임으로 구성된 고품질 샘플을 합성하는 데 불과 3초가 걸립니다.응용 프로그램 이 섹션에서는 동작 완료, 키 프레임 안내 생성, 무한 루핑, 동작 재조립과 같은 다양한 응용 프로그램에 적용하여 프레임워크의 다재다능함을 보여줍니다.결과는 그림 10에 나와 있습니다.더 자세한 데모는 첨부된 비디오에서 볼 수 있습니다.동작 완료.스켈레톤 인식 동작 패치 추출을 활용하는 우리 프레임워크는 특정 신체 부위의 움직임만 포함하는 부분 동작을 완료할 수 있게 합니다.예를 들어, 하체 동작 시퀀스 Tlower가 제공되면 하체 و... و 상체 동작은 예제 동작을 사용하여 완료할 수 있습니다.특히, 부분 신체 동작 Tlower에 대한 피라미드를 만들고 출력 F¸의 해당 부분 동작은 각 단계 s에서 Tlower로 고정됩니다. 그런 다음 프레임워크는 나머지 부분의 움직임을 자동으로 합성하여 일관되고 자연스러운 동작으로 부분적 제약을 완성합니다.ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. 1:• Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen 동작 완료 무한 반복: 고정 키 프레임 #키 프레임 안내 생성 키 프레임 #&gt;f&gt; 예제동작 재조립 예제XX 그림 10. 응용 프로그램. (1) 동작 완료. 사용자는 하체 동작(노란색으로 표시)을 제공할 수 있으며, 저희 방법은 다양한 동작으로 완료됩니다. (2) 키 프레임 안내 생성. 키 프레임 세트(빨간색 상자로 표시)가 주어지면 키 프레임 포즈를 따르는 다양하고 새로운 동작 시퀀스를 생성할 수 있습니다. (3) 무한 반복. 시작 및 종료 포즈를 동일하게 지정하기만 하면 저희 방법은 군중 시뮬레이션에 유용할 수 있는 무한 반복 애니메이션을 생성할 수 있습니다. (4) 모션 재조립. 이질적인 골격을 가진 두 개의 모션 시퀀스가 주어지면, 우리 방법은 그것들을 결합하여 일관되고 자연스러운 모션을 가진 새로운 생물을 형성할 수 있습니다. 키 프레임 가이드 생성. 우리 방법은 또한 사용자가 합성된 모션의 내용을 가이드하기 위해 희소한 키 프레임 세트를 수동으로 지정할 수 있도록 합니다. 그런 다음 우리 방법은 시퀀스 전체에 분산된 이러한 희소한 포즈 제약 조건을 효과적으로 처리하고 부드럽고 매우 자세한 모션을 생성할 수 있습니다. 가장 거친 단계에 키 프레임 세트가 주어지면 F₁의 해당 프레임을 지정된 프레임으로 바꾸고 전체 생성 프로세스에서 고정하여 간단히 실현합니다. 이러한 수동으로 지정된 키 프레임은 예제의 포즈 분포에서 크게 벗어나지 않아야 합니다. 실제로는 사용자가 약간의 수동 수정을 통해 예제에서 기존 포즈를 선택하기만 하면 얻을 수 있습니다. 무한 루핑. 우리 프레임워크는 합성의 모든 단계에서 끝 포즈를 시작 포즈와 동일하게 고정하여 무한 루핑 모션을 쉽게 합성할 수 있습니다. 이를 통해 전체 모션 시퀀스의 원활한 루핑이 가능합니다. 애니메이션 제작에 유용할 수 있습니다. 경기장 밖에서 응원하는 관중과 같은 반복적인 군중을 만드는 것과 같은 것입니다. 모션 재조립. 섹션 3.4에서 앞서 언급했듯이, 우리의 방법은 프랑켄슈타인을 합성할 수 있는 능력이 있습니다. 몬스터의 오른팔을 좀비에 꿰매는 예를 보여줍니다. 그림 10과 함께 제공되는 비디오를 참조하세요. 이 두 캐릭터의 예제 시퀀스가 다르고 좀비에는 ACM Trans. Graph., Vol. 42, No. 4, Article 1이 없습니다. 출판일: 2023년 8월. 그림 11. 무작위 이동 생성. 원형 경로를 걷는 캐릭터의 예제 이동 시퀀스(왼쪽)가 주어지면, 우리의 방법으로 생성한 고품질의 새로운 모션 시퀀스를 보여줍니다. 여기서 캐릭터는 부분적으로 사라진 오른팔로 다른 움직임을 따라 걷습니다(오른쪽). 그러나 우리의 방법은 여전히 자연스럽고 의미 있는 동작을 성공적으로 합성할 수 있습니다. 무작위 이동 생성. 우리의 방법은 운동 클립이 주어졌을 때 고품질의 새로운 동작 시퀀스를 생성할 수도 있다. 그림 11에서 볼 수 있듯이, 예제 시퀀스에는 원형 경로를 걷는 사람이 포함되어 있는 반면, 우리의 방법은 생성적 동작 매칭을 통한 새로운 예제 기반 동작 합성을 생성할 수 있다. 1:1351 수집 te 업로드± 일시 정지 || 재시작 || 합성 레이어 모델링 조각 텍스처 페인트 음영 애니메이션 렌더링 캠핑 지오메트리? Nity 10: 전송 leop 완성도 che 최적화 mede 알파 재생 Kaying 뷰 마커 н 55 시작 1 종료D OD DOX 표면 표면 매트 패스 칼로리 wireet_02_ 그림 12. 위: 웹 기반 인터페이스의 스크린샷, 회색 문자는 합성된 동작을 나타낸다. 아래: Blender 애드온의 스크린샷, 합성된 동작은 가운데에 강조 표시되어 있다. 다른 궤적을 가진 출력(상단 행에서 해당 궤적의 차이 참조). 더 많은 애니메이션 결과는 첨부된 비디오에서 확인할 수 있습니다.사용자 인터페이스 프레임워크는 일반적이고 가벼우며 많은 제작 도구에 쉽게 통합할 수 있습니다.초보자를 위해 사용자 친화적인 웹사이트를 구축하여 사용자가 모션 파일을 업로드한 다음 한 번의 클릭으로 다양한 새로운 모션을 합성할 수 있습니다(그림 12의 상단 참조).또한 프로 아티스트를 위한 Blender 애드온을 개발하여 그림 12 하단에 표시된 대로 기존 워크플로에 완벽하게 통합합니다.두 인터페이스 모두 소비자 수준의 노트북에서 효율적으로 실행될 수 있습니다.결과는 첨부된 비디오를 참조하십시오.토론 및
--- CONCLUSION ---
우리는 소수의 사례만으로 다양한 동작 시퀀스를 합성하기 위한 생성 프레임워크를 제시했습니다. 우리는 캐릭터 애니메이션을 위한 업계 최첨단 기술인 모션 매칭에 생성 기능을 주입하여 이를 달성했습니다. 결과적으로, 우리의 프레임워크는 훈련이 필요 없는 특성과 뛰어난 품질을 계승했으며, 매우 복잡하고 큰 뼈대에서도 단 몇 초 만에 고품질 샘플을 생성할 수 있습니다. 우리는 다양한 애플리케이션에서 우리 프레임워크의 유용성을 보여줍니다. 장점에도 불구하고 현재 형태의 우리 방법에는 몇 가지 단점이 있습니다. 그것은 불연속 패치 분포를 사용하는 반면, GANimator[Li et al. 2022]는 연속 분포를 학습합니다. 따라서 GANimator는 학습된 분포에서 높은 우도로 새로운 포즈를 생성할 수 있습니다. 뼈대 인식 구성 요소가 해결책이 될 수 있지만, 우리 방법에는 이 기능이 없습니다. 그럼에도 불구하고, 우리는 이러한 일반화가 동작 합성에 불리할 수 있다고 주장한다.새로운 포즈로 형성된 시퀀스에는 종종 인간의 눈에 매우 눈에 띄는 떨림과 불일치와 같은 시각적 아티팩트가 포함되기 때문이다.우리는 처음부터 동작 품질을 우선시했고, 이것이 동작 매칭 접근 방식으로 이어졌다.우리의 방법은 동작의 참신성과 품질 간의 균형을 맞추려고 애쓰는 것보다는 예제에서 채굴할 수 있는 한 많은 변형을 합성하는 것을 목표로 한다.결과적으로, 우리 방법의 다양한 결과가 표시되기는 하지만, 우리 방법의 생성적 다양성은 GANimator보다 낮다.따라서 향후 작업 방향은 동작 매칭의 높은 품질을 생성 신경 모델에 주입하는 것이며, 아마도 이산 신경 표현 학습 기술[Van Den Oord et al. 2017]을 사용하여 양쪽의 장점을 얻는 것이다.우리의 방법은 충분한 내재적 주기성을 가진 예제 동작을 선호하는데, 이는 일반적인 인간 동작의 중요한 속성으로 점점 더 인식되고 있다[Holden et al. 2017; Starke et al. 2022], 매우 다양한 새로운 변형을 생성합니다. 가능한 한 많은 일관된 변형을 채굴하기 위해 단일 예제에서 이러한 패턴을 활용하려고 합니다. 예제에 포즈 변경이 하나만 포함된 극단적인 경우 이러한 입력에만 기반하여 시간적 변형을 만드는 것은 의미가 없을 수 있습니다. 그럼에도 불구하고, 보충 비디오에서 비동기적으로 손을 흔드는 것으로 입증된 것처럼, 우리의 골격 인식 구성 요소는 골격 축을 따라 변형을 도입할 수 있습니다. 키 프레임 가이드 애플리케이션에서 필요한 수동 제약 조건과 관련하여, 수동으로 지정된 키 프레임은 앞서 언급한 예제 포즈와 크게 다를 수 없습니다. 그렇지 않으면 생성된 시퀀스가 위에서 설명한 대로 완전히 새로운 포즈를 생성할 수 있는 능력이 부족하여 해당 제약 포즈를 충실히 따르지 않을 수 있습니다. 마지막으로, 정규화된 유사성 행렬이 지나치게 커지기 때문에 우리의 방법은 지나치게 긴 예제 시퀀스를 처리할 수 없습니다. [Barnes et al. 2009]와 같은 근사 최근접 이웃 검색을 채택하면 이 문제를 완화하는 데 도움이 될 수 있습니다. 감사의 말 건설적인 의견을 주신 익명의 심사자분들께 감사드립니다. 이 연구는 중국 국가중점연구개발프로그램 2022ZD0160801과 유럽연합의 Horizon 2020 연구 및 혁신 프로그램(ERC Consolidator Grant, 계약 번호 101003104, MYCLOTH)에 따라 일부 지원되었습니다. 또한 아바타 Ailing(그림 1)의 동작 데이터를 제공해 주신 Tencent AI Lab의 Han Liu에게도 감사드립니다. 참고문헌 Adobe Systems Inc. 2022. Mixamo. https://www.mixamo.com 접근: 2022-03-25. Okan Arikan and David A Forsyth. 2002. Interactive motion generation from examples. ACM Transactions on Graphics (TOG) 21, 3 (2002), 483-490. Connelly Barnes, Eli Shechtman, Adam Finkelstein, Dan B Goldman. 2009. PatchMatch: 구조적 이미지 편집을 위한 무작위 대응 알고리즘. ACM Transactions on Graphics(TOG) 28, 3(2009), 24. Connelly Barnes와 Fang-Lue Zhang. 2017. 패치 기반 합성의 최신 기술에 대한 조사. Computational Visual Media 3, 1(2017), 3-20. Blender Online Community. 2023. Blender - 3D 모델링 및 렌더링 패키지. Blender Foundation, Blender Institute, Amsterdam. Richard Bowden. 2000. 인간 동작의 통계적 모델 학습. IEEE Workshop on Human Modeling, Analysis and Synthesis, CVPR, Vol. 2000. Citeseer. Matthew Brand와 Aaron Hertzmann. 2000. Style machines. 컴퓨터 그래픽 및 상호 작용 기술에 대한 제27회 연례 컨퍼런스의 회의록에서. 183-192쪽. Peter J Burt와 Edward H Adelson. 1987. 컴팩트 이미지 코드로서의 라플라시안 피라미드. 컴퓨터 비전에 대한 독서에서. Elsevier, 671-679쪽. ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월. 1:• Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen Michael Buttner. 2019. 게임에서 동작 합성 및 캐릭터 제어를 위한 머신 러닝. Proc. of I3D 2019(2019). Michael Büttner와 Simon Clavet. 2015. 동작 매칭 - 차세대 애니메이션으로 가는 길. 한국어: https://www.youtube.com/watch?v=z_wpgHFSWss&amp;t=658s Jinxiang Chai와 Jessica K Hodgins. 2007. 통계적 동적 모델을 사용한 제약 기반 동작 최적화. ACM SIGGRAPH 2007 논문. 8-es. Jeremy S De Bonet. 1997. 텍스처 이미지의 분석 및 합성을 위한 다중 해상도 샘플링 절차. 제24회 컴퓨터 그래픽 및 대화형 기술 연례 컨퍼런스 회의록. 361-368. Yinglin Duan, Yue Lin, Zhengxia Zou, Yi Yuan, Zhehui Qian, Bohan Zhang. 2022. 실시간 동작 완료를 위한 통합 프레임워크. (2022). Katerina Fragkiadaki, Sergey Levine, Panna Felsen, Jitendra Malik. 2015. 인간 역학을 위한 순환 네트워크 모델. IEEE 국제 컴퓨터 비전 컨퍼런스 논문집에서. 4346-4354. Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville, Yoshua Bengio. 2014. 생성적 적대적 네트워크. NIPS에서. Niv Granot, Ben Feinstein, Assaf Shocher, Shai Bagon, Michal Irani. 2022. gan 삭제: 단일 이미지 생성 모델로서 패치 최근접 이웃을 옹호. 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR)에서. 13460-13469. Keith Grochow, Steven L Martin, Aaron Hertzmann, Zoran Popović. 2004. 스타일 기반 역 운동학. ACM SIGGRAPH 2004 논문에서. 522-531. Ikhsanul Habibie, Mohamed Elgharib, Kripasindhu Sarkar, Ahsan Abdullah, Simbarashe Nyatsanga, Michael Neff, and Christian Theobalt. 2022. 음성에서 제어 가능한 제스처 합성을 위한 모션 매칭 기반 프레임워크. ACM SIGGRAPH 2022 컨퍼런스 회의록. 1-9. Charles Han, Eric Risser, Ravi Ramamoorthi, and Eitan Grinspun. 2008. 다중 스케일 텍스처 합성. ACM SIGGRAPH 2008 논문. 1-8. Geof Harrower. 2018. 실제 플레이어 모션 기술 in&#39;ea sports ufc 3&#39;. GDC(2018) 회의록. Félix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. 2020. 견고한 모션 중간. ACM Transactions on Graphics(TOG) 39, 4(2020), 60-1. Chengan He, Jun Saito, James Zachary, Holly Rushmeier, and Yi Zhou. 2022. NeMF: Neural Motion Fields for Kinematic Animation. In Advances in Neural Information Processing Systems. David J Heeger and James R Bergen. 1995. Pyramid-based texture analysis/synthesis. In Proceedings of the 22nd Annual Conference on Computer graphics and interactive techniques. 229-238. Gustav Eje Henter, Simon Alexanderson, and Jonas Beskow. 2020. Moglow: Probabilistic and controllable motion synthesis using normalising flows. ACM Transactions on Graphics(TOG) 39, 6(2020), 1-14. Daniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. 학습된 동작 매칭. ACM Transactions on Graphics(TOG) 39, 4(2020), 53-1. Daniel Holden, Taku Komura, Jun Saito. 2017. 캐릭터 제어를 위한 위상 함수 신경망. ACM Transactions on Graphics(TOG) 36, 4(2017), 1–13. Daniel Holden, Jun Saito, Taku Komura. 2016. 캐릭터 동작 합성 및 편집을 위한 딥 러닝 프레임워크. ACM Transactions on Graphics(TOG) 35,(2016), 1-11. Daniel Holden, Jun Saito, Taku Komura, Thomas Joyce. 2015. 합성곱 자동 인코더를 사용한 동작 매니폴드 학습. SIGGRAPH Asia 2015 기술 브리핑. 1-4. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros. 2017. 조건부 적대적 네트워크를 사용한 이미지-이미지 변환. CVPR(2017). 장덕경, 박수민, 이성희. 2022. 동작 퍼즐: 신체 부위별 임의의 동작 스타일 전송. ACM Transactions on Graphics(TOG)(2022). 테로 카라스, 티모 아일라, 사무리 라인, 자코 레티넨. 2018. 품질, 안정성, 변화 개선을 위한 GAN의 점진적 성장. 학습 표현 국제 컨퍼런스에서. 루카스 코바르, 마이클 글레이셔, 프레데릭 피긴. 2002. 동작 그래프. 제29회 컴퓨터 그래픽 및 대화형 기술 연례 컨퍼런스(텍사스주 샌안토니오)(SIGGRAPH &#39;02)의 회의록에서. 미국 뉴욕 컴퓨팅 머시너리 협회, 473–482. https://doi.org/10.1145/566570.Jehee Lee, Jinxiang Chai, Paul SA Reitsma, Jessica K Hodgins, and Nancy S Pollard. 2002. Interactive control of avatars animated with human motion data. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques. 491-500. Seyoung Lee, Jiye Lee, and Jehee Lee. 2022. Learning Virtual Chimeras by Dynamic Motion Reassembly. ACM Trans. Graph. 41, 6, Article 182 (2022). Yongjoon Lee, Kevin Wampler, Gilbert Bernstein, Jovan Popović, and Zoran Popović. 2010. Motion fields for interactive character locomotion. In ACM Transactions on Graphics (TOG). 1-8. Sergey Levine, Jack M Wang, Alexis Haraux, Zoran Popović, Vladlen Koltun. 2012. 저차원 임베딩을 사용한 연속적 문자 제어. ACM Transactions on Graphics(TOG) 31, 4(2012), 1-10. Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, Olga Sorkine-Hornung. 2022. GANimator: 단일 시퀀스에서의 신경 동작 합성. ACM Transactions on Graphics(TOG) 41, 4(2022), 138. Yan Li, Tianshu Wang, Heung-Yeung Shum. 2002. 동작 텍스처: 문자 동작 합성을 위한 2단계 통계적 모델. 제29회 컴퓨터 그래픽 및 대화형 기술 연례 컨퍼런스 회의록. 465-472. Dario Pavllo, David Grangier, and Michael Auli. 2018. Quaternet: 인간 동작을 위한 사원수 기반 순환 모델. arXiv 사전 인쇄본 arXiv:1805.06485 (2018). Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. 2018. Deepmimic: 물리 기반 캐릭터 기술의 예제 기반 심층 강화 학습. ACM Transactions On Graphics (TOG) 37, 4 (2018), 1-14. Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. 2021. Amp: 양식화된 물리 기반 캐릭터 제어를 위한 적대적 동작 사전 확률. ACM Transactions on Graphics (TOG) 40, 4 (2021), 1-20. Ken Perlin. 1985. 이미지 합성기. ACM Siggraph Computer Graphics 19,3 (1985), 287-296. Ken Perlin과 Athomas Goldberg. 1996. Improv: 가상 세계에서 대화형 배우를 스크립팅하는 시스템. 제23회 컴퓨터 그래픽 및 대화형 기술 연례 컨퍼런스 회의록. 205-216. Katherine Pullen과 Christoph Bregler. 2000. 다중 레벨 샘플링을 통한 애니메이션. 컴퓨터 애니메이션 회의록 2000. IEEE, 36-42. Katherine Pullen과 Christoph Bregler. 2002. 모션 캡처 지원 애니메이션: 텍스처링 및 합성. 제29회 컴퓨터 그래픽 및 대화형 기술 연례 컨퍼런스 회의록. 501-508. Jia Qin, Youyi Zheng, Kun Zhou. 2022. 2단계 변압기를 통한 모션 인비트위닝. ACM Transactions on Graphics(TOG) 41, 6(2022), 1-16. Sigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung, and Daniel Cohen-Or. 2023a. MoDi: 다양한 데이터에서 무조건 모션 합성. (2023). Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H Bermano, and Daniel Cohen-Or. 2023b. 단일 모션 확산. arXiv 사전 인쇄본 arXiv:2302.05905(2023). Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J Guibas. 2021. 유머: 견고한 포즈 추정을 위한 3D 인간 모션 모델. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집. 11488-11499. Tamar Rott Shaham, Tali Dekel, Tomer Michaeli. 2019. Singan: 단일 자연 이미지에서 생성 모델 학습. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집. 4570-4580. Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani Lischinski, Daniel Cohen-Or, Baoquan Chen. 2020. Motionet: 골격 일관성을 갖춘 단안 비디오에서 3D 인간 동작 재구성. ACM Transactions on Graphics(TOG) 40, 1(2020), 1-15. Denis Simakov, Yaron Caspi, Eli Shechtman, Michal Irani. 2008. 양방향 유사성을 사용하여 시각적 데이터 요약. 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스에서. IEEE, 1-8. Sebastian Starke, Ian Mason, Taku Komura. 2022. Deepphase: 모션 위상 매니폴드 학습을 위한 주기적 자동 인코더. ACM Transactions on Graphics(TOG) 41,(2022), 1-13. Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, Daniel Cohen-Or. 2022a. MotionCLIP: CLIP 공간에 인간 모션 생성 노출. arXiv 사전 인쇄본 arXiv:2203.08063(2022). Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, Amit H Bermano. 2022b. 인간 모션 확산 모델. arXiv 사전 인쇄본 arXiv:2209.(2022). Truebones Motions Animation Studios. 2022. Truebones. https://truebones.gumroad. com/ 액세스: 2022-9-2. Jonathan Tseng, Rodrigo Castellon, and C Karen Liu. 2022. EDGE: Editable Dance Generation From Music. arXiv 사전 인쇄본 arXiv:2211.10658 (2022). Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. Advances in neural information processing systems 30 (2017). Jack M Wang, David J Fleet, and Aaron Hertzmann. 2007. Gaussian process dynamical models for human motion. IEEE transaction on pattern analysis and machine intelligence 30, 2 (2007), 283–298. Li-Yi Wei, Sylvain Lefebvre, Vivek Kwatra, and Greg Turk. 2009. 예제 기반 텍스처 합성의 최신 기술. Eurographics 2009, 최신 기술 보고서, EG-STAR(2009), 93-117. Li-Yi Wei 및 Marc Levoy. 2000. 트리 구조 벡터 양자화를 사용한 빠른 텍스처 합성. 컴퓨터 그래픽 및 대화형 기술에 대한 제27회 연례 컨퍼런스의 진행 중. 479-488. Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai 및 Nicu Sebe. 2023. Modiff: 노이즈 제거 확산 확률 모델을 사용한 동작 조건 3D 모션 생성. arXiv 사전 인쇄본 arXiv:2301.03949(2023). Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang 및 Hao Li. 2019. 신경망에서 회전 표현의 연속성에 관하여. 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스에서. 5745-5753. 이 저우, 지모 리, 슈앙지우 샤오, 청 허, 젱 황, 하오 리. 2018. 확장된 복잡한 인간 동작 합성을 위한 자동 조건화 순환 네트워크. 국제 학습 표현 컨퍼런스에서. ACM Trans. Graph., Vol. 42, No. 4, Article 1. 출판일: 2023년 8월.
