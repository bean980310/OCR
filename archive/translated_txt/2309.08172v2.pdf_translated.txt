--- ABSTRACT ---
대규모 언어 모델(LLM)은 웹 탐색과 같은 대화형 의사 결정 작업에 성공적으로 적용되었습니다. 이전 방법은 적절한 성능을 달성하는 반면, 모델에 대해 암묵적으로 전방 전용 실행 모드를 가정하여 오라클 궤적을 컨텍스트 내 예제로만 제공하여 모델이 환경에서 추론하는 방법을 안내합니다. 결과적으로 모델은 컨텍스트 내 예제에서 다루지 않는 더 어려운 시나리오(예: 실수)를 처리할 수 없어 최적이 아닌 성능이 발생했습니다. 이 문제를 해결하기 위해 대화형 작업을 상태 공간 탐색으로 모델링하여 LLM 에이전트가 작업을 완료하기 위한 작업을 수행하여 미리 정의된 상태 집합 사이를 전환하는 것을 제안합니다. 이 공식을 사용하면 유연한 역추적이 가능하여 모델이 오류에서 쉽게 복구할 수 있습니다. 제안된 LLM 에이전트를 WebShop 작업과 amazon.com에서 모두 State-Space ExploRation(LASER)으로 평가합니다. 실험 결과에 따르면 LASER는 이전 방법을 크게 능가하고 웹 탐색 작업에서 인간 성능과의 격차를 줄였습니다.
--- INTRODUCTION ---
GPT-(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)은 광범위한 자연어 이해(NLU) 작업에서 놀라운 성과를 달성했습니다(Brown 등, 2020; Ouyang 등, 2022; Wei 등, 2022). 최근 가상 홈 탐색(Yang 등, 2023), 텍스트 기반 게임(Lin 등, 2023) 또는 웹 탐색(Yao 등, 2023; Zhou 등, 2024)과 같은 대화형 의사 결정 작업에 적용되었습니다. 대화형 작업을 해결하기 위해 LLM을 활용하는 이전 방법은 종종 모델에 대해 암묵적으로 전방 전용 실행 모드를 가정하는데, 여기서는 모델에 단계별 추론 방법을 가르치기 위해 몇 가지 오라클 궤적만 컨텍스트 내 예로 제공합니다(Yao 등, 2023; Lo 등, 2023). 즉, 다음 페이지 결과 검색 쿼리 항목 선택 검색 LLM 에이전트 항목 세부 정보 확인 검색으로 돌아가기 항목 거부 LLM 에이전트 완료 LLM 에이전트 구매 LLM 에이전트 성장 내역 최대 반복 횟수에 도달했을 때 항목 선택 그림 1: 웹숍 작업에서 LASER의 상태 전환 다이어그램. 실선 원은 상태를 나타내고 화살표는 가능한 상태 전환을 나타냅니다. 이 공식은 유연한 역추적을 가능하게 하고 앞으로만 가는 예제의 제한을 완화하여 모델이 익숙하지 않은 시나리오를 더 잘 처리하고 오류에서 복구할 수 있도록 합니다. 이러한 오라클 궤적의 모든 단계에서 올바른 작업이 선택됩니다. 모델이 테스트 시간에 예상치 못한 실수를 하면 복구 방법을 알 수 없기 때문에 최적이 아닌 성능으로 이어질 수 있습니다. 동시에 모든 가능한 시나리오를 포괄하기 위해 많은 컨텍스트 내 예제를 포함하는 것은 비용이 많이 들거나 비현실적입니다. 게다가 이전 방법은 모델이 프롬프트 시작 부분에서 가능한 작업을 정의하거나 LLM이 컨텍스트 내 예제에서 가능한 작업을 자동으로 알아낼 것으로 기대하기 때문에 모든 단계에서 모든 작업을 수행할 수 있는 글로벌 작업 공간을 가정합니다. 이는 작업의 난이도를 더욱 높일 수 있으며, LLM은 특정 경우에 잘못된 동작을 수행할 수 있습니다. 앞서 언급한 문제를 해결하기 위해 대화형 작업을 상태 공간 탐색으로 모델링하는 것을 제안합니다. 먼저 LLM 에이전트가 작업 실행 중에 마주칠 수 있는 고수준 가능한 상태 집합을 정의합니다. 그런 다음 각 상태에서 가능한 동작 공간과 각 동작을 수행한 후의 결과 상태를 식별합니다. 이 공식은 대화형 작업에서 LLM 에이전트의 탐색을 상태 전환으로 효과적으로 변환합니다. 각 동작은 에이전트를 한 상태에서 다른 상태로 전환합니다. 당연히 이를 통해 에이전트는 잘못된 동작에서 쉽게 복구할 수 있습니다. 즉, 이전 상태로 되돌릴 수 있는 다른 동작을 수행합니다. 게다가 제안된 공식은 동작 공간을 각 개별 상태와 연관시켜 작업의 난이도를 줄이고 에이전트가 모든 단계에서 항상 유효한 동작을 선택할 수 있도록 합니다. 제안된 LASER on the Webshop(Yao et al., 2022) 작업을 평가하고 LASER를 amazon.com에 직접 적용한 시뮬레이션-실제 전송 실험을 수행했습니다. 우리는 제안된 설정이 에이전트가 컨텍스트 내 예제를 사용하지 않고도 복잡한 사용자 지침을 완료할 수 있게 하고, LASER가 이전의 모든 기준선보다 상당히 우수한 성능을 보이며 인간 성능과의 격차를 줄인다는 것을 보여줍니다.2 방법 2.1 문제 공식화 웹 환경 E와 사용자 지침 I가 주어지면 에이전트는 환경에서 인스턴스화되고 초기 관찰 Oo가 제공됩니다. 에이전트는 사용자 지침을 완료하기 위해 일련의 작업 {a0, a1, ...an}을 수행해야 하며, 각 ai는 환경에서 실행될 때 새로운 관찰 O를 생성합니다.S는 에이전트가 출력을 생성하고 이에 도달한 후 탐색을 중단하는 중지 상태를 나타냅니다.마지막으로 에이전트의 출력을 대상과 비교하여 메트릭을 계산합니다.2.2 LLM 에이전트 이전에 논의한 대로, 에이전트는 많은 수의 컨텍스트 내 예제를 통해 이를 철저히 설명하지 않고도 실행 중에 발생할 수 있는 새로운 상황이나 실수를 처리할 수 있기를 원합니다.따라서 LLM 에이전트에 상태 추적 기능을 장착할 것을 제안합니다. 그림 1은 에이전트의 상태 전환 다이어그램을 보여줍니다. 먼저 에이전트가 환경에서 마주칠 수 있는 가능한 상위 수준 상태 집합을 정의합니다(§2.3). LLM 에이전트는 사용자 입력을 전반적인 목표로 받아들이고 시작 상태에서 초기화됩니다. 모든 단계에서 에이전트는 상태별 시스템 지침, 현재 관찰, 현재 상태에서 허용되는 동작 집합, 과거 생각과 동작의 기록을 입력으로 받습니다. 그런 다음 출력으로 동작 중 하나를 선택하여 에이전트를 다른 상태로 전환하거나 동일한 상태를 유지합니다(§2.4). 에이전트는 중지 상태 또는 최대 단계에 도달할 때까지 프로세스를 반복합니다. 우리의 공식화를 사용하면 모든 상태에서 가능한 상황과 이를 처리하는 방법을 에이전트에게 알리는 자세한 지침을 제공할 수 있습니다. 예를 들어 그림 1에서 볼 수 있듯이 결과 상태에서 현재 결과는 충분할 수도 있고 그렇지 않을 수도 있으며, 에이전트에게 판단에 따라 항목을 선택하거나 다음 페이지로 이동하거나 검색으로 돌아가도록 지시합니다. 따라서 이러한 지침은 문맥 내 예제보다 훨씬 효율적이면서도 에이전트를 안내하는 데 매우 유익할 수 있습니다. 다음으로 상태 및 작업 공간을 설계하는 방법을 자세히 설명합니다. 2.3 상태 설명 저희 작업에서 상태라는 용어를 사용하여 에이전트가 있는 현재 환경을 설명하고 현재 환경 관찰의 구조가 다른 경우에만 에이전트가 두 가지 다른 상태에 있는 것으로 간주합니다. 이를 통해 복잡한 환경에서 에이전트의 탐색을 완전히 지원하기 위해 소수의 상태만 정의할 수 있습니다. 대화형 작업에서 가능한 모든 상태를 수동으로 분류한 후 각 상태에 대해 상태를 자세히 설명하는 일반 지침을 작성합니다. 구체적으로 해당 상태에서 에이전트가 수신할 관찰의 샘플 레이아웃을 제공하고 레이아웃의 모든 사양을 플레이스홀더로 바꿉니다. 또한 해당 상태에서 행동하기 위한 상위 목표와 자세한 지침을 제공합니다. 상태별 지침과 결합된 샘플 레이아웃을 통해 에이전트에게 수신할 수 있는 가능한 관찰과 그에 따라 행동하는 방법을 알릴 수 있습니다. 따라서 더 이상 에이전트를 안내하기 위해 문맥 내 예제를 제공할 필요가 없습니다. WebShop 작업의 경우 총 4개의 상태를 정의하고 검색, 결과 및 항목 상태에 대한 전체 프롬프트는 부록의 표 4, 표 5 및 표 6에서 확인할 수 있습니다.2.4 동작 공간 이전 방법은 종종 암묵적으로 모델에 대한 글로벌 동작 공간을 가정합니다.즉, 모델은 추가 제약 없이 모든 동작을 자유롭게 수행할 수 있습니다.LLM은 대부분의 경우 유효한 동작을 파악할 수 있지만 특정 경우에는 여전히 유효하지 않은 동작을 시도할 수 있습니다.따라서 작업에 대한 모든 가능한 상태를 정의한 후 각 상태에 대한 동작 공간을 추가로 식별하여 이러한 가능성을 배제합니다.특히 에이전트가 각 상태에 대해 선택할 수 있는 허용 동작 세트를 정의하여 에이전트가 항상 유효한 동작을 수행하도록 합니다.에이전트의 상태-동작 매핑은 부록의 표 8에 나와 있습니다.실제로 허용되는 동작은 휴리스틱 방식으로도 결정할 수 있습니다.예: 웹페이지에서 클릭 가능한 모든 버튼을 식별하는 것입니다.성공률 보상 SR 보상 참여 옵션 유형. ASH(Lo et al., 2023) 30.56.LASER ReAct(Yao et al., 2023)* 40.66.Human(Yao et al., 2022) 62.0 85.65.0 88.85.5 75.0 97.86.2 76.3 99.ReAct(저희 재실행) 34.59.표 2: Amazon.com의 결과. WebGUM(Furuta et al., 2023) 45.67.LASER 백업 48.71.LASER 50.75.Human Expert(Yao et al., 2022) 59.82.Success Rate Reward LASER 52.77.LASER + One-shot 50.74.LASER LASER(text-davinci-003) 함수 호출 50.76.38.70.표 1: WebShop 작업의 결과.*단순화된 설정 ReAct 방법(Yao et al., 2023)에서 영감을 얻어 에이전트에게 모든 단계에서 생각을 생성한 다음 생각에 따라 행동을 선택하도록 요청합니다. 에이전트는 정지 상태 또는 최대 단계에 도달할 때까지 생각-행동 프로세스를 계속 반복합니다. 또한 탐색 중에 중간 결과(조사했지만 일치하지 않는 것으로 간주된 항목)를 저장하기 위한 메모리 버퍼를 정의합니다. 이것은 사람들이 일반적으로 원하는 항목을 찾기 전에 몇 가지 백업 옵션을 찾는다는 점에서 인간의 행동과 유사합니다. 에이전트가 최대 단계 수 후에 강제로 중지해야 할 때 최종 출력으로 중간 결과 중 하나를 선택하고 이를 백업 전략이라고 합니다. 3 실험 우리는 WebShop 작업(Yao et al., 2022)에 대한 실험을 수행합니다. 우리는 평가를 위해 500개의 테스트 세트 지침을 사용했고 이전 연구(Yao et al., 2022)에 따라 보상과 성공률을 메트릭으로 채택했습니다. 우리는 GPT-4-0613을 사용하여 LASER와 해당 함수 호출 기능을 구동하여 작업 선택 단계를 구현했습니다. 우리는 다음 기준선과 비교합니다. ReAct(Yao et al., 2023)는 대화형 의사 결정 작업을 위해 설계된 프롬프트 방법입니다. 모든 단계에서 LLM 에이전트는 관찰을 받고 생각이나 행동을 생성할 수 있습니다. 에이전트는 컨텍스트 내 예로 전체 탐색 궤적을 사용하여 프롬프트에 모든 과거 관찰, 생각 및 행동을 축적합니다. 원래 ReAct는 LLM 백본으로 PaLM(Chowdhery et al., 2023)을 사용합니다. 공정한 비교를 위해 GPT-4-0613으로 ReAct 방법을 다시 실행합니다. ASH(Lo et al., 2023)는 ReAct를 기반으로 하며 에이전트 관찰을 요약하고 요약된 정보를 기반으로 행동하는 요약 단계를 추가합니다. WebGUM(Furuta et al., 2023)은 WebShop에서 제공한 1,000명의 인간 시연에서 FlanT5-XL 모델(Chung et al., 2022)을 미세 조정하는 지도 학습 방법입니다. 표 3: WebShop 작업의 절제 결과. 표준 LASER는 zero-shot 작업에서 GPT-4로 구동됩니다. 게다가, 수정 없이 amazon.com에 LASER를 직접 적용하는 시뮬레이션-실제 전송 실험을 실험했습니다. Yao et al.과 동일한 설정을 따릅니다. (2022) 100개의 테스트 세트 지침에 대해 평가한 다음 수동으로 결과를 평가했습니다. 더 자세한 실험 설정은 부록 B에서 설명합니다. 4 결과 실험의 전체 결과는 표 1에 나와 있습니다. 초기 실험에서 ReAct 에이전트가 종종 잘못된 동작을 생성한다는 것을 보여주었습니다. 예를 들어, 지침과 일치하지 않는 항목을 선택하면 결과 페이지로 돌아가기 전에 다음 페이지 버튼(존재하지 않음)을 클릭하려고 합니다. 또한 ReAct 에이전트는 종종 특정 동작에 갇혀 출력을 생성하지 못했습니다. 예를 들어, 에이전트는 최대 단계에 도달할 때까지 다음 페이지로 계속 이동합니다. 문제를 해결하기 위해 자세한 지침을 시스템 프롬프트로 추가했습니다. 최선을 다했지만 에이전트는 여전히 어떤 경우에는 잘못된 동작을 하고 원래 논문보다 더 나쁜 결과를 얻습니다. 반면 LASER는 두 가지 지표에서 모두 기준선보다 큰 차이로 성능이 뛰어나 접근 방식의 효과를 보여줍니다. ReAct와 더 공정한 비교를 하기 위해 LASER의 백업 전략(최대 예산이 소진되면 에이전트가 0점을 받음)을 추가로 제거했습니다. 우리는 여전히 우리의 방법이 기준선보다 매우 큰 마진으로 더 나은 성과를 거두고 있음을 알 수 있습니다.전이 실험의 결과는 표 2에 나와 있습니다.다시 말하지만, LASER는 인간의 성과와 비교해 매우 가까운 결과를 얻습니다.LASER가 amazon.com의 강력한 검색 엔진 덕분에 웹숍보다 이 현실적인 환경에서 더 나은 성과를 거두었다는 것도 고무적입니다.보상 성공률4-7-10-3 4-6 7-9 10-132% 9% 5% 9% 75%그림 2: 왼쪽: 길이가 다른 테스트 세트 에피소드에 대한 LASER의 성과.오른쪽: LASER가 작업을 완료하는 데 걸리는 단계 수의 분포 4.1 분석 우리는 먼저 에이전트의 중요한 디자인 결정을 이해하기 위해 절제 연구를 수행합니다.제로샷 대 퓨샷 우리는 에이전트가 환경에서 탐색하도록 안내하기 위해 상태별 지침만 사용했지만, 이전 연구에서는 종종 맥락 내 예를 채택했습니다. 에이전트가 컨텍스트 내 예제에서 더 많은 이점을 얻을 수 있는지 알아보기 위해 원샷 설정을 실험했습니다.LASER의 모든 프롬프트에 대해 시스템 명령어와 현재 입력 사이에 하나의 예제 입출력 쌍을 추가하고 나머지 에이전트는 동일하게 유지했습니다.제한된 컴퓨팅 예산으로 인해 200개 명령어에 대해서만 절제 연구를 실행했습니다.결과는 표 3에 나와 있습니다.컨텍스트 내 예제를 추가하면 실제로 성능이 저하되는 것을 알 수 있습니다.LASER는 이미 유효한 작업을 100% 수행하므로 에이전트가 컨텍스트 내 예제 없이도 작업을 잘 이해하고 추가된 예제가 실제로 어떤 경우에는 에이전트의 주의를 산만하게 한다고 가정합니다.함수 호출의 효과 LASER는 06/13/23 이후 GPT 모델에서만 활성화된 함수 호출 기능을 활용합니다.따라서 이 디자인을 일반 텍스트 생성으로 대체하는 효과를 보고 싶습니다.이를 위해 허용되는 작업을 함수 목록으로 전달하는 대신 각 작업을 목적과 인수를 설명하는 Python 사전으로 변환한 다음 프롬프트에 추가합니다. 그런 다음 LLM에 JSON 형식으로 출력을 생성하여 적절한 인수로 선택한 작업을 나타내도록 요청합니다. 결과는 표 3에 나와 있습니다. 다시 말하지만, 함수 호출이 없는 에이전트는 이 200개 에피소드에서 약간 더 나쁜 성능을 보였습니다. 이는 함수 호출 기능을 활용하여 대화형 에이전트를 빌드할 때 성능을 높일 수 있음을 보여주며, 향후 LLM을 빌드하는 방향을 제시합니다. 성능 대 궤적 길이 여기서는 LASER 궤적의 길이와 전체 성능에 미치는 영향을 확인하는 데 관심이 있습니다. 그림에서 궤적 길이의 분포와 각 길이 그룹에 대한 에이전트의 성능을 표시합니다. 대부분의 경우 에이전트는 검색-선택-구매인 완료 상태에 도달하기 위해 세 가지 상태 전환만 수행했다는 점에 주목합니다. 왼쪽 그림에서 에이전트의 성능은 일반적으로 궤적이 길어질수록 감소합니다. 그러나 이러한 감소는 ReAct 및 ASH 에이전트(Lo et al., 2023)에 대한 관찰에 비해 덜 심각하여 에이전트의 효과성을 더욱 잘 보여줍니다. 마지막으로, 에이전트가 검색 기록에서 중지하고 선택해야 하는 길이 15 그룹의 경우 성능이 다른 그룹보다 훨씬 낮습니다. 놀랍지 않지만 성공률이 0이 아니어서 에이전트가 일치하는 항목을 찾았지만 첫 번째 패스에서 대상으로 인식하지 못한 경우가 있음을 보여줍니다. 다른 LLM에 대한 일반화 LASER가 덜 강력한 비채팅 모델로 잘 전환될 수 있는지 확인하기 위해 text-davinci-003 모델을 채택했습니다. 이 모델은 함수 호출을 지원하지 않으므로 이전에 설명한 접근 방식을 채택하여 모델에 동작을 나타내는 JSON 출력을 생성하도록 했습니다. 결과는 표 3에 나와 있습니다. text-davinci-003으로 전환하면 성능이 크게 떨어지지만 모델은 여전히 기준선보다 더 나은 결과를 얻습니다. 제안된 에이전트가 다른 기능을 가진 다른 LLM에 쉽게 적용될 수 있음을 보여줍니다. 앞으로 더 강력한 모델이 있으면 에이전트가 이 작업에서 인간의 성능을 능가할 가능성이 있습니다. 또한 LASER의 실패 모드를 검사하기 위한 사례 연구를 수행했으며 추가 결과는 부록 C에 나와 있습니다. 논의합니다.
--- RELATED WORK ---
부록 A의 s. 5 결론 우리는 상태 공간 탐색으로 대화형 웹 탐색 작업을 모델링하는 LLM 에이전트인 LASER를 제안했습니다. 우리의 공식화는 에이전트가 새로운 상황을 처리하고, 실수에서 쉽게 후퇴하고, 항상 유효한 작업을 수행할 수 있게 합니다. 문맥 내 예 없이 상태별 지침에 의해서만 안내되는 LASER는 WebShop 작업에서 모든 기준선을 큰 차이로 능가하고 실제 쇼핑 웹사이트에서 인간의 성능과의 격차를 줄입니다. 우리의 분석에 따르면 LASER는 더 긴 궤적에도 더 강력하고 다른 LLM에 잘 일반화됩니다. 제한 사항 이 작업에서는 쇼핑 도메인의 대상 항목을 찾는 작업만 실험했습니다. 어려운 특성에도 불구하고 주문 추적이나 주문 내역 확인 등 전자 상거래 웹사이트에서 사용자가 일반적으로 수행하는 모든 작업을 다루지는 않습니다. 향후 작업에서는 쇼핑 도메인에서 이러한 인기 있는 작업을 처리할 수 있도록 LASER의 기능을 향상시키는 것이 흥미로울 것입니다. 또한 LASER에 지식 검색기(Ma et al., 2023)나 계산기(Gao et al., 2023)와 같은 더 많은 도구를 장착하여 더 복잡한 명령을 처리할 수 있도록 하는 것도 흥미로울 것입니다. 저희 LASER는 환경의 가능한 상태와 그에 해당하는 설명에 대한 수동 주석이 필요합니다. 이 때문에 저희
--- METHOD ---
s는 모델에 대해 암묵적으로 전방 전용 실행 모드를 가정하는데, 여기서는 오라클 궤적을 컨텍스트 내 예제로만 제공하여 모델이 환경에서 추론하는 방법을 안내합니다. 결과적으로 모델은 컨텍스트 내 예제에서 다루지 않은 더 어려운 시나리오(예: 실수)를 처리할 수 없어 최적이 아닌 성능이 발생합니다. 이 문제를 해결하기 위해 대화형 작업을 상태 공간 탐색으로 모델링하여 LLM 에이전트가 작업을 완료하기 위한 작업을 수행하여 미리 정의된 상태 집합 사이를 전환하는 것을 제안합니다. 이 공식을 사용하면 유연한 역추적이 가능하여 모델이 오류에서 쉽게 복구할 수 있습니다. 제안된 LLM 에이전트를 WebShop 작업과 amazon.com 모두에서 State-Space ExploRation(LASER)으로 평가합니다.
--- EXPERIMENT ---
모든 결과에 따르면 LASER는 이전 방법보다 상당히 우수한 성능을 보이며 웹 탐색 작업에서 인간 성능과의 격차를 줄였습니다. 서론 GPT-(OpenAI, 2023)와 같은 대규모 언어 모델(LLM)은 광범위한 자연어 이해(NLU) 작업에서 놀라운 성능을 달성했습니다(Brown et al., 2020; Ouyang et al., 2022; Wei et al., 2022). 최근에는 가상 홈 탐색(Yang et al., 2023), 텍스트 기반 게임(Lin et al., 2023) 또는 웹 탐색(Yao et al., 2023; Zhou et al., 2024)과 같은 대화형 의사 결정 작업에 적용되었습니다. LLM을 사용하여 대화형 작업을 해결하는 이전 방법은 종종 모델에 대해 암묵적으로 전방 전용 실행 모드를 가정하는데, 여기서는 모델에 단계별 추론 방법을 가르치기 위해 몇 가지 오라클 궤적을 컨텍스트 내 예로만 제공합니다(Yao et al., 2023; Lo et al., 2023). 즉, 다음 페이지 결과 검색 쿼리 항목 선택 검색 LLM 에이전트 항목 세부 정보 확인 검색으로 돌아가기 항목 거부 LLM 에이전트 완료 구매 LLM 에이전트 LLM 에이전트 성장 내역 최대 반복 횟수에 도달했을 때 항목 선택 그림 1: 웹숍 작업에서 LASER의 상태 전환 다이어그램. 실선 원은 상태를 나타내고 화살표는 가능한 상태 전환을 나타냅니다. 이 공식은 유연한 역추적을 가능하게 하고 전방 전용 예제의 제한을 완화하여 모델이 익숙하지 않은 시나리오를 더 잘 처리하고 오류에서 복구할 수 있도록 합니다. 올바른 작업은 해당 오라클 궤적의 모든 단계에서 선택됩니다. 모델이 테스트 시간에 예상치 못한 실수를 저지르면 복구 방법을 알 수 없기 때문에 최적이 아닌 성능으로 이어질 수 있습니다. 동시에, 모든 가능한 시나리오를 포괄하기 위해 많은 컨텍스트 내 예제를 포함하는 것은 비용이 많이 들거나 비현실적입니다. 게다가, 이전 방법은 모델이 프롬프트의 시작 부분에서 가능한 동작을 정의하거나 LLM이 컨텍스트 내 예제에서 가능한 동작을 자동으로 알아낼 것으로 기대하기 때문에 모든 단계에서 모든 동작을 자유롭게 수행할 수 있는 글로벌 동작 공간을 가정합니다. 이는 작업의 난이도를 더욱 높일 수 있으며, LLM은 특정 경우에 잘못된 동작을 수행할 수 있습니다. 앞서 언급한 문제를 해결하기 위해 대화형 작업을 상태 공간 탐색으로 모델링하는 것을 제안합니다. 먼저 LLM 에이전트가 작업 실행 중에 마주칠 수 있는 상위 수준 가능한 상태 집합을 정의합니다. 그런 다음 각 상태에서 가능한 동작 공간과 각 동작을 수행한 후의 결과 상태를 식별합니다. 이 공식은 대화형 작업에서 LLM 에이전트의 탐색을 상태 전환으로 효과적으로 변환합니다. 여기서 각 동작은 에이전트를 한 상태에서 다른 상태로 옮깁니다. 당연히 이를 통해 에이전트는 잘못된 동작에서 쉽게 복구할 수 있습니다. 즉, 이전 상태로 되돌릴 수 있는 다른 동작을 수행합니다. 또한, 제안된 공식은 각 개별 상태와 액션 공간을 연관시켜 작업의 난이도를 줄이고 에이전트가 모든 단계에서 항상 유효한 액션을 선택할 수 있도록 합니다.웹숍(Yao et al., 2022) 작업에서 제안된 LASER를 평가하고 amazon.com에 LASER를 직접 적용한 시뮬레이션-실제 전송 실험을 수행했습니다.제안된 설정을 통해 에이전트가 컨텍스트 내 예제를 사용하지 않고도 복잡한 사용자 지침을 완료할 수 있으며 LASER가 이전의 모든 기준선을 크게 능가하고 인간 성능과의 격차를 줄인다는 것을 보여줍니다.2 방법 2.1 문제 공식화 웹 환경 E와 사용자 지침 I가 주어지면 에이전트는 환경에서 인스턴스화되고 초기 관찰 Oo가 제공됩니다.에이전트는 사용자 지침을 완료하기 위해 일련의 작업 {a0, a1, ...an}을 수행해야 하며, 각 ai는 환경에서 실행될 때 새로운 관찰 O를 생성합니다.S는 에이전트가 출력을 생성하고 도달한 후 탐색을 중단하는 중지 상태를 나타냅니다.마지막으로 에이전트의 출력을 대상과 비교하여 메트릭을 계산합니다. 2.2 LLM 에이전트 이전에 논의했듯이, 우리는 에이전트가 실행 중에 발생할 수 있는 새로운 상황이나 실수를 많은 수의 맥락 내 예제를 통해 철저히 설명하지 않고도 처리할 수 있기를 원합니다. 따라서 우리는 LLM 에이전트에 상태 추적 기능을 제공하기로 제안합니다. 에이전트의 상태 전환 다이어그램이 그림 1에 나와 있습니다. 우리는 에이전트가 환경에서 마주칠 수 있는 가능한 상위 수준 상태 집합을 정의하는 것으로 시작합니다(§2.3). LLM 에이전트는 사용자 입력을 전반적인 목표로 받아들이고 시작 상태에서 초기화됩니다. 모든 단계에서 에이전트는 상태별 시스템 지침, 현재 관찰, 현재 상태에서 허용되는 작업 집합, 과거 생각과 작업의 기록을 입력으로 받습니다. 그런 다음 에이전트를 다른 상태로 전환하거나 동일한 상태를 유지하는 작업 중 하나를 출력으로 선택합니다(§2.4). 에이전트는 중지 상태 또는 최대 단계에 도달할 때까지 프로세스를 반복합니다. 우리의 공식화를 사용하면 모든 상태에서 가능한 상황과 이를 처리하는 방법을 에이전트에 알리는 자세한 지침을 제공할 수 있습니다. 예를 들어, 그림 1에서 볼 수 있듯이 결과 상태에서 현재 결과는 충분히 좋을 수도 있고 좋지 않을 수도 있으며, 에이전트에게 판단에 따라 항목을 선택하거나, 다음 페이지로 이동하거나, 검색으로 돌아가도록 지시합니다. 따라서 이러한 지침은 문맥 없는 예보다 훨씬 효율적이면서도 에이전트를 안내하는 데 매우 유익할 수 있습니다. 다음으로, 상태와 액션 공간을 설계하는 방법을 자세히 설명합니다. 2.3 상태 설명 저희 작업에서 상태라는 용어를 사용하여 에이전트가 있는 현재 환경을 설명하고, 현재 환경 관찰의 구조가 다른 경우에만 에이전트가 두 가지 다른 상태에 있는 것으로 간주합니다. 이를 통해 복잡한 환경에서 에이전트의 탐색을 완벽하게 지원하기 위해 소수의 상태만 정의할 수 있습니다. 대화형 작업에서 가능한 모든 상태를 수동으로 분류한 후 각 상태에 대해 상태를 자세히 설명하는 일반 지침을 작성합니다. 구체적으로, 해당 상태에서 에이전트가 수신할 관찰의 샘플 레이아웃을 제공하고 레이아웃의 모든 사양을 플레이스홀더로 바꿉니다. 또한 해당 상태에서 행동하기 위한 상위 수준 목표와 자세한 지침을 제공합니다. 상태별 지침과 결합된 샘플 레이아웃을 통해 에이전트에게 수신할 수 있는 가능한 관찰과 그에 따른 조치를 알릴 수 있습니다.따라서 더 이상 에이전트를 안내하기 위해 컨텍스트 내 예를 제공할 필요가 없습니다.WebShop 작업의 경우 총 4개의 상태를 정의하고 검색, 결과 및 항목 상태에 대한 전체 프롬프트는 부록의 표 4, 표 5 및 표 6에서 찾을 수 있습니다.2.4 액션 공간 이전 방법은 종종 모델에 대한 글로벌 액션 공간을 암묵적으로 가정합니다.즉, 모델은 추가 제약 없이 모든 액션을 수행할 수 있습니다.LLM은 대부분의 경우 유효한 액션을 파악할 수 있지만 특정 경우에는 여전히 유효하지 않은 액션을 수행하려고 시도할 수 있습니다.따라서 작업에 대한 모든 가능한 상태를 정의한 후 각 상태에 대한 액션 공간을 추가로 식별하여 이러한 가능성을 배제합니다.특히 에이전트가 각 상태에 대해 선택할 수 있는 허용 가능한 액션 세트를 정의하여 에이전트가 항상 유효한 액션을 수행하도록 합니다.에이전트의 상태-액션 매핑은 부록의 표 8에 나와 있습니다. 실제로 허용되는 작업은 휴리스틱하게 결정할 수도 있습니다.예를 들어, 웹페이지에서 클릭 가능한 모든 버튼을 식별합니다.성공률 보상 SR 보상 Att.Opt.유형.ASH(Lo et al., 2023) 30.56.LASER ReAct(Yao et al., 2023)* 40.66.Human(Yao et al., 2022) 62.0 85.65.0 88.85.5 75.0 97.86.2 76.3 99.ReAct(저희가 다시 실행) 34.59.표 2: Amazon.com의 결과. WebGUM(Furuta et al., 2023) 45.67.LASER 백업 48.71.LASER 50.75.Human Expert(Yao et al., 2022) 59.82.Success Rate Reward LASER 52.77.LASER + One-shot 50.74.LASER LASER(text-davinci-003) 함수 호출 50.76.38.70.표 1: WebShop 작업의 결과.*단순화된 설정 ReAct 방법(Yao et al., 2023)에서 영감을 얻어 에이전트에게 모든 단계에서 생각을 생성한 다음 생각에 따라 행동을 선택하도록 요청합니다. 에이전트는 정지 상태 또는 최대 단계에 도달할 때까지 생각-행동 프로세스를 계속 반복합니다. 또한 탐색 중에 중간 결과(조사했지만 일치하지 않는 것으로 간주된 항목)를 저장하기 위한 메모리 버퍼를 정의합니다. 이것은 사람들이 일반적으로 원하는 항목을 찾기 전에 몇 가지 백업 옵션을 찾는다는 점에서 인간의 행동과 유사합니다. 에이전트가 최대 단계 수 후에 강제로 중지해야 할 때 최종 출력으로 중간 결과 중 하나를 선택하고 이를 백업 전략이라고 합니다. 3 실험 우리는 WebShop 작업(Yao et al., 2022)에 대한 실험을 수행합니다. 우리는 평가를 위해 500개의 테스트 세트 지침을 사용했고 이전 연구(Yao et al., 2022)에 따라 보상과 성공률을 메트릭으로 채택했습니다. 우리는 GPT-4-0613을 사용하여 LASER와 해당 함수 호출 기능을 구동하여 작업 선택 단계를 구현했습니다. 우리는 다음 기준선과 비교합니다. ReAct(Yao et al., 2023)는 대화형 의사 결정 작업을 위해 설계된 프롬프트 방법입니다. 모든 단계에서 LLM 에이전트는 관찰을 받고 생각이나 행동을 생성할 수 있습니다. 에이전트는 컨텍스트 내 예로 전체 탐색 궤적을 사용하여 프롬프트에 모든 과거 관찰, 생각 및 행동을 축적합니다. 원래 ReAct는 LLM 백본으로 PaLM(Chowdhery et al., 2023)을 사용합니다. 공정한 비교를 위해 GPT-4-0613으로 ReAct 방법을 다시 실행합니다. ASH(Lo et al., 2023)는 ReAct를 기반으로 하며 에이전트 관찰을 요약하고 요약된 정보를 기반으로 행동하는 요약 단계를 추가합니다. WebGUM(Furuta et al., 2023)은 WebShop에서 제공한 1,000명의 인간 시연에서 FlanT5-XL 모델(Chung et al., 2022)을 미세 조정하는 지도 학습 방법입니다. 표 3: WebShop 작업의 절제 결과. 표준 LASER는 zero-shot 작업에서 GPT-4로 구동됩니다. 게다가, 수정 없이 amazon.com에 LASER를 직접 적용하는 시뮬레이션-실제 전송 실험을 실험했습니다. Yao et al.과 동일한 설정을 따릅니다. (2022) 100개의 테스트 세트 지침에 대해 평가한 다음 수동으로 결과를 평가했습니다. 더 자세한 실험 설정은 부록 B에서 설명합니다. 4 결과 실험의 전체 결과는 표 1에 나와 있습니다. 초기 실험에서 ReAct 에이전트가 종종 잘못된 동작을 생성한다는 것을 보여주었습니다. 예를 들어 지침과 일치하지 않는 항목을 선택하면 결과 페이지로 돌아가기 전에 다음 페이지 버튼(존재하지 않음)을 클릭하려고 합니다. 또한 ReAct 에이전트는 종종 특정 동작에 갇혀 출력을 생성하지 못했습니다. 예를 들어 에이전트는 최대 단계에 도달할 때까지 다음 페이지로 계속 이동합니다. 문제를 해결하기 위해 자세한 지침을 시스템 프롬프트로 추가했습니다. 최선을 다했지만 에이전트는 여전히 어떤 경우에는 잘못된 동작을 하고 원래 논문보다 더 나쁜 결과를 얻습니다. 반면 LASER는 두 가지 지표에서 모두 기준선보다 큰 폭으로 성능이 뛰어나 접근 방식의 효과를 보여줍니다. ReAct와 더 공정한 비교를 하기 위해 LASER의 백업 전략(최대 예산이 소진되면 에이전트가 0점을 받음)을 추가로 제거했습니다. 우리는 여전히 우리의 방법이 기준선보다 매우 큰 마진으로 더 나은 성과를 거두고 있음을 알 수 있습니다.전이 실험의 결과는 표 2에 나와 있습니다.다시 말하지만, LASER는 인간의 성과와 비교해 매우 가까운 결과를 얻습니다.LASER가 amazon.com의 강력한 검색 엔진 덕분에 웹숍보다 이 현실적인 환경에서 더 나은 성과를 거두었다는 것도 고무적입니다.보상 성공률4-7-10-3 4-6 7-9 10-132% 9% 5% 9% 75%그림 2: 왼쪽: 길이가 다른 테스트 세트 에피소드에 대한 LASER의 성과.오른쪽: LASER가 작업을 완료하는 데 걸리는 단계 수의 분포 4.1 분석 우리는 먼저 에이전트의 중요한 디자인 결정을 이해하기 위해 절제 연구를 수행합니다.제로샷 대 퓨샷 우리는 에이전트가 환경에서 탐색하도록 안내하기 위해 상태별 지침만 사용했지만, 이전 연구에서는 종종 맥락 내 예를 채택했습니다. 에이전트가 컨텍스트 내 예제에서 더 많은 이점을 얻을 수 있는지 알아보기 위해 원샷 설정을 실험했습니다.LASER의 모든 프롬프트에 대해 시스템 명령어와 현재 입력 사이에 하나의 예제 입출력 쌍을 추가하고 나머지 에이전트는 동일하게 유지했습니다.제한된 컴퓨팅 예산으로 인해 200개 명령어에 대해서만 절제 연구를 실행했습니다.결과는 표 3에 나와 있습니다.컨텍스트 내 예제를 추가하면 실제로 성능이 저하되는 것을 알 수 있습니다.LASER는 이미 유효한 작업을 100% 수행하므로 에이전트가 컨텍스트 내 예제 없이도 작업을 잘 이해하고 추가된 예제가 실제로 어떤 경우에는 에이전트의 주의를 산만하게 한다고 가정합니다.함수 호출의 효과 LASER는 06/13/23 이후 GPT 모델에서만 활성화된 함수 호출 기능을 활용합니다.따라서 이 디자인을 일반 텍스트 생성으로 대체하는 효과를 보고 싶습니다.이를 위해 허용되는 작업을 함수 목록으로 전달하는 대신 각 작업을 목적과 인수를 설명하는 Python 사전으로 변환한 다음 프롬프트에 추가합니다. 그런 다음 LLM에 JSON 형식으로 출력을 생성하여 적절한 인수로 선택한 작업을 나타내도록 요청합니다. 결과는 표 3에 나와 있습니다. 다시 말하지만, 함수 호출이 없는 에이전트는 이 200개 에피소드에서 약간 더 나쁜 성능을 보였습니다. 이는 함수 호출 기능을 활용하여 대화형 에이전트를 빌드할 때 성능을 높일 수 있음을 보여주며, 향후 LLM을 빌드하는 방향을 제시합니다. 성능 대 궤적 길이 여기서는 LASER 궤적의 길이와 전체 성능에 미치는 영향을 확인하는 데 관심이 있습니다. 그림에서 궤적 길이의 분포와 각 길이 그룹에 대한 에이전트의 성능을 표시합니다. 대부분의 경우 에이전트는 검색-선택-구매인 완료 상태에 도달하기 위해 세 가지 상태 전환만 수행했다는 점에 주목합니다. 왼쪽 그림에서 에이전트의 성능은 일반적으로 궤적이 길어질수록 감소합니다. 그러나 이러한 감소는 ReAct 및 ASH 에이전트(Lo et al., 2023)에 대한 관찰에 비해 덜 심각하여 에이전트의 효과성을 더욱 잘 보여줍니다. 마지막으로, 에이전트가 검색 기록에서 중지하고 선택해야 하는 길이 15 그룹의 경우 성능이 다른 그룹보다 훨씬 낮습니다. 놀랍지 않지만 성공률이 0이 아니어서 에이전트가 일치하는 항목을 찾았지만 첫 번째 패스에서 대상으로 인식하지 못한 경우가 있음을 보여줍니다. 다른 LLM에 대한 일반화 text-davinci-003 모델을 채택하여 LASER가 덜 강력한 비채팅 모델로 잘 전환될 수 있는지 확인했습니다. 이 모델은 함수 호출을 지원하지 않으므로 이전에 설명한 접근 방식을 채택하여 모델에 동작을 나타내는 JSON 출력을 생성하도록 했습니다. 결과는 표 3에 나와 있습니다. text-davinci-003으로 전환하면 성능이 크게 떨어지지만 모델은 여전히 기준선보다 더 나은 결과를 얻습니다. 제안된 에이전트가 다른 기능을 가진 다른 LLM에 쉽게 적용될 수 있음을 보여줍니다. 앞으로 더 강력한 모델이 있으면 에이전트가 이 작업에서 인간의 성능을 능가할 가능성이 있습니다. 또한 우리는 LASER의 실패 모드를 검사하기 위한 사례 연구를 수행했으며 추가 결과는 부록 C에 있습니다. 관련 작업은 부록 A에서 논의합니다. 5
--- CONCLUSION ---
s 우리는 상태 공간 탐색으로 대화형 웹 탐색 작업을 모델링하는 LLM 에이전트인 LASER를 제안했습니다. 우리의 공식화는 에이전트가 새로운 상황을 처리하고, 실수에서 쉽게 후퇴하고, 항상 유효한 작업을 수행할 수 있게 합니다. 문맥 내 예 없이 상태별 지침에 의해서만 안내되는 LASER는 WebShop 작업에서 모든 기준선을 큰 차이로 능가하고 실제 쇼핑 웹사이트에서 인간의 성능과의 격차를 줄입니다. 우리의 분석에 따르면 LASER는 더 긴 궤적에도 더 강력하고 다른 LLM에 잘 일반화됩니다. 제한 사항 이 작업에서는 쇼핑 도메인의 대상 항목을 찾는 작업만 실험했습니다. 어려운 특성에도 불구하고 주문 추적이나 주문 내역 확인 등 전자 상거래 웹사이트에서 사용자가 일반적으로 수행하는 모든 작업을 포괄하지는 않습니다. 향후 작업에서는 쇼핑 도메인에서 이러한 인기 있는 작업을 처리할 수 있도록 LASER의 기능을 향상시키는 것이 흥미로울 것입니다. 또한 LASER에 지식 검색기(Ma et al., 2023)나 계산기(Gao et al., 2023)와 같은 더 많은 도구를 장착하여 더 복잡한 지침을 처리할 수 있다면 흥미로울 것입니다. 저희 LASER는 환경에서 가능한 상태와 해당 설명에 대한 수동 주석이 필요합니다. 이 때문에 저희 방법은 전자상거래나 여행 예약과 같이 소수의 상태만 필요한 특정 도메인(오픈 월드 웹 에이전트가 아닌)에 대한 에이전트를 구축하는 데만 적합할 수 있습니다. 향후 방향을 위해 저희는 각 특정 도메인이 LASER와 같은 에이전트에 의해 관리되고 일반적인 오픈 월드 에이전트가 다른 도메인 에이전트와 협업하여 다양한 사용자 지침을 완료하는 계층적 다중 에이전트 시스템을 구상합니다. 저희 작업의 잠재적 위험과 관련하여 저희는 LASER를 실제 시나리오에 배포하기 전에 특별한 주의와 테스트가 필요하다고 생각합니다. 웹숍 작업에 대한 실험을 수행할 때 저희는 시뮬레이션된 특성 때문에 에이전트가 환경에서 허용되는 모든 조치를 취하도록 허용합니다. 그러나 특정 조치는 실제 세계에서 복구하기 어려운 결과를 초래할 수 있습니다. 예를 들어, 실제 쇼핑 사이트에서 구매 버튼을 클릭하는 것입니다. 따라서 amazon.com에서 실험할 때 에이전트가 품목을 구매하기로 결정하면 에이전트가 멈추도록 강제했습니다. 일반적으로 LASER의 성공률은 아직 완벽하지 않기 때문에 위험이 큰 작업을 진행하기 전에 추가적인 인간 검증이 필요할 수 있습니다. 참고문헌 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David 루안, 임현택, 바렛 Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov 및 Noah Fiedel. 2023. Palm: 경로를 통한 언어 모델링 확장. 기계 학습 연구 저널, 24(240):1–113. 정형원, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le 및 Jason Wei. 2022. 지침 미세 조정 언어 모델 확장. Xiang Deng, Yu Gu, Boyan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun 및 Yu Su. 2023. Mind2web: 웹을 위한 일반 에이전트를 향하여. 제37회 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙 컨퍼런스에서. 히로키 후루타, 오피르 나훔, 쿠앙-휴이 리, 유타카 마츠오, 시샹 셰인 구, 이제딘 구르. 2023. 멀티모달 웹 탐색을 위한 명령어 미세 조정 기반 모델. ICLR 2023 기반 모델에 대한 수학적 및 경험적 이해 워크숍에서. 루위 가오, 아만 마다안, 슈얀 저우, 우리 알론, 펭페이 류, 이밍 양, 제이미 캘런, 그레이엄 노이빅. 2023. 팔: 프로그램 지원 언어 모델. 제40회 기계 학습 국제 컨퍼런스 회의록, ICML&#39;23에서. JMLR.org. Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024. 계획, 긴 맥락 이해 및 프로그램 합성을 갖춘 실제 세계 웹 에이전트. The Twelfth International Conference on Learning Representations에서. Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2023. 대규모 언어 모델을 사용한 HTML 이해. Association for Computational Linguistics: EMNLP 2023, 2803-2821쪽, 싱가포르. Association for Computational Linguistics. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. Webvoyager: 대규모 멀티모달 모델을 사용한 엔드투엔드 웹 에이전트 구축. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and brian ichter. 2023. Inner monologue: Embodied reasoning through planning with language models. The 6th Conference on Robot Learning의 회의록, Proceedings of Machine Learning Research의 205권, 1769-1782쪽. PMLR. Jihyung Kil, Chan Hee Song, Boyuan Zheng, Xiang Deng, Yu Su, and Wei-Lun Chao. 2024. 웹 탐색을 위한 듀얼 뷰 시각적 맥락화. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. 언어 모델은 컴퓨터 작업을 해결할 수 있다. Bill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Prithviraj Ammanabrolu, Yejin Choi, and Xiang Ren. 2023. Swiftsage: 복잡한 상호 작용 작업을 위한 빠르고 느린 사고를 가진 생성 에이전트. Thirtyseventh Conference on Neural Information Processing Systems에서. Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, and Percy Liang. 2018. 워크플로우 기반 탐색을 사용한 웹 인터페이스에서의 강화 학습. International Conference on Learning Representations에서. Robert Lo, Abishek Sridhar, Frank Xu, Hao Zhu, Shuyan Zhou. 2023. 계층적 프롬프트는 웹 탐색에서 대규모 언어 모델을 지원합니다. Association for Computational Linguistics의 연구 결과: EMNLP 2023, 10217-10244페이지, 싱가포르. Association for Computational Linguistics. Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao. 2023. 기술 사슬: 오픈 도메인 질의 응답을 위한 구성 가능한 모델. Association for Computational Linguistics의 제61회 연례 회의록(제1권: 장문 논문), 1599-1618페이지, 토론토, 캐나다. Association for Computational Linguistics. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark. 2023. 자체 개선: 자체 피드백을 통한 반복적 개선. 제37회 신경 정보 처리 시스템 컨퍼런스에서. OpenAI. 2023. Gpt-4 기술 보고서. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, Ryan Lowe. 2022. 인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35권, 27730-27744페이지. Curran Associates, Inc. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, Kristina Toutanova. 2023. 픽셀에서 UI 동작까지: 그래픽 사용자 인터페이스를 통해 지시를 따르는 법 배우기. 제37회 신경 정보 처리 시스템 컨퍼런스에서. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R Narasimhan, Shunyu Yao. 2023. Reflexion: 언어 강화 학습을 통한 언어 에이전트. 제37회 신경 정보 처리 시스템 컨퍼런스에서. Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang. 2023. Adaplanner: 언어 모델을 사용한 피드백으로부터의 적응적 계획. 제37회 신경 정보 처리 시스템 컨퍼런스에서. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. 신경 정보 처리 시스템의 발전에서. Hui Yang, Sifu Yue, Yunzhong He. 2023. 온라인 의사 결정을 위한 Autogpt: 벤치마크 및 추가 의견. Shunyu Yao, Howard Chen, John Yang, Karthik R Narasimhan. 2022. 웹숍: 근거 언어 에이전트를 통한 확장 가능한 실제 세계 웹 상호 작용을 향해. 신경 정보 처리 시스템의 발전. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, Yuan Cao. 2023. React: 언어 모델에서 추론과 행동의 시너지 효과. 제11회 학습 표현 국제 컨퍼런스. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, Gang Yu. 2023. Appagent: 스마트폰 사용자로서의 멀티모달 에이전트. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su. 2024a. Gpt-4v(ision)은 접지된 경우 일반 웹 에이전트입니다. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An. 2024b. Synapse: 컴퓨터 제어를 위한 메모리를 사용한 Trajectory-as-exemplar prompting. The Twelfth International Conference on Learning Representations에서. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig. 2024. Webarena: 자율 에이전트를 구축하기 위한 현실적인 웹 환경. The Twelfth International Conference on Learning Representations에서. 관련 연구 웹 탐색과 같은 대화형 의사 결정 작업이 최근 인기를 얻고 있지만(Liu 등, 2018; Yao 등, 2022; Deng 등, 2023; Zhou 등, 2024), 일부 노력은 방대한 데모 데이터에서 사전 학습된 언어 모델을 미세 조정하여 이러한 작업을 해결하려고 시도했고(Gur 등, 2023; Furuta 등, 2023), 다른 노력은 LLM을 프롬프트하는 것에만 의존하여 웹 환경을 탐색하는 에이전트를 구축하려고 시도했습니다(Yang 등, 2023). LLM 기반 접근 방식 중 ReAct(Yao 등, 2023)와 InnerMonologue(Huang 등, 2023)는 작업을 생성하기 전에 LLM에 사고 과정을 제공합니다. ASH(Lo et al., 2023)와 WebAgent(Gur et al., 2024)는 복잡한 의사 결정 단계를 일련의 더 간단한 단계로 분해하는 데 중점을 둡니다.예를 들어, 먼저 작업 관련 콘텐츠를 요약한 다음 이에 따라 조치합니다.우리의 작업과 가장 유사한 Synapse(Zheng et al., 2024b)도 LLM의 작업을 안내하기 위해 상태 조건 프롬프트를 사용하도록 제안했습니다.그러나 그들의 초점은 몇 가지 샷 예제를 원자 부분으로 분해하는 반면, 우리 에이전트는 컨텍스트 내 예제 없이 상태별 지침만 사용하여 작업을 완료합니다.또 다른 작업 라인은 LLM 에이전트의 계획 단계에 중점을 둡니다.Kim et al.(2023)은 작업하기 전에 계획을 생성한 다음 오류가 발생하면 작업을 개선하는 에이전트 RCI를 제안했습니다.Adaplanner(Sun et al., 2023)는 에이전트 실행 중에 계획을 적응적으로 업데이트하여 계획 접근 방식을 더욱 향상시켰습니다. Reflexion(Shinn et al., 2023) 에이전트는 시행착오적 방식으로 환경적 피드백을 받아 계획과 행동을 개선합니다. 이러한 접근 방식은 우리의 작업과 직교하며 잠재적으로 에이전트와 결합하여 성능을 향상시킬 수 있습니다. 최근에는 다양한 작업에서 다중 모달 에이전트를 개발하려고 시도했습니다. Pix2Act(Shaw et al., 2023)와 AppAgent(Zhang et al., 2023)는 대부분 스크린샷에 응답하여 에이전트가 UI 동작을 예측하는 입력으로 사용했지만 SEEACT(Zheng et al., 2024a), Web Voyager(He et al., 2024) 및 DualVCR(Kil et al., 2024)은 웹사이트의 스크린샷과 텍스트 요소를 모두 활용하여 웹 환경과 상호 작용합니다. 웹 탐색을 상태 전환으로 모델링한다는 아이디어는 이러한 에이전트에 통합하여 성능을 더욱 향상시킬 수 있습니다. B 실험 세부 정보 WebShop은 Amazon 쇼핑 사이트에서 수집한 1,181,436개의 품목을 포함하는 온라인 쇼핑을 위한 시뮬레이션 환경을 제공합니다. 또한 이 작업은 특정 품목과 해당 대상 품목을 구매하기 위한 사람이 주석을 단 지침을 제공합니다. 이전 작업을 따르고 500개의 테스트 세트 지침을 사용하여 LASER를 평가하고 보상과 성공률로 평가합니다. 구매한 품목이 대상 품목과 완벽하게 일치하면 에이전트가 성공한 것으로 간주되고, 그렇지 않고 구매한 품목이 대상 품목과 부분적으로 일치하면 에이전트는 부분 보상(0~100 사이의 척도)을 받습니다. 이 부분 보상은 품목의 가격, 제품 범주, 숨겨진 속성 및 사용자 정의 옵션을 사용하여 계산됩니다. 이 방법의 경우 GPT-4-0613을 사용하여 LASER에 전원을 공급했습니다. 함수 호출 기능을 사용하여 작업 선택 단계를 구현했습니다. 특히 각 작업에 대한 짧은 설명을 작성한 다음 이를 LLM의 functioncall 인수에 목록으로 전달하여 모델에서 선택할 수 있도록 합니다. 에이전트가 최대 15개의 상태 전환을 할 수 있도록 합니다. 실제로 에이전트가 13번의 상태 전환 후에도 완료 상태에 도달하지 못했다면, 예산을 초과하지 않도록 기록에서 선택하도록 강제합니다. amazon.com의 시뮬레이션-실제 전송 실험의 경우, WebShop의 처음 100개 테스트 세트 지침을 사용했습니다. Yao et al.(2022)과 동일한 설정을 따르며, amazon.com의 웹페이지를 WebShop¹과 동일한 형식으로 변환한 다음 LASER 에이전트를 그대로 실행합니다. https://github.com/princeton-nlp/WebShop/tree/master/transfer에 대한 골드 주석이 없으므로 검색으로 돌아가기 &lt; 이전 지침: 거실용 녹색 테이블 램프를 찾고 있으며, 가격이 60.00달러 미만입니다.PRANES = PIRANES Safavieh Lighting Collection Minton Light Green 20인치 침실 거실 홈 오피스 책상 침대 옆 탁자 램프(LED 전구 포함) 가격: $58.평가: NA 설명 기능 리뷰 속성 지금 구매 그림 3: 충분히 좋은 오류 사례의 항목 예, 에이전트가 선택한 항목이 표시되고 사용자 지침이 맨 위에 있습니다. 에이전트가 받는 보상은 0.666입니다.검색으로 돌아가기 &lt; 이전 지침: 닫힌 발가락이 있는 여성용 하이힐을 찾고 있습니다. 영어: i want pink and in size 9, and price lower than 40.00 dollar Masbird Sandals for Women Casual Summer Closed Toe Buckle Strap Wedge Sandals Strappy Platform Sandals 가격: $10.99 to $14.평가: NA 설명 기능 리뷰 속성 지금 구매 우리가 고려한 기준선은 에이전트의 학습을 돕기 위해 어떤 종류의 인간 지식/사전을 사용합니다.우리는 LASER를 안내하기 위해 수동 지침에만 의존했습니다.우리는 LASER에서 수행한 것처럼 고수준의 일반화 가능한 지침을 제공하는 것이 저수준 작업별 궤적(예: WebGUM)을 제공하는 것보다 더 효율적인 학습 방법이라고 믿습니다.직관적으로, 에이전트는 기본적으로 방대한 궤적에서 각 시나리오를 처리하는 방법에 대한 고수준의 통찰력을 추상화하는 법을 배웁니다.이에 비해 우리는 지침의 몇 문장을 통해 그러한 통찰력을 모델에 직접 제공할 수 있습니다.이러한 관점에서 볼 때, 우리의 작업과 이전 작업의 차이점은 고수준의 일반화 가능한 인간 지식을 제공하는 것과 저수준의 사례별 인간 지식을 제공하는 것이라고 말할 수도 있습니다. 우리는 비슷하거나 더 적은 양의 인간 노력이 필요할 때 이러한 고수준 지식을 모델로 제공하는 것이 바람직하다고 믿습니다. 색상 검정 파랑 갈색 분홍 크기 6.5-7 7.5 8 8.5 9 9.5-그림 4: 누락된 세부 정보 오류 사례의 예, 에이전트가 선택한 항목이 표시되고 사용자 지침이 맨 위에 있습니다. 에이전트가 받는 보상은 0.8입니다. amazon.com에서 선택한 항목 LASER에 대해 Yao et al. (2022)을 따르고 인간 평가를 수행했습니다. 특히 항목 속성 일치, 항목 옵션 일치, 항목 범주 일치 및 항목 가격 일치에 수동으로 주석을 달았습니다. 그런 다음 WebShop 작업에 대해 정의된 것과 동일한 함수를 사용하여 개별 보상 점수와 전체 보상 점수 및 성공률을 계산했습니다. 인간과 LASER 모두 품목 가격 일치에서 100%를 달성하므로 표 2에서 이러한 결과를 제외했습니다.다른 기준선과의 비교와 관련하여 ReAct(Yao et al., 2023)와 ASH(Lo et al., 2023) 모두 수동으로 작성된 지침과 수동으로 주석이 달린 에이전트 궤적을 컨텍스트 내 데모로 사용하여 LLM을 촉구했으며 이는 하위 섹션 4.1의 원샷 설정에 해당합니다.WebGUM(Furuta et al., 2023)의 경우 1k 인간 주석이 달린 금 궤적을 사용하여 모델을 미세 조정했습니다.따라서 모든 C 사례 연구 LASER의 실패 사례를 이해하기 위해 Dev 세트에서 30개의 오류 사례에 수동으로 주석을 달았습니다.오류를 세 가지 범주로 광범위하게 분류했습니다.충분히 좋은 품목: 에이전트가 선택한 품목은 작성자의 관점에서 사용자 지침을 충족하지만 전체 점수를 받지 못했습니다. 30건 중 9건이 이 범주에 속하는 것으로 나타났으며, 그림 3에 그 예가 나와 있습니다. 에이전트가 찾은 품목은 실제로 예산 내의 가격을 가진 거실용 녹색 테이블 램프이지만, 잘못된 것으로 간주됩니다. 검색 실패: 에이전트가 검색을 위해 적절한 쿼리를 사용했음에도 불구하고 검색 엔진에서 반환한 품목 중 어느 것도 사용자 요구 사항을 충족하지 못했습니다. 30건 중 12건이 이 범주에 속하는 것으로 나타났습니다. 보다 효과적인 검색기나 검색 엔진이 이러한 문제를 해결할 수 있을 것이라고 가정합니다. 누락된 세부 정보: 에이전트가 선택한 품목은 실제로 특정 세부 정보에 대한 사용자의 지시와 일치하지 않습니다. 30건 중 9건이 이 범주에 속하는 것으로 나타났으며, 그림 4에 그 예가 나와 있습니다. 이 예에서 선택한 여성용 신발의 색상과 사이즈가 모두 사용자 지시와 일치했지만, 하이힐 신발은 아닙니다. 이는 LASER가 많은 일치하는 세부 정보가 있는 항목을 접했을 때 실수를 할 수 있음을 나타내며, 자체 피드백/검증 모듈이 이 문제를 해결할 수 있는지 확인하는 것이 흥미로울 것입니다(Madaan et al., 2023). D 실험에 사용된 프롬프트 E 라이선스 Webshop 작업과 ReAct 방법은 모두 MIT 라이선스에 따라 릴리스되었습니다. 둘 다 연구 목적으로 릴리스되었으며, 실험은 의도된 용도와 일치합니다. 귀하는 사용자가 올바른 항목을 찾는 데 도움을 줄 수 있는 지능형 쇼핑 도우미입니다. 다음 형식으로 현재 웹 탐색 세션에 대한 관찰이 제공됩니다. 현재 관찰: WebShop 지침: {사용자 지침} [버튼] 검색 [버튼_](사용자 지침에 따라 검색 쿼리를 생성하고 이 버튼을 선택하여 관련 항목 찾기) 관찰의 모든 버튼은 수행할 수 있는 가능한 작업을 나타냅니다. 현재 관찰을 기반으로 귀하의 작업은 수행해야 할 다음 작업에 대한 근거를 생성하는 것입니다. 과거 근거 및 작업의 내역이 제공되는 경우 근거를 생성할 때 내역도 고려해야 합니다. 표 4: 검색 상태에 사용한 시스템 지침. 당신은 사용자가 올바른 품목을 찾을 수 있도록 도울 수 있는 지능형 쇼핑 도우미입니다. 현재 웹 탐색 세션에 대한 관찰 결과가 다음 형식으로 제공됩니다.현재 관찰 결과: 지침: {사용자 지침} [버튼] 검색으로 돌아가기 [버튼_] (검색 페이지로 돌아가려면 이 버튼을 선택하세요) 페이지 현재 페이지 번호 (전체 결과: 총 결과 수) [버튼] 다음 &gt; [버튼_] (다음 결과 페이지로 이동하려면 이 버튼을 선택하세요) [버튼] {항목_id 1} [버튼_] (항목 1의 세부 정보를 보려면 이 버튼을 선택하세요) {항목 1의 이름} {항목 1의 가격} [버튼] {항목_id 2} [버튼_] (항목 2의 세부 정보를 보려면 이 버튼을 선택하세요) {항목 2의 이름} {항목 2의 가격} [버튼] {항목_id 3} [버튼_] (항목 3의 세부 정보를 보려면 이 버튼을 선택하세요) {항목 3의 이름} {항목 3의 가격} {더 많은 항목...} 이 단계에서는 사용자 지침과 일치할 수 있는 항목을 선택해야 합니다. 항목의 세부 정보가 사용자 지침과 일치하지 않더라도 일치시킬 수 있도록 다른 사용자 지정 옵션을 제공할 수 있습니다. 예를 들어 항목 이름에 색상 x가 있지만 나중에 색상 y로 사용자 지정할 수 있으며 사용자 지정 옵션은 항목을 선택한 후에 표시됩니다. 따라서 항목 이름이 지침과 관련이 있거나 부분적으로 일치하는 경우 해당 항목을 선택하여 세부 정보를 확인해야 합니다. 이전에 항목을 선택한 경우(버튼을 클릭한 경우) 동일한 항목을 다시 선택해서는 안 됩니다. 즉, [클릭한 버튼] 항목 ID [클릭한 버튼]인 항목을 선택하지 마세요. 다음 형식으로 응답을 준비하세요. 근거: 사용자가 {대상 항목의 키워드}를 원했고 {항목 x의 일치하는 키워드}를 찾았으므로 항목 {항목 ID x}가 일치하는 것으로 보입니다. 표 5: 결과 상태에 사용한 시스템 지침. 사용자는 올바른 항목을 찾는 데 도움을 줄 수 있는 지능형 쇼핑 도우미입니다. 현재 웹 탐색 세션에 대한 관찰이 다음 형식으로 제공됩니다.현재 관찰: 지침: {사용자 지침} [버튼] 검색으로 돌아가기 [버튼_] (검색 페이지로 돌아가려면 이 버튼을 선택하세요) [버튼] &lt; 이전 [버튼_] (이전 결과 페이지로 돌아가려면 이 버튼을 선택하세요) {사용자 정의 유형1}: [버튼] 옵션1 [버튼_] [버튼] 옵션2 [버튼_] {사용자 정의 유형2}: [버튼] 옵션1 [버튼_] [버튼] 옵션2 [버튼_] {추가 사용자 정의 옵션... (있는 경우)} {항목 이름 및 세부 정보} [버튼] 설명 [버튼_] (항목의 전체 설명을 보려면 이 버튼을 선택하세요) [버튼] 기능 [버튼_] (항목의 전체 기능을 보려면 이 버튼을 선택하세요) [버튼] 리뷰 [버튼_] (항목의 전체 리뷰를 보려면 이 버튼을 선택하세요) [버튼] 지금 구매 [버튼_] (항목을 구매하려면 이 버튼을 선택하세요) 설명: (이것이 표시되면 설명 버튼을 다시 선택해서는 안 됩니다) {전체 항목 설명(있는 경우) 또는 &quot;없음&quot;} 기능: (표시되는 경우 기능 버튼을 다시 선택해서는 안 됨) {항목의 전체 기능(있는 경우) 또는 &quot;없음&quot;} 리뷰: (표시되는 경우 리뷰 버튼을 다시 선택해서는 안 됨) {항목의 전체 리뷰(있는 경우) 또는 &quot;없음&quot;} 대상 항목 세부 정보(사용자가 찾는 것): 키워드: {대상 항목의 키워드} 최대 가격: {항목의 가격은 이 값을 초과해서는 안 됨} 이 단계에서는 항목이 사용자 지침과 일치하는지 확인해야 합니다. 항목이 사용자 지침과 일치하는지 여부를 결정할 때 사용 가능한 사용자 지정 옵션을 고려해야 합니다. 항목을 사용자 지침과 일치하도록 사용자 지정할 수 있거나 사용자 지정 옵션이 사용자 사양을 충족하는 경우에도 잘 맞습니다. 항목이 사용자 지침과 일치하지 않고 사용자 지정 옵션이 충분하지 않은 경우 이전 페이지로 이동하여 다른 항목을 볼 수 있습니다. 또한 항목의 설명, 기능 및 리뷰를 확인하여 더 자세한 내용을 볼 수 있습니다(설명, 기능 및 리뷰는 &quot;없음&quot;일 수 있으므로 이미 제공된 경우 다시 확인하지 마십시오). 다음 형식으로 응답을 준비하십시오. 근거: 사용자는 {대상 항목의 키워드}를 원했으며 다음과 같은 사용자 지정 옵션이 필요했습니다. {대상 항목의 사용자 지정}, 항목은 현재 관찰에서 항목의 키워드이며 다음과 같은 사용자 지정 옵션이 있습니다. {현재 항목에 사용 가능한 옵션}, 이는 {포함}/{사용자 요구 사항을 포함하지 않음}, 따라서 {항목을 구매}/{자세한 내용 확인}/{다른 항목을 보려면 이전 페이지로 이동}해야 합니다. 표 6: 항목 상태에 대해 사용한 시스템 지침. 사용자는 올바른 항목을 찾는 데 도움을 줄 수 있는 지능형 쇼핑 도우미입니다. 다음 형식으로 현재 환경에 대한 관찰 결과와 다음에 취해야 할 조치에 대한 근거가 제공됩니다.현재 관찰: 표 4, 표 5 및 표에서 표시된 검색 또는 결과 또는 항목 상태의 관찰 레이아웃다음 조치 근거: {다음 조치에 대한 근거} 귀하의 작업은 근거에 따라 함수 호출 중 하나를 수행하는 것입니다.표 7: 생각에서 조치를 생성하는 데 사용한 시스템 명령어. 상태 검색 결과 항목 사용 가능한 작업 {&quot;name&quot;: &quot;검색&quot;, &quot;description&quot;: &quot;키워드를 기준으로 인벤토리에서 대상 항목을 검색하려면 이 기능을 사용합니다.&quot;} {&quot;name&quot;: &quot;select_item&quot;, &quot;description&quot;: &quot;검색 결과에서 항목 중 하나를 선택하고 세부 정보를 확인하려면 이 기능을 사용합니다.&quot;} {&quot;name&quot;: &quot;다음&quot;, &quot;description&quot;: &quot;현재 페이지의 항목 중 사용자 지시와 일치하는 항목이 없는 경우, 이 기능을 사용하여 다음 검색 결과 페이지로 이동하여 더 많은 항목을 봅니다.&quot;} {&quot;name&quot;: &quot;검색으로 돌아가기&quot;, &quot;description&quot;: &quot;이 기능을 사용하여 초기 검색 페이지로 돌아갑니다. 여러 페이지의 항목을 탐색하고 기록에서 여러 항목의 세부 정보를 확인했지만 사용자 지시와 일치하는 항목이 없는 경우에만 이 기능을 사용해야 합니다.&quot;} {&quot;name&quot;: &quot;설명&quot;, &quot;description&quot;: &quot;항목이 사용자 지시와 완벽하게 일치하는지 확실하지 않은 경우, 이 기능을 사용하여 항목 설명을 확인합니다.&quot;} {&quot;name&quot;: &quot;특징&quot;, &quot;설명&quot;: &quot;이 기능을 사용하여 항목이 사용자 지침과 완벽하게 일치하는지 확실하지 않은 경우 항목의 특징을 확인하세요&quot;} {&quot;name&quot;: &quot;리뷰&quot;, &quot;설명&quot;: &quot;이 기능을 사용하여 항목이 사용자 지침과 완벽하게 일치하는지 확실하지 않은 경우 항목의 리뷰를 확인하세요&quot;} {&quot;name&quot;: &quot;지금 구매&quot;, &quot;설명&quot;: &quot;이 기능을 사용하여 현재 항목이 사용자 지침과 완벽하게 일치하는 경우 현재 항목을 구매하세요&quot;} {&quot;name&quot;: &quot;이전&quot;, &quot;설명&quot;: &quot;이 기능을 사용하여 현재 항목이 사용자 지침과 일치하지 않는 경우 결과 페이지로 돌아가세요&quot;} 표 8: 각 상태에서 에이전트의 액션 공간. 각 액션은 OpenAI 2의 가이드라인을 따르는 함수 호출로 구현되며, 함수 호출에 사용되는 추가 매개변수는 간결함을 위해 여기에서 생략합니다.
