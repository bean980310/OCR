--- ABSTRACT ---
Polyglot은 다국어 언어 모델의 비영어 언어 성능을 향상시키는 것을 목표로 하는 선구적인 프로젝트입니다. mBERT(Devlin et al., 2019), XGLM(Lin et al., 2022), BLOOM(Scao et al., 2022)과 같은 다양한 다국어 모델이 있음에도 불구하고, 연구자와 개발자는 현재 다국어 모델의 비영어 언어 기능에 대한 불만으로 인해 종종 각자의 언어로 단일 언어 모델을 구축하는 데 의존합니다. 이러한 격차를 해소하기 위해, 우리는 비영어 언어에서 향상된 성능을 제공하는 고급 다국어 언어 모델을 개발하고자 합니다. 이 논문에서는 다국어적 특성이 아닌 특정 초점을 나타내는 Polyglot 한국어 모델을 소개합니다. 저희 팀은 TUNiB¹와 협력하여 연구 여정을 위해 세심하게 큐레이팅된 1.2TB의 한국어 데이터를 수집했습니다. 우리는 다국어 모델에 착수하기 전에 한국어 모델 개발을 우선시하기로 의도적으로 결정했습니다. 이러한 선택은 여러 요인에 의해 동기가 부여되었습니다. 첫째, 한국어 모델은 기존 다국어 모델과의 성과 비교를 용이하게 했으며, 마지막으로 한국 기업과 연구자의 특정 요구 사항을 충족했습니다. 이 논문은 다국어 모델에서 비영어 언어 성과 격차를 해결하기 위한 몇 가지 단계를 제안하는 Polyglot Korean 모델을 개발하는 작업을 제시합니다. 1
--- EXPERIMENT ---
s, 우리는 Polyglot-Ko가 다양한 벤치마크 데이터 세트에서 경쟁력 있는 결과를 달성한다는 것을 성공적으로 입증했습니다. 성과를 제시하는 것 외에도 잠재적인 한계를 인정하고 향후 개선이 필요한 영역을 식별합니다. 추가 연구에 대한 권장 사항을 제공함으로써 이 분야의 발전을 촉진하고자 합니다. 우리는 Polyglot-Ko가 한국어 자연어 처리 커뮤니티에 귀중한 리소스가 되어 혁신적인 응용 프로그램을 개발하고 한국어의 복잡성과 역동성에 대한 더 깊은 이해에 기여할 것이라고 굳게 믿습니다. 2 데이터 세트 우리는 TUNiB와 협력하여 연구를 위한 대규모 한국어 데이터 세트를 수집했습니다. 총 1.2TB에 달하는 데이터 세트는 우리의 협력적 노력을 통해 세심하게 수집되었습니다. 그런 다음 이 데이터 세트에 대한 전처리를 수행하여 863GB의 텍스트 데이터를 얻었으며 이는 분석 및 모델 학습의 기초가 되었습니다. 한국어: 소스 크기(GB) 한국어 블로그 게시물 682.한국 뉴스 데이터 세트 87.모두 코퍼스 26.한국 특허 데이터 세트 19.한국 Q&amp;A 데이터 세트 18.KcBert 데이터 세트 12.한국 소설 데이터 세트 6.한국 온라인 댓글 4.한국 위키피디아 1.클로바 콜 &lt; 1.네이버 센티먼트 무비 코퍼스 &lt; 1.한국 증오 발언 데이터 세트 &lt; 1.자막 열기 &lt; 1.AIHub 다양한 작업 데이터 세트 &lt; 1.표준 한국어 사전 &lt; 1.표 1: 한국어 데이터 세트 2.데이터 분석 데이터 분석의 주요 동기는 학습 및 추론 단계에서 발생할 수 있는 잠재적 위험을 완화하는 것입니다.우리는 모델 성능과 신뢰성에 부정적인 영향을 미칠 수 있는 다양한 문제를 극복하기 위해 노력합니다.예를 들어, 모델 학습 중에 문제가 될 수 있는 비어 있거나 지나치게 짧은 텍스트 데이터, 반복되는 단어 및 문자, 중복된 데이터 인스턴스와 같은 과제를 식별하여 처리합니다. 또한, 개인 식별 정보(PII)가 있으면 복잡해질 수 있는 추론 단계에 특히 주의를 기울입니다. 데이터를 철저히 분석하여 이러한 위험을 최소화하고 모델의 견고성과 개인 정보 보호 준수를 보장하기 위해 노력합니다. 이러한 위험을 완화하기 위해 데이터를 세심하게 검토한 결과, 데이터를 네 가지 유형으로 분류하는 데 성공했습니다. • 학습에 사용할 수 있는 데이터: 이 범주는 주로 뉴스와 위키피디아 데이터로 구성되어 있으며, 충분히 긴 텍스트 시퀀스로 상당한 정보를 제공합니다. • 학습에 맥락적 정보가 필요한 데이터: 이 범주에서는 주로 블로그 데이터와 뉴스 데이터를 접했습니다. 이러한 데이터 세트에는 잘못 스크래핑된 짧은 텍스트가 많이 포함되어 있어 학습 과정에서 맥락적 정보를 포함해야 했습니다. • 증오 표현이 포함된 데이터: 특정 커뮤니티 웹사이트에서 얻은 데이터 세트에서 상당한 증오 표현이 발견되어 모델 학습 중에 이 문제를 해결하는 것이 중요함을 강조했습니다. • NLP 작업별 데이터: 이 범주에는 텍스트 분류나 엔터티 인식과 같은 NLP 작업을 위해 특별히 설계된 데이터가 포함됩니다. 이 데이터는 모델 학습에 활용할 수 있지만 모델 평가 중에 별도의 처리가 필요합니다. 데이터 유형을 검사하는 동안 최적의 학습을 보장하기 위해 사전 처리가 필요한 다양한 품질 문제가 발생했습니다. 이러한 문제가 모델 학습에 미치는 잠재적인 부정적인 영향을 인식하고 이를 정리하여 텍스트 사전 처리 파이프라인에 통합했습니다. 해결한 특정 품질 문제는 다음과 같습니다. • • 빈 텍스트: 텍스트 콘텐츠가 없는 인스턴스. 불필요한 공백: 불필요한 공백이 여러 개 포함된 인스턴스. • 익명화: 데이터 인스턴스 내에서 개인 식별 정보를 식별하고 제거합니다. • 정리되지 않은 HTML 태그: 제대로 정리되지 않은 HTML 태그를 제거합니다. • 중복 제거: 정확한 일치 항목을 기반으로 중복된 데이터 인스턴스를 식별하고 제거합니다. • 손상된 코드: HTML 또는 마크다운의 일부만 있는 인스턴스를 처리합니다. • • 짧은 텍스트: 지나치게 짧은 데이터 인스턴스를 감지하고 처리합니다. 반복되는 문자: 반복되는 문자가 있는 인스턴스를 식별하고 주소 지정합니다. 이러한 품질 문제를 텍스트 전처리 워크플로의 일부로 해결함으로써 모델 학습에 사용되는 데이터의 품질과 안정성을 향상시키고자 했습니다. 데이터 전처리의 중요한 측면 중 하나는 다국어 모델의 초점이 한국어 텍스트 생성에 있기 때문에 HTML과 유사한 코드를 대부분 제거하는 것이었습니다. 또한 데이터 길이는 전처리 작업에서 중요한 역할을 했습니다. 텍스트 길이가 길수록 모델 학습에 더 많은 맥락적 정보를 제공하는 반면, 텍스트 길이가 짧을수록 텍스트 생성에 사용할 수 있는 맥락이 제한됩니다. 3개 모델 다국어 모델을 학습하기 위해 EleutherAI의 GPT-NeoX 코드베이스(Andonian et al., 2021)를 사용했습니다. 이 코드베이스는 학습 프로세스에 견고한 기반을 제공했습니다. 또한 HPC 클러스터³에서 256개의 A100(8 * 32 노드)에 대한 액세스를 제공한 Stability AI²로부터 귀중한 지원을 받았습니다. 이 컴퓨팅 인프라는 모델을 효율적으로 학습하는 데 중요한 역할을 했습니다. 각 모델의 학습 토큰 정보는 다음과 같습니다.• 1.3B 모델은 213B 토큰에서 학습되었습니다.• 3.8B 모델은 219B 토큰에서 학습되었습니다.2https://stability.ai/ https://hpc.stability.ai/ • 5.8B 모델은 172B 토큰에서 학습되었습니다.• 12.8B 모델은 167B 토큰에서 학습되었습니다.사용 가능한 계산 리소스의 한계로 인해 모델은 다양한 수의 토큰으로 학습되었습니다.또한 노력에도 불구하고 4, 1.3B 및 3.8B 모델의 1 에포크 경계 근처에서 손실이 급격히 떨어지면서 생성이 중단되었습니다(예: 동일한 토큰을 반복적으로 생성).결과적으로 생성 성능이 더 우수했기 때문에 에포크 경계 이전에 모델 체크포인트를 선택하고 조기에 중지하기로 결정했습니다. 모든 모델에서 일관된 토크나이저가 활용되었으며, 어휘 크기는 30003입니다. 토크나이저는 형태소 인식 ByteLevel BPE(Byte-Pair Encoding)를 사용하여 학습되었습니다. 한국어 텍스트에 널리 사용되는 형태소 분석 도구인 MeCab5를 사용하여 형태소 분석을 수행했습니다. 이를 통해 모델에 한국어에 맞는 효과적이고 일관된 토큰화 체계가 제공되었습니다. 이제 각 모델의 학습 프로세스에 대한 세부 정보를 살펴보겠습니다. 1.3B 모델 1.3B 모델은 모델 병렬성 없이 학습되었으며 총 배치 크기는 1024였습니다. 그러나 손실이 약 100,000단계에서 급격히 떨어지면서 생성이 중단되었습니다. 이를 처리하기 위해 최적의 모델 선택을 보장하기 위해 모델 체크포인트를 시작하기 전에 평가하고 검증했습니다. 3.8B 모델 1.3B 모델과 유사하게 3.8B 모델도 약 100,000단계에서 동일한 증상을 경험했습니다. 학습하는 동안 모델 병렬성을 적용했으며, 전체 배치 크기는 1.3B 모델과 동일하게 유지되었습니다. 결과적으로 모델 학습 프로세스를 중단하기로 결정했습니다. 5.8B 모델 5.8B 모델은 모델 병렬성을 활용했으며, 전체 배치 크기는 1.3B 및 3.8B 모델에 비해 1/4로 줄었습니다. 총 320,000단계로 구성된 172B 토큰으로 학습했습니다. 1.3B 및 3.8B에 비해 더 낮은 토큰으로 학습했기 때문에 모델의 성능은 학습 단계 수가 증가함에 따라 지속적으로 향상되었습니다. 4 과도하게 학습되었거나 과적합되었을 것으로 생각하지만, 과학적 증거는 없습니다. Shttps://bitbucket.org/eunjeon/mecab-ko-dic/ src/master/2, Hyperparameter 1.3B 3.8B 5.8B 12.8B nparameters 1,331,810,304 3,809,974,272 5,885,059,072 12,898,631, nlayers dmodel3,4,5,8,12,16,20,30,003 / 30, Rotary (ROPE)30,003 / 30, Rotary (ROPE)30,003 / 30, Rotary (ROPE)dff nheads dhead nvocab 위치 인코딩 ROPE 차원 표 2: Polyglot-Ko 모델의 구성 설정. 12.8B 모델 12.8B 모델의 경우 5.8B 모델보다 2배 더 큰 규모로 모델 병렬성이 적용되었습니다. 전반적인 배치 크기는 경사 축적 단계(GAS)를 사용하여 유지되었습니다. 모델은 총 301,000단계에 대해 학습되었습니다. 4가지 실험 KOBEST 데이터 세트(Kim et al., 2022)에서 Polyglot-Ko에 대한 평가를 수행했습니다. 여기에는 COPA, HellaSwag, SentiNeg, BoolQ, WiC의 5가지 다운스트림 작업이 포함됩니다. 각 작업은 언어 이해 및 추론의 다양한 측면에 초점을 맞춥니다. • COPA는 주어진 전제의 원인/결과인 대안을 선택해야 합니다. • • • HellaSwag는 상식적 추론과 자연어 추론을 평가합니다. BoolQ는 여러 문장에 대한 추론이 필요한 질문에 답하는 모델의 능력을 테스트하도록 설계되었습니다. SentiNeg는 한국어의 감정 분석에 초점을 맞춥니다. • WiC는 두 가지 주어진 맥락에서 대상 단어의 의미가 같은지 다른지 식별하는 것을 요구합니다. 평가하는 동안, 우리는 Polyglot-Ko의 성능을 ko-gpt-trinity-1.2B6, KoGPT(Kim et al., 2021), XGLM-7.5B(Lin et al., 2022)를 포함한 다른 유사한 모델과 비교했습니다. 이러한 모델은 현재 유일하게 &quot;https://huggingface.co/skt/ko-gpt-trinity-1. 2B-v0.publicly available billion-scale Korean language model&quot;이며, 다른 다국어 모델을 제외합니다. 평가 프로세스에는 제공된 프롬프트를 사용하는 것이 포함되었으며, F1-점수는 모든 작업에 대한 평가 지표로 사용되었습니다. 우리는 실험에 대한 평가 코드베이스를 제공한 EleutherAI의 Im-evaluation-harness(Gao et al., 2021) 저장소의 polylgotbranch를 활용했습니다. 이를 통해 다양한 모델에서 일관되고 표준화된 평가 절차가 가능해져 공정한 비교가 가능했습니다. 모든 모델에서 거의 무작위적인 성능이 관찰되었기 때문에 이 섹션에서는 WiC 작업에서 Polyglot-Ko의 성능을 보고하지 않기로 했습니다. 그러나 WiC 작업에 대한 자세한 결과는 부록 B에서 찾을 수 있습니다. 그림 1은 few-shot 예제의 수를 변경했을 때 얻은 결과를 보여줍니다. 4.1 COPA 실험은 COPA 작업에서 수행되었으며, 결과는 표 3의 왼쪽 부분에 나와 있습니다. 이 표는 few-shot 예제의 수를 기반으로 한 모델 성능의 포괄적인 비교를 제공합니다. 공정성을 보장하기 위해 모든 모델은 동일한 조건에서 동일한 프롬프트를 사용하여 평가되었습니다. 결과는 우리의 12.8B 모델이 모든 시나리오에서 다른 모델보다 성능이 우수하다는 것을 분명히 보여줍니다. 구체적으로, 우리의 12.8B 모델은 0-shot 및 50-shot 설정에서 가장 높은 F1 점수를 달성합니다. O-shot의 경우, 우리 모델은 다른 모든 모델을 능가하는 인상적인 F1 점수 0.7937을 달성합니다. 마찬가지로 50-shot 시나리오에서 우리의 12.8B 모델은 가장 높은 F 점수 0.8368을 달성합니다. 이러한 결과는 비교 가능한 모델에 비해 12.8B 모델이 우수함을 강조합니다.https://github.com/EleutherAI/ 1m-evaluation-harness/tree/polyglot COPA(n=shot) HellaSwag 모델 매개변수 n=Ko-GPT-Trinity 1.2B Polyglot-Ko(우리의) 1.3B Polyglot-Ko(우리의) 3.8B 0.760 0.Polyglot-Ko(우리의) 5.8B 0.775 0.KOGPT 6.0B XGLM 7.5B Polyglot-Ko(우리의) 12.8B 0.670 0.648 0.642 0.651 0.0.720 0.719 0.720 0.721 0.525 0.526 0.528 0.0.764 0.779 0.571 0.583 0.567 0.0.778 0.789 0.598 0.600 0.598 0.0.735 0.729 0.728 0.748 0.559 0.583 0.583 0.0.672 0.673 0.677 0.712 0.566 0.569 0.556 0.0.794 0.811 0.804 0.837 0.595 0.631 0.610 0.n=5 n=10 n=50 n=0 n=5 n=10 n=0.527 0.517 0.표 3: COPA 및 HellaSwag 작업에서의 성과(F1 점수) BoolQ (n=샷) SentiNeg 모델 매개변수 n=0 n=5 n=10 n=50 n=0 n=5 n=10 n=Ko-GPT-Trinity 1.2B 다국어-Ko(우리) 1.3B 0.336 0.401 0.0.355 0.475 0.다국어-Ko(우리) 3.8B 0.432 0.526 0.다국어-Ko(우리) 5.8B KoGPT 6.0B XGLM 다국어-Ko(우리) 0.606 0.688 0.728 0.0.404 0.679 0.626 0.551 0.0.404 0.486 0.795 0.732 0.0.436 0.570 0.519 0.524 0.0.451 0.598 0.550 0.520 0.7.5B 0.446 0.332 0.332 0.332 0.358 0.447 0.396 0.12.8B 0.482 0.604 0.629 0.645 0.912 0.902 0.934 0.0.0.884 0.881 0.0.894 0.929 0.표 4: BoolQ 및 SentiNeg 작업의 성능(F1 점수) 비교 결과. 0.0.0.0.0.0.0.0.0.0.0.0.0-샷 COPA(F1) 5샷 10샷 skt/ko-gpt-trinity-1.2B-v0.facebook/xglm-7.5B -EleutherAI/polyglot-ko-3.8b EleutherAI/polyglot-ko-12.8b 0.0.0.0.0.0.0.0.0.50-샷 0샷 kakaobrain/kogpt EleutherAI/polyglot-ko-1.3b EleutherAI/polyglot-ko-5.8b HellaSwag(F1) 5샷 10샷 skt/ko-gpt-trinity-1.2B-v0.facebook/xglm-7.5B EleutherAI/polyglot-ko-3.8b EleutherAI/polyglot-ko-12.8b SentiNeg (F1)0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0-샷 50샷 0샷 5샷 10샷 skt/ko-gpt-trinity-1.2B-v0.kakaobrain/kogpt facebook/xglm-7.5B EleutherAI/polyglot-ko-1.3b EleutherAI/polyglot-ko-3.8b -EleutherAI/polyglot-ko-5.8b -EleutherAI/polyglot-ko-12.8b 50샷 kakaobrain/kogpt EleutherAI/polyglot-ko-1.3b -EleutherAI/polyglot-ko-5.8b BoolQ (F1) 5샷 10샷 50-shot skt/ko-gpt-trinity-1.2B-v0.kakaobrain/kogpt facebook/xglm-7.5B -EleutherAI/polyglot-ko-3.8b EleutherAI/polyglot-ko-1.3b EleutherAI/polyglot-ko-5.8b EleutherAI/polyglot-ko-12.8b 그림 1: KOBEST 데이터 세트를 사용한 COPA(왼쪽 위), HellaSwag(오른쪽 위), SentiNeg(왼쪽 아래), BoolQ(오른쪽 아래) 작업의 성능 지표. 모든 지표는 F1 점수를 사용하여 측정되었습니다. 성능(F1-점수) 0.0.0.70.0.5COPA HellaSwag BoolQ SentiNegCompute(1021 FLOPS) 그림 2: 각 작업에서 Polyglot-Ko 모델의 5샷 성능은 계산이 증가함에 따라 성능이 향상된다는 명확한 추세를 보여줍니다. COPA 작업에서. 이는 모델의 뛰어난 성능을 보여주고 이해 및 추론 작업에서의 효과성을 강화합니다. 4.2 HellaSwag 표 3의 오른쪽 부분은 한국어에 대한 HellaSwag 작업에서 다양한 언어 모델의 성능을 나타냅니다. 이 표에는 모델 이름, 각 모델의 매개변수 수, KOBEST 데이터 세트를 사용하여 HellaSwag 작업에서 다양한 샷 수에서 각각의 성능이 포함되어 있습니다. 사용된 평가 지표는 F1 점수입니다. 표에 따르면, 우리의 12.8B 모델은 5샷, 10샷, 50샷 시나리오에서 각각 0.6306, 0.6098, 0.6118의 F1 점수로 가장 높은 점수를 받았습니다. 그러나 0샷 시나리오에서는 kakaobrain의 kogpt 모델이 0.5590의 가장 높은 점수를 받았습니다. 전반적으로, 우리의 12.8B 모델은 한국어에 대한 HellaSwag 과제에서 나열된 모델 중 가장 좋은 성능을 보여줍니다. 이러한 결과는 HellaSwag 과제의 맥락 내에서 일관된 응답을 이해하고 생성하는 데 있어 우리의 12.8B 모델이 효과적임을 강조합니다. 대부분의 샷 시나리오에서 뛰어난 성능을 보인 것은 복잡한 언어 이해 과제를 처리하고 맥락에 맞는 응답을 생성하는 능력을 보여줍니다. 4.3 BoolQ 부울 질문에 답하는 데 초점을 맞춘 BoolQ 과제에서 결과에 대한 철저한 분석을 통해 우리의 모델이 다른 모델보다 우수한 성과를 보였다는 것을 보여줍니다. 구체적으로, 가장 큰 모델인 Polyglotko-12.8B가 가장 높은 F1 점수를 달성했습니다. 이는 이 모델이 부울 질문에 대한 답변을 예측하는 데 뛰어난 정확도를 가지고 있음을 나타냅니다. 반대로, SKT의 ko-gpt-trinity 모델은 모든 프롬프트 번호에서 비교적 낮은 F1 점수를 보인 반면, Facebook의 XGLM 모델은 지속적으로 성과가 낮았습니다. 결과적으로, 저희 모델은 BoolQ 작업에서 강력한 성능을 보여줍니다. 4.4 SentiNeg SentiNeg 작업 결과도 표 4의 왼쪽 부분에 나와 있습니다. 이 작업은 부정 감지를 위한 감정 분석에 중점을 둡니다. 결과에 대한 포괄적인 분석 결과, 저희 모델이 우수한 성능을 보였다는 것을 알 수 있습니다. 특히, 저희 12.8B 모델은 가장 높은 F 점수를 달성하여 부정 감정을 정확하게 감지하는 뛰어난 능력을 나타냅니다. SKT의 ko-gpt-trinity 모델도 일관된 개선을 보인 반면, kakaobrain의 KoGPT 모델은 프롬프트 번호에 따라 F1 점수가 약간 증가하거나 감소하면서 다양한 성능을 보였습니다. 그러나 Facebook의 XGLM 모델은 SentiNeg 작업에서 지속적으로 낮은 성능을 보였습니다. 나아가 조사하는 동안 이 작업에 사용된 기본 프롬프트가 특히 제로샷 성능에서 상당한 불안정성을 초래한다는 것을 발견했습니다. 이 문제를 해결하기 위해 수정된 프롬프트를 고안하여 모델 성능이 상당히 향상되었습니다. 자세한 결과는 부록을 참조하세요. 전반적으로, 당사 모델은 SentiNeg 작업에서 강력한 성능을 보여 감정 분석 및 부정 감지에서 효과적임을 보여줍니다. 4가지 작업 모두에서 5샷 평가에서만 PolyglotKo 모델의 성능을 분석하면 그림 2에서 볼 수 있듯이 성능이 컴퓨팅 증가에 따라 향상된다는 것이 분명해집니다(Kaplan et al., 2020). 5 제한 사항 및 면책 조항 Polyglot-Ko는 주로 다음 토큰 예측을 최적화하도록 훈련되었으므로 광범위한 작업에 적합합니다. 그러나 예상치 못한 결과의 가능성을 인정하는 것이 중요합니다. Polyglot-Ko는 통계적으로 가장 가능성 있는 응답을 생성하기 위해 노력하지만 항상 가장 정확하거나 사실적인 답변을 제공하지는 않을 수 있습니다. 모델의 출력에 의존할 때는 주의를 기울이는 것이 중요합니다. 또한 Polyglot-Ko는 사회적으로 용납할 수 없거나 공격적인 콘텐츠를 생성할 수 있다는 점에 유의해야 합니다. 이러한 위험을 완화하기 위해 인간 큐레이터를 구현하거나 다른 필터링 메커니즘을 사용하여 민감하거나 부적절한 콘텐츠를 검열하는 것이 좋습니다. 학습에 사용된 하드웨어와 관련하여 모델은 현재 준비 중인 Polyglot의 차기 버전에 비해 상대적으로 낮은 TFLOPS의 하드웨어 설정에서 학습되었다는 점을 언급하는 것이 중요합니다. 이로 인해 학습 프로세스를 성공적으로 완료하는 데 더 긴 학습 시간과 리소스가 필요했습니다. 또한 실험 중에 데이터 전처리 단계에서 실수를 발견했습니다. 구체적으로, 데이터에서 줄바꿈이 잘못 제거되어 문서 구조가 손실되었습니다. 이로 인해 모델 학습 프로세스 중에 일부 정보가 손실되었을 가능성이 있습니다. 문서 구조를 보존하고 정보 손실을 최소화하기 위해 향후 반복 작업에서 이 문제를 해결하는 것이 중요합니다. 이러한 고려 사항은 지속적으로 교육 프로세스를 개선하고 발생하는 제한이나 오류를 해결하는 것의 중요성을 강조합니다. 이를 통해 광범위한 작업과 애플리케이션에 대한 Polyglot-Ko의 성능과 안정성을 향상시킬 수 있습니다. 6
--- CONCLUSION ---
현재, 우리는 Polyglot 한국어 언어 모델의 새로운 버전을 훈련하는 데 적극적으로 노력하고 있습니다. 우리의 목표는 궁극적으로 40B 매개변수에 도달할 수 있도록 용량을 확장하는 것입니다. 이 프로세스는 우리가 모델의 성능과 역량을 향상시키기 위해 노력하면서 상당한 시행착오를 거쳤습니다. 한국어 언어 모델을 개발하는 데 있어서의 우리의 경험과 전문성을 바탕으로, 우리는 또한 두 가지 유형의 다국어 모델을 만드는 데 착수했습니다. 첫 번째 유형은 한국어, 중국어, 일본어, 인도네시아어, 말레이어, 베트남어, 태국어, 영어를 포함하는 동아시아 모델입니다. 이 모델은 동아시아 지역 국가의 언어적 요구를 충족하는 것을 목표로 합니다. 두 번째 유형은 스페인어, 포르투갈어, 프랑스어, 루마니아어, 이탈리아어를 통합한 로망스어 모델입니다. 이 모델은 로망스어 사용 국가의 언어적 요구 사항을 지원하도록 설계되었습니다. 이러한 다국어 모델을 개발함으로써, 우리는 전 세계적으로 언어 모델 기술에 대한 접근성을 민주화하고 촉진하는 것을 목표로 합니다. 우리는 이것이 다양한 국가의 연구와 학문의 발전에 기여하여 사용자가 다양한 응용 프로그램과 언어적 맥락에서 언어 모델의 힘을 활용할 수 있게 할 것이라고 믿습니다. 우리는 이러한 모델이 전 세계의 연구자, 실무자 및 언어 애호가에게 가져올 수 있는 잠재적 영향과 이점에 대해 기대하고 있습니다. 감사의 말 우리는 이 프로젝트의 성공적인 실행에 중요한 컴퓨팅 리소스를 아낌없이 제공해 준 Stability AI에 감사드립니다. 그들의 지원과 인프라는 우리 모델을 훈련하고 평가하는 데 필수적이었습니다. 또한, 우리는 대규모 한국어 데이터 세트를 제공하는 데 귀중한 기여를 해준 TUNiB에 감사를 표하고 싶습니다. 이 데이터 세트는 우리 언어 모델의 개발과 훈련에 중추적인 역할을 했으며, 그들의 협력과 파트너십에 감사드립니다. 마지막으로, 우리는 우리 작업의 질과 명확성을 크게 향상시킨 논문에 대한 귀중한 피드백을 해준 Stella Biderman에게 감사드리고 싶습니다. 참고문헌 Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, Wang Phil, Samuel Weinbach. 2021. GPT-NeoX: PyTorch에서의 대규모 자기회귀 언어 모델링. 로한 아닐, 앤드류 M. 다이, 오르한 피라트, 멜빈 존슨, 드미트리 레피킨, 알렉상드르 파소스, 시아막 샤케리, 에마누엘 타로파, 페이지 베일리, 지펭 첸, 에릭 추, 조나단 H. 클라크, 로랑 엘 샤페이, 얀핑 황, 캐시 마이어-헬스턴, 가우라브 미슈라, 에리카 모레이라, 마크 오머닉, 케빈 로빈슨, 세바스찬 루더, 이 타이, 케판 샤오, 위안중 쉬, 유징 장, 구스타보 에르난데스 아브레고, 준완 안, 제이콥 오스틴, 폴 바람, 얀 보타, 제임스 브래드버리, 시다르타 브라흐마, 케빈 브룩스, 미셸 카타스타, 용 청, 콜린 체리, 크리스토퍼 A. 쇼켓-추, 아칸샤 초우데리, 클레멘트 크레피, 샤치 데이브, 모스타파 데흐가니, 수니파 Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, 임현택, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant 미스라, 메이삼 무살렘, 재커리 Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov 및 Yonghui Wu. 2023. Palm 2 기술 보고서. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O&#39;Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal. 2023. Pythia: 훈련 및 확장을 통해 대규모 언어 모델을 분석하기 위한 제품군. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach. 2022. Gpt-neox-20b: 오픈소스 자기회귀 언어 모델. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel. 2022. Palm: 경로로 언어 모델링 확장. Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. 2020. Electra: 생성기가 아닌 판별자로 텍스트 인코더 사전 학습. Together Computer. 2023. Redpajama: 라마 학습 데이터 세트를 재생성하는 오픈 소스 레시피. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: 언어 이해를 위한 딥 양방향 트랜스포머의 사전 학습. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. Few-Shot 언어 모델 평가를 위한 프레임워크. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. 신경 언어 모델을 위한 스케일링 법칙. Dohyeong Kim, Myeongjun Jang, Deuk Sin Kwon, and Eric Davis. 2022. Kobest: Korean balanced evaluation of significant tasks. 김일두, 한건수, 함지연, 백운혁. 2021. Kogpt: Kakaobrain korean(hangul) generative pre-trained transformer. https://github. com/kakaobrain/kogpt. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O&#39;Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li. 2022. 다국어 모델을 사용한 Few-shot learning. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: 견고하게 최적화된 bert 사전 학습 접근법. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: 176b매개변수 오픈 액세스 다국어 언어 모델. arXiv 사전 인쇄본 arXiv:2211.05100. Mosaic ML NLP 팀. 2023. mpt-7b 소개: 오픈 소스, ly 사용 가능한 llms를 위한 새로운 표준. 액세스: 2023-03-28. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer. 2022. Opt: 개방형 사전 학습된 변환기 언어 모델. SentiNeg 과제를 위한 프롬프트 수정 SentiNeg의 경우 KOBEST 논문에서 사용된 프롬프트가 간단한 분류 과제를 채택한 것을 관찰했습니다. 그러나 질문 범위가 광범위하고 종종 모호함을 보였습니다. 결과적으로 결과를 얻기 위해 프롬프트를 임의로 수정했습니다. 이러한 프롬프트 수정의 결과는 표 5의 왼쪽 부분과 그림 3a에 나와 있습니다. 수정된 프롬프트로 달성한 점수가 원래 프롬프트로 달성한 점수보다 평균적으로 상당히 높다는 것이 결과에서 분명합니다. 이는 SentiNeg 작업에서 더 나은 성과를 달성하는 데 있어 신속한 설계와 사용자 정의의 중요성을 보여줍니다.B WiC 작업의 결과 WiC 작업의 결과는 표 5의 오른쪽 부분에 표시되고 그림 3b에 시각화되어 있습니다.특히, 모든 모델이 이 작업에서 무작위 성능을 보였습니다.정확도 메트릭을 활용했는데, 특히 무작위 성능에 대한 성능을 보다 간단하게 평가할 수 있기 때문입니다.모든 모델에서 일관된 무작위 성능은 WiC 작업에 대한 정확한 예측을 하는 데 어려움이 있었음을 나타냅니다.따라서 이 작업을 효과적으로 처리할 수 있는 모델의 역량을 향상시키기 위해 추가 조사와 개선이 필요합니다. 수정된 SentiNeg(n=샷) WiC 모델 매개변수 n=0 n=5 n=10 n=n=0 n=5 n=10 n=Ko-GPT-Trinity 1.2B koGPT 6B 0.927 0.0.767 0.699 0.713 0.836 0.0.0.492 0.479 0.0.0.0.495 0.479 0.XGLM 7.5B 0.835 0.0.0.Polyglot-Ko(우리) Polyglot-Ko(우리) Polyglot-Ko(우리) 5.8B Polyglot-Ko(우리) 12.8B 1.3B 3.8B 0.0.889 0.850 0.887 0.907 0.489 0.486 0.506 0.0.942 0.894 0.906 0.952 0.489 0.499 0.491 0.0.878 0.927 0.906 0.960 0.484 0.494 0.480 0.0.893 0.985 0.982 0.982 0.493 0.494 0.488 0.0.490 0.498 0.표 5: 수정된 프롬프트를 사용한 SentiNeg 작업과 WiC 작업의 성능을 비교한 결과, SentiNeg의 F1 점수와 WiC의 정확도는 무작위 성능을 보여줍니다. 센티네그(F1) 와이씨(F1) 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0-샷 5샷 10샷 -카카오브레인/코그프 50샷 0샷 ―skt/ko-gpt-트리니티-1.2B-v0.facebook/xglm-7.5B 엘루서AI/폴리글로트-코-3.8b —엘루서AI/폴리글로트-코-12.8b -엘루서AI/폴리글로트-코-1.3b 엘루서AI/폴리글로트-코-5.8b 5샷 10샷 50샷 skt/ko-gpt-트리니티-1.2B-v0.kakaobrain/kogpt facebook/xglm-7.5B EleutherAI/polyglot-ko-3.8b EleutherAI/polyglot-ko-12.8b EleutherAI/polyglot-ko-1.3b EleutherAI/polyglot-ko-5.8b ----- 랜덤 그림 3: KOBEST 데이터 세트를 사용하여 수정된 프롬프트(왼쪽)와 WiC 작업(오른쪽)을 적용한 SentiNeg 작업의 성능 지표. SentiNeg의 F1 점수와 WiC의 정확도는 랜덤 성능을 보여줍니다.
