--- ABSTRACT ---
오디오 생성의 최근 발전은 대규모 딥 러닝 모델과 광범위한 데이터 세트의 진화에 의해 촉진되었습니다. 그러나 비디오-오디오(V2A) 생성 작업은 주로 고차원 시각 및 청각 데이터 간의 복잡한 관계와 시간 동기화와 관련된 과제 때문에 여전히 어려운 과제입니다. 이 연구에서는 언어 모델링 패러다임을 기반으로 하는 오픈 도메인 V2A 생성 시스템인 FoleyGen을 소개합니다. FoleyGen은 파형과 이산 토큰 간의 양방향 변환을 위해 기성형 신경 오디오 코덱을 활용합니다. 오디오 토큰 생성은 시각적 인코더에서 추출한 시각적 특징에 따라 조건화된 단일 Transformer 모델에 의해 용이해집니다. V2A 생성에서 널리 퍼진 문제는 생성된 오디오와 비디오의 시각적 동작의 정렬이 맞지 않는 것입니다. 이를 해결하기 위해 세 가지 새로운 시각적 주의 메커니즘을 탐구합니다. 또한 단일 모달 또는 다중 모달 작업에 대해 각각 사전 학습된 여러 시각적 인코더에 대한 철저한 평가를 수행합니다. VGGSound 데이터 세트에 대한 실험 결과는 제안된 FoleyGen이 모든 객관적 지표와 인간 평가에서 이전 시스템보다 성능이 우수함을 보여줍니다. 색인 용어― 사운드 생성, 시청각 학습, 비디오-오디오 생성, 멀티모달 학습 1.
--- METHOD ---
s는 오픈 도메인 비디오로 일반화하는 데 실패합니다. 그러나 최근의 발전은 오픈 도메인, 시각적으로 안내되는 오디오 생성에 대한 관심이 증가하고 있음을 나타냅니다. SpecVQGAN [9]과 IM2WAV [10]는 모두 언어 모델링 방법을 사용하여 Transformer 모델을 활용하여 벡터 양자화 변형 자동 인코더(VQ-VAE)로 인코딩된 시각적 특징과 이산 오디오 토큰의 공동 분포를 캡처합니다. SpecVQGAN에서 VQ-VAE는 스펙트로그램에서 특별히 작동한 후 신경 보코더를 사용하여 생성된 스펙트로그램을 다시 파형으로 변환합니다. 반면 IM2WAV는 파형에서 직접 작동하여 VQ-VAE의 잠재 공간을 두 수준으로 분할하고 이중 Transformer 모델을 활용하여 해당 분포를 모델링합니다. 또한 Diff-Foley [11]는 대조적 오디오-비주얼 사전 학습(CAVP) 표현을 조건으로 하는 잠재 확산 방법을 도입합니다. AudioGen [2] 및 MusicGen [4]의 선구적인 작업에서 영감을 얻어 언어 모델링 패러다임을 채택한 비디오-오디오 생성 프레임워크인 FoleyGen을 소개합니다.FoleyGen의 개요는 그림 1에 나와 있습니다.특히, 우리 시스템은 세 가지 주요 구성 요소를 포함합니다.오디오와 이산 토큰 간 양방향 변환을 위한 신경 오디오 코덱-EnCodec [12], 시각적 특징을 추출하기 위한 시각적 인코더, 시각적 맥락에 따라 오디오 토큰을 생성하는 Transformer 모델입니다.SpecVQGAN [9]과 달리 EnCodec을 도입하면 더 나은 재구성 품질이 제공되고 스펙트로그램-파형 변환 프로세스 중에 종종 발생하는 충실도 손실이 완화됩니다.또한 여러 Transformer 모델 IM2WAV [10]를 배포할 필요가 없습니다.V2A 생성에서 흔한 문제는 생성된 오디오와 비디오의 눈에 보이는 동작의 정렬이 맞지 않는 것입니다.눈에 보이는 동작과 해당 오디오 이벤트 간의 시간적 정렬을 향상시키기 위해 세 가지 다른 시각적 주의 메커니즘을 제안하고 탐색합니다. 또한, 우리는 단일 모달 및 다중 모달 작업 모두에 대해 사전 학습된 다양한 시각 인코더에 대한 철저한 평가를 수행합니다.
--- EXPERIMENT ---
VGGSound 데이터 세트에 대한 모든 결과는 제안하는 FoleyGen이 모든 객관적 지표와 인간 평가에서 이전 시스템보다 성능이 우수함을 보여줍니다. 색인 용어― 사운드 생성, 시청각 학습, 비디오-오디오 생성, 멀티모달 학습 1. 서론 gen최근 몇 년 동안 대규모 딥 러닝 모델과 데이터 세트의 발전에 힘입어 오디오 분야에서 놀라운 혁신이 이루어졌습니다.텍스트-오디오[1, 2] 및 텍스트-음악[3, 4] 생성 분야에서 큰 성과를 거두었음에도 불구하고 비디오-오디오(V2A) 생성은 고유한 과제로 인해 유망하지만 탐구가 부족한 분야로 뒤처져 있습니다.비디오-오디오 생성은 주어진 시각 신호에 대해 일치하는 사운드스케이프를 생성하는 작업으로, 시각 데이터를 구문 분석하고 소리를 내는 물체를 식별한 다음 해당 소리를 만들어야 합니다. V2A 모델은 다양한 응용 분야에서 유용합니다.예를 들어, 컴퓨터 폴리 아티스트로서 영화의 사운드를 생성하거나, 가상 현실 응용 프로그램에서 몰입형 경험을 향상시키거나, 시각 장애인의 공간 인식을 개선하는 데 도움이 됩니다.* Meta에서 인턴십을 하는 동안 수행한 작업.생성된 파형 비디오 프레임 인코덱 디코더 비주얼 인코더 사전 학습 및 동결된 변압기 디코더 인코덱 인코더 참조 파형 그림 1. FoleyGen 시스템 개요.점선 블록은 파형을 개별 토큰으로 변환하는 EnCodec 인코더를 보여주며, 이는 학습 중에만 사용됩니다.정확하고 사실적인 V2A 생성을 달성하려면 몇 가지 과제가 있습니다.첫째, 시각 및 청각 데이터를 동시에 해석하는 것은 각각의 고차원적 특성으로 인해 복잡합니다.둘째, 실제 비디오에는 종종 시각적으로 무관한 사운드가 포함되어 있어 소리를 내는 객체가 보이는 프레임에 없습니다.이러한 불일치로 인해 시간적으로 동기화된 오디오를 생성하는 것이 매우 어렵습니다. 마지막으로, 단일 객체는 다양한 환경과의 상호 작용에 따라 다양한 범위의 소리를 낼 수 있어 이 작업이 더욱 복잡해집니다. V2A 생성의 초기 노력은 주로 제한된 시각적 맥락과 제한된 사운드 클래스 세트에 초점을 맞춰 문제를 단순화했습니다[5, 6, 7]. 이러한 접근 방식은 일반적으로 클래스 인식 전략[6]을 활용하거나 심지어 별개의 사운드 범주에 대해 별도의 모델을 학습했습니다[7, 8]. 결과적으로 이러한 방법은 오픈 도메인 비디오로 일반화하는 데 실패합니다. 그러나 최근의 발전은 오픈 도메인, 시각적으로 안내되는 오디오 생성에 대한 관심이 증가하고 있음을 나타냅니다. SpecVQGAN[9]과 IM2WAV[10]는 모두 언어 모델링 방법을 사용하여 Transformer 모델을 활용하여 벡터 양자화 변형 자동 인코더(VQ-VAE)로 인코딩된 시각적 특징과 이산 오디오 토큰의 공동 분포를 캡처합니다. SpecVQGAN에서 VQ-VAE는 스펙트로그램에서 특별히 작동한 다음 신경 보코더를 사용하여 생성된 스펙트로그램을 다시 파형으로 변환합니다. 이와 대조적으로 IM2WAV는 파형에서 직접 작동하여 VQ-VAE의 잠재 공간을 두 수준으로 분할하고 이중 Transformer 모델을 활용하여 각각의 분포를 모델링합니다.또한 Diff-Foley[11]는 대조적 오디오-비주얼 사전 학습(CAVP) 표현을 조건으로 하는 잠재 확산 방법을 도입합니다.AudioGen[2]과 MusicGen[4]의 선구적인 작업에서 영감을 받아 언어 모델링 패러다임을 채택한 비디오-오디오 생성 프레임워크인 FoleyGen을 소개합니다.FoleyGen에 대한 개요는 그림 1에 나와 있습니다.특히, 우리 시스템은 세 가지 주요 구성 요소를 포함합니다.오디오와 이산 토큰 간 양방향 변환을 위한 신경 오디오 코덱-EnCodec[12], 시각적 특징을 추출하기 위한 시각적 인코더, 시각적 맥락을 조건으로 오디오 토큰을 생성하는 Transformer 모델입니다. SpecVQGAN [9]과 달리 EnCodec을 도입하면 재구성 품질이 더 좋아지고 스펙트로그램-파형 변환 프로세스 중에 종종 발생하는 충실도 손실이 완화됩니다.또한 여러 Transformer 모델 IM2WAV [10]를 배포할 필요가 없습니다.V2A 생성에서 흔히 발생하는 문제는 생성된 오디오와 비디오의 시각적 동작이 일치하지 않는 것입니다.시각적 동작과 해당 오디오 이벤트 간의 시간적 일치를 향상시키기 위해 세 가지 다른 시각적 주의 메커니즘을 제안하고 탐색합니다.또한 단일 모달 및 다중 모달 작업 모두에서 사전 학습된 다양한 시각적 인코더에 대한 철저한 평가를 수행합니다.실험 결과에 따르면 제안된 FoleyGen은 모든 객관적 지표와 인간 평가에서 이전 시스템보다 성능이 우수합니다.2.1. 신경 오디오 코덱 시간 영역 파형의 분포를 모델링하는 것은 주로 고차원 및 긴 특성으로 인해 상당한 과제와 계산 비효율성을 나타냅니다. 오디오 생성 시스템에서 자동 인코더는 일반적으로 오디오 파형을 연속적[1]이거나 불연속적[2]일 수 있는 잠재 공간으로 인코딩하는 데 사용됩니다. AudioLM[13] 및 AudioGen[2]에서 영감을 받아 최첨단 신경 오디오 코덱[12]인 EnCodec을 실험에 채택했습니다. EnCodec은 오디오 파형을 잠재 벡터로 압축하는 인코더, 이러한 잠재 벡터를 불연속 토큰으로 변환하는 잔여 벡터 양자화기(RVQ), 이러한 토큰을 다시 오디오 파형으로 변환하는 대칭 디코더로 구성됩니다. 오디오 클립 a € Rt×fs가 주어지면(여기서 t는 지속 시간이고 f¸는 샘플링 속도), 인코더는 먼저 a를 잠재 표현 z = RL×d로 압축합니다. 여기서 d는 잠재 벡터의 차원이고 L은 다운 샘플링된 시간 단계의 수입니다. Ng 코드북이 있는 RVQ는 인코딩된 잠재 벡터를 N₁ × L 이산 토큰으로 변환합니다. 이산 오디오 토큰은 언어 모델링 단계에서 오디오의 표현으로 추가로 사용됩니다. EnCodec 디코더는 생성된 오디오 토큰을 파형으로 변환합니다. EnCodec 인코더는 학습 중에만 사용됩니다. EnCodec 논문에 설명된 것과 동일한 하이퍼파라미터 설정을 준수합니다. 자세한 내용은 [12]를 참조하십시오. EnCodec을 채택하면 높은 재구성 품질을 유지하면서도 높은 압축률을 제공합니다. 스펙트로그램에서 작동하는 다른 자동 인코더[9, 11]와 달리 EnCodec은 추가 보코더가 필요 없으므로 생성된 스펙트로그램을 다시 파형으로 변환할 때 발생할 수 있는 잠재적인 충실도 손실을 없앱니다. 2. 제안된 방법 비디오 클립이 주어지면 비디오-오디오 생성 시스템은 의미적으로 일관되고 시간적으로 수반되는 비디오 콘텐츠와 일치하는 오디오 클립을 생성하도록 설계되었습니다. 비디오-오디오 생성 프로세스는 H: va로 공식화할 수 있는데, 여기서 v는 비디오 입력의 프레임을 나타내고 a는 생성된 오디오 파형에 해당합니다. 그림 1은 제안하는 시스템인 FoleyGen의 아키텍처를 보여줍니다. FoleyGen은 세 가지 주요 구성 요소로 구성됩니다. 파형과 개별 토큰 간의 양방향 변환을 위한 신경 오디오 코덱, 비디오 프레임에서 피처 추출을 위한 시각적 인코더, 추출된 시각적 피처를 기반으로 개별 오디오 토큰을 생성하는 오디오 언어 디코더입니다. 이 섹션에서는 먼저 FoleyGen의 각 주요 구성 요소에 대한 자세한 소개를 제공합니다. 시각적 입력과 생성된 오디오의 시간적 정렬을 개선하기 위해 이 섹션의 마지막에 설명되어 있는 다양한 시각적 주의 메커니즘을 사용할 것을 제안합니다. 2.2. 시각적 인코더 주어진 시각적 입력 v € RT×C×H×W, 여기서 T는 프레임 수(단일 이미지의 경우 1일 수 있음)를 나타내고, C는 채널 수이며, H와 W는 각각 시각적 입력의 높이와 너비를 나타낼 때, 시각적 인코더는 언어 디코더의 차원 수인 D를 갖는 피처 벡터 FЄRTXD를 생성합니다. 추출된 시각적 피처 F의 품질은 의미적으로 일관되고 시간적으로 정렬된 오디오 생성을 달성하는 데 중요합니다. 최적이 아닌 시각적 인코더는 중요한 시각적 단서를 손실하여 원래 비디오 콘텐츠와의 충실도나 일치성이 부족한 오디오 출력을 초래할 수 있습니다. 다양한 시각적 인코더의 효능을 알아보기 위해 단일 모달 및 다중 모달 작업으로 훈련된 다양한 인기 있는 시각적 인코더를 사용하여 일련의 실험을 수행했습니다. 이러한 시각적 인코더에는 ViT[14], CLIP[15], ImageBind[16] 및 VideoMAE[17]가 있습니다. 참석 허용 참석 금지 프레임별 시각적 주의: 보다 제한적인 접근 방식에서 모델의 주의가 생성 중에 동시 시간 프레임의 시각적 특징에 엄격하게 국한되는 &quot;프레임별 시각적 주의&quot;를 도입합니다. 이 엄격한 주의 메커니즘은 모델이 현재 시각적 맥락에 따라서만 오디오를 생성하도록 합니다. 모든 프레임 시각적 주의 인과적 시각적 주의 프레임별 시각적 주의 그림 2. 세 가지 시각적 주의 메커니즘 개요. 단순화를 위해 여기서는 프레임 속도가 2Hz인 2개의 시각적 특징 &#39;V&#39;와 4개의 오디오 토큰 &#39;A&#39;가 있다고 가정합니다. 2.3. 오디오 언어 디코더 오디오는 EnCodec [12]으로 인코딩된 후 이산 토큰으로 표현되므로 비디오-오디오 생성 문제는 조건부 언어 모델링 작업으로 공식화할 수 있습니다. 조건부 정보로 추출된 시각적 특징이 주어지면 Transformer 모델 [18]을 사용하여 이산 오디오 토큰을 자기 회귀적으로 생성합니다. Transformer 모델은 디코더 전용이며 교차 주의 블록을 생략합니다. 시각적 특징은 컨디셔닝을 위해 오디오 토큰 시퀀스에 추가됩니다.EnCodec의 잔여 벡터 양자화로 인해 각 타임스텝은 잔여 코드북을 사용하여 다중 스트림 토큰을 인코딩합니다.이러한 다중 스트림 토큰을 효과적으로 캡처하기 위해 MusicGen [4]에서 도입한 지연 패턴을 채택합니다.이 접근 방식은 스트림 간 오프셋을 유지하면서 다중 오디오 토큰 스트림을 병렬로 모델링합니다.지연 패턴을 통합하면 높은 효율성이 보장되고 평평한 패턴에서 토큰을 예측할 필요가 없습니다.또한 다중 Transformer 모델 [13, 10]의 요구 사항을 회피합니다.2.4. 시각적 주의 메커니즘 비디오와 시간적으로 일치하는 오디오를 생성하는 것은 상당한 과제를 제시합니다.이를 해결하기 위해 세 가지 고유한 시각적 주의 메커니즘을 소개하고 탐색합니다.그림 2는 세 가지 주의 메커니즘의 개요를 보여줍니다.모든 프레임 시각적 주의: 기준 설정에서 Transformer 디코더에 내재된 기본 인과적 주의 메커니즘을 사용합니다. 시각적 특징이 이산 토큰에 미리 추가되어 있기 때문에 생성 과정에서 오디오 토큰은 모든 시각적 특징에 주의를 기울일 수 있습니다. 이는 광범위한 맥락을 제공하지만 시각적 정보가 너무 많아서 사운드 생성의 정확한 타이밍과 관련하여 모델을 혼란스럽게 할 수 있습니다. 인과적 시각적 주의: 대책으로, 오디오 토큰 생성 중에 모델이 현재 타임스텝에 선행하고 일치하는 시각적 프레임에만 주의를 기울이도록 제한되는 &quot;인과적&quot; 접근 방식을 조사합니다. 이 순차적 주의는 모델이 오디오를 시각적 단서와 더 잘 동기화하는 데 도움이 될 수 있습니다. 3.1. 데이터 세트 3. 실험 우리는 오픈 도메인 시각적으로 안내되는 오디오 생성을 목표로 합니다. 따라서 다양한 콘텐츠가 있는 YouTube에서 가져온 약 200,000개의 10초 비디오 클립이 포함된 VGGSound [19] 데이터 세트를 사용합니다. 일부 비디오 클립은 더 이상 다운로드할 수 없으므로 우리 버전은 훈련 세트에 159,318개의 샘플과 테스트 세트에 13,161개의 샘플을 포함합니다. 3.2. 구현 세부 정보 데이터 세트의 모든 오디오 클립은 16kHz 모노포닉 오디오로 샘플링됩니다. EnCodec의 경우 인코더에서 동일한 다운샘플링 스트라이드 [2, 4, 5, 8]를 따르며, 프레임 속도가 50Hz입니다. 코드북 크기가 2048인 코드북 4개를 사용합니다. 비디오 데이터의 경우 초당 1프레임을 샘플링하고 시각적 인코더에서 사전 처리 프로토콜(예: 크기 조정, 정규화)을 따릅니다. 시각적 인코더 다음에 선형 레이어를 사용하여 시각적 특징을 Transformer 모델의 동일한 차원으로 투영합니다. Transformer 디코더는 16개 헤드와 1024 차원이 있는 24개 레이어로 구성됩니다. 메모리 효율적인 플래시 어텐션[20]을 사용하여 속도와 메모리 사용량을 개선합니다. = 모델은 배치 크기가 256인 20k 단계로 학습됩니다. ẞ1 0.9, 62 = 0.95, 가중치 감소가 0.1인 AdamW 옵티마이저를 사용합니다. 학습률은 1 × 10−로 설정되고 처음 4k 단계에서 워밍업을 사용합니다. 또한 분류기 없는 안내[21]도 사용하여 시각적 고착을 개선합니다. 훈련하는 동안 시각적 조건은 확률 0.1로 삭제됩니다(즉, 널 벡터로 대체). 추론하는 동안 분류기 없는 안내 척도 3.0이 사용되고 k를 256으로 설정한 상위 k 샘플링을 사용합니다. 3.3 평가 지표 FoleyGen의 성능을 평가하기 위해 객관적 평가와 주관적 평가를 모두 수행합니다. 객관적 평가를 위해 Fréchet Audio Distance(FAD)[22], KullbackLeibler Divergence(KLD) 및 ImageBind(IB) 점수[16]를 사용합니다. FAD는 생성된 오디오 클립과 참조 오디오 클립의 특징 간 분포 거리를 계산합니다. 여기서 특징은 AudioSet에서 훈련된 VGGish 네트워크[23]를 사용하여 계산됩니다. KLD는 사전 훈련된 PaSST 모델[24]에 의해 계산된 대상 오디오와 생성된 오디오의 레이블 분포를 비교합니다. FAD는 오디오 품질에 대한 인간의 지각과 강력한 상관 관계를 보이는 반면 KLD는 주로 표 1. VGGSound 데이터 세트의 실험 결과. 여기에서는 모든 프레임 시각적 주의를 사용합니다. 방법 시각적 인코더 FAD↓ KL↓ IB (%) ↑ OVR (%) ↑ REL (%) ↑ SpecVQGAN [9] ResNet-6.3.5.5.IM2WAV [10] CLIP 6.2.16.31.Ours CLIP 1.2.26.77.63.표 2. 다양한 시각적 인코더를 사용하여 학습한 모델을 사용한 VGGSound 데이터 세트에 대한 실험 결과. IB(%) ↑ 시각적 인코더 FAD KL↓ CLIP 1.2.26.ᏙᎥᎢ 1.2.23.ImageBind 1.2.26.VideoMAE 2.3.17.표 3. 다양한 주의 메커니즘을 사용하여 학습한 모델을 사용한 VGGSound 데이터 세트에 대한 실험 결과. 사용된 시각적 인코더는 CLIP입니다. ALI (%) ↑ 주의 FAD↓↓ KL↓ 모든 프레임 1.65 2. 인과적 2.18 2. IB(%) ↑ OVR (%) ↑ 26.63.55.25.24.14.13.22.31. 프레임별 2.49 2. 녹음에 존재하는 오디오 개념 [2]. 생성된 오디오와 비디오 간의 관련성을 평가하기 위해 ImageBind 모델 [16]을 사용하여 관련성 점수를 계산하는 것을 제안합니다. ImageBind는 6가지 서로 다른 모달리티에 걸쳐 공동 임베딩을 학습하도록 훈련되었으므로 비디오와 생성된 오디오 모두에 대한 임베딩의 코사인 유사성은 이들 간의 의미적 관련성을 포착할 수 있습니다. 주관적인 평가를 위해 인간 청취자에게 서로 다른 모델에서 생성한 샘플을 비교하고 전반적인 품질(OVR), 해당 시각적 입력과의 관련성(REL)을 포함한 특정 기준에 따라 우수한 성능을 입증한 샘플을 식별하도록 요청합니다. 시간적 정렬(ALI)은 주의 메커니즘을 평가할 때 고려됩니다.3.4. 결과 표 1은 우리 연구의 주요 결과를 제시하며, 여기서 우리는 제안된 FoleyGen 시스템을 이전의 두 가지 최첨단 방법인 SpecVQGAN [9] 및 IM2WAV [10]와 벤치마킹했습니다.IM2WAV가 평가 지표로 FAD와 KLD를 사용했기 때문에 우리는 그들의 점수를 직접 채택했습니다.주관적인 평가의 경우, 우리는 사전 학습된 모델을 사용하여 샘플을 생성했습니다.결과를 통해 FoleyGen이 객관적 및 주관적 지표에서 SpecVQGAN과 IM2WAV를 모두 지속적으로 능가한다는 것이 분명합니다.특히 FAD 점수가 현저히 감소했습니다.주관적 평가의 추세는 객관적 지표와 일치합니다.여러 요인이 이러한 개선에 기인할 수 있습니다.첫째, EnCodec을 통합하면 오디오 토큰의 압축률이 높아지고 재구성 품질이 향상됩니다.이 향상된 압축률은 언어 모델에 대한 분포 모델링을 간소화합니다. 둘째, 토큰 생성에서 지연 패턴을 활용하면 여러 Transformer 모델이 필요 없어져 더 뛰어난 성능을 얻을 수 있습니다.표 2는 다양한 시각적 인코더를 사용하여 학습했을 때의 모델 결과를 보여줍니다.다중 모달 작업을 통해 사전 학습된 시각적 인코더(예: CLIP[15] 및 ImageBind[16])는 비슷한 성능을 보이고 단일 모달 작업에서만 학습한 인코더를 능가하는 것을 관찰할 수 있습니다.차별적 작업을 통해 사전 학습된 ViT는 VideoMAE보다 성능이 뛰어납니다.VideoMAE는 자기 감독 학습이 있는 마스크 자동 인코더를 사용하여 학습되므로 다운스트림 작업에 채택할 때 미세 조정이 필요할 수 있습니다.표 3은 다양한 어텐션 메커니즘을 사용하여 달성한 결과를 보여줍니다.모든 프레임 시각적 어텐션은 객관적 지표와 인간 평가 모두에서 다른 두 가지를 현저히 능가했습니다.흥미롭게도 프레임별 어텐션은 객관적 평가에서 뒤처졌지만 인과적 시각적 어텐션과 비교하여 인간 평가에서 향상된 성능을 보였습니다. 그러나 인간의 평가에서 얻은 중요한 통찰력에 따르면 시스템은 여전히 시간적 정렬에 어려움을 겪고 있으며 때로는 비디오 내에서 중요한 동작을 포착하지 못하는 것으로 나타났습니다.
--- CONCLUSION ---
S 이 논문에서 우리는 언어 모델링 패러다임을 따르는 비디오-오디오 생성 모델인 FoleyGen을 소개했습니다. FoleyGen은 양방향 파형-토큰 변환을 위한 EnCodec, 시각적 특징 추출을 위한 시각적 인코더, 조건부 오디오 토큰 생성을 위한 Transformer 디코더를 활용합니다. 우리의 평가는 FoleyGen이 객관적 지표와 인간 평가 모두에서 이전 방법론을 능가한다는 것을 보여줍니다. 우리의 탐구를 통해 멀티모달 작업에서 훈련된 시각적 인코더가 더 뛰어난 성능을 보인다는 것을 관찰했습니다. 오디오-비디오 시간적 정렬을 향상시키기 위해 시각적 주의 메커니즘을 도입했지만, 그것은 여전히 이 분야에서 지속적인 과제로 남아 있습니다. 향후 연구는 비디오-오디오 생성 시스템의 시간적 응집력을 개선하는 데 더 깊이 파고들어야 합니다. 5. 참고문헌 [1] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D Plumbley, &quot;AudiOLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성&quot;, 국제기계학습회의록, 2023. [2] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi, &quot;AudioGen: 텍스트로 안내되는 오디오 생성&quot;, 제11회 학습 표현 국제회의, 2023. [3] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, 및 Mark D. Plumbley, &quot;AudioLDM 2: 자기 감독 사전 학습을 통한 전체적인 오디오 생성 학습,&quot; arXiv 사전 인쇄본 arXiv:2308.05734, 2023. [4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez, &quot;간단하고 제어 가능한 음악 생성,&quot; arXiv 사전 인쇄본 arXiv:2306.05284, 2023. [5] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, William T. Freeman, &quot;시각적으로 표시된 소리,&quot; IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 2016년 6월. [6] Kan Chen, Chuanxi Zhang, Chen Fang, Zhaowen Wang, Trung Bui 및 Ram Nevatia, &quot;지각적으로 최적화된 분류를 통한 시각적으로 표시된 사운드 생성&quot;, 유럽 컴퓨터 비전 워크숍 회의록, 2018. [7] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui 및 Tamara L Berg, &quot;시각적에서 사운드로: 야외에서 비디오를 위한 자연스러운 사운드 생성&quot;, IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 회의록, 2018, 3550-3558쪽. [8] Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang 및 Chuang Gan, &quot;비디오에서 시각적으로 정렬된 사운드 생성&quot;, IEEE 이미지 처리 저널, 제29권, 8292-8302쪽, 2020년. [9] Vladimir Iashin 및 Esa Rahtu, &quot;시각적으로 유도되는 사운드 생성 길들이기&quot;, 영국 머신 비전 컨퍼런스(BMVC), 2021년. [10] Roy Sheffer 및 Yossi Adi, &quot;당신의 진정한 색깔을 듣습니다: 이미지 유도 오디오 생성&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2023, 1-5쪽. [11] Simian Luo, Chuanhao Yan, Chenxu Hu 및 Hang Zhao, &quot;DiffFoley: 잠재 확산 모델을 사용한 동기화된 비디오-오디오 합성,&quot; arXiv 사전 인쇄본 arXiv:2306.17203, 2023. [12] Alexandre Défossez, Jade Copet, Gabriel Synnaeve 및 Yossi Adi, &quot;고충실도 신경 오디오 압축,&quot; arXiv 사전 인쇄본 arXiv:2210.13438, 2022. [13] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi 등, &quot;Audiolm: 오디오 생성을 위한 언어 모델링 접근 방식,&quot; IEEE/ACM Transactions on 오디오, 음성 및 언어 처리, 2023. [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, &quot;이미지는 16xwords의 가치가 있습니다: 대규모 이미지 인식을 위한 변압기&quot;, 국제 학습 표현 컨퍼런스, 2021. [15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al., &quot;자연어 감독에서 이전 가능한 시각적 모델 학습&quot;, 국제 기계 학습 컨퍼런스. PMLR, 2021, 8748-8763쪽. [16] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin 및 Ishan Misra, &quot;ImageBind: 모든 것을 묶을 수 있는 하나의 임베딩 공간&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 2023, pp. 15180-15190. [17] Zhan Tong, Yibing Song, Jue Wang 및 Limin Wang, &quot;VideoMAE: 마스크 자동 인코더는 자기 감독 비디오 사전 학습을 위한 데이터 효율적인 학습기입니다&quot;, Advances in Neural Information Processing Systems, vol. 35, pp. 10078-10093, 2022. [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin, &quot;주의만 있으면 됩니다.&quot;, 신경 정보 처리 시스템의 발전. 2017, vol. 30, Curran Associates, Inc. [19] Honglie Chen, Weidi Xie, Andrea Vedaldi 및 Andrew Zisserman, &quot;VggSound: 대규모 시청각 데이터 세트&quot;, IEEE 국제 음향, 음성 및 신호 처리 회의(ICASSP), 2020, pp. 721–725 [20] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra 및 크리스토퍼 레(Christopher Ré), &quot;FlashAttention: 영어: io 인식을 통한 빠르고 메모리 효율적인 정확한 주의,&quot; 신경 정보 처리 시스템의 발전, 제35권, 16344-16359쪽, 2022. [21] Jonathan Ho 및 Tim Salimans, &quot;분류자 없는 확산 안내,&quot; arXiv 사전 인쇄본 arXiv:2207.12598, 2022. [22] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek 및 Matthew Sharifi, &quot;Fréchet 오디오 거리: 음악 향상 알고리즘을 평가하기 위한 지표,&quot; arXiv 사전 인쇄본 arXiv:1812.08466, 2018. [23] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold 외, &quot;대규모 오디오 분류를 위한 CNN 아키텍처&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP). IEEE, 2017, pp. 131-135. [24] Khaled Koutini, Jan Schlüter, Hamid Eghbal-zadeh 및 Gerhard Widmer, &quot;패치아웃을 사용한 오디오 변압기의 효율적 학습&quot;, Interspeech. 2022, pp. 2753-2757, ISCA.
