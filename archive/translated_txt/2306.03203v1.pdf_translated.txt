--- ABSTRACT ---
코드에서 학습된 대규모 언어 모델은 소프트웨어 개발자의 생산성을 높이는 데 큰 잠재력을 보여주었습니다. 간단한 프로그래밍 문제에서 모델에서 생성된 코드의 기능적 정확성을 평가하기 위해 여러 실행 기반 벤치마크가 제안되었습니다. 그럼에도 불구하고 실행 비용을 고려할 때 복잡한 실제 프로젝트에서 동일한 평가를 수행하는 것은 비용이 많이 듭니다. 반대로 프로그램을 실행하지 않고도 오류를 감지할 수 있는 린터와 같은 정적 분석 도구는 코드 생성 모델을 평가하는 데 잘 탐구되지 않았습니다. 이 연구에서는 추상 구문 트리를 활용하여 Python 코드 완성의 정적 오류를 정량화하는 정적 평가 프레임워크를 제안합니다. 실행 기반 평가와 비교할 때, 우리의 방법은 더 효율적일 뿐만 아니라 야생 코드에도 적용할 수 있습니다. 실험을 위해 오픈 소스 리포에서 코드 컨텍스트를 수집하여 공개 모델을 사용하여 백만 개의 함수 본문을 생성합니다. 정적 분석 결과 언어 모델에서 가장 흔히 발생하는 오류는 정의되지 않은 이름과 사용되지 않은 변수입니다. 광범위한 연구를 통해 샘플링 온도, 모델 크기 및 컨텍스트가 코드 완성의 정적 오류에 미치는 영향도 보여줍니다. 1
--- METHOD ---
더 효율적일 뿐만 아니라 야생의 코드에도 적용할 수 있습니다.
