--- ABSTRACT ---
: 로봇공학을 위한 자기 감독형 감각 운동 사전 훈련 접근법을 제시합니다. RPT라고 하는 저희 모델은 감각 운동 토큰 시퀀스에서 작동하는 Transformer입니다. 카메라 이미지, 고유 감각 로봇 상태 및 동작 시퀀스가 주어지면 시퀀스를 토큰으로 인코딩하고, 하위 집합을 마스크 처리하고, 나머지에서 누락된 내용을 예측하는 모델을 훈련합니다. 로봇이 마스크 처리된 내용을 예측할 수 있다면 행동할 수 있는 물리적 세계의 좋은 모델을 습득했을 것이라고 가정합니다. RPT는 예측을 쉽게 만들고, 더 큰 모델로 확장하고, 실제 로봇에 대한 빠른 추론을 가능하게 하는 잠재적 시각적 표현에서 작동하도록 설계되었습니다. 저희의 접근법을 평가하기 위해, 동작 계획 및 파악 알고리즘을 조합하여 9개월 동안 20,000개의 실제 궤적 데이터 세트를 수집했습니다. 저희는 감각 운동 사전 훈련이 처음부터 훈련하는 것보다 지속적으로 성능이 우수하고, 유리한 확장 속성을 가지고 있으며, 다양한 작업, 환경 및 로봇 간 전송을 가능하게 한다는 것을 발견했습니다. 키워드: 로봇 학습, 자기 감독, 감각 운동, 사전 훈련 1
--- INTRODUCTION ---
지난 몇 년 동안 시각[1, 2, 3, 4]과 언어[5, 6, 7]에서 영감을 받아 로봇공학을 위한 사전 훈련에 대한 관심이 증가했습니다. 예를 들어, 우리는 방대하고 다양한 이미지 컬렉션[8]에 대한 자체 감독 시각적 사전 훈련에서 유망한 결과를 보았습니다. 그러나 로봇 데이터에는 시각적 사전 훈련만으로는 포착하기 어려운 풍부한 감각 및 운동 정보가 포함되어 있습니다. 우리는 다음과 같이 묻습니다. 로봇 궤적에서 좋은 감각 운동 표현을 학습할 수 있을까요? 이 논문에서 우리는 로봇공학을 위한 자체 감독 감각 운동 사전 훈련 접근 방식을 제안합니다. 우리는 로봇 사전 훈련을 일반적인 감각 운동 시퀀스 예측 문제로 공식화합니다. 우리는 자연어 처리 및 컴퓨터 비전[6, 3, 4]의 대응 항목과 유사한 마스크된 예측 작업을 통해 이 아이디어를 인스턴스화합니다. 우리는 로봇이 누락된 감각 운동 내용을 예측할 수 있다면 행동할 수 있도록 하는 물리적 세계에 대한 좋은 모델을 습득했을 것이라고 가설을 세웁니다. RPT라고 불리는 저희 모델은 감각운동 토큰 시퀀스에서 작동하는 Transformer[9]입니다. 카메라 이미지, 고유수용성 로봇 상태, 동작의 입력 시퀀스가 주어지면, 저희는 인터리브된 시퀀스를 토큰으로 인코딩하고, 시퀀스의 하위 집합을 마스크 아웃하고, 나머지에서 마스크 아웃된 내용을 예측합니다. 저희는 높은 마스크 비율을 사용하여 모든 모달리티와 시간에 걸쳐 마스킹을 수행하는데, 이는 모델이 크로스 모달, 시공간적 표현을 학습하도록 장려합니다. 저희는 사전 훈련된 비전 인코더[8]를 사용하여 카메라 이미지를 인코딩하고, 감각운동 시퀀스 학습을 위해 잠재 시각 표현을 사용합니다. 이를 통해 저희는 인터넷에서 방대하고 다양한 이미지 컬렉션으로 훈련된 강력한 시각 표현을 기반으로 구축할 수 있습니다. 픽셀 공간에서 예측하는 것과 비교했을 때, 잠재 표현 공간에서 예측을 수행하면 작업이 더 다루기 쉽습니다. 이 설계는 계산적 비전 비용을 감각운동 맥락 길이에서 분리하여 300M개 이상의 매개변수 모델과 긴 맥락 길이를 가진 10Hz 제어를 물리적 로봇에서 실현 가능하게 합니다. *동등한 기여. 비디오는 프로젝트 페이지에서 볼 수 있습니다.실제 세계 궤적 감각 운동 사전 훈련 다운스트림 이미지, 고유 감각, 동작 전송 그림 1: 감각 운동 사전 훈련을 통한 로봇 학습.왼쪽: 다중 뷰 RGB 이미지, 고유 감각 로봇 상태 및 동작을 포함하는 대규모 실제 세계 궤적 데이터 세트를 수집하여 감각 운동 사전 훈련에 사용합니다.가운데: 감각 운동 궤적이 주어지면 하위 집합(줄무늬 패턴으로 표시)을 마스크하고 Transformer 모델을 훈련하여 마스크된 내용을 나머지에서 예측합니다.오른쪽: 사전 훈련된 표현을 다른 다운스트림 작업 및 로봇으로 전송합니다.사전 훈련 방식을 연구하기 위해 동작 계획 및 모델 기반 파악 알고리즘을 결합하여 9개월 동안 20,000개가 넘는 실제 세계 궤적 데이터 세트를 수집했습니다.각 궤적은 다중 뷰 RGB 카메라 이미지, 고유 감각 로봇 상태 및 동작의 시퀀스입니다. 우리는 단일 객체 선택, 빈 선택, 스태킹 및 디스태킹과 같은 고전적인 로봇 작업에 대한 궤적을 수집했습니다. 모든 작업에는 객체 포즈, 모양 및 외관의 변형이 포함됩니다. 사전 훈련의 효과를 이해하기 위해 일련의 실제 실험을 수행합니다. RPT가 처음부터 훈련하는 것보다 지속적으로 성능이 우수하고 더 어려운 작업에서 개선이 더 크다는 것을 발견했습니다(블록 스태킹 작업의 경우 최대 2배). 또한 감각 운동 사전 훈련 접근 방식이 다양한 작업, 실험실 환경 및 로봇 간에 성공적인 전송을 가능하게 한다는 것을 발견했습니다. 게다가 우리의 접근 방식은 유리한 스케일링 속성을 가지고 있으며 더 나은 비전 인코더, 더 긴 감각 운동 맥락 길이 및 더 큰 사전 훈련 데이터 세트의 이점을 얻습니다. 마지막으로 높은 마스킹 비율로 두 모달리티와 시간에 걸친 마스킹이 좋은 성능을 위해 중요하다는 것을 발견했습니다. 2 감각 운동 사전 훈련을 통한 로봇 학습 우리의 접근 방식은 사전 훈련과 미세 조정 단계로 구성됩니다(그림 1). 우리는 카메라 이미지, 고유 감각 상태 및 동작 시퀀스에 대한 마스크된 예측을 사용하여 감각 운동 표현을 사전 훈련합니다(그림 2). 사전 훈련 후 표현을 다운스트림 작업으로 전송합니다(그림 3). 2. 감각 운동 사전 훈련 = 우리는 접근 방식의 사전 훈련 단계를 설명하면서 시작합니다. 사전 훈련 단계에서 우리는 감각 운동 궤적 T의 데이터 집합 D를 받습니다. 각 감각 운동 궤적은 카메라 이미지, 고유 감각 로봇 상태 및 동작의 시퀀스입니다: T : (i1, S1, a1,..., iT, ST, aT). 우리는 언어 지침이나 작업 레이블과 같은 추가 의미 정보에 액세스할 수 없다고 가정합니다. 우리는 레이블이 없는 감각 운동 궤적이 물리적 세계의 구조를 암묵적으로 인코딩하고 그것을 사용하여 다운스트림 로봇 작업에 대한 감각 운동 표현을 학습할 수 있다고 가정합니다. 우리는 로봇 사전 훈련을 모든 모달리티와 시간에 걸친 일반적인 감각 운동 시퀀스 예측 문제로 공식화합니다. 우리는 시각 및 언어의 대응물과 유사한 마스크된 예측 작업을 통해 이 아이디어를 구현합니다[6, 3, 4]. 우리는 궤적의 하위 집합을 마스크하고 누락된 내용을 예측하는 모델을 훈련합니다. 구체적으로, L개 토큰의 감각운동 시퀀스가 주어지면 마스크 하위 시퀀스 MC[1, L]를 샘플링하고 관찰된 토큰 T[1,L]\M에 따라 조건화된 마스크된 토큰™의 평균 제곱 오차를 최소화하는 모델을 훈련합니다. 직감적으로 로봇이 누락된 감각운동 내용을 채울 수 있다면 행동할 수 있는 물리적 세계의 좋은 모델을 획득했을 것입니다. 이 일반적인 공식화를 통해 다양한 마스크 패턴을 사용하여 다양한 상황적 예측 문제를 표현할 수 있습니다. 모달리티, 타임스텝 및 토큰 수준에서의 랜덤 마스크와 인과적 마스크 등 여러 변형을 고려합니다.Lo ↑ Transformer ↑ M↑ MM ^ MM ●●● M ↑ ↑ 그림 2: 감각운동 사전 훈련. 우리 모델은 카메라 이미지, 고유 감각 로봇 상태, 과거 로봇 동작의 인터리브 시퀀스에서 작동하는 Transformer입니다. 우리는 감각 운동 입력을 토큰으로 인코딩하고, 하위 집합을 마스크하고, 누락된 내용을 예측하도록 모델을 훈련합니다. 2.2 아키텍처 토큰 마스킹. 감각 운동 토큰 시퀀스가 주어지면 하위 집합을 마스크합니다. 우리는 모달리티별로 학습되고 시간에 따라 공유되는 마스크 토큰을 사용하여 마스크된 각 입력을 표현합니다. 우리는 시간 단계나 모달리티를 고려하지 않고 입력 토큰을 독립적으로 마스크하여 모델이 모달리티와 시간 모두에서 정보를 상관시키는 법을 배우도록 합니다. 시각 잠복. 우리의 감각 운동 모델은 여러 보기에서 고차원 이미지 시퀀스를 처리해야 하는데, 이는 학습과 계산적 관점 모두에서 어렵습니다. 이를 극복하기 위해 우리는 사전 훈련된 시각 인코더를 사용하여 시각적 표현[8]을 계산하고 잠복 공간에서 작동합니다. 이를 통해 인터넷의 방대하고 다양한 이미지 컬렉션에서 훈련된 강력한 시각적 표현을 구축할 수 있습니다. 픽셀 공간에서의 예측과 비교했을 때 잠재 공간에서의 예측은 작업을 더 다루기 쉽게 만듭니다. 이 설계는 또한 계산적 시각 비용을 감각 운동 맥락 길이에서 분리하여 대규모 모델로 빠른 추론을 가능하게 합니다. 토큰 인코더. 모달리티당 별도의 선형 인코더를 사용합니다. 모달리티 인코더는 가중치를 공유하지 않으므로 추가 모달리티 임베딩을 사용하지 않습니다. 시간을 표현하기 위해 각 토큰에 위치 임베딩을 추가합니다. 단일 타임스텝의 모든 토큰은 위치 임베딩 값을 공유합니다. 변환기 모델. 인코딩되고 마스크된 토큰 시퀀스는 변환기 모델[9]로 전달됩니다. 다중 헤드 자체 주의 작업이 있는 일련의 변환기 블록으로 구성된 표준 변환기 설계를 따릅니다. 모델은 각 입력에 대한 잠재 표현을 예측합니다. 예측 헤드. 선형 프로젝트 계층을 사용하여 숨겨진 표현을 예측 대상으로 디코딩합니다. 각 잠재는 숨겨진 크기에서 원래 모달리티 차원으로 디코딩됩니다. 관절의 경우 원래 입력 공간에서 예측하고 이미지의 경우 시각적 잠재 공간에서 예측합니다. 마스크된 목적. 우리는 예측과 실제 입력 값 사이의 평균 제곱 오차 재구성 손실을 계산합니다. 우리는 마스크된 입력에 해당하는 잠재 표현의 예측에만 손실을 적용합니다. 관찰된 입력에 대한 예측은 손실이 발생하지 않습니다. 다양한 모달리티의 중요성과 규모를 가중하기 위해 모달리티별 손실 가중치를 적용합니다. 2.3 다운스트림 전송 우리의 목표는 다양한 다운스트림 작업과 로봇으로 전송할 수 있는 일반적인 감각 운동 표현을 학습하는 것입니다. 시각과 언어[6, 3, 4]에서 영감을 얻어 두 가지 설정을 탐색합니다. (1) 미세 조정. 다운스트림 작업 데이터에 대한 사전 학습된 모델 검사점을 미세 조정합니다. 이 설정에서 사전 학습은 학습을 위한 좋은 초기화를 제공하는 것으로 볼 수 있습니다. (2) 선형 프로브. 우리는 동결된 사전 학습된 모델을 사용하여 감각 운동 특징을 추출하고 단일 선형 계층을 학습하여 동작을 예측합니다. 이를 통해 사전 훈련된 표현의 품질만 평가할 수 있습니다.aa 훈련 동결 22222 • • • M^^^^^^...M (a) 미세 조정 (b) 선형 프로브 그림 3: 다운스트림 전송. 다운스트림 작업에서 표현을 평가하기 위해 두 가지 다른 설정을 고려합니다. (a) 미세 조정: 다운스트림 작업 데이터에서 사전 훈련된 전체 모델을 미세 조정합니다. (b) 선형 프로브: 사전 훈련된 모델을 동결하고 선형 동작 판독 레이어를 훈련합니다. 3 실험 설정 로봇 및 작업. 기본 1-DoF 병렬 턱 그리퍼가 있는 7-DoF Franka 로봇을 사용합니다. 10Hz에서 관절 위치 제어를 수행합니다. 고유 감각 정보에 관절 위치와 그리퍼 상태를 포함합니다. 로봇 손에 부착된 하나와 측면에 부착된 두 개의 RGB 카메라를 사용합니다(그림 9). 픽, 빈 픽, 디스태킹 및 스택의 네 가지 다른 작업을 고려합니다. 제안된 접근 방식을 연구하기 위해 동작 계획과 모델 기반 파악 알고리즘을 결합하여 감각 운동 궤적의 데이터 세트를 수집했습니다(자세한 내용은 부록 A 참조).비전 인코더.ViT(Vision Transformer) 아키텍처[10]를 사용하여 이미지 입력을 인코딩합니다.특히, Ego4D[11], Epic[12], Something-Something[13], 100 Days of Hands[14], ImageNet[15] 이미지의 450만 개 이미지 컬렉션에서 MAE[4]를 통해 학습된 [8]의 사전 학습된 모델을 사용합니다.동일한 모델을 사용하여 세 카메라 모두에서 기능을 추출합니다.비전 인코더의 출력 토큰의 평균 풀링을 비전 기능으로 사용합니다.감각 운동 변환기.감각 운동 모델은 ~1M 매개변수가 있는 인코더 전용 변환기입니다. 변환기는 192의 숨겨진 차원과 4개의 변환기 블록을 가지고 있으며, 각각 4개의 헤드와 2의 MLP 비율을 가지고 있습니다. 감각 운동 입력에는 여러 단계가 있으며, 각 단계에는 3개 카메라 뷰의 시각적 특징, 고유 감각 상태 및 동작이 포함됩니다. 다른 모달리티의 입력은 차원이 다르기 때문에(예: 이미지의 경우 768, 고유 감각 및 동작의 경우 8) 모달리티당 선형 레이어를 사용하여 각 모달리티를 동일한 차원인 192로 투영합니다. 사전 학습. 감각 운동 사전 학습 동안, 먼저 고정된 범위의 마스킹 비율에서 마스킹 확률을 무작위로 샘플링한 다음, 동일한 확률로 각 입력 토큰(시각적, 고유 감각 또는 동작)을 독립적으로 마스킹합니다. 기본 마스킹 비율 범위는 [0.7, 0.9]이며, 경험적으로 가장 잘 작동하는 것으로 나타났습니다. 또한 모달리티 또는 타임스텝의 모든 토큰을 마스킹하는 것과 같은 다른 마스킹 전략도 연구합니다. 섹션 4.5에서 마스킹 전략 및 마스킹 비율에 대한 절제를 참조하세요. 각 모달리티에 대한 마스크된 재구성 손실은 균일한 가중치를 사용하여 함께 가중치가 부여됩니다. 모든 모델은 배치 크기가 4096이고 워밍업 에포크가 50개인 300개 에포크 동안 사전 학습됩니다. 학습 속도가 4 × 10−4이고 가중치 감소가 0.01인 AdamW 옵티마이저를 사용합니다. 미세 조정. 사전 학습된 가중치로 감각 운동 모델을 초기화하고 다운스트림 작업에서 동작 복제로 미세 조정합니다. 미세 조정 시 모델은 마지막 시간 단계의 동작을 마스크 토큰으로 대체하여 사전 학습과 동일한 길이의 시퀀스를 가져옵니다. 동작을 예측하기 위해 마스크 입력 토큰에 해당하는 출력 토큰을 사용하고 다른 예측은 버립니다. 마지막 마스크 토큰에서 다음 16개 동작을 예측하는 선형 계층을 학습합니다. Stack 작업의 경우 900개 에포크, 다른 작업의 경우 300개 에포크 동안 모델을 미세 조정합니다. 학습률 및 배치 크기와 같은 다른 하이퍼파라미터는 사전 학습과 동일하게 유지됩니다.추론.미세 조정된 모델을 테스트할 때 과거의 감각 운동 궤적을 입력으로 제공하고 모델은 다음 16단계에 대한 동작을 예측합니다.예측된 동작은 10Hz로 동작을 실행하는 로봇 컨트롤러에 전달됩니다.한 번에 한 동작을 실행하고 다시 예측하는 실험을 했지만 성능에 큰 차이는 없었습니다.각 동작이 실행된 후 시각적 관찰과 달성된 상태가 기록되고 실행된 동작과 함께 자기 회귀 방식으로 다음 예측을 위한 감각 운동 입력으로 피드백됩니다.성공률(%)선택 RPT, 사전 학습된 Destack RPT, 사전 학습된 MVP, 스크래치 스택 RPT, 사전 학습된 MVP, 스크래치 MVP, 스크래치 250개 궤적 500개 궤적 궤적 그림 4: 샘플 복잡도, 미세 조정. 우리는 미세 조정 데이터의 양이 증가함에 따라 감각 운동 사전 훈련의 효과를 연구합니다. 우리는 감각 운동 사전 훈련(RPT)이 처음부터 감각 운동 모델을 훈련하는 것(MVP)보다 일관된 개선을 가져오고 더 어려운 작업(스택)에서 이득이 더 크다는 것을 발견했습니다. 비전 인코더는 둘 다에 대해 사전 훈련되고 동결된다는 점에 유의하십시오[8].실험 결과 우리는 실제 세계에서 평가를 수행하고 다양한 작업에 대한 샘플 복잡성, 작업 및 로봇 간 전송, 스케일링 속성 및 다양한 설계 결정을 연구합니다. 모든 실험에서 개체 위치 및 방향의 변화에 따른 16가지 실제 시도에 대한 성공률을 보고합니다.4.1 샘플 복잡성 우리는 미세 조정 성능을 스크래치 훈련과 비교하여 감각 운동 사전 훈련의 효과를 연구하는 것으로 시작합니다. 스크래치 기준으로 우리는 단일 단계 MLP 정책이 다단계 Transformer 정책으로 대체된 개선된 버전의 MVP[8]를 사용합니다. 공정한 비교를 위해 동일한 사전 훈련된 비전 인코더, 감각 운동 아키텍처를 사용하고 학습률, 배치 크기 및 모델당 에포크 수를 최적화합니다. 점점 어려워지는 세 가지 다운스트림 작업, 즉 피킹, 디스태킹 및 스태킹을 고려합니다. 미세 조정과 마찬가지로 동일한 작업의 다른 데이터 하위 집합을 사용하여 사전 훈련하고 평가를 위해 보이지 않는 집합을 보류합니다. 그림에서 미세 조정 데모 수가 증가함에 따라 성능을 보여줍니다. 사전 훈련은 다른 작업 및 데이터 체제에서 처음부터 훈련하는 것보다 일관된 개선으로 이어진다는 것을 관찰했습니다. 게다가 가장 어려운 블록 스태킹 작업에서 개선이 가장 컸습니다. 4.2 작업 간 전송 이전 섹션에서는 사전 훈련 및 미세 조정을 위해 동일한 작업의 데이터를 사용했습니다. 사전 훈련 접근 방식이 특정 작업에 대한 표현 대신 일반적인 감각 운동 표현을 학습할 수 있는지 평가하기 위해 다양한 작업에서 사전 훈련 및 미세 조정을 연구합니다. 구체적으로, 우리는 먼저 스태킹, 피킹, 성공적인 빈 피킹 궤적, 실패 사례를 포함하는 빈 피킹 궤적 등 다양한 작업의 데이터에 대한 모델을 사전 학습합니다.그런 다음 스태킹에 대한 모델을 미세 조정하고 평가합니다.그림 5에 결과를 보고합니다.우리는 모든 스태킹, 피킹 또는 빈 피킹에 대한 사전 학습이 스태킹에서 유사한 다운스트림 성능으로 이어지는 것을 관찰했는데, 이는 우리의 감각운동 사전 학습 접근 방식이 작업 간에 전이 가능한 표현을 학습할 수 있음을 시사합니다.또한 실패한 궤적이 포함된 모든 빈 피킹 데이터에 대한 사전 학습 시 성능이 낮아지는 것을 확인했는데, 이는 사전 학습 데이터 품질의 중요성을 강조합니다.성공률(%)스택 스택 피킹 빈 피킹 빈 모두 그림 5: 작업 간 전이.우리는 다양한 작업에 대한 사전 학습과 스태킹에 대한 미세 조정을 비교합니다.사전 학습이 작업 간에도 강력한 다운스트림 성능으로 이어질 수 있음을 알 수 있습니다.성공률(%)비전 인코더 컨텍스트 길이 사전 학습 데이터300개 토큰 수 궤적 수 매개변수(M) 그림 6: 스케일링 연구. 우리의 접근 방식은 더 나은 비전 인코더(왼쪽), 더 긴 컨텍스트 길이(가운데), 더 많은 사전 학습 데이터(오른쪽)에서 이점을 얻는다는 것을 발견했습니다. 블록 스태킹에서 평가되었습니다. 4.3 스케일링 연구 비전 인코더. 우리는 비전 인코더의 크기가 증가함에 따라 사전 학습 접근 방식의 성능을 연구합니다. 모든 경우에서 [8]의 사전 학습되고 동결된 비전 인코더를 사용합니다. 특히, 우리는 크기가 증가하는 세 가지 ViT 변형인 ViT-S, ViT-B 및 ViT-L을 고려합니다. 우리는 정확한 공간적 위치가 필요한 블록 스태킹 작업에서 성능을 평가합니다. 결과는 그림 6, 왼쪽에 나와 있습니다. 우리는 더 나은 비전 모델로 성능이 상당히 향상되는 것을 관찰합니다. 우리는 우리의 모델이 ViT-L 비전 인코더를 사용하는 경우에도 여전히 10Hz 추론이 가능하다는 것을 알았습니다. 컨텍스트 길이. 우리는 다양한 컨텍스트 길이와 함께 감각운동 사전 학습을 비교합니다. 즉, 1, 4, 8 및 16 타임스텝의 컨텍스트 길이를 고려합니다. 각 타임스텝에 토큰 5개가 포함되어 있음을 유의하십시오.그림 6 중간에서 더 큰 컨텍스트에서 사전 학습이 일관된 개선으로 이어지는 것을 관찰했는데, 이는 더 긴 컨텍스트가 더 풍부한 감각운동 사전 학습 문제를 촉진할 수 있음을 시사할 수 있습니다.사전 학습 데이터.사전 학습 데이터의 양이 증가함에 따라 접근 방식의 확장을 연구합니다.480, 960 및 1920개 궤적에서 사전 학습을 고려하고 블록 스태킹에서 다운스트림 성능을 평가합니다.그림 6 오른쪽에서 접근 방식이 더 많은 사전 학습 데이터에서 이점을 얻는 것을 관찰했는데, 이는 더 큰 궤적 컬렉션으로 감각운동 사전 학습을 확장하기 위한 유망한 신호입니다.4.4 로봇 간 전송 실험실 간 전송.다른 로봇 간 전송을 평가합니다.먼저 동일한 로봇 유형의 다른 인스턴스로의 전송을 고려합니다.다운스트림 로봇은 환경 조건, 다양한 카메라 배치, 배경 및 조명이 있는 다른 실험실에 있습니다.사전 학습을 처음부터 학습하는 것과 비교합니다. 그림 1a에 결과를 보고하고 사전 학습이 처음부터 학습하는 것보다 상당히 우수한 성과를 보인다는 것을 관찰했습니다.교차 로봇 전이.다음으로, 이 설정을 더욱 확장하여 다양한 로봇 유형 간의 전이를 평가합니다.특히, 8가지 다른 작업에 걸쳐 원격 조작을 통해 수집한 640개 궤적이 포함된 [8]의 xArm 데이터에 대해 사전 학습합니다.여기서 격차는 로봇 유형, 카메라 유형 및 배치, 작업, 배경, 조명, 데이터 빈도 및 수집 전략의 차이로 인해 상당히 큽니다.xArm에 대한 사전 학습을 (a) 사전 학습 없음 및 (b) 동일한 양의 원래 Franka 데이터에 대한 사전 학습과 비교합니다.표 1b에서 xArm에 대한 사전 학습이 처음부터 학습하는 것보다 상당히 우수한 성과를 보이고 동일한 로봇에 대한 사전 학습과 매우 가까운 것을 알 수 있습니다.Franka A 사전 학습 Franka B 미세 조정 Franka B 성공(%) 25.68.(a) Franka →→Franka. 한 연구실의 프랑카에서 다른 연구실의 프랑카로 카메라 위치, 배경 및 조명에 차이가 있는 효과적인 전송.사전 훈련 미세 조정 프랑카 성공률(%) 25.xArm 프랑카 프랑카 프랑카 50.56.(b) xArm → 프랑카. 감각 운동 사전 훈련은 다른 로봇 유형에서도 효과적일 수 있으며 동일한 로봇에서의 사전 훈련과 매우 유사하다는 것을 발견했습니다.표 1: 로봇 간 전송.한 로봇의 데이터로 사전 훈련을 하고 다른 로봇으로의 다운스트림 전송을 평가합니다.교차 연구실 및 교차 로봇 전송을 모두 실험합니다.부분 미세 조정 마스킹 유형 마스킹 비율 성공률(%)높음 선형 프로브 낮음 보통 전체 미세 조정 토큰 시간 모달리티 미세 조정된 블록 수 그림 7: 절제 연구.선형 프로빙은 사소하지 않은 성능을 달성하는 반면 미세 조정은 상당히 더 나은 결과로 이어지고 모델의 더 큰 부분을 미세 조정하면 더 높은 성공으로 이어진다는 것을 발견했습니다(왼쪽). 모든 토큰에 대한 마스킹이 한 번에 한 타임스텝의 모든 토큰이나 한 모달리티의 모든 토큰을 마스킹하는 것보다 상당히 더 잘 작동한다는 것을 관찰했습니다(가운데). 게다가 높은 마스킹 비율을 사용하는 것은 좋은 감각운동 표현을 학습하는 데 필수적입니다(오른쪽). 4.5 절제 연구 선형 프로브. 선형 프로빙을 통해 사전 훈련된 감각운동 표현을 평가합니다. 즉, 동결된 감각운동 모델을 사용하여 표현을 추출하고 픽킹을 위한 궤적을 사용하여 그 위에 선형 계층을 훈련합니다. 그림 7(왼쪽)에 결과를 보고합니다. 선형 프로빙이 43.75%의 성공률에 도달하는 것을 관찰했는데, 이는 사소하지 않지만 미세 조정을 사용한 93.8%보다 상당히 낮습니다. 부분 미세 조정. 미세 조정 평가에서 기본적으로 전체 모델을 미세 조정합니다. 여기서는 점점 더 많은 수의 Transformer 블록을 미세 조정하는 영향을 연구합니다. 그림 7(왼쪽)에 결과를 보고합니다. 0개 블록과 4개 블록을 미세 조정하는 것은 각각 선형 프로빙과 전체 미세 조정과 동일합니다. 우리는 미세 조정된 블록의 수에 따라 성능이 증가하고 최상의 성능을 달성하려면 대부분의 블록을 미세 조정해야 함을 관찰했습니다.마스킹 유형.우리는 다양한 마스킹 전략을 실험합니다(섹션 4.8 참조).특히, 우리는 시간 단계의 모든 토큰을 마스크하는 시간 단계 마스킹, 모달리티의 모든 토큰을 마스크하는 모달리티 마스킹, 모든 토큰을 독립적으로 마스크하는 토큰 마스킹을 고려합니다.그림 7 중간에서 토큰 마스킹이 다른 대안보다 상당히 우수한 성능을 보이는 것을 볼 수 있습니다.마스킹 비율.우리는 마스킹 비율의 다른 값을 제거합니다.그림 7 오른쪽에서 볼 수 있듯이 마스킹 비율이 낮음[0.1, 0.9] 또는 중간[0.4, 0.9]일 때 높은 마스킹 비율[0.7, 0.9]에 비해 성능이 크게 떨어집니다.이는 입력 감각 운동 시퀀스에서 다양한 시간 단계와 모달리티 간에 높은 중복성을 시사하며 높은 마스킹 비율은 다운스트림 작업에 대한 유용한 표현을 학습하는 데 중요합니다. 이는 시각적 사전 훈련에 대한 이전 작업[4]과 일치합니다.4.6 추론 속도 저희의 감각운동 모델은 이미지당 계산되는 잠재적 시각적 표현에서 작동하도록 설계되어 시각 모델을 감각운동 모델에서 분리하고 시각 계산 비용을 컨텍스트 길이와 무관하게 만듭니다.이를 통해 2080 Ti GPU에서 10Hz의 시스템 수준 추론 속도로 대규모 시각 모델(307M 매개변수가 있는 ViT-L까지)을 사용할 수 있습니다.4.7 출현 자체 수정 저희는 감각운동 예측으로 사전 훈련된 일부 모델이 테스트 시간에 출현 자체 수정 동작을 보일 수 있음을 관찰했습니다.예를 들어, 로봇이 처음에 물체를 잡지 못하면 뒤로 이동하여 성공적으로 물체를 잡습니다.동영상은 프로젝트 페이지를 참조하세요.이러한 모델은 성공적인 궤적을 통해서만 사전 훈련되고 미세 조정되었기 때문에 훈련 중에 이러한 유형의 동작이 나타나지 않았습니다. 게다가 일부 상태는 데이터에 전혀 존재하지 않았습니다(예: 그립을 시작하거나 완전히 닫힌 그립퍼로 위로 이동).4. 인과적 마스킹 우리는 지금까지의 실험처럼 무작위 마스킹 대신 인과적 마스킹을 사용하는 감각운동 사전 훈련 접근 방식의 변형을 실험합니다. 우리는 최소한의 변경을 하고 샘플링된 마스크 시퀀스를 간단히 정렬합니다. 자기 주의는 여전히 인과적이 아니라 양방향이라는 점에 유의하세요. 그러나 사전 훈련 예측 문제는 인과적입니다. 우리는 두 가지 설정을 고려합니다. 동일한 작업에 대한 사전 훈련(픽 투 픽)과 다른 작업에 대한 사전 훈련(픽 투 스택)입니다. 그림 8에 결과를 보고합니다. 인과적 사전 훈련은 동일한 작업에서 더 나은 성능을 가져오고 다른 작업 간에 더 나쁜 전송으로 이어진다는 것을 발견했습니다.5
--- EXPERIMENT ---
s. RPT가 처음부터 훈련하는 것보다 지속적으로 더 나은 성과를 보이고, 더 어려운 작업(블록 스태킹 작업의 경우 최대 2배)에서 개선이 더 크다는 것을 발견했습니다. 또한, 감각 운동 사전 훈련 접근 방식이 다양한 작업, 실험실 환경 및 로봇 간에 성공적인 전이를 가능하게 한다는 것을 발견했습니다. 게다가, 우리의 접근 방식은 유리한 스케일링 속성을 가지고 있으며, 더 나은 비전 인코더, 더 긴 감각 운동 맥락 길이 및 더 큰 사전 훈련 데이터 세트의 이점을 얻습니다. 마지막으로, 높은 마스킹 비율로 두 모달리티와 시간에 걸친 마스킹이 좋은 성능을 위해 중요하다는 것을 발견했습니다. 2 감각 운동 사전 훈련을 통한 로봇 학습 우리의 접근 방식은 사전 훈련과 미세 조정 단계로 구성됩니다(그림 1). 카메라 이미지, 고유 감각 상태 및 동작 시퀀스에 대한 마스크 예측을 사용하여 감각 운동 표현을 사전 훈련합니다(그림 2). 사전 훈련 후 표현을 다운스트림 작업으로 전이합니다(그림 3). 2. 감각 운동 사전 훈련 = 먼저 우리 접근 방식의 사전 훈련 단계를 설명합니다. 사전 훈련 단계에서 우리는 감각 운동 궤적 T의 데이터 집합 D를 제공받습니다. 각 감각 운동 궤적은 카메라 이미지, 고유 감각 로봇 상태 및 동작의 시퀀스입니다: T : (i1, S1, a1,..., iT, ST, aT). 우리는 언어 지침이나 작업 레이블과 같은 추가 의미 정보에 액세스할 수 없다고 가정합니다. 우리는 레이블이 없는 감각 운동 궤적이 물리적 세계의 구조를 암묵적으로 인코딩하고 그것을 사용하여 다운스트림 로봇 작업에 대한 감각 운동 표현을 학습할 수 있다고 가정합니다. 우리는 로봇 사전 훈련을 모든 양식과 시간에 걸친 일반적인 감각 운동 시퀀스 예측 문제로 공식화합니다. 우리는 시각 및 언어의 대응물과 유사한 마스크된 예측 작업을 통해 이 아이디어를 인스턴스화합니다[6, 3, 4]. 우리는 궤적의 하위 집합을 마스크하고 누락된 내용을 예측하는 모델을 훈련합니다. 구체적으로 L 토큰의 감각운동 시퀀스가 주어지면 마스크 하위 시퀀스 MC [1, L]을 샘플링하고 관찰된 토큰 T[1,L]\M에 조건화된 마스크된 토큰™의 평균 제곱 오차를 최소화하는 모델을 학습합니다. 직감적으로 로봇이 누락된 감각운동 내용을 채울 수 있다면 행동할 수 있는 물리적 세계의 좋은 모델을 획득했을 것입니다. 이 일반적인 공식을 사용하면 다양한 마스킹 패턴을 사용하여 여러 가지 상황적 예측 문제를 표현할 수 있습니다. 모달리티, 타임스텝 및 토큰 수준에서의 임의 마스킹과 인과적 마스킹을 포함한 여러 변형을 고려합니다. Lo ↑ Transformer ↑ M↑ MM ^ MM ●●● M ↑ ↑ 그림 2: 감각운동 사전 학습. 우리 모델은 카메라 이미지, 고유 감각 로봇 상태 및 과거 로봇 동작의 인터리브 시퀀스에서 작동하는 Transformer입니다. 우리는 감각운동 입력을 토큰에 인코딩하고, 하위 집합을 마스크하고, 누락된 내용을 예측하는 모델을 학습합니다. 2.2 아키텍처 토큰 마스킹. 감각 운동 토큰 시퀀스가 주어지면 하위 집합을 마스크합니다. 마스크된 각 입력을 모달리티별로 학습하고 시간에 따라 공유하는 마스크 토큰을 사용하여 표현합니다. 시간 단계나 모달리티를 고려하지 않고 입력 토큰을 독립적으로 마스크하여 모델이 모달리티와 시간 모두에서 정보를 상관시키는 법을 배우도록 합니다. 시각 잠복. 감각 운동 모델은 여러 보기에서 고차원 이미지 시퀀스를 처리해야 하는데, 이는 학습과 계산적 관점에서 모두 어렵습니다. 이를 극복하기 위해 사전 학습된 시각 인코더를 사용하여 시각적 표현[8]을 계산하고 잠복 공간에서 작동합니다. 이를 통해 인터넷에서 방대하고 다양한 이미지 컬렉션을 학습한 강력한 시각적 표현을 기반으로 구축할 수 있습니다. 픽셀 공간에서의 예측과 비교할 때 잠복 공간에서의 예측은 작업을 더 쉽게 처리합니다. 이 설계는 또한 계산적 시각 비용을 감각 운동 맥락 길이에서 분리하여 대규모 모델로 빠르게 추론할 수 있게 합니다. 토큰 인코더. 모달리티별로 별도의 선형 인코더를 사용합니다. 모달리티 인코더는 가중치를 공유하지 않으므로 추가 모달리티 임베딩을 사용하지 않습니다. 시간을 표현하기 위해 각 토큰에 위치 임베딩을 추가합니다. 단일 타임스텝의 모든 토큰은 위치 임베딩 값을 공유합니다. Transformer 모델. 인코딩되고 마스크된 토큰 시퀀스는 Transformer 모델[9]로 전달됩니다. 다중 헤드 셀프 어텐션 작업이 있는 일련의 Transformer 블록으로 구성된 표준 Transformer 설계를 따릅니다. 모델은 각 입력에 대한 잠재 표현을 예측합니다. 예측 헤드. 선형 프로젝트 계층을 사용하여 숨겨진 표현을 예측 대상으로 디코딩합니다. 각 잠재는 숨겨진 크기에서 원래 모달리티 차원으로 디코딩됩니다. 조인트의 경우 원래 입력 공간에서, 이미지의 경우 시각적 잠재 공간에서 예측을 수행합니다. 마스크된 목적. 예측과 기준 진실 입력 값 사이의 평균 제곱 오차 재구성 손실을 계산합니다. 마스크된 입력에 해당하는 잠재 표현의 예측에만 손실을 적용합니다. 관찰된 입력에 대한 예측에는 손실이 발생하지 않습니다. 다양한 모달리티의 중요성과 규모를 가중하기 위해 모달리티별 손실 가중치를 적용합니다.2.3 다운스트림 전송 우리의 목표는 다양한 다운스트림 작업과 로봇으로 전송될 수 있는 일반적인 감각운동 표현을 학습하는 것입니다.시각과 언어에서 영감을 받아 [6, 3, 4] 두 가지 설정을 탐색합니다.(1) 미세 조정.다운스트림 작업 데이터에 대한 사전 학습된 모델 체크포인트를 미세 조정합니다.이 설정에서 사전 학습은 학습을 위한 좋은 초기화를 제공하는 것으로 볼 수 있습니다.(2) 선형 프로브.동결된 사전 학습된 모델을 사용하여 감각운동 특징을 추출하고 단일 선형 계층을 학습하여 동작을 예측합니다.이를 통해 사전 학습된 표현의 품질만 평가할 수 있습니다.aa 학습 동결 22222 • • • M^^^^^^...M (a) 미세 조정 (b) 선형 프로브 그림 3: 다운스트림 전송. 다운스트림 작업에서 표현을 평가하기 위해 두 가지 다른 설정을 고려합니다.(a) 미세 조정: 다운스트림 작업 데이터에서 사전 훈련된 전체 모델을 미세 조정합니다.(b) 선형 프로브: 사전 훈련된 모델을 동결하고 선형 동작 판독 레이어를 훈련합니다.3 실험 설정 로봇 및 작업.기본 1-DoF 병렬 턱 그리퍼가 있는 7-DoF Franka 로봇을 사용합니다.10Hz에서 관절 위치 제어를 수행합니다.고유 감각 정보에 관절 위치와 그리퍼 상태를 포함합니다.로봇 손에 부착된 하나와 측면에 부착된 두 개의 RGB 카메라(그림 9)를 사용합니다.4가지 다른 작업(Pick, Bin Pick, Destack 및 Stack)을 고려합니다.제안된 접근 방식을 연구하기 위해 동작 계획과 모델 기반 파악 알고리즘을 결합하여 감각운동 궤적 데이터 세트를 수집했습니다(자세한 내용은 부록 A 참조).비전 인코더.Vision Transformer(ViT) 아키텍처[10]를 사용하여 이미지 입력을 인코딩합니다. 구체적으로, 우리는 Ego4D [11], Epic [12], Something-Something [13], 100 Days of Hands [14], ImageNet [15] 이미지의 4.5M 이미지 컬렉션에서 MAE [4]를 통해 훈련된 [8]의 사전 훈련된 모델을 사용합니다. 우리는 동일한 모델을 사용하여 세 카메라 모두에서 기능을 추출합니다. 우리는 비전 인코더의 출력 토큰의 평균 풀링을 비전 기능으로 사용합니다. 감각 운동 변환기. 우리의 감각 운동 모델은 ~1M 매개변수가 있는 인코더 전용 변환기입니다. 변환기는 192의 숨겨진 차원과 4개의 변환기 블록을 가지고 있으며, 각각 4개의 헤드와 2의 MLP 비율을 가지고 있습니다. 감각 운동 입력에는 여러 단계가 포함되어 있으며, 각 단계에는 3개 카메라 뷰의 시각적 기능, 고유 감각 상태 및 동작이 포함됩니다. 다른 모달리티의 입력은 차원이 다르므로(예: 이미지의 경우 768, 고유 감각 및 동작의 경우 8), 모달리티당 선형 레이어를 사용하여 각 모달리티를 동일한 차원인 192로 투영합니다.사전 훈련.감각운동 사전 훈련 동안, 먼저 고정된 범위의 마스킹 비율에서 마스킹 확률을 무작위로 샘플링한 다음, 동일한 확률로 각 입력 토큰(시각적, 고유 감각 또는 동작)을 독립적으로 마스킹합니다.기본 마스킹 비율 범위는 [0.7, 0.9]이며, 경험적으로 가장 잘 작동하는 것으로 나타났습니다.또한 모달리티 또는 타임스텝의 모든 토큰을 마스킹하는 것과 같은 다른 마스킹 전략도 연구합니다.4.5절에서 마스킹 전략 및 마스킹 비율에 대한 절제를 참조하세요.각 모달리티의 마스크된 재구성 손실은 균일한 가중치를 사용하여 함께 가중치가 지정됩니다.모든 모델은 배치 크기가 4096이고 워밍업 에포크가 50개인 300개 에포크 동안 사전 훈련됩니다. AdamW 최적화 도구를 학습률 4 × 10−4, 가중치 감소 0.01로 사용합니다.미세 조정. 사전 훈련된 가중치로 감각 운동 모델을 초기화하고 다운스트림 작업에서 동작 복제로 미세 조정합니다.미세 조정 시점에 모델은 사전 훈련과 동일한 길이의 시퀀스를 가져오고 마지막 시간 단계의 동작을 마스크 토큰으로 바꿉니다.동작을 예측하기 위해 마스크 입력 토큰에 해당하는 출력 토큰을 사용하고 다른 예측은 버립니다.마지막 마스크 토큰에서 다음 16개 동작을 예측하는 선형 계층을 훈련합니다.Stack 작업의 경우 900에포크, 다른 작업의 경우 300에포크 동안 모델을 미세 조정합니다.학습률 및 배치 크기와 같은 다른 하이퍼파라미터는 사전 훈련과 동일하게 유지됩니다.추론.미세 조정된 모델을 테스트할 때 과거 감각 운동 궤적을 입력으로 제공하고 모델은 다음 16단계의 동작을 예측합니다. 예측된 동작은 로봇 컨트롤러에 전달되고 컨트롤러는 10Hz로 동작을 실행합니다. 한 번에 한 동작을 실행하고 다시 예측하는 실험을 했지만 성능에 큰 차이는 관찰되지 않았습니다. 각 동작이 실행된 후 시각적 관찰과 달성된 상태가 기록되고 실행된 동작과 함께 자기 회귀 방식으로 다음 예측을 위한 감각 운동 입력으로 피드백됩니다. 성공(%) RPT 선택, 사전 학습된 Destack RPT, 사전 학습된 MVP, 스크래치 스택 RPT, 사전 학습된 MVP, 스크래치 MVP, 스크래치 250개 궤적 500개 궤적 궤적 그림 4: 샘플 복잡성, 미세 조정. 미세 조정 데이터 양이 증가함에 따라 감각 운동 사전 학습의 효과를 연구합니다. 감각 운동 사전 학습(RPT)이 처음부터 감각 운동 모델을 학습하는 것(MVP)보다 일관된 개선을 가져오고 더 어려운 작업(스택)에서 이득이 더 크다는 것을 발견했습니다. 비전 인코더는 [8] 둘 다에 대해 사전 학습되고 동결되어 있습니다.실험 결과 실제 세계에서 평가를 수행하고 다양한 작업에 대한 샘플 복잡성, 작업 및 로봇 간 전송, 스케일링 속성 및 다양한 설계 결정을 연구합니다.모든 실험에서 개체 위치 및 방향의 변화에 따른 16가지 실제 시도에 대한 성공률을 보고합니다.4.1 샘플 복잡성 우리는 미세 조정 성능을 스크래치 형식 학습과 비교하여 감각 운동 사전 학습의 효과를 연구하는 것으로 시작합니다.스크래치 기준으로 단일 단계 MLP 정책이 다단계 Transformer 정책으로 대체된 개선된 버전의 MVP [8]를 사용합니다.공정한 비교를 위해 동일한 사전 학습된 비전 인코더, 감각 운동 아키텍처를 사용하고 학습 속도, 배치 크기 및 모델당 에포크 수를 최적화합니다.점점 어려워지는 세 가지 다운스트림 작업, 즉 피킹, 디스태킹 및 스태킹을 고려합니다. 우리는 미세 조정에서와 같이 동일한 작업의 다른 데이터 하위 집합을 사용하여 사전 학습하고, 평가를 위해 보이지 않는 집합을 보류합니다.그림에서 미세 조정 데모의 수가 증가함에 따라 성능을 보여줍니다.사전 학습은 다른 작업과 데이터 체제에서 처음부터 학습하는 것보다 일관된 개선으로 이어진다는 것을 관찰했습니다.또한 가장 어려운 블록 스태킹 작업에서 개선이 가장 컸습니다.4.2 작업 간 전송 이전 섹션에서는 사전 학습 및 미세 조정을 위해 동일한 작업의 데이터를 사용했습니다.사전 학습 접근 방식이 특정 작업에 대한 표현 대신 일반적인 감각 운동 표현을 학습할 수 있는지 평가하기 위해 다른 작업에서 사전 학습 및 미세 조정을 연구합니다.특히, 먼저 스태킹, 피킹, 성공적인 빈 피킹 궤적, 실패 사례를 포함하는 빈 피킹 궤적 등 다른 작업의 데이터에 대한 모델을 사전 학습합니다.그런 다음 스태킹에 대한 모델을 미세 조정하고 평가합니다. 그림 5에 결과를 보고합니다. 모든 스태킹, 피킹 또는 빈 피킹에 대한 사전 학습이 스태킹에서 유사한 다운스트림 성능으로 이어지는 것을 관찰했는데, 이는 우리의 감각운동 사전 학습 접근 방식이 작업 간에 전이 가능한 표현을 학습할 수 있음을 시사합니다. 또한 실패한 궤적을 포함하는 모든 빈 피킹 데이터에 대한 사전 학습 시 성능이 낮아지는 것을 확인했는데, 이는 사전 학습 데이터 품질의 중요성을 강조합니다. 성공(%) 스택 스택 피킹 빈 피킹 빈 모두 그림 5: 작업 간 전이. 다양한 작업에 대한 사전 학습과 스태킹에 대한 미세 조정을 비교합니다. 사전 학습이 작업 간에도 강력한 다운스트림 성능으로 이어질 수 있음을 알 수 있습니다. 성공(%) 비전 인코더 컨텍스트 길이 사전 학습 데이터 300개 토큰 수 궤적 수 매개변수(M) 그림 6: 스케일링 연구. 우리의 접근 방식은 더 나은 비전 인코더(왼쪽), 더 긴 컨텍스트 길이(가운데), 더 많은 사전 학습 데이터(오른쪽)에서 이점을 얻습니다. 블록 스태킹에서 평가됨. 4.3 스케일링 연구 비전 인코더. 비전 인코더의 크기가 증가함에 따라 사전 학습 접근 방식의 성능을 연구합니다. 모든 경우에서 [8]의 사전 학습되고 동결된 비전 인코더를 사용합니다. 구체적으로, 크기가 증가하는 세 가지 ViT 변형인 ViT-S, ViT-B 및 ViT-L을 고려합니다. 정확한 공간적 위치가 필요한 블록 스태킹 작업에서 성능을 평가합니다. 결과는 왼쪽의 그림 6에 나와 있습니다. 더 나은 비전 모델로 성능이 크게 향상되는 것을 관찰합니다. ViT-L 비전 인코더를 사용하더라도 모델이 여전히 10Hz 추론이 가능하다는 것을 알 수 있습니다. 맥락 길이. 다양한 맥락 길이와 함께 감각 운동 사전 학습을 비교합니다. 즉, 1, 4, 8 및 16 타임스텝의 맥락 길이를 고려합니다. 각 타임스텝에는 토큰이 5개 포함되어 있습니다. 가운데의 그림 6에서 더 큰 맥락에서 사전 학습하면 일관된 개선이 이루어지는 것을 관찰했는데, 이는 더 긴 맥락이 더 풍부한 감각 운동 사전 학습 문제를 용이하게 할 수 있음을 시사할 수 있습니다. 사전 학습 데이터. 사전 학습 데이터의 양이 증가함에 따라 접근 방식의 확장을 연구합니다. 480, 960 및 1920 궤적에 대한 사전 학습을 고려하고 블록 스태킹에서 다운스트림 성능을 평가합니다. 오른쪽 그림 6에서 접근 방식이 더 많은 사전 학습 데이터에서 이점을 얻는 것을 관찰했는데, 이는 더 큰 궤적 컬렉션으로 감각 운동 사전 학습을 확장하는 데 유망한 신호입니다. 4.4 로봇 간 전이 실험실 간 전이. 다른 로봇 간 전이를 평가합니다. 먼저 동일한 로봇 유형의 다른 인스턴스로의 전이를 고려합니다. 다운스트림 로봇은 환경 조건, 카메라 배치, 배경 및 조명이 다른 다른 실험실에 있습니다. 사전 학습과 처음부터의 학습을 비교합니다. 그림 1a에 결과를 보고하고 사전 학습이 처음부터의 학습보다 상당히 우수한 성과를 거두는 것을 관찰합니다. 로봇 간 전이. 다음으로 이 설정을 더욱 발전시키고 다른 로봇 유형 간 전이를 평가합니다. 구체적으로, 우리는 8개의 다른 작업에 걸쳐 원격 조작을 통해 수집된 640개의 궤적을 포함하는 [8]의 xArm 데이터에 대해 사전 훈련을 합니다. 로봇 유형, 카메라 유형 및 배치, 작업, 배경, 조명, 데이터 빈도 및 수집 전략의 차이로 인해 여기서 격차가 상당히 크다는 점에 유의하십시오. 우리는 xArm에 대한 사전 훈련을 (a) 사전 훈련 없음 및 (b) 동일한 양의 원래 Franka 데이터에 대한 사전 훈련과 비교합니다. 표 1b에서 우리는 xArm에 대한 사전 훈련이 처음부터 훈련하는 것보다 상당히 우수하고 동일한 로봇에 대한 사전 훈련과 매우 가깝다는 것을 알 수 있습니다. Franka A 사전 훈련 Franka B 미세 조정 Franka B 성공(%) 25.68.(a) Franka →→Franka. 카메라 위치, 배경 및 조명의 차이가 있는 한 연구실의 Franka에서 다른 연구실의 다른 Franka로의 효과적인 전송. 사전 학습 미세 조정 Franka 성공률(%) 25.xArm Franka Franka Franka 50.56.(b) xArm → Franka. 감각 운동 사전 학습은 다른 로봇 유형에서도 효과적일 수 있으며 동일한 로봇에 대한 사전 학습과 매우 유사하다는 것을 발견했습니다.표 1: 로봇 간 전송.한 로봇의 데이터에 대해 사전 학습하고 다른 로봇으로의 다운스트림 전송을 평가합니다.교차 실험실 및 교차 로봇 전송을 모두 실험합니다.부분 미세 조정 마스킹 유형 마스킹 비율 성공률(%)높음 선형 프로브 낮음 보통 전체 미세 조정 토큰 시간 모달리티 미세 조정된 블록 수 그림 7: 절제 연구.선형 프로빙이 사소하지 않은 성능을 달성하는 반면 미세 조정은 상당히 더 나은 결과로 이어지고 모델의 더 큰 부분을 미세 조정하면 더 높은 성공으로 이어진다는 것을 발견했습니다(왼쪽).모든 토큰에 대한 마스킹이 한 번에 한 타임스텝의 모든 토큰이나 한 모달리티의 모든 토큰을 마스킹하는 것보다(가운데) 상당히 더 잘 작동한다는 것을 관찰했습니다. 또한, 높은 마스킹 비율을 사용하는 것은 좋은 감각운동 표현을 학습하는 데 필수적입니다(오른쪽).4.5 절제 연구 선형 프로브.선형 프로빙을 통해 사전 훈련된 감각운동 표현을 평가합니다.즉, 동결된 감각운동 모델을 사용하여 표현을 추출하고 픽킹을 위한 궤적을 사용하여 그 위에 선형 계층을 훈련합니다.그림 7(왼쪽)에 결과를 보고합니다.선형 프로빙은 43.75%의 성공률에 도달하는 것을 관찰했는데, 이는 사소하지 않지만 미세 조정을 사용한 93.8%보다 상당히 낮습니다.부분적 미세 조정.미세 조정 평가에서 기본적으로 전체 모델을 미세 조정합니다.여기서는 점점 더 많은 수의 Transformer 블록을 미세 조정하는 영향을 연구합니다.그림 7(왼쪽)에 결과를 보고합니다.0개 블록과 4개 블록을 미세 조정하는 것은 각각 선형 프로빙과 전체 미세 조정과 동일하다는 점에 유의하세요.미세 조정된 블록 수에 따라 성능이 증가하고 최상의 성능을 얻으려면 대부분의 블록을 미세 조정해야 함을 관찰했습니다.마스킹 유형. 우리는 다양한 마스킹 전략을 실험합니다(4.8절 참조). 구체적으로, 우리는 시간 단계의 모든 토큰을 마스크하는 시간 단계 마스킹, 모달리티의 모든 토큰을 마스크하는 모달리티 마스킹, 모든 토큰을 독립적으로 마스크하는 토큰 마스킹을 고려합니다. 그림 7 중간에서 토큰 마스킹이 다른 대안보다 상당히 우수한 성능을 보이는 것을 볼 수 있습니다. 마스킹 비율. 우리는 마스킹 비율의 다양한 값을 제거합니다. 그림 7 오른쪽에서 볼 수 있듯이 마스킹 비율이 낮음[0.1, 0.9] 또는 중간[0.4, 0.9]일 때 높은 마스킹 비율[0.7, 0.9]에 비해 성능이 크게 떨어집니다. 이는 입력 감각 운동 시퀀스에서 다양한 시간 단계와 모달리티 간에 높은 중복성이 있음을 시사하며, 높은 마스킹 비율은 다운스트림 작업에 대한 유용한 표현을 학습하는 데 중요합니다. 이는 시각적 사전 학습[4]에 대한 이전 작업과 일치합니다. 4.6 추론 속도 저희의 감각운동 모델은 이미지당 계산되는 잠재적 시각 표현에서 작동하도록 설계되어 시각 모델을 감각운동 모델에서 분리하고 시각 계산 비용을 컨텍스트 길이와 무관하게 만듭니다. 이를 통해 2080 Ti GPU에서 10Hz의 시스템 수준 추론 속도로 대규모 시각 모델(최대 307M 매개변수가 있는 ViT-L)을 사용할 수 있습니다. 4.7 새로운 자기 교정 저희는 감각운동 예측으로 사전 학습된 일부 모델이 테스트 시간에 새로운 자기 교정 동작을 보일 수 있음을 관찰했습니다. 예를 들어, 로봇이 처음에 물체를 잡지 못하면 뒤로 이동하여 성공적으로 물체를 잡습니다. 비디오는 프로젝트 페이지를 참조하세요. 이러한 모델은 성공적인 궤적을 통해서만 사전 학습되고 미세 조정되었기 때문에 이러한 유형의 동작은 학습 중에 나타나지 않았습니다. 게다가 일부 상태는 데이터에 전혀 존재하지 않았습니다(예: 그립 시작 또는 완전히 닫힌 그리퍼로 위로 이동).4. 인과적 마스킹 우리는 지금까지의 실험처럼 무작위 마스킹 대신 인과적 마스킹을 사용하는 감각운동 사전 훈련 접근 방식의 변형을 실험합니다.우리는 최소한의 변경을 하고 샘플링된 마스크 시퀀스를 간단히 정렬합니다.자기 주의는 여전히 인과적이기보다는 양방향입니다.그러나 사전 훈련 예측 문제는 인과적입니다.우리는 두 가지 설정을 고려합니다.같은 작업에 대한 사전 훈련(픽 투 픽)과 다른 작업에 대한 사전 훈련(픽 투 스택).그림 8에 결과를 보고합니다.인과적 사전 훈련은 같은 작업에서는 더 나은 성능을 가져오지만 다른 작업으로의 전이는 더 나쁘다는 것을 발견했습니다.5 관련 작업 성공(%)마스킹 무작위 인과적 픽 픽 픽 → 스택 그림 8: 인과적 마스킹.로봇공학에서의 자기 지도 학습.로봇공학에서의 자기 지도 학습에 대한 풍부한 연구가 있습니다. [16, 17] 대규모 자기 감독으로부터 파악 정책을 학습합니다.[18] 자기 감독 시각적 대응으로 시각 운동 정책을 학습합니다.[19] 파악 성공 피드백을 기반으로 특정 개체에 대한 학습된 파악 정책을 보정합니다.[20] 동시에 여러 작업에 대한 로봇 궤적을 수집하고 해당 정책을 학습합니다.[21] 인간 시연에 보조 대조 학습 작업을 사용합니다.[22] 원격 조작 데이터에서 목표 조건 로봇 정책을 학습합니다.[23] 모델 학습을 위해 다중 뷰 재구성을 사용합니다.전반적으로 이전 연구는 주로 특정 로봇 작업을 학습하기 위해 자기 감독을 사용하는 데 중점을 두었습니다.반대로, 우리는 다른 다운스트림 작업으로 전송할 수 있는 일반적인 감각 운동 표현을 사전 학습하기 위해 자기 감독을 사용합니다.로봇공학을 위한 다중 작업 및 대규모 모델.많은 연구에서 로봇공학을 위한 다중 작업 및 대규모 모델을 학습하는 것을 탐구했습니다.BC-Z [24]는 대규모 비전 기반 모방 학습을 수행합니다.[25] 교차 도메인 데이터 세트로 정책을 학습합니다. [26] 언어 조작을 위한 다중 작업 Transformer 정책을 학습합니다.[27] 오프라인 강화 학습으로 사전 학습합니다.[28] 다른 작업의 궤적에 대해 공동으로 일반 에이전트를 학습합니다.반대로, 우리는 가면을 쓴 목적을 사용하고 잠재적인 시각적 표현에 대해 작업하며 실제 궤적에 집중합니다.[29] 인간이나 전문가 시연에서 언어 조건화된 Transformer 정책을 학습합니다.마찬가지로, 우리는 Transformer 모델을 활용하지만 감각 운동 궤적에서 자기 감독 학습에 집중합니다.[30] 구체화된 시각적 질문 답변 모델을 통해 시각적 입력으로 언어 모델을 구축합니다.전반적으로, 우리는 다중 작업 로봇 학습과 로봇 공학을 위한 대규모 모델 학습이라는 목표를 공유하지만 실제 감각 운동 궤적에서만 학습하는 일반적인 자기 감독 사전 학습 방식을 제안합니다.6 토론 제한 사항.우리는 우리의 작업에 몇 가지 중요한 제한 사항이 있음을 알고 있습니다.첫째, 우리의 데이터 세트는 단일 연구실에서 단일 로봇을 사용하여 수집되므로 데이터의 다양성과 사실성이 제한됩니다. 더욱 다양한 환경과 로봇으로의 확장은 미래 연구의 중요한 영역으로 남아 있습니다. 둘째, 사전 훈련과 미세 조정에서 고려하는 작업은 픽 앤 플레이스의 비교적 간단한 변형입니다. 복잡한 역학과 접촉을 가진 더욱 능숙한 작업을 탐색하는 것이 좋을 것입니다. 다음으로, 우리 모델은 MSE 손실로 사전 훈련되었으며 다중 모달리티에 어려움을 겪을 수 있습니다. 이는 큰 컨텍스트 길이에 대한 조건부 예측으로 부분적으로 완화됩니다. 그럼에도 불구하고 생성 모델 변형을 탐색하는 것이 좋을 것입니다. 마지막으로 실패 사례의 비디오는 프로젝트 페이지를 참조하세요.
--- CONCLUSION ---
. 우리는 감각 운동 사전 훈련을 통한 로봇 학습에 대한 접근 방식을 설명합니다. 우리 모델은 감각 운동 토큰 시퀀스에서 작동하는 Transformer입니다. 우리는 마스크 예측을 통해 모델을 사전 훈련합니다. 우리는 이 데이터에 대한 사전 훈련이 처음부터 훈련하는 것보다 지속적으로 성능이 뛰어나고, 블록 스태킹 작업에서 2배의 개선으로 이어지고, 유리한 스케일링 속성이 있다는 것을 발견했습니다. 마지막으로, 우리는 다양한 작업, 실험실 환경 및 로봇에서 성공적인 전송을 보여줍니다. 작업 공간 설정 궤적 수 Pick Bin Pick Stack Destack Pick One Object Bin Picking Stack &amp; Destack 그림 9: 데이터 세트. 우리는 Franka 로봇 팔을 사용하여 20,000개가 넘는 실제 궤적 데이터 세트를 수집했습니다. 각 궤적은 세 개의 카메라(하나는 손에 부착되고 두 개는 측면에 부착됨), 고유 감각 로봇 상태 및 동작에서 얻은 고품질 이미지 시퀀스입니다. 우리는 객체 위치, 모양 및 외관에 변화가 있는 피킹, 빈 비킹, 디스태킹 및 스태킹 작업을 고려합니다. 부록 A: 데이터 수집 하드웨어. 우리는 7-DOF 암과 평행한 턱 그리퍼(그림 9, 왼쪽 위)가 있는 Franka 로봇을 사용했습니다. 우리는 관절 위치와 속도의 형태로 고유 감각 정보를 기록했습니다. 작업 공간에는 고품질 Logitech Brio 컬러 카메라 3개가 장착되어 있습니다. 하나의 자기 중심 카메라가 로봇 손에 부착되고 두 개의 외중심 카메라가 로봇의 좌우측에 부착됩니다. 우리는 카메라와 로봇의 데이터 스트림을 60Hz로 동기화하고 30Hz로 데이터를 저장했습니다. (1) 픽: 작업은 테이블에서 K개의 큐브 중 하나를 잡고 들어 올리는 것입니다. 로봇이 충돌하지 않는 로봇 프레임에서 무작위로 생성된 3개 DoF 포즈에서 K개의 큐브를 가져오고 각 큐브에 대해 3개 DoF 픽 앤 플레이스 포즈의 시퀀스를 생성하는 데이터 생성 체계를 스크립팅합니다. 그런 다음 스크립팅된 정책이 큐브를 잡습니다. 로봇은 파지 후 파지한 큐브를 다음 무작위로 생성된 포즈에 놓고 시작 관절 구성으로 돌아가 다음 파지를 시작합니다.(2) 빈 픽: 로봇은 부드럽고 단단한 물체가 무작위로 배치된 빈에서 물체를 집어 올립니다(그림 9 참조). [31]을 사용하여 오버헤드 깊이 카메라로 촬영한 깊이 이미지에서 파지를 생성합니다. 각 파지 전에 로봇은 빈의 카메라 뷰를 가리지 않는 관절 구성으로 이동합니다. 파지 포즈를 얻은 후 스크립트된 정책을 사용하여 물체를 집어 올립니다. 그리퍼가 궤적의 끝에서 완전히 닫히지 않으면 궤적이 성공한 것으로 간주합니다.(3) 스택 및 (4) 스택 해제: 테이블 위에 놓인 서로 다른 색상의 두 큐브로 시작하여 로봇은 먼저 큐브 중 하나를 다른 큐브로 옮긴 다음 쌓인 큐브를 다시 테이블로 옮기는 작업을 맡습니다. 큐브 포즈는 (1)과 비슷한 방식으로 생성됩니다. 스태킹의 경우, 플레이스 포즈의 높이는 큐브의 높이만큼 오프셋됩니다.마찬가지로, 디스태킹의 경우, 픽 포즈 높이는 큐브의 높이만큼 오프셋됩니다.그런 다음 스크립트화된 정책이 픽 앤 플레이스 궤적을 실행합니다.통계.9개월 동안 ~20,000개의 실제 궤적을 수집했습니다(그림 9).데이터 세트에는 단일 객체와 빈 피킹에 대한 ~7,000개의 궤적과 각각 스태킹 및 디스태킹에 대한 ~3,000개의 궤적이 포함됩니다.픽킹 및 디스태킹 궤적의 평균 길이는 ~300인 반면 스태킹 궤적은 ~600단계로 더 깁니다.데이터 세트에는 객체 포즈, 모양 및 모양의 변형이 포함되어 있습니다.그림 9에서 예시 궤적의 프레임을 보여줍니다.감사의 말 검토 프로세스 동안 도움이 되는 피드백과 제안을 해주신 익명의 검토자분들께 감사드립니다. 이 작업은 DARPA Machine Common Sense 프로그램, ONR MURI 프로그램(N00014-21-1-2801), NVIDIA, Autodesk 및 BAIR의 산업 제휴 프로그램의 지원을 받았습니다.참고문헌 [1] A. Krizhevsky, I. Sutskever 및 GE Hinton. 딥 합성곱 신경망을 사용한 Imagenet 분류.NeurIPS, 2012. [2] R. Girshick, J. Donahue, T. Darrell 및 J. Malik. 정확한 객체 감지 및 의미 분할을 위한 풍부한 기능 계층.CVPR에서, 2014. [3] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan 및 I. Sutskever. 픽셀에서의 생성적 사전 학습.ICML에서, 2020. [4] K. He, X. Chen, S. Xie, Y. Li, P. Dollár 및 R. Girshick. 마스크 자동 인코더는 확장 가능한 비전 학습기입니다.arXiv:2111.06377, 2021. [5] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. 생성적 사전 학습을 통한 언어 이해 향상.2018. [6] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습.NAACL-HCT, 2019. [7] TB Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. 언어 모델은 few-shot 학습기입니다. NeurIPS, 2020. [8] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik 및 T. Darrell. 마스크된 시각적 사전 훈련을 통한 실제 로봇 학습. arXiv:2210.03109, 2022. [9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, Ł. Kaiser, I. Polosukhin. 주의가 필요한 전부입니다. NeurIPS, 2017. [10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly 등. 이미지는 16x16 단어의 가치가 있습니다. 대규모 이미지 인식을 위한 변환기입니다. ICLR, 2020. [11] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu, et al. Ego4d: 3,000시간 분량의 자기 중심적 영상으로 전 세계를 여행합니다. arXiv:2110.07058, 2021. [12] D. Damen, H. Doughty, GM Farinella, A. Furnari, J. Ma, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price 및 M. Wray. 자기중심적 비전 재조정: epic-kitchens-100을 위한 수집, 파이프라인 및 과제. IJCV, 2021. [13] R. Goyal, S. Ebrahimi Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag, et al. 시각적 상식을 학습하고 평가하기 위한 &quot;something something&quot; 비디오 데이터베이스. ICCV, 2017. [14] D. Shan, J. Geng, M. Shu, DF Fouhey. 인터넷 규모에서 접촉하는 인간 손 이해. CVPR, 2020. [15] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei. Imagenet: 대규모 계층적 이미지 데이터베이스. CVPR, 2009. [16] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E.jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, et al. Qt-opt: 비전 기반 로봇 조작을 위한 확장 가능한 심층 강화 학습입니다. arXiv:1806.10293, 2018.[17] L. 핀토와 A. 굽타. 슈퍼사이징 자체 감독: 5만 번의 시도와 로봇 시간을 통해 파악하는 방법을 학습합니다. ICRA, 2016. [18] P. Florence, L. Manuelli, R. Tedrake. 시력 운동 정책 학습의 자기 감독 서신. RA-L, 2019. [19] M. Danielczuk, A. Balakrishna, DS Brown, S. Devgon 및 K. Goldberg. 탐색적 파악: 까다로운 다면체 물체를 파악하기 위한 점근 최적 알고리즘.arXiv:2011.05632, 2020. [20] D. Kalashnkov, J. Varley, Y. Chebotar, B. Swanson, R. Jonschkowski, C. Finn, S. Levine, K. Hausman. Mt-opt: 대규모 연속 멀티태스크 로봇 강화 학습.arXiv, 2021. [21] A. Zhan, R. Zhao, L. Pinto, P. Abbeel, M. Laskin. 대조적 사전 학습 및 데이터 증강을 통해 시각적 로봇 제어를 효율적으로 학습.IROS에서, 2022. [22] ZJ Cui, Y. Wang, N. Muhammad, L. Pinto, et al. 놀이에서 정책으로: 큐레이션되지 않은 로봇 데이터에서 조건부 동작 생성. arXiv:2210.10047, 2022. [23] Y. Seo, J. Kim, S. James, K. Lee, J. Shin, and P. Abbeel. 시각적 로봇 조작을 위한 다중 뷰 마스크 세계 모델. arXiv:2302.02408, 2023. [24] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z: 로봇 모방 학습을 통한 제로샷 작업 일반화. 로봇 학습 컨퍼런스, 2022. [25] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and S. Levine. 브리지 데이터: 교차 도메인 데이터 세트를 사용하여 로봇 기술의 일반화 강화. arXiv:2109.13396, 2021. [26] M. Shridhar, L. Manuelli, D. Fox. Perceiver-actor: 로봇 조작을 위한 다중 작업 변환기. 제6회 로봇 학습 컨퍼런스(CoRL) 회의록, 2022. [27] A. Kumar, A. Singh, F. Ebert, Y. Yang, C. Finn, S. Levine. 로봇을 위한 사전 학습: 오프라인 rl을 통해 소수의 시도에서 새로운 작업을 학습할 수 있음. arXiv:2210.05178, 2022. [28] S. Reed, K. Zolna, E. Parisotto, SG Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky, J. Kay, JT Springenberg 등. 일반 에이전트. arXiv:2205.06175, 2022. [29] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu, et al. Rt-1: 규모에 따른 실제 제어를 위한 로봇공학 변압기. arXiv:2212.06817, 2022. [30] D. Driess, F. Xia, MS Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, 외. Palm-e: 구현된 다중 모드 언어 모델입니다. arXiv:2303.03378, 2023. [31] V. Satish, J. Mahler 및 K. Goldberg. 완전 합성 딥 네트워크를 사용하여 로봇 파악 정책 학습을 위한 온-폴리시 데이터 세트 합성. RAL, 2019.
