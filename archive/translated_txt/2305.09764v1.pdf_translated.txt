--- ABSTRACT ---
온디바이스 자동 음성 인식 시스템은 서버 기반 시스템에 비해 여러 가지 과제에 직면합니다. 동일한 정확도를 유지하면서 속도, 디스크 크기 및 메모리 측면에서 더 엄격한 제약 조건을 충족해야 합니다. 가상 비서와 통신하고 음성을 텍스트로 변환하는 것과 같이 한 번에 여러 배포판의 여러 애플리케이션을 제공해야 하는 경우가 많습니다. 여러 애플리케이션을 제공하는 가장 간단한 솔루션은 애플리케이션별(언어) 모델을 구축하는 것이지만, 이는 메모리 증가로 이어집니다. 따라서 단일 애플리케이션 독립적 모델을 구축하기 위해 다양한 데이터 및 아키텍처 기반 언어 모델링 접근 방식을 탐구합니다. 다양한 온디바이스 제약 조건 간에 최적의 균형을 찾는 두 가지 새로운 피드포워드 아키텍처를 제안합니다. 애플리케이션별 솔루션과 비교할 때, 새로운 접근 방식 중 하나는 원래 모델의 속도와 정확도를 유지하면서 디스크 크기를 절반으로 줄입니다. 1
--- INTRODUCTION ---
기기 내 자동 음성 인식(ASR)에는 여러 가지 제약이 있습니다. 메모리와 디스크 공간을 너무 많이 소모하지 않고도 적절한 시간 내에 정확한 결과를 반환해야 합니다. 최첨단 연구는 종종 정확도에 초점을 맞추는 반면, 리소스가 제한된 애플리케이션은 성능과 크기도 처리해야 합니다. 모든 제약을 충족하는 아키텍처를 찾는 것은 쉬운 일이 아닙니다. 또 다른 과제는 ASR 시스템이 종종 다양한 요청을 처리한다는 것입니다. ASR 시스템은 기기 내 가상 비서(VA) 역할을 할 수 있지만 구술 메시지, 메모, 이메일 등도 허용합니다. 후자의 애플리케이션을 음성-텍스트(STT)라고 합니다. 일반적인 VA 요청은 &quot;버락 오바마의 나이는?&quot;과 같은 지식 기반 질문이나 &quot;레이디 가가 음악 재생&quot;과 같은 명령입니다. STT 요청은 일반적인 VA 요청보다 길고 특성이 다릅니다. VA와 STT 모두에 가장 높은 정확도를 제공하는 솔루션은 각 애플리케이션에 대해 별도의 모델을 학습하는 것이지만, 추가 모델 크기는 금지적입니다. 대신 단일 모델을 개발하는 것을 목표로 합니다. 이 논문에서 우리는 ASR 시스템의 신경망 언어 모델(NNLM)에 초점을 맞춥니다. 우리의 기준은 고정 크기 순서 망각 인코딩(FOFE) 피드포워드 NNLM(Zhang et al., 2015)입니다. ASR에서 검색 공간은 쉽게 증가할 수 있으므로 허용 가능한 지연 시간과 더 낮은 메모리에 도달하기 위해 디코딩에 사용되는 컨텍스트 길이를 제한해야 합니다. 이 짧은 컨텍스트 길이를 감안할 때, FOFE 피드포워드 LM은 정확도 측면에서 Transformer(Vaswani et al., 2017)와 경쟁할 수 있고 지연 시간 측면에서 더 나은 것으로 나타났습니다. Irie(2020)도 Transformer가 짧은 컨텍스트 길이에 덜 강력하다는 것을 보여주었습니다. 단일 Application-Agnostic(AA) NNLM을 구축하기 위해 우리는 학습 데이터를 최적으로 샘플링하는 방법을 개발했습니다. 우리는 다양한 소스에서 데이터를 샘플링합니다. 예를 들어 VA 및 STT를 위한 옵트인 사용자의 익명화되고 무작위로 샘플링된 사용자 요청과 분포의 꼬리 부분을 개선하는 데 초점을 맞춘 여러 다른 도메인에 걸친 인공 요청입니다. 데이터 기반 접근 방식은 균형 잡힌 개발 세트를 생성하고 각 데이터 소스와 해당 개발 세트의 각 애플리케이션의 중요도에 따라 샘플링 가중치를 분배하여 애플리케이션별 데이터 소스 간의 최적의 균형을 찾으려고 합니다. 결합된 데이터 세트에서 단일 FOFE NNLM을 학습하면 더 큰 모델이나 더 긴 학습을 사용하더라도 정확도가 저하될 수 있습니다. 기준 FOFE NNLM에 대한 두 가지 확장을 살펴봅니다. 첫째, 병렬 하위 네트워크의 앙상블과 모든 하위 네트워크에 걸쳐 정규화된 확률을 생성하는 혼합 하위 네트워크로 구성된 혼합 FOFE NNLM(Oualil 및 Klakow, 2017; Irie et al., 2018)입니다. 이러한 혼합 가중치는 소프트맥스 출력 전에 앙상블의 가중 평균을 계산하는 데 사용됩니다. 두 번째 확장은 각 애플리케이션에 대해 다른 하위 네트워크가 있는 애플리케이션 종속(AD) FOFE NNLM입니다. 학습 시간에 데이터와 그래디언트는 애플리케이션에 속한 해당 하위 네트워크를 통해서만 (역)전파됩니다. 추론 시간에 사용자가 ASR을 호출하는 방식은 어떤 애플리케이션이 필요한지 알려줍니다(웨이크 문구 = VA, 마이크 버튼 STT). 활성 애플리케이션에 속한 하위 네트워크만 사용됩니다. 두 접근 방식 모두 애플리케이션별 모델과 일치하거나 더 나은 성능을 낼 수 있습니다. 혼합 NNLM의 정확도는 ADNNLM보다 약간 더 좋지만 속도 측면에서는 상황이 반대입니다. 이 논문의 기여는 다음과 같습니다. = • 섹션 3에서 애플리케이션별 데이터 소스를 최적으로 결합하여 애플리케이션에 독립적인 LM을 학습하는 방법을 제안합니다. • 섹션 4에서 각각 두 개의 애플리케이션별 언어 모델의 정확도와 일치하는 두 개의 새로운 FOFE 기반 신경 LM을 제안합니다. • 섹션 6에서 새로운 NNLM의 정확도와 속도를 기준 FOFE 및 최신 Transformer 모델과 비교합니다. 우리는 미국 영어, 독일어, 중국어(만다린어)의 세 가지 언어와 세 가지 유형의 테스트 세트에 대해 이를 수행합니다(자세한 내용은 섹션 5 참조). 2
--- RELATED WORK ---
우리는 여러 도메인/작업을 동시에 모델링하는 관련 작업에 대해 논의하는 것으로 시작합니다. 많은 패턴 인식 작업은 서로 다른 범주의 데이터가 동일한 빈도로 발생하지 않기 때문에 불균형합니다. 따라서 덜 빈번한 범주는 교육 데이터에서 잘 표현되지 않습니다(Anand et al., 1993; Johnson and Khoshgoftaar, 2019). 이는 최적이 아닌 모델을 초래합니다. 데이터 불균형을 처리하기 위한 데이터 기반 접근 방식에는 과소 및 과다 샘플링이 포함됩니다(Van Hulse et al., 2007). 이러한 개선 사항
--- METHOD ---
최적의 훈련 데이터 샘플링. 다양한 소스에서 데이터를 샘플링합니다. 예를 들어, VA 및 STT에 대한 옵트인 사용자의 익명화되고 무작위로 샘플링된 사용자 요청과 분포의 꼬리 부분을 개선하는 데 초점을 맞춘 여러 다른 도메인에 걸친 인공 요청입니다. 데이터 기반 접근 방식은 균형 잡힌 개발 세트를 만들고 각 데이터 소스와 해당 개발 세트의 각 애플리케이션의 중요도에 따라 샘플링 가중치를 분배하여 애플리케이션별 데이터 소스 간의 최적의 균형을 찾으려고 합니다. 결합된 데이터 세트에서 단일 FOFE NNLM을 훈련하면 더 큰 모델이나 더 긴 훈련에서도 정확도가 저하될 수 있습니다. 기준 FOFE NNLM에 대한 두 가지 확장을 살펴봅니다. 첫째, 병렬 하위 네트워크의 앙상블과 모든 하위 네트워크에 걸쳐 정규화된 확률을 생성하는 혼합 하위 네트워크로 구성된 혼합 FOFE NNLM(Oualil 및 Klakow, 2017; Irie et al., 2018)입니다. 이러한 혼합 가중치는 소프트맥스 출력 전에 앙상블의 가중 평균을 계산하는 데 사용됩니다.두 번째 확장은 각 애플리케이션에 대해 다른 하위 네트워크가 있는 애플리케이션 종속(AD) FOFE NNLM입니다.학습 시간에 데이터와 그래디언트는 애플리케이션에 속한 해당 하위 네트워크를 통해서만 (역)전파됩니다.추론 시간에 사용자가 ASR을 호출하는 방식은 어떤 애플리케이션이 필요한지 알려줍니다(웨이크 문구 = VA, 마이크 버튼 STT).그리고 활성 애플리케이션에 속한 하위 네트워크만 사용됩니다.두 접근 방식 모두 애플리케이션별 모델과 일치하거나 더 나은 성능을 낼 수 있습니다.혼합 NNLM의 정확도는 ADNNLM보다 약간 더 좋지만 속도 측면에서는 상황이 반대입니다.이 논문의 기여는 다음과 같습니다.= • 섹션 3에서 애플리케이션별 데이터 소스를 최적으로 결합하여 애플리케이션과 무관한 LM을 학습하는 방법을 제안합니다.• 섹션 4에서 두 개의 새로운 FOFE 기반 신경 LM을 제안하며 각각 두 개의 애플리케이션별 언어 모델의 정확도와 일치합니다. • 6장에서는 새로운 NNLM의 정확도와 속도를 기준 FOFE 및 최첨단 Transformer 모델과 비교합니다.이는 미국 영어, 독일어 및 중국어의 세 가지 언어와 세 가지 유형의 테스트 세트에 대해 수행합니다(자세한 내용은 5장 참조).2 관련 작업 여러 도메인/작업을 동시에 모델링하는 것과 관련된 작업에 대해 논의하는 것으로 시작합니다.많은 패턴 인식 작업은 서로 다른 범주의 데이터가 동일한 빈도로 발생하지 않기 때문에 불균형합니다.따라서 덜 빈번한 범주는 교육 데이터에서 제대로 표현되지 않습니다(Anand et al., 1993; Johnson and Khoshgoftaar, 2019).결과적으로 최적이 아닌 모델이 생성됩니다.데이터 불균형을 해결하기 위한 데이터 기반 접근 방식에는 과소 샘플링과 과다 샘플링이 있습니다(Van Hulse et al., 2007). 이러한 방법을 개선하면 데이터를 더 지능적으로 선택할 수 있습니다(Kubat 및 Matwin, 1997; Chawla 등, 2002; Zhang 및 Mani, 2003; Barandela 등, 2004). 다른 접근 방식은 훈련 및/또는 모델 아키텍처를 수정합니다. 커리큘럼 학습(Bengio 등, 2009; Shi 등, 2015)은 훈련이 끝날 때까지 소비되는 코퍼스를 미세 조정하여 데이터를 강조합니다. Smith 등(2020)
--- EXPERIMENT ---
다중 작업 학습, 데이터 증강 및 단일 작업 모델과 결합된 분류기를 사용하여 대화 에이전트에서 여러 기술을 적절히 모델링합니다. 다양한 코퍼스의 인터리브 샘플링을 통한 균형은 (Xing et al., 2022)에서 조사되었으며, 다중 작업 및 가중 학습과 같은 모델 기반 접근 방식은 모델이 다양한 코퍼스의 영향을 자체 제어할 수 있도록 합니다. 모델링 능력을 높이는 다른 방법으로는 전문가 혼합(Shazeer et al., 2017; Zhou et al., 2022) 또는 앙상블 네트워크(Oualil and Klakow, 2017; Irie et al., 2018; Ganaie et al., 2022)를 사용하는 것입니다. 언어 모델링을 위한 아키텍처 선택도 연구의 반복적인 주제였습니다. 초기 신경 LM은 피드포워드 계층을 사용합니다(Schwenk and Gauvain, 2002; Bengio et al., 2003). Mikolov 등(2010)은 원칙적으로 무제한 히스토리를 사용할 수 있는 순환 신경망 LM을 도입했습니다. 이러한 네트워크는 시간에 따른 역전파로 학습되어 기울기 계산을 위해 시간에 따라 네트워크를 &#39;펼치지만&#39; 이로 인해 기울기가 사라지고(Bengio 등, 1993; Pascanu 등, 2013) 본질적으로 학습할 수 있는 히스토리가 제한됩니다. 게이트 순환 아키텍처(Sundermeyer 등, 2012; Cho 등, 2014)는 이 문제를 완화합니다. 최근 피드포워드 아키텍처의 확장이 제안되어 다양한 단점을 완화했습니다. Zhang 등(2015)은 단어 순서를 포착하는 고정 길이의 벡터로 단어 시퀀스를 나타내는 FOFE를 제안했습니다. 그들은 FOFE 인코딩을 사용한 피드포워드 네트워크가 언어 모델링에서 순환 모델보다 성능이 우수함을 보여줍니다. 최근 몇 년 동안 가장 널리 사용된 아키텍처는 피드포워드 계층과 멀티헤드 어텐션, 잔여 연결 및 계층 정규화(Ba et al., 2016)를 결합한 Transformer(Vaswani et al., 2017)입니다. ASR에 성공적으로 적용되었습니다(예: Irie et al., 2019; Beck et al., 2020). 이 논문에서는 FOFE 피드포워드 LM을 Transformer LM 및 기본 FOFE 피드포워드 LM의 두 가지 확장과 비교합니다. 3 데이터 밸런싱 이 논문의 ASR 시스템은 매우 다른 언어 패턴을 관찰하는 두 가지 응용 프로그램인 VA와 STT를 제공합니다. 이러한 차이점을 보여주기 위해 두 가지 영어 개발 세트에 대한 통계를 계산합니다. 각 데이터 세트에는 23,000개의 익명화된 쿼리가 포함되어 있으며 2་ྲ에서 설명한 테스트 세트와 유사하게 실제 사용자 데이터에서 무작위로 샘플링됩니다. x개의 단어(로그)가 있는 쿼리 수 10ºזי •VA STT 80 100 120쿼리당 단어 수 그림 1: 영어 VA 및 STT 개발 세트에서 x개의 단어(x축)가 있는 쿼리 수(로그 스케일의 y축). wakewakethe 1S를 호출하여 I you and it that에 대해 what을 텍스트로 보냅니다. VA STT 2,000 4,000 6,000 8,000 10, 그림 2: 영어 VA 및 STT 개발 세트에서 가장 빈번하게 나타나는 10개 단어의 합집합 개수. &quot;wake2&quot; 및 &quot;wakel&quot;은 &quot;<wakeword_2> 그리고 &quot;<wakeword_1> . 섹션 5. VA 요청은 일반적으로 STT 요청보다 짧습니다. 그림 1에서 두 데이터 세트에 대해 x개의 단어가 있는 쿼리 수(로그 스케일)를 표시합니다. 예를 들어, VA 개발 세트에는 두 단어만 있는 요청이 9968개 있습니다(238개 요청은 &quot;로만 구성됨).<wakeword_1><wakeword_2> &quot;), STT 테스트 세트에는 두 단어가 포함된 요청이 1327개 포함되어 있습니다. 30개 이상의 단어로 구성된 요청을 &#39;긴&#39; 요청으로 정의하면 STT 테스트에는 긴 요청이 2030개 있는 반면 VA에는 긴 요청이 21개에 불과합니다. 둘째, 요청의 내용과 스타일은 두 애플리케이션 간에 다릅니다. 그림 2는 각 데이터 세트에서 가장 자주 사용되는 상위 10개 단어를 VA 개발 세트의 빈도 순으로 정렬하여 합친 것입니다. 사용자가 구두점을 지정할 수 있도록 허용하므로 단어 목록에 점이 있습니다. 이 분포에서 VA 쿼리는 종종 질문(what) 또는 명령(call, text)인 반면 STT 쿼리는 종종 구두점으로 메시지를 더 읽기 쉽게 만들고자 하는 사용자(I, you)의 관점에서 본 메시지라는 것이 분명합니다. 이 두 애플리케이션의 언어적 특성이 다르기 때문에 NNLM 학습 데이터를 균형 있게 조정하면 모델의 품질에 큰 영향을 미칩니다. 각 NNLM 샘플링 가중치를 결정하는 일반적인 전략은 응용 프로그램은 각 데이터 소스에서 개별 n-gram LM을 학습하고 개발 세트에서 최적의 선형 보간 가중치를 기반으로 관련성 가중치를 선택하는 것입니다(Raju et al., 2019). 저희의 설정에서 응용 프로그램별 텍스트 소스에 대한 샘플링 가중치는 선형 결합 대신 카운트 병합 가중치(Bacchiani et al., 2006; Hsu, 2007; Pusateri et al., 2019)에서 파생됩니다. 저희는 두 응용 프로그램 모두에 이로운 I 텍스트 소스에 대한 샘플링 가중치를 도출하기 위한 밸런싱 방식을 제안합니다. 저희는 대략 동일한 양의 VA 및 STT 데이터를 포함하는 균형 잡힌 개발 세트를 만듭니다. a₁,..., a1 = [0,1]이 샘플링 가중치가 되어 Σ(±1 αi = = 1이고 p(i) = {D, A}가 되어 텍스트 소스가 STT 또는 VA에 속하는지 여부를 나타냅니다. STT 및 VA에 대한 재분배 확률 질량 BD 및 ẞA는 각각 공동 애플리케이션을 제공하도록 계산됩니다. 이러한 확률 질량은 균형 잡힌 개발 세트에서 선형 애플리케이션별(AS) 언어 모델 조합의 복잡도를 최소화하는 최적 가중치에 의해 결정됩니다. 각 애플리케이션에서 할당하는 애플리케이션별 확률 질량은 다음과 같이 공식화할 수 있습니다. αD == Σαπ 및 α A == Σ α. i,p(i)=D i,p(i)=A 이제 재분배와 애플리케이션별 확률 질량 간의 비율을 고려합니다. YA== ΒΑ QA BD 및 YD == 이러한 비율은 균형을 달성하기 위해 원래 샘플링 가중치의 크기 조정을 결정합니다. 균형 잡힌 샘플링 가중치는 다음과 같은 재정규화에 의해 결정됩니다. 확장된 샘플링 가중치: λi = Yp(i) ai i = 1,..., I. ΣΎρωα; j NNLM 트레이닝을 위한 홀드아웃 및 트레이닝 세트는 균형 샘플링 가중치에 따라 텍스트 소스에서 무작위로 샘플링됩니다. 4 애플리케이션 독립적 및 애플리케이션 종속 FOFE NNLMS 이 섹션에서는 디바이스 내 ASR을 위한 세 가지 유형의 NNLM 아키텍처를 소개합니다. 다음에서 w := W1,..., WN을 단어 시퀀스로 둡니다. 여기에서 고려한 모든 NNLM 아키텍처는 유사한 방식을 따릅니다. 각 아키텍처에서 단어 임베딩 뒤에 FOFE 계층이 옵니다(Zhang et al., 2015). FOFE 계층의 망각 요소를 a &gt; 0으로 하고 단어 wm의 단어 임베딩을 em으로 하면 zm := Zm−1+α • em은 FOFE 인코딩을 생성합니다. 그 후, 각 위치에 대해 n개의 후속 FOFE 인코딩을 연결하여 FOFE 인코딩의 ngram 컨텍스트가 생성됩니다: Zm−n+1, . . ., Zm. 그 다음, 이 컨텍스트가 평면화되어 숨겨진 계층으로 전달됩니다. 그림 3a에 표시된 기준 FOFE NNLM은 평면화된 FOFE n-gram 컨텍스트에 피드포워드 계층의 스택을 적용합니다. 마지막 피드포워드 계층의 출력은 최종 소프트맥스 계층 전에 차원 감소를 위해 투영 계층에 공급됩니다. 이 아키텍처는 각 애플리케이션이 자체 NNLM을 갖는 AS-NNLM과 두 애플리케이션 모두에 대한 균형 잡힌 데이터에서 학습된 Application-Agnostic(AA) NNLM에 사용됩니다. 그림 3b는 M개의 병렬 하위 네트워크와 혼합 하위 네트워크가 있는 혼합 NNLM을 보여줍니다. 각 하위 네트워크는 피드포워드 계층의 스택입니다. 혼합 하위 네트워크도 각 병렬 하위 네트워크에 대한 혼합 가중치를 생성하기 위해 차원 M의 소프트맥스 출력으로 마무리되는 피드포워드 계층의 스택으로, 혼합이 FOFE 네트워크를 결합한다는 점을 제외하면 (Oualil 및 Klakow, 2017; Irie et al., 2018; Zhou et al., 2022)와 유사합니다. 후속 계층은 혼합 하위 네트워크 소프트맥스 출력의 해당 가중치로 조정된 모든 병렬 하위 네트워크의 출력을 평균화합니다. 그림 3c는 애플리케이션 종속 NNLM(AD-NNLM)을 보여줍니다. 이 아키텍처는 애플리케이션 정보를 사용하여 멀티태스크 스타일로 NNLM을 학습합니다. 이 NNLM에는 각 애플리케이션에 대한 별도의 하위 네트워크와 소프트맥스 출력 바이어스가 있습니다. 학습을 위해 멀티태스크 방식을 따릅니다. 각 데이터 샘플에 대한 애플리케이션의 정보는 알려져 있으며 애플리케이션에 해당하는 하위 네트워크와 소프트맥스 출력 바이어스를 선택하고 NNLM의 일부만 역전파하는 데 사용됩니다. 추론 시점에 데이터는 활성 애플리케이션에 속하는 해당 하위 네트워크와 소프트맥스 출력 바이어스를 통해 전달됩니다. 단어 수준 NNLM은 임베딩에서 대부분의 매개변수를 보유합니다. 따라서 혼합 및 AD-NNLM의 디스크 크기는 기준 아키텍처와 비교하여 약간 증가해야 합니다. 또한 AD-NNLM 속도는 추론 시점에 기준 아키텍처와 동일하므로 증가하지 않아야 합니다. 5 실험 설정 LM의 학습 데이터는 다양한 데이터 소스로 구성됩니다. VA와 STT에서 익명화되고 무작위로 샘플링된 사용자 요청은 수동 또는 자동으로 필사되며, 합성 테일 중심 데이터 세트도 있습니다. 후자의 경우 도메인 종속 템플릿과 해당 슬롯을 채울 수 있는 엔터티 목록에서 샘플링하는데, 둘 다 실제 사용자 데이터에서 파생되었습니다. 서론에서 언급했듯이 미국 영어, 독일어, 중국어(만다린)의 세 가지 언어에 대한 NNLM을 학습합니다. NNLM의 경우 섹션 3에 설명된 방법에 따라 가중치를 얻습니다. AS 모델의 경우 6B개의 단어를 샘플링하고 AA 및 AD 모델의 경우 12B개의 단어를 샘플링합니다. 베이지안 하이퍼파라미터 최적화를 실행하고 최적의 크기-정확도 트레이드오프에 따라 최종 값을 선택합니다. 결과적으로 모델의 매개변수 수가 약간 다르지만 섹션 6에서 이것이 결과에 눈에 띄는 영향을 미치지 않는다는 것을 보여줍니다. 모든 모델은 피드포워드 계층이 4개이고 임베딩 크기가 256입니다. 디스크 크기를 줄이기 위해 입력 및 출력 임베딩 가중치를 연결합니다(Press 및 Wolf, 2017). 숨겨진 크기는 기본 FOFE 모델의 경우 768, AD FOFE 및 혼합 FOFE의 경우 512, Transformer의 경우 256입니다. Transformer는 Vaswani et al.(2017)과 동일한 구성을 가지고 있으며 크기가 256인 4개의 어텐션 헤드를 사용합니다. 가장 자주 등장하는 상위 100k 단어를 어휘로 사용합니다. 학습 속도를 높이기 위해 잡음 대조 추정(NCE)(Liza 및 Grzes, 2017)을 사용하는데, 이는 추론 중에 소프트맥스로 대체됩니다. NNLM을 Block Momentum Stochastic Gradient Descent(Chen 및 Huo, 2016)로 학습하며, AS, AA 및 AD FOFE의 경우 초기 학습 속도가 0.256이고 AA Mixture FOFE의 경우 초기 학습 속도가 1.024입니다. AS 모델의 경우 최적화는 64에포크 후에 수렴하지만 AA 및 AD 모델의 경우 최적이 128에포크로 지연됩니다. 초기 임베딩 FOFE n-그램 컨텍스트 피드 포워드 소프트맥스 •HHH h 임베딩 FOFE n-그램 컨텍스트 hm 임베딩 혼합 FFFF N W₁...., WN FoFE n-그램 컨텍스트 FF1 W₁+. ... + FFN WN Assistant Dictation 소프트맥스 소프트맥스 Wm(a) Base. W m (b) 혼합 W &#39;m (c) 응용 프로그램에 따라 다름.그림 3: wm과 history hm을 위치 m의 단어와 history라고 하자.학습 속도는 AS의 경우 16에포크, 다른 모델의 경우 에포크 동안 고정하고, 홀드아웃 퍼플렉시티가 4에포크 동안 증가하면 학습 속도 감소를 0.7로 적용한다.학습을 안정화하기 위해 클립 노름 6.0을 적용하고 NCE 샘플 수를 4096으로 설정한다.평가를 위해 세 가지 유형의 테스트 세트에서 테스트한다.(1) VA 및 (2) STT는 VA/STT에서 관찰한 분포에 따라 샘플링된 사용자 요청으로 구성되며 따라서 많은 헤드 쿼리를 포함한다.(3) Tail은 테일 엔터티가 있는 쿼리에 초점을 맞추도록 설계되었다.이것들은 사용자 데이터에서 자주 발생하지 않으므로 Tail은 합성 학습 데이터를 생성하는 동일한 템플릿과 엔터티 목록에서 샘플링된 합성 요청으로 구성된다. 요청은 음악, 스포츠, 홈 오토메이션과 같은 다양한 도메인을 포괄하며 오디오는 Text-to-Speech를 사용하여 생성됩니다.표 1은 각 테스트 세트의 단어 수를 보여줍니다.우리는 단어 오류율(WER)을 사용하여 모델의 정확도를 평가하고 P95 실시간 계수(RTF)를 사용하여 지연 시간을 평가합니다.y가 오디오 신호의 지속 시간이고 x가 y를 디코딩하는 데 걸리는 시간인 경우 RTF는 x/y로 정의됩니다.P95는 95번째 백분위수를 나타내므로 가장 어려운 쿼리의 지연 시간을 포착합니다.우리는 각 테스트를 세 번 실행하고 RTF 숫자의 평균을 내어 이상치를 포착합니다.ASR 시스템은 (Huang et al., 2020; Pratap et al., 2020)에 설명된 대로 딥 합성곱 신경망 음향 모델(AM)을 사용합니다.AS 모델의 경우 VA 및 Tail 테스트 세트를 VA 특정 NNLM으로 디코딩하고 STT 테스트 세트를 STT 특정 NNLM으로 디코딩합니다. 디코딩하는 동안 NNLM의 컨텍스트 길이는 8단어로 제한됩니다.VA STT Tail 영어 226k 292k 454k 독일어 130k 154k 204k 중국어 221k 219k 368k 표 1: 언어별 테스트 세트당 단어 수.온디바이스 ASR의 메모리 및 대기 시간 제약 조건을 충족합니다.최적화된 가중치를 사용하여 AM 점수와 NNLM 점수를 결합하는 단일 디코딩 패스를 수행합니다.테일 데이터로 학습된 n-그램 LM으로 NNLM을 보간하고 재채점 패스를 추가하여 더 나은 WERS를 얻을 수 있지만, 다양한 신경 아키텍처를 사용하는 영향을 비교하고자 하므로 비교를 모호하게 할 수 있는 모든 요소를 제거합니다.6 결과 먼저 다양한 신경 아키텍처의 정확도를 평가합니다.표 2에서는 VA, STT 및 Tail 테스트 세트의 다양한 모델에 대한 WER과 디스크의 크기를 추정하기 위한 모델의 매개변수 수를 보고합니다. AS FOFE 모델의 경우 VA+Tail용 모델과 STT용 모델이라는 두 개의 별도 모델을 학습했기 때문에 AA FOFE 모델보다 매개변수가 두 배 더 많습니다.먼저 AS에서 AA FOFE로 이동하여 매개변수 수를 절반으로 줄이면 어떤 경우 WER이 1.5-3.8% 저하됩니다.두 번째로 Transformer 아키텍처는 FOFE 기반 모델과 유사한 베이지안 최적화를 사용하여 최적화되었지만 혼합된 결과를 제공합니다.영어 VA 및 STT의 경우 LM #Par VA STT Tail LM VA STT Tail 영어 영어 AS FOFE 58M 4.AA FOFE 29M AA Transf. 27M AA M-FOFE 37M 광고 FOFE 31M 3.68 17.4.11 3.68 17.3.99 3.56 47.3.99 3.56 17.3.99 3.62 17.AA Transf. AA M-FOFE AD FOFE -18.00 -21.75 -11.-23.79 -31.54 -17.7.-8.4.German AA Transf. 독일어 AS FOFE 58M 5.6.47 29.AD FOFE -19.59 -13.92 -24.AA M-FOFE -17.77 -31.45 -79.7.84 3.41 5.AA FOFE 29M 5.AA Transf. 27M 11.23.6.35 29.34.Mandarin AA Transf. AA M-FOFE 37M 5.AD FOFE 31M 5.6.26 30.6.33 32.AA M-FOFE AD FOFE -10.06 -14.04 -8.-9.89 -30.21 -36.-2.11 1.63 -3.Mandarin AS FOFE 58M AA FOFE 29M AA Transf. 27M AA M-FOFE 37M 5.17 6.04 39.5.25 6.27 38.8.88 13.29 40.5.13 5.94 38.AD FOFE 31M 5.12 6.05 36.표 3: 지연 결과: 영어, 독일어 및 중국어 설정에 대한 VA, STT 및 Tail 엔터티 테스트 세트에 대한 AA FOFE 모델과 관련된 상대적 P95 RTF 감소.AA = 응용 프로그램 독립적, AD 응용 프로그램 종속, Transf. = 변압기, MFOFE = 혼합 FOFE = 중국어(매개변수 수 동일) AS FOFE 68M 5.14 6.00 39.AA FOFE 34M AA Transf. 34M AA M-FOFE 34M AD FOFE 34M 5.26 6.27 38.9.03 13.40 40.5.10 6.02 38.5.12 5.98 36.표 2: 영어, 독일어, 중국어 설정에 대한 VA, STT 및 Tail 엔티티 테스트 세트의 매개변수 수(#Par) 및 WERS.AS = 애플리케이션별, AA = 애플리케이션 독립적, AD = 애플리케이션 종속, 전송 변환기, MFOFE = 혼합 FOFE.= 다른 모든 설정에서는 큰 저하가 나타나는 반면 WER 개선이 관찰됩니다.AD FOFE 모델은 모든 언어의 VA에서 가장 정확한 정확도를 제공하는 반면 AA 혼합 FOFE는 STT에서 가장 정확한 정확도를 제공하지만 두 아키텍처 간의 차이는 작습니다.거의 모든 경우에서 기준 AS/AA FOFE 및 변환기 모델보다 성능이 우수합니다. 유일한 예외는 영어와 독일어 Tail 테스트 세트입니다.AS FOFE 모델은 여전히 가장 높은 정확도를 달성하는데, 아마도 드물게 발생하는 쿼리가 매개변수 수를 두 배로 늘리는 데 가장 큰 이점을 얻기 때문일 것입니다.섹션 5에서 설명한 대로 최적의 정확도-크기 트레이드오프를 기반으로 하이퍼 매개변수를 선택합니다.결과적으로 표 2 상단의 모델 매개변수 수는 정확히 같지 않습니다.작은 크기 차이가 결과에 큰 영향을 미치지 않도록 각각 34M 매개변수가 있는 중국어 모델에 대한 결과를 평가하고 표 2 하단에 결과를 추가했습니다.동일한 추세를 관찰했습니다.AD FOFE와 AA Mixture FOFE가 가장 좋은 결과를 제공합니다.매개변수 수를 늘리면 더 나은 결과가 나오지 않는다는 것을 확인합니다.마지막으로 표 3의 기준 AA FOFE 모델과 비교한 P95 RTF(P50 RTF는 동일한 추세를 보임)의 상대적 변화를 보고합니다.RTF는 하드웨어에 따라 달라지므로 주로 기준과 비교한 상대적 변화에 관심이 있습니다. Transformer와 Mixture FOFE가 모두 기준선보다 상당히 느리다는 것을 관찰했습니다. 영어 테스트 세트의 경우 Transformer가 Mixture FOFE보다 빠른 반면, 독일어와 중국어의 경우 속도는 테스트 세트에 따라 다릅니다. AD FOFE는 제안된 모델 중 가장 빠른 추론 속도를 제공하며 영어 VA와 모든 독일어 테스트 세트에서 vanilla FOFE보다 성능이 뛰어나며 다른 설정에서는 저하가 제한적입니다. 7
--- CONCLUSION ---
우리는 디스크 크기를 약 절반으로 줄이면서 애플리케이션별 NNLMS와 동일한 정확도와 속도로 VA 및 STT 요청을 모두 처리할 수 있는 단일 NNLM을 개발하는 것을 목표로 합니다. 우리는 VA 및 STT 애플리케이션의 데이터를 최적으로 균형 잡는 방법을 개발하고 두 가지 새로운 FOFE 피드포워드 아키텍처를 제안합니다. 애플리케이션 독립적 혼합 FOFE와 애플리케이션 종속 FOFE는 모두 정확도 측면에서 기준 FOFE 및 Transformer 모델보다 성능이 뛰어나고 후자는 대기 시간 측면에서도 경쟁력이 있습니다. 제한 사항 제안된 두 모델(AD FOFE 및 AA FOFE 혼합)은 이 논문에서 언급한 것보다 더 많은 언어에서 테스트되었지만 Transformer 모델과의 비교는 모든 언어에서 수행되지 않았습니다. 이 논문에서는 단어 수준 LM만 사용합니다. 우리는 하위 단어 수준 LM으로 예비 실험을 수행했지만 적절한 결론을 도출하려면 더 광범위한 조사가 필요합니다. 윤리적 성명 이 논문은 실제 VA의 LM에 초점을 맞추고 있기 때문에 결과를 정확하게 재현할 수 없습니다. 예를 들어 VA와 STT 애플리케이션 모두에 사용할 수 있는 ASR, 60억 단어를 넘는 여러 언어로 된 교육 데이터와 실제 사용자 데이터에서 샘플링한 수십만 개 단어의 테스트 세트 등 설정을 모방하는 공개 데이터 세트를 알지 못합니다. 모든 데이터는 익명화되고 무작위로 샘플링되었으며, 테스트 세트를 생성하기 위한 인간 필사본은 선택한 사용자 데이터에서만 수행됩니다. 감사의 말 이 논문을 검토해 주신 Barry Theobald, Arturo Argueta, Thiago Fraga Da Silva에게 감사드립니다. 참고문헌 Rangachari Anand, Kishan G. Mehrotra, Chilukuri K. Mohan, Sanjay Ranka. 1993. 불균형 교육 세트의 신경망 분류를 위한 개선된 알고리즘. IEEE 신경망 거래, 4(6):962-969. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton. 2016. 레이어 정규화. arXiv 사전 인쇄본 arXiv:1607.06450. Michiel Bacchiani, Michael Riley, Brian Roark, Richard Sproat. 2006. 확률적 문법의 MAP 적응. Computer Speech and Language, 20:4168. Ricardo Barandela, Rosa M. Valdovinos, J. Salvador Sánchez, Francesc J. Ferri. 2004. 불균형 학습 샘플 문제: 샘플링 부족 또는 과다? 패턴 인식(SPR) 및 구조 및 구문적 패턴 인식(SSPR)에 대한 통계적 기법에 대한 공동 IAPR 국제 워크숍, 806-814페이지. Eugen Beck, Ralf Schlüter, Hermann Ney. 2020. Transformer 언어 모델을 사용한 LVCSR에서. Proceedings Interspeech, 1798-1802쪽. Yoshua Bengio, Réjean Ducharme, Pascal Vincent, Christian Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3:1137-1155. Yoshua Bengio, Paolo Frasconi, Patrice Simard. 1993. The problem of learning long-term dependencies in recurrent networks. In Proceedings of the IEEE International Conference on Neural Networks, 1183-1195쪽. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, Jason Weston. 2009. Curriculum learning. In Proceedings of the Annual International Conference on Machine Learning (ICML), 382권, 41-48쪽. Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, W. Philip Kegelmeyer. 2002. SMOTE: 합성 소수 과다 샘플링 기술. Journal of Artificial Intelligence Research, 16:321–357. Kai Chen과 Qiang Huo. 2016. 블록 내 병렬 최적화 및 블록별 모델 업데이트 필터링을 사용한 증분 블록 학습을 통한 딥 러닝 머신의 확장 가능한 학습. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP) 회의록, 5880-5884쪽. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio. 2014. 통계적 기계 번역을 위한 RNN 인코더 디코더를 사용한 구문 표현 학습. 자연어 처리 경험적 방법(EMNLP) 회의록, 1724-1734쪽. MA Ganaie, Minghui Hu, AK Malik, M. Tanveer, PN Suganthan. 2022. 앙상블 딥 러닝: 리뷰. 인공 지능의 공학적 응용, 115쪽. Bo-June Hsu. 2007. 언어 모델의 일반화된 선형 보간. IEEE 자동 음성 인식 및 이해 워크숍(ASRU), 136-140쪽. Zhen Huang, Tim Ng, Leo Liu, Henry Mason, Xiaodan Zhuang, Daben Liu. 2020. SNDCNN: 음성 인식을 위한 확장된 지수 선형 단위를 사용한 딥 CNN 자체 정규화. IEEE 음향, 음성 및 신호 처리 국제 회의(ICASSP) 회의록, 6854-6858쪽. Kazuki Irie. 2020. 자동 음성 인식에서 신경 언어 모델링 발전. 박사 학위 논문, 독일 RWTH 아헨 대학교. Kazuki Irie, Shankar Kumar, Michael Nirschl, Hank Liao. 2018. RADMM: 도메인 강건 언어 모델링에 응용되는 순환 적응 혼합 모델. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP) 회의록, 6079-6083쪽. Kazuki Irie, Albert Zeyer, Ralf Schlüter, Hermann Ney. 2019. 딥 트랜스포머를 사용한 언어 모델링. Interspeech 회의록, 3905-3909쪽. Justin M. Johnson과 Taghi M. Khoshgoftaar. 2019. 클래스 불균형을 고려한 딥 러닝에 대한 조사. 빅데이터 저널, 6쪽. Miroslav Kubat과 Stan Matwin. 1997. 불균형 학습 세트의 저주에 대한 대처: 일방적 선택. 국제 기계 학습 컨퍼런스(ICML)의 회의록, 179-186쪽. Farhana Ferdousi Liza와 Marek Grzes. 2017. 잡음 대조 추정을 통한 언어 모델링 개선. AAAI 인공 지능 컨퍼런스에서. Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černocký, Sanjeev Khudanpur. 2010. 순환 신경망 기반 언어 모델. Interspeech 회의록, 1045-1048쪽. Youssef Oualil과 Dietrich Klakow. 2017. 언어 모델을 혼합하기 위한 신경망 접근 방식. IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스(ICASSP)의 회의록, 5710-5714쪽. Razvan Pascanu, Tomáš Mikolov, Yoshua Bengio. 2013. 순환 신경망 학습의 어려움에 관하여. 국제 기계 학습 컨퍼런스(ICML) 회의록, 1310-1318쪽. Vineel Pratap, Qiantong Xu, Jacob Kahn, Gilad Avidov, Tatiana Likhomanenko, Awni Hannun, Vitaliy Liptchinsky, Gabriel Synnaeve, Ronan Collobert. 2020. ConvNets를 사용한 온라인 음성 인식 확장. Proc. Interspeech 2020, 3376-3380쪽. Ofir Press와 Lior Wolf. 2017. 출력 임베딩을 사용하여 언어 모델 개선. 유럽 컴퓨터 언어학 협회(EACL) 회의록, 157-163쪽. Ernest Pusateri, Christophe Van Gysel, Rami Botros, Sameer Badaskar, Mirko Hannemann, Youssef Qualil, Ilya Oparin. 2019. 언어 모델 보간 기술 연결 및 비교. Proceedings Interspeech, 3500-3504페이지. Anirudh Raju, Denis Filimonov, Gautam Tiwari, Guitang Lan, Ariya Rastrow. 2019. ASR을 위한 확장 가능한 다중 코퍼스 신경 언어 모델. Proceedings Interspeech 2019, 3910-3914페이지. Holger Schwenk, Jean-Luc Gauvain. 2002. 대규모 어휘 연속 음성 인식을 위한 연결주의 언어 모델링. IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP) 회의록, 765768페이지. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, Jeff Dean. 2017. 엄청나게 큰 신경망: 희소 게이트 혼합 전문가 계층. 국제 학습 표현 컨퍼런스(ICLR)에서. Yangyang Shi, Martha A. Larson, Catholijn M. Jonker. 2015. 커리큘럼 학습을 통한 순환 신경망 언어 모델 적응. Computer Speech and Language, 33(1):136–154. Can Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, Y-Lan Boureau. 2020. 모두 합치세요: 대화형 에이전트의 기술 혼합 능력 평가. Association for Computational Linguistics(ACL) 연례 회의록, 2021-2030쪽. Martin Sundermeyer, Ralf Schlüter, Hermann Ney. 2012. 언어 모델링을 위한 LSTM 신경망. Interspeech 회의록, 194-197쪽. Jason Van Hulse, Taghi M. Khoshgoftaar, Amri Napolitano. 2007. 불균형 데이터에서 학습하는 것에 대한 실험적 관점. International Conference on Machine Learning(ICML) 회의록, 935-942쪽. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 2017. 주의만 있으면 됩니다. Neural Information Processing Systems(NeurIPS) 컨퍼런스의 진행 과정에서. Yujie Xing, Jinglun Cai, Nils Barlaug, Peng Liu, Jon Atle Gulla. 2022. 오픈 도메인 응답 생성을 위한 다중 도메인 코퍼스 학습 균형 맞추기. 북미 컴퓨터 언어학 협회(NAACL) 연례 컨퍼런스의 진행 과정에서, 2104-2120쪽. Jianping Zhang과 Inderjeet Mani. 2003. 불균형 데이터 분포에 대한 kNN 접근 방식: 정보 추출을 포함하는 사례 연구. 불균형 데이터 세트에서 학습에 대한 워크숍의 진행 과정에서, 1-7쪽. ICML. Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai. 2015. 신경망 언어 모델을 위한 고정 크기 순서적 망각 인코딩 방법. ACL(Associational Linguistics) 연례 회의록, 495-500쪽. 저우 얀치, 타오 레이, 류 한샤오, 두 난, 황 얀핑, 자오 빈센트, 다이 앤드류, 첸 지펭, 르 꾸옥, 제임스 로돈. 2022. 전문가 선택 라우팅을 사용한 전문가 혼합. CORR, abs/2202.09368.
