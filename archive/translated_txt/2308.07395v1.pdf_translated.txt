--- ABSTRACT ---
자동 음성 인식(ASR)을 위한 텍스트 주입은 쌍을 이룬 오디오-텍스트 데이터를 보완하기 위해 쌍을 이루지 않은 텍스트 전용 데이터를 사용하는 방식으로, 단어 오류율에 대한 유망한 개선이 나타났습니다. 이 연구에서는 E2E 모델에서 종종 수행하는 비-ASR 작업인 보조 작업에 대한 텍스트 주입 사용을 검토합니다. 이 작업에서 우리는 공동 엔드투엔드 및 내부 언어 모델 학습(JEIT)을 텍스트 주입 알고리즘으로 사용하여 두 가지 보조 작업을 수행하는 ASR 모델을 학습합니다. 첫 번째는 비정규화 작업인 대문자입니다. 두 번째는 턴테이킹 예측으로, 디지털 보조 상호작용에서 사용자가 대화 턴을 완료했는지 여부를 식별하려고 시도합니다. 우리는 텍스트 주입 방법이 롱테일 데이터에 대한 대문자 성능을 높이고 턴테이킹 감지 회수를 개선한다는 것을 보여주는 결과를 보여줍니다. 색인 용어: 음성 인식, 텍스트 주입, 보조 작업 1.
--- INTRODUCTION ---
자동 음성 인식(ASR)은 오랫동안 음성 받아쓰기, 디지털 보조, 비디오 자막을 포함한 중요한 기술의 필수적인 부분이었습니다[1]. ASR 시스템은 일반적으로 단어 오류율(WER)을 기준으로 평가되지만, 프로덕션 애플리케이션에서 우려되는 유일한 지표는 아닙니다. 여러 &quot;보조 작업&quot;을 전체 시스템에서 ASR 작업과 통합해야 합니다. 이러한 작업에는 다음이 포함될 수 있습니다. 가독성을 개선하는 대문자 사용 및 구두점, 저지연 시스템 구현에 중요한 음성 활동 감지(VAD) 및 쿼리 끝(EOQ) 감지, 진행 중인 대화의 리듬과 차례를 예측하는 자연스러운 대화 이해. 이 연구에서는 텍스트 주입을 통해 엔드투엔드(E2E) ASR 설정에서 이러한 보조 작업의 품질을 개선하는 데 중점을 둡니다. 음성 모델에 대한 두 가지 최신 기능을 기반으로 합니다. 첫 번째는 보조 작업을 ASR 작업과 단일 모델로 E2E 통합하는 것입니다. 과거에는 보조 작업이 일반적으로 ASR 하류의 별도 모델에서 수행되었습니다[2, 3, 4, 5]. 최근 작업에서는 엔드포인트[6, 7, 8], 대문자[9], 자연스러운 대화 이해[10], 화자 일화[11]와 같은 보조 작업을 ASR 예측과 동일한 모델에 통합하는 것을 성공적으로 탐구했습니다. 그러나 ASR과 보조 작업의 E2E 통합에는 주요 단점이 있습니다. E2E ASR 모델에 접목하면 순수한 텍스트 대 텍스트 작업(대문자 등)은 더 이상 풍부한 텍스트 전용 데이터(즉, 연관된 오디오가 없는 텍스트 데이터)에서 학습할 수 없습니다. 대신 해당 학습 예제는 쌍을 이룬 오디오-텍스트 레이블 데이터에서 사용할 수 있는 필사본으로 제한됩니다. 이는 텍스트 전용 데이터가 일반적으로 레이블이 있는 오디오 데이터보다 풍부하고 얻기 쉽고, 레이블이 있는 오디오 형태로 수집하기 어려울 수 있는 희귀 단어 및 기타 롱테일 현상에 모델을 더 쉽게 노출하는 데 사용할 수 있기 때문에 E2E 방법에 불리합니다[12]. 현재 연구를 가능하게 하는 두 번째 기능은 ASR 품질을 개선하는 수단으로 &quot;텍스트 주입&quot;을 사용하는 것입니다[13]. ASR 모델의 내부 언어 모델(ILM)은 오디오 입력과 무관하게 이전 토큰 기록을 고려하여 다음 토큰을 예측하는 네트워크의 가상 부분입니다. 일반적으로 오디오 입력의 영향을 이전 토큰 예측에서 정확히 분리하는 것은 불가능하지만 ILM 점수를 추정하기 위한 여러 방법이 개발되었습니다[14, 15]. 그런 다음 텍스트 전용 데이터를 사용하여 ASR 네트워크의 ILM 기능을 개선할 수 있습니다[16, 17]. 이 연구에서 우리는 E2E ASR 모델에서 보조 작업 성능을 개선하기 위해 텍스트 주입 기술을 활용하는 방법을 제안합니다. 그렇게 하면 보조 작업이 ASR과 공동 훈련의 다중 작업 학습 이점에 액세스하면서도 훈련 코퍼스에 풍부한 텍스트 전용 데이터를 포함할 수 있습니다. 우리는 연구를 대문자 사용과 대화 턴테이킹 예측이라는 두 가지 작업에 집중합니다. 전자는 대문자가 단순히 구어체에서 서면 도메인으로의 비정규화 형태일 뿐이고 대문자로 시작하는 단어는 다르게 발음되지 않기 때문에 강력한 텍스트 기반 작업입니다. 후자의 작업은 언어적 및 음향적 이해를 결합하는 것을 분명히 포함하며, 입력 음성의 음운론과 현재 인식의 의미론은 모두 일시 정지가 잠깐일 뿐인지 또는 사용자가 말을 마쳤는지 예측하는 데 유익합니다. 이러한 작업을 프로덕션 준비 모델인 스트리밍 E2E RNN-T ASR 모델[18, 19]에 통합합니다. 텍스트 주입이 보조 작업 성능, 특히 롱테일 설정을 의미 있게 개선할 수 있음을 보여주는 결과를 보여줍니다. 2.
--- RELATED WORK ---
보조 작업은 일반적으로 ASR과 별도의 모델에서 수행되지만 [20, 21], 보조 작업 모델링에 대한 E2E 접근 방식은 최근 프로덕션 등급 시스템에서 인기를 얻고 있습니다. 엔드포인트 [7], 대문자 사용 [9, 22], 의도된 쿼리 감지 [23, 24], 문장 분할 [25] 등을 사용한 ASR의 공동 훈련이 탐구되었습니다. 저희의 작업은 여러 병렬 레이블 시퀀스를 구축하여 ASR, 대문자 사용 및 턴테이킹 예측을 공동 훈련하는 Wang et al. [9]에 가장 밀접하게 기반을 두고 있습니다. 저희가 아는 한, 이는 텍스트 전용 데이터를 사용하여 E2E ASR 모델에서 보조 작업을 개선하려는 첫 번째 시도입니다. ASR 작업에 대해 페어링되지 않은 텍스트 데이터를 활용하는 데 오랫동안 관심이 있었습니다. ASR 인식 품질을 개선하기 위해 외부 LM을 사용하는 LM 융합에 대한 여러 접근 방식이 제안되었습니다 [26]. 이러한
--- METHOD ---
영어: 롱테일 데이터의 대문자 사용 성능을 높이고 턴테이킹 감지 회수율을 개선합니다. 색인 용어: 음성 인식, 텍스트 주입, 보조 작업 1. 서론 자동 음성 인식(ASR)은 오랫동안 음성 받아쓰기, 디지털 보조 장치, 비디오 자막을 포함한 중요한 기술의 필수적인 부분이었습니다[1]. ASR 시스템은 일반적으로 단어 오류율(WER)을 기준으로 평가되지만 프로덕션 애플리케이션에서 우려되는 유일한 지표는 아닙니다. 여러 &quot;보조 작업&quot;을 전체 시스템에서 ASR 작업과 통합해야 합니다. 이러한 작업에는 다음이 포함될 수 있습니다. 가독성을 개선하는 대문자 사용 및 구두점, 저지연 시스템 구현에 중요한 음성 활동 감지(VAD) 및 쿼리 끝(EOQ) 감지, 진행 중인 대화의 리듬과 턴테이킹 측면을 예측하는 자연스러운 대화 이해. 이 연구에서는 텍스트 주입을 통해 엔드투엔드(E2E) ASR 설정에서 이러한 보조 작업의 품질을 개선하는 데 중점을 둡니다. 음성 모델에 대한 두 가지 최신 기능을 기반으로 합니다. 첫 번째는 보조 작업과 ASR 작업을 단일 모델로 E2E로 통합하는 것입니다. 과거에는 보조 작업이 일반적으로 ASR 하류의 별도 모델에서 수행되었습니다[2, 3, 4, 5]. 최근 연구에서는 엔드포인트[6, 7, 8], 대문자[9], 자연스러운 대화 이해[10], 화자 일화[11]와 같은 보조 작업을 ASR 예측과 동일한 모델로 통합하는 것을 성공적으로 탐구했습니다. 그러나 ASR과 보조 작업의 E2E 통합에는 주요 단점이 있습니다. E2E ASR 모델로 접으면 순수한 텍스트-텍스트 작업(대문자 등)은 더 이상 풍부한 텍스트 전용 데이터(즉, 연관된 오디오가 없는 텍스트 데이터)에서 학습할 수 없습니다. 대신 해당 학습 예제는 쌍을 이룬 오디오-텍스트 레이블이 지정된 데이터에서 사용 가능한 필사본으로 제한됩니다. 이는 텍스트 전용 데이터가 일반적으로 레이블이 지정된 오디오 데이터보다 풍부하고 얻기 쉽고, 레이블이 지정된 오디오 형태로 수집하기 어려울 수 있는 희귀 단어 및 기타 롱테일 현상에 모델을 더 쉽게 노출시키는 데 사용할 수 있기 때문에 E2E 방법을 불리하게 만듭니다[12]. 현재 연구를 가능하게 하는 두 번째 기능은 ASR 품질을 개선하는 수단으로 &quot;텍스트 주입&quot;을 사용하는 것입니다[13]. ASR 모델의 내부 언어 모델(ILM)은 오디오 입력과 관계없이 이전 토큰 기록을 고려하여 다음 토큰을 예측하는 네트워크의 가상적인 부분입니다. 일반적으로 오디오 입력의 영향을 이전 토큰 예측과 정확히 분리하는 것은 불가능하지만 ILM 점수를 추정하기 위한 여러 방법이 개발되었습니다[14, 15]. 그런 다음 텍스트 전용 데이터를 사용하여 ASR 네트워크의 ILM 기능을 개선할 수 있습니다[16, 17]. 이 작업에서 우리는 E2E ASR 모델에서 보조 작업 성능을 개선하기 위해 텍스트 주입 기술을 활용하는 방법을 제안합니다. 그렇게 하면 보조 작업이 ASR과 공동 훈련의 다중 작업 학습 이점에 액세스하면서도 훈련 코퍼스에 풍부한 텍스트 전용 데이터를 포함할 수 있습니다. 우리는 연구를 대문자 사용과 대화 턴테이킹 예측이라는 두 가지 작업에 집중합니다. 전자는 대문자 사용이 단순히 구어체에서 서면 영역으로의 비정규화 형태이고 대문자로 시작하는 단어는 다르게 발음되지 않기 때문에 강력한 텍스트 기반 작업입니다. 후자의 작업은 언어적 및 음향적 이해를 결합하는 것을 분명히 포함하며 입력 음성의 음운론과 현재 인식의 의미론은 모두 일시 정지가 순간적인지 또는 사용자가 말을 마쳤는지 예측하는 데 유익합니다. 우리는 이러한 작업을 프로덕션 준비 모델인 스트리밍 E2E RNN-T ASR 모델[18, 19]에 통합합니다. 우리는 텍스트 주입이 보조 작업 성능을 의미 있게 개선할 수 있음을 보여주는 결과를 보여줍니다. 특히 롱테일 설정에서 그렇습니다. 2. 관련 연구 보조 작업은 일반적으로 ASR과 별도의 모델에서 수행되지만 [20, 21], 보조 작업 모델링에 대한 E2E 접근 방식이 최근 프로덕션 등급 시스템에서 인기를 얻고 있습니다. 엔드포인트 [7], 대문자 사용 [9, 22], 의도된 쿼리 감지 [23, 24], 문장 분할 [25] 등을 사용한 ASR의 공동 훈련이 탐구되었습니다. 저희 작업은 여러 병렬 레이블 시퀀스를 구축하여 ASR, 대문자 사용 및 턴테이킹 예측을 공동 훈련하는 Wang et al. [9]에 가장 밀접하게 기반합니다. 저희가 아는 한, 이는 텍스트 전용 데이터를 사용하여 E2E ASR 모델에서 보조 작업을 개선하려는 첫 번째 시도입니다. ASR 작업에 페어링되지 않은 텍스트 데이터를 활용하는 데 오랫동안 관심이 있었습니다. 외부 LM을 사용하여 ASR 인식 품질을 개선하는 LM 융합에 대한 여러 접근 방식이 제안되었습니다 [26]. 이러한 방법은 총 매개변수 수(LM 모델의 크기로 인해)와 추론 중 계산 비용이 증가한다는 단점이 있습니다. 텍스트 주입[13]은 LM 스타일의 비페어 텍스트 데이터를 사용하여 ASR 모델 자체를 학습함으로써 이러한 문제를 해결합니다. 일부 방법은 오디오-텍스트 데이터로 학습된 기존 ASR 모델을 미세 조정하는 데 중점을 둡니다. ASR 디코더의 ILM 적응이 잘 작동하는 것으로 나타났습니다[27, 28, 29]. 여기에서 사용하는 텍스트 주입 방법은 Meng 등이 도입한 JEIT(joint end-to-end and ILM training)입니다[30]. 가벼운 특성 때문에 JEIT를 방법으로 선택했습니다. ASR 디코더를 개선하는 데 주로 중점을 두므로 오디오 인코더의 동작이 유지되므로 표준 방법과 비교하기가 쉽습니다. 다른 방법은 고정 및 학습된 지속 시간 모델을 사용하여 텍스트 및 오디오 시퀀스를 정렬하여 텍스트 데이터를 인코더에 직접 주입합니다[16, 17]. 위의 모든 작업은 표준 및 롱테일 데이터 모두에 대한 ASR 품질을 개선하는 데 중점을 둡니다. 우리가 아는 한 이러한 기술을 보조 작업에 적용하는 것은 문헌에 대한 새로운 기여입니다. ILM 손실 JEIT 손실 HAT 손실 ASR Cap Pause ASR Cap Pause Blank Posterior Softmax Projection U Tanh Tanh Projection 시그모이드 투영 투영 3. 보조 작업 3.1. 대문자 사용 대문자 사용은 노이즈가 있는 텍스트의 올바른 대소문자(대문자 또는 소문자)를 복원하는 프로세스입니다. 특히 대문자 사용은 서면 도메인에 국한되며 구어체 음성에는 마커가 없습니다. 이 작업은 특히 장문 캡션의 경우 ASR 출력에서 가독성을 유지하는 데 중요합니다. 3.2. 대화 턴테이킹 턴테이킹은 E2E 음성 모델링을 위한 활발한 연구 분야입니다[10, 31]. 사람은 일반적으로 음성 지원자와 상호 작용할 때 말을 조절하지만[31] 대화 중 자연스러운 인간의 말투 패턴은 종종 자연스러운 유창하지 못한 말로 가득 차 있습니다. 디지털 지원 제품의 경우 음성 지원자가 말하는 사람이 응답을 기대하는 경우와 단순히 말하기를 다시 시작하려는 의도로 잠깐 멈춘 경우를 예측할 수 있는 기능이 있는 것이 바람직합니다. 우리는 이 현상을 Chang et al. [10]과 유사하게 모델링하는데, 이들은 말의 멈춤을 완전한 생각 내부 또는 완전한 생각을 마친 후로 분류합니다.즉, 사용자가 말을 멈췄을 때 모델은 짧은 멈춤 후에 말을 계속할지 아니면 시스템 응답이 예상되는지 예측해야 합니다.관심 있는 활성 영역이 오디오의 멈춤이기 때문에 이 논문에서 이 작업을 &quot;멈춤 예측&quot;이라고 합니다.4. 모델 4.1. 다중 출력 HAT 디코더 HAT는 RNN-T의 디코더 구조로, (빈) 확률이 다음 토큰 예측과 별도로 계산되어 더 정확한 ILM 추정을 용이하게 합니다 [14].Wang et al. [9]은 각 작업마다 하나씩 여러 개의 조인트 네트워크를 도입하는 HAT 디코더 변형을 제안합니다(이 경우 ASR, 대문자 사용 및 멈춤 예측).모든 병렬 조인트 네트워크는 예측 네트워크와 오디오 인코더의 기능에 따라 조건이 지정됩니다. 이 모델은 RNN-T 목적함수[18]를 사용하여 훈련되며, 각 시간 단계에서 모델은 단어 조각 토큰을 방출하거나 비방출을 나타내는 특수 토큰(공백)을 삽입하도록 선택할 수 있습니다.형식적으로 X를 입력 발화, Y를 레이블 시퀀스로 합니다.ASR 출력 공간 YASR은 {y(공백), y¹, y², ...}로 구성됩니다.T |X를 입력 수, |Y를 필사본의 길이로 합니다.음향 인코더는 f(x) = [ƒo, ..., ƒT−1], ft Є RDa를 생성하고 예측 네트워크는 g(X) = [90, ..., GU−1], gu ERP를 생성합니다.원래 HAT 구현에서와 같이 조인트 오디오 프레임과 U = = 레이블 디코더 Yunpaired Ypaired 음향 인코더 Xpaired 그림 1: JEIT 훈련을 위한 모델 다이어그램.파란색 화살표는 페어링된 오디오-텍스트 데이터의 데이터 흐름을 나타냅니다. 빨간색 화살표는 페어링되지 않은 텍스트 데이터가 네트워크를 통과하는 경로를 나타냅니다. 기준선
