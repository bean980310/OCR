--- ABSTRACT ---
텍스트 임베딩은 문장 유사성, 텍스트 클러스터링, 의미 검색과 같은 여러 NLP 애플리케이션에 유용한 기능입니다. 이 논문에서는 의미적으로 의미 있는 단어 임베딩을 생성하도록 최적화된 다국어 대규모 언어 모델인 8비트 Siamese-BLOOM 위에 대조적 목적이 있는 Low-rank Adaptation을 제시합니다. 혁신은 세 가지입니다. 첫째, BLOOM 가중치를 8비트 값으로 캐스팅합니다. 둘째, 문장 유사성 분류를 위해 확장 가능한 어댑터(LORA)와 8비트 Adam 최적화 도구를 사용하여 BLOOM을 미세 조정합니다. 셋째, 다국어 레이블이 지정된 데이터 부족을 완화하기 위해 대조적 목적이 있는 BLOOM 모델에 Siamese 아키텍처를 적용합니다. 실험 결과는 LACOS-BLOOM에서 학습된 임베딩의 품질이 모델 매개변수 수와 레이블이 지정되지 않은 교육 데이터의 양에 비례한다는 것을 보여줍니다. 매개변수 효율적 미세 조정 설계를 통해 32GB 메모리가 있는 단일 GPU 머신에서 BLOOM 71억 개의 매개변수를 엔드투엔드로 실행할 수 있습니다. 이전 솔루션인 Sentence-BERT와 비교했을 때 영어와 다국어 STS 작업 모두에서 상당한 개선을 달성했습니다. 키워드 매개변수 효율적 미세 조정, 대규모 언어 모델, 다국어 의미 유사성 임베딩 1.
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 인간과 유사한 언어를 생성할 수 있으며 질문 답변, 요약 등을 포함한 광범위한 응용 프로그램에 활용할 수 있습니다. 자연어 작업의 성능은 일반적으로 모델의 규모가 커질수록 향상됩니다[1]. 따라서 현대 언어 모델에는 수천억 개의 매개변수가 있습니다[2, 3, 4]. LLM에 대한 언급은 시퀀스의 다음 토큰을 예측하는 것이 목표인 디코더 전용 Transformer 모델에 대한 논의를 불러일으킬 가능성이 높습니다[5, 6, 2]. 그러나 텍스트 임베딩 모델도 똑같이 중요합니다. 텍스트 임베딩이라고도 하는 텍스트 표현은 인코더 기반 Transformer의 출력입니다[7, 8]. 검색 및 클러스터링과 같은 다운스트림 작업에 적용할 수 있는 텍스트의 의미를 포착하도록 설계되었습니다. Sentence-BERT[9]는 유사한 텍스트 표현을 생성하기 위한 고전적 모델입니다. BERT [7] 위에 구축된 다음 문장 쌍에 Siamese 아키텍처를 적용하여 쌍이 의역 동일한지 분류합니다. 결과적으로 유사한 맥락 단어는 더 가까운 임베딩 표현을 갖게 됩니다. ReNeuIR&#39;23: 신경 정보 검색에서 효율성 달성에 대한 워크숍, 2023년 7월 23-27일, 대만 타이베이 *연락 저자. CC wenyu_hua@apple.com (W. Hua); brian_d_williams@apple.com (B. Williams); davood@apple.com (D. Shamsi) © 2023 이 논문의 저작권은 저자에게 있습니다. 크리에이티브 커먼즈 라이선스 저작자 표시 4.0 International(CC BY 4.0)에 따라 사용할 수 있습니다. CEUR 워크숍 회의록(CEUR-WS.org) 1단계: 8비트 양자화 2단계: LORAH(h) 추가 및 정규화 피드 포워드 추가 및 정규화 멀티헤드 어텐션 임베딩(E) 3단계: 대조 학습 + 몇몇 남자들이 LORA 스포츠를 하는 Ppp입니다. 임베딩 + 풀링 hhhh가 있는 축구 경기, 여러 남자들이 하는 경기입니다. 임베딩 + 풀링 그림 1: LACOS-BLOOM 설계. 변압기 모듈을 나타내는 녹색 큐브 그룹을 고려합니다. 1단계에서 먼저 32개의 부동 소수점(녹색 큐브)에서 8비트 정수(빨간색 큐브)로 모델 매개변수를 양자화합니다. 2단계에서 매개변수(회색 큐브)를 동결하고 어댑터의 1% 미만(녹색 및 주황색 큐브)만 활성화하여 모델을 미세 조정합니다. 주황색 큐브는 조정할 어댑터 수를 나타냅니다. 3단계에서는 NLI 데이터 세트의 함축 클래스 전제 및 가설 쌍만 사용하여 학습하고, MNR 대조 목적이 있는 시암 아키텍처를 적용하여 성능을 개선합니다(보라색 큐브는 양수 쌍을 나타내고 다른 큐브는 음수 쌍을 나타냄).Sentence-BERT는 산업과 학계 모두에서 여러 응용 프로그램에서 성공적이었으며 영어만 지원하고 비교적 작은 모델입니다.BigScience Large Open-science Open-access Multilingual Language Model(BLOOM)은 2022년에 출시되었으며[10] 46개의 자연어와 13개의 프로그래밍 언어에서 학습되었습니다.학습 데이터 세트는 기능, 제한 사항, 잠재적 개선 사항, 편견, 윤리, 환경 영향 및 일반 AI 인지 연구 환경[11]과 같은 고급 주제를 둘러싼 많은 연구 질문을 다룹니다.저희가 아는 한(2023년 2월), BLOOM은 자연어 처리(NLP)에서 가장 큰 공개적으로 사용 가능한 LLM입니다.가장 큰 BLOOM에는 1,760억 개의 매개변수가 있습니다. BLOOM은 강력하지만 자연어 생성을 목표로 하는 자기회귀 언어 모델입니다. 여러 비지도 NLP 작업에서 최첨단(SOTA) 성능을 달성했지만 의미적으로 의미 있는 표현을 생성하는 것과 같은 도메인별 작업의 경우 사전 학습된 LLM을 미세 조정해야 합니다. 초기 시도는 Siamese 아키텍처로 BLOOM을 미세 조정하는 것입니다. 그러나 BLOOM은 수백 개의 GPU가 있는 클러스터에서 대규모 매개변수로 학습되므로 많은 상황에서 덜 현실적입니다. 또한 성능이 좋은 텍스트 임베딩에는 일반적으로 많은 양의 레이블이 지정된 데이터가 필요한데, 이는 다국어 레이블이 지정된 데이터의 유용성이 부족하고 비용이 많이 들기 때문에 또 다른 제한 사항입니다. 이러한 과제를 극복하기 위해 8비트 Siamese-BLOOM(LACOS-BLOOM) 위에 대조적 목적을 갖춘 Lowrank Adaptation이라는 매개변수 효율적인 미세 조정 솔루션을 제안합니다. 우리는 모델 가중치가 8비트 형식으로 고정된 bitsandbytes [12]의 작업에서 영감을 얻었습니다(71억 개의 매개변수가 있는 모델은 20Gb에서 6Gb로 줄어듬). 그런 다음 Low-Rank Adaptation(LoRA) [13]을 사용하여 매개변수의 1% 미만으로 BLOOM을 미세 조정하고 효율적인 8비트 Adam 최적화 프로그램으로 가중치를 업데이트합니다 [14]. 마지막으로 표현을 의미적으로 의미 있게 만들기 위해 Siamese 아키텍처에서 다중 부정 순위(MNR) 목표를 사용하여 단일 클래스 샘플에서 모델을 훈련합니다 [15, 16]. LACOS의 설계로 다양한 BLOOM을 단일 GPU로 실행할 수 있습니다(BLOOM 모델 매개변수는 5억 6천만(560m)에서 71억(7b1)까지). 의미 텍스트 유사성(STS) 작업 평가에서 기준 Sentence-BERT보다 상당한 개선을 달성했습니다. 다음으로, 섹션 2에서 LACOS-BLOOM 모델을 제시합니다. 이어서 섹션 3에서 실험 설정 및 결과가 이어집니다.
--- RELATED WORK ---
는 섹션 4에 있습니다. 마지막으로 작업을 마무리하고 섹션 5에서 다음 단계에 대해 논의합니다. 2. 모델 LACOS-BLOOM(그림 1)은 다국어 텍스트에 대한 의미적으로 의미 있는 표현을 생성하는 텍스트 임베딩 모델입니다. 더 적은 계산 리소스로 LACOS-BLOOM을 보다 실용적으로 만들고 고품질 표현을 생성하기 위해 여러 기술이 적용되었습니다. 여기에는 8비트 블록별 양자화를 사용하여 많은 수의 모델 가중치를 양자화하는 것이 포함됩니다. 이 모델은 확장 가능한 LORA 및 8비트 Adam 최적화기를 사용하여 미세 조정됩니다. 마지막으로 이 모델은 MNR 손실이 있는 Siamese 네트워크로 향상됩니다. 2.1. 8비트 블록 단위 양자화 3. 입력 텐서 저장 인덱스 값 1.-3.0.-1.0 0.-1.0.-0.1.-0.025 1.0.0242 1. absmax로 정규화된 블록 단위 absmax 찾기 가장 가까운 8비트 값에 대응되는 인덱스 찾기 양자화 역양자화 인덱스 값 텐서 역양자화-1.0 0.-1.0 0.3.-3.1 0.-0.0242 1.-0.0242 1.1.-0.029 1. 주어진 블록에서 absmax로 비정규화된 조회 값 그림 2: 블록 B = 2(빨간색과 녹색 블록)를 사용한 블록 단위 양자화 및 역양자화 [12]의 8비트 블록 단위 양자화를 사용합니다. 그림 2는 단계를 보여줍니다. 블록 크기 B = 2(빨간색과 녹색 블록)로 분할된 2 × 행렬. 각 블록 내에서 절대 최대값을 찾은 다음, 이 값을 사용하여 float32 가중치를 8비트 정수 값/인덱스에 매핑합니다. 가중치가 양자화되면 인덱스가 저장되어 모델의 풋프린트를 크게 줄일 수 있습니다. 모델을 업데이트하면 해당 매개변수는 적시 곱셈을 위해 32 또는 16개의 부동 소수점으로 다시 양자화 해제됩니다.
--- METHOD ---
다른 8비트 접근 방식과 달리 8비트 양자화만 저장에 사용하고 float16 또는 float32에서 계산을 수행합니다. 이를 통해 각 개별 가중치의 분포에 맞게 조정된 비선형 양자화를 사용할 수 있어 추론 성능에 영향을 주지 않고 오류를 줄일 수 있습니다. 2.2. 저랭크 적응 어댑터 접근 방식은 하위 작업에 대한 가중치 업데이트를 근사하기 위해 저랭크의 작고 학습 가능한 행렬을 활용합니다. LORA라는 접근 방식은 사전 학습된 가중치 행렬의 Eq. (1)에서 저랭크 분해를 사용하여 업데이트를 나타냅니다. W+AW=W + W down × Wup (1) 및 Eq. (1)의 분해는 두 개의 조정 가능한 행렬 Wa € Rdxr down Wup Є Rrxk, (r &lt; min(d, k))로 표현되고 [13]의 멀티헤드 어텐션 계층에서 쿼리 및 가치 투영 행렬에 적용됩니다. 이 작업에서 우리는 이전 연구[17]에서 제안한 대로 LoRA를 피드포워드 선형 계층과 마지막 은닉 임베딩 계층에 적용합니다.2.3. 시암 네트워크 LLM에서 LORA를 사용하면 도메인별 작업의 미세 조정에 성공적이었지만 미세 조정 작업의 또 다른 제한은 레이블이 지정된 데이터의 가용성입니다.이 문제를 해결하기 위해 대조 목적, 즉 MNR 손실이 있는 시암 아키텍처를 사용하는 것을 제안합니다[18].시암 아키텍처를 사용한 MNR 손실은 제한된 레이블이 지정된 데이터의 한계에도 불구하고 모델이 정확한 의미적 유사성 표현을 학습할 수 있게 하는 접근 방식입니다.미니 배치 크기 n의 시퀀스가 주어지면 P = (u1, v1), (u2, v2), ..., (Un, Un)이고, 여기서 (ui, vi)는 양의 쌍이고 (ur, vj)는 i ‡ j에 대해 음의 쌍입니다.문장 쌍은 LoRA 8비트 BLOOM을 통해 전달되어 각 토큰의 마지막 은닉 계층 임베딩을 얻습니다. 그런 다음 평균 풀링 레이어를 적용하여 문장 수준 임베딩을 생성합니다. 임베딩 쌍 (u, v) 간의 유사도 점수는 코사인 함수로 계산되고 sim(u, v)로 표시됩니다. 각 미니 배치가 주어지면 양수 쌍이 1개만 있고 다른 쌍은 음수(P로 표시)입니다(그림 1의 3단계). 목표는 Eq (2)에서 소프트맥스 정규화된 점수에 대한 음수 로그 가능도를 최소화하는 것입니다. L = Σ log (u,v)EP 3.
--- EXPERIMENT ---
결과는 LACOS-BLOOM에서 학습된 임베딩의 품질이 모델 매개변수 수와 레이블이 지정되지 않은 학습 데이터의 양에 비례한다는 것을 보여줍니다. 매개변수 효율적 미세 조정 설계를 통해 32GB 메모리가 있는 단일 GPU 머신에서 BLOOM 71억 개의 매개변수를 종단 간으로 실행할 수 있습니다. 이전 솔루션인 Sentence-BERT와 비교하여 영어와 다국어 STS 작업 모두에서 상당한 개선을 달성했습니다. 키워드 매개변수 효율적 미세 조정, 대규모 언어 모델, 다국어 의미 유사성 임베딩 1. 서론 대규모 언어 모델(LLM)은 인간과 유사한 언어를 생성할 수 있으며 질문 답변, 요약 등 광범위한 응용 분야에 활용할 수 있습니다. 자연어 작업의 성능은 일반적으로 모델의 규모가 커질수록 향상됩니다[1]. 따라서 최신 언어 모델은 수천억 개의 매개변수를 갖습니다[2, 3, 4]. LLM에 대한 언급은 시퀀스의 다음 토큰을 예측하는 것이 목적인 디코더 전용 Transformer 모델에 대한 논의를 불러일으킬 가능성이 높습니다[5, 6, 2]. 그러나 텍스트 임베딩 모델도 똑같이 중요합니다. 텍스트 임베딩이라고도 하는 텍스트 표현은 인코더 기반 Transformer[7, 8]의 출력입니다. 검색 및 클러스터링과 같은 다운스트림 작업에 적용할 수 있는 텍스트의 의미를 포착하도록 설계되었습니다. Sentence-BERT[9]는 유사한 텍스트 표현을 생성하기 위한 고전적인 모델입니다. BERT[7] 위에 구축된 다음 문장 쌍에 Siamese 아키텍처를 적용하여 쌍이 의역 동일한지 분류합니다. 결과적으로 유사한 맥락 단어는 더 가까운 임베딩 표현을 갖게 됩니다. ReNeuIR&#39;23: 신경 정보 검색의 효율성 달성 워크숍, 2023년 7월 23-27일, 대만 타이베이 *연락 저자. CC wenyu_hua@apple.com (W. Hua); brian_d_williams@apple.com (B. Williams); davood@apple.com (D. Shamsi) © 2023 이 논문의 저작권은 저자가 소유합니다. 크리에이티브 커먼즈 라이선스 저작자표시 4.0 International(CC BY 4.0)에 따라 사용할 수 있습니다. CEUR 워크숍 회의록(CEUR-WS.org) 1단계: 8비트 양자화 2단계: LORAH(h) 추가 및 정규화 피드 포워드 추가 및 정규화 멀티헤드 어텐션 임베딩(E) 3단계: 대조 학습 + 몇몇 남자들이 LORA 스포츠를 하는 Ppp입니다. 임베딩 + 풀링 hhhh가 있는 축구 경기, 여러 남자들이 하는 경기입니다. 임베딩 + 풀링 그림 1: LACOS-BLOOM 설계. 변압기 모듈을 나타내는 녹색 큐브 그룹을 고려합니다. 1단계에서는 먼저 32개의 부동 소수점(녹색 큐브)에서 모델 매개변수를 8비트 정수(빨간색 큐브)로 양자화합니다. 2단계에서는 매개변수(회색 큐브)를 동결하고 어댑터(녹색 및 주황색 큐브)의 1% 미만만 활성화하여 모델을 미세 조정합니다. 주황색 큐브는 조정할 어댑터 수를 나타냅니다. 3단계에서는 NLI 데이터 세트에서 함축 클래스 전제 및 가설 쌍만 사용하여 학습하고 MNR 대조 목적이 있는 시암 아키텍처를 적용하여 성능을 개선합니다(보라색 큐브는 양의 쌍을 나타내고 다른 큐브는 음의 쌍을 나타냄). Sentence-BERT는 산업과 학계 모두에서 여러 응용 프로그램에서 성공적이었으며 영어만 지원하고 비교적 작은 모델입니다. BigScience Large Open-science Open-access Multilingual Language Model(BLOOM)은 2022년에 출시되었으며[10] 46개의 자연어와 13개의 프로그래밍 언어에서 학습되었습니다. 학습 데이터 세트는 기능, 한계, 잠재적 개선 사항, 편향, 윤리, 환경 영향 및 일반적인 AI 인지 연구 환경과 같은 고급 주제를 둘러싼 많은 연구 질문을 다룹니다[11]. 저희가 아는 한(2023년 2월), BLOOM은 자연어 처리(NLP)에서 가장 큰 공개적으로 사용 가능한 LLM입니다. 가장 큰 BLOOM에는 1,760억 개의 매개변수가 있습니다. BLOOM은 강력하지만 자연어 생성을 목표로 하는 자기 회귀 언어 모델입니다. 여러 비지도 NLP 작업에서 최첨단(SOTA) 성능을 달성했지만 의미적으로 의미 있는 표현을 생성하는 것과 같은 도메인별 작업의 경우 여전히 사전 학습된 LLM을 미세 조정해야 합니다. 저희의 초기 시도는 Siamese 아키텍처로 BLOOM을 미세 조정하는 것입니다. 그러나 BLOOM은 수백 개의 GPU가 있는 클러스터에서 대규모 매개변수로 학습되었으며 이는 많은 상황에서 덜 현실적입니다. 또한 성능이 좋은 텍스트 임베딩은 일반적으로 많은 양의 레이블이 지정된 데이터를 필요로 하는데, 이는 다국어 레이블이 지정된 데이터의 유용성이 부족하고 비용이 많이 들기 때문에 또 다른 제한 사항입니다. 이러한 과제를 극복하기 위해 8비트 Siamese-BLOOM(LACOS-BLOOM) 위에 대조적 목적이 있는 Lowrank Adaptation이라는 매개변수 효율적인 미세 조정 솔루션을 제안합니다. 모델 가중치가 8비트 형식으로 고정된 bitsandbytes [12]의 작업에서 영감을 얻었습니다(71억 개의 매개변수가 있는 모델이 20Gb에서 6Gb로 줄어듬). 그런 다음 Low-Rank Adaptation(LoRA) [13]을 사용하여 매개변수의 1% 미만으로 BLOOM을 미세 조정하고 효율적인 8비트 Adam 최적화 프로그램으로 가중치를 업데이트합니다 [14]. 마지막으로 표현을 의미적으로 의미 있게 만들기 위해, 우리는 시암 아키텍처[15, 16]에서 다중 부정 순위(MNR) 목적을 가진 단일 클래스 샘플에 대해 모델을 훈련합니다. LACOS의 설계로, 우리는 다양한 BLOOM을 단일 GPU로 실행할 수 있습니다(BLOOM 모델 매개변수는 5억 6천만(560m)에서 71억(7b1)까지). 의미적 텍스트 유사성(STS) 평가 작업에서, 우리는 기준 Sentence-BERT에 비해 상당한 개선을 이루었습니다. 다음으로, 우리는 섹션 2에서 LACOS-BLOOM 모델을 제시합니다. 이어서, 섹션 3에서 실험 설정과 결과가 나옵니다. 관련 연구는 섹션 4에 있습니다. 마지막으로, 우리는 작업을 마무리하고 섹션 5에서 다음 단계에 대해 논의합니다. 2. 모델 LACOS-BLOOM(그림 1)은 다국어 텍스트에 대한 의미적으로 의미 있는 표현을 생성하는 텍스트 임베딩 모델입니다. LACOS-BLOOM을 더 적은 계산 리소스로 더 실용적으로 만들고 고품질 표현을 생성하기 위해 여러 기술이 적용되었습니다. 여기에는 8비트 블록별 양자화를 사용하여 많은 수의 모델 가중치를 양자화하는 것이 포함됩니다. 모델은 확장 가능한 LORA 및 8비트 Adam 최적화기를 사용하여 미세 조정됩니다. 마지막으로 모델은 MNR 손실이 있는 Siamese 네트워크로 향상됩니다. 2.1. 8비트 블록 단위 양자화 3. 입력 텐서 저장 인덱스 값 1.-3.0.-1.0 0.-1.0.-0.1.-0.025 1.0.0242 1. absmax로 정규화된 블록 단위 absmax 찾기 가장 가까운 8비트 값에 대응되는 인덱스 찾기 양자화 역양자화 인덱스 값 텐서 역양자화-1.0 0.-1.0 0.3.-3.1 0.-0.0242 1.-0.0242 1.1.-0.029 1. 주어진 블록에서 absmax로 비정규화된 조회 값 그림 2: 블록 B = 2(빨간색과 녹색 블록)를 사용한 블록 단위 양자화 및 역양자화 [12]의 8비트 블록 단위 양자화를 사용합니다. 그림 2는 단계를 보여줍니다. 블록 크기 B = 2(빨간색과 녹색 블록)로 분할된 2 × 행렬. 각 블록 내에서 절대 최대값을 찾은 다음 이러한 값을 사용하여 float32 가중치를 8비트 정수 값/인덱스에 매핑합니다. 가중치가 양자화되면 인덱스가 저장되어 모델의 풋프린트를 크게 줄일 수 있습니다. 모델을 업데이트할 때 이러한 매개변수는 적시 곱셈을 위해 32 또는 16개의 부동 소수점으로 다시 양자화 해제됩니다. 사용하는 방법은 저장에만 8비트 양자화를 사용하고 float16 또는 float32에서 계산을 수행한다는 점에서 다른 8비트 접근 방식과 다릅니다. 이를 통해 각 개별 가중치의 분포에 맞게 조정된 비선형 양자화를 사용할 수 있어 추론 성능에 영향을 주지 않고 오류를 줄일 수 있습니다. 2.2. 저랭크 적응 어댑터 접근 방식은 하위 작업에 대한 가중치 업데이트를 근사하기 위해 저랭크의 작고 학습 가능한 행렬을 활용합니다. LORA 접근 방식은 Eq.에서 저랭크 분해를 사용하여 업데이트를 나타냅니다. (1) 사전 훈련된 가중치 행렬: W+AW=W + W down × Wup (1) 및 방정식 (1)의 분해는 두 개의 조정 가능한 행렬 Wa € Rdxr down Wup Є Rrxk, (r &lt; min(d, k))로 표현되고 [13]의 멀티헤드 어텐션 계층의 쿼리 및 가치 투영 행렬에 적용됩니다. 이 작업에서 우리는 이전 연구 [17]에서 제안한 대로 피드포워드 선형 계층과 마지막 숨겨진 임베딩 계층에 LoRA를 적용합니다. 2.3. 시암 네트워크 LLM에서 LORA를 사용하면 도메인별 작업을 미세 조정하는 데 성공적이었지만 미세 조정 작업의 또 다른 제한은 레이블이 지정된 데이터의 가용성입니다. 이 문제를 해결하기 위해 대조 목적, 즉 MNR 손실이 있는 시암 아키텍처를 사용하는 것을 제안합니다 [18]. Siamese 아키텍처를 사용한 MNR 손실은 제한된 레이블이 지정된 데이터의 한계에도 불구하고 모델이 정확한 의미적 유사성 표현을 학습할 수 있게 하는 접근 방식입니다. 미니 배치 크기 n의 시퀀스가 주어지면 P = (u1, v1), (u2, v2), ..., (Un, Un)이며, 여기서 (ui, vi)는 양의 쌍이고, (ur, vj)는 i ‡ j에 대해 음의 쌍입니다. 문장 쌍은 각 토큰에 대한 마지막 숨겨진 계층 임베딩을 얻기 위해 LoRA 8비트 BLOOM을 통과합니다. 그런 다음 평균 풀링 계층을 적용하여 문장 수준 임베딩을 생성합니다. 임베딩 쌍 (u, v) 간의 유사성 점수는 코사인 함수로 계산되고 sim(u, v)로 표시됩니다. 각 미니 배치가 주어지면 양의 쌍이 하나뿐이고 다른 쌍은 음의 쌍(P로 표시)입니다(그림 1의 3단계). 목표는 Eq (2)에서 softmax-normalized 점수에 대한 음의 로그 가능도를 최소화하는 것입니다.L = Σ log (u, v)EP 3. 실험 exp(sim(u, v)) *exp(sim(u, v)) + Σwɛp exp(sim(u, w) &#39; (2) 3.1. 실험 설정 LACOS-BLOOM 모델을 훈련하기 위해 두 가지 실험을 수행합니다.첫 번째 실험은 Stanford Natural Language Inference(SNLI) [19] 및 Multi-Genre NLI(MNLI) [20] 데이터 세트입니다.두 마리의 개가 달리고 있습니다.레이블 = 함의 한 아이가 스케이트보드를 타고 있습니다.EE 레이블 = 함의 한 남자가 바다에서 서핑을 하고 있습니다.레이블 함의 There are animals outdoors 레이블 함의 한 아이가 스케이트보드를 타고 있습니다.레이블 = 함의 한 남자가 바다에서 잠수복을 입고 있습니다.레이블 함의 그림 3: NLI 데이터에서 함의 쌍만 있는 샴 네트워크; 여기서 실선은 양의 쌍을 보여주고 점선은 음의 쌍을 보여줍니다. 미니 배치가 주어진 샘플이고 두 번째는 Multilingual NLI(multi-NLI) 데이터 세트[21]입니다. 훈련하는 동안, 우리는 entailment 클래스에 속하는 데이터 쌍만 사용하고 MNR 손실이 있는 Siamese 네트워크를 적용합니다. 그림 3은 우리가 모든 미니 배치에 대해 양성 및 음성 샘플을 어떻게 생성했는지 보여줍니다. 우리는 32와 64 사이에서 선택하여 미니 배치 크기에 대한 그리드 검색을 수행하고, 1e-4, 2e-5 또는 5e-5의 학습 속도를 가진 8비트 Adam 옵티마이저를 수행합니다. 우리는 1 에포크 동안 모델을 미세 조정합니다. 우리는 크기가 560m에서 7b1까지이고 어댑터 차원(r)이 1, 2, 4, 8 또는 16인 LACOS-BLOOM 모델을 사용하여 실험을 수행합니다. 각 BLOOM 모델 크기에 대해 최종 평가를 위해 최상의 체크포인트를 유지합니다. 우리는 기준선에 대한 소프트맥스 목적과 함께 SentenceBERT(SBERT) [9]와 동일한 구성을 활용하는데, 여기서 사전 학습된 모델은 &quot;bert-base-multilingual-cased&quot;입니다. 실험은 Volta 아키텍처와 32GB 메모리의 단일 GPU에서 실행되었습니다. 최적의 모델을 선택하기 위해 SNLI와 MNLI의 테스트 데이터 세트를 검증 세트로 활용합니다. 검증 손실을 집계하여 공통 범위로 표준화합니다. 그림 4는 다양한 BLOOM에서 어댑터 수와 검증 오류를 표시합니다. BLOOM 560m은 모듈당 4개의 어댑터로 가장 낮은 검증 오류를 보이는 반면, BLOOM 7b1은 레이어당 1개의 어댑터로 가장 낮은 검증 오류를 보입니다. 이는 모델 크기가 작을 때 더 많은 어댑터를 활성화해야 하는 반면 모델 크기가 클 때는 몇 개의 어댑터만으로 충분하다는 것을 보여줍니다. 3.2. 성능 비교 우리는 7개의 STS 영어 과제(STS12-[22], STS-B [23] 및 SICK-R [24])와 1개의 다국어 STS 과제(xSTS [25])에서 LACOS-BLOOM 모델의 성능을 평가합니다. 이 과제들은 모두 훈련 과정에 포함되지 않았습니다. STS 데이터 세트는 문장 쌍의 의미적 관련성을 나타내는 0~5 범위의 레이블을 제공합니다. 우리는 문장 임베딩과 골든 레이블 데이터 세트의 코사인 유사도, 맨해튼 거리, 유클리드 거리 및 점곱 유사도 간의 최대 스피어만 순위 상관 관계인 기준 SBERT와 동일한 평가 메트릭을 사용합니다. 성능을 평가하기 위해 STS-Avg.와 xSTS-Avg.의 두 가지 메트릭을 더 사용합니다. STS-Avg. 는 조정된 집계 검증 손실에 대한 일반적인 벤치마크인 STS12-16, STS-B 및 SICK-R의 평균 점수입니다.BLOOM-560m BLOOM-1b BLOOM-3b BLOOM-7b 각 계층의 어댑터 수 그림 4: 의미 텍스트 유사도 모델의 성능을 평가하는 각 계층의 집계 검증 손실 대 어댑터 수.xSTS-Avg.는 모든 언어의 평균 점수이며 모델의 언어 간 성능을 평가하는 데 사용되었습니다.그림 4에서 식별된 BLOOM 모델을 STS 작업에서 평가하고 표 1에 상관 관계 점수를 제시합니다.모델 크기가 커질수록 점수가 증가했으며 LACOSBLOOM 7b1이 가장 좋은 성능을 달성했습니다.LACOS-BLOOM 7b1을 사용하여 영어 STS 작업을 평가하고 xSTS 작업에 적용합니다.LACOS-BLOOM 방법은 영어 및 다국어 STS 작업의 성능을 최소 4% 이상 향상시켰습니다. 한 가지 관찰 결과는 SBERT의 다국어 작업에서의 성과가 영어 작업에서의 성과만큼 좋지 않다는 것입니다. 이는 SBERT가 비교적 작은 모델이기 때문에 학습에서 상당히 다른 평가 작업으로 지식을 이전하는 능력이 제한될 수 있기 때문일 수 있습니다. STS 작업에서의 TableSentence 임베딩 성과; max(스피어만 순위 상관 관계) (%); STS-Avg.는 STS12-16, STS-B 및 SICK-R의 평균 점수입니다. xSTS-Avg.는 모든 언어 데이터 세트 STS12 STS13 STS14 STS15 STS16 STS-B SICK-R STS-Avg. xSTS-Avg.의 평균 점수입니다. 학습 데이터 모델 크기 SBERT SNLI + MNLI 59.46 56.49 53.58 61.15 57.42 57.69 61.LACOS-BLOOM-560m 47.54 39.54 34.56 49.71 42.10 45.84 46.LACOS-BLOOM-1b1 59.25 62.22 56.04 62.88 59.38 59.LACOS-BLOOM-3b1 59.74 63.81 57.12 63.95 61.78 61.18 55.LACOS-BLOOM-7b1 63.92 65.04 58.46 66.47 63.46 62.23 57.58.52.43.42.54.59.54.60.56.62.56.multiNLI SBERT 43.58 51.19 45.82 51.97 58.16 41.LACOS-BLOOM-7b1 66.27 71.93 67.41 75.78 71.44 70.46.48.48.57.76 68.70.3.3. Ablation study BLOOM은 LLM이므로 장점 중 하나는 전이 학습에 적합하다는 것입니다.따라서 BLOOM에서 제로샷 추론을 수행합니다.반면, 이전 솔루션(즉, simCSE) [15]은 MNR 대조 목적으로 전체 크기 모델을 미세 조정하면 STS 작업에서 SOTA 결과를 얻을 수 있음을 보여주었습니다. 공정한 비교를 위해, 우리는 simCSE 설정에서 단일 GPU에 들어갈 수 있는 가장 큰 모델 크기이기 때문에, 제로샷 추론, LACOS-BLOOM 및 전체 모델 미세 조정을 위한 10억 개의 매개변수를 갖는 BLOOM 모델을 선택했습니다. 훈련 데이터에는 영어 작업을 위한 SNLI와 MNLI가 포함되어 있으며, STS 작업에서 성능을 평가합니다. 결과는 표 2에 보고되어 있습니다. 결과에서, 우리는 LACOS-BLOOM이 제로샷 솔루션보다 성능이 뛰어나다는 것을 알 수 있습니다. LACOS-BLOOM의 성능은 전체 모델 미세 조정 성능과 비슷하며 계산 비용은 훨씬 더 낮습니다. TableAblation 연구에서, 우리는 SNLI와 MNLI 데이터 세트에서 LACOS-BLOOM과 전체 크기 모델을 훈련했고, 문장 임베딩은 STS 작업에서 평가되었습니다. max(스피어만 순위 상관 관계) (%); STS-Avg.는 STS12-16, STS-B 및 SICK-R의 평균 점수입니다. XSTS-Avg.는 모든 언어 학습 데이터 세트 STS12 STS13 STS14 STS15 STS16 STS-B SICK-R STS-Avg.데이터 SNLI + 모델 설정 제로샷 학습 LACOS-BLOOM-1bMNLI 49.59.전체 크기 모델 미세 조정 69.48.89 40.16 56.24 52.13 39.63 53.62.22 56.04 62.88 59.38 59.13 54.73.14 68.64 76.94 76.17 70.44 68.48.59.71.4. 관련 연구 4.1. LLM 압축 딥 러닝 모델, 특히 트랜스포머 기반 언어 모델은 NLP, 컴퓨터 비전, 음성 분석 및 기타 작업에서 SOTA 결과를 달성했습니다. 그러나 이러한 모델은 계산 비용이 많이 들 수 있으므로 가지치기, 양자화, 지식 증류, 매개변수 공유, 텐서 분해 및 하위 2차 변압기 기반 방법과 같은 다양한 모델 압축 방법이 성능을 유지하면서 계산 비용을 줄이기 위해 개발되었습니다[26, 27].8비트 양자화는 아키텍처를 조작할 필요 없이 메모리와 컴퓨팅 요구 사항을 모두 줄이고 머신 러닝 프레임워크 및 하드웨어 툴체인과 함께 사용할 수 있으므로 최적화에 널리 사용되는 접근 방식입니다.이전 작업에서 애플리케이션에 양자화를 적용하는 것과 달리 블록별 양자화 및 역양자화를 사용하여 풋프린트를 절약하고 복잡도 점수를 유지합니다.결과적으로 이러한 솔루션은 네트워크뿐만 아니라 전체 애플리케이션(예: 네트워크 대역폭, 추론 지연 및 전력 소비)을 최적화합니다.4.2. 매개변수 효율적 미세 조정 방법 NLP에서 다운스트림 작업에서 대규모 사전 학습된 언어 모델을 미세 조정하는 것은 일반적인 관행이지만 모델 크기와 작업 수가 증가함에 따라 비실용적일 수 있습니다. 이를 해결하기 위해 어댑터[28], 접두사 튜닝[29] 및 LORA[13]와 같은 다양한 매개변수 효율적 전이 학습 접근 방식이 제안되었습니다. 어댑터의 기본 아이디어는 각 변환기 계층에서 작은 매개변수 하위 집합만 활성화하여 대규모 모델을 미세 조정하는 것입니다. 접두사 튜닝은 모델 매개변수를 고정하고 제어 가능한 접두사 텍스트를 기반으로 목표를 최적화하는 동안 작업 입력에 몇 가지 예만 추가합니다. LoRA는 어댑터 접근 방식을 활용하지만 매개변수 하위 집합을 추가하는 대신 추론 지연 시간을 증가시키지 않는 어텐션 모듈과 병렬로 몇 가지 낮은 내재적 어댑터를 활성화합니다. 이 작업에서 우리는 LORA에 관심이 있는데, 이 설계는 어디에나 어댑터를 추가하는 데 유연성을 허용하기 때문입니다[17]. 따라서 특정 작업에서 성능을 개선하기 위해 대규모 언어 모델로 확장하는 데 유용합니다. 4.3. 대조 손실이 있는 샴 네트워크 샴 아키텍처는 컴퓨터 비전, NLP 등에서 널리 사용되었습니다. 목표는 두 입력 간의 유사성 함수를 학습하는 것입니다. 최근 연구에 따르면 Siamese 아키텍처는 여러 자연어 작업에서 자체 학습 목표로 성능을 높일 수 있음이 밝혀졌습니다(예: [9, 15, 30]). 이 연구에서는 MNR 목표를 통합하는 것을 제안합니다. MNR의 한 가지 장점은 레이블이 지정된 데이터에 의존하지 않고 여러 개의 서로 다른 예제 세트보다 유사한 항목 세트의 순위만 더 높게 고려한다는 것입니다. 이를 통해 계산 및 메모리 측면에서 효율성이 높아지고 새로운 예제에 잘 일반화되는 보다 강력한 모델을 만들 수 있습니다. 5.
--- CONCLUSION ---
및 향후 작업 이 논문에서는 대규모 언어 모델(LLM)에서 다국어 텍스트 임베딩을 추출하기 위한 LACOS-BLOOM이라는 매개변수 효율적 미세 조정 방법을 제안합니다. 8비트 양자화를 사용하여 모델 풋프린트를 줄입니다. 그런 다음 LoRA를 사용하여 LLM 미세 조정의 성능을 개선하고 MNR이 있는 시암 네트워크를 사용하여 의미적 유사성을 더욱 향상시킵니다. 저희 솔루션은 단일 GPU에서 71억 개의 BLOOM을 종단 간으로 학습할 수 있습니다. STS 작업에서 저희 방법은 베이스라인과 제로샷 LLM BLOOM보다 상당히 우수한 성능을 보입니다. 저희 솔루션은 LLM을 71억 개의 모델로 확장할 수 있습니다. 앞으로 DeepSpeed를 LACOS-BLOOM과 통합하여 학습 작업을 전체 BLOOM으로 효율적으로 확장할 계획입니다. 참고문헌 [1] J. Kaplan, S. McCandlish, T. Henighan, TB Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, D. Amodei, 신경 언어 모델의 스케일링 법칙, arXiv 사전 인쇄본 arXiv:2001.(2020). [2] T. Brown, B. Mann, N. Ryder, M. Subbiah, JD Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., 언어 모델은 소수 학습자입니다, 신경 정보 처리 시스템의 발전 33(2020) 1877–1901. [3] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, HW Chung, C. Sutton, S. Gehrmann, et al., Palm: 경로를 통한 언어 모델링 확장, arXiv 사전 인쇄본 arXiv:2204.02311(2022). [4] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, et al., deepspeed 및 megatron을 사용하여 대규모 생성 언어 모델인 megatronturing nlg 530b를 훈련, arXiv 사전 인쇄본 arXiv:2201.(2022). [5] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever 외, 생성적 사전 학습을 통한 언어 이해 향상(2018). [6] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever 외, 언어 모델은 비지도 멀티태스크 학습기, OpenAI 블로그 1(2019) 9. [7] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습, arXiv 사전 인쇄본 arXiv:1810.04805(2018). [8] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, Roberta: 강력하게 최적화된 bert 사전 학습 접근법, arXiv 사전 인쇄 arXiv:1907.11692(2019). [9] N. Reimers, I. Gurevych, Sentence-bert: siamese bert-networks를 사용한 문장 임베딩, arXiv 사전 인쇄 arXiv:1908.10084(2019). [10] TL Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, AS Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b 매개변수 개방형 액세스 다국어 언어 모델, arXiv 사전 인쇄 arXiv:2211.05100(2022). [11] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, AV del Moral, T. Le Scao, L. Von Werra, C. Mou, EG Ponferrada, H. Nguyen 외, bigscience roots corpus: 1.6 tb 복합 다국어 데이터 세트, 제36회 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙 컨퍼런스, 2022. [12] T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, Llm. int8(): 대규모 변압기를 위한 8비트 행렬 곱셈, arXiv 사전 인쇄본 arXiv:2208.07339(2022). [13] EJ Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: 대규모 언어 모델의 저순위 적응, arXiv 사전 인쇄본 arXiv:2106.09685(2021). [14] T. Dettmers, M. Lewis, S. Shleifer, L. Zettlemoyer, 블록별 양자화를 통한 8비트 최적화기, arXiv 사전 인쇄본 arXiv:2110.02861(2021). [15] T. Gao, X. Yao, D. Chen, Simcse: 문장 임베딩의 간단한 대조 학습, arXiv 사전 인쇄본 arXiv:2104.08821(2021). [16] A. Neelakantan, T. Xu, R. Puri, A. Radford, JM Han, J. Tworek, Q. Yuan, N. Tezak, JW Kim, C. Hallacy, et al., 대조적 사전 학습을 통한 텍스트 및 코드 임베딩, arXiv 사전 인쇄본 arXiv:2201.10005 (2022). [17] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, G. Neubig, 매개변수 효율적 전이 학습에 대한 통합된 관점을 향하여, arXiv 사전 인쇄본 arXiv:2110.04366 (2021). [18] M. Henderson, R. Al-Rfou, B. Strope, Y. Sung, L. Lukács, R. Guo, S. Kumar, B. Miklos, R. Kurzweil, 스마트 답변을 위한 효율적인 자연어 응답 제안. arxiv, 2017년 5월 1일에 온라인에 게시된 사전 인쇄본. [19] SR Bowman, G. Angeli, C. Potts, CD Manning, 자연어 추론 학습을 위한 대규모 주석 코퍼스, arXiv 사전 인쇄본 arXiv:1508.05326(2015). [20] A. Williams, N. Nangia, SR Bowman, 추론을 통한 문장 이해를 위한 광범위한 범위의 챌린지 코퍼스, arXiv 사전 인쇄본 arXiv:1704.05426(2017). [21] A. Conneau, G. Lample, R. Rinott, A. Williams, SR Bowman, H. Schwenk, V. Stoyanov, Xnli: 언어 간 문장 표현 평가, arXiv 사전 인쇄본 arXiv:1809.(2018). [22] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, A. Bordes, 자연어 추론 데이터에서 범용 문장 표현의 지도 학습, arXiv 사전 인쇄본 arXiv:1705.02364(2017). [23] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, L. Specia, Semeval-2017 과제 1: 의미적 텍스트 유사성-다국어 및 교차 언어 중심 평가, arXiv 사전 인쇄본 arXiv:1708.00055(2017). [24] M. Marelli, S. Menini, M. Baroni, L. Bentivogli, R. Bernardi, R. Zamparelli 외, 구성적 분포 의미 모델 평가를 위한 치료법, Lrec, Reykjavik, 2014, pp. 216-223. [25] P. May, 기계 번역된 다국어 sts 벤치마크 데이터 세트, 2021. URL: https://github. com/PhilipMay/stsb-multi-mt. [26] P. Ganesh, Y. Chen, X. Lou, MA Khan, Y. Yang, H. Sajjad, P. Nakov, D. Chen, M. Winslett, 대규모 변환기 기반 모델 압축: bert에 대한 사례 연구, 계산 언어학 협회 논문 9(2021) 1061–1080. [27] M. Gupta, P. Agrawal, 텍스트에 대한 딥 러닝 모델의 압축: 조사, ACM Transactions on Knowledge Discovery from Data(TKDD) 16(2022) 1–55. [28] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, S. Gelly, nlp를 위한 매개변수 효율적 전이 학습, 국제 기계 학습 컨퍼런스, PMLR, 2019, pp. 2790-2799. [29] XL Li, P. Liang, 접두사 튜닝: 생성을 위한 연속 프롬프트 최적화, arXiv 사전 인쇄본 arXiv:2101.00190(2021). [30] L. Xiong, C. Xiong, Y. Li, K.-F. Tang, J. Liu, P. Bennett, J. Ahmed, A. Overwijk, 밀집 텍스트 검색을 위한 근사 최근접 이웃 부정 대조 학습, arXiv 사전 인쇄본 arXiv:2007.00808(2020).
