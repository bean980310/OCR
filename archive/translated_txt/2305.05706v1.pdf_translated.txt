--- ABSTRACT ---
일반 용도 로봇을 가능하게 하려면 로봇이 인간처럼 일상적인 관절 물체를 조작해야 합니다. 현재의 로봇 조작은 병렬 그리퍼를 사용하는 데 크게 의존하여 로봇을 제한된 물체 세트로 제한합니다. 반면, 여러 손가락 로봇 손으로 조작하면 인간 행동에 더 잘 근사할 수 있고 로봇이 다양한 관절 물체에서 조작할 수 있습니다. 이를 위해 DexArt라는 새로운 벤치마크를 제안합니다. 이는 물리적 시뮬레이터에서 관절 물체로 민첩하게 조작하는 것을 포함합니다. 벤치마크에서 여러 복잡한 조작 작업을 정의하고 로봇 손은 각 작업 내에서 다양한 관절 물체를 조작해야 합니다. 우리의 주요 초점은 보이지 않는 관절 물체에 대한 학습된 정책의 일반화 가능성을 평가하는 것입니다. 이는 손과 물체의 높은 자유도를 감안할 때 매우 어렵습니다. 우리는 강화 *동등한 기여를 사용합니다. UC 샌디에이고에서 인턴으로 일하는 동안 수행한 작업. 일반화를 달성하기 위해 3D 표현 학습으로 학습. 광범위한 연구를 통해 3D 포인트 클라우드 입력을 사용한 RL에서 3D 표현 학습이 의사 결정에 어떤 영향을 미치는지에 대한 새로운 통찰력을 제공합니다. 자세한 내용은 https://www.chenbao.tech/dexart/에서 확인할 수 있습니다. 1.
--- INTRODUCTION ---
대부분의 도구와 인간이 상호작용하는 물체는 관절이 있는 물체입니다. 가정용 로봇이 우리의 일상 생활을 편리하게 하려면 인간처럼 여러 손가락이 있는 손으로 다양한 관절이 있는 물체를 조작할 수 있도록 해야 합니다. 그러나 로봇 손의 높은 자유도(DOF) 관절을 감안할 때 능숙한 조작을 배우는 것은 여전히 어려운 과제입니다. 최근 연구에서는 능숙한 조작을 위해 강화 학습(RL)[1, 8,29,69]을 사용하는 데 있어 격려적인 진전이 있었지만, 대부분의 연구는 단일 단단한 물체를 조작하는 데 중점을 두고 있습니다. 다양한 관절이 있는 물체를 조작하면 관절 자유도가 추가될 뿐만 아니라 테스트 시간에 보이지 않는 물체를 일반화하는 데 새로운 과제가 발생하는데, 이는 RL의 주요 병목 현상이었습니다. 이를 위해서는 새로운 벤치마크에서 3D 시각적 이해와 로봇 학습을 통합하기 위한 노력이 필요합니다. 최근 제안된 로봇 조작 벤치마크[7, 12, 34, 65]는 로봇 학습 알고리즘 개발에서 중요한 역할을 합니다. 예를 들어, MetaWorld[65] 벤치마크는 RL 알고리즘을 평가하기 위한 50개 이상의 작업을 제공합니다. 그러나 제안된 각 MetaWorld 작업은 객체 인스턴스 전체에 걸친 일반화를 고려하지 않고 하나의 단일 객체에만 초점을 맞춥니다. 로봇의 일반화를 가능하게 하기 위해 다양한 조작 작업과 각 작업 내에서 조작할 수 있는 많은 수의 객체를 사용하여 ManiSkill[19,41] 벤치마크가 제안되었습니다. 이는 고무적이지만 병렬 그리퍼를 사용하면 로봇이 수행할 수 있는 작업과 로봇이 작동하는 방식이 제한됩니다. 예를 들어, 병렬 그리퍼가 핸들을 사용하여 양동이를 집어 올리는 것은 매우 어렵습니다. 이 논문에서는 다양한 관절 객체를 사용한 민첩한 조작(DexArt)에 대한 새로운 벤치마크를 제안합니다. 시뮬레이션에서 관절 객체를 조작하는 민첩한 손(Allegro Hand)으로 여러 작업을 소개합니다. 각 작업에 대해 특정 객체로 작동하는 대신 다양한 관절 객체의 교육 세트를 제공하고 목표는 정책을 다른 관절 객체 테스트 세트로 일반화하는 것입니다. 이러한 일반화를 달성하기 위해 우리는 일반화 가능한 시각적 표현 학습과 함께 RL을 통합합니다. 우리는 3D 포인트 클라우드를 관찰로 채택하고 PointNet 인코더[44]를 사용하여 의사 결정을 위한 시각적 표현을 추출합니다. 정책의 일반화 가능성은 PointNet 인코더가 모델링한 3D 구조 이해에 따라 달라집니다. 우리는 다양한 방법과 설정으로 실험하고 벤치마킹하며 다음과 같이 네 가지 주요 관찰 결과를 제공합니다. (i) 더 많은 객체로 훈련하면 일반화가 더 좋아집니다. 각 작업에서 우리는 각 작업에 대해 다양한 수의 객체를 사용하여 정책을 훈련하고 동일한 보이지 않는 객체 세트에서 테스트했습니다. 우리는 더 많은 객체로 훈련하면 일관되게 더 나은 성공률을 얻는다는 것을 발견했습니다. 병렬 그리퍼를 사용한 조작에 대한 연구에서도 유사한 결과가 보고되었습니다(Generalist-Specialist Learning[24], ManiSkill[41]). 이는 지각 관점에서 놀라운 일이 아닐 수 있지만 단일 RL 정책이 동시에 여러 객체로 작업하는 데 더 많은 과제를 제시합니다. 이는 RL에 대한 일반화 가능한 시각적 표현을 학습하는 것의 중요성을 강조합니다. (ii) 용량이 더 큰 인코더가 반드시 도움이 되는 것은 아닙니다. 우리는 다양한 크기의 PointNet 인코더를 실험했고, 네트워크가 사전 학습되었는지 여부에 관계없이 매개변수가 가장 적은 가장 간단한 인코더가 가장 좋은 샘플 효율성과 성공률을 달성하는 것을 관찰했습니다. 이는 비전 관점에서 놀라운 일이지만, RL 최적화가 대형 인코더에서 훨씬 더 어려워진다는 것을 보여주는 이전 문헌과 일치합니다[41].(iii) 객체 부분 추론이 필수적입니다. 여러 손가락이 다른 객체 부분과 상호 작용하기 때문에 객체 부분 인식과 추론이 조작에 필수적일 수 있다는 직감이 있습니다. 직감을 검증하기 위해 객체 부분 분할 작업으로 PointNet 인코더를 사전 학습했습니다. 객체 부분 사전 학습이 사전 학습이 없는 접근 방식과 다른 사전 학습 방법에 비해 샘플 효율성과 성공률을 크게 향상시킬 수 있음을 보여줍니다.(iv) 기하학적 표현 학습은 견고한 정책을 제공합니다. 보이지 않는 카메라 포즈에서 정책의 견고성을 평가합니다. 부분 포인트 클라우드로 학습된 정책이 카메라 포즈의 변화에 놀라울 정도로 탄력적이라는 것을 발견했는데, 이는 정책에서 완전한 포인트 클라우드를 사용하는 이전 연구와 일치합니다[32]. 정확도는 큰 시점 변화에도 일관되게 유지됩니다. 이는 시뮬레이션과 실제 간에 카메라를 정렬하는 것이 어렵기 때문에 실제 로봇 애플리케이션에 특히 유용합니다. 제안된 기준선과 그 사이의 자세한 분석을 통해 DexArt 벤치마크가 일반화 가능한 능숙한 조작 기술 자체를 연구할 수 있는 플랫폼을 제공할 뿐만 아니라 더 나은 의사 결정을 목표로 시각 인식을 개선할 수 있는 방법을 연구할 수 있기를 바랍니다. 우리는 인식과 행동의 통합과 DexArt에서 이를 연구하는 것이 많은 연구 기회를 창출할 수 있다고 믿습니다. 2.
--- RELATED WORK ---
손재주 있는 조작. 손가락이 여러 개 달린 로봇 손을 이용한 손재주 있는 조작은 로봇공학에서 오랜 문제였습니다. 이전
--- METHOD ---
영어: 보이는 객체에 대한 범주 수준 조작 정책을 학습하는 s. (c) 보이지 않는 객체 모음에 대한 정책의 일반화 가능성과 카메라 시점 변경에 대한 견고성을 평가합니다. 초록 범용 로봇을 가능하게 하려면 로봇이 인간처럼 일상적인 관절 객체를 조작해야 합니다. 현재의 로봇 조작은 병렬 그리퍼 사용에 크게 의존하여 로봇을 제한된 객체 집합으로 제한했습니다. 반면, 여러 손가락 로봇 손으로 조작하면 인간 행동에 더 잘 근사할 수 있고 로봇이 다양한 관절 객체에서 작동할 수 있습니다. 이를 위해 물리적 시뮬레이터에서 관절 객체로 민첩하게 조작하는 DexArt라는 새로운 벤치마크를 제안합니다. 벤치마크에서 여러 복잡한 조작 작업을 정의하고 로봇 손은 각 작업 내에서 다양한 관절 객체를 조작해야 합니다. 우리의 주요 초점은 보이지 않는 관절 객체에 대한 학습된 정책의 일반화 가능성을 평가하는 것입니다. 이는 손과 객체의 높은 자유도를 감안할 때 매우 어렵습니다. 우리는 강화 *동등한 기여를 사용합니다.UC 샌디에이고에서 인턴으로 일하는 동안 수행한 작업.일반화를 달성하기 위해 3D 표현 학습으로 학습합니다.광범위한 연구를 통해 3D 포인트 클라우드 입력을 사용하여 RL에서 3D 표현 학습이 의사 결정에 어떤 영향을 미치는지에 대한 새로운 통찰력을 제공합니다.자세한 내용은 https://www.chenbao.tech/dexart/에서 확인할 수 있습니다.1. 서론 인간이 상호 작용하는 대부분의 도구와 물체는 관절이 있는 물체입니다.가정용 로봇이 우리의 일상 생활을 편리하게 하려면 인간처럼 여러 손가락이 있는 손으로 다양한 관절이 있는 물체를 조작할 수 있도록 해야 합니다.그러나 로봇 손의 높은 자유도(DOF) 관절을 감안할 때 능숙한 조작을 배우는 것은 여전히 어려운 작업입니다.최근 연구에서는 능숙한 조작을 위해 강화 학습(RL) [1, 8,29,69]을 사용하는 데 있어 격려적인 진전이 있었지만 대부분의 연구는 단일 단단한 물체를 조작하는 데 중점을 두고 있습니다. 다양한 관절 물체의 조작은 관절 DoF로 추가적인 복잡성을 추가할 뿐만 아니라 테스트 시간에 보이지 않는 물체로 일반화하는 데 새로운 과제를 가져오는데, 이는 RL의 주요 병목 현상이었습니다. 이를 위해서는 새로운 벤치마크에서 3D 시각적 이해와 로봇 학습을 통합하기 위한 노력이 필요합니다. 최근 제안된 로봇 조작 벤치마크[7, 12, 34, 65]는 로봇 학습 알고리즘 개발에서 중요한 역할을 합니다. 예를 들어, MetaWorld[65] 벤치마크는 RL 알고리즘을 평가하기 위한 50개 이상의 작업을 제공합니다. 그러나 제안된 각 MetaWorld 작업은 객체 인스턴스 전체에 걸친 일반화를 고려하지 않고 하나의 객체에만 초점을 맞춥니다. 로봇의 일반화를 가능하게 하기 위해 다양한 조작 작업과 각 작업 내에서 조작할 수 있는 많은 수의 객체가 있는 ManiSkill[19,41] 벤치마크가 제안되었습니다. 이는 고무적이지만 병렬 그리퍼를 사용하면 로봇이 수행할 수 있는 작업과 로봇이 작동할 수 있는 방법이 제한됩니다. 예를 들어, 병렬 그리퍼가 핸들을 사용하여 양동이를 집어 올리는 것은 매우 어렵습니다. 이 논문에서 우리는 다양한 관절 물체(DexArt)를 사용한 Dexterous 조작에 대한 새로운 벤치마크를 제안합니다. 우리는 시뮬레이션에서 관절 물체를 조작하는 민첩한 손(Allegro Hand)을 사용한 여러 작업을 소개합니다. 각 작업에서 특정 물체로 작업하는 대신 다양한 관절 물체의 훈련 세트를 제공하고 목표는 정책을 다른 관절 물체 테스트 세트로 일반화하는 것입니다. 이러한 일반화를 달성하기 위해 우리는 일반화 가능한 시각적 표현 학습을 사용한 RL을 통합합니다. 우리는 3D 포인트 클라우드를 관찰로 채택하고 PointNet 인코더[44]를 사용하여 의사 결정을 위한 시각적 표현을 추출합니다. 정책의 일반화 가능성은 PointNet 인코더가 모델링한 3D 구조 이해에 따라 달라집니다. 우리는
--- EXPERIMENT ---
영어: 보이는 객체에 대한 범주 수준 조작 정책을 학습하는 광범위한 벤치마크 방법을 사용합니다.(c) 보이지 않는 객체 모음에 대한 정책의 일반화 가능성과 카메라 시점 변경에 대한 견고성을 평가합니다.초록 범용 로봇을 가능하게 하려면 로봇이 인간처럼 일상적인 관절 객체를 조작해야 합니다.현재 로봇 조작은 병렬 그리퍼 사용에 크게 의존하여 로봇을 제한된 객체 집합으로 제한합니다.반면에 다중 손가락 로봇 손으로 조작하면 인간 행동에 더 잘 근사할 수 있고 로봇이 다양한 관절 객체에서 작동할 수 있습니다.이를 위해 물리적 시뮬레이터에서 관절 객체로 민첩하게 조작하는 DexArt라는 새로운 벤치마크를 제안합니다.벤치마크에서 여러 복잡한 조작 작업을 정의하고 로봇 손은 각 작업 내에서 다양한 관절 객체를 조작해야 합니다.주요 초점은 보이지 않는 관절 객체에 대한 학습된 정책의 일반화 가능성을 평가하는 것입니다.손과 객체 모두의 자유도가 높기 때문에 이는 매우 어렵습니다. 우리는 강화 *동등한 기여를 사용합니다.UC 샌디에이고에서 인턴으로 일하는 동안 수행한 작업.일반화를 달성하기 위해 3D 표현 학습으로 학습합니다.광범위한 연구를 통해 3D 포인트 클라우드 입력을 사용하여 RL에서 3D 표현 학습이 의사 결정에 어떤 영향을 미치는지에 대한 새로운 통찰력을 제공합니다.자세한 내용은 https://www.chenbao.tech/dexart/에서 확인할 수 있습니다.1. 서론 인간이 상호 작용하는 대부분의 도구와 물체는 관절이 있는 물체입니다.가정용 로봇이 우리의 일상 생활을 편리하게 하려면 인간처럼 여러 손가락이 있는 손으로 다양한 관절이 있는 물체를 조작할 수 있도록 해야 합니다.그러나 로봇 손의 높은 자유도(DOF) 관절을 감안할 때 능숙한 조작을 배우는 것은 여전히 어려운 작업입니다.최근 연구에서는 능숙한 조작을 위해 강화 학습(RL) [1, 8,29,69]을 사용하는 데 있어 격려적인 진전이 있었지만 대부분의 연구는 단일 단단한 물체를 조작하는 데 중점을 두고 있습니다. 다양한 관절 물체의 조작은 관절 DoF로 추가적인 복잡성을 추가할 뿐만 아니라 테스트 시간에 보이지 않는 물체로 일반화하는 데 새로운 과제를 가져오는데, 이는 RL의 주요 병목 현상이었습니다. 이를 위해서는 새로운 벤치마크에서 3D 시각적 이해와 로봇 학습을 통합하기 위한 노력이 필요합니다. 최근 제안된 로봇 조작 벤치마크[7, 12, 34, 65]는 로봇 학습 알고리즘 개발에서 중요한 역할을 합니다. 예를 들어, MetaWorld[65] 벤치마크는 RL 알고리즘을 평가하기 위한 50개 이상의 작업을 제공합니다. 그러나 제안된 각 MetaWorld 작업은 객체 인스턴스 전체에 걸친 일반화를 고려하지 않고 하나의 객체에만 초점을 맞춥니다. 로봇의 일반화를 가능하게 하기 위해 다양한 조작 작업과 각 작업 내에서 조작할 수 있는 많은 수의 객체가 있는 ManiSkill[19,41] 벤치마크가 제안되었습니다. 이는 고무적이지만 병렬 그리퍼를 사용하면 로봇이 수행할 수 있는 작업과 로봇이 작동할 수 있는 방법이 제한됩니다. 예를 들어, 병렬 그리퍼가 핸들을 사용하여 양동이를 집어 올리는 것은 매우 어렵습니다. 이 논문에서 우리는 다양한 관절 물체(DexArt)를 사용한 Dexterous 조작에 대한 새로운 벤치마크를 제안합니다. 우리는 시뮬레이션에서 관절 물체를 조작하는 민첩한 손(Allegro Hand)을 사용한 여러 작업을 소개합니다. 각 작업에서 특정 물체로 작업하는 대신 다양한 관절 물체의 훈련 세트를 제공하고 목표는 정책을 다른 관절 물체 테스트 세트로 일반화하는 것입니다. 이러한 일반화를 달성하기 위해 우리는 일반화 가능한 시각적 표현 학습을 사용한 RL을 통합합니다. 우리는 관찰로 3D 포인트 클라우드를 채택하고 PointNet 인코더[44]를 사용하여 의사 결정을 위한 시각적 표현을 추출합니다. 정책의 일반화 가능성은 PointNet 인코더가 모델링한 3D 구조 이해에 따라 달라집니다. 우리는 다양한 방법과 설정으로 실험하고 벤치마킹하며 다음과 같은 네 가지 주요 관찰 결과를 제공합니다. (i) 더 많은 물체로 훈련하면 일반화가 더 좋아집니다. 각 작업에서 우리는 각 작업에 대해 다양한 수의 물체를 사용하여 정책을 훈련하고 동일한 보이지 않는 물체 세트에서 테스트했습니다. 우리는 더 많은 물체로 훈련하면 일관되게 더 나은 성공률을 얻는다는 것을 발견했습니다. 유사한 결과가 병렬 그리퍼를 사용한 조작에 대한 연구에서 보고되었습니다(일반-전문 학습[24], ManiSkill[41]). 이는 지각 관점에서 놀라운 일이 아닐 수 있지만, 단일 RL 정책이 동시에 여러 객체를 처리하는 데 더 많은 과제를 제시합니다. 이는 RL에 대한 일반화 가능한 시각적 표현을 학습하는 것의 중요성을 강조합니다. (ii) 용량이 더 큰 인코더가 반드시 도움이 되는 것은 아닙니다. 다양한 크기의 PointNet 인코더를 실험해 보았고, 네트워크가 사전 학습되었는지 여부에 관계없이 매개변수가 가장 적은 가장 간단한 인코더가 최상의 샘플 효율성과 성공률을 달성하는 것을 관찰했습니다. 이는 시각 관점에서 놀라운 일이지만, RL 최적화가 대형 인코더에서 훨씬 더 어려워진다는 것을 보여주는 이전 문헌과 일치합니다[41]. (iii) 객체 부분 추론이 필수적입니다. 여러 손가락 손이 다양한 객체 부분과 상호 작용하기 때문에 객체 부분 인식과 추론이 조작에 필수적일 수 있다는 직감이 있습니다. 직감을 검증하기 위해 객체 부분 분할 작업으로 PointNet 인코더를 사전 학습했습니다. 우리는 객체 부분 사전 학습이 사전 학습이 없는 접근 방식과 다른 사전 학습 방법을 사용한 접근 방식에 비해 샘플 효율성과 성공률을 크게 개선할 수 있음을 보여줍니다.(iv) 기하학적 표현 학습은 견고한 정책을 가져옵니다.우리는 보이지 않는 카메라 포즈에서 정책의 견고성을 평가합니다.우리는 부분 포인트 클라우드로 학습된 정책이 카메라 포즈의 변화에 놀라울 정도로 회복력이 뛰어나다는 것을 발견했는데, 이는 정책에서 완전한 포인트 클라우드를 사용하는 이전 연구와 일치합니다[32].정확도는 큰 시점 변화에도 일관되게 유지됩니다.이는 시뮬레이션과 실제 간에 카메라를 정렬하기 어렵기 때문에 실제 로봇 애플리케이션에 특히 유용합니다.제안된 기준선과 그 사이의 자세한 분석을 통해 DexArt 벤치마크가 일반화 가능한 능숙한 조작 기술 자체를 연구할 수 있는 플랫폼을 제공할 뿐만 아니라 더 나은 의사 결정을 목표로 시각 인식을 개선할 수 있는 방법을 연구할 수 있기를 바랍니다.우리는 인식과 행동의 통합과 DexArt에서 이를 연구하면 많은 연구 기회를 창출할 수 있다고 믿습니다.2. 관련 연구 능숙한 조작. 여러 손가락이 있는 로봇 손을 이용한 능숙한 조작은 로봇 공학에서 오랜 문제였습니다. 이전 방법에서는 능숙한 조작을 계획 문제로 공식화하고 [3, 5, 13, 21, 43, 51] 궤적 최적화로 해결했습니다 [28,39,58]. 이러한 방법은 로봇과 조작 대상에 대한 잘 조정된 동역학 모델이 필요하므로 일반화가 제한됩니다. 반면, 데이터 기반 방법은 미리 구축된 모델을 가정하지 않습니다. 정책은 모방 학습을 사용한 데모 [10, 20, 26, 48–50, 63,68] 또는 강화 학습을 사용한 상호 작용 데이터 [1,8,23,29,69]에서 학습합니다. 그러나 대부분의 방법은 움켜쥐기 또는 손으로 조작하는 것과 같은 단일 신체 물체에 대한 작업에 초점을 맞춥니다. 관절 물체에 대한 능숙한 조작은 여전히 어려운 문제로 남아 있습니다. 이 논문에서는 포인트 클라우드 관찰을 통해 관절 물체에 대한 일반화 가능한 조작 정책을 학습하는 새로운 벤치마크를 제안합니다. 관절형 물체 조작. 관절형 물체를 인지하고 조작하는 능력은 가정용 로봇에 매우 중요합니다. 포즈 추정 및 추적[30, 33, 57], 관절 매개변수 예측[25, 40, 56, 67], 부분 분할[15, 38, 64], 역학 속성 추정[22]과 같은 관절형 물체의 인지에 대한 최근의 많은 발전이 있었습니다. 로봇 공학 측면에서 이전 연구[11,52]도 관절형 물체에 대한 모델 기반 제어 및 계획을 탐구합니다. 자연스러운 확장은 두 연구 라인을 결합하여 먼저 인지 알고리즘으로 관절형 물체 모델을 추정한 다음 모델 기반 제어로 조작하는 것입니다[35,59]. 또 다른 연구 라인은 원시 감각 입력에서 실행 가능한 정보를 직접 학습하여 상태 및 모델 추정을 우회합니다[36,62]. 그러나 이러한 접근 방식은 단일 단계 액션 표현을 정의하고 오픈 루프 방식으로 미리 정의된 컨트롤러로 실행합니다. 이러한 접근 방식과 달리 우리는 시각적 피드백이 폐쇄 루프 제어에서 사용되는 순차적 의사 결정 프로세스로 관절형 객체 조작을 공식화합니다. 정책 학습 중에 우리는 또한 3D 관절형 객체 표현 학습이 의사 결정에 어떻게 도움이 될 수 있는지 연구합니다. 포인트 클라우드에서 학습. 포인트 클라우드 학습은 3D 비전에서 오랫동안 연구되어 온 주제입니다. 포인트 클라우드의 선구적 아키텍처인 PointNet[44,45], SSCNS[18]은 부분 분할[15, 38, 64] 및 3D 재구성[15, 25] 작업에서 기하학적 표현 학습에 널리 사용되었습니다. 로봇공학에서 학습된 포인트 클라우드 표현은 하위 조작 작업, 예를 들어 파악 제안[6, 31, 46, 55], 조작 가능도[27, 36, 37] 및 주요 포인트[16]도 용이하게 합니다. 최근 연구자들은 RL 정책에 대한 직접적인 입력 관찰로 포인트 클라우드를 사용하는 방법을 모색했습니다[8,23,41,60]. 이러한 작업에서 영감을 얻은 DexArt 벤치마크는 관절이 있는 물체를 작동하기 위해 여러 손가락 손을 사용하는 새로운 작업을 도입합니다. 조작자와 물체 모두에 대한 높은 자유도를 감안할 때 이전 환경에 비해 더 어렵습니다. 이러한 작업을 해결하기 위해 기하학적 표현 학습(예: 부분 추론)이 의사 결정에 어떤 영향을 미칠 수 있는지에 대한 광범위한 실험을 수행했는데, 이는 이전에 철저히 연구되지 않았습니다. 3. DexArt 벤치마크 다양한 난이도의 작업을 포함하는 DexArt 벤치마크를 제안합니다. 이를 사용하여 다양한 정책 학습 방법의 샘플 효율성과 일반화 가능성을 평가할 수 있습니다. 이 작업에서는 각각 보이는 것과 보이지 않는 여러 물체가 있는 4가지 능숙한 조작 작업인 수도꼭지, 양동이, 노트북 및 변기를 제공합니다(표 1 참조). 3.1. 작업 설명 수도꼭지. 그림 1의 첫 번째 행에서 볼 수 있듯이 로봇은 회전 조인트가 있는 수도꼭지를 켜야 합니다. 로봇 손은 손잡이를 단단히 잡은 다음 약 90도 회전해야 합니다. 이 작업은 능숙한 손과 팔의 동작 간의 조정을 평가합니다. 2개 턱 평행 그리퍼가 이 작업을 수행할 수 있지만 낮은 자유도 엔드 이펙터로 인해 정밀한 팔 동작에 크게 의존합니다. 평가 기준은 손잡이의 회전 각도를 기준으로 합니다. 그림 2. 작업 및 능숙한 손. 왼쪽: DexArt Benchmark에서 네 가지 작업 모두에 대한 시각화. 오른쪽: 회전 조인트 위치를 나타내는 빨간색 화살표가 있는 Allegro 손의 시각화. 양동이. 그림 1의 두 번째 행에서 표시된 대로 작업 개체 모두 보임 보이지 않음 수도꼭지 양동이 노트북 화장실 이 작업에서는 로봇이 양동이를 들어 올려야 합니다. 스테이플러블 들어올리는 동작을 보장하기 위해 로봇은 양동이 손잡이 아래로 손을 뻗고 잡고 형태 폐쇄를 구성해야 합니다[4]. 반면에 단일 평행 그리퍼는 힘 폐쇄로만 잡을 수 있으며[2], 충분히 큰 마찰 없이는 성공하기 어렵습니다. 평가에서 이 작업은 버킷이 주어진 높이까지 들어올려지면 성공으로 간주됩니다.표 1. 작업 통계.노트북.그림 1의 세 번째 행에 표시된 대로 이 작업에서 로봇은 화면 중앙을 잡은 다음 노트북 뚜껑을 열어야 합니다.이 작업은 또한 손재주가 좋은 손에 잘 맞습니다.평행 그리퍼는 턱 사이에 뚜껑을 정확하게 막아서 이를 수행할 수 있습니다.그러나 이러한 제약으로 인해 팔 동작의 어려움이 증가하고 뚜껑을 열려면 더 큰 작업 공간이 필요합니다.이 작업은 노트북 뚜껑의 변경된 각도를 기준으로 평가됩니다.변기.그림 1의 네 번째 행에 표시된 대로 이 작업은 로봇이 더 큰 변기 뚜껑을 열어야 하는 노트북 작업과 유사합니다.뚜껑의 기하학이 더 불규칙하고 다양하기 때문에 작업이 더 어렵습니다.변기 뚜껑이 임계 각도에서 열리면 작업이 성공적으로 해결됩니다.3.2. 환경 설정 벤치마크에서 우리는 인간형 손인 Allegro Hand(16 DoF)가 있는 XArm6 로봇 팔(6 DoF)을 사용하여 SAPIEN 물리적 시뮬레이터[61]에서 작업을 구현합니다. 예비 지식. 우리는 능숙한 손으로 제어 문제를 마르코프 결정 과정(MDP), M = {S, A, R, T, Po, Y}로 모델링합니다. 여기서 S = Rn, A = Rm은 각각 상태와 동작을 나타냅니다. RS × A → R은 작업 진행 상황을 측정하는 보상 함수로, 인간의 지식이 종종 통합되어 어려운 작업의 달성을 안내합니다. T : S × A → S는 전이 역학입니다. po는 초기 확률 분포이고 yЄ [0, 1)은 할인 계수입니다. 관찰 공간. 관찰은 두 부분으로 구성됩니다. 첫째, 고유 감각 데이터 Sr에는 전체 로봇의 현재 관절 위치, 선형 속도, 각속도, 엔드 이펙터 손바닥의 위치 및 포즈가 포함됩니다. 둘째, 깊이 카메라로 캡처한 부분 포인트 클라우드 P에는 관절 객체와 로봇이 포함됩니다. 관찰된 포인트 클라우드는 먼저 로봇 작업 공간 내에서 잘라낸 다음 균일하게 다운 샘플링됩니다. 또한 관찰된 포인트 클라우드 Po를 이미지화된 로봇 포인트 클라우드 Pi와 연결합니다(섹션 4.1 참조). 이러한 모든 관찰 결과는 실제 로봇에서 쉽게 접근할 수 있으며 오라클 정보는 사용되지 않습니다.행동 공간.행동은 팔의 경우 6자유도, 손의 경우 16자유도의 두 부분으로 구성된 22차원 벡터입니다.로봇 팔의 경우 첫 번째 6차원 벡터가 손바닥의 목표 선형 및 각속도인 운영 공간 제어를 사용합니다.Allegro 손의 경우 관절 위치 컨트롤러를 사용하여 16개 관절의 위치 목표를 명령합니다.두 컨트롤러 모두 PD 제어로 구현됩니다.3.3. 보상 설계 모든 능숙한 조작 작업에 대한 보상 설계는 세 가지 원칙을 따릅니다.(i) 각 작업을 합리적인 시간 내에 해결할 수 있도록 하려면 밀도 있는 보상이 필요합니다.(ii) 예상치 못한 동작을 제거하기 위해 보상은 정책의 동작을 자연스럽고(인간답고) 안전하게 조절해야 합니다.(iii) 보상 구조는 모든 작업에서 일반적이고 표준화되어야 합니다. 우리는 작업을 세 단계로 분해합니다. 기능적 부분에 도달, 손과 조작된 물체 사이의 접촉 구축, 조작된 부분을 움직이기 위한 작업별 동작 실행. 도달 및 파악 단계. 우리는 로봇 손이 조작된 물체에 가까이 다가가도록 장려하기 위해 처음 두 단계에 대한 도달 보상을 다음과 같이 설계합니다.Treach = == : 1) min(-||X palm — Xobject ||, \), (1) 1(stage 여기서 1()은 지표 함수, Xpalm이고 Xobject는 월드 프레임에서 손바닥과 물체의 3D 위치, &gt;는 보상의 갑작스러운 급증을 방지하기 위한 정규화 항입니다.식 3.3은 손과 물체 사이의 데카르트 거리만 고려하므로 예상치 못한 동작(예: 손으로 뚜껑 측면을 누르는 대신 꽉 움켜쥔 동작으로 노트북 뚜껑을 여는 동작)이 발생할 수 있습니다.현실 세계에서 이러한 동작은 조작된 물체와 로봇 자체에 손상을 줄 수 있습니다.[47]에서 영감을 얻어 손가락과 물체 사이의 더 나은 접촉을 장려하기 위해 접촉 항을 추가합니다.contact 1(stage 2) IsContact(palm, object) AND Σ IsContact(finger, object) ≥finger(2) 여기서 IsContact는 부울 함수입니다. 두 링크가 접촉하는지 확인하기 위해 충돌 감지를 수행합니다. 손바닥과 최소 두 손가락이 모두 물체에 닿으면 좋은 접촉 관계가 구축된다고 믿습니다. 부분 조작 단계. 마지막 단계에서 로봇은 관절 물체의 특정 부분을 조작하여 주어진 포즈로 이동해야 합니다. 이 단계의 보상은 다음과 같이 설계되었습니다. &quot;진행 = 1(단계 == 3) 진행(작업), (3) 여기서 진행은 현재 작업 진행에 대한 작업별 평가 함수입니다. 예를 들어 Faucet 환경에서 핸들 조인트 각도의 변화를 사용하여 작업 진행을 나타냅니다. 갑작스럽고 불안정한 로봇 동작을 제거하기 위해 동작의 L2 표준과 작업별 용어를 포함하는 페널티 용어 페널티를 추가합니다. 전체 보상은 4개의 보상 용어의 가중 합입니다. 보상 설계에 대한 자세한 내용은 보충 자료에서 찾을 수 있습니다. 3.4. 자산 선택 및 주석 PartNetMobility[61] 데이터 세트의 관절형 개체 모델을 사용합니다. 잘못된 모델링을 방지하고 일관된 운동학 구조를 보장하기 위해 각 작업에 대한 개체 모델을 수동으로 선택합니다. 또한 개체별로 크기와 초기 위치에 주석을 달아 적절한 크기를 갖고 처음에 로봇과 교차하지 않도록 합니다. 또한 개체 초기 위치에 무작위성을 적용합니다. 즉, 각 작업에 대해 적절한 회전을 수행하고 영어: 주석이 달린 위치에서의 변환을 통해 목표가 여전히 달성 가능한지 확인하고, 정책 학습 중에 학습 세트에서 객체가 무작위로 추출됩니다.4. 방법 RL 방법으로 능숙한 조작 작업을 해결하면 차원이 높은 작업 공간으로 인해 샘플 복잡도가 높아집니다. 관절이 있는 객체와 포인트 클라우드 관찰이 있는 작업은 복잡도를 더욱 높입니다.이 섹션에서는 정책 학습 성능을 개선하기 위한 여러 가지 방법을 설명합니다.4.1절에서는 정책 학습 아키텍처에 대해 설명합니다.4.2절에서는 시각적 사전 학습을 위한 데이터를 생성하는 방법에 대한 세부 정보를 설명합니다.마지막으로 4.3절에서 벤치마크에서 평가한 사전 학습 방법에 대해 설명합니다.4.1. 정책 개요 정책 학습.다양한 객체에 대한 범주 수준 일반화를 달성하기 위해 3D 포인트 클라우드를 관찰로 채택하고 RL 알고리즘으로 Proximal Policy Optimization(PPO)[53]을 사용합니다. 아키텍처 설계에서 가치 네트워크와 정책 네트워크는 그림 3의 오른쪽 부분에 표시된 것처럼 포인트 클라우드와 로봇 고유 감각에서 추출한 동일한 기능을 공유합니다. 포인트 클라우드 기능 추출기로 PointNet[44]을 사용합니다. 포인트 클라우드를 사용한다는 점에 주목할 가치가 있습니다. a. 분할 b. 재구성 포인트 넷 I c. SimSiam d. ...... 가중치 초기화 Po 포인트 넷 Pi 관찰된 + 상상된 포인트 클라우드 P MLP 로봇 고유 감각 정책 F₁ FValue 1. PointNet 사전 학습 2. 정책 학습 그림 3. 개요. 관절형 물체에 대한 능숙한 조작을 학습하기 위해 PointNet 백본이 있는 PPO 알고리즘을 채택합니다. 사전 학습을 사용하여 정책 학습 프로세스를 용이하게 합니다. (1) PointNet은 분할, 재구성, SimSiam 등을 포함하는 지각 전용 작업에 대해 사전 학습됩니다. (2) 사전 학습된 PointNet 가중치는 RL 학습 전에 PPO에서 시각적 백본을 초기화하는 데 사용됩니다. PointNet의 간단한 버전. 로컬 MLP에는 GELU 활성화 함수가 있는 은닉 계층이 하나 있고, 그 뒤에 출력 피처 F₁를 직접 생성하는 최대 풀링이 이어집니다. 한편, MLP를 사용하여 로봇 고유 감각 벡터 Sr에서 출력 피처 F₂를 추출합니다. 그런 다음 출력 피처 F₁과 F2를 연결하여 값 MLP와 정책 MLP를 통과시킵니다. 실험을 통해 비전 추출기의 볼륨을 늘리면 실제로 정책 학습에 해가 된다는 것을 보여줍니다. 피처 추출기 사전 학습. 3D 표현 학습이 3D 정책 학습에 어떻게 도움이 되는지 조사합니다. 섹션 4.3에서 논의할 자기 지도 학습과 지도 학습을 모두 포함하여 다섯 가지 다른 3D 표현 학습 방법으로 비전 사전 학습을 벤치마킹합니다. 모든 방법에 대해 PointNet 백본을 사용하여 지각 전용 작업에 시각적 모델을 사전 학습한 다음 이를 사용하여 RL에 대한 피처 추출기를 초기화합니다. 사전 학습 파이프라인은 그림 3에 나와 있습니다. 포인트 클라우드 상상. 포인트 클라우드 RL에는 두 가지 과제가 있습니다. 첫째, 손-물체 상호작용은 여러 폐색을 발생시킵니다.둘째, RL 훈련은 메모리 제한으로 인해 저해상도 포인트 클라우드만 처리할 수 있습니다.따라서 관찰에서 소수의 포인트만이 손 손가락에서 나오는데, 이는 의사 결정에 필수적인 정보입니다.[47]에서 영감을 얻어 로봇 모델을 활용하여 순방향 운동학을 통해 손가락 형상을 계산합니다.그런 다음 계산된 형상에서 상상의 포인트 클라우드라고 하는 포인트 Pi를 샘플링할 수 있습니다.그림 3의 오른쪽 부분에 표시된 것처럼 포인트 클라우드 특징 추출기는 관찰된 포인트 Po와 상상의 포인트 Pi(그림 3의 진한 색상 포인트)를 모두 입력으로 사용합니다.이렇게 하면 포인트 클라우드 관찰에서 누락된 로봇 세부 정보를 제공합니다.Pi는 실제 로봇에서도 액세스할 수 있습니다.입력 포인트 클라우드 분할 재구성 그림 4. 사전 훈련 시각화.변기(위쪽 행)와 수도꼭지(아래쪽 행)에 대한 분할 및 재구성 사전 훈련 결과를 시각화합니다.4.2. 사전 훈련 데이터 세트 DexArt 조작 데이터 세트(DAM). 우리는 조작 작업으로 설정하여 포인트 클라우드 관찰을 렌더링합니다. 데이터 세트에는 관찰된 점과 상상된 점을 포함하여 각 객체에 대한 6k 포인트 클라우드가 포함되어 있으며, 그림 4의 왼쪽 열에 표시된 대로 로봇과 관절 객체의 상태가 무작위로 샘플링됩니다. 분할 사전 학습을 위해 포인트 클라우드를 객체의 기능적 부분, 객체의 나머지 부분, 로봇 손, 로봇 팔의 4개 그룹으로 레이블을 지정합니다(그림 4의 가운데 열 참조). PartNet-Mobility 조작 데이터 세트(PMM). DAM과 달리 PMM은 로봇과 같은 작업 정보 없이 PartNetMobility[61]에서 직접 렌더링됩니다. PMM에는 46개의 객체 범주와 각 범주에 대한 1k 포인트 클라우드가 포함됩니다. 객체의 상태와 카메라 관점은 무작위로 샘플링됩니다. 분류를 위해 같은 범주에 있는 각 객체는 같은 레이블을 공유합니다. 분할을 위해 [17]의 절차에 따라 실제 segTask Split Seen Faucet Unseen Bucket Seen Unseen Laptop Seen Unseen Toilet Seen Unseen No Pre-train 0.30 0.22 0.28±0.21 0.51 ±0.12 0.56±0.08 0.81 ± 0.01 0.41±0.09 0.71 ± 0.05 0.46±0.PMM에서의 분할 0.27±0.12 0.17 ± 0.09 0.35±0.25 0.34±0.24 0.85±0.09 0.55 ± 0.09 0.66±0.08 0.44±0.PMM에서의 분류 0.20±0.12 0.18±0.14 0.56±0.06 0.58±0.12 0.80±0.20 0.41 0.14 0.69±0.08 0.38±0.DAM에서의 재구성 0.35±0.02 0.21 0.03 0.51±0.08 0.50±0.05 0.85±0.04 0.54±0.08 0.76±0.03 0.52±0.DAM에서의 SimSiam 0.60 0.15 0.45±0.12 0.41 0.30 0.38±0.31 0.84 0.04 0.49±0.13 0.82±0.02 0.50±0.분할 DAM 0.79±0.02 0.58±0.07 0.75±0.04 0.76±0.07 0.92±0.02 0.60±0.07 0.85±0.01 0.55±0.표 2. 다양한 사전 학습 방법의 성공률. 보이는 물체와 보이지 않는 물체 모두에 대한 4가지 작업의 성공률(평균±표준편차)을 보고합니다. DAM = DexArt 조작 데이터 세트, PMM = PartNet-Mobility 조작 데이터 세트(4.2절에서 설명). 그림 5. RL 튜닝 후 분할. 정책 학습 중 PointNet이 튜닝된 후 분할 결과를 시각화합니다. PPO 포인트 클라우드 피처 추출기의 가중치를 분할 네트워크에 직접 적용하여 분할 예측을 수행할 수 있습니다. 관절형 물체의 기능적 부분에 대한 멘테이션 마스크. 4.3. 사전 학습 방법 지도 사전 학습. 의미적 분할과 분류를 포함한 두 가지 지도 사전 학습 방법을 실험합니다. 분류의 경우, 46개 객체 범주에 대한 레이블을 예측하기 위해 PMM 데이터에 PointNet을 학습합니다. 파악과 같은 더 간단한 작업에 비해 관절형 객체 조작에는 3D 부분에 대한 이해가 더 필요합니다. 정책은 기능적 부분을 찾고 그것과 상호 작용하는 방법을 추론해야 합니다. 따라서 부분 분할에 대한 사전 학습이 정책 학습에 어떻게 도움이 될 수 있는지도 조사합니다. DAM과 PMM 모두에서 분할을 학습합니다. 자기 지도 사전 학습. 또한 포인트 클라우드 재구성과 SimSiam[9]을 포함한 두 가지 자기 지도 사전 학습 방법을 실험합니다. OcCo[54]에 따라 DAM 데이터 세트에 대한 포인트 클라우드 재구성을 위해 인코더-디코더 아키텍처를 사용합니다. 인코더는 글로벌 임베딩을 추출하는 PointNet이고 디코더는 글로벌 임베딩에서 원래 포인트 클라우드를 재구성하는 PCN[66]입니다. 재구성은 Chamfer loss[14]를 통해 학습됩니다. 재구성 결과는 그림 4의 오른쪽 열에 시각화되어 있습니다. 사전 학습 후 PointNet 인코더를 사용하여 PPO에서 PointNet을 초기화합니다. SimSiam을 따르고 PointNet을 사용하여 시암 네트워크를 설계합니다. SimSiam 학습에서 네트워크는 동일한 포인트 클라우드의 두 가지 증강된 뷰를 가져와 동일한 PointNet 인코더로 전달합니다. MLP는 한 쪽에 연결되어 유사성을 예측하는 반면 다른 쪽에서는 그래디언트가 중지됩니다. 이 방법은 양쪽 간의 유사성을 최대화하도록 학습됩니다. SimSiam 내에서 DAM 데이터 세트에 PointNet 인코더를 사전 학습합니다. 5. 실험 섹션 3.1에 정의된 Faucet, Bucket, Laptop, Toilet을 포함하여 제안된 작업에 대한 실험을 수행합니다. 세 가지 측면에서 실험을 수행합니다. (i) 모든 작업에 대해 보이는 관절형 객체와 보이지 않는 관절형 객체를 모두 평가하여 다양한 사전 학습 방법을 벤치마킹합니다. 학습 중과 학습 후에 성공률을 테스트합니다. (ii) 우리는 보이는 객체의 수와 시각적 백본의 아키텍처 크기가 정책 학습에 영향을 미칠 수 있는 방법을 절제합니다.(iii) 우리는 다양한 방법에 대한 카메라 시점 변화에 대한 견고성을 연구하는데, 여기서 우리는 입력 포인트 클라우드가 새로운 포즈에서 카메라에 의해 캡처될 때 작업 성공률을 평가합니다. 전반적으로, 우리는 보이는 객체와 보이지 않는 객체 모두에 대한 성공률과 에피소드별 수익률로 방법을 평가합니다. 우리는 각 실험에 대해 3가지 다른 난수 시드로 RL 정책을 훈련합니다.5.1. 주요 결과 우리는 표 2에 모든 벤치마크 방법의 성공률을 제공합니다. 우리는 처음부터 훈련된 RL 정책(첫 번째 행)을 다섯 가지 다른 사전 훈련 방법(다음 행)과 비교합니다. 결과에 따르면 적절한 시각적 사전 훈련이 정책 학습에 도움이 될 수 있습니다. 우리는 다음과 같이 우리의 발견 사항을 강조합니다.(i) 부분 분할은 모든 작업에서 정책 학습을 향상시킵니다. 모든 작업에서 가장 좋은 성과를 보입니다. 분할 사전 학습을 통해 PointNet은 기능적 부분을 더 잘 구별하고 찾을 수 있으며, 이는 매우 중요합니다.에피소드 리턴 성공률 수도꼭지 양동이 노트북 변기 1.01.01.01.00.80.80.80.80.80.60.60.60.60.40.40.0.40.20.20.20.20.00.00.00.0300-500200100200100-0+환경 단계(×106)환경 단계(×106) DAM에서의 분할(50%) 0-0-う환경 단계(×106) 환경 단계(×106) DAM에서의 분할(100%) 그림 6. 보이는 개체의 수가 다른 학습 프로세스. x축은 환경 단계입니다. 위쪽 행의 y축은 3개의 랜덤 시드로 평가한 보이지 않는 객체에 대한 성공률이고, 음영 처리된 영역은 표준 편차를 나타냅니다. 아래쪽 행의 y축은 에피소드 수익률이며, 음영 처리된 영역은 표준 오차를 나타냅니다. 회색 곡선은 각 범주 내에서 보이는 DexArt 객체의 약 50%에 대해 분할 사전 학습을 수행한 방법을 보여주는 반면, 빨간색 곡선은 100%로 사전 학습되었습니다. 성공률 수도꼭지 1.0.80.60.0.20.0-200-에피소드 반환 버킷 1.00.80.60.40.20.0-노트북 1.00.80.60.40.20.0200-화장실 1.00.80.60.40.20.0200100-8환경 단계(×106) 10 20환경 단계(×106)환경 단계(×106) 환경 단계(×106) 작은 PointNet 중간 PointNet 큰 PointNet 그림 7. 다양한 PointNet 크기로 학습. 축은 그림 6과 같은 의미입니다. 트리 곡선을 사용한 실험은 5.2절에 설명된 소형/중간/대형 PointNet을 사용하여 관절형 객체 조작을 위해 DAM(100%)에서 미리 학습된 분할입니다. (ii) 대표적인 글로벌 임베딩을 학습하기 위한 다른 사전 학습 방법, 즉 분류, 재구성 및 SimSiam도 어떤 경우에는 정책 학습을 개선하며, 특히 노트북 작업에서 그렇습니다.(iii) 수도꼭지 손잡이와 같이 작은 기능 부품을 조작하는 작업은 분할 사전 학습에서 더 많은 이점을 얻습니다.그림 8에서 볼 수 있듯이 분할 결과는 작은 수도꼭지 손잡이의 라벨을 올바르게 예측할 수 있는 반면, 재구성은 글로벌 모양 완전성에 더 초점을 맞추고 작은 부품의 세부 정보를 무시합니다.따라서 부품 분할은 보다 섬세한 조작 작업에 더 효과적인 사전 학습 방법입니다.5.2. 절제 연구 우리는 훈련에 사용된 객체 수, 비전 추출기의 크기 및 다양한 시각적 표현 학습 방법이 일반화에 어떻게 영향을 미치는지 절제합니다.보이는 객체의 수.섹션 5.2의 이전 실험과 달리, 우리는 보이는 객체의 50%만을 사용하여 모델과 정책을 훈련합니다. 우리는 50%와 100%의 객체를 사용하는 방법에 대한 학습 곡선과 새로운 객체에 대한 성공률을 보고합니다. 그림에서 성공률 Faucet Bucket 1.1.0.80.0.0.6€ 0.40.0.0.0.00.0-환경 단계(×106) PMM에서의 분할 PMM에서의 분류 DAM에서의 재구성 SimSiam on DAME환경 단계(×106) DAM에서의 분할 사전 학습 없음그림 8. 다양한 사전 학습 방법. Faucet 및 Bucket 작업에서 다양한 방법의 평가 성공률. 음영 처리된 영역은 표준 편차를 나타냅니다. 영어: ure 6, 100% 보이는 객체의 수렴 속도(에피소드 반환으로 표현)는 더 다양한 객체 형상으로 인해 50%에 비해 느리지만, 보이지 않는 객체에 대한 100% 훈련의 성공률(맨 위 행)은 전체 훈련 프로세스와 모든 작업에서 더 높게 유지됩니다. 이는 더 많은 훈련 객체가 더 나은 정책 일반화에 필수적임을 보여줍니다. Vision Extractor의 크기. PointNet에 대해 세 가지 다른 크기로 실험합니다. (i) 숨겨진 레이어가 하나 있는 작은 PointNet. (ii) 숨겨진 레이어가 세 개 있는 중간 크기 PointNet. (iii) 숨겨진 레이어가 다섯 개 있는 큰 PointNet. 이 세 PointNet의 다른 모든 구성 요소는 동일합니다. 놀랍게도 가장 작은 PointNet이 그림 7에서 볼 수 있듯이 성공률과 에피소드 반환 모두에서 가장 좋은 성능을 달성한다는 것을 알게 되었습니다. 비전 관점에서의 일반적인 이해와 달리 작은 네트워크는 더 빠르게 훈련할 뿐만 아니라 더 잘 일반화합니다. 사전 훈련된 PointNet(보임) 사전 훈련된 PointNet(보이지 않음) 1.1.0.8y 0.8y 0.0.0.0.0.0.+0.0.-50-Ophi -50-Ophi --theta-theta-사전 훈련된 ResNet-18(보임) 1.+1.0.8 S R3M [42]. 0.0.0.0.0.0.0.+0.+0.Ophi -0-Ophi --theta50 -비3D 표현. 우리는 Laptop 작업에서 DAM 분할에 대해 사전 학습된 PointNet을 다음의 2D 사전 학습 표현과 비교합니다: R3M에서, Ego4D 인간 비디오 데이터 세트는 시간 대조 학습 및 videoEncoder PointNet Seen Unseen 0.78 ± 0.ResNet-18 0.64±0.0.41 0.0.28 ± 0.-50-theta표 3. 비-3D 표현. 언어 정렬을 사용하여 ResNet18을 사전 학습하는 데 사용되었습니다. 표 3은 실험 결과를 보여줍니다. 결과에 따르면 PointNet을 사용한 3D 시각적 표현 학습은 객체를 조작하는 데 더 효과적입니다. 비-3D 표현 학습과 비교하여 3D 정책은 보이는 객체와 보이지 않는 객체 모두에서 더 나은 조작 성능을 달성할 수 있습니다. 5.3. 시점 변경에 대한 견고성 우리는 PointNet 및 ResNet-18 기반 정책의 견고성을 평가하기 위해 Laptop 작업에서 카메라의 시점 변경을 실험합니다. PointNet 정책은 DAM 분할에 대해 사전 학습되었고 ResNet-18 정책은 R3M에 대해 사전 학습되었습니다. 시점 샘플링 절차는 다음과 같이 설명할 수 있습니다. (i) 카메라 포즈 샘플링을 위한 반구를 결정합니다. 먼저 초기 카메라 위치에서 조작된 객체까지의 거리를 사용하여 반구의 반경 r을 계산합니다. 이 반구의 중심은 거리 r로 카메라 광학 선을 따라 이동하여 정의됩니다. (ii) 반구의 한 지점을 카메라 위치로 샘플링합니다. 훈련 시점을 기준으로 -60°에서 60°까지 20°마다 방위각을 균일하게 샘플링하고 -20°에서 20°까지 20°마다 극각을 샘플링합니다. 총 7 x 5 = 35개의 카메라 위치가 생성됩니다. (iii) 카메라를 회전하여 반구 중심을 가리킵니다. 위의 절차를 사용하여 35개의 카메라 포즈를 샘플링합니다. 추론 중에 이러한 카메라 포즈를 설정합니다. 그림 9에서 보듯이, 훈련된 PointNet 정책은 방위각을 60°, 극각을 20° 변경하더라도 시점 변경에 대해 뛰어난 견고성을 보여줍니다. 반면, ResNet-18 정책의 성공률은 훈련 시점과 새로운 평가 시점의 차이가 증가하면 극적으로 떨어집니다. 이는 견고성이 주로 포인트 클라우드 표현 학습과 PointNet 아키텍처에서 비롯된다는 것을 알려줍니다. 50 - 사전 훈련된 ResNet-18(보이지 않음) 그림 9. 다양한 시점에서의 성공률. x축은 극각 &amp;(훈련 시점을 기준으로 함)이고 y축은 물체를 중심으로 하는 반구의 방위각 0입니다. z축은 성공률을 나타냅니다. 훈련 중 시점은 파란색 별로 강조 표시됩니다. 6.
--- CONCLUSION ---
우리는 관절 물체를 이용한 능숙한 조작에 대한 새로운 벤치마크를 제안하고 RL 정책의 일반화 가능성을 연구합니다. 우리는 여러 가지 통찰력을 제공하기 위해 다양한 방법을 실험하고 벤치마킹합니다. (i) 더 다양한 물체를 이용한 RL은 더 나은 일반화 가능성을 가져옵니다. 우리는 더 많은 물체를 이용한 훈련이 보이지 않는 물체에 대한 더 나은 성능을 지속적으로 가져온다는 것을 발견했습니다. (ii) 능숙한 조작 작업을 수행하기 위해 RL 훈련에 큰 인코더가 필요하지 않을 수 있습니다. 우리는 모든 환경에서 가장 간단한 PointNet이 항상 더 나은 샘플 효율성과 최고의 일반화 가능성을 가져온다는 것을 발견했습니다. (iii) 3D 시각적 이해는 정책 학습에 도움이 됩니다. 부분 분할은 작은 기능적 부분을 이용한 조작을 제공하는 반면, 더 큰 기능적 부분을 이용한 작업은 모든 시각적 사전 훈련 방법의 이점을 얻습니다. (iv) PointNet 특징 추출기를 이용한 기하학적 표현 학습은 카메라 시점 변화에 대한 정책에 강력한 견고성을 제공합니다. 결론적으로, 우리는 DexArt가 일반화 가능한 능숙한 조작과 지각과 의사 결정 간의 공동 개선을 연구하는 플랫폼 역할을 할 수 있기를 바랍니다. 감사의 말. 이 논문에 귀중한 피드백을 제공해 주신 Hao Su에게 감사드립니다. 이 프로젝트는 대한민국 산업통상자원부에서 지원하는 산업기술혁신사업(20018112, 시각촉각 감지 기반 모방 학습을 활용한 자율 조작 및 그립 기술 개발), Amazon Research Award 및 Qualcomm의 기부금으로 일부 지원되었습니다. 참고문헌 [1] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. 손재주 있는 인핸드 조작 학습. The International Journal of Robotics Research, 39(1):3–20, 2020. 1,[2] Antonio Bicchi. 로봇 그립의 폐쇄 특성에 관하여. The International Journal of Robotics Research, 14(4):319–334, 1995.[3] Antonio Bicchi and Vijay Kumar. 로봇식 움켜잡기와 접촉: 리뷰.IEEE 국제 로봇 및 자동화 컨퍼런스, 1권, 348-353페이지.IEEE, 2000.[4] Antonio Bicchi 및 Vijay Kumar. 로봇식 움켜잡기와 접촉: 리뷰.Proceedings 2000 ICRA.밀레니엄 컨퍼런스.IEEE 국제 로봇 및 자동화 컨퍼런스.심포지엄 회의록(Cat. No. 00CH37065), 1권, 348-353페이지.IEEE, 2000.[5] Jeannette Bohg, Antonio Morales, Tamim Asfour 및 Danica Kragic.데이터 기반 움켜잡기 합성-조사.IEEE 로봇공학 저널, 30(2):289-309, 2013.[6] Samarth Brahmbhatt, Ankur Handa, James Hays 및 Dieter Fox.접촉 움켜잡기: 접촉에서 기능적 다중 손가락 움켜잡기 합성. 2019년 IEEE/RSJ 국제 지능형 로봇 및 시스템 컨퍼런스(IROS), 2386-2393페이지. IEEE, 2019.[7] Yu-Wei Chao, Chris Paxton, Yu Xiang, Wei Yang, Balakumar Sundaralingam, Tao Chen, Adithyavairavan Murali, Maya Cakmak, Dieter Fox. Handoversim: 인간-로봇 객체 핸드오버를 위한 시뮬레이션 프레임워크 및 벤치마크. 2022년 국제 로봇 및 자동화 컨퍼런스(ICRA), 6941-6947페이지. IEEE, 2022.[8] Tao Chen, Jie Xu, Pulkit Agrawal. 일반적인 손 안의 객체 재조정을 위한 시스템. 로봇 학습 컨퍼런스, 297-307페이지. PMLR, 2022. 1, 2,[9] Xinlei Chen과 Kaiming He. 간단한 샴 표현 학습 탐색. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 15750-15758페이지, 2021.[10] Zoey Qiuyu Chen, Karl Van Wyk, Yu-Wei Chao, Wei Yang, Arsalan Mousavian, Abhishek Gupta, Dieter Fox. Dextransfer: 최소한의 인간 시연을 통한 실제 세계의 여러 손가락 능숙한 움켜잡기. arXiv 사전 인쇄본 arXiv:2209.14284, 2022.[11] Sachin Chitta, Benjamin Cohen, Maxim Likhachev. 모바일 조작기를 사용한 자율 도어 개방 계획. 2010년 IEEE 로봇 및 자동화 국제 컨퍼런스, 1799-1806페이지. IEEE, 2010.[12] Sudeep Dasari, Jianren Wang, Joyce Hong, Shikhar Bahl, Yixin Lin, Austin Wang, Abitha Thankaraj, Karanbir Chahal, Berk Calli, Saurabh Gupta, et al. Rb2: 변형된 로봇 조작 벤치마킹. arXiv 사전 인쇄본 arXiv:2203.08098, 2022.[13] Mehmet R Dogar 및 Siddhartha S Srinivasa. 능숙한 손으로 밀고 잡기: 역학 및 방법. 2010.[14] Haoqiang Fan, Hao Su 및 Leonidas J Guibas. 단일 이미지에서 3D 객체를 재구성하기 위한 점 집합 생성 네트워크. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 605-613페이지, 2017.[15] Samir Yitzhak Gadre, Kiana Ehsani 및 Shuran Song. 역할 수행: 관절형 객체 부분 발견을 위한 상호 작용 전략 학습. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 15752-15761페이지, 2021. 2,[16] Wei Gao 및 Russ Tedrake. kpam-sc: 키포인트 어포던스 및 모양 완성을 사용한 일반화 가능한 조작 계획. 2021 IEEE 국제 로봇 및 자동화 컨퍼런스(ICRA), 6527-6533페이지. IEEE, 2021.[17] Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi, Siyuan Huang 및 He Wang. Gapartnet: 일반화 가능하고 실행 가능한 부분을 통한 교차 범주 도메인 일반화 가능한 객체 인식 및 조작, 2022.[18] Benjamin Graham, Martin Engelcke 및 Laurens Van Der Maaten. 부분 다양체 희소 합성 신경망을 사용한 3차원 의미 분할. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9224-9232쪽, 2018.[19] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiaing Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, Hao Su. Maniskill2: 일반화 가능한 조작 기술을 위한 통합 벤치마크. International Conference on Learning Representations, 2023.[20] Abhishek Gupta, Clemens Eppner, Sergey Levine, Pieter Abbeel. 인간 시연을 통해 부드러운 로봇 손에 대한 능숙한 조작 학습. IEEE/RSJ International Conference on Intelligent Robots and Systems, 3786-3793쪽. IEEE, 2016.[21] Li Han 및 Jeffrey C Trinkle. 롤링 및 손가락 보행을 통한 능숙한 조작. IEEE 국제 로봇 및 자동화 컨퍼런스, 1권, 730-735페이지. IEEE, 1998.[22] Eric Heiden, Ziang Liu, Vibhav Vineet, Erwin Coumans 및 Gaurav S Sukhatme. RGBD 비디오에서 관절형 강체 역학 추론. arXiv 사전 인쇄본 arXiv:2203.10488, 2022.[23] Wenlong Huang, Igor Mordatch, Pieter Abbeel 및 Deepak Pathak. 기하학 인식 멀티태스크 학습을 통한 능숙한 조작의 일반화. arXiv 사전 인쇄본 arXiv:2111.03062, 2021. 2,[24] Zhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu, Hao Su. 일반 전문가 학습을 통한 정책 최적화 개선. 기계 학습 국제 컨퍼런스, 10104-10119페이지. PMLR, 2022. 동일: [25] Zhenyu Jiang, Cheng-Chun Hsu, Yuke Zhu. 상호 작용을 통한 관절 객체의 디지털 트윈 구축. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 5616-5626페이지, 2022. 2,[26] Edward Johns. 거친 모방 학습: 단일 데모를 통한 로봇 조작. IEEE 로봇 및 자동화 국제 컨퍼런스(ICRA), 4613-4619페이지. IEEE, 2021.[27] David Inkyu Kim 및 Gaurav S Sukhatme. 로봇 조작을 위한 객체 제공성을 갖춘 3D 포인트 클라우드의 의미적 라벨링. 2014년 IEEE 국제 로봇 및 자동화 컨퍼런스(ICRA), 5578-5584페이지. IEEE, 2014.[28] Vikash Kumar, Emanuel Todorov 및 Sergey Levine. 학습된 로컬 모델을 사용한 최적 제어: 능숙한 조작에 대한 응용. 2016년 IEEE 국제 로봇 및 자동화 컨퍼런스(ICRA), 378-383페이지. IEEE, 2016.[29] Sergey Levine, Nolan Wagener 및 Pieter Abbeel. 안내 정책 검색을 통한 접촉이 풍부한 조작 기술 학습. IEEE 국제 로봇 및 자동화 컨퍼런스, ICRA, 156-163페이지. IEEE, 2015. 1,[30] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn Abbott, Shuran Song. 범주 수준 관절형 객체 포즈 추정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3706-3715페이지, 2020.[31] Hongzhuo Liang, Xiaojian Ma, Shuang Li, Michael Görner, Song Tang, Bin Fang, Fuchun Sun, Jianwei Zhang. Pointnetgpd: 포인트 세트에서 그립 구성 감지. 2019년 국제 로봇 및 자동화 컨퍼런스(ICRA), 3629-3635페이지, 2019.[32] Minghua Liu, Xuanlin Li, Zhan Ling, Yangyan Li, Hao Su. 프레임 마이닝: 3D 포인트 클라우드에서 로봇 조작을 배우기 위한 무료 점심. arXiv 사전 인쇄본 arXiv:2210.07442, 2022.[33] Qihao Liu, Weichao Qiu, Weiyao Wang, Gregory D Hager, Alan L Yuille. 기하학적 제약 조건만: 관절형 객체 포즈 추정을 위한 모델 없는 방법. arXiv 사전 인쇄본 arXiv:2012.00088, 2020.[34] Ziyuan Liu, Wei Liu, Yuzhe Qin, Fanbo Xiang, Minghao Gou, Songyan Xin, Maximo A Roa, Berk Calli, Hao Su, Yu Sun, et al. Ocrtoc: 로봇 그립 및 조작을 위한 클라우드 기반 경쟁 및 벤치마크. IEEE Robotics and Automation Letters, 7(1):486-493, 2021.[35] Mayank Mittal, David Hoeller, Farbod Farshidian, Marco Hutter, and Animesh Garg. 전신 모바일 조작을 통한 알려지지 않은 장면에서의 관절형 객체 상호작용. arXiv 사전 인쇄본 arXiv:2103.10534, 2021.[36] Kaichun Mo, Leonidas J Guibas, Mustafa Mukadam, Abhinav Gupta, and Shubham Tulsiani. Where2act: 픽셀에서 관절형 3D 객체의 동작까지. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 6813-6823페이지, 2021.[37] Kaichun Mo, Yuzhe Qin, Fanbo Xiang, Hao Su, and Leonidas Guibas. 020-afford: 주석 없는 대규모 객체-객체 어포던스 학습. 로봇 학습 컨퍼런스, 1666-1677페이지. PMLR, 2022.[38] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna Tripathi, Leonidas J Guibas, Hao Su. Partnet: 세분화되고 계층적인 부분 수준 3D 객체 이해를 위한 대규모 벤치마크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 909-918페이지, 2019. 2,[39] Igor Mordatch, Zoran Popović, Emanuel Todorov. 손 조작을 위한 접촉 불변 최적화. ACM SIGGRAPH/Eurographics 컴퓨터 애니메이션 심포지엄 회의록, 137-144페이지, 2012.[40] Jiteng Mu, Weichao Qiu, Adam Kortylewski, Alan Yuille, Nuno Vasconcelos, Xiaolong Wang. A-sdf: 관절 모양 표현을 위한 얽힘이 풀린 부호 거리 함수 학습. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 13001-13011쪽, 2021.[41] Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia, Hao Su. Maniskill: 대규모 시연을 통한 일반화 가능한 조작 기술 벤치마크. arXiv 사전 인쇄본 arXiv:2107.14483, 2021. 2,[42] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta. R3m: 로봇 조작을 위한 범용 시각적 표현, 2022.[43] Domenico Prattichizzo, Jeffrey C Trinkle. Grasping. Springer handbook of robotics, 955-988페이지. Springer, 2016.[44] Charles R Qi, Hao Su, Kaichun Mo, Leonidas J Guibas. Pointnet: 3D 분류 및 분할을 위한 점 집합에 대한 딥 러닝. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 652-660페이지, 2017. 2, 3,[45] Charles Ruizhongtai Qi, Li Yi, Hao Su, Leonidas J Guibas. Pointnet++: 메트릭 공간의 점 집합에 대한 딥 계층적 특징 학습. 신경 정보 처리 시스템의 발전, 30, 2017.[46] Yuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu, Hao Su. S4g: 복잡한 장면에서 비모달 단일 뷰 단일 샷 se(3) 파악 감지. 로봇 학습 회의록, 기계 학습 연구 회의록 100권, 53-65페이지. PMLR, 2020.[47] Yuzhe Qin, Binghao Huang, Zhao-Heng Yin, Hao Su, Xiaolong Wang. Dexpoint: 시뮬레이션에서 실제 능숙한 조작으로의 일반화 가능한 포인트 클라우드 강화 학습. 2022. 4,[48] Yuzhe Qin, Hao Su, Xiaolong Wang. 한 손에서 여러 손으로: 단일 카메라 원격 조작에서 능숙한 조작을 위한 모방 학습. IEEE Robotics and Automation Letters, 7(4):10873-10881, 2022.[49] Yuzhe Qin, Yueh-Hua Wu, Shaowei Liu, Hanwen Jiang, Ruihan Yang, Yang Fu, Xiaolong Wang. Dexmv: 인간 비디오에서 능숙한 조작을 위한 모방 학습. arXiv 사전 인쇄본 arXiv:2108.05877, 2021.[50] Ilija Radosavovic, Xiaolong Wang, Lerrel Pinto, Jitendra Malik. 능숙한 조작을 위한 상태 전용 모방 학습. 2021년 IEEE/RSJ 지능형 로봇 및 시스템 국제 컨퍼런스(IROS), 7865-7871페이지. IEEE, 2020.[51] Daniela Rus. 조각적으로 매끄러운 3차원 물체의 손으로 능숙하게 조작. 국제 로봇 연구 저널, 1999.[52] Andreas J Schmid, Nicolas Gorges, Dirk Goger, Heinz Worn. 다중 감각 촉각 피드백을 사용하여 휴머노이드 로봇으로 문 열기. 2008년 IEEE 국제 로봇 및 자동화 컨퍼런스, 285-291페이지. IEEE, 2008.[53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. 근접 정책 최적화 알고리즘. arXiv 사전 인쇄본 arXiv:1707.06347, 2017.[54] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, Matt J Kusner. 폐색 완료를 통한 비지도 포인트 클라우드 사전 학습. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 9782-9792페이지, 2021.[55] Lirui Wang, Yu Xiang, Wei Yang, Arsalan Mousavian, Dieter Fox. 포인트 클라우드를 사용한 6d 로봇 파악을 위한 목표 보조 행위자 비평가. 로봇 학습 컨퍼런스, 70-80페이지. PMLR, 2022.[56] Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qinping Zhao, Kai Xu. Shape2motion: 3D 모양의 동작 부분 및 속성에 대한 공동 분석. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 8876-8884페이지, 2019.[57] Yijia Weng, He Wang, Qiang Zhou, Yuzhe Qin, Yueqi Duan, Qingnan Fan, Baoquan Chen, Hao Su, Leonidas J Guibas. Captra: 포인트 클라우드에서 강체 및 관절형 객체에 대한 범주 수준 포즈 추적. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 회의록, 13209-13218페이지, 2021.[58] Albert Wu, Michelle Guo, and C Karen Liu. 생성 모델과 2단계 최적화를 통해 다양하고 물리적으로 실행 가능한 능숙한 그립을 학습합니다. arXiv 사전 인쇄본 arXiv:2207.00195, 2022.[59] Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian Wang, Tianhao Wu, Qingnan Fan, Xuelin Chen, Leonidas Guibas, and Hao Dong. Vat-mart: 3D 관절 물체를 조작하기 위한 시각적 동작 궤적 제안 학습. arXiv 사전 인쇄본 arXiv:2106.14440, 2021.[60] Yueh-Hua Wu, Jiashun Wang, and Xiaolong Wang. 인간의 그립 가능성을 통해 일반화 가능한 능숙한 조작을 학습합니다. arXiv 사전 인쇄본 arXiv:2204.02320, 2022.[61] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J. Guibas 및 Hao Su. SAPIEN: 시뮬레이션된 부품 기반 대화형 환경입니다. 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE 컨퍼런스, 2020년 6월. 3, 4,[62] Zhenjia Xu, Zhanpeng He 및 Shuran Song. 연결된 객체에 대한 범용 조작 정책 네트워크. IEEE 로봇공학 및 자동화 서한, 7(2):2447-2454, 2022.[63] Jianglong Ye, Jiashun Wang, Binghao Huang, Yuzhe Qin 및 Xiaolong Wang. 인간의 시연을 통해 능숙한 손으로 지속적인 잡기 기능을 학습합니다. arXiv 사전 인쇄본 arXiv:2207.05053, 2022.[64] Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, Leonidas Guibas. 관절형 객체 쌍에서 심부 부분 유도. arXiv 사전 인쇄본 arXiv:1809.07417, 2018. 2,[65] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, Sergey Levine. Metaworld: 멀티태스크 및 메타 강화 학습을 위한 벤치마크 및 평가. 로봇 학습 컨퍼런스, 1094-1100페이지. PMLR, 2020.[66] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, Martial Hebert. Pcn: 포인트 완성 네트워크. InInternational Conference on 3D Vision(3DV), 728-737페이지, 2018.[67] Vicky Zeng, Tabitha Edith Lee, Jacky Liang, Oliver Kroemer. 관절형 물체 부분의 시각적 식별. 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS), 2443-2450페이지. IEEE, 2021.[68] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, Pieter Abbeel. 가상 현실 원격 조작에서 복잡한 조작 작업을 위한 심층적 모방 학습. 2018 IEEE International Conference on Robotics and Automation(ICRA), 5628-5635페이지, 2018.[69] Henry Zhu, Abhishek Gupta, Aravind Rajeswaran, Sergey Levine, Vikash Kumar. 딥 강화 학습을 통한 능숙한 조작: 효율적이고 일반적이며 저비용. 2019년 국제 로봇 및 자동화 컨퍼런스(ICRA), 3651~3657페이지. IEEE, 2019. 1,
