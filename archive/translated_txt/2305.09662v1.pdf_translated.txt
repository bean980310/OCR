--- ABSTRACT ---
텍스트 기반 인간 동작 생성은 애니메이션과 로봇 공학에 걸친 영향력 있는 응용 프로그램으로 인해 상당한 관심을 받았습니다. 최근 동작 생성을 위한 확산 모델을 적용하여 생성된 동작의 품질을 개선할 수 있었습니다. 그러나 기존 접근 방식은 비교적 소규모 동작 캡처 데이터에 의존하여 제한되어 더 다양한 야생 프롬프트에서 성능이 저하됩니다. 이 논문에서는 대규모 이미지-텍스트 데이터 세트에서 더 다양한 포즈와 프롬프트를 학습하여 이전 작업에 비해 성능이 크게 향상된 텍스트 조건 인간 동작 생성 모델인 Make-An-Animation을 소개합니다. Make-An-Animation은 두 단계로 학습합니다. 먼저 이미지-텍스트 데이터 세트에서 추출한 (텍스트, 정적 가상 포즈) 쌍의 큐레이팅된 대규모 데이터 세트에서 학습합니다. 둘째, 동작 캡처 데이터를 미세 조정하여 시간 차원을 모델링하기 위해 추가 레이어를 추가합니다. 동작 생성을 위한 이전 확산 모델과 달리 Make-An-Animation은 최근의 텍스트-비디오 생성 모델과 유사한 UNet 아키텍처를 사용합니다. 모션 사실성과 입력 텍스트와의 정렬에 대한 인간의 평가는 우리 모델이 텍스트-모션 생성에서 최첨단 성능에 도달했음을 보여줍니다. 생성된 샘플은 https://azadis.github.io/make-ananimation에서 볼 수 있습니다. 1.
--- INTRODUCTION ---
세계가 가상 공간으로 이동하고 구체화된 에이전트가 더욱 유능해짐에 따라 그럴듯한 인간 동작을 생성하는 것이 점점 더 중요해질 것입니다. 음성이나 텍스트는 아마도 생성 모델을 촉발하는 가장 자연스러운 방법일 것이며, 이것이 텍스트-투-엑스 생성 연구가 폭발적으로 증가한 이유 중 하나입니다. 텍스트에서 인간 동작을 생성하는 것은 가상 세계와 실제 세계에서 다양한 응용 프로그램을 가지고 있습니다. 예를 들어, 음성으로 로봇을 제어하고, 더 빠른 비디오 게임을 개발하고, 인간을 특징으로 하는 특수 효과를 만들고, 사용자가 음성이나 텍스트를 통해 자신이나 다른 사람의 아바타의 동작을 제어할 수 있는 새로운 메타버스 사용자 상호 작용 모드를 가능하게 합니다. 한 사람이 도끼로 나무를 자르고 있습니다. 한 사람이 아이스 스케이팅을 하고 있습니다. 그림 1. 텍스트 조건부 동작 생성을 위해 Make-An-Animation에서 생성한 샘플. 신체 모델의 조명은 시간에 따른 진행 상황을 나타냅니다. 더 어두운 색상은 시퀀스에서 나중의 프레임을 나타냅니다. 더 나은 시각화를 위해 위 이미지에서 프레임은 수평으로 분포됩니다. 텍스트 조건부 인간 동작 생성은 여러 가지 이유로 어렵습니다. 첫째, 인간의 동작과 그 동작을 설명하는 텍스트 설명의 엄청난 다양성이 존재합니다(즉, 작업은 다대다 분포 매칭입니다). 둘째, 동작 캡처 데이터를 수집하는 데 비용이 많이 들기 때문에 인간 동작 데이터 세트는 규모와 다양성이 제한적입니다. 최근 텍스트-동작 생성은 생성된 포즈의 품질과 다양성에서 빠른 개선을 보였습니다. 특히, 텍스트 조건 이미지 및 비디오 생성에서 인상적인 결과를 낸 확산 모델은 인체 자세를 생성하는 데 효과적인 것으로 입증되었습니다[35, 16, 38]. 그러나 이러한 모델은 동작 캡처 데이터 분포 밖에 있는 야생 프롬프트에 어려움을 겪습니다. 이러한 단점을 해결하기 위해, 야생 이미지-텍스트 데이터 세트를 통해 다양한 인간 자세를 자연어 설명에 매핑하는 방법을 학습하고 동작 캡처 데이터에만 의존하는 이전 최첨단 모델보다 상당히 개선된 텍스트 조건 인간 동작 생성 모델인 Make-AnAnimation을 제시합니다. 우리 접근 방식의 핵심은 이미지-텍스트 데이터 세트에서 추출한 대규모 인간 자세 데이터 세트입니다. 우리의 주요 관찰 결과는 우리가 대규모 야외 비디오와 인간 포즈 이미지에 접근할 수 있을 때 모션 캡처 데이터에서 인간 동작을 학습하는 데 국한되어서는 안 된다는 것입니다. 따라서 모션 캡처 데이터 세트의 제한된 규모와 다양성을 해결하기 위해 인간이 포함된 이미지로 필터링된 이미지-텍스트 데이터 세트에서 대규모 텍스트 가상 포즈(TPP) 데이터 세트를 추출합니다. 이 TPP 데이터 세트에는 35M(텍스트, 정적 포즈) 쌍이 포함되어 있습니다. Make-An-Animation은 두 단계로 학습합니다. (1) TPP 데이터 세트에서 텍스트 조건 정적 3D 포즈 생성 확산 모델을 학습합니다. 이 단계에서 Make-AnAnimation은 인간 포즈의 분포와 텍스트와의 정렬을 학습합니다. (2) 그런 다음 새로운 시간 차원을 모델링하고 널리 사용되는 모션 캡처 데이터에서 학습하는 시간 합성곱 및 어텐션 계층을 추가하여 사전 학습된 확산 모델을 모션 생성으로 확장합니다. 이 두 번째 단계에서 모델은 모션, 즉 시간적으로 일관된 방식으로 포즈를 연결하는 방법을 학습합니다. 중요한 점은 실행 가능한 포즈의 분포나 텍스트와의 정렬을 다시 학습할 필요가 없다는 것입니다.Make-An-Animation 아키텍처는 최근의 텍스트-비디오 확산 모델과 유사한 U-Net입니다.우리는 대규모 언어 데이터로 훈련된 언어 모델에서 추출한 텍스트 표현에 U-Net을 조건화합니다.우리는 신체 관절과 루트 오리엔트에 대한 6D 연속 SMPL 표현을 사용하여 인간의 동작을 3D SMPL 신체 매개변수의 시퀀스로 표현합니다.또한 x, y, z 차원 각각에서 위치를 나타내는 3D 벡터를 통해 프레임당 글로벌 위치를 표현합니다.400개의 다양한 텍스트 프롬프트를 수집하여 인간 평가를 통해 생성된 포즈 사실성과 텍스트 정렬 측면에서 우리 방법이 이전 작업보다 성능이 우수함을 보여줍니다.요약하자면, 우리의 주요 기여는 다음과 같습니다.• 우리는 Make-An-Animation을 제시합니다.이것은 특히 다양한 야생 텍스트 프롬프트에서 이전의 최첨단 모델을 개선한 텍스트 조건화된 인간 동작 생성 모델입니다. • 우리는 처음으로 대규모 이미지 데이터 세트를 활용하여 생성을 위한 야생 인간 포즈를 학습하는 방법을 보여줍니다. 우리는 수집된 텍스트 가상 포즈 데이터 세트에 대한 사전 학습이 널리 사용되는 모션 캡처 데이터 세트의 분포를 벗어난 프롬프트에 대한 일반화를 크게 개선하는 동시에 모션 캡처 테스트 세트에서 비슷한 성능을 보여준다는 것을 절제를 통해 보여줍니다. • 우리는 대규모 언어 데이터에서 사전 학습된 언어 모델을 활용하고 시간 합성 및 어텐션 계층을 추가하여 정적 텍스트-포즈 생성 확산 모델을 모션 생성으로 자연스럽게 확장하는 인간 모션 생성을 위한 U-Net 아키텍처를 제시합니다. 2.
--- RELATED WORK ---
s 인간 포즈 및 동작 생성 인간 포즈 및 동작 합성에 대한 이전 작업에서는 무조건적으로 [36, 40, 25] 또는 사전 동작 [20, 29], 동작 클래스 [11, 23, 5] 또는 음악 [17, 18]과 같은 다양한 입력 신호에 따라 생성 모델을 탐구했습니다. 포즈 및 동작 합성에서 텍스트 설명을 지침으로 사용하는 것은 많은 기존 작업에서 포즈 표현으로 2D 키포인트를 사용하는 최근의 연구 방향이었습니다. [41]은 GAN 모델에 공급된 입력 텍스트를 기반으로 8개 클러스터에서 기본 포즈를 선택하여 인간 이미지를 생성합니다. [39]는 GAN을 사용하여 텍스트 입력에 따라 신체 키포인트에 대한 히트맵 세트를 생성합니다. [28]은 키포인트 히트맵의 초기 대략적 추정 위에 정제 단계가 도입되는 대략적-미세적 접근 방식을 제안합니다. 위와 다른
--- METHOD ---
생성된 포즈 사실성과 텍스트 정렬 측면에서 이전 작업보다 성능이 뛰어납니다. 요약하자면, 우리의 주요 기여 사항은 다음과 같습니다. • Make-An-Animation을 제시합니다. 이는 텍스트 조건화된 인간 동작 생성 모델로, 특히 다양한 실제 텍스트 프롬프트에서 이전 최첨단 모델을 개선합니다. • 처음으로 대규모 이미지 데이터 세트를 활용하여 실제 인간 포즈를 학습하여 생성하는 방법을 보여줍니다. 수집된 텍스트 가상 포즈 데이터 세트에 대한 사전 학습이 널리 사용되는 모션 캡처 데이터 세트의 분포를 벗어난 프롬프트에 대한 일반화를 크게 개선하는 동시에 모션 캡처 테스트 세트에서 비슷한 성능을 보여준다는 것을 절제를 통해 보여줍니다. • 대규모 언어 데이터에서 사전 학습된 언어 모델을 활용하고 시간적 합성곱 및 어텐션 계층을 추가하여 정적 텍스트-포즈 생성 확산 모델을 자연스럽게 모션 생성으로 확장하는 인간 동작 생성을 위한 U-Net 아키텍처를 제시합니다. 2. 관련 연구 인간 포즈 및 동작 생성 인간 포즈 및 동작 합성에 대한 이전 연구에서는 무조건적으로[36, 40, 25] 또는 사전 동작[20, 29], 동작 클래스[11, 23, 5] 또는 음악[17, 18]과 같은 다양한 입력 신호에 따라 생성 모델을 탐구했습니다. 포즈 및 동작 합성에서 텍스트 설명을 지침으로 사용하는 것은 많은 기존 연구에서 2D 키포인트를 포즈 표현으로 사용하는 최근의 연구 방향이었습니다.[41]은 GAN 모델에 공급된 입력 텍스트를 기반으로 8개 클러스터에서 기본 포즈를 선택하여 인간 이미지를 생성합니다.[39]는 GAN을 사용하여 텍스트 입력에 따라 신체 키포인트에 대한 히트맵 세트를 생성합니다.[28]은 키포인트 히트맵의 초기 대략적 추정 위에 정제 단계를 도입하는 대략적-정밀화 접근 방식을 제안합니다. 위의 방법과 달리 [3]은 SMPL을 사용하여 텍스트 입력에서 LSTM GAN이 생성한 3D 신체 포즈를 표현합니다. 많은 기존 연구는 특징 공간에서 텍스트와 포즈 또는 모션 임베딩을 정렬하는 방법을 학습하여 텍스트-모션에 접근합니다. JL2P [1]는 커리큘럼 학습을 사용하여 자동 인코더를 사용하여 텍스트와 포즈의 공동 임베딩을 학습하는 것을 제안합니다. [7]은 상체와 하체 모션을 별도로 인코딩하는 2스트림 모델을 제안합니다. AvatarCLIP [15]는 사전 학습된 VPoser 모델을 사용하여 후보 포즈를 생성한 다음 이를 사용하여 모션 VAE를 최적화합니다. MotionCLIP [34]는 자동 인코더를 학습하는 동시에 모션을 재구성하고 모션 매니폴드를 CLIP의 잠재 공간에 맞춥니다. TEMOS [24]는 별도의 텍스트 및 모션 변환기 인코더를 통해 공동 잠재 공간을 학습하여 비결정적 모션 샘플링을 허용합니다. T2M[9]도 VAE 모델을 사용하지만 동작을 스니펫으로 인코딩하고 입력 텍스트에 따라 동작 길이에 대한 추가 샘플링을 도입합니다. 확산 모델의 최근 발전을 활용하여 Motion Diffusion Model(MDM)[35], MotionDiffuse[38], FLAME[16]은 변환기를 사용하여 텍스트에서 동작으로의 확산 프로세스를 채택하여 합성된 샘플의 다양성을 크게 개선할 수 있습니다. MDM 및 Motion Diffuse P&#39;t-p&#39;t-PN t-Attention ResBlock Attention ResBlock 텍스트 임베딩 ResBlock: Attention 1D Conv 2D 1 x 1 Conv SiLU GroupNorm + Attention 선형 RoFormerSelfAttention QKV 선형 GroupNorm 선형 T5 텍스트 인코더 1D Conv ResBlock 선형 2D 1x1 Conv Attention KVQKV Attention SiLU GroupNorm ResBlock 텍스트 PP PN t 타임스텝 임베딩 선형 선형 GroupNorm 텍스트 임베딩 그림 2. Make-An-Animation 모델 아키텍처. 우리의 확산 모델은 최근 이미지 및 비디오 생성 모델에서 영감을 받은 U-Net 아키텍처를 기반으로 구축되었습니다. U-Net은 1x1 2D-합성곱 레이어가 있는 잔여 블록 시퀀스와 텍스트 정보에 대한 교차 어텐션이 있는 어텐션 블록으로 구성됩니다. 시간 차원을 모델링하기 위해 각 1x2D 합성곱 뒤에 1D 시간 합성곱 계층을 추가하고 각 교차 주의 계층 뒤에 시간 주의 계층을 추가합니다. 이러한 시간 계층(그림에서 회색으로 표시됨)은 동작 미세 조정 단계에서만 추가됩니다. HumanML3D[9]에서 학습하여 텍스트-동작 생성을 위한 표현 선택을 부분적으로 스틱 피규어로 제한하는 반면, 큐레이팅된 대규모 데이터 세트는 수백만 개의 SMPL 포즈 레이블을 제공합니다. MDM은 종종 비현실적인 신체 포즈를 생성하는 최적화 SIMPLIFY-X[22] 절차를 통해서만 스틱 피규어에서 SMPL 바디로 포즈를 변환하는 것을 지원합니다. 확산 생성 모델 확산 모델은 열역학적 확률적 확산 프로세스를 기반으로 하는 클래스 A 생성 모델입니다. 전방 확산 프로세스에서 샘플은 데이터 분포에서 추출되고 확산 프로세스에 의해 점차적으로 노이즈가 제거됩니다. 신경망은 역방향 프로세스를 학습하여 샘플의 노이즈를 점차적으로 제거합니다. 확산 모델은 최근 이미지 생성의 급속한 발전을 가능하게 했습니다. [13]은 무조건 이미지 생성을 위한 확산 모델을 처음으로 시연했습니다. GANS [8] 및 VAE와 비교하여 확산 모델은 우수한 학습 안정성을 보였고 모드 드롭아웃을 피하며 더 강력한 성능을 보였습니다. 조건부 생성을 위해 확산 모델을 적용한 [6]은 분류기 가이드 확산을 도입했습니다. 최근에는 별도의 분류기 없이 컨디셔닝을 가능하게 하는 분류기 없는 가이드 [14]가 도입되었습니다. Imagen [30]은 사전 학습된 언어 모델에서 추출한 텍스트 임베딩에 대한 컨디셔닝이 텍스트 정렬을 향상시키고 텍스트 인코더 크기에 따라 성능이 향상됨을 보여주었습니다. 저희의 U-Net 아키텍처는 Imagen과 유사합니다. 즉, Make-AnAnimation은 동일한 텍스트 인코더인 T5-XXL [26]과 유사한 U-Net 아키텍처를 사용합니다. 역 확산 분산 [21] 및 vparameterization [31]을 학습하는 것과 같이 학습 안정성과 성능을 개선하기 위한 다른 개선 사항이 제안되었습니다. 저희 방법은 이 두 가지 개선 사항을 모두 활용합니다. 3. 텍스트-3D 인간 동작 생성 3.1. 데이터 세트 3D 인간 동작 데이터 세트. 우리는 3D 인간 동작의 AMASS 데이터 세트[19]와 HumanML3D 데이터 세트[10]의 텍스트 주석을 사용합니다. 우리는 HumanML3D 데이터 세트의 처리된 데이터 대신 AMASS의 원래 SMPL 주석을 빌렸습니다. 왜냐하면 (1) 우리는 그들의 동작 표현이 중복된 정보를 포함하고 있다는 것을 발견했고, (2) 그들의 동작 표현을 원래 SMPL 형식으로 변환하기 위한 추가 최적화 단계가 필요하기 때문에 SMPL 형식의 동작 생성의 품질과 속도가 저하되기 때문입니다. HumanML3D와 유사하게, 우리는 동작을 미러링하고 그에 따라 텍스트 설명을 편집하여 데이터 세트의 크기를 두 배로 늘렸고, 그 결과 훈련 세트에 26850개의 동작 예제가 포함되었습니다. 또한, 우리는 20000개의 동작 샘플에 대해 가상 인간에 중첩된 SMPL 주석을 제공하는 오픈 월드 액션 게임에서 구축된 GTA-Human 데이터 세트[4]의 이점을 활용합니다. 이 데이터 세트에는 텍스트 주석이 없으므로 텍스트 조건부 훈련과 동시에 모델의 무조건 훈련에 사용합니다.기존 모캡 데이터 세트는 샘플 수와 다양성에 제한이 있으므로 다음과 같이 이미지 데이터 세트에서 실제 인간 포즈의 대규모 데이터 세트도 수집했습니다.대규모 텍스트 가상 포즈(TPP) 데이터 세트.[2]와 유사하게 몇 개의 대규모 이미지-텍스트 데이터 세트에서 3,500만 쌍의 인간 포즈와 텍스트 설명이 포함된 데이터 세트를 수집했습니다.Detectron2 키포인트 감지기를 실행하여 모든 이미지를 처리하여 단일 인간이 있는 이미지를 찾은 다음 사전 훈련된 PyMAF-X 모델[37]을 사용하여 3D 가상 포즈 SMPL 주석을 추출했습니다.이 대규모 데이터는 다양한 인간 포즈와 엄청난 수의 (텍스트, 3D 포즈) 샘플 쌍을 제공함으로써 기존 모캡 데이터 세트의 한계를 극복합니다.앞서 언급한 데이터 세트에서 얼굴이나 손 표정을 추출하지 않습니다. 3.2. 확산 모델 배경 확산 모델은 반복적 잡음 제거를 통해 가우시안 잡음을 학습된 분포의 샘플로 변환하는 생성 모델의 한 종류입니다. 순방향 프로세스는 마르코베 체인 {x}0을 따르는 가우시안 잡음 프로세스입니다. 여기서 xo는 데이터 분포에서 가져오며, 우리의 경우 3D SMPL 바디 포즈 집합입니다. 순방향 프로세스를 q(xt|xt−1) = √(√αtxt−1, (1 — αt)I)로 표현할 수 있습니다. 여기서 at = (0, 1)입니다. 확산 모델을 학습할 때 표준처럼 잡음 예측 손실을 최소화하여 순방향 프로세스를 역전하는 방법을 학습합니다[13]. Lsimple = E~N (0,1),t~U[1,1] [|| — €(Tr,c,t)||}] (1) 여기서 c는 확산 모델에 대한 선택적 조건이며, 우리의 경우 텍스트 설명입니다. 이 간단한 손실 외에도 [21, 27]에 따라 하이브리드 손실을 최적화합니다.이 손실 Lvl은 추정된 변분 하한(VLB)에 제약을 추가합니다.Lulb 항은 [21]과 같은 방식으로 적용됩니다.[12, 31]에 따라 € 또는 x를 예측하는 대신 vparameterization(vt = &amp;t€ -σ+x)을 사용하여 모델을 매개변수화합니다.3.3. 애니메이션 만들기 3D SMPL 신체 매개변수를 통해 아바타 신체 포즈 P를 표현하고 아바타 동작을 N개 프레임의 신체 포즈 시퀀스로 표현합니다.[P1, P2,···,PN]. 21개 신체 관절과 루트 방향에 대해 6D 연속 SMPL 표현을 사용합니다. 또한 우리는 아바타의 위치를 나타내는 3D 벡터를 통해 프레임당 글로벌 위치를 표현합니다.실제 방법 R-정밀도 ↑ FID↓ 다양성 ↑ 0.797±0.0.002±0.9.5±0.0.4861.J2LP[[1]] MDM[[35]] 0.611±.007 0.544.044 9.559±.T2M[[9]] 0.740±.11.02.7.676±.MAA 1.067±.9.188±.0.6755.002 0.7740±0.007 8.23±0.표 1. HumanML3D 테스트 세트의 정량적 결과.우리는 평가를 20번 실행하고 ±는 95% 신뢰 구간을 나타냅니다. x, y, z 차원으로, 각 P₁ = R135가 됩니다.MakeAn-Animation은 세 가지 주요 구성 요소로 구성됩니다.(1) 대규모 언어 데이터(T5-XXL [26])에서 학습된 사전 학습된 언어 모델, (2) 수집된 TPP 데이터 세트에서 학습된 텍스트-3D 확산 기반 포즈 생성 모델, (3) 정적 포즈 생성 모델을 동작 생성을 위한 시간 차원으로 확장하고 프레임 간 종속성을 캡처하는 시간 합성 및 주의 계층 세트.= 우리는 [12]를 따르고 수치적 안정성을 위해 포즈 및 동작 확산 모델을 학습하기 위해 v-예측 매개변수화(vt α+€ σ+x)를 사용합니다.위의 U-Net 아키텍처에서 잡음 제거 모델은 모든 동작 프레임에서 동시에 작동하여 자기 회귀 프레임워크[35]에 비해 생성된 동작에서 더 높은 시간적 일관성을 제공하며 원활한 동작 합성을 위해 동작 속도에 대한 특정 손실이 필요하지 않습니다. 또한 훈련 중 10%의 시간 동안 널 텍스트에 모델을 조건화하여 추론 시 분류기 없는 안내[14]의 이점을 얻습니다.3.3.1 텍스트-3D 포즈 생성 대규모 동결 언어 모델[26]의 텍스트 임베딩을 조건으로 3D SMPL 포즈 매개변수를 생성하기 위해 U-Net 기반 확산 모델을 훈련합니다.이미지 및 비디오 생성 모델[32]에서 영감을 받아 포즈 입력을 B×C× 1 × 1로 재구성합니다.B와 C는 각각 배치 크기와 채널 차원 135를 나타냅니다.UNet 모델은 (1) 확산 시간 단계 임베딩 및 텍스트 임베딩을 조건으로 하는 1x1 2D-합성곱 레이어가 있는 잔여 블록과 (2) 텍스트 정보와 확산 시간 단계에 주의를 기울이는 주의 블록의 시퀀스로 구성됩니다. 우리는 모든 훈련 예제에서 변환 매개변수를 0으로 설정한 대규모 TPP 데이터 세트에서 이 네트워크를 훈련합니다.즉, 장면의 중심에 사람이 있습니다.3.3.2 시간 및 주의 계층 앞서 언급한 포즈 생성 모델을 확장하여 동작 생성을 위한 시간적 차원을 학습하기 위해 U-Net의 합성곱 및 주의 계층을 다음과 같이 수정합니다.MAA에 대한 선호도 % 0.0.0.0.0.MAA 대 TEMOS MAA 대 MotionCLIP MAA 대 MDM MAA 대 T2M MAA에 대한 선호도 % 0.0.0.0.35ཀླལྤ 0.0.MAA 대 TEMOS MAA 대 MotionCLIP MAA 대 MDM MAA 대 T2M (a) 어느 동작이 텍스트 설명과 더 잘 일치합니까?(b) 어느 동작이 더 사실적입니까?그림 3. 인간 평가.여기서 우리는 텍스트 정렬 및 동작 사실성 측면에서 각 베이스라인을 모델 MAA와 비교합니다. 결과는 선별된 400개 프롬프트 세트에서 각 베이스라인보다 우리 방법을 선호하는 다수 투표 평가자의 백분율로 보고됩니다.MAA 선호도 % 0.0.0.0.0.0.0.0.0.MAA 대 Transformer MAA w/ 대 (둘 다 TPP로 사전 학습됨) w/o 시간적 주의 MAA w/ 대 w/o GTA 그림 4. 절제 연구.여기서 우리는 텍스트 정렬과 동작 사실성 측면에서 각 절제 모델을 우리 모델 MAA와 비교합니다.결과는 선별된 400개 프롬프트 세트에서 각 베이스라인보다 우리 방법을 선호하는 다수 투표 평가자의 백분율로 보고됩니다.입력 동작 시퀀스를 B × C × N × 1 × 1 모양의 텐서로 재구성합니다.여기서 C는 포즈 표현의 길이이고 N은 프레임 수입니다.[32, 12]에서 영감을 얻어 각 1x1 2D 합성곱 다음에 1D 시간 합성곱 계층을 쌓습니다. 이를 통해 포즈 생성 모델에서 사전 훈련된 합성곱 가중치를 로드하는 동안 새 1D 시간 계층을 처음부터 훈련할 수 있습니다. 이러한 시간 합성곱 계층의 커널 크기를 2D 합성곱의 단위 커널 크기와 대조적으로 3으로 설정합니다. 유사한 차원 분해 전략을 어텐션 계층에 적용하여 포즈 생성 네트워크에서 사전 훈련된 각 어텐션 블록에 새로 초기화된 시간 어텐션 계층을 쌓습니다. 시간 어텐션 계층의 경우 회전 위치 임베딩[33]을 활용합니다. 4.
--- EXPERIMENT ---
s 우리는 400개의 크라우드 소싱 프롬프트에 대한 인간 평가와 HumanML3D 테스트 세트에 대한 자동 메트릭을 통해 우리의 모션 생성 모델의 성능을 기존의 최첨단 텍스트-인간 모션 생성 모델과 비교합니다.4.1. 자동 메트릭 우리는 Fréchet Distance(FID), RPrecision 및 Diversity 점수 측면에서 HumanML3D 테스트 세트에 대한 자동 평가를 수행합니다.FID는 분포를 기준 진실 테스트 세트와 비교하여 샘플의 다양성과 품질을 모두 측정하는 반면, R-Precision은 생성된 각 모션과 입력 프롬프트 간의 충실도를 측정합니다.다양성은 모든 설명에서 생성된 모션의 분산을 측정합니다.위의 점수를 계산하기 위해 모션 인코더는 [9]의 대조 손실을 통해 텍스트 인코더와 공동으로 훈련되었습니다.우리는 동일한 인코더를 사용하여 텍스트와 모션 임베딩을 추출합니다. 이 모델은 263차원 포즈 표현 벡터에서 학습 및 최적화되었으므로 위의 메트릭을 계산하기 전에 [11]에서 제공한 변환에 따라 기준 진실 SMPL 데이터와 생성된 샘플을 모두 유사하게 변환합니다. 세대를 196개 프레임으로 업샘플링하고 위의 메트릭을 계산하는 동안 기준 진실 샘플과 출력에 대해 고정합니다. 기준선과 달리 263차원 동작 표현에서 모델을 최적화하지 않았지만 표 1에 보고된 대로 모든 메트릭에서 비슷한 성능을 달성했습니다. 4.2. 인간 평가 보다 어려운 인간 포즈와 동작을 합성하는 모델의 일반화 능력을 평가하기 위해 Amazon Mechanical Turk(AMT)에서 400개의 프롬프트로 구성된 평가 세트를 수집했습니다. 주석자에게 장면의 맥락과 함께 동작을 설명하는 프롬프트를 요청했습니다. 어린이 또는 NSFW 콘텐츠에 대한 참조를 제거하기 위해 윤리적 우려 사항에 따라 프롬프트를 필터링했습니다. 이러한 프롬프트는 포즈나 이미지를 생성하지 않고 선택되었으며 모든 평가에서 고정되었습니다. 우리는 신체 포즈와 동작의 정확성과 품질을 측정하기 위해 변환 없이 모든 모델에서 애니메이션 포즈를 생성했습니다. 우리는 평가자에게 텍스트 설명과 두 모델에서 렌더링된 해당 아바타를 보여주고 어느 출력이 입력 텍스트와 더 잘 일치하는지 묻습니다. 각 비교에서 우리는 400개 프롬프트에 대한 7명의 다른 주석자의 다수결을 최종 결과로 사용하여 그림 3에 보고합니다. 이 연구는 우리 모델의 우수성과 인간 동작 생성에서 대규모 TPP 데이터 세트의 영향과 모캡 데이터 세트의 한계를 극복하는 것을 확인합니다. 우리 모델에서 생성한 몇 가지 동작 샘플이 그림 7에 나와 있습니다. 4.3. 절제 연구 결과에 대한 U-Net 아키텍처의 영향과 TPP 데이터 세트의 영향을 분리하고 연구하기 위해 우리는 인과적 주의 마스크가 있는 디코더 전용 변환기로 대체 확산 모델을 구현하고 학습하여 텍스트에서 3D 포즈 표현을 생성했습니다. 이 모델은 토큰화된 캡션과 해당 텍스트 임베딩, 확산 시간 단계 임베딩, 노이즈가 있는 신체 포즈 및 루트 방향 표현, 그리고 각각 노이즈가 없는 포즈와 루트 방향을 예측하기 위한 두 개의 최종 포즈 및 방향 쿼리의 시퀀스에서 작동합니다. 이 모델을 TPP 데이터 세트에서 사전 학습한 다음 포즈, 루트 방향 및 변환 매개변수에 대해 프레임당 세 개의 토큰을 추가하여 동작 합성을 위해 모델을 확장했습니다. 이 모델을 Sec 3.1에 설명된 모캡 데이터 세트에서 학습하고 Amazon Mechanical Turk를 통해 U-Net 모델과 성능을 비교했습니다. 그 결과 평가자의 58%가 U-Net 아키텍처의 세대를 선호했습니다. 이 실험은 U-Net 모델이 자기 회귀 대안에 비해 우수함을 확인합니다. 또한 시간적 주의 모듈이 샘플의 최종 품질에 미치는 영향을 제거하여 사용자의 54%가 시간적 주의가 있는 U-Net의 샘플을 선호했습니다. GTA 데이터로 무조건적으로 학습하면 수렴 후 샘플의 품질에는 영향을 미치지 않지만 모델의 수렴이 더 빨라졌습니다.또한, 모션 캡처 학습 세트에서 U-Net 모델을 처음부터 사전 학습 없이 학습하는 실험을 했습니다.그러나 다양한 학습 속도와 워밍업 업데이트를 사용하더라도 이 설정에서 안정성과 수렴 문제가 관찰되었습니다.따라서 대규모 데이터 세트에 대한 사전 학습 단계는 안정적인 솔루션이었으며 결국 고품질 모션 샘플을 생성했습니다.모든 절제 결과는 그림 4에 요약되어 있습니다.4.4. 정성적 연구 그림 6에서 새로운 모션을 합성하는 모델의 일반화 기능을 시각화합니다.최근접 이웃을 통해 각 합성 모션에 대한 학습 데이터에서 유사한 프롬프트 또는 모션의 존재를 조사합니다.운동장에서 축구를 하는 사람 거리에서 춤추는 사람 그림 5. 텍스트 조건부 모션 생성을 위해 Make-An-Animation에서 생성한 다양한 샘플.신체 모델의 조명은 시간에 따른 진행 상황을 나타냅니다.더 어두운 색상은 시퀀스에서 나중의 프레임을 나타냅니다. 더 나은 시각화를 위해 프레임은 시간 차원을 따라 다운 샘플링되어 수평으로 분포됩니다.MAA 세대 최근접 이웃 예제 ALADALA 숲에서 곰과 씨름하는 사람 두 팔을 공중으로 들어올리는 사람 플루트 연주를 하는 사람 손으로 박수 치는 사람 손을 가슴까지 들어 올려 박수를 친 다음 다시 옆으로 내립니다 사람이 나무에서 가지를 잘라내고 있습니다 그림 6. 왼쪽: 텍스트 조건부 모션 생성을 위해 Make-An-Animation에서 생성한 샘플.오른쪽: 모션 및 텍스트 클립 유사도 점수에 따라 모캡 훈련 세트에서 찾은 생성된 각 샘플에 대한 최근접 이웃 예제.신체 모델의 조명은 시간에 따른 진행 상황을 나타냅니다.어두운 색상은 시퀀스에서 나중의 프레임을 나타냅니다.더 나은 시각화를 위해 프레임은 수평으로 분포됩니다.bor 검색. 우리는 사전 훈련된 motionCLIP 모델[34, 16]을 사용하여 프롬프트의 테스트 세트와 모캡 훈련 데이터에서 각 샘플에 대한 모션 임베딩과 텍스트 임베딩을 추출합니다. 이 모델은 조인트 모션과 언어 임베딩 공간을 학습했습니다. 합성된 각 모션에 대해 우리는 훈련 데이터에서 코사인 유사도 점수가 가장 높은 상위 6개 모션과 프롬프트를 찾습니다. 그림에서 볼 수 있듯이, 우리 모델은 훈련 데이터에 존재하지 않는 새로운 모션으로 일반화되었습니다. 또한, 그림 5는 다른 텍스트 프롬프트에 대해 우리 모델이 생성한 여러 가지 다양한 모션을 보여줍니다. 5.
--- CONCLUSION ---
현재의 텍스트-모션 모델은 비교적 소규모의 모션 캡처 데이터 세트로 인해 제약을 받고 있으며, 이는 더 다양하고 실제 상황에서의 프롬프트에서 성능이 좋지 않은 것에서 알 수 있습니다.이러한 단점에 동기를 부여하여 대규모 정적 가상 포즈 및 모션 캡처 데이터 모음에서 학습된 인간 모션 생성 모델인 Make-AnAnimation을 제시합니다.이전 연구와 비교한 절제 및 인간 연구를 통해 보여주듯이 대규모 TPP 데이터 세트에서 사전 학습하면 모션 캡처 분포 외부의 캡션에서 성능이 크게 향상됩니다.사람이 선인장에서 핸드스탠드를 하고 있습니다.사람이 축구공을 차고 있습니다.사람이 강에서 작은 딩기를 저어주고 있습니다.사람이 레크리에이션 센터에서 테니스를 치고 있습니다.배구 선수가 공을 들어올립니다.사진작가가 카메라를 들여다보고 있습니다.사람이 눈보라 후에 차도에서 눈삽질을 하고 있습니다.사람이 바닥에 앉아 있습니다.그림 7. 텍스트 조건부 모션 생성을 위해 Make-An-Animation에서 생성한 샘플.신체 모델의 조명은 시간에 따른 진행 상황을 나타냅니다. 더 어두운 색상은 시퀀스에서 나중의 프레임을 나타냅니다. 더 나은 시각화를 위해 프레임은 수평으로 분산됩니다. 데이터. 더욱이, 우리의 새로운 U-Net 아키텍처는 정적 포즈 사전 훈련과 동적 포즈(즉, 동작) 미세 조정 간의 원활한 전환을 가능하게 합니다. 전반적으로 이 작업은 인간 3D 포즈 매개변수의 학습 생성을 위해 대규모 이미지 및 비디오 데이터 세트를 활용하는 길을 열었습니다. 참고문헌 [1] Chaitanya Ahuja 및 Louis-Philippe Morency. Language2pose: 자연어 기반 포즈 예측. IEEE 국제 3D 비전 컨퍼런스(3DV), 719-728페이지, 2019. [2] Samaneh Azadi, Thomas Hayes, Akbar Shah, Guan Pang, Devi Parikh 및 Sonal Gupta. 제로 샷 개인화를 위한 텍스트 조건부 문맥화 아바타. arXiv 사전 인쇄본 arXiv:2304.07410, 2023. [3] Rania Briq, Pratika Kochar, 및 Juergen Gall. 텍스트에서 인간 이미지의 더 나은 적대적 합성을 향해. arXiv 사전 인쇄본 arXiv:2107.01869, 2021. [4] Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, 및 Ziwei Liu. 3D 인간 회복을 위한 플레이. arXiv 사전 인쇄본 arXiv:2110.07588, 2021. [5] Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, 및 Koichi Shinoda. 가변 길이 인간 동작 생성을 위한 암묵적 신경 표현. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. [6] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델은 이미지 합성에서 gans를 이겼습니다. 신경 정보 처리 시스템의 발전(NeurIPS), 34, 2021. [7] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt 및 Philipp Slusallek. 텍스트 설명에서 구성 애니메이션의 합성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스(ICCV) 회의록, 1396-1406페이지, 2021년 10월. [8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville 및 Yoshua Bengio. 생성적 적대적 네트워크. 신경 정보 처리 시스템의 발전, 2014. [9] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li 및 Li Cheng. 텍스트로부터 다양하고 자연스러운 3D 인간 모션을 생성합니다. 컴퓨터 비전 및 패턴 인식(CVPR)에 관한 IEEE/CVF 회의 간행물, 페이지 5152-5161, 2022년 6월. [10] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li 및 Li Cheng. 텍스트로부터 다양하고 자연스러운 3D 인간 모션을 생성합니다. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 5152-5161페이지, 2022년 6월. Ac[11] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, Li Cheng. tion2motion: 3차원 인간 동작의 조건부 생성. 제28회 ACM 국제 멀티미디어 컨퍼런스 논문집, 2021-2029페이지, 2020년. [12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303, 2022. [13] Jonathan Ho, Ajay Jain 및 Pieter Abbeel. 확산 확률적 모델의 잡음 제거. 신경 정보 처리 시스템의 발전(NeurIPS), 33, 2020. [14] Jonathan Ho 및 Tim Salimans. 분류자 없는 확산 안내. arXiv 사전 인쇄본 arXiv:2207.12598, 2022. [15] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang 및 Ziwei Liu. Avatarclip: 3D 아바타의 제로샷 텍스트 기반 생성 및 애니메이션. ACM Transactions on Graphics(TOG), 41(4):1–19, 2022. [16] Jihoon Kim, Jiseob Kim 및 Sungjoon Choi. Flame: 자유형 언어 기반 모션 합성 및 편집. arXiv 사전 인쇄본 arXiv:2209.00349, 2022. [17] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, Jan Kautz. 음악에 맞춰 춤추기. 신경 정보 처리 시스템의 발전(NeurIPS), 32, 2019. [18] Ruilong Li, Shan Yang, David A. Ross, Angjoo Kanazawa. Ai 안무가: aist++를 사용한 음악 조건 3D 댄스 생성. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스(ICCV) 회의록, 2021. [19] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, Michael J. Black. AMASS: 표면 모양으로서 모션 캡처의 아카이브. International Conference on Computer Vision, 5442-5451페이지, 2019년 10월. [20] Julieta Martinez, Michael J. Black, Javier Romero. 순환 신경망을 사용한 인간 동작 예측에 관하여. IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR) 회의록, 2017년. [21] Alexander Quinn Nichol, Prafulla Dhariwal. 개선된 노이즈 제거 확산 확률적 모델. International Conference on Machine Learning, 8162-8171페이지. PMLR, 2021. [22] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, Michael J. Black. 표현력 있는 신체 캡처: 단일 이미지에서 3D 손, 얼굴 및 신체. 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), 2019. [23] Mathis Petrovich, Michael J. Black 및 Gül Varol. 변압기 VAE를 사용한 Actionconditioned 3D 인체 동작 합성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스(ICCV) 회의록, 10985-10995페이지, 2021년 10월. [24] Mathis Petrovich, Michael J. Black 및 Gül Varol. Temos: 텍스트 설명에서 다양한 인체 동작 생성. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. [25] Sigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung 및 Daniel Cohen-Or. Modi: 다양한 데이터에서 무조건 동작 합성. arXiv 사전 인쇄본 arXiv:2206.08010, 2022. [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. JMLR, 2020. [27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022. [28] Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, Michael Blumenstein. 팁: 텍스트 유도 포즈 합성. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. [29] Alejandro Hernandez Ruiz, Juergen Gall 및 Francesc Moreno-Noguer. 시공간적 인페인팅을 통한 인간 동작 예측. IEEE/CVF 컴퓨터 비전 국제 컨퍼런스(ICCV) 회의록, 2019. [30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet 및 Mohammad Norouzi. 단일 단안경 시야에서 인간의 이미지 합성을 위한 스타일 및 포즈 제어. arXiv 사전 인쇄본 arXiv:2205.11487, 2022. [31] T. Salimans 및 J Ho. 확산 모델의 빠른 샘플링을 위한 점진적 증류. 학습 표현 국제 컨퍼런스, 2022. [32] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄본 arXiv:2209.14792, 2022. [33] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. Roformer: 회전 위치 임베딩이 있는 향상된 변압기. arXiv 사전 인쇄본 arXiv:2104.09864, 2021. [34] Guy Tevet, Brian Gordon, Amir Hertz, Amit H. Bermano, Daniel Cohen-Or. Motionclip: 클립 공간에 인간의 동작 생성 노출. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. [35] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano, Daniel Cohen-Or. 인간 동작 확산 모델. arXiv 사전 인쇄본 arXiv:2209.14916, 2022. [36] Sijie Yan, Zhizhong Li, Yuanjun Xiong, Huahan Yan, Dahua Lin. 골격 기반 동작 합성을 위한 합성 시퀀스 생성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스(ICCV) 회의록, 4393-4401페이지, 2019. [37] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, Yebin Liu. Pymaf-x: 단안 이미지에서 잘 정렬된 전신 모델 회귀를 향해. arXiv 사전 인쇄본 arXiv:2207.06400, 2022. [38] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, 및 Ziwei Liu. Motiondiffuse: 확산 모델을 사용한 텍스트 기반 인간 동작 생성. arXiv 사전 인쇄본 arXiv:2208.15001, 2022. [39] Yifei Zhang, Rania Briq, Julian Tanke, 및 Juergen Gall. 텍스트에서 인간 포즈의 적대적 합성. 제42회 DAGM 독일 패턴 인식 컨퍼런스, 145-158쪽, 2021. [40] Rui Zhao, Hui Su, 및 Qiang Ji. 베이지안 적대적 인간 동작 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 6224-6233페이지, 2020. [41] Xingran Zhou, Siyu Huang, Bin Li, Yingming Li, Jiachen Li, Zhongfei Zhang. 텍스트 안내 인물 이미지 합성. IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록, 2019.
