--- ABSTRACT ---
기존의 대규모 언어 모델은 K개의 토큰 시퀀스를 생성하기 위해 K번 실행해야 합니다. 이 논문에서는 여러 단계로 전체 모델을 실행하지 않고 미리 생성된 모델 상태를 재활용하여 디코딩 속도가 빠른 생성 언어 모델인 RecycleGPT를 제시합니다. 저희의 접근 방식은 시퀀스에서 인접한 토큰은 일반적으로 강력한 상관 관계를 가지고 있으며 시퀀스의 다음 토큰은 이전 토큰을 기반으로 합리적으로 추측하거나 추론할 수 있다는 관찰에 의존합니다. 실험과 분석은 추론 지연 시간을 낮추고 높은 성능을 유지하면서 최대 1.4배의 속도 향상을 달성하는 저희 접근 방식의 효과를 보여줍니다.
--- INTRODUCTION ---
대규모 언어 모델(LLM)(Brown 등, 2020; OpenAI, 2023; Touvron 등, 2023; Chowdhery 등, 2022; Biderman 등, 2023; Smith 등, 2022)은 다양한 애플리케이션 도메인에서 만족스러운 텍스트를 생성하는 능력으로 인해 자연어 생성 분야에 혁명을 일으켰습니다. 뛰어난 성능은 모델 크기(100B+ 매개변수)의 확장으로 크게 이점을 얻지만 동시에 모델이 커질수록 단일 디코딩 단계가 더 느려진다는 사실이 남아 있습니다. 대규모 모델이 도입한 엄청난 계산 외에도 더 큰 메모리 공간도 LLM의 추론 속도를 느리게 하는 주요 요인입니다(Dao 등, 2022; Pope 등, 2023). 이 대규모 메모리 공간에는 학습된 모델 매개변수, 추론 중에 사용된 임시 상태가 포함되며, 여기에 더해 KV 캐시도 메모리에 저장됩니다. 각 디코딩 단계에서는 고대역폭 메모리(HBM)에서 매개변수와 KV 캐시를 컴퓨팅 코어로 로드해야 하며, 이로 인해 상당한 메모리 트래픽이 발생하고, 따라서 주어진 지연 시간 목표를 충족하려면 높은 총 메모리 대역폭이 필요합니다. 다시 말해, LLM에서 토큰을 생성하는 속도는 주로 메모리에 얼마나 빨리 액세스할 수 있는지에 따라 제한됩니다(Shazeer, 2019; Pope et al., 2023; Chen et al., 2023). 그리고 각 토큰을 생성하는 데 걸리는 시간은 대략 모델 매개변수의 수에 비례합니다. 모델에서 생성하는 각각의 새 토큰은 이전 토큰에 따라 달라지므로 전체 시퀀스를 생성하려면 변환기 모델에 대한 많은 호출이 필요합니다. 추론을 보다 효율적으로 만들기 위해 여러 연구가 제안되었습니다. 이러한 연구의 핵심 아이디어는 메모리 공간을 줄이고 메모리 트래픽 문제를 완화하는 방법입니다. 예를 들어, 증류(Hinton 등, 2015), 희소화(Jaszczur 등, 2021), 양자화(Shen 등, 2020; Zafrir 등, 2019) 및 공유 가중치(Xiao 등, 2019; Zeng 등, 2021)는 모델 크기를 줄이기 위해 제안되었습니다. 적응형 계산(Sukhbaatar 등, 2019; Schwartz 등, 2020)은 더 쉬운 추론 단계를 위해 더 적은 컴퓨팅 리소스를 사용하는 것을 목표로 합니다. 다중 쿼리 어텐션(Shazeer, 2019; Ainslie 등, 2023)은 키와 값을 공유하여 크기 메모리 대역폭 요구 사항을 줄이는 반면, 플래시 어텐션(Dao 등, 2022)은 소량의 계산을 사용하여 메모리 읽기/쓰기 수를 줄입니다. 위의 작업들은 효과적인 접근 방식을 제안하지만, 일반적으로 모델 아키텍처나 어텐션 알고리즘을 변경하고, 더 많은 학습 과제를 추가하고, 이러한 복잡한 모델을 다시 학습해야 합니다. 최근 추측적 디코딩 방법이 인기를 얻고 있습니다(Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2023). 대규모 모델의 실행 횟수를 줄이기 위해 그들은 2단계 접근 방식을 채택합니다. 첫째, 효율적인 소규모 모델이 텍스트의 더 간단한 부분을 추측적으로 생성합니다. 그런 다음 대규모 모델이 전체 텍스트를 단독으로 생성하는 대신 대규모 모델을 사용하여 해당 부분을 검증합니다. 이 아이디어는 간단하고 편리하며 오픈 소스 프레임워크에도 통합되었습니다. 그러나 효율적인 모델을 선택하는 것은 여전히 미해결 문제입니다. LLM의 소규모 버전을 사용하는 것은 시퀀스 수준의 증류가 여전히 필요하지만 하나의 솔루션이 될 수 있습니다. 자연스럽게 시퀀스에서 인접한 토큰은 강력한 상관 관계를 갖습니다. 즉, 많은 경우 시퀀스의 다음 토큰은 이전 토큰을 기반으로 합리적으로 추측하거나 추론할 수 있습니다. 이러한 현상은 동일한 양의 메모리 처리 예산으로 가능한 한 많은 토큰을 생성하는 목표로 다른 연구 방향으로 효율적인 디코딩 방법을 조사하게 합니다. 우리는 사전 생성된 모델 상태를 재활용하여 빠르게 디코딩할 수 있는 새로운 언어 모델 아키텍처인 RecycleGPT를 제안합니다. 우리의 접근 방식에서 우리는 이전에 생성된 상태를 사용하여 다음 여러 토큰을 예측하는 추가 재활용 모듈을 추가하여 원래 언어 모델을 수정합니다. 이는 전체 모델을 여러 번 실행하지 않고도 재활용 프로세스로 볼 수 있습니다. 재활용 모듈은 예측을 위한 보다 효율적인 표현을 달성하기 위한 변환기 기반 레이어의 스택으로 구성됩니다. 추론 중에 이 모듈은 다양한 방식으로 표준 언어 모델 디코딩 파이프라인과 함께 사용할 수 있습니다. 이 논문에서는 이를 번갈아 사용하기로 선택했습니다(즉, 두 토큰을 생성할 때마다 전체 모델을 한 번 실행해야 함). 그리고 향후 작업에서 더 많은 전략을 탐색하기로 했습니다. 간단한 아키텍처에도 불구하고 재활용 가능한 모듈은 맥락적 정보를 효과적으로 표현하고 정확한 예측을 할 수 있어 디코딩 프로세스를 가속화하는 목표를 달성합니다.우리는 일련의 표준 벤치마크에서 RecycleGPT를 평가합니다.표준 언어 모델보다 1.4배 더 빠른 속도를 달성하지만 성능은 저하되지 않습니다.더 중요한 것은 이전 방법과 직교적이며 다양한 LLM에 간단히 적용할 수 있다는 것입니다.이 작업의 주요 기여는 다음과 같이 요약됩니다.• 새로운 생성 언어 모델 RecycleGPT를 제안하고 RecycleGPT-1.3B를 릴리스합니다.표준 언어 모델과 비교할 때, 우리 모델은 다운스트림 작업에서 비슷한 성능을 유지하면서 도입된 15%의 추가 매개변수만으로 1.4배 더 빠른 속도를 달성합니다.향후 다양한 크기의 RecycleGPT 변형을 릴리스할 예정입니다.• 우리의 재활용 방법은 유연하고 확장 가능하여 다양한 사전 학습된 모델에 적용할 수 있습니다.또한 재활용 가능한 모듈의 크기와 생성 전략을 조정하여 원하는 속도 향상 성능을 달성할 수 있습니다. 2 배경 이 섹션에서는 추론 시 메모리 비용에 대한 배경 정보를 제공합니다. 또한 자기 회귀 언어 모델에 대한 간략한 소개도 제공합니다. 2.1 추론 메모리 비용 모델 규모가 기하급수적으로 폭발적으로 증가함에 따라 언어 모델 디코딩은 매우 비용이 많이 들고 비효율적이 됩니다. 더 큰 모델이 특정 시간을 차지하는 더 많은 텐서 계산을 도입한다는 점을 제외하면 메모리 전송도 상당한 시간을 차지합니다. 일반적으로 대규모 언어 모델은 일반적으로 온디바이스 고대역폭 메모리(HBM)에 저장되는 모델 매개변수와 KV 캐시를 모두 저장하기 위한 대용량 메모리 풋프린트를 갖습니다. 이러한 텐서는 각 포워드 패스에서 HBM에서 컴퓨트 코어로 전송되어야 하며 이는 특정 시간이 걸립니다. 그리고 자기 회귀 언어 모델은 끝 심볼에 도달할 때까지 각 단계마다 하나의 토큰을 생성하기 때문에 전체 시퀀스를 생성하려면 언어 모델에 대한 많은 호출이 필요합니다. Pope et al.에 따르면 (2023), 작은 배치 크기와 시퀀스 길이에서 가중치를 로드하는 데 가장 많은 시간이 걸리는 반면, 대규모에서는 KV 캐시를 로드하는 데 추론 시간이 가장 많이 걸립니다. 더욱이, 더 큰 언어 모델은 여러 장치가 병렬로 함께 작동해야 하므로 통신 오버헤드도 추가됩니다. 따라서 메모리 크기와 전송 빈도를 줄이는 방법은 모델 디코딩 프로세스를 가속화하는 또 다른 핵심 요소입니다. 기술 보고서 2. 자기 회귀 언어 모델 토큰 X = · {x1, ..., xn}의 코퍼스가 주어지면, 자기 회귀 언어 모델(그림 1(a))은 결합 확률을 왼쪽에서 오른쪽으로 인과 구조를 갖는 조건부 확률 체인으로 인수분해합니다. n Par(X;0ar) = [[p(xi|x
--- RELATED WORK ---
자기 회귀 언어 모델의 규모는 1억 1,700만(Radford et al., 2018)개의 매개변수에서 5,000억 개 이상의 매개변수(Smith et al., 2022)로 확대되고 다양한 접근 방식을 탐색하여 기술 보고서 모델 인문학 STEM 사회 과학 기타 평균 OPT 1.3B 22.25.23.26.24.Pythia 1.4B 26.25.24.26.25.GPT-Neo 2.7B 25.25.27.27.26.LLAMA-ours 1.3B 27.26.23.23.25.RecycleGPT-std 1.3B 26.28.24.25.26.RecycleGPT-rec 1.5B 26.28.24.24.26.표 2: 5샷 성능 대규모 멀티태스크 언어 이해(MMLU). 모델 ms/토큰 평균 평균 속도 향상 64256 512KV 캐시 RecycleGPT-std 18.4 19.2 18.7 18.5 18.6 18.RecycleGPT-rec 13.8 13.1 13.4 13.0 13.7 13.w/o KV 캐시 1X 1.40X RecycleGPT-std 20.8 24.1 33.0 55.3 103.7 47.RecycleGPT-rec 14.8 16.6 24.4 41.4 80.4 35.1X 1.34X 표 3: 다른 시퀀스 길이에서 RecycleGPT-std 및 RecycleGPT-rec의 디코딩 속도. 추론 효율성. 대량의 모델 계산과 메모리 이동은 추론 속도를 늦추는 주요 요인입니다(Pope et al., 2023). 모델 크기를 더 작게 만들기 위해 여러 연구에서 증류(Hinton et al., 2015; Sanh et al., 2019), 가지치기(Li et al., 2020; Brix et al., 2020; Zhou et al., 2021), 가중치 공유(Xiao et al., 2019) 또는 int8 또는 int로 양자화(Dettmers et al., 2022; Shen et al., 2020; Zafrir et al., 2019;?)가 제안되었습니다. 적응 계산(Sukhbaatar et al., 2019; Schwartz et al., 2020)은 더 쉬운 추론 단계를 위해 계산량을 줄이려고 시도합니다. Sukhbaatar et al. (2019); Kitaev et al. (2020); Zeng 등 (2021); Roy 등 (2021); Choromanski 등 (2020)은 시퀀스 길이에서 시간과 메모리가 2차적으로 확장되는 계산적 병목 현상을 극복하기 위해 효율적인 어텐션 계층을 제안합니다. 셀프 어텐션 계층의 메모리 복잡도를 기반으로 Dao 등 (2022); Shazeer (2019)는 (HBM)과 GPU 온칩 SRAM 간의 메모리 읽기/쓰기 횟수를 줄이기 위한 새로운 어텐션 알고리즘을 제안합니다. 더 빠른 디코딩을 위한 모델 아키텍처를 개선하는 것 외에도 샘플링 전략과 파티셔닝 전략은 저지연 추론을 달성할 수도 있습니다(Stern 등, 2018; Ge 등, 2022). 추측 샘플링
--- METHOD ---
s가 인기를 얻었습니다(Leviathan 등, 2023; Chen 등, 2023; Miao 등, 2023). 대형 모델의 실행 횟수를 줄이기 위해 그들은 2단계 접근 방식을 채택합니다. 첫째, 효율적인 소형 모델이 추측적으로 텍스트의 더 간단한 부분을 생성합니다. 그런 다음 대형 모델이 전체 텍스트를 단독으로 생성하는 대신 대형 모델을 사용하여 해당 부분을 검증합니다. 이 아이디어는 간단하고 편리하며 오픈 소스 프레임워크에도 통합되었습니다. 그러나 효율적인 모델을 선택하는 것은 여전히 미해결 문제입니다. LLM의 소형 버전을 사용하는 것은 시퀀스 수준의 증류가 여전히 필요한 동안 하나의 솔루션이 될 수 있습니다. 자연스럽게 시퀀스의 인접한 토큰은 강력한 상관 관계를 갖습니다. 즉, 많은 경우 시퀀스의 다음 토큰은 이전 토큰을 기반으로 합리적으로 추측하거나 추론할 수 있습니다. 이러한 현상은 동일한 양의 메모리 처리 예산으로 가능한 한 많은 토큰을 생성하는 목표로 다른 연구 방향에서 효율적인 디코딩 방법을 조사하게 합니다. 우리는 사전 생성된 모델 상태를 재활용하여 빠르게 디코딩할 수 있는 새로운 언어 모델 아키텍처인 RecycleGPT를 제안합니다. 우리의 접근 방식에서 우리는 이전에 생성된 상태를 사용하여 다음 여러 토큰을 예측하는 추가 재활용 모듈을 추가하여 원래 언어 모델을 수정합니다. 전체 모델을 여러 번 실행하지 않고도 재활용 프로세스로 볼 수 있습니다. 재활용 모듈은 예측을 위한 보다 효율적인 표현을 달성하기 위한 변환기 기반 레이어의 스택으로 구성됩니다. 추론 중에 이 모듈은 다양한 방식으로 표준 언어 모델 디코딩 파이프라인과 함께 사용할 수 있습니다. 이 논문에서는 이를 번갈아 사용하기로 선택했습니다(즉, 두 개의 토큰을 생성하려면 전체 모델을 한 번 실행해야 함). 그리고 더 많은 전략을 탐색하는 것은 향후 작업으로 미루었습니다. 간단한 아키텍처에도 불구하고 재활용 모듈은 상황 정보를 효과적으로 표현하고 정확한 예측을 할 수 있으므로 디코딩 프로세스를 가속화하는 목표를 달성할 수 있습니다. 우리는 일련의 표준 벤치마크에서 RecycleGPT를 평가합니다. 그것은 표준 언어 모델보다 1.4배 빠른 속도를 달성하지만 성능은 저하되지 않습니다. 더 중요한 것은 이전 방법과 직교적이며 다양한 LLM에 간단히 적용할 수 있다는 것입니다. 이 작업의 주요 기여는 다음과 같이 요약됩니다. • 우리는 새로운 생성 언어 모델 RecycleGPT를 제안하고 RecycleGPT-1.3B를 릴리스합니다. 표준 언어 모델과 비교할 때, 우리의 모델은 다운스트림 작업에서 비슷한 성능을 유지하면서 도입된 15%의 추가 매개변수만으로 1.4배 빠른 속도를 달성합니다. 앞으로 우리는 다양한 크기의 RecycleGPT 변형을 릴리스할 것입니다. • 우리의 재활용 방법은 유연하고 확장 가능하여 다양한 사전 학습된 모델에 적용할 수 있습니다. 게다가, 재활용 가능한 모듈의 크기와 생성 전략을 조정하여 원하는 속도 향상 성능을 달성할 수 있습니다. 2 배경 이 섹션에서는 추론 시점의 메모리 비용에 대한 배경 정보를 제공합니다. 또한 자기 회귀 언어 모델에 대한 간략한 소개를 제공합니다. 2.1 추론 메모리 비용 모델 규모가 기하급수적으로 폭발적으로 증가함에 따라 언어 모델 디코딩은 매우 비용이 많이 들고 비효율적이 됩니다. 더 큰 모델이 특정 시간을 차지하는 더 많은 텐서 계산을 도입한다는 점을 제외하면 메모리 전송도 상당한 시간을 차지합니다. 일반적으로 대규모 언어 모델은 일반적으로 온디바이스 고대역폭 메모리(HBM)에 저장되는 모델 매개변수와 KV 캐시를 모두 저장하기 위한 대용량 메모리 풋프린트를 갖습니다. 이러한 텐서는 각 포워드 패스에서 HBM에서 컴퓨팅 코어로 전송해야 하며, 여기에는 특정 시간이 걸립니다. 그리고 자기 회귀 언어 모델은 끝 심볼에 도달할 때까지 각 단계마다 토큰을 하나씩 생성하므로 전체 시퀀스를 생성하려면 언어 모델에 대한 많은 호출이 필요합니다. Pope et al. (2023)에 따르면, 작은 배치 크기와 시퀀스 길이에서는 가중치를 로드하는 데 가장 많은 시간이 걸리는 반면, 대규모에서는 KV 캐시를 로드하는 데 추론 시간이 가장 많이 걸립니다. 게다가 더 큰 언어 모델은 여러 장치가 병렬로 함께 작동해야 하므로 통신 오버헤드도 추가됩니다. 따라서 메모리 크기와 전송 빈도를 줄이는 방법은 모델 디코딩 프로세스를 가속화하는 또 다른 핵심 요소입니다.기술 보고서 2. 자기 회귀 언어 모델 토큰 X = · {x1, ..., xn}의 코퍼스가 주어지면 자기 회귀 언어 모델(그림 1(a))은 결합 확률을 왼쪽에서 오른쪽으로 인과 구조를 갖는 조건부 확률 체인으로 인수분해합니다.n Par(X;0ar) = [[p(xi|x
--- EXPERIMENT ---
s 및 분석은 추론 지연 시간을 낮추고 높은 성능을 유지하면서 최대 1.4배의 속도 향상을 달성하는 데 있어 우리 접근 방식의 효과를 보여줍니다.소개 대규모 언어 모델(LLM)(Brown 등, 2020; OpenAI, 2023; Touvron 등, 2023; Chowdhery 등, 2022; Biderman 등, 2023; Smith 등, 2022)은 다양한 애플리케이션 도메인에서 만족스러운 텍스트를 생성하는 능력으로 자연어 생성 분야에 혁명을 일으켰습니다.뛰어난 성능은 모델 크기(100B+ 매개변수)의 확장으로 크게 이점을 얻지만 동시에 모델이 커질수록 단일 디코딩 단계가 더 느려진다는 사실이 남아 있습니다.더 큰 모델이 도입하는 엄청난 계산 외에도 더 큰 메모리 공간도 LLM의 추론 속도를 늦추는 주요 요인입니다(Dao 등, 2022; Pope 등, 2023). 이 대용량 메모리 풋프린트에는 학습된 모델 매개변수, 추론 중에 사용된 임시 상태가 포함되며, 여기에 더해 KV 캐시도 메모리에 저장됩니다. 각 디코딩 단계에서 고대역폭 메모리(HBM)에서 매개변수와 KV 캐시를 컴퓨팅 코어로 로드해야 하며, 이로 인해 상당한 메모리 트래픽이 발생하고, 따라서 주어진 지연 시간 목표를 충족하려면 높은 총 메모리 대역폭이 필요합니다. 다시 말해, LLM에서 토큰을 생성하는 속도는 주로 메모리에 얼마나 빨리 액세스할 수 있는지에 따라 제한됩니다(Shazeer, 2019; Pope et al., 2023; Chen et al., 2023). 그리고 각 토큰을 생성하는 데 걸리는 시간은 대략 모델 매개변수의 수에 비례합니다. 모델에서 생성하는 각각의 새 토큰은 이전 토큰에 따라 달라지므로 전체 시퀀스를 생성하려면 변환기 모델에 대한 많은 호출이 필요합니다. 추론을 보다 효율적으로 만들기 위해 여러 연구가 제안되었습니다. 이러한 연구의 핵심 아이디어는 메모리 풋프린트를 줄이고 메모리 트래픽 문제를 완화하는 방법입니다. 예를 들어, 증류(Hinton 등, 2015), 희소화(Jaszczur 등, 2021), 양자화(Shen 등, 2020; Zafrir 등, 2019) 및 공유 가중치(Xiao 등, 2019; Zeng 등, 2021)는 모델 크기를 줄이기 위해 제안되었습니다. 적응형 계산(Sukhbaatar 등, 2019; Schwartz 등, 2020)은 더 쉬운 추론 단계를 위해 더 적은 컴퓨팅 리소스를 사용하는 것을 목표로 합니다. 다중 쿼리 어텐션(Shazeer, 2019; Ainslie 등, 2023)은 키와 값을 공유하여 크기 메모리 대역폭 요구 사항을 줄이는 반면, 플래시 어텐션(Dao 등, 2022)은 소량의 계산을 사용하여 메모리 읽기/쓰기 수를 줄입니다. 위의 작업들은 효과적인 접근 방식을 제안하지만, 일반적으로 모델 아키텍처나 어텐션 알고리즘을 변경하고, 더 많은 학습 과제를 추가하고, 이러한 복잡한 모델을 다시 학습해야 합니다. 최근 추측적 디코딩 방법이 인기를 얻고 있습니다(Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2023). 대규모 모델의 실행 횟수를 줄이기 위해 그들은 2단계 접근 방식을 채택합니다. 첫째, 효율적인 소규모 모델이 텍스트의 더 간단한 부분을 추측적으로 생성합니다. 그런 다음 대규모 모델이 전체 텍스트를 단독으로 생성하는 대신 대규모 모델을 사용하여 해당 부분을 검증합니다. 이 아이디어는 간단하고 편리하며 오픈 소스 프레임워크에도 통합되었습니다. 그러나 효율적인 모델을 선택하는 것은 여전히 미해결 문제입니다. LLM의 소규모 버전을 사용하는 것은 시퀀스 수준의 증류가 여전히 필요하지만 하나의 솔루션이 될 수 있습니다. 자연스럽게 시퀀스에서 인접한 토큰은 강력한 상관 관계를 갖습니다. 즉, 많은 경우 시퀀스의 다음 토큰은 이전 토큰을 기반으로 합리적으로 추측하거나 추론할 수 있습니다. 이러한 현상은 동일한 양의 메모리 처리 예산으로 가능한 한 많은 토큰을 생성하는 목표로 다른 연구 방향으로 효율적인 디코딩 방법을 조사하게 합니다. 우리는 사전 생성된 모델 상태를 재활용하여 빠르게 디코딩할 수 있는 새로운 언어 모델 아키텍처인 RecycleGPT를 제안합니다. 우리의 접근 방식에서 우리는 이전에 생성된 상태를 사용하여 다음 여러 토큰을 예측하는 추가 재활용 모듈을 추가하여 원래 언어 모델을 수정합니다. 이는 전체 모델을 여러 번 실행하지 않고도 재활용 프로세스로 볼 수 있습니다. 재활용 모듈은 예측을 위한 보다 효율적인 표현을 달성하기 위한 변환기 기반 레이어의 스택으로 구성됩니다. 추론 중에 이 모듈은 다양한 방식으로 표준 언어 모델 디코딩 파이프라인과 함께 사용할 수 있습니다. 이 논문에서는 이를 번갈아 사용하기로 선택했습니다(즉, 두 토큰을 생성할 때마다 전체 모델을 한 번 실행해야 함). 그리고 향후 작업에서 더 많은 전략을 탐색하기로 했습니다. 간단한 아키텍처에도 불구하고 재활용 가능한 모듈은 맥락적 정보를 효과적으로 표현하고 정확한 예측을 할 수 있어 디코딩 프로세스를 가속화하는 목표를 달성합니다.우리는 일련의 표준 벤치마크에서 RecycleGPT를 평가합니다.표준 언어 모델보다 1.4배 더 빠른 속도를 달성하지만 성능은 저하되지 않습니다.더 중요한 것은 이전 방법과 직교적이며 다양한 LLM에 간단히 적용할 수 있다는 것입니다.이 작업의 주요 기여는 다음과 같이 요약됩니다.• 새로운 생성 언어 모델 RecycleGPT를 제안하고 RecycleGPT-1.3B를 릴리스합니다.표준 언어 모델과 비교할 때, 우리 모델은 다운스트림 작업에서 비슷한 성능을 유지하면서 도입된 15%의 추가 매개변수만으로 1.4배 더 빠른 속도를 달성합니다.향후 다양한 크기의 RecycleGPT 변형을 릴리스할 예정입니다.• 우리의 재활용 방법은 유연하고 확장 가능하여 다양한 사전 학습된 모델에 적용할 수 있습니다.또한 재활용 가능한 모듈의 크기와 생성 전략을 조정하여 원하는 속도 향상 성능을 달성할 수 있습니다. 2 배경 이 섹션에서는 추론 시 메모리 비용에 대한 배경 정보를 제공합니다. 또한 자기 회귀 언어 모델에 대한 간략한 소개도 제공합니다. 2.1 추론 메모리 비용 모델 규모가 기하급수적으로 폭발적으로 증가함에 따라 언어 모델 디코딩은 매우 비용이 많이 들고 비효율적이 됩니다. 더 큰 모델이 특정 시간을 차지하는 더 많은 텐서 계산을 도입한다는 점을 제외하면 메모리 전송도 상당한 시간을 차지합니다. 일반적으로 대규모 언어 모델은 일반적으로 온디바이스 고대역폭 메모리(HBM)에 저장되는 모델 매개변수와 KV 캐시를 모두 저장하기 위한 대용량 메모리 풋프린트를 갖습니다. 이러한 텐서는 각 포워드 패스에서 HBM에서 컴퓨트 코어로 전송되어야 하며 이는 특정 시간이 걸립니다. 그리고 자기 회귀 언어 모델은 끝 심볼에 도달할 때까지 각 단계마다 하나의 토큰을 생성하기 때문에 전체 시퀀스를 생성하려면 언어 모델에 대한 많은 호출이 필요합니다. Pope et al.에 따르면 (2023), 작은 배치 크기와 시퀀스 길이에서 가중치를 로드하는 데 가장 많은 시간이 걸리는 반면, 대규모에서는 KV 캐시를 로드하는 데 추론 시간이 가장 많이 걸립니다. 게다가, 더 큰 언어 모델은 여러 장치가 병렬로 함께 작동해야 하므로 통신 오버헤드도 추가됩니다. 따라서 메모리 크기와 전송 빈도를 줄이는 방법은 모델 디코딩 프로세스를 가속화하는 또 다른 핵심 요소입니다. 기술 보고서 2. 자기 회귀 언어 모델 토큰 코퍼스 X = · {x1, ..., xn}이 주어지면, 자기 회귀 언어 모델(그림 1(a))은 결합 확률을 왼쪽에서 오른쪽으로 인과 구조를 갖는 조건부 확률 체인으로 인수분해합니다. n Par(X;0ar) = [[p(xi|x
--- CONCLUSION ---
이 작업에서 우리는 추론 지연 시간이 짧은 새로운 아키텍처인 RecycleGPT를 제안합니다. 재활용 가능한 모듈로 여러 토큰을 동시에 예측함으로써 RecycleGPT는 성능 손실 없이 최대 1.4배의 속도 향상을 달성할 수 있습니다. 제안된 접근 방식은 모델에 구애받지 않으며 이전 가속 기술을 보완합니다. 앞으로는 재활용 가능한 모듈과 원래 모델을 다양한 방식으로 결합하여 더 많은 디코딩 전략을 탐색할 것입니다. 기술 보고서 참고문헌 Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, Sumit Sanghai. Gqa: 다중 헤드 체크포인트에서 일반화된 다중 쿼리 변환기 모델 학습. arXiv 사전 인쇄본 arXiv:2305.13245, 2023. Stella Biderman, Kieran Bicheno, Leo Gao. arXiv:2201.07311, 2022. 파일에 대한 데이터시트. arXiv 사전 인쇄본 Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O&#39;Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: 훈련 및 확장을 통해 대규모 언어 모델을 분석하기 위한 제품군. International Conference on Machine Learning, pp. 2397–2430. PMLR, 2023. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: 자연어에서 물리적 상식에 대한 추론. AAAI 인공지능 컨퍼런스 회의록, 34권, 7432-7439쪽, 2020. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang 등. Gpt-neox-20b: 오픈소스 자기회귀 언어 모델. arXiv 사전 인쇄본 arXiv:2204.06745, 2022. Christopher Brix, Parnia Bahar, Hermann Ney. 안정화된 복권 가설을 변압기 아키텍처에 성공적으로 적용. arXiv 사전 인쇄본 arXiv:2005.03454, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901, 2020. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 추측 샘플링을 통한 대규모 언어 모델 디코딩 가속화. arXiv 사전 인쇄본 arXiv:2302.01318, 2023. Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 신경 기계 번역의 속성: 인코더-디코더 접근 방식. arXiv 사전 인쇄 arXiv:1409.1259, 2014. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. 공연자들과 함께 관심을 다시 생각해보세요. arXiv 사전 인쇄 arXiv:2009.14794, 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, et al. Palm: 경로를 통한 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311, 2022. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord. 질문 답변을 해결했다고 생각하세요? ai2 추론 챌린지인 arc를 시도해 보세요. arXiv 사전 인쇄본 arXiv:1803.05457, 2018. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré. Flashattention: io-awareness를 갖춘 빠르고 메모리 효율적인 정확한 주의. 신경 정보 처리 시스템의 발전, 35:16344-16359, 2022. Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer. Llm. int8(): 대규모 변압기를 위한 8비트 행렬 곱셈. arXiv 사전 인쇄본 arXiv:2208.07339, 2022. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language model. arXiv 사전 인쇄본 arXiv:2101.00027, 2020. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. A framework for few-shot language model evaluation. Version v0. 0.1. 2021년 9월. 기술 보고서 Tao Ge, Heming Xia, Xin Sun, Si-Qing Chen, Furu Wei. 공격적 디코딩을 통한 seq2seq 생성을 위한 무손실 가속. arXiv 사전 인쇄본 arXiv:2205.10350, 2022. Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He. 정확하고 큰 미니배치 sgd: 1시간 만에 imagenet 학습. arXiv 사전 인쇄본 arXiv:1706.02677, 2017. Alex Graves와 Alex Graves. 장기 단기 기억. 순환 신경망을 사용한 감독 시퀀스 라벨링, pp. 37-45, 2012. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt. 대규모 멀티태스크 언어 이해 측정. arXiv 사전 인쇄본 arXiv:2009.03300, 2020. Geoffrey Hinton, Oriol Vinyals, Jeff Dean. 신경망에서 지식 추출. arXiv 사전 인쇄본 arXiv:1503.02531, 2015. Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, Jonni Kanerva. Sparse는 변압기 확장에 충분합니다. 신경 정보 처리 시스템의 발전, 34:9895-9907, 2021. Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya. Reformer: 효율적인 변압기. arXiv 사전 인쇄 arXiv:2001.04451, 2020. Yaniv Leviathan, Matan Kalman 및 Yossi Matias. 추론적 디코딩을 통해 변환기로부터 빠른 추론. 기계 학습에 관한 국제 회의, pp. 19274–19286. PMLR, 2023. Bingbing Li, Zhenglun Kong, Tianyun Zhang, Ji Li, Zhengang Li, Hang Liu 및 Caiwen Ding. 하드웨어 친화적인 블록 구조 가지치기를 사용하는 효율적인 변환기 기반 대규모 언어 표현입니다. arXiv 사전 인쇄 arXiv:2009.08065, 2020. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang 및 Yue Zhang. Logiqa: 논리적 추론을 통한 기계 독해를 위한 챌린지 데이터세트입니다. arXiv 사전 인쇄본 arXiv:2007.08124, 2020. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao Jia. Specinfer: 추측 추론 및 토큰 트리 검증을 통한 생성적 Ilm 제공 가속화. arXiv 사전 인쇄본 arXiv:2305.09781, 2023. OpenAI. Gpt-4 기술 보고서, 2023. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, Jeff Dean. 효율적으로 변압기 추론 확장. Proceedings of Machine Learning and Systems, 5, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. 비지도 학습을 통한 언어 이해 향상. 2018. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu. 통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구. The Journal of Machine Learning Research, 21(1):5485-5551, 2020. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. Zero: 조개 매개변수 모델 학습을 위한 메모리 최적화. SC20: 고성능 컴퓨팅, 네트워킹, 스토리지 및 분석 국제 컨퍼런스, pp. 1–16. IEEE, 2020. Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier. 라우팅 변환기를 사용한 효율적인 콘텐츠 기반 희소 주의. Association for Computational Linguistics의 거래, 9:53-68, 2021. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi. Winogrande: 대규모 적대적 Winograd 스키마 챌린지. Communications of the ACM, 64(9):99–106, 2021. 기술 보고서 Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. Distilbert, bert의 증류된 버전: 더 작고, 더 빠르고, 더 저렴하고, 더 가볍습니다. arXiv 사전 인쇄 arXiv:1910.01108, 2019. Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge 및 Noah A Smith. 작업에 적합한 도구: 모델 및 인스턴스 복잡성 일치. arXiv 사전 인쇄 arXiv:2004.07453, 2020. Noam Shazeer. 빠른 변환기 디코딩: 쓰기 헤드 하나만 있으면 됩니다. arXiv:1911.02150, 2019. arXiv 사전 인쇄 Noam Shazeer. Glu 변형은 변압기를 개선합니다. arXiv 사전 인쇄 arXiv:2002.05202, 2020. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney 및 Kurt Keutzer. Q-bert: 헤시안 기반의 bert의 초저정밀 양자화입니다. 인공 지능에 관한 AAAI 회의 진행, 34권, pp. 8815–8821, 2020. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper 및 Bryan Catanzaro. Megatron-lm: 모델 병렬성을 사용하여 수십억 개의 매개변수 언어 모델을 교육합니다. arXiv 사전 인쇄 arXiv:1909.08053, 2019. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti 등. deepspeed와 megatron을 사용하여 대규모 생성 언어 모델인 megatron-turing nlg 530b를 훈련합니다. arXiv 사전 인쇄본 arXiv:2201.11990, 2022. Mitchell Stern, Noam Shazeer, Jakob Uszkoreit. 심층 자기 회귀 모델을 위한 블록별 병렬 디코딩. 신경 정보 처리 시스템의 발전, 31, 2018. Shane Storks, Qiaozi Gao, Joyce Y Chai. 자연어 추론의 최근 발전: 벤치마크, 리소스 및 접근 방식 조사. arXiv 사전 인쇄본 arXiv:1904.01172, 2019. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu. Roformer: 회전 위치 임베딩을 사용한 향상된 변압기. arXiv 사전 인쇄본 arXiv:2104.09864, 2021. Pedro Javier Ortiz Suárez, Benoît Sagot, Laurent Romary. 중간에서 낮은 리소스 인프라에서 거대한 코퍼스를 처리하기 위한 비동기 파이프라인. 제7회 대규모 코퍼스 관리의 과제 워크숍(CMLC-7). Leibniz-Institut für Deutsche Sprache, 2019. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin. 변압기의 적응적 주의 지속 시간. arXiv 사전 인쇄 arXiv:1905.07799, 2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: 개방적이고 효율적인 기초 언어 모델입니다. arXiv 사전 인쇄 arXiv:2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser 및 Illia Polosukhin. 주의가 필요한 전부입니다. I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett(편집자), 신경 정보 처리 시스템의 발전, 30권. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 기계 번역을 위한 딥 트랜스포머 모델 학습. 57회 연례 총회 논문집, pp. 1810–1822, 이탈리아 피렌체, 2019년 7월. Association for Computational Linguistics. doi: 10.18653/v1/P19-1176. URL https: //aclanthology.org/P19-1176. Johannes Welbl, Nelson F Liu, Matt Gardner. 크라우드소싱 객관식 과학 문제. arXiv 사전 인쇄본 arXiv:1707.06209, 2017. Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, Tongran Liu. 빠른 변환기에 대한 주의 가중치 공유. arXiv 사전 인쇄본 arXiv:1906.11024, 2019. 기술 보고서 Ofir Zafrir, Guy Boudoukh, Peter Izsak, Moshe Wasserblat. Q8bert: 양자화된 8비트 bert. 2019년 제5회 에너지 효율적 머신 러닝 및 인지 컴퓨팅 워크숍-NeurIPS Edition(EMC2-NIPS), 36-39쪽. IEEE, 2019. Jiali Zeng, Shuangzhi Wu, Yongjing Yin, Yufan Jiang, Mu Li. 신경망 기계 번역을 위한 반복적 주의. 자연어 처리의 경험적 방법에 대한 2021년 컨퍼런스 회의록, 3216-3225쪽, 2021. Biao Zhang 및 Rico Sennrich. 평균 제곱근 계층 정규화. 신경 정보 처리 시스템의 발전, 32, 2019. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, Hongsheng Li. 처음부터 n: m개의 세분화된 구조화된 희소 신경망 학습. arXiv 사전 인쇄본 arXiv:2102.04010, 2021. 부록 사전 학습 하이퍼파라미터 1.3B 레이어 수 숨김 크기 FFN 내부 숨김 크기 어텐션 헤드 어텐션 헤드 크기 임베딩 크기 워밍업 단계 1.5k 학습률 2e-Adam € le-Adam ẞ0.Adam0.어텐션 드롭아웃 0.드롭아웃 0.가중치 감소 0.최대 시퀀스 길이 배치 크기 학습 단계 140k RmsNorm eps 1e-표 4: 사전 학습 하이퍼파라미터 기술 보고서 OPT-1.3B Pythia-1.4B GPT-Neo LLAMA-ours RecycleGPT-std RecycleGPT-rec 추상 대수 STEM 해부학 STEM 0.0.0.0.0.0.0.0.0.0.0.0.0.천문학 STEM 0.0.0.0.0.0.0.비즈니스 윤리 기타 0.0.0.0.0.0.0.임상 지식 기타 0.0.0.0.0.0.대학 생물학 STEM 0.0.0.0.0.0.대학 화학 STEM 0.0.0.0.0.0.대학 컴퓨터 과학 STEM 0.0.0.0.0.0.대학 수학 STEM 0.0.0.0.0.0.대학 의학 기타 0.0.0.0.0.0.대학 물리학 컴퓨터 보안 STEM 0.0.0.0.0.0.STEM 0.0.0.0.0.0.개념 물리학 STEM 0.0.0.0.0.0.경제학 사회 과학 0.0.0.0.0.0.0.전기공학 STEM 0.0.0.0.0.0.초등수학 STEM 0.0.0.0.0.0.형식논리 인문학 0.0.0.0.0.0.전반적 사실 고등학교 생물학 고등학교 화학 기타 0.0.0.0.0.0.STEM 0.0.0.0.0.0.STEM 0.0.0.0.0.0.고등학교 컴퓨터 과학. STEM 0.0.0.0.0.0.고등학교 유럽사 인문학 0.0.0.0.0.0.고등학교 지리 사회과학 0.0.0.0.0.0.고등학교 정부 및 정치 사회과학 0.0.0.0.0.0.고등학교 거시경제학 사회과학 0.0.0.0.0.0.고등학교 수학 STEM 0.0.0.0.0.0.고등학교 미시경제학 사회과학 0.0.0.0.0.0.고등학교 물리학 STEM 0.0.0.0.0.0.고등학교 심리학 사회과학 0.0.0.0.0.0.고등학교 통계학 STEM 0.0.0.0.0.0.고등학교 미국사 인문학 0.0.0.0.0.0.고등학교 세계사 인문학 0.0.0.0.0.0.인문학 노화 기타 0.0.0.0.0.0.인문학 성생활 사회과학 0.0.0.0.0.0.국제법 인문학 0.0.0.0.0.0.법학 인문학 0.0.0.0.0.0.논리적 오류 인문학 0.0.0.0.0.0.기계 학습 STEM 0.0.0.0.0.0.경영 기타 0.0.0.0.0.0.마케팅 기타 0.0.0.0.0.0.의학 유전학 기타 0.0.0.0.0.0.기타 기타 0.0.0.0.0.0.도덕적 분쟁 인문학 0.0.0.0.0.0.도덕적 시나리오 인문학 0.0.0.0.0.0.영양 기타 0.0.0.0.0.철학 인문학 0.0.0.0.0.0.선사 시대 인문학 0.0.0.0.0.0.0.전문 회계 기타 0.0.0.0.0.0.전문 법학 인문학 0.0.0.0.0.0.0.전문 의학 기타 0.0.0.0.0.0.0.전문 심리학 사회 과학 0.0.0.0.0.0.홍보 사회 과학 0.0.0.0.0.0.보안 연구 사회 과학 0.0.0.0.0.0.0.사회학 사회 과학 0.0.0.0.0.0.미국 외교 정책 사회 과학 바이러스학 세계 종교 기타 인문학 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.표 5: MMLU 테스트 세트에서 도메인별 자세한 5샷 결과.
