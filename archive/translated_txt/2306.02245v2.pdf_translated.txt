--- INTRODUCTION ---
지난 몇 년 동안 기초 모델은 언어 및 시각적 작업에서 번창하고 성공하여 놀라운 제로샷 및 퓨샷 기능을 보여주었습니다. 이러한 발전은 연구자와 산업이 인공 지능이 할 수 있는 경계를 확장하도록 격려했으며 세상을 바꿀 잠재력이 있는 환상적인 제품(예: ChatGPT [1])을 보여주었습니다. 최근 Kirillov et al. [11]은 SA1B라는 거대한 데이터 세트에서 학습된 이미지 분할을 위한 새로운 비전 기초 모델인 Segment Anything Model(SAM)을 제안했습니다. 유연한 프롬프트 지원, 모호성 인식 및 방대한 학습 데이터는 SAM에 강력한 일반화를 부여하여 프롬프트 엔지니어링을 사용하여 다운스트림 분할 문제를 해결할 수 있는 기능을 제공합니다. 일부 후속 연구는 SAM의 뛰어난 제로샷 기능을 활용하여 다른 2D 비전 작업(예: 의료 영상 처리 [25] 및 위장된 객체 분할 [20])을 해결합니다. SAM은 일부 2D 비전 작업에서 강력한 성능을 제공하지만 3D 비전 작업에 적용할 수 있는지 여부는 아직 발견해야 합니다. 이러한 영감을 바탕으로 몇몇 연구에서는 SAM을 사전 학습된 3D 모델과 결합하여 3D 장면 표현(예: SA3D [2])과 단일 뷰 재구성(예: anything-3D [15])을 학습하려고 시도했으며, 유망한 결과를 보여주었습니다. 3D 비전의 기본 작업 중 하나인 3D 객체 감지는 광범위한 실제 적용 분야(예: 자율 주행)를 가지고 있습니다. 많은 연구가 이 작업을 해결하는 것을 목표로 하지만 3D 객체 감지의 제로샷 설정은 아직 탐구되어야 합니다. 따라서 SAM의 발전을 고려할 때 자연스럽게 다음과 같은 의문이 제기됩니다. SAM의 제로샷 기능을 3D 객체 감지에 적용할 수 있을까요? 이 논문에서는 SAM [11]만으로 제로샷 3D 객체 감지를 탐구하는 것을 목표로 합니다. SAM은 원래 2D 이미지용으로 구축되었다는 점을 고려할 때 3D 감지에 SAM을 사용할 때 많은 과제가 있습니다(자세한 내용은 부록 참조). 중요한 통찰력은 *연락처 저자를 활용할 수 있다는 것입니다. 영어: 2D 이미지와 같은 데이터 형식으로 중요한 3D 정보(예: 깊이)를 전달하는 Bird&#39;s Eye View(BEV)를 사용하여 3D 객체 감지를 위한 SAM의 강력한 기능을 활용합니다.따라서 3D 감지에 SAM을 사용하는 데 따른 과제를 상당히 해결할 수 있습니다.이러한 관찰을 바탕으로 BEV 맵에서 SAM을 사용하여 분할하고 출력의 마스크를 기반으로 객체를 예측하는 SAM3D를 제시합니다.대규모 Waymo Open Dataset[19]에서 방법을 평가하고 그 결과 3D 객체 감지에서 SAM의 엄청난 잠재력을 보여줍니다.이 논문은 초기 시도에 불과하지만 특히 3D 객체 감지에 대해 SAM과 같은 비전 기반 모델을 3D 비전 작업에 적용하는 데 긍정적인 신호를 제공합니다.2. 방법 우리는 3D 표현이고 자연스럽게 희소한 포인트 클라우드를 방법의 입력으로 고려하는 반면 SAM은 밀도 의미론을 가진 2D 이미지에 대해 훈련되었습니다. 우리의 기본적인 아이디어는 LiDAR 포인트를 도메인 갭을 좁히는 3D 정보가 있는 2D 이미지와 같은 표현으로 변환하는 것이므로 BEV는 간단한 선택입니다. 우리는 그림 1(a)에 표시된 BEV를 기반으로 SAM으로 전체 파이프라인을 구축합니다. 우리의 방법은 주로 5단계로 구성됩니다. 첫째, 우리의 방법은 LiDAR-BEV 투영을 수행하여 희소한 LiDAR 신호를 구별 가능한 BEV 이미지로 변환합니다. 이 단계에서 우리는 투영 방정식을 사용하여 이미지 평면에서 각 포인트의 좌표와 사전 정의된 강도-RGB 매핑을 사용하여 BEV 이미지의 픽셀에 대한 RGB 벡터를 얻어 처리 중에 더 구별 가능하게 만듭니다. 그런 다음 BEV 후처리는 SAM이 &quot;희소한&quot; BEV 이미지와 다른 &quot;밀도&quot; 신호가 있는 자연스러운 이미지에서 학습되었기 때문에 형태 팽창(최대 풀링으로 해석)으로 원래 BEV 이미지를 수정합니다. 이 단계는 SAM에 더 적합한 입력을 형성하는 데 도움이 되어 더 쉬운 분할과 더 나은 성능으로 이어집니다. 원하는 BEV 이미지를 얻은 후 다양한 프롬프트를 지원하는 SAM을 사용하여 BEV 이미지를 분할합니다.3D &quot;2D&quot; LiDAR Projection LevelLevelVersion AP APH AP APH ViT-B 17.11.16.11.3D BBoxes ViT-L 19.13.19.12.ViT-H 19.13.19.12.3D (b) LevelLevelO OO 기둥 크기 메시 그리드(프롬프트) Mask2Box AP APH AP APH OOO 0.05m 16.11.16.11.0.075m 18.12.18.12.0.1m 19.13.19.12.Mask &quot;2D&#39; 사후 처리 0.125m 19.13.18.12.0.15m 18.12.17.12.분할 마스크 0.175m 16.10.15.10.(a) 0.2m 14.9.13.9.0.4m ~~~-(c) BEV 후처리 SAM BEV 이미지 그림 1: (a) 방법의 전체 프레임워크. 먼저 사전 정의된 팔레트를 통해 LiDAR 포인트를 다채로운 BEV 이미지에 투사한 다음, SAM의 요구 사항에 더 잘 맞도록 BEV 이미지를 후처리합니다. 분할 후 노이즈가 있는 마스크를 후처리하고 마지막으로 LiDAR 포인트를 사용하여 3D 경계 상자를 예측합니다. (b) 다양한 버전의 SAM을 사용한 SAM3D의 결과. (c) 다양한 기둥 크기를 사용한 SAM3D의 결과. Waymo 검증 세트에서 [0,30) 범위에 있는 VEHICLE의 메트릭을 보고합니다. 포인트, 상자 및 마스크 프롬프트와 같습니다. 이 단계에서 우리의 목표는 가능한 한 많은 전경 객체를 분할하는 것이므로 메시 그리드 프롬프트로 전체 이미지를 덮기로 했습니다.또한 이 단계에서 성능 희생 없이 프롬프트를 정리하여 분할을 가속화합니다.SAM의 강력한 제로샷 기능에도 불구하고 무시할 수 없는 도메인 갭이 여전히 존재합니다.따라서 사전에서 도출한 몇 가지 규칙에 따라 노이즈가 있는 마스크를 필터링하기 위한 마스크 후처리를 제안하여 거짓 양성의 수를 줄이고 최종 성능을 개선하는 데 도움이 됩니다.마지막으로 분할 및 후처리 후 전경 마스크에서 3D 경계 상자를 예측합니다.BEV 이미지에는 이미 깊이 정보가 있으므로 2D 마스크에서 3D 경계 상자의 수평 속성(예: 수평 객체 중심, 길이, 너비 및 방향)을 직접 추정할 수 있습니다.한편 수직 속성(예: 수직 객체 중심 및 높이)의 경우 LiDAR 포인트가 추가 정보 보상으로 활용됩니다.더 자세한 방법은 부록을 참조하세요. 3. 실험 자율 주행을 위한 대규모 데이터 세트 중 하나인 Waymo Open Dataset[19]에서 우리 방법을 평가합니다. 이 데이터 세트는 798개의 훈련 시퀀스, 검증 시퀀스, 150개의 테스트 시퀀스로 나뉩니다. 우리 방법은 제로샷 객체 감지를 수행하므로 검증 시퀀스에만 초점을 맞춥니다. 메트릭의 경우 포인트 클라우드의 자연스러운 희소성과 의미 레이블 출력의 부족으로 인해 이 논문에서는 최대 30m 거리의 VEHICLE의 mAP와 mAPH에만 관심을 둡니다. SAM은 복잡도가 다른 여러 백본을 사용하므로 그림 1(b)에 표시된 방법의 효과성을 평가하기 위한 실험을 수행합니다. 이는 용량이 적은 SAM을 사용하면 성능이 떨어진다는 것을 보여줍니다. 그러나 ViT-L과 ViT-H를 사용한 SAM 간에는 미미한 차이만 있습니다. 우리는 대규모 모델을 사용할 때 모델 용량이 성능 병목 현상이 아니라고 주장하며 SAM의 힘은 여전히 최대한 발휘되어야 합니다. 보험 목적으로 ViT-H를 사용한 SAM을 사용합니다. 또한 그림 1(c)에서 기둥 크기가 성능에 어떤 영향을 미치는지 확인하기 위한 실험을 수행합니다. 0.2m 및 0.4m와 같은 더 큰 기둥 크기를 사용하는 경우 이산화 오류가 비교적 크고 서로 가까이 있는 다른 객체를 구별하기 어렵습니다. 그러나 기둥 크기가 너무 작아도 성능이 저하됩니다. 한 가지 가능한 이유는 작은 기둥 크기의 높은 해상도와 LiDAR 신호의 희소성으로 인해 개별 인스턴스가 완전히 연결된 영역을 형성하기 어렵기 때문입니다. SAM은 하나의 객체를 여러 부분으로 분리하는 경향이 있습니다. 기둥 크기를 적절한 균형인 0.1m로 설정했습니다. 모든 자세한 결과는 부록을 참조하세요. 4. 결론 이 논문에서는 시각적 기초 모델 SAM을 사용한 제로샷 3D 객체 감지를 살펴보고 SAM3D를 제안합니다. SAM과 3D LiDAR 신호의 훈련 데이터 간의 격차를 줄이기 위해 BEV 이미지를 사용하여 3D 실외 장면을 표현합니다. 우리는 SAM의 뛰어난 제로샷 처리 능력을 활용하여 제로샷 3D 객체 감지를 위한 SAM 기반 BEV 처리 파이프라인을 제안합니다. Waymo 오픈 데이터 세트에 대한 정성적 및 절제 실험은 SAM의 제로샷 능력을 3D 객체 감지에 적용하는 데 유망한 결과를 보여줍니다. 이 논문은 아직 초기 시도에 불과하지만, 앞으로 few-shot learning, model distillation, prompt engineering과 같은 기술을 사용하여 SAM과 같은 기초 모델의 힘을 3D 작업에 활용할 가능성과 기회를 제공한다고 믿습니다. 코드는 https://github.com/DYZhang09/SAM3D에서 공개되었습니다. 5. 감사의 말 본 연구는 중국 우수한 젊은 학자를 위한 국가 과학 기금(지원금 번호 62225603), 후베이 핵심 R&amp;D 프로그램(지원금 번호 2022BAA078), 그리고 &quot;Qisun Ye&quot; 과학 기금(U2341227)의 지원을 받아 수행되었습니다. 6. 부록 6.1.
--- RELATED WORK ---
6.1.2D SAM 작업 Kirillov 등[11]은 이미지 분할을 위한 새로운 비전 기반 모델인 Segment Anything Model(SAM)을 제안했으며, 이는 SA-1B라는 거대한 데이터 집합에서 학습되어 segment anything 작업을 해결했습니다. 유연한 프롬프트 지원, 모호성 인식 및 방대한 학습 데이터는 SAM에 강력한 일반화를 부여하여 프롬프트 엔지니어링을 사용하여 다운스트림 분할 문제를 해결할 수 있는 능력을 제공하여 많은 후속 연구에 영감을 불어넣었습니다. SAMPolyp[25]는 프롬프트되지 않은 설정에서 SAM을 폴립 분할 작업에 적용합니다. Deng 등[6]은 디지털 병리학 작업에서 SAM 모델의 제로샷 분할 성능을 평가하고 SAM이 연결된 대형 객체에 대해 놀라운 분할 성능을 달성한다는 것을 발견했습니다. He 등[7]은 12개의 의료 이미지 데이터 집합에서 SAM의 정확도를 테스트하여 SAM이 2D 의료 이미지, 더 큰 대상 영역 크기 및 더 쉬운 사례에서 더 정확하다는 것을 밝혔습니다. SAMCOD[20]는 위장 물체 감지(COD) 벤치마크에서 SAM의 성능을 평가하여 SAM이 일부 COD 지향 벤치마크에 비해 주목할 만한 성능을 달성할 수 있음을 나타냅니다.
--- METHOD ---
대규모 Waymo Open Dataset[19]에서, 그 결과는 3D 객체 감지에 대한 SAM의 엄청난 잠재력을 보여줍니다. 이 논문은 아직 초기 시도에 불과하지만, 특히 3D 객체 감지에 대한 3D 비전 작업에 SAM과 같은 비전 기반 모델을 적용하는 데 긍정적인 신호를 줍니다. 2. 방법 우리는 3D 표현이고 자연스럽게 희소한 포인트 클라우드를 방법의 입력으로 고려하는 반면, SAM은 밀도 의미론을 가진 2D 이미지에 대해 훈련됩니다. 우리의 기본적인 아이디어는 LiDAR 포인트를 도메인 갭을 좁히는 3D 정보가 있는 2D 이미지와 같은 표현으로 변환하는 것이므로 BEV가 간단한 선택입니다. 우리는 그림 1(a)에 표시된 BEV에 기반한 SAM으로 전체 파이프라인을 구축합니다. 우리의 방법은 주로 5단계로 구성됩니다. 첫째, 우리의 방법은 희소한 LiDAR 신호를 차별적인 BEV 이미지로 변환하는 LiDAR-BEV 투영을 수행합니다. 이 단계에서는 투영 방정식을 사용하여 이미지 평면에서 각 점의 좌표를 결정하고 사전 정의된 강도-RGB 매핑을 사용하여 BEV 이미지의 픽셀에 대한 RGB 벡터를 얻어 처리 중에 더 구별하기 쉽게 만듭니다. 그런 다음 BEV 후처리는 SAM이 &quot;밀도&quot; 신호가 있는 자연스러운 이미지에서 학습되었기 때문에 원래 BEV 이미지를 형태 팽창(최대 풀링으로 해석)으로 수정합니다. 이는 &quot;희소&quot; BEV 이미지와 다릅니다. 이 단계는 SAM에 더 적합한 입력을 형성하는 데 도움이 되어 더 쉬운 분할과 더 나은 성능으로 이어집니다. 원하는 BEV 이미지를 얻은 후 다양한 프롬프트를 지원하는 SAM을 사용하여 BEV 이미지를 분할합니다.3D &quot;2D&quot; LiDAR Projection LevelLevelVersion AP APH AP APH ViT-B 17.11.16.11.3D BBoxes ViT-L 19.13.19.12.ViT-H 19.13.19.12.3D (b) LevelLevelO OO 기둥 크기 메시 그리드(프롬프트) Mask2Box AP APH AP APH OOO 0.05m 16.11.16.11.0.075m 18.12.18.12.0.1m 19.13.19.12.Mask &quot;2D&#39; 사후 처리 0.125m 19.13.18.12.0.15m 18.12.17.12.분할 마스크 0.175m 16.10.15.10.(a) 0.2m 14.9.13.9.0.4m ~~~-(c) BEV 후처리 SAM BEV 이미지 그림 1: (a) 방법의 전체 프레임워크. 먼저 사전 정의된 팔레트를 통해 LiDAR 포인트를 다채로운 BEV 이미지에 투사한 다음, SAM의 요구 사항에 더 잘 맞도록 BEV 이미지를 후처리합니다. 분할 후 노이즈가 있는 마스크를 후처리하고 마지막으로 LiDAR 포인트를 사용하여 3D 경계 상자를 예측합니다. (b) 다양한 버전의 SAM을 사용한 SAM3D의 결과. (c) 다양한 기둥 크기를 사용한 SAM3D의 결과. Waymo 검증 세트에서 [0,30) 범위에 있는 VEHICLE의 메트릭을 보고합니다. 포인트, 상자 및 마스크 프롬프트와 같습니다. 이 단계에서 우리의 목표는 가능한 한 많은 전경 객체를 분할하는 것이므로 메시 그리드 프롬프트로 전체 이미지를 덮기로 했습니다. 또한 이 단계에서 성능 희생 없이 프롬프트를 정리하여 분할을 가속화합니다. SAM의 강력한 제로샷 기능에도 불구하고 무시할 수 없는 도메인 갭이 여전히 존재합니다. 따라서 사전에서 도출한 몇 가지 규칙에 따라 노이즈가 있는 마스크를 필터링하기 위한 마스크 후처리를 제안하여 거짓 양성의 수를 줄이고 최종 성능을 개선하는 데 도움이 됩니다. 마지막으로 분할 및 후처리 후 전경 마스크에서 3D 경계 상자를 예측합니다. BEV 이미지에는 이미 깊이 정보가 포함되어 있으므로 2D 마스크에서 3D 경계 상자의 수평 속성(예: 수평 객체 중심, 길이, 너비 및 방향)을 직접 추정할 수 있습니다. 한편 수직 속성(예: 수직 객체 중심 및 높이)의 경우 LiDAR 포인트가 추가 정보 보상으로 활용됩니다. 더 자세한 방법은 부록을 참조하세요. 3.
--- EXPERIMENT ---
s 우리는 자율 주행을 위한 대규모 데이터 세트 중 하나인 Waymo Open Dataset[19]에서 우리 방법을 평가합니다. 이 데이터 세트는 798개의 훈련 시퀀스, 검증 시퀀스, 150개의 테스트 시퀀스로 나뉩니다. 우리 방법은 제로샷 객체 감지를 수행하므로 검증 시퀀스에만 초점을 맞춥니다. 메트릭의 경우 포인트 클라우드의 자연스러운 희소성과 의미 레이블 출력의 부족으로 인해 이 논문에서는 최대 30m 거리의 VEHICLE의 mAP와 mAPH에만 관심을 둡니다. SAM은 복잡도가 다른 여러 백본을 사용하므로 그림 1(b)에 표시된 것처럼 우리 방법의 효과성을 평가하기 위한 실험을 수행합니다. 이는 용량이 적은 SAM을 사용하면 성능이 떨어진다는 것을 보여줍니다. 그러나 ViT-L과 ViT-H를 사용한 SAM 간에는 미미한 차이만 있습니다. 우리는 대규모 모델을 사용할 때 모델 용량이 성능 병목 현상이 아니라고 주장하며 SAM의 힘은 여전히 최대한 발휘되어야 합니다. 보험 목적으로 ViT-H와 함께 SAM을 사용합니다. 또한 Fig. 1(c)에서 기둥 크기가 성능에 어떤 영향을 미치는지 확인하기 위한 실험을 수행합니다. 0.2m 및 0.4m와 같은 더 큰 기둥 크기를 사용할 경우 이산화 오류가 비교적 크고 서로 가까이 있는 다른 객체를 구별하기 어렵습니다. 그러나 기둥 크기가 너무 작아도 성능이 저하됩니다. 한 가지 가능한 이유는 작은 기둥 크기의 높은 해상도와 LiDAR 신호의 희소성으로 인해 개별 인스턴스가 완전히 연결된 영역을 형성하기 어렵다는 것입니다. SAM은 하나의 객체를 여러 부분으로 분리하는 경향이 있습니다. 기둥 크기를 0.1m로 설정했는데 이는 좋은 균형입니다. 모든 자세한 결과는 부록을 참조하십시오. 4.
--- CONCLUSION ---
이 논문은 시각적 기초 모델 SAM을 사용한 제로샷 3D 객체 감지를 탐구하고 SAM3D를 제안합니다. SAM의 훈련 데이터와 3D LiDAR 신호 간의 격차를 줄이기 위해 BEV 이미지를 사용하여 3D 실외 장면을 표현합니다. SAM 기반 BEV 처리 파이프라인을 제안하여 제로샷 3D 객체 감지를 위해 SAM의 뛰어난 제로샷 처리 능력을 활용합니다. Waymo 오픈 데이터 세트에 대한 정성적 및 절제 실험은 SAM의 제로샷 기능을 3D 객체 감지에 적용하는 데 유망한 결과를 보여줍니다. 이 논문은 아직 초기 시도에 불과하지만 향후 few-shot learning, model distillation, prompt engineering과 같은 기술을 사용하여 SAM과 같은 기초 모델의 힘을 3D 작업에 활용할 가능성과 기회를 제공한다고 믿습니다. 코드는 https://github.com/DYZhang09/SAM3D에서 공개되었습니다. 5. 감사의 말 이 연구는 중국 저명한 젊은 학자를 위한 국가 과학 기금(보조금 번호 62225603), 후베이 핵심 R&amp;D 프로그램(보조금 번호 2022BAA078), 그리고 &quot;Qisun Ye&quot; 과학 기금(U2341227)의 지원을 받았습니다.6. 부록 6.1. 관련 연구 6.1.2SAM을 사용한 D 작업 Kirillov et al.[11]은 이미지 분할을 위한 새로운 비전 기반 모델인 SAM(Segment Anything Model)을 제안했으며, SA-1B라는 거대한 데이터 세트를 사용하여 Segment Anything 작업을 해결했습니다.유연한 프롬프트 지원, 모호성 인식 및 방대한 학습 데이터는 SAM에 강력한 일반화를 부여하여 프롬프트 엔지니어링을 사용하여 다운스트림 분할 문제를 해결할 수 있게 했으며, 많은 후속 연구에 영감을 주었습니다.SAMPolyp[25]는 프롬프트되지 않은 설정에서 SAM을 폴립 분할 작업에 적용합니다.Deng et al. [6]은 디지털 병리학 작업에서 SAM 모델의 제로샷 분할 성능을 평가하고 SAM이 큰 연결 객체에 대해 놀라운 분할 성능을 달성한다는 것을 발견했습니다. He et al. [7]은 12개의 의료 이미지 데이터 세트에서 SAM의 정확도를 테스트하여 SAM이 2D 의료 이미지, 더 큰 대상 영역 크기 및 더 쉬운 사례에서 더 정확하다는 것을 밝혔습니다. SAMCOD [20]는 위장된 객체 감지(COD) 벤치마크에서 SAM의 성능을 평가하여 SAM이 일부 COD 지향 방법에 비해 주목할만한 성능을 달성할 수 있음을 나타냅니다. Ji et al. [10]은 SAM을 최첨단 방법과 비교하고 숨겨진 장면에서 SAM의 제한된 힘을 관찰합니다. 이러한 연구는 의료 이미지 분석 및 위장된 객체 감지와 같은 2D 작업에서 SAM을 활용하는 데 중점을 두는 반면, 우리의 방법은 3D 인식 작업에서 SAM의 능력을 탐구합니다. 6.1.2 SAM을 사용한 3D 작업 SAM은 일부 2D 비전 작업에서 큰 성능을 보이지만 3D 비전 작업에 적용할 수 있는지 여부는 아직 발견해야 합니다. 이러한 영감을 바탕으로 몇몇 연구에서는 SAM을 사전 훈련된 3D 모델과 결합하려고 시도했습니다. SA3D [2]는 저렴하고 기성품인 사전 모델인 Neural Radiance Field(NeRF)를 활용하여 3D 객체를 분할하도록 SAM을 일반화하여 다양한 뷰에서 제안된 마스크 역 렌더링과 크로스 뷰 셀프 프롬프트를 번갈아 수행하여 대상 객체의 3D 마스크를 구성합니다. Anything-3D [15]는 사전 훈련된 2D 텍스트-이미지 확산 모델인 BLIP과 단일 뷰 조건부 3D 재구성 작업을 위한 SAM을 결합하여 유망한 결과를 보여줍니다. 3D-Box-Segment-Anything [3]은 사전 훈련된 3D 감지기 VoxelNeXt [4]와 함께 SAM을 활용하여 대화형 3D 감지 및 레이블 지정을 수행합니다. 영어: 다른 방법과 달리, 우리의 방법은 SAM만을 사용하여 제로샷 3D 객체 감지를 탐구합니다(즉, 다른 사전 훈련된 모델에 의존하지 않습니다).6.1.3 3D 객체 감지 3D 비전의 기본 작업 중 하나인 3D 객체 감지는 광범위한 실제 적용 분야(예: 자율 주행)를 가지고 있습니다.이 작업을 해결하기 위한 많은 연구[22, 12, 16, 4, 13, 24, 26, 21]가 있습니다.SECOND[22]와 PointPillars[12]는 포인트 클라우드를 그리드 기반 표현으로 변환하고 각각 희소 합성곱과 기둥 표현을 도입하여 시간 소모를 줄여 3D 객체 감지를 해결합니다.PointRCNN[18]은 포인트 클라우드를 직접 처리하고 2단계 세분화 전에 포인트 클라우드를 분할하여 고품질 제안을 생성하는 것을 제안합니다.PVRCNN[16]은 3D CNN과 포인트 기반 작업을 결합하여 더 대표적인 특징을 학습합니다. VoxelNeXt [4]는 수작업 프록시에 의존하지 않고 3D 객체 감지를 위한 완전히 희소한 폭셀 기반 파이프라인을 도입하여 더 나은 속도 정확도 균형을 달성합니다.QTNet [9]은 시간 감지 효율성을 용이하게 하는 새로운 쿼리 기반 시간 융합 방법을 제안합니다.ViT-WSS3D [24]는 값비싼 3D 주석에 대한 종속성을 낮추는 새로운 약한 반지도 패러다임을 제공합니다.3D 주석을 사용한 3D 객체 감지는 널리 연구되었지만 상당한 실용적 가치(예: 저렴한 데이터 레이블링)가 있는 3D 객체 감지의 제로샷 설정은 아직 탐구할 필요가 있습니다.이 논문에서는 SAM의 도움으로 제로샷 3D 객체 감지를 탐구하는 것을 목표로 합니다.6.2. 예비 6.2. 3D 객체 감지의 정의 2D 객체 감지와 유사하게 3D 객체 감지의 목표는 인지 센서 데이터(예: LiDAR 기반 3D 객체 감지의 경우 LiDAR 포인트, 카메라 기반 3D 객체 감지의 경우 카메라 이미지)를 고려하여 모든 관심 객체의 위치와 범주를 예측하는 것입니다. 이 논문에서는 LiDAR 기반 3D 객체 감지에 중점을 둡니다. 더 구체적으로, 장면의 LiDAR 포인트 P = {(xi, Yi, Zi)}ı가 주어지면 3D 감지기는 장면의 모든 객체 0 {(cr, BD)}에 대해 추론해야 합니다. 여기서 c; 및 B3D는 다음과 같습니다. = No i = i번째 객체의 범주 및 기하학적 속성이고 No는 객체의 수입니다. 일반적으로 기하학적 속성을 객체 중심, 차원 및 방향으로 정의하며, 공식적으로 다음과 같이 작성합니다.B³D = (x³D, y 3D 3D 3D Zi 3D , dx3D, dy3D, dz³D, 03D), (1) 3D 여기서 (×³D, y³D, z³D), (dx³D, dy³D, dz³D), 0³D는 각각 i번째 객체의 3D 중심, 차원 및 방향입니다.6.2.2 SAM을 사용한 3D 객체 감지의 과제 SAM은 원래 자연스러운 이미지로 2D 분할을 위해 훈련되었기 때문에 LiDAR 기반 3D 객체 감지에 SAM을 채택하는 데에는 많은 고유한 과제가 있습니다.이 섹션에서는 이러한 과제에 대해 보다 자세히 설명합니다.다음과 같은 부분으로 분류할 수 있습니다.SAM 및 3D 감지기의 입력 데이터는 크게 다릅니다.한편으로 입력 데이터의 형식이 다릅니다. 원래 SAM은 전체 2D 이미지 평면에 고르게 분포된 &quot;밀도&quot; 픽셀로 구성된 2D 이미지를 입력으로 사용합니다. 그러나 LiDAR 기반 3D 객체 감지의 경우 입력은 LiDAR 포인트로, 3D 공간에 고르지 않게 분포된 &quot;희소&quot; 포인트의 위치를 나타냅니다. 반면 입력에 포함된 정보는 다릅니다. 원래 SAM은 픽셀이 풍부한 의미 정보를 전달하는 자연스러운 이미지로 학습한 반면, LiDAR 포인트는 3D 장면의 기하학적 정보만 전달합니다. 입력 데이터의 격차를 줄이기 위해 2D 형식과 3D 정보 인식으로 인해 Bird&#39;s Eye View(BEV)를 미디어로 사용합니다. SAM과 3D 감지기의 출력 데이터는 상당히 다릅니다. SAM은 가능한 전경 픽셀을 나타내는 2D 분할 마스크를 출력하는 반면, 일반적인 3D 감지기는 3D 경계 상자를 출력합니다. 2D 분할 마스크를 3D 경계 상자로 변환하는 것은 핵심적인 문제입니다. 실외 장면의 특성 덕분에 동일한 위치에 수직으로 쌓이는 물체가 없기 때문에 BEV 맵과 입력 LiDAR 포인트를 활용하여 변환을 완료하고 마지막으로 SAM을 장착하여 3D 감지기를 형성할 수 있습니다. SAM의 3D 인식 기능은 제한적입니다. 이 논문에서는 제로샷 3D 객체 감지를 탐구하는 것을 목표로 합니다. 즉, 모델을 훈련할 3D 샘플이 없고 SAM은 2D 이미지로만 훈련되기 때문에 3D 인식 기능이 제한됩니다. 이러한 과제를 최대한 극복하기 위해 BEV를 사용하여 3D 정보를 2D 형태로 &quot;위장&quot;합니다. 게다가 SAM의 제한된 3D 용량으로 인해 출력에 노이즈가 있을 것이고, 노이즈가 있는 마스크를 필터링하기 위한 규칙 기반 사후 처리를 제안하여 성능을 크게 개선하는 데 도움이 됩니다. 6.3. 제안된 방법 6.3.1 전체 프레임워크 저희의 방법은 주로 5단계로 구성됩니다. • LiDAR-to-BEV 투영은 LiDAR 신호를 BEV 이미지로 변환합니다. • BEV 사후 처리에서는 간단한 작업으로 원본 BEV 이미지를 수정합니다. • SAM[11]은 수정된 BEV 이미지와 메시 그리드 프롬프트를 사용하여 BEV의 전경 객체를 분할합니다. 분할 프로세스를 가속화하기 위해 이 단계에서 성능 희생 없이 프롬프트를 정리합니다. • 마스크 사후 처리에서는 사전 확률에서 도출한 몇 가지 규칙에 따라 노이즈가 있는 마스크를 필터링하여 거짓 양성의 수를 줄입니다. • Mask2Box는 전경 마스크의 최소 경계 상자를 찾아 2D 상자를 추출합니다. BEV는 LiDAR 포인트와 상호 작용하여 객체의 최종 3D 경계 상자를 예측합니다.다음 섹션에서는 각 단계에 대한 자세한 디자인을 설명합니다.6.3.2 LiDAR-BEV 투영 i = LiDAR-BEV 투영의 임무는 범위 Lx ≤ x; ≤ Ux, Ly ≤ Yi ≤ Uy인 Np개의 LiDAR 포인트 P = {(xi, Yi, Zi)}를 단일 BEV 이미지 I Є RH×W×로 변환하는 것입니다.각 포인트는 BEV 이미지의 그리드에 속하고 그리드 (cx, cy)의 위치는 다음과 같이 계산됩니다.CXi = суі = [(Ux — xi)/Sx], [(Uy — Yi)/Sy], (3) 여기서 sx, Sy는 x 및 y 축의 기둥 크기이고, Ux, Uy는 각각 x 및 y 축의 좌표 상한이며 바닥 함수를 의미합니다. 격자의 위치를 얻은 후에는 BEV 이미지에 값을 채워야 합니다. 분할을 더 쉽게 하기 위해 BEV 이미지가 차별적이기를 바랍니다. 한 가지 발견은 점의 반사 강도가 유용하다는 것입니다. 즉, 강도 R = {r}을 사용하여 BEV 이미지에서 격자의 특징 벡터를 형성할 수 있습니다. 구체적으로, 먼저 강도를 [0,1]로 정규화한 다음, 이를 사용하여 사전 정의된 팔레트에서 색상 벡터를 선택합니다. 이는 다음과 같이 공식적으로 작성할 수 있습니다. Ci = Palette (Norm(r)) = R³, I[cxi, cyi,] = = Ci, (4) (5) 여기서 Palette: R → R³는 강도 스칼라를 RGB 벡터로 변환하는 데 사용되는 사전 정의된 팔레트입니다. 투영된 점이 없는 격자의 경우 모든 0 벡터를 채웁니다. 이제 우리는 차별적인 BEV 이미지 I ЄRH×W×6.3.3 BEV 후처리를 얻습니다.SAM은 &quot;밀도&quot; 신호를 포함하고 &quot;희소&quot; BEV 이미지와 다른 자연스러운 이미지에서 학습되었으므로 갭을 좁히기 위해 BEV 이미지를 후처리해야 합니다.이 논문에서는 형태 팽창을 사용하는데, 이는 방정식 6으로 표시된 최대 풀링으로 해석될 수 있습니다.I&#39; = MaxPool2D(I), 여기서 I&#39;는 후처리 후의 BEV 이미지입니다.6.3.4 SAM을 사용한 분할(6) 이제 점, 상자 및 마스크 프롬프트와 같은 다양한 프롬프트를 지원하는 SAM을 사용하여 BEV 이미지를 분할합니다.이 단계에서의 목표는 가능한 한 많은 전경 객체를 분할하는 것이므로 전체 이미지를 메시 그리드 프롬프트로 덮기로 했습니다.특히 이미지 평면에 고르게 분포된 32×메시 그리드를 만들고 이를 SAM에 대한 점 프롬프트로 간주합니다. 이렇게 하면 전체 이미지를 다룰 수 있지만, 많은 프롬프트가 빈 공간에 빠지기 때문에 BEV 이미지의 자연스러운 희소성으로 인해 더 효율적일 수 있습니다.이러한 관찰을 바탕으로 프롬프트를 가지치기합니다.특히 이러한 프롬프트를 BEV 이미지에 투사하고 각 프롬프트의 이웃 영역을 확인한 다음 주변에 활성화된 픽셀이 없는 프롬프트를 버립니다.이 작업은 전체 파이프라인을 극적으로 가속화하여 속도를 5배 높입니다(단일 NVIDIA GeForce RTX 4090에서 0.4FPS에서 2FPS로).이 단계가 끝나면 이제 SAM에서 Nm 분할 마스크 M = {miЄ RHXWm을 얻습니다.N Ji=6.3.5 마스크 후처리 SAM의 강력한 제로 샷 기능에도 불구하고 무시할 수 없는 도메인 갭이 여전히 존재합니다.따라서 SAM의 마스크에는 노이즈가 많고 추가 처리가 필요합니다. 자율 주행 장면에서 일반적인 자동차는 특정 영역과 종횡비를 가지며, 이를 사용하여 마스크 M에서 일부 거짓 양성을 필터링할 수 있습니다. 자세히 설명하면, 영역 임계값 [Ta, Th]과 종횡비 임계값 [TT]을 사용하여 노이즈가 있는 분할 마스크를 필터링합니다. 이러한 연산을 통해 최종적으로 다음을 얻습니다. 상대적으로 고품질의 전경 마스크 M&#39; = {mi Є RHXW, 각 마스크는 전경 객체에 해당합니다. N 6.3.6 Mask2Box 분할 후 전경 마스크 M&#39;에서 3D 경계 상자 B3D를 예측해야 합니다. BEV 이미지에는 이미 깊이 정보가 포함되어 있으므로 2D 마스크에서 3D 경계 상자의 수평 속성(예: 수평 객체 중심, 길이, 너비 및 방향)을 직접 추정할 수 있습니다. 한편, 수직 속성(예: 수직 객체 중심 및 높이)의 경우 LiDAR 포인트가 추가 정보 보상으로 활용됩니다. 먼저 마스크에서 2D 최소 경계 상자를 추출합니다. 이는 방정식으로 정의됩니다. 7. B2D={(2D,2D,dx2D, dy 2D, 2D), (7) 여기서 (x2D, y2D), (dx2D, dy2D), 및 02D는 i번째 객체의 2D 중심, 차원 및 회전 각도입니다. No는 객체의 수입니다. 그런 다음 이러한 2D 속성을 해당 3D 속성으로 다시 투영합니다. x3D = Ux − (x² + 0.5) × Sx, (8) y3D = Uy − (y² + 0.5) × Sy, 2D (9) dx3D = dx²D 2D × Sx (10) dy3D = dy?D 2D × Sy, (11) (12) 03D = 02D, = 여기서 Ux, Uy는 포인트 클라우드 범위이고 sx, Sy는 6.3.2절에 정의된 기둥 크기입니다. 마지막으로 LiDAR 지점으로 수직 중심과 높이를 추정합니다. 가장 중요한 아이디어는 BEV 투영이 2D 경계 상자 내부에 있는 지점을 선택하고 수직 좌표를 사용하여 수직 속성을 계산하는 것입니다. Z i = { zj | (xj, Y j, Zj) 내부 B3D}, (13) dz 3D = max (Z) — min (Zi), (14) 23D = min (Z₁) + dzi dz³D (15)6.4. 실험 하이퍼파라미터 6.4. Ly = -30.0m, Ux = 기본값으로 포인트 클라우드 범위 Lx를 30.0m, 기둥 크기 8x = Sy 0.1m로 설정합니다. BEV 사후 처리에서 확장에 3 × 3 커널을 사용합니다. 마스크 후처리의 경우, 각각 면적 임계값 Ta =200, Ta 5000픽셀, 종횡비 임계값 T 1.5, Th 4를 설정했습니다.SAM 아키텍처의 경우, 공식 저장소에서 사전 학습된 가중치가 있는 기본 버전(ViT-H)을 사용합니다.= 6.4.2 정성적 결과 먼저 방법의 정성적 결과를 보여드립니다.그림은 SAM 출력에서 비교적 고품질의 2D 회전 경계 상자가 생성되는 것을 보여주며, 이는 SAM의 뛰어난 제로 기능을 나타냅니다.즉, SAM은 학습 중에 BEV 이미지와 3D 주석을 건드리지 않고도 우수한 마스크를 생성할 수 있습니다.마스크 후처리와 Mask2Box 모듈은 전경 마스크를 고품질 3D 경계 상자로 변환할 수 있습니다.이는 BEV 이미지에서 완전히 인식되고 구별 가능한 모양을 가진 객체의 경우 SAM3D가 이를 쉽게 식별하고 합리적인 예측을 생성할 수 있음을 보여줍니다. SAM의 놀라운 성능에도 불구하고 몇 가지 명백한 실패 사례가 여전히 존재합니다.(1) SAM은 객체가 서로 가까울 때 중복된 마스크를 생성합니다(그림 2에서 빨간색 거품으로 표시). SAM은 이러한 상황을 처리하도록 훈련되지 않았기 때문에 이러한 지점이 다른 객체에 속하는지 또는 하나의 큰 객체에 속하는지 식별하기 어렵다고 주장합니다.(2) 일부 배경 객체는 BEV 이미지에서 자동차와 유사해 보이며 SAM은 때때로 실수로 이를 전경으로 간주합니다(그림 2에서 파란색 거품으로 표시).(3) 잘림, 폐색 및 LiDAR 신호의 희소성으로 인해 일부 자동차는 BEV 이미지에서 부분적으로 활성화됩니다. 따라서 SAM은 이러한 객체를 무시하고 많은 거짓 부정을 남깁니다(그림 2에서 흰색 거품으로 표시).(2)와 (3)은 LiDAR 기반 3D 객체 감지에 내재된 과제이며 처음에 2D 분할 작업을 위해 훈련된 모델에 대해서는 훨씬 더 복잡하다고 주장합니다. 영어: 저희의 방법은 몇 가지 순진한 해결책을 제공하지만, 이러한 문제를 더 완벽하게 해결하기 위해서는 여전히 많은 노력(예: 더 강력한 배경 억제 또는 전경 완성 방법이 포함될 수 있음)이 필요합니다.게다가 [10]에서 알 수 있듯이 SAM은 위장된 물체를 분할하는 데 어려움을 겪습니다.저희는 (2)와 (3)에서 이러한 물체를 위장된 것으로 볼 수 있으므로 결과가 좋지 않습니다.즉, SAM의 능력을 향상시키기 위해 더 강력한 기술이 필요하다는 뜻입니다.6.4.3 절제 연구 저희는 다양한 설계의 기여도를 파악하기 위해 절제 연구를 수행합니다.저희는 [0, 30] 범위의 VEHICLE에 대해서만 모든 실험에 대한 AP와 APH를 보고합니다.필러 크기의 효과.식 2에 따르면 필러 크기는 BEV 이미지의 해상도에 영향을 미쳐 SAM의 분할 결과에 영향을 미칩니다.본문에서 저희는 필러 크기의 효과를 보고합니다. 효과를 더 직관적으로 만들기 위해 그림 3에서 다양한 기둥 크기 설정에서 BEV를 시각화합니다. 0.2m 및 0.4m와 같은 더 큰 기둥 크기를 사용하는 경우 이산화 오류가 비교적 크고 서로 가까이 있는 다른 객체를 구별하기 어렵습니다. 그러나 기둥 크기가 너무 작아도 성능에 해가 됩니다. 한 가지 가능한 이유는 작은 기둥 크기의 높은 해상도와 LiDAR 신호의 희소성으로 인해 개별 인스턴스가 표 1: 다양한 유형의 BEV 이미지를 사용한 SAM3D 결과. Waymo 검증 세트에서 [0,30) 범위에 있는 VEHICLE의 메트릭을 보고합니다. 수준 수준 BEV 유형 AP APH AP APH 이진 0.0.0.0. 강도 1.1.1.1.13.30 19.05 12. 강도 + 팔레트 19. 표 2: 사후 처리의 절제. BEV 사후.는 BEV 사후 처리를 의미합니다. 면적과 종횡비는 마스크 사후 처리에서 각각 면적과 종횡비에 따른 필터 마스크에 해당합니다. Waymo 검증 세트에서 [0,30) 범위의 VEHICLE에 대한 메트릭을 보고합니다. BEV 사후. 면적 종횡비 ✓ 레벨 레벨 AP APH AP APH 11.01 7.68 10.93 7.17.52 11.14.05 9.17.13.11.9.19.51 13.30 19.05 12. 완전히 연결된 영역을 형성합니다. SAM은 하나의 객체를 여러 부분으로 분리하는 경향이 있습니다. 기둥 크기를 0.1m로 설정했는데, 이는 적절한 균형입니다. 반사 강도의 효과. BEV에 점을 투사하는 방법은 BEV 이미지의 시각적 모양을 결정하여 분할에 영향을 미칩니다. 6.3.2에서 점의 반사 강도와 미리 정의된 강도-RGB 매핑을 사용하는 것이 도움이 된다고 주장하며 이 하위 섹션에서 그 효과를 평가합니다. 우리는 우리의 방법을 이진 및 강도라는 두 가지 다른 유형과 비교합니다. 이진 유형의 경우, BEV 이미지의 픽셀은 어떤 점이 투영되면 흰색으로 설정되고 그렇지 않으면 검은색이 됩니다. 강도 유형의 경우, 우리는 정규화된 강도를 회색조로 사용합니다. 표 1에서 강도를 사용하면 순진한 이진 유형에 비해 이점이 있고 강도를 rgb 공간에 매핑하면 성능이 더욱 극적으로 향상되는 것을 볼 수 있습니다. 강도와 RGB 매핑은 BEV 이미지를 더 구별하기 쉽게 만들기 때문에 SAM은 이를 더 쉽고 정확하게 분할하여 제안된 방법의 성능을 향상시킵니다. BEV 후처리의 효과. 6.3.3에서 우리는 더 나은 분할을 위해 BEV 이미지를 후처리하기 위해 형태 팽창을 사용합니다. 우리는 이 하위 섹션에서 그 효과를 알아내기 위한 실험을 수행하며, 표의 첫 번째 행에 표시됩니다. 2. BEV 후처리 없이 레벨 1과 레벨 2 모두에서 약 8% AP와 5% APH가 떨어집니다. 앞서 주장했듯이 BEV 후처리는 BEV와 자연 이미지(SAM의 훈련 데이터) 간의 격차를 줄이는 데 도움이 되어 더 나은 결과를 얻을 수 있습니다. 마스크 후처리의 효과. 6.3.5절에서, (a) (b) (c) FO REP (d) (e) (f)그림 2: SAM3D 결과의 시각화. 각 하위 그림은 단일 프레임에 해당합니다. 각 하위 그림의 왼쪽은 BEV(Bird&#39;s Eye View) 아래의 2D 경계 상자의 시각화이고 오른쪽은 3D 경계 상자의 시각화입니다. Sx Sy=0.05m.. Sx = Sy = 0.1m Sx = Sy = 0.2m Sx = Sy = 0.4m 그림 3: 다양한 기둥 크기 설정에서 BEV 이미지의 시각화. 우리는 SAM에서 노이즈가 있는 마스크를 필터링하기 위해 마스크 영역 및 종횡비 임계값(TT), (TT)을 제안합니다. 또한 표 2의 두 번째와 세 번째 행에 표시된 효과를 테스트하기 위한 실험을 수행합니다. 무시할 수 없는 도메인 갭이 여전히 존재하고 SAM의 마스크에 노이즈가 있으므로 마스크 후처리의 모든 작업이 필수적이라는 것은 분명합니다. 이 중 하나라도 삭제하면 전체 모델에 비해 성능이 크게 저하됩니다. 6.4.4 완전 지도 3D 객체 감지기와의 비교 우리의 방법과 일반적인 완전 지도 3D 감지기 간의 갭을 더 잘 이해하기 위해 표 3에 결과를 나열합니다. 기존의 완전 지도 3D 감지기와 비교할 때 우리의 방법은 상당한 갭으로 뒤처집니다. SAM은 2D 분할에 대해서만 훈련되었으므로 당연한 일입니다. 게다가 우리의 방법의 AP와 APH 간의 차이가 다른 방법의 차이보다 훨씬 더 크다는 것을 관찰했습니다. 이는 SAM이 방향을 인식하지 못하고 방향 예측이 분할 마스크의 최소 방향 경계 상자 추정에서 나오기 때문이며, 이는 표 3: SAM3D 및 완전 감독 3D 감지기의 결과입니다. Waymo 검증 세트에서 [0,30) 범위에 있는 VEHICLE의 메트릭을 보고합니다. 방법 Levell LevelTraining 데이터 SECOND [22] CenterPoint [23] VoxelRCNN [5] PVRCNN [16] PVRCNN++ [17] Waymo (3D Det) Waymo (3D Det) Waymo (3D Det) Waymo (3D Det) Waymo (3D Det) CenterFormer [27] Waymo (3D Det) Ours SA-1B (2D Seg) 89.90.91.19.AP APH AP APH 85.60 84.94 84.25 83.81.93 81.20 80.53 79.88.85 88.42 87.52 87.88.92 88.19 87.89.61 88.77 88.90.58 89.84 89.13.30 19.05 12. 간단하지만 노이즈가 많습니다.6.5. 논의 정성적 결과와 절제 연구를 통해 3D 주석 없이 대규모 분할 데이터 세트에서 학습된 SAM을 활용하여 야외 장면의 제로 샷 객체 감지 작업을 해결할 수 있음을 보여줍니다.그러나 현재 방법에는 개선해야 할 부분이 몇 가지 있으며 이러한 문제는 앞으로 해결해야 합니다.• BEV 이미지를 표현으로 사용한다는 것은 우리 방법 I이 실내 장면에 적합하지 않을 수 있음을 의미합니다.더 나은 장면 표현을 찾는 것이 좋은 해결책이 될 것입니다.• LiDAR 포인트의 폐색, 절단 및 희소성으로 인해 우리 방법은 특히 먼 객체의 경우 많은 거짓 부정을 생성합니다.다른 모달리티의 정보를 고려하는 것이 도움이 될 것입니다. • 추론 시간은 이미 5배 단축되었지만 추론 속도(단일 NVIDIA GeForce RTX 4090에서 2FPS)는 여전히 SAM의 복잡성에 제한되어 있으며, 특히 포인트 프롬프트 수가 많은 경우에 그렇습니다. 모델 압축 및 증류를 수행하면 이 문제가 해결될 수 있습니다. • 현재 저희 방법은 SAM의 의미 레이블 출력이 없기 때문에 다중 클래스 감지를 지원하지 않습니다. 한 가지 가능한 해결책은 제로샷 분류를 위해 3D 비전 언어 모델(예: CLIP Goes 3D [8], CrowdCLIP [14])을 활용하는 것입니다. 저희 방법은 특히 3D 객체 감지에서 3D 비전 작업에서 SAM과 같은 기초 모델의 잠재력을 최대한 활용할 수 있는 큰 가능성과 기회를 보여준다고 믿습니다. few-shot learning 및 프롬프트 엔지니어링과 같은 기술을 사용하면 비전 기초 모델을 보다 효과적으로 사용하여 3D 작업을 더 잘 해결할 수 있으며, 특히 2D와 3D 데이터의 규모 간의 엄청난 차이를 고려할 때 더욱 그렇습니다. 참고문헌 [1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전 과정, 33:1877-1901, 2020. [2] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, Qi Tian. nerfs를 사용하여 3D로 모든 것을 분할합니다. arXiv 사전 인쇄본 arXiv:2304.12308, 2023. [3] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, Jiaya Jia. 3d-box-segment-anything. 한국어: https://github.com/dvlab-research/ 3D-Box-Segment-Anything.git, 2023. [4] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, Jiaya Jia. Voxelnext: 3D 객체 감지 및 추적을 위한 완전 희소 복셀넷. IEEE 국제 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2023. [5] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, Houqiang Li. Voxel r-cnn: 고성능 복셀 기반 3D 객체 감지를 향하여. AAAI 컨퍼런스 논문집 인공지능에 관한 논문, 35권, 1201-1209페이지, 2021. [6] Ruining Deng, Can Cui, Quan Liu, Tianyuan Yao, Lucas Walker Remedios, Shunxing Bao, Bennett A Landman, Yucheng Tang, Lee E Wheless, Lori A Coburn 외. 디지털 병리학을 위한 모든 것 분할 모델(sam): 전체 슬라이드 영상에서 제로 샷 분할 평가. Medical Imaging with Deep Learning, 짧은 논문 트랙, 2023. [7] Sheng He, Rina Bao, Jingpeng Li, P Ellen Grant, Yangming Ou. 의료 영상 분할 작업에서 모든 것 분할 모델(sam)의 정확도. arXiv 사전 인쇄본 arXiv:2304.09324, 2023. [8] Deepti Hegde, Jeya Maria Jose Valanarasu, Vishal M Patel. 클립이 3D로 전환: 언어 기반 3D 인식을 위한 신속한 튜닝 활용. arXiv 사전 인쇄본 arXiv:2303.11313, 2023. [9] Jinghua Hou, Zhe Liu, Zhikang Zou, Xiaoqing Ye, Xiang Bai, et al. 3D 객체 감지를 위한 명시적 동작을 사용한 쿼리 기반 시간 융합. 신경 정보 처리 시스템 발전 논문, 2023. [10] Ge-Peng Ji, Deng-Ping Fan, Peng Xu, Ming-Ming Cheng, Bowen Zhou, Luc Van Gool. Sam은 숨겨진 장면에서 어려움을 겪습니다. &quot;모든 것을 분할하세요&quot;에 대한 경험적 연구. Sci. China Inf. Sci., 2023. [11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. 모든 것을 분할하세요. IEEE 국제 컴퓨터 비전 컨퍼런스 논문집, 2023. [12] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom. Pointpillars: 포인트 클라우드에서 객체 감지를 위한 고속 인코더. IEEE 국제 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 12697-12705쪽, 2019. [13] Jingyu Li, Zhe Liu, Jinghua Hou, Dingkang Liang. Dds3d: 반지도 3차원 객체 감지를 위한 동적 임계값을 사용한 고밀도 가상 레이블. 국제 로봇 및 자동화 컨퍼런스 논문집, 2023. [14] Dingkang Liang, Jiahao Xie, Zhikang Zou, Xiaoqing Ye, Wei Xu, Xiang Bai. Crowdclip: 시각 언어 모델을 통한 비지도 군중 계산. IEEE 국제 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2893-2903쪽, 2023. [15] Qiuhong Shen, Xingyi Yang, Xinchao Wang. Anything3d: 야외에서 단일 뷰 무엇이든 재구성을 향해. arXiv 사전 인쇄 arXiv:2304.10261, 2023. [16] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang 및 Hongsheng Li. Pv-rcnn: 3D 객체 감지를 위한 Pointvoxel 기능 세트 추상화입니다. Proc에서 IEEE Intl. 회의 컴퓨터 비전 및 패턴 인식, 페이지 10529-10538, 2020. [17] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang 및 Hongsheng Li. Pvrcnn++: 3D 객체 감지를 위한 로컬 벡터 표현을 사용한 포인트 복셀 기능 세트 추상화입니다. International Journal of Computer Vision, 131(2):531-551, 2023. [18] Shaoshuai Shi, Xiaogang Wang, Hongsheng Li. Pointrcnn: 포인트 클라우드에서 3D 객체 제안 생성 및 감지. IEEE 국제 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 770-779페이지, 2019. [19] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. 자율 주행을 위한 지각의 확장성: Waymo 오픈 데이터 세트. IEEE 국제 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2446-2454페이지, 2020. [20] Lv Tang, Haoke Xiao, Bo Li. 샘은 무엇이든 분할할 수 있을까?샘이 위장된 객체 감지를 만났을 때.arXiv 사전 인쇄본 arXiv:2304.04709, 2023. [21] Kaixin Xiong, Dingyuan Zhang, Dingkang Liang, Zhe Liu, Hongcheng Yang, Wondimu Dikubab, Jianwei Cheng, Xiang Bai. 단안 3D 객체 감지의 경우 하향식으로만 찾습니다.IEEE Robotics and Automation Letters, 2023. [22] Yan Yan, Yuxing Mao, Bo Li. 두 번째: 희소하게 포함된 합성곱 감지.센서, 18(10):3337, 2018. [23] Tianwei Yin, Xingyi Zhou, Philipp Krahenbuhl.센터 기반 3D 객체 감지 및 추적.IEEE Intl.Conf.의 Proc.에서. 컴퓨터 비전 및 패턴 인식, 페이지 11784-11793, 2021. [24] Dingyuan Zhang, Dingkang Liang, Zhikang Zou, Jingyu Li, Xiaoqing Ye, Zhe Liu, Xiao Tan 및 Xiang Bai. 약하게 반지도된 3D 객체 감지를 위한 간단한 비전 변환기입니다. 포크에서. IEEE Intl. 회의 컴퓨터 비전, 페이지 8373-8383, 2023. [25] Tao Zhou, Yizhe Zhang, Yi Zhou, Ye Wu 및 Chen Gong. 샘이 폴립을 분할할 수 있나요? arXiv 사전 인쇄 arXiv:2304.07583, 2023. [26] Xin Zhou, Jinghua Hou, Tingting Yao, Dingkang Liang, Zhe Liu, Zhikang Zou, Xiaoqing Ye, Jianwei Cheng 및 Xiang Bai. 랜덤 박스를 사용한 확산 기반 3D 객체 감지. 중국 패턴 인식 및 컴퓨터 비전 컨퍼런스, 28-40페이지. Springer, 2023. [27] Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, Hassan Foroosh. Centerformer: 3D 객체 감지를 위한 센터 기반 변환기. 유럽 컴퓨터 비전 컨퍼런스 논문집, 2022.
