--- ABSTRACT ---
우리는 이미지 태깅을 위한 강력한 기반 모델인 Recognize Anything Model(RAM)을 제시합니다. RAM은 컴퓨터 비전에서 대규모 모델을 위한 실질적인 진전을 이루며, 높은 정확도로 모든 공통 범주를 인식하는 제로샷 기능을 보여줍니다. RAM은 이미지 태깅을 위한 새로운 패러다임을 도입하여 수동 주석 대신 대규모 이미지 텍스트 쌍을 활용하여 학습합니다. RAM의 개발은 네 가지 핵심 단계로 구성됩니다. 첫째, 주석이 없는 이미지 태그는 자동 텍스트 의미 분석을 통해 대규모로 얻습니다. 그런 다음, 원래 텍스트와 구문 분석된 태그가 각각 감독하는 캡션 및 태그 작업을 통합하여 자동 주석을 위한 예비 모델을 학습합니다. 셋째, 데이터 엔진을 사용하여 추가 주석을 생성하고 잘못된 주석을 정리합니다. 마지막으로, 모델은 처리된 데이터로 다시 학습하고 더 작지만 더 높은 품질의 데이터 세트를 사용하여 미세 조정합니다. 우리는 수많은 벤치마크에서 RAM의 태그 기능을 평가하고 인상적인 제로샷 성능을 관찰하여 CLIP 및 BLIP보다 상당히 우수한 성능을 보입니다. 놀랍게도 RAM은 완전 감독 방식을 능가하고 Google 태그 API와 경쟁력 있는 성능을 보여줍니다. 우리는 컴퓨터 비전에서 대형 모델의 발전을 촉진하기 위해 https://recognize-anything.github.io/에서 RAM을 출시합니다. 1.
--- METHOD ---
영어: 수동 검사, WordNet[7] 참조, 태그 번역 및 병합 등을 포함한 ologies. 동일한 동의어 그룹 내의 태그에는 동일한 태그 ID가 지정되어 레이블 시스템에 4585개 태그 ID가 생성됩니다. 3.2. 데이터 세트 BLIP[15] 및 Tag2Text[10]와 유사하게 널리 사용되는 오픈 소스 데이터 세트에서 모델을 사전 학습합니다. 백만(4M)개 이미지와 1,400만(14M)개 이미지 설정을 채택합니다. 4M 설정에는 두 개의 인간 주석이 달린 데이터 세트인 COCO[16](113K개 이미지, 557K개 캡션) 및 Visual Genome[13](101K개 이미지, 822K개 캡션)과 두 개의 대규모 웹 기반 데이터 세트인 Conceptual Captions[6](3M개 이미지, 3M 캡션) 및 SBU Captions[21](849K개 이미지, 849K개 캡션)이 포함됩니다. 14M 설정은 Conceptual 12M [6] (10M 이미지, 10M 캡션)을 추가하여 4M을 기반으로 구축됩니다.3.3. 데이터 엔진 대부분 인터넷에서 크롤링한 교육 데이터 세트의 대부분 오픈 소스 특성을 감안할 때, 누락되거나 잘못된 레이블이 무시할 수 없을 정도로 많이 발생합니다.이를 완화하기 위해 추가 태그를 생성하고 오류가 있는 태그를 정리하는 자동 데이터 엔진을 설계합니다.생성.첫 번째 단계는 Tag2Text [10]에서 사용된 접근 방식과 유사하게 이러한 캡션에서 구문 분석된 캡션과 태그를 사용하여 기준 모델을 교육하는 것입니다.그런 다음 이 기준 모델을 활용하여 캡션과 태그를 모두 보완하고 각각 생성 및 태그 지정 기능을 활용합니다.원래 캡션과 태그는 생성된 캡션, 해당 구문 분석된 태그 및 생성된 태그와 함께 병합되어 임시 데이터 세트를 형성합니다.이 단계에서는 4M 이미지 데이터 세트의 태그 수가 1,200만에서 3,980만으로 크게 늘어납니다.정리. 잘못된 태그 문제를 해결하기 위해, 우리는 처음에 Grounding-Dino[29]를 사용하여 모든 이미지 내에서 특정 범주에 해당하는 영역을 식별하고 잘라냅니다. 그런 다음, 우리는 K-Means++[4]를 기반으로 이 범주의 영역을 클러스터링하고 이상치 10%와 관련된 태그를 제거합니다. 동시에, 우리는 또한 기준 모델을 사용하여 이 특정 범주를 예측하지 않은 태그를 제거합니다. 동기는 태그 모델의 정확도가 전체 이미지가 아닌 영역을 예측함으로써 향상될 수 있다는 것입니다. 4.
--- EXPERIMENT ---
4.1. 실험 설정 테스트 벤치마크. 분류, 탐지, 분할을 포함한 다양한 컴퓨터 비전 작업에서 다양한 인기 있는 벤치마크 데이터세트에 대한 모델에 대한 포괄적인 평가를 표에 요약하여 수행했습니다. 분류를 위해 9605개 범주가 포함된 OpenImages V6[14]을 채택했습니다. 그러나 OpenImages 데이터세트 내에서 레이블이 누락되고 주석이 잘못되었다는 문제로 인해 214개의 잘 주석이 달린 일반 범주로 구성된 OpenImagescommon과 오픈 세트 실험을 위한 레이블 시스템에 포함되지 않은 200개 범주로 구성된 OpenImages-rare의 두 가지 고품질 하위 세트를 큐레이션했습니다. 또한 더 나은 제로샷 평가를 용이하게 하기 위해 높은 주석 품질을 보이는 OPPO-common이라는 내부 테스트 세트를 사용했습니다. 탐지 및 분할 데이터세트의 경우 널리 알려진 COCO[16] 및 ADE20k[31, 32] 데이터세트를 선택했습니다. 영어: 이러한 데이터 세트에서 우리는 경계 상자와 마스크를 무시하고 이미지 수준 태그 기준 진실인 의미 레이블에만 집중했습니다. ADE20k에는 주류 개념(예: &quot;버핏&quot;)에서 벗어나는 매우 작은 기준 진실 주석과 모호한 범주가 많이 포함되어 있다는 점에 유의하는 것이 중요합니다. 따라서 몇 가지 작은 대상과 모호한 범주를 제거하여 ADE20k-clean이라는 ADE20k의 하위 세트를 만들었습니다. 평가 지표. 모델의 성능을 평가하기 위해 다양한 평가 지표를 사용했습니다. 평균 평균 정밀도(mAP)는 절제 실험의 결과를 보고하고 다른 분류 모델과 비교하는 데 사용되었습니다. mAP를 사용할 수 없는 모델의 경우 정밀도/재현율 지표를 활용하고 다양한 모델의 임계값을 수동으로 조정하여 평가 간 비교 가능성을 보장했습니다. Туре 표 1. 테스트 벤치마크의 세부 정보. 데이터 세트 #범주 #이미지 OPPO-common44,Cls. OpenImages-common [14]57,OpenImages-rare [14]21,Det.COCO-80 [16]5,COCO-133 [16]5,Seg.ADE20k [31, 32] ADE20k-clean [31, 32]2,2,4.2.SOTA 모델과의 비교 다중 레이블 분류 모델과의 비교.표 2에서 볼 수 있듯이 다중 레이블 분류에서 RAM을 최첨단(SOTA) 모델과 비교합니다.일반적으로 일반주의 모델은 특정 도메인에 대한 전문성이 부족한 반면 전문가 모델은 생성에 어려움을 겪습니다.표 2. mAP의 분류 모델과의 비교.×로 표시된 셀은 이러한 설정에서 평가할 수 없음을 의미합니다.셀 배경색: 녹색은 완전 지도 학습을 의미합니다.파란색은 제로 샷 성능을 의미합니다.노란색은 모델이 해당 교육 이미지를 보았지만 주석은 보지 못했음을 나타냅니다. 특히, RAM의 OpenImages-common에 대한 제로샷 일반화는 ML-Decoder의 전체 감독보다 우수합니다. RAM은 훈련 중에 본 적이 없더라도 OpenImages-rare의 범주를 인식할 수도 있습니다. 다중 레이블 분류 감지 분할 방법 태그 OPPO OpenImages OpenImages ADE20k COCO-80 COCO-133 ADE20k -common -common -rare -clean ML-Decoder [23] MKT [8] 33.9M 82.4* 85.79.72.8* X ☑ 0.6M 78.77.63.62.51.37.38.Tag2Text-4M [10] 11.4M 83.82.☑ 78.3* 66.✓ X Tag2Text-14M [10] 33.6M 85.83.78.2* 67.1* ✓ ✓ RAM-4M 39.3M 85.86.66.79.68.51.53.RAM-14M 119.9M 86.86.69.80.69.55.56.† 모델에서 지원하지 않는 몇 가지 범주는 mAP를 계산할 때 제외됩니다. * 훈련 세트와 상위 10k 구문 분석 태그에서 동시에 발생하는 일반 태그의 총 수. 표 3. Precision/Recall에서 감지, 분할 및 시각 언어 모델과의 비교. *로 표시된 셀은 대규모 범주에서 성능이 좋지 않거나 높은 이미지 해상도(예: ODISE의 경우 1024)로 인해 추론 시간이 길다는 것을 의미합니다. 주목할 점은 RAM이 일반 범주에서 큰 마진으로 CLIP 및 BLIP보다 성능이 우수하다는 것입니다. 다중 레이블 분류 감지 분할 방법 백본 OPPO -common OpenImages OpenImages COCO-80 COCO-ADE20k -common -rare Grounding-DINO [17] ODISE [26] SEEM [33] Swin-B Diffusion-vFocalNet-L * * * 83.1/86.9 66.4/48.* * * ✓ ✓ 78.5/85.75.7/67.71.1/80.71.8/61.34.3 / 24.47.4/48.ADE20k -clean 35.6/26.48.2 / 50.х CLIP-400M [22] ViT-B BLIP-129M [15] ViT-B Tag2Text-4M [10] Swin-B Tag2Text-14M [10] 스윈-B 76.6/54.1 77.9/52.76.7/57.5 78.6/55.76.6/74.8 75.9/71.77.9/79.4 76.4/73.67.5 / 46.64.0/38.47.8 / 36.30.3/5.31.0/5.65.2/46.67.0/39.53.8/34.28.5/8.29.1/9.✓ RAM-4M RAM-14M Swin-B Swin-L 78.4/75.78.8/79.79.2/73.80.3/75.✓ 53.9/48.53.8/54.81.8/66.82.9 / 66.80.5/66.1* 80.164.5 71.2/53.2* 74.3/54.74.3/54.71.2/54.X ✓ 47.0/47.53.2/50.47.8 / 50.53.7 / 52. 모델에서 지원하지 않는 몇 가지 범주는 정밀도와 재현율을 계산할 때 제외됩니다. 전문 분야를 넘어 일반화하는 데 어려움을 겪습니다. 구체적으로, 지도 전문가 모델 ML-Decoder[23]는 지정된 전문 분야인 OpenImages에서 탁월하지만 다른 도메인과 보이지 않는 범주로 일반화하는 데 어려움을 겪습니다. MKT[8]는 CLIP에서 지식을 전달하여 태그를 지정하는 일반주의 모델로, 모든 도메인에서 만족스러운 정확도를 달성하지 못합니다. Tag2Text[10]는 제로샷 태그 지정에는 강력하지만 오픈셋 시나리오를 처리하는 기능이 부족합니다. RAM은 인상적인 태그 지정 능력을 보여주며, 인상적인 정확도와 광범위한 적용 범위를 보여줍니다. 특히 주목할 만한 것은 OpenImages-common 데이터 세트에서 ML-Decoder를 능가하는 RAM-4M의 성능입니다. ML-Decoder가 OpenImages의 주석이 달린 900만 개의 이미지에 의존하는 반면, 당사의 RAM-4M은 주석이 없는 400만 개의 이미지-텍스트 데이터로 구성된 학습 세트로 더 높은 정확도를 달성합니다. 이러한 개선은 400만 개의 이미지에서 파생된 3,900만 개의 공통 태그를 활용하여 900만 개의 이미지에서 3,390만 개의 공통 태그로만 학습한 ML-Decoder보다 우수한 성능을 발휘한 데 기인합니다. 게다가 RAM은 6,400개 이상의 널리 알려진 공통 카테고리를 광범위하게 활용하고, openvocabulary 기능을 결합하여 모든 공통 카테고리를 인식할 수 있습니다. 탐지 및 분할 모델과의 비교. 표 3의 비교는 감독 탐지 및 분할 모델이 제한된 수의 카테고리를 포함하는 COCO 데이터 세트와 같은 특정 도메인에서 탁월함을 보여줍니다. 그러나 이러한 모델은 더 많은 수의 범주를 인식하는 데 있어 어려움에 직면합니다. 한편으로는 더 복잡한 네트워크와 추가 지역화 작업을 위한 더 큰 입력 이미지 크기가 필요하기 때문에 훨씬 더 많은 계산 오버헤드가 발생합니다. 특히, ODISE[26]는 확산 모델을 채택하고 입력 이미지 해상도가 크기 때문에 추론 시간이 오래 걸립니다. 반면에 탐지 및 분할을 위한 훈련 데이터의 확장성이 제한되어 이러한 모델의 일반화 성능이 저하됩니다. Grounding-DINO[17]는 일반주의 모델로 사용되지만 만족스러운 결과를 얻는 데 어려움을 겪습니다.Table 4. Tag2Text 기준선에 기반한 RAM 모델의 절제 연구. &quot;보이는 범주&quot;는 훈련 범주의 수를 나타냅니다. &quot;캡션&quot;은 캡션 및 태그 작업의 공동 훈련을 나타냅니다. &quot;텍스트 쿼리&quot;는 텍스트 인코더를 사용하여 의미 정보를 보유한 레이블 쿼리를 생성하는 것을 말합니다. &quot;증류&quot;는 CLIP의 이미지 인코더를 사용하여 이미지 피처 증류를 말합니다. 사례 보기 범주 캡션 텍스트 쿼리 3, 태그2텍스트 3, (a) 3, (b) 3, (c) 6, OPPO OpenImages 증류 - 일반 - 일반 - 희귀 80.83.☑ 81.84.81.84.60.✓ 81.84.61.80.83.63.표 5. 데이터 엔진의 절제 연구. &quot;구문 분석&quot;은 캡션에서 구문 분석된 학습 태그를 의미합니다. &quot;생성&quot;은 캡션과 태그의 보완을 의미합니다. &quot;정리&quot;는 데이터 정리를 의미합니다. &quot;미세 조정&quot;은 COCO로 사전 학습된 모델을 미세 조정하는 것을 말합니다. 사전 학습 백본 파싱 #이미지 #태그 4M 12.0M 4M 41.7M Swin-Base 4M 39.8M 4M 39.8M 14M 121.5M 14M 121.5M 14M 121.5M Swin-Large 14M 121.5M 세대 청소 미세 조정 OPPO OpenImages -common -common -rare 80.83.63.82.84.67.82.84.66.85.86.66.83.85.68.86.86.68.83.84.68.86.69.대규모 범주에 대한 성능. 이와 대조적으로 RAM은 인상적인 오픈 세트 능력을 보여주며 기존 감지 기능을 능가합니다. 및 분할 모델.RAM은 더 광범위한 범주에서 일반화하는 기능을 보여주며, 기존 감지 및 분할 모델이 직면한 과제에 대한 강력한 솔루션을 제공합니다.시각-언어 모델과 비교.CLIP[22] 및 BLIP[15]의 오픈 세트 인식 기능에도 불구하고 이러한 모델은 정확도가 낮습니다.또한 이미지-텍스트 쌍의 밀집 임베딩에 대한 코사인 유사도 계산에 의존하기 때문에 해석 가능성이 제한적입니다.반대로 RAM은 CLIP 및 BLIP보다 상당한 마진으로 뛰어난 성능을 보이며 거의 모든 데이터 세트에서 정확도가 20% 이상 증가했습니다.그러나 RAM은 OpenImages-rare 데이터 세트의 경우 CLIP 및 BLIP보다 성능이 약간 떨어진다는 점에 주목할 가치가 있습니다.이 불일치는 RAM에 사용되는 더 작은 교육 데이터 세트와 교육 중에 드문 클래스에 대한 상대적으로 덜 중요한 강조점에 기인합니다.4.3. 모델 소거 연구 표 4에서는 Tag2Text [10] 기반 RAM에 대한 다양한 모델 개선의 영향을 연구하고 다음과 같은 주요 관찰 결과를 도출합니다. 1) 캡션과 태그의 교육 통합은 태그 기능을 촉진할 수 있습니다. 2) 오픈 세트 인식 기능은 CLIP [22]의 텍스트 쿼리를 통해 달성할 수 있지만 교육에서 보이는 범주에는 거의 영향을 미치지 않습니다. 3) 레이블 시스템의 확장은 기존 범주에 미치는 영향이 최소화되는데, 이는 추가 범주로 인해 모델 교육의 어려움이 증가하기 때문일 수 있습니다. 그러나 이 확장은 동시에 모델의 적용 범위를 강화하고 보이지 않는 범주의 오픈 세트 기능을 강화합니다. 4.4. 데이터 엔진 소거 연구 표 5에 데이터 엔진의 소거 연구를 제시합니다. 연구 결과는 다음과 같습니다. 1) 12.0M에서 41.7M으로 태그를 더 추가하면 모든 테스트 세트에서 모델 성능이 크게 향상되어 원본 데이터 세트에서 심각한 레이블 누락 문제가 있음을 나타냅니다. 2) 일부 범주의 태그를 추가로 정리하면 OPPO-common 및 OpenImages-common 테스트 세트에서 성능이 약간 향상됩니다. GroundingDino의 추론 속도에 제한을 받아 534개 범주에 대해서만 정리 프로세스를 수행합니다. 3) 훈련 이미지를 4M에서 14M으로 확장하면 모든 테스트 세트에서 현저한 개선이 이루어집니다. 4) 더 큰 백본 네트워크를 사용하면 OpenImages-rare에서 약간의 개선이 이루어지고 일반 범주에서는 성능이 약간 떨어집니다. 이 현상은 하이퍼 매개변수 검색을 수행하는 데 사용할 수 있는 리소스가 부족하기 때문입니다. 5) COCO Caption 데이터 세트[16]에서 구문 분석한 태그로 미세 조정하면 OPPO-common 및 OpenImages-common 테스트 세트에서 성능이 현저히 향상됩니다. COCO Caption 데이터 세트는 각 이미지에 대해 5개의 설명 문장을 제공하여 완전한 태그 레이블 세트에 근접하는 포괄적인 설명을 제공합니다. 5.
--- CONCLUSION ---
우리는 이미지 태그 지정을 위해 설계된 강력한 기반 모델인 Recognize Anything Model(RAM)을 제시합니다. 이는 이 분야에서 새로운 패러다임을 예고합니다. RAM은 모든 범주를 높은 정확도로 인식하는 제로샷 능력을 보여주며, 완전 지도 모델과 CLIP 및 BLIP과 같은 기존의 범용적 접근 방식의 성능을 모두 능가합니다. RAM은 컴퓨터 비전 분야에서 대규모 모델에 상당한 진전을 나타내며, 모든 시각적 작업이나 데이터 세트의 인식 기능을 강화할 수 있는 잠재력을 가지고 있습니다. 여전히 RAM을 더욱 개선할 여지가 있습니다. 예를 들어, 다양한 도메인을 더 잘 포괄하기 위해 1,400만 개 이상의 이미지로 학습 데이터 세트를 확장하고, 여러 라운드의 데이터 엔진을 사용하고, 백본 매개변수를 늘려 모델 용량을 향상시킵니다. 한계. CLIP과 유사하게 현재 버전의 RAM은 일반적인 객체와 장면을 효율적으로 인식하지만 객체 계산과 같은 추상적인 작업에는 어려움을 겪습니다. 게다가 제로샷 RAM의 성능은 자동차 모델을 구별하거나 특정 꽃이나 새 종을 식별하는 것과 같은 세분화된 분류에서 작업별 모델보다 뒤떨어집니다. 또한 RAM은 오픈 소스 데이터 세트에서 학습되었으며 잠재적으로 데이터 세트 편향을 반영할 수 있다는 점도 주목할 만합니다.참고 문헌 [1] Apple Developer. https://developer.apple.com/documentation/vision. [2] Google Cloud vision API. https://cloud.google.com/vision.service. https: [3] Microsoft Azure cognitive //azure.microsoft.com/zh-cn/products/cognitive-services/vision-services/. [4] David Arthur 및 Sergei Vassilvitskii. K-means++ 신중한 시딩의 장점. 이산 알고리즘에 대한 제18회 연례 ACM-SIAM 심포지엄 회의록, 1027~1035페이지, 2007. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell 등. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020. [6] Soravit Changpinyo, Piyush Sharma, Nan Ding 및 Radu Soricut. 개념적 12m: 긴 꼬리 시각적 개념을 인식하기 위한 웹 스케일 이미지-텍스트 사전 학습 추진. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 3558-3568페이지, 2021. [7] Christiane Fellbaum. WordNet: 전자 어휘 데이터베이스. Bradford Books, 1998. [8] Sunan He, Taian Guo, Tao Dai, Ruizhi Qiao, Bo Ren 및 Shu-Tao Xia. 다중 모달 지식 전달을 통한 개방형 어휘 다중 레이블 분류. CORR, abs/2207.01887, 2022. [9] Xinyu Huang, Youcai Zhang, Ying Cheng, Weiwei Tian, Ruiwei Zhao, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo 및 Xiaobo Zhang. 아이디어: 비전 언어 사전 훈련을 위한 온라인 다중 레이블 인식을 통해 텍스트 다양성을 높입니다. 제30회 멀티미디어에 관한 ACM 국제 컨퍼런스 진행, 페이지 4573–4583, 2022. [10] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo 및 Lei Zhang. Tag2text: 이미지 태깅을 통해 비전 언어 모델을 안내합니다. arXiv 사전 인쇄본 arXiv:2303.05657, 2023. [11] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, Tom Duerig. 노이즈가 많은 텍스트 감독을 통한 시각 및 시각 언어 표현 학습 확장. 기계 학습 국제 컨퍼런스, 4904-4916페이지. PMLR, 2021. [12] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick. 무엇이든 분할하세요. arXiv:2304.02643, 2023. [13] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma 등. 시각적 게놈: 크라우드소싱된 조밀한 이미지 주석을 사용하여 언어와 시각을 연결합니다. 컴퓨터 비전 국제 저널, 123:32-73, 2017. [14] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig 및 Vittorio Ferrari. 개방형 이미지 데이터 세트 v4: 대규모 통합 이미지 분류, 객체 감지 및 시각적 관계 감지. IJCV, 2020. [15] Junnan Li, Dongxu Li, Caiming Xiong 및 Steven Hoi. BLIP: 통합 비전-언어 이해 및 생성을 위한 언어-이미지 사전 학습 부트스트래핑. arXiv:2201.12086 [cs], 2022년 2월. arXiv: 2201.12086. [16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár 및 C Lawrence Zitnick. Microsoft coco: 컨텍스트의 공통 객체. Computer Vision-ECCV 2014: 제13회 유럽 컨퍼런스, 스위스 취리히, 2014년 9월 6-12일, 회의록, 5부 13, 740-755페이지. Springer, 2014. [17] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu 등. 접지 공룡: 오픈 세트 물체 감지를 위한 접지 사전 훈련과 공룡의 결합입니다. arXiv 사전 인쇄 arXiv:2303.05499, 2023. [18] Shilong Liu, Lei Zhang, Xiao Yang, Hang Su 및 Jun Zhu. Query2label: 다중 레이블 분류를 위한 간단한 변환기 방법입니다. arXiv 사전 인쇄 arXiv:2107.10834, 2021. [19] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin 및 Baining Guo. Swin 변환기: 이동된 창을 사용하는 계층적 비전 변환기입니다. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 논문집, 10012-10022쪽, 2021. [20] OpenAI. Gpt-4 기술 보고서, 2023. [21] Vicente Ordonez, Girish Kulkarni, Tamara Berg. Im2text: 캡션이 있는 100만 장의 사진을 사용하여 이미지 설명. 신경 정보 처리 시스템의 발전, 24, 2011. [22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스, 8748-8763쪽. PMLR, 2021. [23] Tal Ridnik, Gilad Sharir, Avi Ben-Cohen, Emanuel Ben Baruch, Asaf Noy. Ml-decoder: 확장 가능하고 다재다능한 분류 헤드. IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, 2023년 1월 2-7일, 32-41페이지. IEEE, 2023. [24] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, Xiaogang Wang, Yu Qiao. Internimage: 변형 가능한 합성곱을 사용하여 대규모 비전 기반 모델 탐색. CORR, abs/2211.05778, 2022. [25] Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun 및 Wei-Ying Ma. 통합된 시각-의미 임베딩: 구조화된 의미 표현을 통한 시각과 언어 연결. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6609-6618페이지, 2019. [26] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang 및 Shalini De Mello. 텍스트-이미지 확산 모델을 사용한 개방형 어휘 파노라마 분할. arXiv 사전 인쇄 arXiv:2303.04803, 2023. [27] Jianwei Yang, Chunyuan Li, Xiyang Dai 및 Jianfeng Gao. 초점 변조 네트워크. NeurIPS, 2022. [28] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang 및 Chen Change Loy. 조건부 일치를 통한 개방형 어휘 detr. Computer Vision-ECCV 2022: 제17차 유럽 컨퍼런스, 이스라엘 텔아비브, 2022년 10월 23~27일, 절차, 파트 IX, 106~122페이지. Springer, 2022. [29] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni 및 Heung-Yeung Shum. DINO: 엔드투엔드 개체 감지를 위해 개선된 노이즈 제거 앵커 박스를 갖춘 DETR입니다. CoRR, abs/2203.03605, 2022. [30] Youcai Zhang, Yuhao Cheng, Xinyu Huang, Fei Wen, Rui Feng, Yaqian Li, Yandong Guo. 누락된 레이블이 있는 다중 레이블 학습을 위한 간단하고 견고한 손실 설계. arXiv 사전 인쇄본 arXiv:2112.07368, 2021. [31] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, Antonio Torralba. ade20k 데이터 세트를 통한 장면 구문 분석. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 633-641페이지, 2017. [32] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, Antonio Torralba. ade20k 데이터 세트를 통한 장면의 의미적 이해. International Journal of Computer Vision, 127:302-321, 2019. [33] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, Yong Jae Lee. 모든 곳을 한꺼번에 분할합니다. CoRR, abs/2304.06718, 2023.
