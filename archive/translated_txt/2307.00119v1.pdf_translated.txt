--- ABSTRACT ---
대규모 언어 모델은 few-shot NLP 작업에서 인상적인 결과를 보여줍니다. 그러나 이러한 모델은 메모리와 계산 집약적입니다. 메타트레이닝을 사용하면 도메인 일반 및 작업에 독립적인 방식으로 few-shot 일반화를 위해 더 작은 모델을 활용할 수 있습니다(Min et al., 2022a; Wei et al., 2022; Chen et al., 2022). 그러나 이러한 방법만으로는 다양한 작업에 빠르게 적응할 수 있는 충분한 매개변수화 또는 지식이 없는 모델이 생성될 수 있습니다. 이 문제를 극복하기 위해 우리는 더 다양한 감독을 위해 각 예제와 의미적으로 유사한 레이블이 지정된 데모를 검색하는 데 밀집 구절 검색기를 사용하는 데모 검색을 사용한 메타트레이닝을 제안합니다. 외부 지식을 모델 매개변수에서 분리함으로써 메타트레이닝을 사용하여 더 다양한 작업에서 잘 일반화되는 매개변수 효율적인 모델을 학습할 수 있습니다. 우리는 UNIFIEDQA와 CROSSFIT에서 메타트레이닝 세트를 구성하고 UNIFIEDQA 작업을 기반으로 하는 데모 뱅크를 제안합니다. 저희가 아는 한, 저희의 작업은 메타 트레이닝과 검색을 결합하고, DPR 모델을 사용하여 데모를 검색하고, 대상 작업의 트레이닝 세트에서 데모를 무작위로 샘플링하는 대신 여러 작업의 데모를 동시에 활용하는 최초의 작업입니다. 저희의 접근 방식은 QA, NLI 및 텍스트 분류 작업(SQUAD, QNLI 및 TREC 포함)에서 다양한 대상 매개변수 효율적 및 검색 증강된 few-shot 방법보다 성능이 뛰어납니다. 저희의 접근 방식은 단일 GPU에서 메타 트레이닝되고 빠르게 미세 조정될 수 있습니다.
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 많은 NLP 작업과 도메인에서 인상적인 fewshot 성능으로 인해 점점 더 인기를 얻고 있습니다(Brown et al., 2020; Chowdhery et al., 2022). 이로 인해 점점 더 큰 GPU와 * Meta에서 인턴으로 수행한 작업 질문: 2차 세계 대전에서 싸운 미국인은 몇 명입니까? \n 답변: 16,000, \n 맥락: ... 메모리 뱅크(z) 검색된 데모({z}.....K) HotpotQA 질문: 1차 세계 대전에서 사상자는 몇 명이었습니까? \n 답변: 40,000, BART 출력(y) 질문: 1차 세계 대전에서 사상자는 몇 명이었습니까? 입력(x) \n 답변:<mask> \n 컨텍스트: NewsQA 메타 학습 작업 ... BoolQ 그림 1: 접근 방식. 가능한 많은 QA 작업 중 하나에서 입력 x가 주어지면, 우리는 밀집 구절 검색기를 사용하여 레이블이 지정된 예제로 구성된 메모리 뱅크 z에서 K개의 의미적으로 유사한 데모 Z = {k}1,...,k를 검색합니다. 우리는 BART를 메타 학습하여 다양한 QA 작업 컬렉션에서 x와 Z가 주어진 (질문과) 답변 y를 생성하도록 감독합니다. 계산 증가. 컨텍스트 내 학습(Brown et al., 2020)과 같은 매개변수 업데이트가 필요 없는 방법과 어댑터(Houlsby et al., 2019)와 같은 매개변수 효율적인 방법은 이러한 단점을 부분적으로 완화하지만 궁극적으로 최첨단 few-shot 성능을 달성하기 위해 더 큰 계산 예산이 점점 더 필요해지고 있습니다. 단순히 모델을 로드하고 추론을 수행하는 경우에도 마찬가지입니다. 메타 학습(Vilalta 및 Drissi, 2002; Finn 등, 2017) 및 메타 훈련(Min 등, 2022a)은 더 작은 언어 모델이 여러 작업 및 도메인에서 더 빠르고 더 강력한 few-shot 성능을 낼 수 있도록 하는 방법입니다. 그러나 더 작은 모델은 여러 도메인 및 작업에서 동시에 효과적인 일반화를 위한 충분한 지식을 저장하지 못할 수 있습니다. 검색은 이를 극복하는 한 가지 방법입니다. 언어 모델의 매개변수 지식을 외부 지식(검색 가능한 텍스트로 저장됨)에서 분리함으로써 언어 모델의 매개변수에 저장될 수 있는 것보다 훨씬 더 많은 정보를 활용할 수 있습니다. 예를 들어, 검색 증강 생성(RAG; Lewis et al., 2020) 및 검색 강화 변환기(RETRO; Borgeaud et al., 2022)는 메타 학습이나 메타 훈련을 수행하지 않고 리소스가 많은 지식 집약적 작업에 대해서만 평가하지만, 지식 집약적 NLP 작업에서 성능을 개선하기 위해 자연어 구절을 검색합니다. 따라서 우리는 few-shot 학습을 위한 데모를 활용하는 매개변수 효율적인 방법으로 데모 검색을 사용한 메타 훈련을 제안합니다. 우리는 메타 훈련 및 미세 조정 중에 각 훈련 및 테스트 예제에 대해 의미적으로 유사한 레이블이 지정된 데모를 검색합니다. 비교적 작은 시퀀스 대 시퀀스 모델(BART 대형, 440M 매개변수)에서 제안하는 접근 방식이 다양한 다운스트림 작업에서 빠르고 잘 일반화할 수 있음을 보여줍니다(표 1). 검색 증강 생성(RAG) 모델(Lewis et al., 2020)에서 영감을 얻어, 우리는 위키피디아 구절 대신 데모를 검색하기 위해 dense passage retriever(DPR; Karpukhin et al., 2020)를 사용합니다. 우리는 대부분의 현대적 작업(Min et al., 2022a; Brown et al., 2020; Gao et al., 2021)처럼 대상 작업의 훈련 세트에서 데모를 무작위로 샘플링하는 대신, 많은 기존 질의응답 작업(App. A)에서 컴파일된 크고 다양한 뱅크(§3.3)에서 의미적으로 유사한 데모를 검색합니다. 실험 결과, 우리 방법(§3)은 자연어 추론(NLI), 의역 감지, 추출 질의응답(§5)을 포함한 다양한 작업에서 맞춤형 효율적인 few-shot 베이스라인 및 기타 검색 증강 모델보다 성능이 우수합니다. 저희가 아는 한, 저희의 작업은 메타 훈련(또는 더 광범위하게는 멀티태스크 훈련)과 검색을 결합하고, DPR 모델을 사용하여 데모를 검색하고, 대상 작업의 훈련 세트에서 무작위 또는 k-최근접 데모를 검색하는 대신 여러 작업의 데모를 동시에 활용하는 최초의 작업입니다. 저희의 코드는 GitHub에서 제공됩니다.¹ 2
--- RELATED WORK ---
메타학습(Vilalta and Drissi, 2002; Finn et al., 2017)은
--- METHOD ---
s만으로는 다양한 작업에 빠르게 적응할 수 있는 충분한 매개변수화 또는 지식이 없는 모델이 생성됩니다. 이 문제를 해결하기 위해 우리는 데모 검색을 사용한 메타 학습을 제안합니다. 여기서 우리는 밀집 구절 검색기를 사용하여 각 예제에 대해 의미적으로 유사한 레이블이 지정된 데모를 검색하여 더욱 다양한 감독을 수행합니다. 외부 지식을 모델 매개변수에서 분리함으로써 메타 학습을 사용하여 더 다양한 작업에서 잘 일반화되는 매개변수 효율적인 모델을 학습할 수 있습니다. 우리는 UNIFIEDQA와 CROSSFIT에서 메타 학습 세트를 구성하고 UNIFIEDQA 작업을 기반으로 하는 데모 뱅크를 제안합니다. 우리의 지식에 따르면, 우리의 작업은 검색과 메타 학습을 결합하고, DPR 모델을 사용하여 데모를 검색하고, 대상 작업의 학습 세트에서 데모를 무작위로 샘플링하는 대신 여러 작업의 데모를 동시에 활용하는 최초의 작업입니다. 영어: 우리의 접근 방식은 QA, NLI 및 텍스트 분류 작업(SQUAD, QNLI 및 TREC 포함)에서 다양한 타겟 매개변수 효율적 및 검색 증강형 소수 샷 방법보다 성능이 뛰어납니다. 우리의 접근 방식은 단일 GPU에서 빠르게 메타 학습되고 미세 조정될 수 있습니다. 소개 대규모 언어 모델(LLM)은 많은 NLP 작업 및 도메인에서 인상적인 소수 샷 성능으로 인해 점점 더 인기를 얻고 있습니다(Brown et al., 2020; Chowdhery et al., 2022). 이로 인해 더 큰 GPU와 * Meta에서 인턴으로 수행한 작업 질문: 2차 세계 대전에서 몇 명의 미국인이 싸웠습니까? \n 답변: 16,000, \n 맥락: ... 메모리 뱅크(z) 검색된 데모({z}.....K) HotpotQA 질문: 1차 세계 대전에서 몇 명의 사상자가 있었습니까? \n 답변: 40,000, BART 출력(y) 질문: 1차 세계대전 사상자는 몇 명이었나요? 입력(x) \n 답변:<mask> \n 컨텍스트: NewsQA 메타 학습 작업 ... BoolQ 그림 1: 접근 방식. 가능한 많은 QA 작업 중 하나에서 입력 x가 주어지면, 우리는 밀집 구절 검색기를 사용하여 레이블이 지정된 예제로 구성된 메모리 뱅크 z에서 K개의 의미적으로 유사한 데모 Z = {k}1,...,k를 검색합니다. 우리는 BART를 메타 학습하여 다양한 QA 작업 컬렉션에서 x와 Z가 주어진 (질문과) 답변 y를 생성하도록 감독합니다. 계산 증가. 컨텍스트 내 학습(Brown et al., 2020)과 같은 매개변수 업데이트가 필요 없는 방법과 어댑터(Houlsby et al., 2019)와 같은 매개변수 효율적인 방법은 이러한 단점을 부분적으로 완화하지만 궁극적으로 최첨단 few-shot 성능을 달성하기 위해 더 큰 계산 예산이 점점 더 필요해지고 있습니다. 단순히 모델을 로드하고 추론을 수행하는 경우에도 마찬가지입니다. 메타 학습(Vilalta 및 Drissi, 2002; Finn 등, 2017) 및 메타 훈련(Min 등, 2022a)은 더 작은 언어 모델이 여러 작업 및 도메인에서 더 빠르고 더 강력한 few-shot 성능을 낼 수 있도록 하는 방법입니다. 그러나 더 작은 모델은 여러 도메인 및 작업에서 동시에 효과적인 일반화를 위한 충분한 지식을 저장하지 못할 수 있습니다. 검색은 이를 극복하는 한 가지 방법입니다. 언어 모델의 매개변수 지식을 외부 지식(검색 가능한 텍스트로 저장됨)에서 분리함으로써 언어 모델의 매개변수에 저장될 수 있는 것보다 훨씬 더 많은 정보를 활용할 수 있습니다. 예를 들어, 검색 증강 생성(RAG; Lewis et al., 2020) 및 검색 강화 변환기(RETRO; Borgeaud et al., 2022)는 메타 학습이나 메타 훈련을 수행하지 않고 리소스가 많은 지식 집약적 작업에 대해서만 평가하지만, 지식 집약적 NLP 작업에서 성능을 개선하기 위해 자연어 구절을 검색합니다. 따라서 우리는 few-shot 학습을 위한 데모를 활용하는 매개변수 효율적인 방법으로 데모 검색을 사용한 메타 훈련을 제안합니다. 우리는 메타 훈련 및 미세 조정 중에 각 훈련 및 테스트 예제에 대해 의미적으로 유사한 레이블이 지정된 데모를 검색합니다. 비교적 작은 시퀀스 대 시퀀스 모델(BART 대형, 440M 매개변수)에서 제안하는 접근 방식이 다양한 다운스트림 작업에서 빠르고 잘 일반화할 수 있음을 보여줍니다(표 1). 검색 증강 생성(RAG) 모델(Lewis et al., 2020)에서 영감을 얻어, 우리는 위키피디아 구절 대신 데모를 검색하기 위해 dense passage retriever(DPR; Karpukhin et al., 2020)를 사용합니다. 우리는 대부분의 현대적 작업(Min et al., 2022a; Brown et al., 2020; Gao et al., 2021)처럼 대상 작업의 훈련 세트에서 데모를 무작위로 샘플링하는 대신, 많은 기존 질의응답 과제(App. A)에서 컴파일된 크고 다양한 뱅크(§3.3)에서 의미적으로 유사한 데모를 검색합니다.
--- EXPERIMENT ---
s는 우리의 방법(§3)이 자연어 추론(NLI), 의역 감지, 추출적 질문 답변(§5)을 포함한 다양한 작업에서 맞춤형 효율적 소수 샷 베이스라인 및 기타 검색 증강 모델보다 성능이 우수함을 보여줍니다.우리의 지식에 따르면, 우리의 작업은 검색을 메타 학습(또는 더 광범위하게는 멀티태스크 학습)과 결합하고, DPR 모델을 사용하여 데모를 검색하고, 대상 작업의 학습 세트에서 무작위 또는 k-최근접 데모를 검색하는 대신 여러 작업의 데모를 동시에 활용하는 최초의 작업입니다.우리의 코드는 GitHub에서 사용할 수 있습니다.¹ 2 관련 작업 메타 학습(Vilalta 및 Drissi, 2002; Finn et al., 2017)은 학습 방법에 대한 모델을 감독하는 방법의 한 종류입니다.목표는 메타 학습 작업 모음을 활용하여 보류된 작업으로 일반화되는 더 나은 학습 알고리즘을 학습하는 것입니다. 메타 학습에서 영감을 얻은 최근의 일부 연구 https://github.com/facebookresearch/ metatrained-demRAG는 메타 훈련을 통해 작업 및 도메인에 독립적인 방식으로 언어 모델에서 특정 능력을 유도하려고 시도했습니다. 이는 다양한 작업에서 레이블이 지정된 예제에 대한 모델을 직접 감독하는 것을 수반합니다(때로는 일부 제어된 형식이나 템플릿을 사용(Chen et al., 2022; Wei et al., 2022)). 이를 통해 일반화를 개선하는 특정 능력이나 더 나은 귀납적 편향을 직접 유도합니다. 메타 훈련은 일반적으로 Min et al.(2022a)에서와 같이 일종의 제어된 멀티태스크 학습을 통해 달성됩니다. 많은 연구에서 다중 작업 및 다중 도메인 학습을 탐구했지만(Khashabi et al., 2020; Zhong et al., 2021; Aghajanyan et al., 2021; Ye et al., 2021; Wei et al., 2022), 이러한 연구는 종종 특정(집합) 다운스트림 작업에 대한 모델의 능력을 향상시키는 작업을 활용합니다. 메타 학습에서 우리는 통제된 감독을 통해 학습 알고리즘을 직접 개선하는 것을 목표로 하며, 이는 모델에 컨텍스트 내 학습과 같은 유용한 능력을 가르쳐 다양한 다운스트림 작업에서 이득을 얻을 수 있도록 함으로써 분포 외부 일반화를 개선해야 합니다(Min et al., 2022a). 우리는 QA 데이터 세트의 예를 사용하여 메타 학습에 중점을 둡니다. 소수 샷 학습은 모델이 몇 개의 레이블이 지정된 예제에서만 감독되는 일반적인 설정입니다. fewshot 성능을 개선하기 위한 많은 방법은 확장 모델과 데이터 크기에 기반을 둡니다(Brown et al., 2020; Chowdhery et al., 2022). 저희의 목표는 계산 및 메모리 효율적인 방식으로 작업 전반에 걸쳐 few-shot 성능을 개선하는 것이므로 단일 GPU에서 효율적으로 학습할 수 있는 더 작은 모델에 집중합니다. cloze 스타일 프롬프트(Schick and Schütze, 2021b), 수동으로 조정된(Schick and Schütze, 2021a) 및 자동으로 조정된 프롬프트와 데모(Gao et al., 2021)를 사용한 미세 조정, 메타 학습(Yu et al., 2018; Bansal et al., 2020; Bao et al., 2020)을 포함하여 일부 매개변수 효율적인 few-shot 방법이 제안되었습니다. 우리 접근 방식의 한 가지 장점은 상당한 신속한 튜닝이 필요하지 않다는 것입니다. 오히려 Chada와 Natarajan(2021)과 유사하게 모든 작업을 단일 형식으로 표준화합니다. 이를 통해 인적 시간과 계산 리소스를 절약할 수 있습니다. 중요한 점은 이러한 접근 방식이 단일 토큰이나 미리 선택된 작은 레이블 세트의 확률을 비교한다는 것입니다. 따라서 질문 답변과 같은 오픈 도메인 작업에는 사용할 수 없습니다. 일부 연구에서는 오픈 도메인 작업을 위한 생성적 few-shot 방법을 제안했습니다. 여기에는 모델의 사전 학습 형식과 일치하도록 입력 데이터를 다시 포맷하고(Chada와 Natarajan, 2021), 맥락 구절에서 관련 범위를 선택하기 위한 모델 사전 학습(Ram et al., 2021), 레이블이 지정된 분류 데이터에서 보조 사전 학습 단계를 실행하는 것(Mueller et al., 2022)이 포함됩니다. 우리 모델은 레이블 공간이 크고 예제마다 다르더라도 많은 작업에서 효과적이어야 합니다. 따라서 우리 방법은 생성적 시퀀스-투-시퀀스 모델을 기반으로 합니다. 영어: In-context learning(ICL; Brown et al., 2020)은 few-shot 방법에서 점점 더 많이 사용되고 있습니다.여기서 레이블이 지정된 데모는 테스트 예제와 동일한 컨텍스트에 연결되어 모델에 추가 그래디언트 업데이트 없이 작업을 수행하는 방법을 가르칩니다.연구에서는 어떤 종류의 데모가 가장 효과적인지(Liu et al., 2022)와 데모를 효과적으로 만드는 요인(Min et al., 2022b; Xie et al., 2022)을 분석했습니다.저희의 데모 검색 접근 방식은 Liu et al.(2022)과 가장 유사합니다.Liu et al.은 데모와 테스트 예제를 문장 임베딩 공간에 인코딩하고 가장 가까운 데모를 검색합니다.저희 방법은 여러 가지 면에서 다릅니다.문장 임베딩 대신 밀집 구절 검색기를 사용합니다.대상 작업의 훈련 세트 대신 여러 훈련 세트의 데모를 사용합니다.그리고 데모와 함께 그래디언트 업데이트를 수행하는데, 이는 비교적 작은 BART 대규모 모델에서 더 실행 가능합니다.Wei et al. (2022)는 ICL이 효과적이려면 매우 큰 LM(&gt;68B 매개변수)이 필요하지만, Min et al. (2022a)은 메타 훈련을 사용하면 데모를 활용할 수 있는 훨씬 더 작은 모델(GPT2large, 774M 매개변수)을 만들 수 있다는 것을 발견했습니다. 여기서 우리는 Min et al. (2022a)과 같이 데모를 사용한 메타 훈련을 통해 BART를 더 크게(440M 매개변수) 만들지만, 그들의 방법은 제로샷 일반화를 위해 설계되었으며, 미리 정의된 레이블의 제한된 집합에서 선택합니다. 우리의 방법은 소수샷 설정을 위해 설계되었으며 오픈 도메인 작업에 적용할 수 있습니다. 검색 증강 생성 모델은 생성기와 검색기라는 두 가지 구성 요소로 구성됩니다. 생성기는 일반적으로 디코더 전용 LM(Guu et al., 2020) 또는 시퀀스 대 시퀀스(seq2seq) 모델(Lewis et al., 2020; Izacard and Grave, 2021)입니다. 저희는 seq2seq 모델을 사용합니다. 검색기는 대부분 BERT 기반을 기반으로 하는 고밀도 구절 검색(DPR; Karpukhin et al., 2020) 모델입니다. RAG 모델은 일반적으로 추상적 QA 및 사실 검증과 같은 지식 집약적 작업에서 평가됩니다. 따라서 메모리 뱅크는 일반적으로 생성기의 매개변수와 별도로 추가적인 사실적 지식으로 모델을 보강하는 Wikipedia 구절로 구성됩니다. Izacard et al.(2022)은 매우 큰 생성기(T5X(X)L)와 Contriever 기반(Izacard et al., 2021) 검색기를 사용하여 이 아키텍처를 소수 샷 지식 집약적 작업에 적용합니다. 그러나 우리는 보다 일반적인 용도의 방법과 단일 GPU에서 빠르게 학습하거나 미세 조정할 수 있는 매개변수 및 메모리 효율적인 방법에 관심이 있습니다.따라서 우리는 소수 샷 설정을 위한 더 작은 생성 모델을 개선하기 위해 작업에 구애받지 않고 도메인에 구애받지 않는 방법을 제안합니다.특히 검색 증강 메타 학습 단계와 Wikipedia 구절 대신 레이블이 지정된 QA 데모의 메모리 뱅크입니다.3 방법 3. 검색 증강 생성 모든 입력에 대해 유사한 레이블이 지정된 예제를 검색하고자 하므로 아키텍처는 검색 증강 생성(RAG) 모델(Lewis et al., 2020)에서 영감을 얻었으며, 이는 사전 학습된 시퀀스 간 구성 요소(BART 대형 사용)와 사전 학습된 밀집 구절 검색기(DPR) 구성 요소로 구성됩니다.입력 x가 주어지면 DPR 구성 요소는 메모리 뱅크 z에서 가장 의미적으로 유사한 K개의 메모리 항목 {k}1,...,K를 검색합니다. 검색은 x의 BERT 기반 입력 인코더 Ej와 z의 BERT 기반 데모 인코더 ED를 사용하여 두 가지를 모두 벡터 공간으로 인코딩한 다음 최대 내적 검색을 실행합니다.² {zk},……..K = top-K {E1(x)˜ED(z))} (1) zЄz DPR 구성 요소는 또한 내적 자체를 문서 점수 p₁(zk|x)로 반환합니다. 그런 다음 입력 및 검색된 항목은 자기 회귀 생성을 위해 사전 학습된 시퀀스 대 시퀀스 모델인 BART 대형으로 전달됩니다. 각 시간 단계에서 입력 x와 하나의 검색된 항목 Zk로 구성된 K개의 별도 입력 컨텍스트를 생성하여 검색된 데모를 주변화합니다. 그런 다음 Zk의 문서로 가중치를 둔 각 컨텍스트가 주어진 BART의 토큰 확률 pe를 합산합니다. 2최대 내적 검색은 대략적으로 준선형 시간 내에 풀 수 있습니다(Johnson et al., 2021). 이를 위해 faiss 라이브러리를 사용하세요: https://github.com/ facebookresearch/faiss 카테고리 데이터셋 튜브 #학습 #테스트 L | | Extractive QA SQUAD (Rajpurkar et al., 2016) BioASQ (Tsatsaronis et al., 2015) QASC (Khot et al., 2020) Open QA 86,588 10,507 10/Open QA 객관식 QA 24,8,1,504 10/지식집약적 QA 분류 TriviaQA (Joshi 등, 2017) TextbookQA (Kembhavi 등, 2017) TREC (Voorhees and Tice, 2000) MRPC (Dolan and Brockett, 2005) MNLI (Williams 등, 2018) MNLI-mm (ibid.) 공개 QA 공개 QA 926 8/61,688 7,785 13/15,154 1,503 10/질문 분류.의역 분류.NLI QNLI(Wang et al., 2018) NLI NLI 5,3,392,702 9,815 22/392,702 9,832 22/11104,743 5,463 11/3022/표 1: 이 연구에 사용된 평가 세트.L: 질문/맥락 또는 입력 문장의 단어 수 평균.이전의 few-shot 질문 답변 및 분류 방법과 더 직접적으로 비교하기 위해 MRQA에서 파생된 Ram et al.(2021)의 SQUAD 및 BioASQ few-shot 분할과 Gao et al.(2021)의 TREC, MRPC, MNLI(-mm), QNLI 분할을 사용했습니다. 각 분할 크기에 대해 5개의 난수 시드를 사용하여 QASC의 몇 번의 샷 분할을 생성합니다.점수: NKP(y|x) = P(zk|x)po(y|x, zk, Y1:i-1) (2) ik 3.2 메타 학습 일반적인 용도의 데모 검색 및 답변 생성을 위한 시퀀스 대 시퀀스 모델을 적용하기 위해 18개의 QA 작업(표 7) 컬렉션에 대한 데모로 모델을 감독하여 메타 학습 단계를 수행합니다.질문과 검색된 데모 세트가 주어지면 BART를 감독하여(정상적인 교차 엔트로피 손실 사용) 질문과 답변을 생성하여 메타 학습 중에 모델의 BART 구성 요소의 매개변수를 업데이트합니다.입력과 레이블의 의미적 다양성으로 인해 QA 작업을 사용합니다.레이블 공간이 훨씬 작고 레이블이 덜 유익한 경우가 많은 텍스트 분류 작업과 비교합니다. 우리는 (Min et al., 2022a)의 QA 메타-트레이닝 작업 컬렉션을 수정하여 사용합니다. 여기에는 CROSSFIT의 다양한 추출, 객관식 및/또는 추상화 QA 작업과 NaturalQuestions, MCTest, BIOMRC 등을 포함한 UNIFIEDQA의 하위 샘플(Khashabi et al., 2020, 2022)이 포함됩니다. 우리는 (1) 평가 세트가 있는 경우 제거하고 (2) 각 3의 형식을 표준화하여 메타-트레이닝 컬렉션을 수정합니다. 이는 Lewis et al. (2020)의 RAG-Token 접근 방식과 유사합니다. 사용할 수 있는 데모 수는 각 데모를 별도의 컨텍스트에서 주변화하기 때문에 컨텍스트 길이에 의해 제한되지 않습니다. 4 이 연구 전체에서 &quot;작업&quot;은 SQUAD 또는 NaturalQuestions와 같은 단일 데이터 세트를 나타내고 &quot;컬렉션&quot;은 작업 세트를 연결하여 얻은 데이터 세트를 나타냅니다. &gt;또한 질문이 모든 훈련 또는 테스트 질문 과제와 Jaccard 유사도 &gt; 0.9인 모든 예를 제거합니다. 최종 메타 훈련 컬렉션에는 32개의 과제가 포함되어 있으며, 평가 과제와의 의미적 유사도에 따라 18개의 과제로 하위 샘플링합니다. 과제의 전체 목록과 의미적 하위 샘플링 절차에 대한 자세한 내용은 부록 A를 참조하고, 의미적 하위 샘플링의 다운스트림 효과에 대한 설명은 §5.2를 참조하십시오. Chada와 Natarajan(2021)에 따라 메타 학습 데이터의 각 입력을 &quot;질문:... \n 답변: [MASK] \n 맥락:...&quot; 형식으로 표준화합니다. 그런 다음 출력 시퀀스는 질문과 답변 시퀀스로 구성되는데, 이는 전체 입력 시퀀스(단순히 마스크된 스팬이 아님)를 재구성한다는 BART의 사전 학습 목표와 일치합니다. Chada와 Natarajan(2021)과 마찬가지로 입력/출력 형식을 BART의 사전 학습 목표와 일치시키면 다운스트림 성능에 긍정적인 차이가 생긴다는 것을 발견했습니다. 객관식 QA 작업인 QASC의 경우 두 맥락 문장 앞의 맥락 필드에 모든 답변 옵션을 넣고 전체 답변 문자열을 생성합니다. 이는 시도한 다른 모든 형식보다 상당한 차이로 성능이 우수했습니다. 분류 작업의 경우 동일한 질문/답변/맥락 형식을 사용합니다. 단일 문장 분류 작업(TREC)의 경우 질문 필드에 입력을 넣고 비슷한 형식을 사용하여 맥락 필드에 모든 가능한 레이블을 표시합니다. QASC의 경우. 문장 쌍 분류의 경우 평가 과제이며 답변이 동일한 경우; 데이터에 그러한 예가 4개만 있습니다. &quot;답변 시퀀스에서만 F₁를 계산합니다. &quot;답변 옵션을 질문 필드에 배치하고 답변 옵션은 전혀 포함하지 않고 전체 답변 문자열 대신 문자 레이블만 생성해 보았습니다. 예와 점수는 부록 B를 참조하십시오. MRPC, MNLI(-mm), QNLI와 같은 다른 과제의 경우, 첫 번째 문장 또는 가설을 질문 필드에 배치하고 두 번째 문장 또는 전제를 맥락 필드에 배치합니다. QA 과제와 마찬가지로 대상 시퀀스에서 질문과 답변 필드를 모두 생성하지만 답변 시퀀스에서만 F₁를 평가합니다. 3. 데모 메모리 데모 메모리 뱅크의 경우 평가 과제를 제외한 UNIFIEDQA의 교육 세트를 사용합니다. 메모리에는 16개 과제의 예가 포함되어 있습니다. UnifiedQA는 QA 메타 교육 컬렉션과 약 40%가 겹치고 비 QA 컬렉션과는 겹치지 않습니다. 부록 A의 표에서 데모 메모리 뱅크의 작업 전체 목록을 참조하세요. 메모리 뱅크의 각 데모를 위에서 설명한 것과 동일한 질문/답변/컨텍스트 형식으로 포맷하지만, 데모에는 [MASK] 토큰 대신 answer: 헤더 뒤에 기준 진실 레이블이 있습니다. 메모리 항목은 텍스트 구절(데모)과 제목으로 구성되며, 제목의 경우 질문에 대한 답을 사용합니다. 4 실험 설정 다양한 QA 및 분류 작업(표 1)에서 평가합니다. 다양한 추출형 QA 형식을 반영하기 위해 MRQA 공유 작업(Fisch et al., 2019)에서 오픈 도메인 QA 작업을 선택했습니다. 여기에는 표준 QA 벤치마크(SQUAD), 도메인별 도전적 벤치마크(BioASQ), 두 가지 지식 집약적 QA 벤치마크(TriviaQA 및 TextbookQA)가 포함됩니다. 이러한 작업에 대한 크기 {16, 32, 64, 128}의 few-shot QA 분할은 Ram et al.에서 가져왔습니다. (2021), MRQA(Fisch et al., 2019)에서 파생되었습니다. 또한 다중선택 QA 과제인 QASC에 대한 few-shot 분할을 생성합니다. 우리는 우리 모델이 훨씬 더 짧은 맥락을 처리하는 데도 효과적인지 여부를 확인하고 보다 일반적인 MRQA 스타일 추출 과제에 과적합되지 않는지 확인하기 위해 QASC에서 평가합니다. 우리의 few-shot 분류 과제 분할은 Gao et al. (2021)에서 가져왔습니다. 우리는 문장 쌍 8에서 평가합니다.&quot;지식 집약적&quot;에는 표준 정의나 간단한 측정이 없지만 맥락의 길이는 질문 답변 과제가 얼마나 지식 집약적인지에 대한 대리 역할을 할 수 있습니다. 지식 집약적 과제에 대한 맥락은 훨씬 더 길기 때문에 모델이 훨씬 더 많은 정보를 합성하거나 입력과 더 관련성이 높은 정보를 검색하여 질문별 정보에 대한 의미론적 프라이밍을 해야 합니다. 메타 훈련이나 데모 과제에 포함되지 않은 분류 과제; 자연어 추론(NLI) 및 의역 분류와 같은 문장 쌍 분류 작업은 질문/답변/맥락 형식으로 쉽게 재구성할 수 있습니다. 또한 TREC에서 평가합니다. TREC는 모델이 답변 자체가 아닌 질문에 대한 답변의 범주(예: 인간, 위치, 숫자)를 추측해야 하는 단일 문장 텍스트 분류 작업입니다. 각 작업 및 few-shot 분할 크기에 대해 5개의 무작위 few-shot 샘플에 대한 평균 점수를 계산합니다. 4. 기준선 강력하고 효율적인 few-shot 방법과 유사한 모델과 비교하여 이 방법이 더 나은 성과를 보이는 이유를 알려줍니다. iPET 및 LM-BFF와 달리 이 접근 방식은 생성적이므로 더 다양한 작업에 사용할 수 있습니다. FewshotQA(Chada 및 Natarajan, 2021). few-shot 질의응답 방법입니다. 이 모델과 유사한 BART large를 기반으로 하며 가장 성능이 좋은 변형인 FewshotBARTL 모델과 비교합니다. 우리는 해당 논문에서 보고된 숫자와 직접 비교할 수 있도록 동일한 few-shot 분할을 사용합니다. 또한 본질적으로 검색 없이 사용하는 방법인 이 검색 증강되지 않은 모델을 메타 학습해 봅니다. 이 기준선을 FewshotQA-m이라고 합니다. Splinter(Ram et al., 2021). 맥락 구절에서 중요한 범위를 선택하도록 사전 학습된 few-shot 질문 답변 모델. RAG(Lewis et al., 2020). Wikipedia 구절의 메모리를 사용하는 원래 RAGToken 모델. 우리는 NaturalQuestions(NQ)에서 미세 조정된 출시된 모델을 사용합니다. 이것이 우리 작업에서 가장 성능이 좋은 RAG 모델이었기 때문입니다. 메타 학습 시 데모 메모리가 Wikipedia 구절보다 효과적인지 확인하기 위해 Wikipedia 메모리를 사용하여 RAG 모델을 메타 학습해 봅니다. 이 기준선을 RAG-m이라고 합니다. iPET(Schick and Schütze, 2021b). GPT3보다 훨씬 작은 LM으로 더 나은 fewshot 성능을 유도하는 수동 프롬프트 튜닝 방식입니다. 우리는 우리의 과제에서 가장 성능이 좋은 ALBERT xxl(Lan et al., 2020) 모델을 튜닝합니다. LM-BFF(Gao et al., 2021). ROBERTa large(Liu et al., 2019)를 기반으로 하는 자동 프롬프트 튜닝 방식입니다. iPET과 달리 잘 작동하려면 레이블이 지정되지 않은 텍스트 데이터가 필요하지 않습니다. 이 모델과 iPET은 토큰 확률을 비교하여 분류를 수행합니다. FSSS SSSplinter FewshotQA FewshotQA-m RAG RAG-m Ours SQUAD BioASQ QASC# 예제 # 예제 # 예제그림 2: 추출 및 객관식 질문 답변 평가 과제에 대한 각 few-shot 분할 크기에서의 F₁ 점수. 점수는 5개의 무작위 few-shot 샘플에 걸쳐 평균화됩니다. 우리의 모델은 각 과제와 분할 크기에서 가장 강력한 기준선과 유사한 성능을 유지하거나 능가합니다. SQUAD의 성능 향상은 특히 3.F₁(4.9% 개선)까지 큽니다. FewshotQA와 Splinter 점수는 Chada와 Natarajan(2021)에서 가져온 것입니다. tion에 따라 달라지므로 질문 답변과 같은 오픈 도메인 작업에는 사용할 수 없습니다. 따라서 분류에 대해서만 이러한 모델과 비교합니다. 4. 하이퍼파라미터 메타 학습의 경우 가능한 경우 Min et al.(2022a)의 하이퍼파라미터를 사용합니다. init. LR 1×10-5, 효과적인 배치 크기 8,9 최대 30,000단계 학습. 2,단계마다 체크포인트를 만들고 16샷 QA 학습 세트에서 평균 손실이 가장 낮은 체크포인트를 선택합니다. 메타 학습은 1개의 A100 GPU(40GB)에서 약 14시간 만에 완료됩니다. 미세 조정의 경우 가능한 경우 Chada와 Natarajan(2021)의 하이퍼파라미터를 사용합니다. init. LR 2 × 10-5, 배치 크기 4, 최대 1,000단계 또는 35에포크(둘 중 더 큰 쪽)에 대한 미세 조정. 2에포크마다 체크포인트를 만들고 학습 세트에서 가장 정확한 일치가 높은 체크포인트를 선택합니다. 미세 조정은 1개의 A100 GPU(40GB)에서 30~60분 안에 완료됩니다. 각 메타 학습 및 미세 조정 입력에 대해 메모리에서 5개의 데모를 검색합니다. 5개의 결과 추출형 질문 답변에 대한 모델의 F₁ 점수(그림 2)는 동일한 학습 데이터를 사용하여 메타 학습된 유사한 모델을 포함하여 유사한 매개변수화 모델보다 높습니다. 영어: 우리 모델은 또한 강력한 클래스보다 성능이 뛰어납니다.&quot;우리는 기울기 축적을 사용하여 단일 GPU에서 이 효과적인 배치 크기를 얻습니다. 10 우리 모델은 비슷한 시간 안에 더 저렴한 32GB GPU(예: V100)에서도 학습하고 조정할 수 있습니다.&quot;값이 높을수록 성능이 더 좋지만(§5.2), 이는 검색된 데모가 5~10개에서 포화 상태가 되고 더 많은 데모를 검색하면 학습 속도가 느려집니다. 대다수 TREC MNLI MNLI-mm 18.32.33.ROBERTa iPET *88.82.1 *45.86.*85.04.1 71.21.LM-BFF *89.41.7 70.71.FewshotQA 91.02.0 *47.96.FewshotQA-m 92.41.4 *50.11.*81.12.0 *62.40.*87.81.7 *70.01.RAG RAG-m 당사 91.71.3 72.91.QNLI MRPC Avg. 49.81.2 43.*47.86.8 *60.26.5 76.62.5 63.71.82.6 *70.36.2 70.44.7 73.*72.01.2 *69.21.9 *78.13.4 75.*46.15.9 61.06.4 *67.64.8 62.*50.62.5 *71.82.1 74.03.7 67.*61.81.2 *74.91.70.23.3 70.69.114 83.21.5 74.92.8 77.69.61.4 84.41.8 73.42.5 78.표 2: 분류 작업의 정확도, 5개의 무작위 few-shot 샘플에 걸쳐 평균화(하위 첨자의 std. dev). MRPC를 제외한 모든 데이터 세트는 균형이 잘 잡혀 있으므로 MRPC를 제외한 모든 작업에 대한 정확도를 보고하며, MRPC에서는 매크로-F₁를 보고합니다. LM-BFF 및 ROBERTa 점수는 Gao et al. (2021)에서 가져왔습니다. *는 모델의 점수와 표시된 점수 간의 t-검정에서 p &lt; .05임을 나타냅니다. TREC, MNLI 및 QNLI에 대한 sification 접근 방식(표 2). 따라서 의미적으로 유사한 데모를 사용한 메타 학습은 다양한 저자원 다운스트림 작업에서 우수한 성능을 발휘할 수 있는 보다 일반적인 용도의 시스템을 유도합니다. 이를 작업 전반에 걸쳐 테스트하는 각 모델 중에서 종종 가장 나쁜 성능을 보이는 RAG와 대조해 보세요. 따라서 아키텍처 자체는 few-shot 설정에서 본질적으로 강력하지 않아 메타 학습이 성능 향상에 상당한 기여를 한다는 것을 시사합니다. 이는 메타 훈련 후 FewshotQA와 RAG에서 관찰되는 성능 향상에서도 뒷받침되지만, 메타 훈련은 검색 증강 모델에 도움이 되는 것과 같은 정도로 FewshotQA에 도움이 되지 않는다는 점에 유의하십시오. 또한 FewshotQA는 분류 작업에서 좋은 성능을 보이지 않는 반면, 저희 방법은 가장 강력한 기준선을 초과하거나 그에 가까운 성능을 달성한다는 점에 유의하십시오. 이는 메타 학습과 검색을 결합하면 Splinter FFewshotQA FewshotQA-m RAG RAG-m -Ours Model SQUAD BioASQ QASC TriviaQA TbQA TriviaQA FewshotQA 68.63.82.65.2 37.FewshotQA-m 76.63.85.65.9 38.RAG-m Ours 80.62.9 88.66.6 27.83.64.89.62.9 37.Ours (oracle) 93.94.99.80.7 83.# Examples TextbookQA# ExamplesFigure 3: 지식 집약적 질문 답변 과제에 대한 각 few-shot 분할 크기에 대한 F₁ 점수. 저희 모델은 강력한 few-shot QA 기준선에 밀려 성능이 떨어지지만, 메타 학습은 여전히 성능을 크게 향상시킵니다. 이러한 구성 요소 중 하나를 별도로 사용합니다. 메타 훈련을 통해 RAG-m은 우리 모델에 훨씬 더 가까운 성능을 얻습니다. 이는 메타 훈련이 우리가 관찰하는 성능 향상의 대부분을 담당하지만 데모 메모리 뱅크도 성능을 약간 개선한다는 것을 알려줍니다. MRPC에서 RAG-m은 우리 모델보다 성능이 뛰어나 Wikipedia 구절이 QA 데모보다 더 유용한 비지식 집약적 작업이 있음을 나타냅니다. 5. 지식 집약적 QA 또한 few-shot 지식 집약적 QA 작업(그림 3)에 대해서도 평가합니다. 여기서는 MRQA 공유 작업의 few-shot 분할을 사용하여 TriviaQA와 TextbookQA를 사용합니다. 이러한 작업도 기술적으로 추출 QA 작업이지만 컨텍스트의 평균 길이는 각각 677개와 581개 단어로, BART가 이러한 작업의 모든 정보를 종합하는 데 더 어려움을 겪을 가능성이 있음을 의미합니다(검색을 포함해서). 우리는 FewshotQA가 이 두 작업 모두에서 우리 방법보다 성능이 뛰어나고, TextbookQA의 경우 더 큰 분할 크기에서 Splinter조차도 우리 방법보다 성능이 뛰어나다는 것을 발견했습니다. 즉, 가장 강력한 기준선, 우리 접근 방식, 메모리가 레이블이 지정된 테스트 예제(오라클)로 대체된 우리 접근 방식에 대한 데모 검색 표 3: QA 작업에서 F₁ 점수. 오라클 접근 방식은 우리 모델에 대한 대략적인 상한을 설정합니다. 우리 접근 방식과 오라클 간의 큰 차이는 메모리 뱅크를 구성하는 것에서 개선의 여지가 있음을 나타냅니다. 이러한 작업에 적극적으로 해로울 수 있습니다. 따라서 우리의 메타 학습 방법은 지식 집약적이지 않은 작업에 대해 RAG 아키텍처를 최적화하지만 지식 집약적 작업에는 최적화하지 않습니다. RAG-m이 우리 접근 방식보다 성능이 뛰어나다는 것에서 알 수 있듯이 Wikipedia 구절은 TriviaQA의 메모리 뱅크에서 데모보다 효과적입니다. 그러나 메모리 뱅크가 있거나 없는 메타 학습은 여전히 Splinter를 제외한 모든 기준선보다 성능이 떨어지는 기본 RAG 모델보다 훨씬 더 나은 성능을 유도합니다. 따라서 우리의 방법은 여전히 RAG보다 개선되고 있으며, 이 모델은 더 다재다능하고 최적의 접근 방식은 아니더라도 이러한 작업을 더 잘 처리할 수 있습니다.5.2 절제 여기에서 개별 모델 구성 요소와 (메타)학습 결정의 기여도를 이해하기 위해 추가 분석을 수행합니다.메모리 뱅크.그림 2 및 표 2와 같이 위키피디아 구절 대신 데모를 검색할 때 질문 답변 및 분류의 성능이 일반적으로 더 높다는 것을 발견했습니다.이로 인해 두 가지 질문이 제기됩니다.최상의 경우 메모리 뱅크가 다운스트림 성능에 얼마나 영향을 미칠 수 있을까요?관련하여 최상의 데모 메모리 뱅크가 주어진 경우 모델의 성능 상한은 무엇일까요?추정치를 얻기 위해 평가 데이터에서 레이블이 지정된 테스트 예제로 구성된 오라클 메모리를 만듭니다.이 설정에서 점수가 우리 방법과 다른 방법보다 상당히 개선되는 것을 발견했으며, 이는 메모리 뱅크가 개선되면 이 아키텍처가 추가 이득을 얻을 수 있는 상당한 잠재력이 있음을 나타냅니다.검색된 데모 수.더 많은 데모를 검색하는 것이 항상 더 나은가요? 우리는 E SQUAD MNLInumber of DemonstrationsNumber of Demonstrations Retriever SQUAD BioASQ QASC TriviaQA TbQA Random 1.1.1.1.2.DPR (Wiki) 11.1.15.4.24.DPR (PAQ) Contriever 16.1.26.29.24.14.7.28.27.9 24를 검색할 때의 성능을 비교합니다. 그림 4: 미세 조정 중 검색된 데모 수({0, 1, 5, 10, 25, 50, 100})에 따른 추출형 QA(SQUAD) 및 문장 쌍 분류(MNLI) 작업의 F₁ 점수. 점수는 일반적으로 검색된 데모 수에 따라 증가하지만 성능은 5-10개 데모에서 일찍 포화 상태에 이릅니다. 표 4: 각 검색기가 기준 진실 답변을 하위 문자열로 포함하는 최소 1개의 데모를 검색하는 테스트 예제의 비율.DPR(PAQ) 및 Contriever는 평균적으로 더 관련성 있는 데모를 검색하는 데 더 나은 것으로 보이지만 이것이 반드시 다운스트림 성능이 더 높아지는 것은 아닙니다(표 5).16샷 SQUAD F$[0,1) [1,2) [2,4) [4,8) [8,16) [16,inf) 검색된 문서의 답변 빈도70-[0,1] [1,2] [2,4) [4,8) [8,16) [16,inf) 검색된 문서의 답변 빈도 그림 5: 검색된 데모에서 참 답변 문자열의 빈도에 따른 비지식 집약적(SQUAD, 왼쪽) 및 지식 집약적(TriviaQA, 오른쪽) QA 작업의 F₁ 점수. 단조롭지는 않지만, 이러한 변수 간에는 명확한 상관관계가 있으며, 이는 어휘적 특징이 검색 성능에 기여하는 많은 부분에 책임이 있음을 나타냅니다.비지식 집약적 QA(SQUAD) 및 문장 쌍 분류(MNLI)에 대한 미세 조정 및 평가 중 K = {0, 1, 5, 10, 25, 50} 데모. 결과(그림 4)에 따르면 F₁ 점수는 두 작업 모두에서 5-10개 데모에서 포화되기 시작합니다. 그러나 일반적으로 더 많은 데모를 사용해도 성능에 해가 되지 않습니다. 모델은 성능이 크게 저하되지 않고 덜 유용한 데모를 처리할 수 있습니다. 검색이 도움이 되는 이유는 무엇일까요? 모델이 검색된 데모에서 의미적 내용을 추상화하여 성능을 개선하는 것일까요, 아니면 검색된 데모에서 토큰 시퀀스를 복사하는 법을 배우는 것일까요? 초기 테스트로서, 검색된 문서에서 기준 진실 답변 시퀀스의 빈도와 QA 작업의 F₁ 점수를 상관시킬 수 있습니다. 결과(그림 5)는 모델이 실제로 데모에서 특정 텍스트 문자열을 검색하는 방법을 학습하고 있음을 시사합니다. 이는 메모리 뱅크를 개선하기 위한 한 가지 가능한 경로를 제공합니다. 평가 작업과 의미적으로 더 많이 중복될수록 이러한 중복 가능성이 높아지므로 향후 작업은 의미적으로 더 유사한 demonRetriever Random DPR(Wiki) SQUAD BioASQ QASC TriviaQA TbQA 74.83.DPR(PAQ) Contriever 78.63.81.62.61.8 88.64.7 89.86.88.56.5 29.62.9 37.57.6 33.58.32.표 5: 리트리버에서 16샷 추출 QA 작업의 F1 점수. 동일한(최상의) 메타 학습 모델이 주어졌을 때 다른 리트리버로 미세 조정합니다. DPR(위키)의 낮은 리트리버 점수(표 4)에도 불구하고, 다운스트림 성능은 우리가 시도한 리트리버 중에서 가장 좋습니다. 어휘적 중복이 더 많은 stations. 그러나 이는 레이블 공간이 작고 레이블이 덜 유익한 분류 작업에서 검색이 어떻게 성능을 개선하는지 설명하지 못합니다. NLI의 경우 레이블 공간에는 &quot;entailment&quot;/&quot;neutral&quot;/&quot;contradiction&quot;이 포함되는데, 이는 데모에서 자주 볼 수 없을 것으로 예상되며 상당한 의미적 내용을 포함하지 않습니다. 그러나 검색 증강 모델은 MNLI(-mm)에서 FewshotQA보다 큰 차이로 성능이 뛰어납니다. 그렇다면 모델에 도움이 되는 것은 무엇일까요? 문장 임베딩 유사성이 이러한 유용성을 포착하지 못할 수 있지만, 모델을 올바른 완성으로 의미적으로 준비하는 QA 데모가 있을 수 있습니다. 향후 작업에서는 데모의 특정 기능을 제거할 수 있습니다. 어떤 유형의 검색기가 가장 좋을까요? 지금까지의 실험을 위해 Wikipedia에서 사전 학습하고 NaturalQuestions에서 미세 조정한 RAG-Token(NQ) 모델의 DPR 구성 요소를 사용했습니다. 이것이 최적의 시작점일까요? 아니면 다른 검색기가 더 좋을까요? ProbablyAsked Questions(PAQ; Lewis et al., 2021) 데이터 세트와 Contriever 모델(Izacard et al., 2021)도 있습니다. Contriever는 비지도 학습이지만 DPR 모델은 preMemory SQUAD BioASQ QASC TriviaQA TbQA 모든 작업 의미적으로 유사한 작업 83.83.63.89.64.7 89.61.4 36.62.9 37.표 6: MetaICL의 QA 메타 학습 컬렉션의 모든 QA 작업 또는 평가 작업과 의미적 유사성에 따라 하위 샘플링된 QA 작업에 대한 메타 학습 후 QA 작업에 대한 16샷 F₁ 점수. 메타 학습 작업의 전체 목록은 부록 A에서 확인할 수 있습니다. 학습. DPR은 다운스트림 작업이 사전 학습 또는 미세 조정 데이터와 유사할 때 더 나은 성능을 보이는 경향이 있습니다. 그러나 우리의 경우, 데모 검색은 위키피디아 구절 검색과 다르며, Contriever는 더 큰 traintest 이동을 더 잘 처리할 수 있습니다(Izacard et al., 2021). 우리는 QA 작업에서 검색된 데모(표 4)와 다운스트림 F₁(표 5)의 관련성을 모두 평가합니다. 우리는 DPR(PAQ)과 Contriever가 모두 비슷한 데모를 검색하는 데 더 뛰어나다는 것을 발견했는데, 이는 답이 포함된 예제를 검색하는 빈도로 측정한 것입니다. BioASQ의 경우, Contriever만이 랜덤 검색기보다 더 관련성 있는 데모를 검색합니다. 그러나 더 관련성 있는 데모를 검색한다고 해서 다운스트림 성능이 향상되는 것은 아닙니다. DPR(Wiki)은 지속적으로 다른 데모보다 성능이 뛰어납니다. 이유는 무엇일까요? 정성적 분석을 통해 DPR(Wiki)은 의미적으로 더 다양한 데모를 검색하는 반면, DPR(PAQ)과 Contriever는 테스트 예제와 기술적으로 더 유사하지만 테스트 예제에서 덜 다양한 데모를 검색한다는 것을 발견했습니다.따라서 다양성과 관련성 간에는 균형이 있어야 합니다.완전히 무작위 검색은 효과적이지 않지만(무작위 검색 기준 점수가 최악임에서 알 수 있듯이) 논란의 여지가 있지만 더 최적의 검색기를 사용하여 검색하는 더 제한된 데모 세트도 효과적이지 않습니다.메타 학습 데이터. 메타 학습이 설정에 포함된 작업의 다양성 때문에 도움이 됩니까(더 많을수록 더 나은 가설), 아니면 더 원칙적인 방식으로 메타 학습 데이터를 선택하는 것이 더 나을까요(비슷한 데이터 세트가 더 나은 가설)? 우리는 MetaICL의 모든 QA 작업에 대해 메타 학습을 할 때의 다운스트림 성능을 평가 작업과의 평균 인스턴스 수준 의미적 유사성을 기준으로 최상위 작업과 비교합니다(표 6). 의미적 유사도를 계산하기 위해 Sentence Transformers(Reimers and Gurevych, 2019)의 stsb-roberta-base-v2 모델을 사용하여 평가 과제의 16샷 학습 예제와 메타 학습 과제의 모든 예제 간의 평균 쌍별 코사인 유사도를 계산합니다. 그런 다음 240,000개가 넘는 예제(배치 크기 8을 사용하여 30,학습 단계에 충분)가 될 때까지 유사도에 따라 상위 과제를 선택합니다. 하위 샘플링 전후의 메타 학습 과제 목록은 부록 A를 참조하세요. 평가 과제와의 의미적 유사도를 기반으로 메타 학습 데이터를 선택하는 것이 QA 및 비QA 과제 모두에 도움이 된다는 것을 발견했습니다. 가장 유사한 데이터에 대해서만 메타 학습을 수행할 때 F₁가 과제 전체에서 증가합니다. 이는 일반적으로 메타 학습 과제가 많을수록 더 좋다는 것을 발견한 Min et al.(2022a)의 연구 결과와 대조됩니다. 6
--- CONCLUSION ---
s 우리는 다양한 데모 뱅크(§3.3)에서 의미적으로 유사한 데모를 검색(§3.1)하는 메타 학습 방법(§3.2)을 제안했습니다. 우리의 방법은 다른 강력한 매개변수 효율적인 소수 샷 베이스라인(§5)보다 많은 작업에서 평균적으로 더 높은 성능을 달성합니다. 향후 작업에서는 지식 집약적 작업을 포함한 더 다양한 작업에서 성능을 개선하기 위해 데모 검색과 구절 검색을 혼합하여 탐색할 수 있습니다. 제한 사항 우리의 방법은 메모리 뱅크에 대한 레이블이 지정된 많은 예제에 액세스해야 하며, 이상적으로는 평가 작업과 어느 정도 관련이 있어야 합니다. 이는 이 방법에 최적화된 언어와 작업을 제한합니다. 리소스가 부족한 언어 종류에 대한 다양한 학습 예제가 존재하지 않으며, 예를 들어 도메인별 고객 데이터가 있는 산업 애플리케이션과 같이 훨씬 더 구체적인 작업에 대한 학습 예제도 존재하지 않습니다. 다국어 모델이 교차 언어 전송을 활용할 수 있지만, 이 모델이 (예를 들어) 다국어 BART를 사용할 때 리소스가 부족한 언어로 얼마나 잘 일반화될지는 불분명합니다. 전체 데모 메모리를 사용할 때 메타 훈련은 현재 구현을 사용하여 16GB GPU에서 실행되지 않습니다. 이는 보다 일반적인 GPU를 제외하지만, 저희의 접근 방식은 몇 시간 안에 32GB GPU에서 빠르게 실행될 수 있으므로 처음부터 비슷한 few-shot 성능의 언어 모델을 사전 훈련하는 것보다 비용이 훨씬 적게 듭니다. 참고문헌 Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, Sonal Gupta. 2021. Muppet: 사전 미세 조정을 통한 대규모 멀티태스크 표현. 2021 자연어 처리 경험적 방법 컨퍼런스 회의록, 5799-5811쪽, 온라인 및 푼타카나, 도미니카 공화국. 계산 언어학 협회. Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum. 2020. few-shot 자연어 분류 작업을 위한 자체 감독 메타러닝. 2020 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 522-534쪽, 온라인. Association for Computational Linguistics. Yujia Bao, Menghua Wu, Shiyu Chang, Regina Barzilay. 2020. 분포적 시그니처를 사용한 few-shot 텍스트 분류. 8th International Conference on Learning Representations, ICLR 2020, 에티오피아 아디스아바바, 2020년 4월 26-30일. OpenReview.net. 세바스찬 보르조, 아서 멘쉬, 조던 호프만, 트레버 카이, 엘리자 러더퍼드, 케이티 밀리칸, 조지 반 덴 드리셰, 장바티스트 레스피오, 보그단 다모크, 에이단 클라크, 디에고 데 라스 카사스, 아우렐리아 가이, 제이콥 메닉, 로만 링, 톰 헤니건, 사프론 황, 로렌 마조레, 크리스 존스, 알빈 카시러, 앤디 브록, 미켈라 파가니니, 제프리 어빙, 오리올 비냔알스, 사이먼 오신데로, 캐런 시모얀, 잭 W. 레이, 에리히 엘슨, 로랑 시프레. 2022. 수조 개의 토큰에서 검색하여 언어 모델 개선. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33권, 1877-1901페이지. Curran Associates, Inc. Rakesh Chada와 Pradeep Natarajan. 2021. FewshotQA: 사전 학습된 텍스트-텍스트 모델을 사용하여 질문 답변 과제의 few-shot 학습을 위한 간단한 프레임워크. 2021 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 6081-6090페이지, 온라인 및 푼타카나, 도미니카 공화국. 계산 언어학 협회. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, He He. 2022. 언어 모델 컨텍스트 내 튜닝을 통한 메타 학습. 계산 언어학 협회 제60회 연례 회의 회의록(제1권: 장문 논문), 719-730페이지, 더블린, 아일랜드. 계산 언어학 협회. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, 정형원, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, 현택 임, 바렛 조프, 알렉산더 Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason 웨이, 캐시 마이어-헬스턴, 더글러스 에크, 제프 딘, 슬라브 페트로프, 노아 피델. 2022. Palm: 경로를 통한 언어 모델링 확장. CORR, ABS/2204.02311. 윌리엄 B. 돌란과 크리스 브로켓. 2005. 문장의 의역 모음을 자동으로 구성합니다. 제3차 국제 패러프레이징 워크숍(IWP2005) 진행 중. Chelsea Finn, Pieter Abbeel, Sergey Levine. 2017. 딥 네트워크의 빠른 적응을 위한 모델 독립적 메타 학습. 2017년 8월 6-11일 호주 NSW 시드니에서 열린 제34회 기계 학습 국제 컨퍼런스 논문집, ICML 2017, 기계 학습 연구 논문집 70권, 1126-1135쪽. PMLR. Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, Danqi Chen. 2019. MRQA공유 과제: 독해 이해에서 일반화 평가. 2019년 11월 4일 중국 홍콩에서 열린 제2회 기계 독해 질의응답 워크숍 논문집, MRQA@EMNLP 2019, 1-13쪽. 계산 언어학 협회. Tianyu Gao, Adam Fisch, Danqi Chen. 2021. 사전 학습된 언어 모델을 더 나은 few-shot 학습자로 만들기. Association for Computational Linguistics의 제59회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스(제1권: 장문 논문), 3816-3830페이지, 온라인. Association for Computational Linguistics. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang. 2020. 검색 증강 언어 모델 사전 학습. ICML 2020에서 개최된 제37회 기계 학습 국제 컨퍼런스, 2020년 7월 13-18일, 가상 이벤트, 기계 학습 연구 논문집 제119권, 3929-3938페이지. PMLR. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly. 2019. NLP를 위한 매개변수 효율적 전이 학습. 2019년 6월 9-15일 미국 캘리포니아 롱비치에서 개최된 제36회 기계 학습 국제 컨퍼런스의 회의록, ICML 2019, 기계 학습 연구 회의록 97권, 2790-2799쪽. PMLR. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, Edouard Grave. 2021. 대조 학습을 통한 비지도 고밀도 정보 검색을 향해. CORR, abs/2112.09118. Gautier Izacard와 Edouard Grave. 2021. 생성 모델을 사용하여 개방 도메인 질문 답변을 위한 구절 검색 활용. 유럽 학회의 제16차 회의록: 주요 권, EACL 2021, 온라인, 2021년 4월 19일-23일, 874880페이지. 학회. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane DwivediYu, Armand Joulin, Sebastian Riedel, Edouard Grave. 2022. 검색 증강 언어 모델을 사용한 Few-shot 학습. CoRR, abs/2208.03299. Jeff Johnson, Matthijs Douze, Hervé Jégou. 2021. GPU를 이용한 Billion-scale similarity search. IEEE Trans. Big Data, 7(3):535–547. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer. 2017. TriviaQA: 독해를 위한 대규모 원격 감독 챌린지 데이터 세트. 제55회 Association for Computational Linguistics 연례 회의록(제1권: 장문 논문), 1601-1611쪽, 캐나다 밴쿠버. Association for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih. 2020. 오픈 도메인 질의응답을 위한 밀집 구절 검색. 2020년 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 6769-6781쪽, 온라인. Association for Computational Linguistics. Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, Hannaneh Hajishirzi. 2017. 당신은 6학년생보다 똑똑할까요? 다중 모드 기계 이해를 위한 교과서 질문 답변. 2017년 IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 5376-5384쪽. Daniel Khashabi, Yeganeh Kordi, Hannaneh Hajishirzi. 2022. Unifiedqa-v2: 더 광범위한 교차 포맷 훈련을 통한 더 강력한 일반화. CoRR, abs/2202.12359. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. Unifiedqa: 단일 QA 시스템으로 포맷 경계를 넘나들다. Findings of the Association for Computational Linguistics: EMNLP 2020, 온라인 이벤트, 2020년 11월 16-20일, Findings of ACL의 EMNLP 2020권, 1896-1907쪽. Association for Computational Linguistics. Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. Qasc: 문장 구성을 통한 질의 응답을 위한 데이터 세트. AAAI 인공지능 컨퍼런스 회의록, 34(05):8082-8090. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. 2020. ALBERT: 언어 표현의 자기 지도 학습을 위한 가벼운 BERT. 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela. 2020. 지식 집약적 nlp 작업을 위한 검색 증강 생성. 신경 정보 처리 시스템의 발전, 33권, 9459-9474페이지. Curran Associates, Inc. Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich Küttler, Aleksandra Piktus, Pontus Stenetorp, Sebastian Riedel. 2021. PAQ: 6,500만 개의 질문과 그 질문으로 할 수 있는 일. Association for Computational Linguistics의 거래, 9:1098-1115. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen. 2022. GPT-3에 대한 좋은 맥락 내 예는 무엇입니까? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, 100-114쪽, 더블린, 아일랜드 및 온라인. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. 2019. Roberta: 견고하게 최적화된 BERT 사전 학습 접근 방식. CORR, abs/1907.11692. Sewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi. 2022a. MetaICL: 맥락에서 학습하는 법. 2022년 북미 컴퓨터 언어학 협회 지부 회의록: 인간 언어 기술, 2791-2809쪽, 미국 시애틀. 컴퓨터 언어학 협회. 민세원, 류신시, 아리 홀츠만, 미켈 아르테체, 마이크 루이스, 하나네 하지시르지, 루크 제틀모이어. 2022b. 시범의 역할 재고: 맥락 내 학습을 작동시키는 요소는 무엇인가? CoRR, abs/2202.12837. 에런 뮐러, 제이슨 크론, 살바토레 로메오, 사브 만수르, 엘만 만시모프, 이 장, 댄 로스. 2022. fewshot 텍스트 분류를 위한 레이블 의미 인식 사전 학습. 제60회 연례 총회 논문집(제1권: 장문 논문), 83188334쪽, 아일랜드 더블린. 전산 언어학 협회. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang. 2016. SQUAD: 텍스트의 기계 이해를 위한 100,000개 이상의 질문. 2016년 자연어 처리 경험적 방법 컨퍼런스 논문집, 2383-2392쪽, 텍사스 오스틴. 전산 언어학 협회. Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy. 2021. 사전 학습 범위 선택을 통한 소수 질문 답변. 제59회 계산언어학 협회 연례 회의록 및 제11회 자연어 처리 국제 공동 컨퍼런스(제1권: 장문 논문), 3066-3079쪽, 온라인. 계산언어학 협회. Nils Reimers와 Iryna Gurevych. 2019. SentenceBERT: Siamese BERTnetworks를 사용한 문장 임베딩. 2019년 자연어 처리 경험적 방법 컨퍼런스 및 제9회 자연어 처리 국제 공동 컨퍼런스(EMNLP-IJCNLP) 회의록, 3982-3992쪽, 중국 홍콩. 계산언어학 협회. Timo Schick와 Hinrich Schütze. 2021a. 빈칸 채우기 질문을 활용하여 소수 텍스트 분류 및 자연어 추론하기. 16번째 유럽 지부 학회 회의록: 주요 권, 255-269페이지, 온라인. 학회 계산 언어학. 티모 쉬크와 힌리히 쉬체. 2021b. 중요한 것은 크기만이 아닙니다. 소규모 언어 모델도 소수 학습자입니다. 2021년 북미 지부 학회 계산 언어학: 인간 언어 기술 회의록, 2339-2352페이지, 온라인. 학회 계산 언어학. 조지 차차로니스, 게오르기오스 발리카스, 프로드로모스 말라카시오티스, 이오안니스 파르탈라스, 마티아스 Zschunke, 마이클 R. 알버스, 더크 바이센보른, 아나스타샤 크리타라, 세르히오스 페트리디스, 디미트리스 폴리크로노풀로스, 야니스 알미란티스, 존 파블로풀로스, 니콜라스 바스키오티스, 패트릭 갈리나리, 티에리 아르티에르, Axel-Cyrille Ngonga Ngomo, Norman Heino, Éric Gaussier, Liliana Barrio-Alvers, Michael Schroeder, Ion Androutsopoulos 및 Georgios Paliouras. 2015. BIOASQ 대규모 생물의학 의미 색인화 및 질의 응답 대회 개요. BMC Bioinform., 16:138:1-138:28. 리카르도 빌랄타와 유세프 드리시. 2002. 메타 학습에 대한 관점과 조사. Artif. Intell. Rev., 18(2):77-95. Ellen M. Voorhees와 Dawn M. Tice. 2000. 질문 답변 테스트 컬렉션 구축. 정보 검색 연구 및 개발에 관한 제23회 국제 ACM SIGIR 연례 컨퍼런스 회의록, SIGIR &#39;00, 200-207페이지, 뉴욕, 뉴욕, 미국. Association for Computing Machinery. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman. 2018. GLUE: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼. 2018년 EMNLP 워크숍 회의록, BlackboxNLP: NLP를 위한 신경망 분석 및 해석, 353-355페이지, 벨기에 브뤼셀. Association for Computational Linguistics. Jason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Mingbo Dai, Quoc V. Le. 2022. Finetuned language models are zero-shot learners. International Conference on Learning Representations에서. Adina Williams, Nikita Nangia, Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana에서 2018년 북미 지부 회의록에서. Association for Computational Linguistics. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma. 2022. 암묵적 베이지안 추론으로서의 맥락 내 학습에 대한 설명. International Conference on Learning Representations에서. Qinyuan Ye, Bill Yuchen Lin, Xiang Ren. 2021. CrossFit: NLP에서 교차 작업 일반화를 위한 몇 번의 샷 학습 과제. 2021 Conference on Empirical Methods in Natural Language Processing의 회의록, 7163-7189쪽, 온라인 및 푼타카나, 도미니카 공화국. Association for Computational Linguistics. Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Yu Cheng, Gerald Tesauro, Haoyu Wang, Bowen Zhou. 2018. 여러 메트릭을 사용한 다양한 몇 번의 샷 텍스트 분류. Association for Computational Linguistics: Human Language Technologies의 북미 지부 2018년 회의록, 제1권(긴 논문), 1206-1215쪽, 루이지애나주 뉴올리언스. Association for Computational Linguistics. Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein. 2021. 데이터 세트 및 프롬프트 컬렉션에 대한 메타 튜닝을 통한 제로샷 학습을 위한 언어 모델 적용. Association for Computational Linguistics의 연구 결과: EMNLP 2021, 2856-2878페이지, 도미니카 공화국 푼타카나. Association for Computational Linguistics. A 과제 메타 훈련. 메타 훈련 데이터는 MetaICL(Min et al., 2022a)의 메타 훈련 세트에서 가져왔습니다. 구체적으로, 표 7에 나와 있는 것처럼 CROSSFIT과 UNIFIEDQA 과제를 혼합한 논문의 QA 과제 컬렉션을 사용합니다. 평가하는 과제는 제외합니다. MetaICL에서와 마찬가지로 과제당 16,384개의 예를 하위 샘플링하여 메타 훈련 중에 개별 과제가 과대 표현되지 않도록 합니다. 일부 작업은 일부 작업의 CROSSFIT 버전과 UNIFIEDQA 버전을 모두 포함하기 때문에 두 번 샘플링됩니다(Min et al., 2022a). 모든 메타 훈련 작업: biomrc, boolq, freebase_qa, hotpot_qa, kilt_hotpotqa, kilt_nq, kilt_trex, kilt_zsre, lama-conceptnet, lama-google_re, lama-trex, mc_taco, numer_sense, quoref, ropes, search_qa, supergluemultirc, superglue-record, tweet_qa, web_questions, unifiedqa:boolq, unifiedqa:commonsenseqa, unifiedqa:drop, unifiedqa:narrativeqa, unifiedqa:natural_questions_with_dpr_para, unifiedqa:newsqa, unifiedqa:physical_iqa, unifiedqa:quoref, unifiedqa:race_string, unifiedqa:ropes, unifiedqa:social_iqa, unifiedqa:winogrande_xl 유사성에 따른 하위 샘플링: biomrc, boolq, freebase_qa, hotpot_qa, lama-google_re, quoref, ropes, superglue-multirc, superglue-record, unifiedqa:boolq, unifiedqa:commonsenseqa, unifiedqa:drop, unifiedqa:narrativeqa, unifiedqa:natural_questions_with_dpr_para, unifiedqa:newsqa, unifiedqa:quoref, unifiedqa:race_string, unifiedqa:ropes 표 7: 메타 학습 데이터에서 사용된 작업. 메타 학습 중 균형 잡힌 감독을 보장하기 위해 작업당 16,384개의 예를 하위 샘플링합니다. &quot;unifiedqa:&quot; 접두사가 붙지 않는 한 모든 작업은 CROSSFIT에서 가져온 것입니다. 또한 평가 작업과의 의미적 유사성에 따라 작업을 선택하는 타겟팅된 하위 샘플링 절차를 수행합니다. 이를 위해 메타 학습 과제의 예제와 각 평가 과제의 16샷 분할 간의 평균 쌍별 의미적 유사도를 계산한 다음 유사도의 감소 순서로 메타 학습 과제를 선택합니다. 의미적 유사도는 SentenceTransformers(Reimers 및 Gurevych, 2019)의 stsb-roberta-base-v2 모델에서 문장 임베딩의 코사인 유사도를 계산하여 계산합니다. 데모. 데모는 표 8에 표시된 대로 추출, 추상 및 객관식 QA 과제를 포함하는 UNIFIEDQA 컬렉션에서 가져온 것입니다. 평가하는 과제는 모두 제외합니다. 데모 세트와 메타 학습 세트 간에 일부 중복이 있지만 데모에는 정답이 포함된 반면 메타 학습 예제에는 정답이 포함되어 있지 않습니다. 데모 작업 뱅크: unifiedqa:ai2_science_middle, unifiedqa:boolq, unifiedqa:commonsenseqa, unifiedqa:drop, unifiedqa:mctest, unifiedqa:narrativeqa, unifiedqa:natural_questions_with_dpr_para, unifiedqa:newsqa, unifiedqa:openbookqa, unifiedqa:openbookqa_with_ir, unifiedqa:physical_iqa, unifiedqa:quoref, unifiedqa:race_string, unifiedqa:ropes, unifiedqa:social_iqa, unifiedqa:winogrande_xl 표 8: 데모 메모리 뱅크에서 사용된 작업. 각 작업 내에 하위 샘플링이 없다는 점에 유의하세요. 검색기는 관련 없는 데모를 무시할 수 있기 때문입니다. 모든 작업은 UNIFIEDQA에서 가져왔습니다. B 객관식 QA를 위한 형식 튜닝 Chada와 Natarajan(2021)은 QA 입력 및 출력 형식을 변경하기만 해도 상당한 성능 향상을 관찰했습니다. 대부분의 QA 작업에 이와 유사한 형식을 사용하지만, 질문/답변/컨텍스트 형식을 객관식 QA로 확장하는 방법이나 컨텍스트에 답변 옵션을 포함하는 것이 전혀 도움이 될지 여부는 즉시 명확하지 않습니다. 따라서 QASC에 대해 세 가지 다른 형식을 시도하고 성능을 비교합니다. ... 모든 예는 질문 q, 두 개의 컨텍스트 문장 C1 및 C2, 문자 레이블이 있는 8개의 답변 옵션 세트 {aд, aB,. аH}, 정답 a Є {a A, ..., aH}로 구성됩니다. 전체 답변 문자열 또는 답변 i의 문자 레이블을 생성할 수 있습니다(여기서 i ≥ {A, B, H}). 답변 옵션을 질문이나 컨텍스트에 넣고, 답변 옵션을 모두 제외하고, 답변 문자열 a를 생성하고, 답변 문자 i를 생성합니다. BARTlarge를 사용한 결과(표 9)는 문자 라벨을 생성하는 것보다 답을 생성하는 것이 더 뛰어나고, 컨텍스트에 옵션을 포함하는 것이 도움이 되며, 컨텍스트에서 옵션을 제외하거나 질문에 옵션을 넣는 것은 성능에 해롭다는 것을 나타냅니다. 다양한 형식 간의 성능 격차는 매우 크며, 이는 Chada와 Natarajan(2021)의 연구 결과와 일치합니다. 모델의 사전 학습 형식에 맞춰진 예시 형식을 사용하는 것은 few-shot 성능에 기여하는 가장 중요한 요소 중 하나입니다. 형식 이름 형식 질문의 옵션: q? {aA,..., aн} \n 문자 답변 생성: [MASK] \n 컨텍스트: C1. C2. 질문: q? \n 답변: i 질문의 옵션: q? {aд,...,aƒ} \n 답변 생성 답변: [MASK] \n 컨텍스트: C1. C2. 질문: q? \n 답변: a 질문의 옵션: q? \n 답변: [MASK] 답변 생성 \n 컨텍스트: {aA,..., aH}. C1. 질문: q? \n 정답: a ate 정답 C2. 선택 사항 없음, 일반- 질문: q? \n 정답: [MASK] \n 맥락: C1. C2. ⇒ 질문: q? \n 정답: a 예시 Fquestion: 햇빛은 식물에게 무엇을 하나요? (A) 낮 동안 (B) 식물을 죽입니다. (C) 볼 수 있습니다. (D) 식물의 생존에 도움이 됩니다. (E) 식물의 물을 마시는 데 도움이 됩니다. (F) 가열됩니다. (G) 열을 더합니다. (H) 색깔이 어두워집니다. \n 정답: [MASK] \n 맥락: 식물은 생존을 위해 음식이 필요합니다. 모든 식물은 음식을 만들기 위해 햇빛이 필요합니다. ⇒ 질문: \n 정답: D ... 질문: 햇빛은 식물에게 무엇을 하나요? (A) 낮 동안 (B) 식물을 죽인다 (C) 볼 수 있다 (D) 생존에 도움이 된다 (E) 물을 마시는 데 도움이 된다 (F) 가열된다 (G) 열이 더해진다 (H) 색깔이 더 어두워진다 \n 정답: [MASK] \n 맥락: 식물은 생존에 음식이 필요합니다. 모든 식물은 음식을 만들기 위해 햇빛이 필요합니다. ⇒ 질문: ... \n 정답: 식물의 생존에 도움이 됩니다 질문: 햇빛은 식물에게 무엇을 합니까? \n 정답: [MASK] \n 맥락: (A) 낮 동안 (B) 식물을 죽인다 (C) 볼 수 있다 (D) 생존에 도움이 됩니다 (E) 물을 마시는 데 도움이 됩니다 (F) 가열된다 (G) 열이 더해진다 (H) 색깔이 더 어두워진다. 식물은 생존에 음식이 필요합니다. 모든 식물은 음식을 만들기 위해 햇빛이 필요합니다. ⇒ 질문: \n 정답: 식물의 생존에 도움이 됩니다 질문: 햇빛은 식물에게 무엇을 합니까? \n 정답: [MASK] \n 맥락: 식물은 생존에 음식이 필요합니다. 모든 식물은 음식을 만들기 위해 햇빛이 필요합니다. ⇒ 질문: ... \n 답변: 생존에 도움이 됨 표 9: 각 형식에 대한 미세 조정 후 BART 대형(검색 없음)에서 QASC 및 16샷 F₁ 점수에 대해 시도한 형식. 우리는 답을 생성하는 것이 문자 레이블을 생성하는 것보다 더 낫다는 것을 발견했고, 컨텍스트에 옵션을 포함하는 것이 도움이 되며, 컨텍스트에서 옵션을 제외하는 것은 성능에 해롭다는 것을 발견했습니다. &quot;&quot;는 입력과 출력 시퀀스를 구분하고, &quot;\n&quot;은 줄 바꿈을 나타냅니다.
