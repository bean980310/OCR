--- ABSTRACT ---
실제 세계에 배치된 자율 로봇은 환경 변화에 빠르게 적응하는 제어 정책이 필요합니다. 이를 위해 AutoML-Zero를 기반으로 하는 방법인 Auto Robotics-Zero(ARZ)를 제안합니다. 이 방법은 처음부터 제로 샷 적응형 정책을 발견합니다. 모델 매개변수만 최적화하는 신경망 적응 정책과 달리 ARZ는 선형 레지스터 머신의 완전한 표현력으로 제어 알고리즘을 구축할 수 있습니다. 우리는 모델 매개변수를 조정하고 추론 알고리즘을 즉석에서 변경하여 갑작스러운 환경 변화에 적응하는 모듈식 정책을 개발합니다. 우리는 현실적인 시뮬레이션된 사족 로봇에서 방법을 시연하고, 개별 팔다리가 갑자기 부러질 때 넘어지는 것을 방지하는 안전한 제어 정책을 개발합니다. 이것은 두 가지 인기 있는 신경망 기준선이 실패하는 어려운 작업입니다. 마지막으로, 우리는 Cataclysmic Cartpole이라는 새롭고 어려운 비상정지 제어 작업에서 우리 방법에 대한 자세한 분석을 수행합니다. 결과는 ARZ가 갑작스러운 환경 변화에 훨씬 더 강력하고 간단하고 해석 가능한 제어 정책을 구축할 수 있다는 우리의 발견을 확인합니다. I.
--- INTRODUCTION ---
실제 세계에 배치된 로봇은 필연적으로 많은 환경 변화에 직면하게 됩니다. 예를 들어, 배터리 수준 및 물리적 마모와 같은 로봇의 내부 조건과 새로운 지형이나 장애물과 같은 외부 조건은 시스템의 역학이 비정지적임을 의미합니다. 이러한 상황에서 항상 동일한 상태를 동일한 동작에 매핑하는 정적 컨트롤러는 거의 최적이 아닙니다. 로봇은 변화하는 환경에 대응하여 제어 정책을 지속적으로 조정할 수 있어야 합니다. 이 기능을 달성하려면 외부 신호 없이도 환경의 변화를 인식하고 동작이 시간이 지남에 따라 시스템 상태를 어떻게 변경하는지 관찰하고 이에 대응하여 제어를 업데이트해야 합니다. 순환 딥 신경망은 빠른 적응을 지원하는 인기 있는 정책 표현입니다. 그러나 이러한 신경망은 종종 (1) 모놀리식이므로 여러 다른 환경 물리학에 견고한 정책을 학습하려고 할 때 주의 산만 딜레마가 발생합니다. [1], [2]; (2) 과매개변수화되어 일반화가 제대로 되지 않고 추론 시간이 길어질 수 있음; (3) 해석하기 어렵습니다. 이상적으로는 간단하고 해석 가능하면서도 여러 행동 모드를 표현할 수 있는 정책을 찾고 싶습니다.우리는 현실적인 4족 로봇 적응 작업에서 동적이고 자체 수정되는 제어 정책의 진화를 구체적으로 지원하기 위해 AutoML-Zero(AMLZ) [3] 기반의 새로운 프레임워크인 AutoRobotics-Zero(ARZ)를 제안합니다.우리는 이러한 출판된 논문과 IROS 2023의 최우수 전체 논문 결선 진출자를 대표합니다.비디오: https://youtu.be/SEFP1Hay4nE 연락처: spkelly@mcmaster.ca # wX: 주소 X의 벡터 메모리.def f(x, v, i): WO = copy(v) W0 [i] =w1 = abs(v) w1 [0] -0.858343 * norm (w2) w2wO WO return log(x), wi # SX: 주소 X의 스칼라 메모리.# vX: 주소 X의 벡터 메모리.#obs, action: 관찰 및 동작 벡터. def GetAction(obs, action): s13s15인 경우: s5 = -0.920261 * s15 &lt; s12인 경우: s8, v14, 113 = 0인 경우 min(v8, sqrt(min(0, v3))), −s1s7인 경우: s7, action = f(s12, v0, 18) action = heaviside(v12) s13인 경우
--- RELATED WORK ---
유전 프로그래밍(GP)의 초기 시연은 간단하고 해석 가능한 최적의 비선형 제어 정책을 처음부터 개발하는 능력을 확립했습니다[5]. 더 최근에 GP는 제어 품질을 희생하지 않고도 심층 강화 학습으로 개발된 복잡한 신경망 정책의 동작을 해석 가능하고 설명 가능한 프로그램으로 정제하는 데 사용되었습니다[6]. 이 작업에서 우리는 이러한
--- METHOD ---
AutoML-Zero를 기반으로 처음부터 제로샷 적응형 정책을 발견합니다. 모델 매개변수만 최적화하는 신경망 적응 정책과 달리 ARZ는 선형 레지스터 머신의 완전한 표현력으로 제어 알고리즘을 구축할 수 있습니다. 우리는 모델 매개변수를 조정하고 추론 알고리즘을 즉석에서 변경하여 갑작스러운 환경 변화에 적응하는 모듈식 정책을 개발합니다. 우리는 현실적인 시뮬레이션된 사족 로봇에서 우리의 방법을 시연하며, 개별 팔다리가 갑자기 부러질 때 넘어지는 것을 방지하는 안전한 제어 정책을 개발합니다. 이것은 두 가지 인기 있는 신경망 기준선이 실패하는 어려운 작업입니다. 마지막으로, 우리는 Cataclysmic Cartpole이라는 새롭고 어려운 비상정지 제어 작업에서 우리 방법에 대한 자세한 분석을 수행합니다. 결과는 ARZ가 갑작스러운 환경 변화에 훨씬 더 강력하고 간단하고 해석 가능한 제어 정책을 구축할 수 있다는 우리의 발견을 확인합니다. I. 서론 현실 세계에 배치된 로봇은 필연적으로 많은 환경 변화에 직면하게 됩니다. 예를 들어, 배터리 수준 및 물리적 마모와 같은 로봇의 내부 조건과 새로운 지형이나 장애물과 같은 외부 조건은 시스템의 역학이 비정지적임을 의미합니다. 이러한 상황에서 항상 동일한 상태를 동일한 동작에 매핑하는 정적 컨트롤러는 거의 최적이 아닙니다. 로봇은 변화하는 환경에 대응하여 제어 정책을 지속적으로 조정할 수 있어야 합니다. 이 기능을 달성하려면 외부 신호 없이도 동작이 시간이 지남에 따라 시스템 상태를 어떻게 변경하는지 관찰하여 환경의 변화를 인식하고 이에 대응하여 제어를 업데이트해야 합니다. 순환 딥 신경망은 빠른 적응을 지원하는 인기 있는 정책 표현입니다. 그러나 이러한 네트워크는 종종 (1) 모놀리식이므로 여러 다른 환경 물리학에 견고한 정책을 학습하려고 할 때 주의 산만 딜레마가 발생합니다 [1], [2]. (2) 과매개변수화되어 일반화가 어렵고 추론 시간이 길어질 수 있습니다. (3) 해석하기 어렵습니다. 이상적으로는 여러 동작 모드를 표현할 수 있으면서도 간단하고 해석 가능한 정책을 찾고 싶습니다. 우리는 현실적인 4족 로봇 적응 작업에서 동적이고 자체 수정되는 제어 정책의 진화를 구체적으로 지원하기 위해 AutoML-Zero(AMLZ) [3]에 기반한 새로운 프레임워크인 AutoRobotics-Zero(ARZ)를 제안합니다.우리는 이러한 출판된 논문과 IROS 2023의 최우수 전체 논문 결선 진출자를 대표합니다.비디오: https://youtu.be/SEFP1Hay4nE 연락처: spkelly@mcmaster.ca # wX: 주소 X의 벡터 메모리.def f(x, v, i): WO = copy(v) W0 [i] =w1 = abs(v) w1 [0] -0.858343 * norm (w2) w2wO WO return log(x), wi # SX: 주소 X의 스칼라 메모리.# vX: 주소 X의 벡터 메모리.#obs, action: 관찰 및 동작 벡터. def GetAction(obs, action): s13s15인 경우: s5 = -0.920261 * s15 &lt; s12인 경우: s8, v14, 113 = 0인 경우 min(v8, sqrt(min(0, v3))), −s1s7인 경우: s7, action = f(s12, v0, 18) action = heaviside(v12) s13인 경우 <s2: s15, v3 = f(s10, v7, 12) if s2s0: s11, v9, 113 = 0, 0, -s7 arcsin (s15) if ss13: s3 = -0.920261 * ss12 dot (v3, obs) s1, s3, s15 = maximum (s3, s5), cos(s3), 0.947679 * sif s2s8: s5, v13, i5 = 0, min(v3, sqrt (min (0, v13))), -if s6s0: s15, v9, i11 = 0, 0, -if s2 s3: s2, v7 = f3(s8, v12, 11) if s1 < s6: s13, v14, 13 = 0, min(v8, sqrt (min(0, 0))), -if s13s2: s7 = -0.920261 * sif ss1: s3 = -0.920261 * sif ss1 s8, action = f(s5, v15, 13) if ss13: s5, v7 = f(s15, v7, i15) s2s10 + sif s7s12: s11, v13 = f(s9, v15, i5) if s4s11: s0, v9, 113 = 0, 0, -s10, action [15] = sqrt(s7), sif s7s9: s15 =if s14s11: s3 = -0.920261 * sif s8s5: s10, v15, i1 = 0, min(v13, sqrt (min(0, 0))), -return action Fig. 1: Automatically discovered Python code representing an adaptable policy for a realistic quadruped robot simulator (top-right inset). This evolved policy outperforms MLP and LSTM baselines when a random leg is suddenly broken at a random time. (Lines in red will be discussed in the text). policies as programs instead of neural networks and demonstrate how the adaptable policy and its initial parameters can be evolved from scratch using only basic mathematical operations as building blocks. Evolution can discover control programs that use their sensory-motor experience to finetune their policy parameters or alter their control logic onthe-fly while interacting with the environment. This enables the adaptive behaviors necessary to maintain near-optimal performance under changing environmental conditions. Unlike the original AMLZ, we go beyond toy tasks by tackling the simulator for the actual Laikago robot [4]. To facilitate this, we shifted away from the supervised learning paradigm of AMLZ. We show that evolved programs can adapt during their lifetime without explicitly receiving any supervised input (such as a reward signal). Furthermore, while AMLZ relied on the hand-crafted application of three discovered functions, we allow the number of functions used in the evolved programs to be determined by the evolutionary process itself. To do this, we use conditional automatically defined functions (CADFS) and demonstrate their impact. With this approach, we find that evolved adaptable policies are significantly simpler than stateof-the-art solutions from the literature because evolutionary search begins with minimal programs and incrementally adds complexity through interaction with the task domain. Their behavior is highly interpretable as a result. In the quadruped robot, ARZ is able to evolve adaptable policies that maintain forward locomotion and avoid falling, even when all motors on a randomly selected leg fail to generate any torque, effectively turning the leg into a passive double pendulum. In contrast, despite comprehensive hyperparameter tuning and being trained with state-of-the-art reinforcement learning methods, MLP and LSTM baselines are unable to learn robust behaviors under such challenging conditions. While the quadruped is a realistic complex task, simulating the real robot is time-consuming. Due to the lack of efficient yet challenging benchmarks for adaptive control, we created a toy adaptation task dubbed Cataclysmic Cartpole and repeated our analysis on this task with similar findings. In both cases, we provide a detailed analysis of evolved control programs to explain how they work, something notoriously difficult with black box neural network representations. In summary, this paper develops an evolutionary method for the automated discovery of adaptable robotic policies from scratch. We applied the method to two tasks in which adaptation is critical, Quadruped Leg-Breaking and Cataclysmic Cartpole. On each task, the resulting policies: surpass carefully-trained MLP and LSTM baselines; • • are represented as interpretable, symbolic programs; and • use fewer parameters and operations than the baselines. These points are demonstrated for each task in Section V. II. RELATED WORK Early demonstrations of Genetic Programming (GP) established its power to evolve optimal nonlinear control policies from scratch that were also simple and interpretable [5]. More recently, GP has been used to distill the behavior of complex neural network policies developed with Deep Reinforcement Learning into interpretable and explainable programs without sacrificing control quality [6]. In this work, we extend these methods to evolve programs that can change their behavior in response to a changing environment. We demonstrate how to automatically discover a controller that can context switch between distinct behavior modes when it encounters diverse tasks, thus avoiding trade-offs associated with generalization across diverse environmental physics. If we can anticipate the nature of the environmental change a robot is likely to encounter, we can simulate environments similar to the expected changes and focus on building multitask control policies [2], [7]. In this case, some form of domain randomization [8] is typically employed to expose candidate policies to a breadth of task dynamics. However, policies trained with domain randomization often trade optimality in any particular environment dynamics for generality across a breadth of dynamics. This is the problem we aim to address with ARZ. Unlike previous studies in learning quadruped locomotion in the presence of non-stationary morphologies (e.g., [9]), we are specifically interested in how controllers can be automatically built from scratch without requiring any prior task decomposition or curriculum learning. This alleviates some burden on robotics engineers and reduces researcher bias toward known machine learning algorithms, opening the possibility for a complex adaptive system to discover something new. In addition to anticipated non-stationary dynamics, another important class of adaptation tasks in robotics is sim-toreal transfer [11], where the robot needs to adapt policies trained in simulation to unanticipated characteristics of the real-world. Successful approaches to learn adaptive policies can be categorized by three broad areas of innovation: (1) New adaptation operators that allow policies to quickly tune their model parameters within a small number of interactions [10], [11], [12], [13]; (2) Modular policy structures that separate the policy from the adaptation algorithm and/or world model, allowing both to be learned [14], [15], [16], [17]; and (3) Hierarchical methods that allow a diverse set of complete or partial behaviors to be dynamically switched in and out of use at run-time, adapting by selecting the best strategy for the current environmental situation [9], [2], [18]. These algorithmic models of behavioral plasticity, modular structure, and hierarchical representations reflect the fundamental properties of meta-learning. In nature, these properties emerged through adaptation at two timescales (evolution and lifetime learning) [19]. ARZ makes these two time scales explicit by implementing an evolutionary search loop that acts on a "genome" of code, and an evaluation that steps through an episode which is analogous to the "lifetime" of the robot. III. METHODS A. Algorithm Representation As in the original AutoML-Zero [3], policies are represented as linear register machines that act on virtual memory [20]. In this work, we support four types of memory: scalar, vector, matrix, and index (e.g. s1, v1, m1, i1). Scalar, vector, and matrix memory are floating-point, while index memory stores integers. Algorithms are composed of two core functions: StartEpisode () and GetAction(). StartEpisode() runs once at the start of each episode of interaction with the environment. Its sole purpose is to initialize the contents of virtual memory with evolved constants. The content of these memories at any point in time can be characterized as the control program's state. Our goal is to discover algorithms that can adapt by tuning their memory state or altering their control code on-the-fly while interacting with their environment. This adaptation, as well as the algorithm's decision-making policy, are implemented by the GetAction() function, in which each instruction executes a single operation (e.g.s0=s7*s1 or s3=v1 [12]). We define a large library of operations (Table S2) and place no bounds on the complexity of programs. Evolutionary search is employed to discover what sequence of operations and associated memory addresses appear in the GetAction() function. Conditional Automatically Defined Functions: In addition to StartEpisode() and GetAction(), up toConditionally-invoked Automatically Defined Functions [21] (CADFs) may be generated in an algorithm. Each CADF represents an additional function block, itself automatically discovered, which is callable from GetAction(). Since each CADF is conditionally invoked, the sequence of CADFS executed at each timestep throughout an episode is dynamic. This property is advantageous for multi-task learning and adaptation because programs that can switch control code in and out of the execution path on-the-fly are able to dynamically integrate general, re-useable code for related tasks and specialized code for disjoint tasks. We demonstrate in Section IV how this improves performance for the quadruped task. Each CADF receives 4 scalars, 2 vectors, and 2 indices as input, and execution of the function is conditional on a < comparison of the first 2 scalars (a configuration chosen for simplicity). The set of operations available is identical to GetAction() except that CADFs may not call each other to avoid infinite recursion. Each CADF uses its own local memory of the same size and dimensionality as the main memory used by Setup() and GetAction(). Their memory is initialized to zero at the start of each episode and is persistent across timesteps, allowing functions to integrate variables over time. Post-execution, the CADF returns the single most recently written index, scalar, and vector from its local memory. The policy-environment interface and evaluation procedure are illustrated in Fig. 2. Sections V-A and V-B provide examples of evolved programs in this representation for the quadruped robot and Cataclysmic Cartpole task, respectively. B. Evolutionary Search Two evolutionary algorithms are employed in this work: Multi-objective search with the Nondominated Sorting genetic algorithm II (NSGA-II) [22] and single-objective search with Regularized evolution (RegEvo) [23], [3]. Both search algorithms iteratively update a population of candidate control programs using an algorithmic model of the Darwinian principle of natural selection. The generic steps for evolutionary search are: 1) Initialize a population of random control programs. 2) Evaluate each program in the task (Fig. 2). 3) Select promising programs using a task-specific fitness metric (See Fig. 2 caption). 4) Modify selected individuals through crossover and then mutation (Fig. S1). 5) Insert new programs into the population, replacing some proportion of existing individuals. 6) Go to step 2. # StartEpisode = initialization code. # GetAction = control algorithm. # Sim simulation environment. # episodes = number of evaluation episodes. # sX/vX/mX/iX: scalar/vector/matrix/index memory # at address X. def EvaluateFitness (StartEpisode, GetAction): sum_reward =for e in episodes: reward =steps =# Initialize sX/vX/mX with evolved parameters. # iX is initialized to zero. StartEpisode () # Set environment initial conditions. state Sim. Reset() while (!Sim. Terminal ()): # Copy state to memory, will be accessible # to Get Action. v1 = state # Execute action-prediction instructions. GetAction(state) if Sim. NumAction() > 1: action = velse: action = sstate Sim. Update (action) reward += Reward(state, action) steps +=sum reward += reward sum_steps += steps return sum_reward/episodes, sum_steps/episodes 그림 2: 진화된 제어 알고리즘의 평가 프로세스. 단일 목적 진화 탐색은 평균 에피소드 보상을 알고리즘의 적합도로 사용하는 반면, 다중 목적 탐색은 평균 보상(첫 번째 반환 값)과 에피소드당 평균 단계(두 번째 반환 값)의 두 가지 적합도 지표를 최적화합니다. 이 연구의 목적을 위해 NSGA-II와 RegEvo의 가장 중요한 차이점은 선택 방법입니다. NSGA-II는 여러 적합도 지표(예: 전진 운동 및 안정성)를 사용하여 유망한 개체를 식별하는 반면 RegEvo는 단일 지표(전진 운동)를 기반으로 선택합니다. 두 탐색 방법은 동시에 다음과 같이 진화합니다. (1) StartEpisode()에서 설정하는 초기 알고리즘 매개변수(즉, 부동 소수점 메모리 sX, vX, mX의 초기 값) (2) GetAction() 함수와 CADFS의 프로그램 내용. 1) 다목적 탐색: 사족보행 로봇 작업에서 목표는 모터 오작동이 있는 경우에도 원하는 속도로 지속적으로 걷는 컨트롤러를 만드는 것입니다. 실제 로봇은 낙하와 관련된 손상을 피하는 것이 중요하며, 로봇이 이를 달성하는 가장 간단한 방법은 비교적 가만히 서 있고 손상을 감지한 후 앞으로 이동하려고 하지 않는 것입니다. 따라서 이 도메인은 예측할 수 없는 역학이 있는 상황에서 안정성을 유지하면서 걷는 것은 동시에 최적화해야 하는 상충되는 목표이기 때문에 다목적 탐색에 적합합니다. 이 작업에서 NSGA-II가 전진 운동과 안정성 간의 트레이드오프 스펙트럼을 포함하는 다양한 제어 알고리즘 집단을 유지하는 방법을 보여줍니다. 이러한 다양한 부분 솔루션 또는 빌딩 블록 집단에서 진화적 탐색 연산자(돌연변이 및 교차)는 두 가지 목표 모두에서 유능한 정책을 구축할 수 있습니다. 사족보행 로봇 작업에 대한 NSGA-II 목적 함수와 제약 조건은 섹션 IV에서 설명합니다. 2) 단일 목적 검색: Cataclysmic Cartpole 작업은 실제 로봇 작업의 안전 제약 및 시뮬레이션 오버헤드 없이 도전적인 적응 벤치마크 환경을 제공합니다. 적응 연구를 더욱 단순화하고
--- EXPERIMENT ---
이 작업에서 RegEvo 검색 알고리즘을 채택하여 빠른 실험을 위해 최적화합니다. NSGA-II와 달리 RegEvo의 비동기 병렬 작업자는 선택도 수행하여 전체 인구가 평가될 때까지 기다린 후 개인을 순위 지정, 선택 및 수정하는 병목 현상을 제거합니다. 교차 및 돌연변이 연산자: 두 부모 알고리즘 간에 무작위로 선택된 CADF를 바꾸는 간단한 교차 연산자를 사용합니다. 모든 CADF는 인수 목록과 반환 값 형식이 동일하므로 교차 지점을 선택하는 데 서명 일치가 필요하지 않습니다. 두 부모 알고리즘에 CADF가 없으면 무작위로 선택된 부모 중 하나가 반환됩니다. 교차 후 자식 프로그램은 확률적 돌연변이의 영향을 받으며, 이는 표 S1에 나열된 연산자를 사용하여 코드를 추가, 제거 또는 수정합니다. C. 알고리즘 구성 및 기준선 시간 기억은 유기체가 수명 동안 변화, 학습 또는 적응할 수 있도록 하는 주요 정신 시스템입니다. 동적 환경에서 주어진 상황에 대한 최상의 조치를 예측하려면 정책이 현재 상황을 과거 상황 및 조치와 비교할 수 있어야 합니다. 이는 적절한 행동을 생성하는 것이 현재 상태와 환경이 어떻게 변화하는지에 대한 예측에 따라 달라지기 때문입니다.진화된 알고리즘은 상태가 있기 때문에 부분적으로 적응할 수 있습니다.메모리 내용(sX, vX, mX 및 iX)은 에피소드의 시간 단계에 걸쳐 지속됩니다.ARZ를 상태 없는 기준선과 상태 있는 기준선과 비교합니다.이러한 정책 아키텍처는 각각 다층 퍼셉트론(MLP)과 장단기 메모리(LSTM) 네트워크로 구성되며 최적화할 매개변수는 순전히 연속적입니다.따라서 최첨단 연속 최적화 도구인 Augmented Random Search(ARS)[24]를 사용하며 로봇 이동 작업 학습에 특히 효과적인 것으로 나타났습니다[12], [25].이에 비해 Proximal Policy Optimization[26]은 성능이 상당히 떨어졌습니다.결과를 생략하고 향후 작업으로 조사를 남겨둡니다.모든 방법은 Supplement S1-A의 세부 정보와 수렴할 때까지 학습할 수 있었습니다.IV. 비정지 작업 도메인 우리는 두 가지 다른 환경을 고려합니다.사족 로봇을 위한 현실적인 시뮬레이터와 새로운 Cataclysmic Cartpole입니다.두 경우 모두 정책은 일반적으로 기능을 방해하는 환경의 전환 기능의 변화를 처리해야 합니다.이러한 변화는 갑작스럽거나 적절하거나 점진적일 수 있으며 변화가 발생하는 시점이나 환경이 어떻게 변화하는지 나타내는 센서 입력은 제공되지 않습니다.A. 사족 로봇 우리는 Unitree Laikago 로봇[4]을 시뮬레이션하기 위해 Tiny Differentiable Simulator[27]를 사용합니다.이것은 다리당 작동 자유도가 있는 사족 로봇입니다.따라서 동작 공간에는 원하는 모터 각도에 해당하는 12차원 실수 값이 있습니다.비례-미분 컨트롤러를 사용하여 이러한 원하는 각도를 추적합니다.관찰 공간에는 각 관절의 각도와 속도, 로봇 본체의 위치, 방향 및 속도를 설명하는 37개의 실수 값이 포함됩니다.각 에피소드는 로봇이 안정적인 수직 위치에서 시작하여 최대 1000개의 타임스텝(초) 동안 계속됩니다. 정책에서 제안하는 각 동작은 10단계 연속 반복됩니다.비정지 4족 보행 작업의 목표는 1.0m/초의 속도로 전진(x축)하는 것입니다.적응은 각 에피소드 내에서 무작위로 선택한 다리의 모든 관절이 무작위 시간에 갑자기 수동이 되는 갑작스러운 다리 부러짐을 처리해야 합니다.다리는 나머지 에피소드 동안 효과적으로 이중 진자가 됩니다.로봇이 넘어지면 에피소드가 일찍 종료되고 이로 인해 반환이 줄어듭니다.다음과 같은 보상 함수를 설계합니다.* r(t) = 1.0 — 2 × |v(t) − v| – ||ā(t) – ā(t − 1)||2, (1) 여기서 첫 번째 항 1.0은 생존 보너스이고, ō는 목표 전진 속도 1m/s이고, v(t)는 로봇의 현재 전진 속도이며, a(t)와 a(t − 1)은 정책의 현재 및 이전 동작 벡터입니다. 이 보상 함수는 관절 가속도의 변화를 최소화하여 모터 스트레스를 완화하는 동시에 로봇이 가능한 한 오랫동안 일정한 속도로 걷도록 장려하도록 형성되었습니다.다목적 검색의 맥락에서 최대 1000개의 타임스텝에 걸쳐 방정식 1의 평균을 최대화하는 것이 목표 1입니다.y축을 따라 너무 많이 벗어나는 행동을 억제하기 위해 로봇의 y축 위치가 ±3.0미터를 초과하면 에피소드를 종료합니다.목표 2는 로봇이 떨어지거나 이 y축 임계값에 도달하지 않고 생존할 수 있었던 타임스텝 수입니다.중요한 점은 단순히 가만히 서 있는 정책에는 관심이 없다는 것입니다.따라서 목표 2가 50보다 크고 목표 1이 50보다 작으면 두 적합도가 모두 0으로 설정됩니다.그림 S2에서 볼 수 있듯이 이러한 적합도 제약은 전진 동작 목표의 진행에 기여하지 않고도 개체군에 지속될 정책을 제거합니다. B. 격변적 카트폴 환경 적응의 본질을 더 자세히 연구하기 위해, 고전적 카트폴([28]) 물리학의 여러 측면이 동적으로 만들어진 격변적 카트폴이라는 새롭고 매우 어렵지만 계산적으로 간단한 도메인을 소개합니다.적응은 다음과 같은 비정상적 속성을 처리해야 합니다.• 트랙 각도: 트랙은 임의의 시간에 임의의 각도로 기울어집니다.폴 각도(0)에 대한 로봇의 기준 프레임이 카트에 상대적이므로 균형을 유지하기 위한 새로운 중력 방향과 O의 원하는 값을 파악하고 폴의 균형을 유지할 만큼 충분히 빠르게 대응해야 합니다.트랙 각도는 [-15, 15]도 범위에서 가변적입니다.이는 외부 환경의 변화를 시뮬레이션합니다.• 힘: 힘 승수 f가 정책의 동작에 적용되어 작동기 강도가 시간이 지남에 따라 증가하거나 감소할 수 있습니다.정책의 효과적인 동작은 ƒ × 동작이며, 여기서 f는 [0.5, 2] 범위 내에서 시간이 지남에 따라 변경됩니다.예를 들어, 배터리가 부족하여 작동기 강도가 떨어지는 것을 시뮬레이션합니다. • 감쇠: 감쇠 계수 D는 관절 토크를 TD-Dar로 수정하여 가변적인 관절 마찰을 시뮬레이션합니다. 여기서 = qr은 관절 속도입니다([29]의 방정식 2.81, 2.83 참조). 이것은 관절 마모를 시뮬레이션합니다. D는 시간이 지남에 따라 [0.0, 0.15] 범위에서 변경됩니다. 각 유형의 변경은 단일 매개변수로 제어됩니다. 그림 S4에 나와 있는 에피소드 중에 이러한 매개변수가 어떻게 변경될 수 있는지에 대한 두 가지 일정을 조사합니다. V. 결과 A. 사족보행 다리 부러뜨리기 1) 기준선과의 비교: CADFS를 포함한 ARZ는 다리 부러뜨리기 작업에서 실행 가능한 제어 정책을 생성한 유일한 방법입니다. 이 문제는 매우 어렵습니다. 원활한 운동을 유지하고 다리 부러뜨리기에 견고한 정책을 찾으려면 20번의 진화 실험 반복이 필요합니다(그림 3a의 적합도 &gt; 600). 그림 3a에서 500~600 사이의 훈련 적합도는 일반적으로 (1) 3/다리 부러짐에만 견고한 실행 가능한 전방 보행 행동 또는 (2) 다리 부러짐에 견고한 정책이지만 실제 로봇에는 실행 가능하지 않은 높은 빈도로 작동하며 그 결과 적합도 형성에 의해 보상이 상당히 페널티를 받는 정책을 나타냅니다. 단일 최상의 반복 내에서 NSGA-II 검색 알고리즘은 원활한 전방 이동(보상 목표)과 안정성(단계 목표) 간에 성능 상충이 있는 다양한 정책을 생성합니다(그림 3b). 이 최종 개인 집합에서 각 기준선의 단일 최상의 정책과 비교할 단일 정책을 선택합니다. 실용적인 벽시계 시간 제한으로 인해 ARS+MLP와 ARS+LSTM 정책 모두를 총 시도 횟수만큼만 훈련할 수 있었지만, 이 샘플 제한 하에서는 최상의 ARS 정책조차 최상의 ARZ 정책에서 발견한 570보다 훨씬 낮은 360의 보상만 달성했으며, 이는 ARZ가 표준 신경망 기준선보다 샘플 효율성이 더 높을 수 있음을 시사합니다.그림 4는 ARZ가 에피소드 중간에 여러 개의 다른 다리가 부러져도 견고한 컨트롤러를 구축할 수 있는 유일한 방법임을 확인합니다. ARS+MLP와 ARS+LSTM에서 발견한 단일 최상의 컨트롤러와 비교하여 하나의 챔피언 ARZ 정책에 대한 훈련 후 테스트 결과를 표시합니다. ARZ의 적응 품질(평균 보상으로 측정)은 각 개별 다리의 경우 기준선보다 우수하며, 정지 작업(그림 4의 &quot;없음&quot; 참조)에서의 성능은 다른 모든 방법보다 상당히 우수합니다. 흥미롭게도, 그림 4는 MLP가 오른쪽 뒤쪽 다리 부러짐의 특정 사례에도 견고한 정책을 학습했다는 것을 나타냅니다.ARZ와 달리, 이 적응을 다른 다리에 일반화할 수 없습니다.마지막으로, LSTM 정책은 정지 작업에서 MLP보다 더 나은 성과를 보였지만, 어떤 다리 부러짐 시나리오에도 적응하지 못했습니다.그림 4의 5개 테스트 에피소드 샘플에 대한 궤적을 시각화하면 ARZ 정책이 유일한 제어자임을 확인할 수 있습니다.보상(최대)400-200-보상(최대)600-400300200CADFS 사용 CADFS 사용 안 함3 4개체 le(a) 진화 진행 • CADFS 사용 • CADFS 사용 안 함 960 980920에피소드당 단계(최대) (b) 최상의 파레토 전선 그림 3: CADFS는 평균적으로 진화 속도를 높이고 최상의 최종 결과를 생성했습니다.(a)는 CADFS를 사용하고 사용하지 않고 20번의 독립적인 반복을 통해 기록된 ARZ 검색 데이터를 보여줍니다. (a)의 수평축은 평가된 개별 프로그램의 총 수를 보여주고, 수직축은 지금까지 발견된 단일 최고 개인에 대한 32개 에피소드에 걸친 평균 수익률(식 1)을 보여줍니다. (b)는 각 실험에서 최대 보상을 받은 단일 반복에 대한 파레토 전선을 보여줍니다. (b)의 각 점은 하나의 제어 프로그램의 이중 목적 적합도를 나타냅니다. 평균 보상 800ARZ MLP LSTM400200전면-오른쪽 전면-왼쪽 후면-오른쪽 후면-왼쪽 없음 그림 4: ARZ는 모든 다리 부러짐에 적응할 수 있는 유일한 정책을 발견합니다. 플롯은 중간 에피소드 다리 부러짐 작업에서 ARZ 및 ARS 기준선(MLP 및 LSTM)의 단일 최고 정책에 대한 테스트 결과를 보여줍니다. 각 다리에 대해 막대는 해당 다리가 무작위로 선택된 타임스텝에서 부러진 100개 에피소드에 걸친 평균 보상을 보여줍니다. 모든 열에서 보상 &lt; 400은 해당 다리에 대한 대부분의 테스트 에피소드가 낙상으로 끝났음을 나타냅니다. 모든 시나리오에서 넘어지는 것을 피할 수 있지만, 앞 왼쪽 다리가 부러진 경우에는 앞으로 움직이는 데 어려움이 있습니다(그림 5). 이는 앞 왼쪽 다리에 대한 비교적 약한 테스트 보상에서 반영됩니다(그림 4 참조). MLP 정책은 부러진 뒤 오른쪽 다리로 걷는 데 성공하지만 다른 모든 동적 작업에서는 넘어집니다. 마지막으로 LSTM은 모든 다리가 신뢰할 수 있는 정지 작업에서만 넘어지는 것을 피할 수 있습니다. (a) ARZ (b) MLP (c) LSTM 그림 5: ARZ는 지속적으로 넘어지는 것을 피하는 유일한 정책을 발견합니다. 플롯은 각 다리 부러짐 작업의 샘플 궤적을 보여줍니다. 수직 막대는 변화 지점(단계 500)을 나타냅니다. ▲는 로봇이 넘어졌음을 나타냅니다. 각 플롯은 고유한 다리가 부러진 4개의 테스트 에피소드를 보여줍니다. 위에서 아래로 영향을 받는 다리는 다음과 같습니다. 없음, 뒤 왼쪽, 뒤 오른쪽, 앞 왼쪽, 앞 오른쪽. 2) 단순성과 해석성: 진화적 탐색을 통해 발견된 네 발 다리 부러뜨리기 과제에 대한 정책은 그림 1에 제시되어 있습니다. 이 알고리즘은 매개변수를 사용하며 40줄 미만의 코드로 표현할 수 있으며, 단계당 최대 2080개의 부동 소수점 연산(FLOP)을 실행합니다. 이는 각각 단계당 2.5k/9k 매개변수와 5k/18k FLOPS 이상을 사용하는 기준 MLP/LSTM 모델에서 사용되는 매개변수와 FLOP 수와 대조되어야 합니다. 이러한 숫자가 어떻게 얻어졌는지에 대한 자세한 설명은 섹션 S4에서 찾을 수 있습니다. 각 함수는 실행 내내 지속되는 고유한 변수와 메모리를 가지고 있습니다. 변수의 초기화 값은 GetAction 함수에 맞게 조정되어 매개변수로 계산되지만, f에 대해서는 모두 0으로 설정됩니다. 여기서는 ARZ 정책에 대한 초기 분석을 제공하고 알고리즘에 대한 전체 분석과 해석은 향후 작업으로 남겨둡니다. 이 알고리즘의 주요 특징은 입력을 4가지 상태로 이산화하고, 사족보행자의 행동은 내부 상태와 이산화 레이블에 의해 완전히 결정된다는 것입니다. 이산화된 상태의 시간적 전이는 다리가 부러지지 않았을 때 안정된 주기적 운동을 보이고, 다리가 부러지면 이 패턴에 명확한 교란이 생깁니다(그림 6 참조). 이는 여러 변수가 이전 단계의 변수를 축적하고 보존하는 상태 알고리즘이므로, 이산화된 상태의 시간적 패턴이 사족보행자의 적응적 행동에 대한 신호 역할을 한다고 추측합니다. 3. No Breaking A llllllll WW AFront-Right BreakingBack-Right BreakingBack-Left Breakingbreaking은 상태의 결정 경계를 정의하는 변수 s7과 s15가 시간에 따라 안정된 주기 함수를 형성하기 때문에 상태 궤적 패턴의 변화로 정확하게 감지할 수 있습니다. 그림 7에서 이를 보여줍니다.앞다리 부러지기의 세 스칼라 s12, $15 및 s7의 값을 수직 빨간색 선으로 표시했습니다.다리 부러지기 후 입력 s12의 동작이 현저하게 바뀌었지만, 두 스칼라 s7과 s15의 동작은 약간만 영향을 받았습니다.흥미롭게도 스칼라 레지스터 s7과 s15의 동작은 리드미컬한 움직임을 생성하는 생물학적 회로의 중앙 패턴 생성기의 동작과 유사합니다[30].wwwStepsss그림 7: 앞다리 부러지기 동안 네발동물의 스칼라 값 s12, s15 및 s7.다리 부러지기에도 불구하고 스칼라 s15와 s의 일관된 주기적 동작에 주목하세요(수직 빨간색 선으로 표시).분석된 모든 다리 부러지기 시나리오에서 동일한 주기가 관찰되었습니다. 정책이 여러 고유한 실패 조건을 빠르게 식별하고 이에 적응하는 능력은 그림 8a에서 명확히 알 수 있습니다.그림 8a는 다리가 부러지기 1초 전과 후의 컨트롤러 동작을 표시합니다.다리가 부러지면 동작이 명확하고 즉각적으로 변하는 것을 볼 수 있습니다.이 정책은 변경이 발생한 시점을 식별하고 빠르게 적응할 수 있습니다.그림 8b는 변경 전후의 각 타임스텝에서 실행되는 특정 CADF 시퀀스를 보여주며, CADF가 정책이 동작을 빠르게 조정하는 능력에 역할을 한다는 것을 나타냅니다.실제로 CADF를 포함한 진화적 실행만이 다리가 부러져도 견고한 정책을 발견할 수 있었습니다.단계그림 6: 다양한 다리 부러지기 패턴의 상태 궤적.다리 부러지기 이벤트는 수직 빨간색 선으로 표시되어 있습니다.다리가 부러지는 패턴이 다르면 상태 궤적도 다르다는 점에 유의하십시오.이러한 궤적이 알고리즘에서 적응적 반응을 유발하는 신호 역할을 한다고 추측합니다. 이제 그림 1에 제시된 ARZ 알고리즘에서 연속 입력 신호가 어떻게 이산화되는지 자세히 살펴보겠습니다. 먼저 들어오는 관찰 벡터가 나머지 알고리즘과 상호 작용하는 유일한 방법은 동적 벡터 v(세 개의 빨간색 코드 줄 중 두 번째)와 내적을 취하여 스칼라 s12를 형성하는 것입니다. 스칼라 s12는 빨간색으로 표시된 두 개의 if 문을 통해서만 동작에 영향을 미칩니다. 따라서 입력 관찰이 동작에 미치는 영향은 스칼라 $15와 s7이 설정한 두 결정 경계에 대한 스칼라 s12의 상대적 위치에 따라 전적으로 결정됩니다. 다시 말해, 시스템에 대한 관찰의 외부 입력은 효과적으로 4가지 상태로 이산화됩니다. 0(s12 ≤ s15, s7), 1(s15, s7 &lt; s12), 2(s7s12 ≤ s15) 또는 3(s15 &lt; s12 ≤ s7). 따라서 환경의 외부 변화, 예를 들어 다리 동작 300 350 400 450 500 550 600 650단계(a) 동작 CADF 시퀀스 기타 5,6,5,3,6,3,3,3,7,54,54.6.5.35,364.3.3.74.6.6.743636644,5,3,6,5,4,3,3,74.54.6.354.6.3.745.3664.3.74,5,3,6,4,6,7,44.5.3636645,6,3,6,3,3,3,5,3,6,4,3,76.3.3.6.6.7.5.3.6.3.6.3.6.44,5,3,6,4,36,36,44.5.6,33,6,4,3,6,36,4-단계 (b) CADF 호출 시퀀스 그림 8: Front-Left leg가 끊어졌을 때 ARZ 정책 동작이 변경됨 중간 에피소드(500단계)에서 CADFS로 인한 동작의 역학과 프로그램 제어 흐름에서 알 수 있듯이.B. 대격변 카트폴 새로운 벤치마크 적응 작업을 도입하는 것은 현실적인 사족보행자 시뮬레이터의 결과에 유익한 추가 사항입니다.왜냐하면 우리는 벤치마크 역학의 본질을 적응 갭을 생성할 만큼 충분히 중요해질 때까지 경험적으로 조정할 수 있기 때문입니다.무상태 정책(즉, MLP 일반 정책)이 비정상적 환경에서 제어 정책을 적응시킬 수 없기 때문에 성능이 좋지 않을 때입니다(자세한 내용은 섹션 S 참조).대격변 카트폴에 적응이 필요하다는 것을 확인한 후, 이 작업에서는 상태 있는 정책만 검사합니다.1) 기준선과의 비교: 대격변 카트폴에서 우리는 ARZ가 갑작스럽고 극적인 변화가 있는 작업에서 (상태 있는) ARS+LSTM 기준선에 비해 우수한 제어를 생성한다는 것을 확인했습니다.그림 9와 10은 검색이 완료된 후 수행된 테스트를 보여줍니다. 800의 적합도 점수는 정책이 약 800개의 타임스텝 동안 폴의 균형을 맞추었고, 활성 역학이 있는 에피소드의 마지막 지점까지 살아남았음을 나타냅니다(그림 S4 참조). &quot;정지&quot;는 표준 카트폴 작업이고 &quot;힘&quot;, &quot;감쇠&quot; 및 &quot;트랙 각도&quot;는 이러한 매개변수에서만 갑작스럽거나 연속적으로 변경되는 카트폴을 나타냅니다(섹션 IV-B 참조). &quot;모두&quot;는 모든 변경 매개변수가 잠재적으로 동시에 변경되는 경우입니다. 범례는 진화 중에 사용된 정책 유형과 해당 작업 유형을 나타냅니다. 첫째, 강력한 적응형 정책은 정지 작업에서만 진화된 ARZ 또는 ARS+LSTM에서 나오지 않는다는 점에 유의하세요(ARZ [정지] 및 LSTM [정지] 참조). 즉, 정지 작업의 숙련도가 비정지 구성으로 직접 전환되지 않는다는 것을 의미합니다. 그러나 검색 중에 비정지 속성에 노출되면 ARZ와 ARS+LSTM은 모든 갑작스럽고 연속적인 비정지 작업에 적응하는 정책을 발견합니다. ARZ는 갑작스러운 변화 작업에서 훨씬 더 능숙하며(그림 10), 모든 작업에서 1000점이라는 완벽한 점수에 가까운 점수를 받았습니다. 연속적 변화에서 단일 최상의 LSTM 정책은 Track Angle 문제에서 ARZ보다 더 강력한 점수로 최상의 멀티태스킹 성능을 달성했으며, 다른 모든 작업에서는 최소한 ARZ만큼 능숙합니다. 그러나 LSTM 네트워크와 달리 ARZ 정책은 고유하게 해석할 수 있습니다. 평균 보상 정책 유형[훈련 작업]: ARZ[모두] ARZ[정지] LSTM[모두] 800-정지 힘 감쇠 Track Angle LSTM[정지] 모두 그림 9: Cataclysmic Cartpole 연속적 변화 작업의 진화 후 테스트 결과. 범례는 정책 유형과 검색 작업을 나타냅니다. [A11]은 진화 중에 모든 작업에 노출된 정책을 표시합니다. ARZ와 LSTM은 모두 이 적응 작업을 해결하며, 정지 작업에서 동적 작업으로의 직접 전이는 관찰되지 않았습니다. 각 실험에서 가장 우수한 5개 정책이 표시됩니다. Average Reward800-Stationary Force Policy Type [Train Task]: ARZ [All] Damping Track Angle LSTM [All] All 그림 10: Cataclysmic Cartpole 갑작스런 변화 작업의 진화 후 테스트 결과. [A11]은 진화 중에 모든 작업에 노출된 정책을 표시합니다. ARZ는 모든 갑작스런 변화 Cataclysmic Cartpole 작업에 적응하는 유일한 정책을 발견합니다. 각 실험에서 가장 우수한 5개 정책이 표시됩니다. 2) 단순성 및 해석성: 여기서는 ARZ 정책을 분해하여 변화하는 환경에서 최적의 동작을 계산하기 위해 시간에 따른 상태 관찰을 통합하는 방법에 대한 자세한 설명을 제공합니다. 그림의 ARZ [All] 설정에서 발견된 알고리즘의 예가 그림 11에 나와 있습니다. 이 작업을 해결하는 데 CADF가 필요하지 않았으므로 프로그램 분석을 단순화하기 위해 검색 공간에서 생략되었습니다. 우리가 찾은 것은 현재 동작을 추론할 수 있는 관찰 및 동작 값의 기록을 수집하는 세 개의 어큐뮬레이터입니다. # sX: 주소 X의 스칼라 메모리. # obs: 벡터(x, theta, ✗_dot, theta_dot]. #23 a, b, c 고정 스칼라 매개변수. #23 V, W: 4차원 벡터 매개변수. def GetAction(obs, action): s0 = a * s2 + action s1 s0s1 + b * action + dot(V, obs) s2s0+ c * saction = s0 + dot(obs, W) return action 그림 11: 모든 매개변수가 지속적으로 변경되는 작업(그림 9의 ARZ [All])에서 진화한 샘플 상태 있는 액션 함수. Python으로 표시된 코드. 이 알고리즘은 11개의 변수를 사용하고 단계당 25FLOPS를 실행합니다. 한편, MLP와 LSTM 대응 제품은 1k와 4.5k 이상의 매개변수를 사용하여 각각 단계당 2k와 9k 이상의 FLOP를 소비합니다. 이 계산에 대한 자세한 내용 섹션 S4에 제시되어 있습니다. = 이 알고리즘을 보는 데는 두 가지 유용한 방법이 있습니다. 첫째, 단계 n에서 so, s1 및 s2의 값을 d = 3차원의 잠재 공간에 있는 벡터로 해석할 수 있는 벡터 Zn으로 구성하면 알고리즘을 다음과 같은 형태로 표현할 수 있습니다. Sn+1 = concat(obs n+1, actn); Zn+1 = Ũ · Zn+P· Sn+1; act n+AT.Zn+1+WT.Sn+1, 상태 벡터를 잠재 공간에 투영하는 투영 행렬 P와 adxd 진화 행렬 Ũ. 이는 내부 상태 Zn이 있는 선형 순환 신경망입니다. 알고리즘을 보는 두 번째 방법은 비례-적분-미분(PID) 컨트롤러의 일반화로 해석하는 것입니다. 이는 먼저 위에 제시된 순환 방정식을 명시적으로 풀고 연속 한계를 취하여 수행할 수 있습니다. 5차원 단일 상태 벡터 s(t) [x(t)‚O(t), x(t),Ġ(t), act(t)]와 d차원 벡터 u, v, w, 5차원 벡터 p와 상수항 c를 도입하면 연속 시간 한계에서 알고리즘을 다음과 같은 형태로 작성할 수 있습니다. act(t) = c+w. Ut.u+pT · s(t) +v² · Så dт Ut−˜ . P s(7) 여기서 P와 U는 P와 Ū의 연속 시간 버전입니다. 우리가 발견한 특정 알고리즘(그림 11)에서 d는 3이 됩니다. 적분 측정이 이제 기존 PID 컨트롤러에 비해 적분 대상에서 시간에 따라 달라지는 가중치 인자를 갖는다는 점에 유의하세요. 이 알고리즘에 대한 추가적인 도출, 논의 및 해석은 보충 자료에 나와 있습니다. . VI.
--- CONCLUSION ---
그리고 토론 = 우리는 ARZ를 사용하여 프로그램 공간과 매개변수 공간에서 동시에 검색하면 능숙하고 간단하며 해석 가능한 제어 알고리즘이 생성되어 제로샷 적응을 수행하고 급진적인 변화를 겪는 환경에서도 거의 최적의 제어를 유지하기 위해 동작을 빠르게 변경할 수 있음을 보여주었습니다. 이 섹션의 나머지 부분에서는 향후 작업에 대한 동기를 부여하고 추측합니다. CADFS와 주의 산만 딜레마. 사족 로봇 도메인에서 검색 공간에 조건부로 호출되는 자동 정의 함수(CADFS)를 포함하면 진화된 제어 알고리즘의 표현력이 향상되는 것을 관찰했습니다. 단일 최적 정책에서 CADF는 관찰 공간을 4가지 상태로 이산화하는 데 사용되었습니다. 그런 다음 동작은 시스템의 내부 상태와 이 이산화된 관찰에 의해 완전히 결정됩니다. 한 가지 해석은 이 이산화가 정책이 주의 산만 딜레마를 극복할 수 있는 전환 동작을 정의하는 데 도움이 된다는 것입니다. 즉, 다중 작업 정책이 여러 다른 작업에서 뛰어난 성과를 거두는 보상과 일반화를 달성하는 궁극적인 목표 간의 균형을 맞추는 과제입니다. [1] 대조적으로, 수작업으로 설계한 MLP 또는 LSTM 네트워크의 매개변수 공간에서만 검색하면 두 개 이상의 고유한 변경 이벤트(예: 다리 하나 부러짐)에 적응할 수 있는 정책을 생성하지 못했습니다. 모듈식/계층적 정책과 주의 산만 딜레마에 미치는 영향에 대한 심층 연구는 향후 작업으로 미루겠습니다. 대격변 카트폴 작업. 실제 로봇을 시뮬레이션하는 데는 계산 집약적인 특성이 있으므로 적응이 중요한 보다 관리하기 쉬운 장난감 작업도 포함해야 한다고 생각했습니다. 이로 인해 대격변 카트폴 작업이 탄생했습니다. 빠른 실험을 수행하고 ARZ 결과의 힘과 해석 가능성을 강조하는 데 유용하다는 것을 알게 되었습니다. 추가 연구에 사용할 수 있는 쉽게 재현할 수 있는 환경도 제공하기를 바랍니다. 보이지 않는 작업 역학에 적응하기. 미래를 내다보면서 개방적이고 야심 찬 질문을 제기하는 자세한 보충 자료를 포함했습니다. 미래에 어떤 유형의 환경 변화가 발생할지에 대한 사전 지식 없이 적응 제어 정책을 어떻게 구축할 수 있을까요? 놀랍게도, 대격변적 카트폴 작업에 대한 ARZ의 예비 결과는 진화(훈련) 중에 부분적 관찰 가능성과 동적 액추에이터 노이즈를 주입하면 비정상적 작업 역학 S2에 대한 일반적인 대용물로 작용할 수 있음을 시사합니다. 예비 작업에서, 우리는 이것이 검색(진화) 중에 경험하지 못했던 새로운 작업 역학에 적응할 수 있는 정책의 출현을 뒷받침한다는 것을 발견했습니다. 이것은 우리의 LSTM 기준선에서는 불가능했습니다. 사실이라면, 이는 작업 환경 역학에 대한 완전한 사전 지식 없이도 능숙한 제어 정책을 진화시킬 수 있음을 의미하기 때문에 중요할 것입니다. 따라서 정확한 물리 시뮬레이터에 대한 필요성이 완화됩니다. 향후 작업에서는 이 예비적 발견의 견고성을 조사할 수 있습니다. 저자 기여 SK와 ER이 프로젝트를 주도했습니다. ER과 JT가 프로젝트를 구상하고 주요 고문으로 활동했습니다. 모든 저자가 방법론에 기여했습니다. SK, MM, PN, DP가 진화 실험을 실행했습니다. XS가 기준선을 실행했습니다. MM과 DP가 알고리즘을 분석했습니다. SK, DP, MM이 논문을 썼습니다. 모든 저자가 논문을 편집했습니다. 감사의 말 기술 토론을 위해 Wenhao Yu, Chen Liang, Sehoon Ha, James Lee, Google Brain Evolution 및 AutoML 그룹에 감사드립니다. 물리 시뮬레이션 조언을 위해 Erwin Coumans에게 감사드립니다. 코드 기여를 위해 Erwin Coumans, Kevin Yap, Jacob Budzis, Heng Li, Kaiyuan Wang, Ryan Gillard에게 감사드립니다. 지침과 지원을 위해 Quoc V. Le, Vincent Vanhoucke, Ed Chi, Erik Goodman에게 감사드립니다. 참고문헌 [1] M. Hessel, H. Soyer, L. Espeholt, W. Czarnecki, S. Schmitt 및 H. van Hasselt, &quot;다중 작업 심층 강화 [...]].&quot; AAAI, 2019. [2] S. Kelly, T. Voegerl, W. Banzhaf 및 C. Gondro, &quot;진화하는 계층적 메모리 예측 [...]],&quot; Genet. Program. 영어: Evolvable Mach., 2021. [3] E. Real, C. Liang, DR So, and QV Le, &quot;AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,&quot; ICML, 2020. [4] &quot;Unitree Robotics.&quot; [온라인]. 사용 가능: http://www.unitree.cc/ [5] JR Koza and MA Keane, &quot;비선형 최적 제어 전략의 유전적 번식 [...]],&quot; 시스템 분석 및 최적화, 1990. [6] Y. Dhebar, K. Deb, S. Nageshrao, L. Zhu, and D. Filev, &quot;해석 가능한 AI 정책을 향하여 [...]],&quot; IEEE. Trans. Cybern., 2022. [7] W. Yu, J. Tan, CK Liu, G. Turk, &quot;미지의 것에 대비하기: 보편적 정책 학습 [...]&quot; RSS, 2017. [8] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, P. Abbeel, &quot;[...]] 전송을 위한 도메인 무작위화&quot; CORR, 2017. [9] A. Cully, J. Clune, D. Tarapore, J.-B. Mouret, &quot;동물처럼 적응할 수 있는 로봇&quot; Nature, 2015. [10] X. Song, Y. Yang, K. Choromanski, K. Caluwaerts, W. Gao, C. Finn, J. Tan, &quot;빠르게 적응할 수 있는 다리 로봇 [...]&quot; IROS, 2020. [11] X. Song, W. Gao, Y. Yang, K. Choromanski, A. Pacchiano 및 Y. Tang, &quot;ES-MAML: 간단한 헤센 없는 메타 학습&quot;, ICLR, 2020. [12] W. Yu, J. Tan, Y. Bai, E. Coumans, 및 S. Ha, &quot;메타 전략 최적화를 통한 빠른 적응 학습&quot;, IEEE Robot. Autom. Lett., 2020. [13] C. Finn, P. Abbeel, 및 S. Levine, &quot;심층 네트워크의 빠른 적응을 위한 모델 독립적 메타 학습&quot;, ICML, 2017. [14] A. Kumar, Z. Fu, D. Pathak, 및 J. Malik, &quot;RMA: 다리가 있는 로봇의 빠른 운동 적응&quot;, CORR, 2021. [15] E. Najarro 및 S. Risi, &quot;무작위 네트워크에서 Hebbian 가소성을 통한 메타 학습&quot;, CORR, 2020. [16] D. Floreano 및 J. Urzelai, &quot;온라인 자기 조직화 및 행동 적합성을 갖춘 진화 로봇&quot;, Neural Networks, 2000. [17] T. Anne, J. Wilkinson, 및 Z. Li, &quot;불확실성이 있는 빠른 적응 운동을 위한 메타 학습 [...]]&quot;, IROS, 2021. [18] A. Li, C. Florensa, I. Clavera, and P. Abbeel, &quot;계층적 강화 학습을 위한 하위 정책 적응&quot;, ICLR, 2020. [19] JX Wang, &quot;자연 및 인공 지능에서의 메타 학습&quot;, Current Opinion in Behavioral Sciences, 2021. [20] M. Brameier and W. Banzhaf, 선형 유전 프로그래밍. Springer, 2007. [21] JR Koza, 유전 프로그래밍 II: 재사용 가능 프로그램의 자동 발견. 미국 매사추세츠주 케임브리지: MIT 출판부, 1994. [22] K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, &quot;빠르고 엘리트적인 다목적 유전 [...]&quot;, IEEE Trans. Evol. Comput., 2002. [23] E. Real, A. Aggarwal, Y. Huang, QV Le, &quot;이미지 분류기 아키텍처 검색을 위한 정규화된 진화,&quot; AAAI, 2019. [24] H. Mania, A. Guy, B. Recht, &quot;정적 선형 정책의 간단한 무작위 검색은 경쟁적입니다 [...]],&quot; NeurIPS, 2018. [25] K.-H. Lee, O. Nachum, T. Zhang, S. Guadarrama, J. Tan, W. Yu, &quot;PI-ARS: 가속화된 진화 학습 [...]],&quot; IROS, 2022. [26] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, &quot;근위 정책 최적화 알고리즘,&quot; CoRR, 2017. [27] E. Heiden, D. Millard, E. Coumans, Y. Sheng 및 GS Sukhatme, &quot;NeuralSim: Augmenting Differentiable [...]],” ICRA, 2021. [28] RS Sutton 및 AG Barto, 강화 학습: 소개. 미국 매사추세츠주 케임브리지: Bradford Book, 2018. [29] S. Sueda, &quot;분석적으로 미분 가능한 관절형 [...]],” 2021. [온라인]. 사용 가능: https://github.com/sueda/redmax/blob/master/notes.pdf [30] E. Marder 및 D. Bucher, &quot;중앙 패턴 생성기 및 리드미컬한 움직임 제어,&quot; Current Biology, 2001. [31] R. Gillard, S. Jonany, Y. Miao, M. Munn, C. de Souza, J. Dungay, C. Liang, DR So, QV Le 및 E. Real, &quot;자동 머신 러닝의 통합 함수 해싱,&quot; arXiv, 2023. [32] KO Stanley 및 R. Miikkulainen, &quot;토폴로지 증강을 통한 신경망 진화,&quot; Evolutionary Computation, 2002. [33] J. Bergstra 및 Y. Bengio, &quot;하이퍼 매개변수 최적화를 위한 무작위 검색,&quot; JMLR, 2012. [34] D. Hafner, J. Davidson 및 V. Vanhoucke, &quot;Tensorflow 에이전트: Tensorflow에서의 효율적인 배치 강화 학습,&quot; CORR, 2017. [35] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, W. Zaremba, &quot;OpenAI Gym,&quot; 2016. 보충 자료 기존 인구 신규 인구 S1. 방법 추가 세부 정보 def Start Episode(): s1 = 0.def GetAction(state): so s7 state [0] def CADF(memory): vabs(memory [2]) def StartEpisode(): s1 = 0.def GetAction(state): s0 s7state [0] def StartEpisode(): s6 1.def GetAction(state): v2sin(state [4]) def CADF(memory): v3 = s2 memory [0] def StartEpisode(): s6 1.def GetAction(state): v2 sin(state [4]) v3 = s2 memory [0] def CADF(memory): vabs(memory [2]), def CADF(memory): def StartEpisode(): v0 = -2.def GetAction(state): v2s1s3 + sdef CADF(memory): ss5 memory [4] def StartEpisode(): v2 = -2.def GetAction(state): v2s1s3 + sdef CADF(memory): s0 s5 memory [4] 그림 S1: 교차 및 돌연변이를 통해 수정되어 새로운 집단을 생성하는 알고리즘 집단의 단순화된 예. 돌연변이 연산자의 전체 목록은 표 SC에 제공됩니다. 격변적 카트폴 작업 카트폴[28], [35]은 마찰이 없는 트랙을 따라 좌우로 움직이는 카트에 작동하지 않는 조인트로 막대가 연결된 고전적인 제어 작업입니다(그림 S3). 각 시간 단계 s(t)에서 시스템의 관찰 가능한 상태는 카트 위치(x), 카트 속도(x), 카트에 대한 막대 각도(0), 막대 각속도(ė)를 포함한 4개 변수로 설명됩니다. 우리는 카트에 힘 = [(-1, 1]을 가함으로써 시스템을 제어하는 문제의 연속 동작 버전을 사용합니다. 막대는 거의 수직으로 시작하고 목표는 넘어지는 것을 방지하는 것입니다. 막대가 수직에서 12도 이상 떨어지거나 카트가 중심에서 2단위 이상 이동하거나 시간 제약에 도달하면(1000개 시간 단계) 에피소드가 종료됩니다. 막대가 수직으로 유지되는 모든 시간 단계에 대해 (1 − | Overt | /12)2의 보상이 제공되며, 여기서 Overt는 수직 평면에 대한 막대 각도의 고정 참조입니다. 따라서 목표는 가능한 한 오랫동안 수직에 가깝게 막대를 균형 잡는 것입니다. Ꮎ Ꮎ A. 기준 세부 정보 증강 랜덤 검색(ARS): 우리는 [24]의 표준 구현을 사용하고 다음 간의 교차 곱에 대해 하이퍼파라미터를 조정했습니다. • 학습률: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5] • 가우시안 표준 편차: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5] 및 Tanh 비선형성을 갖는 은닉 계층 크기(32,32)의 2계층 MLP와 크기 32의 LSTM을 사용했습니다. 근접 정책 최적화(PPO): [26]의 표준 Mujoco 결과를 재생성하는 것으로 검증한 TF-Agents [34]의 표준 구현을 사용했습니다. 다음의 하이퍼파라미터를 변경했습니다.• nsteps(&quot;시퀀스 길이 수집&quot;): [256, 1024] 학습률: [5e-5, 1e-4, 5e-4, 1e-3, 5e-3]• 엔트로피 정규화: [0.0, 0.05, 0.1, 0.5] 공유 가치 네트워크를 사용하기 때문에 ReLU 비선형성을 갖춘 은닉 계층 크기(256, 256)의 2계층 MLP와 크기가 256인 LSTM을 사용했습니다.PPO의 성능이 상당히 떨어졌기 때문에(예: 4족 보행 작업에서 약 100의 보상만 얻음) 이 논문에서는 공간을 절약하기 위해 결과를 생략했습니다.B. 4족 보행 작업 각 방법에 대해 고유한 난수 시드로 20번의 독립적인 반복을 수행합니다.모든 반복은 수렴할 때까지 학습할 수 있습니다.NSGA-II는 각각 100과 1000의 부모 및 자식 인구 크기를 사용합니다. 검색 재시작이나 FEC가 활성화되지 않았습니다. 모든 프로그램에 포함할 수 있는 작업 세트는 표 S2에 나와 있습니다. ARS 실험의 경우 고유한 하이퍼 매개변수를 사용하여 36회 반복으로 구성된 하이퍼 매개변수 스윕을 실행합니다. 그런 다음 최상의 하이퍼 매개변수 구성을 사용하여 추가로 20회 반복을 실행합니다. 단계 보상 그림 S2: NSGA-II 검색 초기의 일반적인 파레토 전선. 점선 상자는 적합도 제약을 통해 효과적으로 제거된 정책을 보여줍니다. XX Χ 그림 S3: 상태 관찰 s(t)의 4개 변수가 있는 Cataclysmic Cartpole 작업에서 트랙 각도 변경의 그림. 0은 항상 막대와 트랙과 카트에 수직으로 달리는 선 사이의 각도를 나타내므로 균형을 유지하기 위한 원하는 값인 0입니다(Overt = 트랙 각도에 따라 변경되며 정책에서 직접 관찰할 수 없음). 1) 갑작스러움: 각 변경 매개변수의 갑작스러운 변경은 [200, 800], 그림 S4a에서 고유한 임의 시간 단계에서 발생합니다. 2) 연속적: 각 매개변수는 [200, 800], 그림 S4b에서 임의로 독립적으로 선택된 시작 및 종료 시간 단계가 있는 창에 따라 변경됩니다. ARZ 방법의 경우 고유한 임의 시드로 각 실험을 10번 반복합니다. ARS의 경우 고유한 하이퍼 매개변수를 사용하여 36번 반복하는 하이퍼 매개변수 스윕을 실행합니다. 각 경우에 최상의 검색 적합도를 가진 5번의 반복을 선택하고 각각에서 최상의 정책을 테스트합니다. 플롯은 각 작업의 각 정책에 대한 100번의 에피소드에 대한 평균 적합도를 보여줍니다. S2. 추가 실험: 대격변적 카트폴 A. 적응 갭 이 섹션에서는 무상태 정책(ARZ 및 MLP)을 사용하여 대격변적 카트폴 동역학이 적응 갭을 생성할 만큼 충분히 중요하다는 것을 확인합니다.무상태 정책(즉, 일반 정책)이 비정상적 환경에서 제어 정책을 적응시킬 수 없기 때문에 성능이 좋지 않을 때입니다.섹션 III-C에서 언급했듯이 진화된 알고리즘은 부분적으로 상태가 있기 때문에 적응할 수 있습니다.메모리 내용(sX, vX, mX 및 iX)은 에피소드의 시간 단계에 걸쳐 지속됩니다.표현은 정책이 메모리 내용을 지우고 GetAction() 함수의 시작 부분에서 상수를 다시 초기화하도록 강제하는 것만으로 무상태 알고리즘을 쉽게 지원할 수 있습니다(그림 2 참조).그림 S5는 연속적 변경 환경에서 모든 유형의 변경이 동시에(ALL) 발생할 때 무상태 기준선(MLP 및 ARZ 무상태)이 충분한 적합도(800)를 달성하지 못한다는 것을 나타냅니다. 이는 지속적인 변화 패러다임이 적응과 평생 학습을 연구하기에 적합한 도전적인 비정지 문제 환경을 제공한다는 것을 확인합니다. 갑작스러운 변화 과제(그림 S6)에서 MLP 기준선은 여전히 실패합니다. 놀랍게도 ARZ는 이러한 유형의 비정지성에서 성공하는 무상태 정책을 발견할 수 있습니다. 연산자 삽입 명령어 삭제 명령어 무작위화 명령어 무작위화 함수 상수 무작위화 매개변수 무작위화 dim 인덱스 무작위화 허용 함수 Prob GetAction() CADF() GetAction() CADF() 0.1.GetAction() CADF() GetAction() CADF() 에피소드 시작() 1.0.0.GetAction() CADF() GetAction() CADF() 0.0.설명 균일하게 샘플링된 줄 번호에 무작위로 생성된 명령어 삽입 균일하게 샘플링된 줄 번호에서 명령어 삭제 균일하게 샘플링된 줄 번호에서 명령어 무작위화 모든 코드 줄을 무작위로 섞기 균일하게 샘플링된 명령어에서 균일하게 샘플링된 상수의 분수(0.2)를 수정합니다. 각 상수에 대해 N(0, 0.052)에서 샘플링된 노이즈를 추가합니다. 균일하게 샘플링된 명령어에서 균일하게 샘플링된 매개변수를 무작위로 지정 균일하게 샘플링된 명령어에서 균일하게 샘플링된 dim 인덱스의 분수(0.2)를 무작위로 지정합니다. 선택한 각 dim 인덱스는 [0, dim)에서 균일하게 샘플링된 새 정수로 설정됩니다. 여기서 dim은 참조되는 메모리 구조의 크기입니다. 표 S1: 돌연변이 연산자. 확률 열은 각 연산을 적용할 상대 확률을 나열합니다. 예를 들어, 삭제 명령어 op는 삽입 명령어보다 두 배 더 자주 적용됩니다. 정규화된 값 힘 감쇠 트랙 각도 단계 힘(a) 갑작스러운 감쇠 트랙 각도 평균 보상 고정 힘 정책 유형[훈련 작업]: ARZ[모두] MLP[모두] 감쇠 트랙 각도 모두 그림 S6: ARZ는 갑작스러운 변경 작업에서 성공하는 상태 없는 정책을 발견할 수 있습니다. 플롯은 Cartpole 갑작스러운 변경 작업에서 상태 없는 기준선에 대한 테스트 결과를 보여줍니다. 범례는 정책 유형과 검색 작업을 나타냅니다. &quot;정지&quot;는 표준 카트폴 작업인 반면 &quot;힘&quot;, &quot;감쇠&quot; 및 &quot;트랙 각도&quot;는 이러한 매개변수에서만 연속적으로 변경되는 카트폴을 나타냅니다(섹션 IV-B 참조). &quot;모두&quot;는 모든 변경 매개변수가 잠재적으로 동시에 변경되는 경우입니다. Y축은 각 작업의 에피소드에 대한 평균 보상입니다. 논의는 섹션 S2-A를 참조하십시오. 평균 보상 정규화된 값 H HStep(b) 연속 그림 S4: 일반적으로 무작위로 생성된 변경 일정. 힘 정책 유형[훈련 작업]: ARZ[모두] 감쇠 모든 MLP[모두] 정지 트랙 각도 그림 S5: 모든 유형의 변경이 동시에(모두) 발생할 때 상태 없는 기준선은 충분한 적합도(≈ 800)를 달성하지 못합니다. 플롯은 Cataclysmic Cartpole 연속 변경 작업에서 상태 없는 기준선에 대한 테스트 결과를 보여줍니다. 범례는 정책 유형과 검색 작업을 나타냅니다. &quot;정지&quot;는 표준 카트폴 작업인 반면 &quot;힘&quot;, &quot;감쇠&quot; 및 &quot;트랙 각도&quot;는 이러한 매개변수에서만 연속적으로 변경되는 카트폴을 나타냅니다(섹션 IV-B 참조). &quot;모두&quot;는 모든 변경 매개변수가 잠재적으로 동시에 변경되는 경우입니다. Y축은 각 작업에서 100회 에피소드의 평균 보상입니다. 논의는 섹션 S2-A를 참조하십시오. B. 격변하는 카트폴에서 보이지 않는 역학에 적응하기 어떤 유형의 환경 변화가 발생할지에 대한 사전 지식 없이 어떻게 적응 제어 정책을 구축할 수 있습니까? 놀랍게도 ARZ의 경우 진화(훈련) 중에 부분적 관찰 가능성과 동적 액추에이터 노이즈를 주입하면 비정지 작업 역학에 대한 일반적인 대용물 역할을 하여 진화 중에 경험하지 못했던 새로운 작업 역학에 적응할 수 있는 정책의 출현을 뒷받침할 수 있습니다. 이는 LSTM 기준선에서는 불가능했습니다. 이는 향후 작업에서 더 많은 주목을 받을 만한 중요한 발견입니다.이는 작업 환경 역학에 대한 완전한 사전 지식 없이도 능숙한 제어 정책을 잠재적으로 개발할 수 있음을 의미하므로 정확한 물리 시뮬레이터에 대한 필요성을 완화할 수 있기 때문입니다.Cataclysmic Cartpole(힘, 감쇠, 추적 각도)의 비정상 작업에 사용할 수 있는 시뮬레이터가 없다고 가정하더라도 이러한 변화에 대처하는 정책을 여전히 구축할 수 있을까요?정책의 관점에서 환경의 물리에 대한 변화는 (1) 센서 관찰의 의미를 변경합니다(예: 수직에 해당하는 극 각도 센서 값(0)이 갑자기 변경됨).및/또는 (2) 동작의 효과를 변경합니다(예: 특정 액추에이터 값이 갑자기 카트의 궤적에 훨씬 더 큰 영향을 미침).이러한 불확실성에 대한 정책을 준비하기 위해 비정상 노이즈를 동작에 적용하여 정책을 개발하고 부분적으로 관찰 가능한 관찰 공간을 도입합니다. 구체적으로, 우리는 다음을 추가하기 위해 작업을 수정합니다.• 액추에이터 노이즈: 각 작업 값 v는 v = v+n이 되도록 수정됩니다.여기서 n은 그림 S4b의 연속 변경 일정에 따라 [-2, 2]의 평균을 갖는 가우시안 분포에서 샘플링됩니다.부분적 관찰성: 위치 상태 변수(카트 위치(x) 및 카트에 대한 폴 각도(0))는 상태 관찰을 정책에 전달하기 전에 0으로 설정됩니다.우리의 가설은 이것이 정책이 관찰과 작업에 덜 의존하도록 장려하고 결과적으로 작업이 미래 상태에 어떤 영향을 미칠지 예측하기 위해 더 강력하고 역동적인 내부 세계 모델을 구축할 수 있다는 것입니다.즉, 환경의 동적 전환 함수를 모델링해야 하는 압력이 더 큽니다.그림 S7에서 ARZ [PO + Act Noise]는 진화 중에 고정 작업 시뮬레이터(즉, 수정되지 않은 Cartpole 환경)를 사용하지만 위에서 설명한 대로 액추에이터 노이즈와 부분적 관찰성을 적용하는 ARZ 실험에 대한 테스트 결과를 보여줍니다. 놀랍게도, 이러한 진화된 정책은 Cataclysmic Cartpole 환경에서 모든 비정상 작업에 대해 상당히 잘 적응할 수 있으며, 모든 작업에서 평균 보상 &gt; 700을 달성합니다. 동일한 검색 구성을 사용하여 ARS는 모든 비정상 작업에 대한 적응을 지원하는 LSTM 네트워크(LSTM [PO + Act Noise])에 대한 매개변수를 발견하지 못합니다. 요약하자면, 이 섹션에 제시된 예비 데이터는 검색 중에 정상 Cartpole 작업에 부분적 관찰 가능성과 액추에이터 노이즈를 추가하면 ARZ가 보이지 않는 비정상 작업에 적응할 수 있는 정책을 발견할 수 있음을 시사하는데, 이 방법은 LSTM 네트워크가 있는 ARS에는 효과가 없습니다. 이러한 결과에 대한 포괄적인 분석은 향후 작업에 맡깁니다. S3. CARTPOLE 알고리즘 분석 여기서는 그림 11에 제시된 알고리즘을 분석합니다.평균 보상 방정식 (2)는 단순 선형 모델이므로 act+1을 명시적으로 작성할 수 있습니다.정책 유형 [학습 작업]: ARZ [PO + Act 노이즈] LSTM [PO + Act 노이즈] 합계:정지력 감쇠 트랙 각도 모두 그림 S7: ARZ는 보이지 않는 작업에 적응하는 정책을 발견할 수 있습니다.플롯은 Cartpole 연속 변경 작업에서 정책을 적응시키기 위한 진화 후 테스트 결과를 보여줍니다.범례는 정책 유형과 검색 작업을 나타냅니다.[모두]는 정책이 진화 중에 모든 작업에 노출되었음을 나타냅니다.[PO + Act 노이즈]는 정책이 고정 작업에서 부분적 관찰 가능성과 action 노이즈와 함께 진화했지만 동적 변경 작업은 테스트할 때까지 보이지 않았음을 나타냅니다.Y축은 각 작업에서 100회 에피소드의 평균 보상입니다.토론은 섹션 S2-B를 참조하세요. # sX: 주소 X의 스칼라 메모리. # obs: 벡터(x, theta, ✗_dot, theta_dot]. %23 a, b, c: 고정 스칼라 매개변수. #3 V, W: 4차원 벡터 매개변수. def GetAction(obs, action): s0 = a * s2 + action s1 = s0s1 + b * action + dot(V, obs) s2 = s0 + c * saction = s0 + dot(obs, W) return action act, =ẤT .Un .Zo+WT n .Sn +Ãª .ΣỮn-i ‚ Ñ.i=· Si⋅ 이 표현식의 연속 한계를 취할 때, sn+1 벡터는 시간 단계 n + 1에서 관찰 벡터와 시간 단계 n에서 작업 값을 구성하여 얻어진다는 미묘한 점이 있습니다. 그럼에도 불구하고 s를 다음과 같이 다시 정의하는 데 주의할 수 있습니다. 영어: 동시 구성 요소로 구성되고 연속 한계에서 다음 형식을 취하는 표현식에 도달합니다.act(t) =c+w.Ut .u+pT · s(t) +vT .du Ut-u.Ps(u). (3) U = Id로 설정하면 이 표현식이 PID 컨트롤러와 유사한 모델로 직접 축소됩니다.act(t) =(c+wu) +pT · s(t) + (vT · P) · .* dus(u). 방정식 (3)을 다시 쓰는 유익한 방법은 U의 고유값 e-wk를 명시적으로 사용하는 것입니다. 이 방정식은 d act(t) =c+ cke -wkt +ps(t) d + S로 다시 매개변수화할 수 있습니다.그림 S8: 모든 매개변수가 연속 변경되는 Cataclysmic Cartpole 작업에서 진화한 샘플 상태 있는 동작 함수(그림 9의 ARZ [All]). Python으로 표시된 코드. 이 그림은 그림 11을 반복한 것입니다. n 단계에서 μ = s0, v = s1 및 = s2의 값을 벡터 Zn = : (μn, Vn+1, εn)T로 구성하고 n + 1 단계에서 관찰 벡터와 n 단계에서 동작을 연결하여 상태 벡터 Sn+1: Sn+1 = (xn+1, On+1, xn+1, On+1, actn)T로 구성하면 n 단계에서 이러한 어큐뮬레이터의 값을 다음과 같이 다시 쓸 수 있습니다. Sn+1 = concat(obsn+1, actn) Zn+1 = Ũ · Zn + P · Sn+1, actn+1 = .ÃT · Zn+1+WT · Sn+1 · (2) 이 공식에서 사용된 특정 변수는 다음과 같이 그림 11의 매개변수에 매핑됩니다. du e-wk (tu)s(u). 여기서 표현식은 PID 컨트롤러의 간단한 일반화이며, 액션을 계산하는 데 히스토리의 가중치 1 큐뮬런트만 사용됩니다. 이제 서로 다른 감소율을 가진 다수의 큐뮬런트를 활용할 수 있습니다. A. 기준선 S4. 복잡도 비교 섹션 S1에서 언급했듯이 MLP 및 LSTM 네트워크는 논문의 적응 작업을 위한 기준선으로 ARS를 사용하여 학습되었습니다. 매개변수 수에 대한 행렬 변수와 부동 소수점 연산에 대한 행렬 곱셈만 계산하여 각 모델에 필요한 매개변수 수와 부동 소수점 연산의 하한을 추정할 수 있습니다. 여기에는 비선형성 적용이나 벡터 성분별 곱셈과 같은 바이어스 변수와 비행렬 곱셈 연산이 무시됩니다. 입력 차원 din, 출력 차원 dout 및 내부 차원 d가 주어지면 단계당 MLP 및 LSTM 모델에 대한 매개변수 수와 부동 소수점 연산은 다음과 같습니다. 0a Ū FLOPS MLP FLOPSLSTM 2 × ParamsMLP &gt; 2d(din + d + dout) 2 × Params LSTM &gt; 2d(4din + 4d + dout) (4) (5) = 0a 0 C a (1 + c) B. 사족보행 다리 부러뜨리기 P = Vi V2 V3 VcVi CV2 CV3 cVb+a+be+c Ã = (1, 0, 0), W = (W1, W2, W3, W4, 0)T. 발견된 모델의 매개변수의 수치적 값은 a = -0.549, b = -0.673, c = 0.082, V = (-1.960, -0.7422, 0.7373, -5.284), W = (0.0, 0.365, 2.878, 2.799)로 주어진다. 방정식 (2)는 선형 순환 모델로 볼 수 있는데, 여기서 Zn은 모델의 내부 상태이다. n번째 단계의 동작은 내부 상태, 관찰 벡터 및 이전 단계의 동작 값의 선형 함수로 얻어진다. 발견된 특정 모델의 흥미로운 측면은 구축에 의한 행렬 Ũ가 고유값 0, 1 및 a(1+ c) ≈ −0.594를 갖는다는 것이다. 그림 1에 제시된 알고리즘은 16 + 16 × 37 =개의 매개변수를 포함하고 단계당 최대 54 × 37 + 82 = 2080개의 부동 소수점 연산을 실행합니다.여기서 모든 &quot;if&quot; 문이 통과한다고 가정하고 부동 소수점 또는 부동 소수점 쌍에 작용하는 모든 연산을 계산했습니다.작업의 입력 및 출력 차원은 37과 12이고, ARS로 훈련된 모델의 내부 차원은 d = 32입니다.위의 공식을 사용하면 MLP 모델이 2592개 이상의 매개변수를 포함하고 5184개 이상의 FLOP를 사용한다는 것을 알 수 있습니다.한편 LSTM 모델은 9216개 이상의 매개변수와 18432개의 FLOPS를 사용합니다.C. 대격변의 카트폴 그림 9에 제시된 알고리즘은 11개의 매개변수를 포함하고 단계당 25개의 부동 소수점 연산을 실행합니다.작업의 입력 및 출력 차원은 4와 1이고, 신경망의 내부 차원은 d = 32입니다. MLP 모델은 1184개 이상의 매개변수를 포함하고 2368개 이상의 FLOP를 사용합니다. LSTM 모델은 4604개 이상의 매개변수와 9280개 이상의 FLOPS를 사용합니다. D. 논의 ARZ 정책의 효율성은 시스템의 두 가지 특성에서 비롯됩니다. 첫째, 많은 유전 프로그래밍 방법과 마찬가지로 ARZ는 간단한 알고리즘에서 시작하여 정책을 구축하고 작업 환경과의 상호 작용을 통해 점진적으로 복잡성을 추가합니다(예: [5], [20]). 이는 행동 추론의 계산 비용이 진화 초기에는 낮고 더 복잡한 구조가 적합도 이득을 제공함에 따라 증가한다는 것을 의미합니다. 즉, 검색은 점진적인 성장에 의해 제한됩니다. 둘째, ARZ에서 돌연변이는 명령을 삽입하는 것보다 명령을 제거할 가능성이 두 배 더 높습니다(표 S1 참조). 이는 개체군에 정규화 효과가 있는 것으로 밝혀졌습니다[3]. S5. 검색 공간 추가 세부 정보 보충 표 S2는 검색 공간의 연산 집합을 설명합니다. 사족 로봇 도메인에는 행렬 연산이 사용되지 않았다는 점에 유의하세요. 표 S2: Ops 어휘. s, v 및 M은 각각 스칼라, 벡터 및 행렬을 나타냅니다. 초기 알파벳 문자(a, b 등)는 메모리 주소를 나타냅니다. 중간 알파벳 문자(예: i, j 등)는 벡터/행렬 인덱스(&quot;인덱스&quot; 열)를 나타냅니다. 그리스 문자는 상수(&quot;상수.&quot; 열)를 나타냅니다. U(a, B)는 [a, b]의 균일 분포에서 샘플을 나타냅니다. N(μ, σ)는 평균 μ와 표준 편차 σ를 갖는 정규 분포와 유사합니다. 1 x는 집합 X에 대한 지시자 함수입니다. 예: &quot;M(i,j) = u(a,B)&quot;는 &quot;[a,ß]의 균일 난수 분포에서 샘플링한 값을 주소 a에 있는 행렬의 i,j번째 항목에 할당&quot;하는 연산을 설명합니다. Ор ID 코드 예제 입력 인수 주소 상수 출력 인수 주소 인덱스 설명(캡션 참조) /유형 / 유형 OPno_op OPs2=s3+sa,b / 스칼라 c/ 스칼라 OPs4=s0-sa,b / 스칼라 c/ 스칼라 OPs8=s5*sa,b / 스칼라 c/ 스칼라 OPs7=s5/sa,b/스칼라 c/ 스칼라 OPs8=abs(s0) a / 스칼라 b/ 스칼라 OPs4=1/sa / 스칼라 b/ 스칼라 OPs5=sin(s4) a / 스칼라 b/ 스칼라 OPs1=cos(s4) a / 스칼라 b/ 스칼라 OPs3=tan(s3) a / 스칼라 b/ 스칼라 OPs0=arcsin(s4) a 스칼라 b/ 스칼라 OPs2 arccos(s0) a / 스칼라 b/ 스칼라 OPs4-arctan(s0) a / 스칼라 b/ 스칼라 OPs1=exp(s2) a/ 스칼라 b/ 스칼라 OPs0=log(s3) a/ 스칼라 b/ 스칼라 OPs3=헤비사이드 (그래서) a 스칼라 b/ 스칼라 OPv2=헤비사이드 (v2) a/ 벡터 b/ 벡터 OPm7=헤비사이드 (m3) a/ 행렬 b/ 행렬 OPv1-s7*va,b/ sc,vec c/ 벡터 OPv1=bcast (s3) a/ 스칼라 b/ 벡터 OPOPv5=1/vs0=norm(v3) a/ 벡터 b/ 벡터 a 스칼라 b/ 벡터 OPv3=abs (v3) a/ 벡터 b/ 벡터 OPv5=v0+va,b/ 벡터 c/ 벡터 OPv1 v0-va,b/ 벡터 c/ 벡터 OPv8=v1*va,b/ 벡터 c/ 벡터 OPv9=v8/va,b/ 벡터 c/ 벡터 OPs6 dot (v1,v5) a,b / 벡터 c / 스칼라 OPm1=outer (v6,v5) a,b / 벡터 c / 행렬 OPOPm1=s4*mm3=1/ma,b/sc/mat c / 행렬 a / 행렬 b / 행렬 OPv6 dot (m1, v0) a,b / mat/vec c / 벡터 OPm2=bcast (v0, axis=0) a / 벡터 b / 행렬 OPm2=bcast (v0, axis=1) a / 벡터 b / 행렬 OPs2=norm (m1) a / 행렬 b / 스칼라 OPv4=norm(m7, axis=0) a / 행렬 b / 벡터 OPv4 norm (m7, axis=1) a / 행렬 b / 벡터 [표는 다음 페이지에 계속됩니다.] Sc Sasb Sc Sa Sb Sc = Sa Sb Sc = Sa/8b Sb = |sa| 8b = 1/8a 86 = sin(sa) 8b = cos(Sa) S₁ = tan(sa) Sb = arcsin(sa) Sb = arccos(sa) Sarctan(sa) 8b = e³a Sb = log sa Sb = 1 R+ (Sa) √(i) = 1R+ (√(²)) Vi M(i,j) = 1R+ (M(i,j)) \i,j บann. = Sa Üb v (i) = Sa Vi (i) = 1/σ (i) Vi 8b = |va| (i) = |v (i)| Vi vc = va + √b vc = va — Ub √ (i) = √(i) (i) Vi √ (i) = √(i) / 8c = JT Jb Mc = va v Mc = sa Mb Vi M(i,j) = 1/M(i,j) \i, j vc = Ma vb M(i,j) = √(i) Vi, j M(j, i) = √(i) Vi, j Sb = || 엄마|| (i) = |M(i)| Vi √(i) = |(i)|\j 표 S2: Ops 어휘(계속) Op ID 코드 예제 OPm9-transpose(m3) a / 행렬 OPm1=abs(m8) a / 행렬 b / 행렬 OPm2=m2+ma,b / 행렬 c / 행렬 OPm2=m3-ma,b / 행렬 c / 행렬 OPOPm3=m2*mm4=m2/ma,b / 행렬 c / 행렬 a,b / 행렬 c / 행렬 OPm5 matmul(m5,m7) a,b 행렬 c / 행렬 OPs1=minimum(s2, s3) a,b / 스칼라 c / 스칼라 OPv4=minimum(v3, v9) a,b / 벡터 c / 벡터 OPOPm2=minimum(m2, m1) s8=maximum(s3, s0) a,b / 행렬 c / 행렬 a,b 스칼라 c / 스칼라 OPv7=최대(v3, v6) a,b/벡터 c/벡터 OPm7=최대(m1, mo) a,b 행렬 c/행렬 OPs2=평균(v2) a/벡터 b/스칼라 OPs2=평균(m8) a/행렬 b/스칼라 OPv1-평균(m2, 축=0) a/행렬 b/벡터 OPv3-std(m2, 축=0) a/행렬 b/벡터 OPs3=std(v3) a/벡터 b/스칼라 OPs4=std(m0) a/행렬 b/스칼라 OPs2=Ca/스칼라 OPv3 [5]=Ca/벡터 i OPm2 [5,1]=Ca/행렬 i, j OPs4=균일(C2, C3) α, β a/스칼라 OPm2=ma/행렬 b/행렬 OPv2=vOP12-a/벡터 a/인덱스 OPv2 거듭제곱(v5,v3) a,b / 벡터 입력 인수 주소 상수 / 유형 출력 인수 주소 / 유형 인덱스 b / 행렬 b / 벡터 b / 인덱스 c / 벡터 설명(캡션 참조) Mb = |MT| M(i,j) = |M(i,j) | Vi, j M(i,j) M(i,j) Mc = Ma + M₁ McMa M₁ = M(i,j) M(i,j) Vi, j M(i,j)/M(i,j) Vi, j Mc = Ma Mb Sc = min(sa, b) √(i) = min(ʊ(i), (i)) Vi M(i,j) = min(M(i,j), M(i,j)) \i, j Sc max(sa, Sb) 7(i) = max(7ái), 7) Vi = max(M(i,j), M(i,j)) \i, jv (i) Sb 평균 (a) Sb 평균 (Ma) = 평균 (M()) Vi = stdev(M(i)) Vi S₁ = stdev (va) Sbstdev(Ma) Sa = Y √(i) =M(i,j) = Y Sa = u(a, ẞ) M₁ = Ma √₁ = va i₁ = ia (i) = 거듭제곱 (σ(i), (i)) Vi M(i,j) OPv3 m2 [,1] a,b 행렬, 인덱스 c/ 벡터 vc = M (·,jb) OPv3 m2 [1,] a,b 행렬, 인덱스 c/ 벡터 OPs3 m2 [1,5] a,b,c / m,i,id / 스칼라 OPs3=v2 [5] a,b 벡터, 인덱스 c/ 스칼라 OPv3=a / 벡터 OPs5=OP12=OPv2=sqrt(v5) a / 벡터 OPv2 거듭제곱 (v5,2) a / 벡터 b/ 벡터 OPOPOPOPOPOPs1=sum(v5) s5=sqrt(s1) s3=s0*s2+ss2=s4*Cm2 [1,]=vm2[,1]=va / 벡터 b/ 스칼라 a / 스칼라 b/ 스칼라 a, b, c/ 스칼라 d/ 스칼라 a/ 스칼라 b/ 스칼라 a/ 벡터 b/ 행렬 a/ 벡터 b/ 행렬 OPi3 = size(m1, 축=0) -a/ 행렬 b/인덱스 OPiOPsize(m1, 축=1) -13 len(v1) -a/ 행렬 b/인덱스 a/ 벡터 b/인덱스 OPs1 v0[3] * v1[3] + sa, b, c, dv, v, s, ie/ 스칼라 OPs3=dot(v0[:5], v1[:5]) a, b, c / v, s, id/ 스칼라 a/ 스칼라 a / 인덱스 b/ 벡터 √₁ = M(ib,.) Sd = M(b, jc) = √(2b) Sc = บี. =8a=ia =√(i) = sqrt(v(i)) Vi(i) = 거듭제곱((i), 2) Vi 8b = 합(v(i)) Vi SdSa8b + sc Sb Sbr(i,.) Mv = va Mε,j) = va ·(·,j) ib = 크기(M(i)) -ib = 크기(M())-i₁ = 길이(va) -Se Va * (id) + Sc 8d = √(:ic+1)(:ic+1) a
