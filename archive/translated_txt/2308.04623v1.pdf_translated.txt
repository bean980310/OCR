--- ABSTRACT ---
Benjamin Spector¹ Chris Re¹ 대규모 언어 모델(LLM)의 최근 발전은 다양한 기능을 보여줍니다. 소규모 배치, 온디바이스 시나리오에서 LLM 추론을 가속화하기 위해 새로운 알고리즘인 단계적 추측 디코딩을 제안합니다. 추측 디코딩에서 이전 작업을 개선하여 소규모 배치 추론의 낮은 산술 강도를 해결합니다. 먼저 추측 배치를 트리로 재구성하여 생성 비용을 줄이고 배치당 예상 토큰을 늘립니다. 둘째, 추측 디코딩의 두 번째 단계를 추가합니다. 이를 모두 적용하면 출력 품질을 완벽하게 유지하면서 762M 매개변수 GPT-2-L 모델로 단일 배치 디코딩 대기 시간을 3.16배 줄입니다. 1.
--- METHOD ---
ologies(Kingma &amp; Ba, 2014)와 방대한 양의 데이터(Halevy et al., 2009; Gao et al., 2020; Kocetkov et al., 2022)는 자연어 처리(Brown et al., 2020), 기계 번역(Raffel et al., 2020), 코드 합성(Chen et al., 2021) 및 그 이상(OpenAI, 2023)과 같이 다양한 분야에서 응용 프로그램을 위한 길을 열었습니다. 그러나 이러한 흥미로운 진전에는 고유한 시스템 수준의 과제가 수반됩니다. LLM이 더욱 강력해짐에 따라 계산 요구 사항도 함께 증가하여 종종 추론을 위해 상당한 클라우드 리소스가 필요했습니다(Sheng et al., 2023). 이러한 요구 사항은 많은 잠재적 응용 프로그램, 특히 &#39;미국 캘리포니아주 스탠포드 대학교 컴퓨터 과학과. Benjamin Spector에게 문의<bfs@stanford.edu> . 미국 하와이 호놀룰루에서 열린 제40회 국제 머신러닝 컨퍼런스 회의록. PMLR 202, 2023. 저작권 2023, 저자(들). 저지연 응답(Wang et al., 2023) 또는 데이터 프라이버시가 가장 중요한 응답(Carlini et al., 2021). 저희 논문은 산술 강도가 낮아 컴퓨팅 활용도가 낮은 LLM에 대한 로컬(소규모 배치) 추론을 가속화하여 이러한 과제를 해결합니다. 저희는 이 문제를 지연, 개인화, 프라이버시라는 세 가지 이유로 중요하다고 생각합니다. 첫째, 로컬 추론 지연을 최적화하면 실시간 상호 작용성과 반응성이 향상됩니다. 로컬 추론을 가속화하면 모델을 개별 사용자에게 로컬로 맞춤화할 수 있으므로 보다 개인화된 LLM 경험을 위한 문이 열립니다. 마지막으로 로컬 추론은 데이터가 사용자 기기를 벗어날 필요성을 없애므로 데이터 프라이버시를 향상시킵니다. 더 철학적으로, 우리는 LLM을 로컬에서 효율적으로 실행하는 방법이 제한된 계산 리소스를 가진 개인에게 권한을 부여함으로써 AI 민주화를 촉진한다고 믿습니다. 이 작업에서 우리는 (Leviathan et al., 2022; Chen et al., 2023)에서 도입한 추측 디코딩 기술을 기반으로 합니다. 이 기술은 빠르지만 정확하지 않은 초안 모델을 사용하여 오라클 모델과 이에 대한 일괄 쿼리를 예상하여 모델 분포를 완벽하게 유지하면서 순차적 디코딩 성능을 개선합니다. 이러한 기술은 처음에는 잘 확장되지만 초안 모델이 많은 순차적 토큰을 올바르게 추측할 확률이 기하급수적으로 작기 때문에 성능 향상이 빠르게 포화됩니다. 우리는 두 가지 주요 방법으로 추측 방법을 개선합니다. 1. 추측 배치를 가능한 토큰 시퀀스의 트리로 재구성하여 더 크고 더 높은 품질의 추측 배치를 더 빠르게 만듭니다. 2. 우리는 또한 초안 모델을 추측적으로 디코딩하여 성능을 더욱 개선합니다. 우리는 이러한 기술이 결정적 및 샘플링 기반 디코딩 모두에서 추측 디코딩의 성능을 크게 개선한다는 것을 발견했습니다.2. 배경 이 섹션에서는 자기 회귀 LLM 추론, GPU 성능 최적화의 핵심 원칙 및 LLM 추론 최적화의 이전 작업에 대한 간략한 개요를 제공합니다.ICML2.1에 대한 제출 및 서식 지정 지침.자기 회귀 LLM 추론 디코더 전용 LLM에서의 자기 회귀 생성은 일반적으로 두 단계로 나뉩니다.먼저, 프롬프트를 모델에서 실행하여 KV 캐시와 첫 번째 출력 로짓을 생성합니다.전체 프롬프트를 병렬로 처리할 수 있으므로 일반적으로 빠릅니다.두 번째 단계는 디코딩입니다.출력된 로짓에서 토큰을 선택하여 모델에 다시 공급하면 다음 토큰에 대한 로짓이 생성됩니다.원하는 수의 토큰이 생성될 때까지 이 과정을 반복합니다. 디코딩은 단일 토큰을 생성하기 위해 매번 전체 모델의 가중치를 컴퓨팅 유닛을 통해 스트리밍하여 순차적으로 수행해야 하므로, 이 두 번째 단계의 산술 강도(즉, 컴퓨팅의 FLOP/메모리 대역폭의 바이트)는 작은 배치로 실행할 때 매우 낮습니다.따라서 디코딩은 일반적으로 자기 회귀 생성에서 가장 비용이 많이 드는 부분입니다.(Leviathan et al., 2022) 2.2. GPU 최적화 최신 LLM 추론은 주로 대규모 행렬 곱셈으로 구성된 작업 부하의 고도로 병렬적인 특성으로 인해 GPU에서 가장 많이 수행됩니다.GPU는 다중 레벨 메모리 계층으로 지원되는 수천 개의 극히 작고 효율적인 코어로 구성됩니다.GPU에 대한 소규모 배치 LLM 추론을 최적화하는 핵심 과제는 극히 낮은 산술 강도를 처리하는 것입니다. 16비트 정밀도로 동작하고 배치 크기가 1인 디코딩의 산술 강도는 1입니다. 예를 들어, GPT-2 Large(762M 매개변수)의 참조 PyTorch(Paszke et al., 2019) 구현의 경우 추론에 약 1.GFLOP가 필요하지만 정지된 NVIDIA RTX 4090은 초당 150개 토큰만 달성하여 컴퓨팅 사용률이 0.13%에 불과합니다(NVIDIA, 2022). 이 처참한 성능은 주로 GPU 루프라인(Ofenbeck et al., 2014)으로 인해 발생하며, 이는 낮은 산술 강도에서 메모리 대역폭에 의해 제어됩니다(그림 1에서 시각화). 2.3. 추측적 디코딩 오늘날 추론을 가속화하기 위해 연구 중인 기술은 양자화(Dettmers 등, 2022; Frantar 등, 2022), 플래시 어텐션(Dao 등, 2022), 추측적 디코딩(Leviathan 등, 2022; Chen 등, 2023)과 같은 여러 가지가 있습니다. 이 섹션에서는 (Leviathan 등, 2022; Chen 등, 2023)에 설명된 대로 추측적 디코딩을 간략히 살펴보겠습니다. 추측적 디코딩은 이 작업의 주요 주제이기 때문입니다. 추측적 디코딩의 기본 아이디어는 더 작고 빠른 초안 모델을 사용하여 여러 토큰을 미리 디코딩한 다음 이를 단일 배치로 오라클 모델에 공급하는 것입니다. 초안 모델이 예측에 대해 옳았다면(대규모 모델도 동의함) 단일 성능(플롭스)4090 Roofline GPT-2 대형 대 Roofline(RTX 4090 GPU, 배치=1) 기준 GPT2-L 성능 단계적 추측 GPT2-L 성능 10-산술 강도(플롭스/바이트)그림 1: RTX 4090에서 단일 쿼리 GPT-2-L 추론에 대한 Roofline 플롯. 작은 배치 크기에서 추론은 완전히 메모리 대역폭에 제한됩니다. 따라서 이 플롯은 성능을 크게 높이는 유일한 방법이 추론의 산술 강도를 높이는 것임을 보여줍니다. 배치를 통해 상당한 메모리 대역폭과 토큰당 시간을 절약할 수 있습니다. 그러나 대규모 모델이 초안 모델에서 예측한 토큰을 거부하면 나머지 배치는 삭제되고 알고리즘은 자연스럽게 표준 토큰별 디코딩으로 돌아갑니다. 추측적 디코딩은 원래 분포에서 샘플링하기 위한 거부 샘플링 체계와 함께 수행될 수도 있습니다. 이는 대역폭이 병목 현상인 소규모 배치 설정에서만 유용합니다. 추측적 디코딩은 대역폭을 위해 컴퓨팅을 거래합니다. 추측적 디코딩이 매력적인 성능 엔지니어링 대상인 데에는 두 가지 주요 이유가 있습니다. 첫째, 모델 품질을 전혀 저하시키지 않습니다. 둘째, 순차적 실행을 병렬 실행으로 변환하여 성능을 얻기 때문에 추측적 디코딩이 제공하는 이득은 일반적으로 다른 방법과 직교합니다. (Leviathan et al., 2022) 3. 방법 추측적 디코딩에 두 가지 개선 사항을 적용합니다. 트리 구조 배치와 추가 단계입니다. 이러한 방법을 조합한 것을 &quot;단계적 추측적 디코딩&quot;이라고 합니다. 3.1. 트리 구조 배치 현재의 추측적 방법은 배치에 대한 단일 시퀀스를 예측합니다. 그러나 이는 대규모 배치 크기나 낮은 초안 모델 정렬에는 잘 확장되지 않습니다. 직관적으로, 두 모델이 토큰의 긴 연속 시퀀스에 대해 동의할 확률은 기하급수적으로 낮습니다.즉, 추측 디코딩은 산술 강도를 확장함에 따라 수익이 빠르게 감소합니다.샘플링 기준선 방법 결정적 상대 대역폭 Topk 1.1.ICML에 대한 제출 및 서식 지정 지침 추측 상대 대역폭 상대 대역폭 0.0.단계적 사양 0.0.기준선 추측 단계적 사양 토큰/초 샘플링 방법 결정적 Topk 토큰/초 토큰/초표 1: 추측 및 단계적 추측 디코딩 방법의 메모리 대역폭 소비(기준선 대비).저희의 접근 방식은 가능한 시퀀스의 트리를 동적으로 구축하는 것이며, 이는 배치당 예상 참 토큰 수 증가, 리프 노드 수 증가, 소규모 초안 모델에 대한 더 나은 병렬성이라는 세 가지 이점을 제공합니다. 첫째, 매우 긴 시퀀스의 끝에서 시작으로 계산을 재할당하고 모델에서 생성할 가능성이 두 번째 또는 세 번째로 높은 토큰을 고려하면 순진한 접근 방식에 비해 배치당 예상 토큰 수가 늘어납니다. 둘째, 배치를 생성하기 위해 초안 모델을 실행하는 비용은 표준 추측 디코딩에서 무시할 수 없습니다. 그러나 오라클 모델에 대한 배치를 구성하는 예측 트리에서 초안 모델은 트리의 내부 노드에서만 실행됩니다. 따라서 더 넓은 트리는 리프 노드의 수를 늘리고, 이는 더 많은 배치를 무료로 얻을 수 있음을 의미합니다. 더 넓은 트리의 세 번째 이점은 트리 전체에서 작은 모델의 실행을 병렬화할 수 있어 비용도 줄어든다는 것입니다. 한계 내에서는 트리의 깊이와 같은 수의 배치에서만 초안을 실행하면 됩니다. 이는 초안 모델이 일반적으로 더 작은 변환기 기반 모델이고 따라서 소규모 배치 추론에서도 메모리에 얽매이기 때문에 중요합니다. 트리 구조 배치를 구현하려면 약간의 주의가 필요합니다. 가장 간단한 접근 방식은 KV 캐시와 배치 내의 셀프 어텐션을 사용하여 교차 어텐션으로 디코딩하는 동안 셀프 어텐션을 분할하는 것입니다.그런 다음 위치 임베딩을 제어하고 트리에 따라 배치 셀프 어텐션 행렬을 인과적으로 마스킹하여 트리 구조 배치를 구성할 수 있습니다.마지막으로 전체 배치에 대한 새 KV 캐시를 별도로 저장한 다음 토큰을 샘플링한 후 적절한 슬라이스를 주 KV 캐시에 추가해야 합니다.3.2. 단계적 추측 현재의 추측 방법은 일반적으로 더 작은 LLM(Chen et al., 2023)인 단일 작은 모델을 초안으로 사용합니다.이 설정에서 초안 모델의 크기는 중요한 하이퍼파라미터입니다.더 큰 초안 모델은 오라클과 더 잘 일치하지만 비용이 더 많이 들고, 작은 모델은 품질이 낮은 추측 배치를 생성하지만 비용은 더 낮습니다. 실제로 오라클보다 약 15-20배 작은 초안 모델이 최적으로 보입니다.표 2: 기준선(비추측적), 표준 추측적 및 단계적 추측적 디코딩 방법을 사용한 상대적 성능(초당 토큰 수로 디코딩).그러나 순진한 추측적 디코딩에서 대량 배치를 조립하면 비용 구조가 반전되어 오라클보다 초안 모델에 더 많은 시간을 할애하게 됩니다.따라서 토큰 시퀀스를 생성하는 데 초안 모델을 가속화해야 하며, 추측적 디코딩은 이에 대한 자연스러운 솔루션이기도 합니다.따라서 우리는 접근 방식에서 초안 모델에 추측적 디코딩을 추가합니다.따라서 &quot;단계적 추측적 디코딩&quot;의 전반적인 방법은 트리 구조 배치가 있는 오라클, 초안 및 초안² 모델로 구성됩니다.4. 결과
--- EXPERIMENT ---
s, 우리는 세 가지 모델을 사용합니다: GPT-2-Large(762M) 매개변수 오라클 모델(Radford et al., 2019)은 Stack의 Python 하위 섹션(Kocetkov et al., 2022)에서 미세 조정되고, 동일한 모델에서 학습된 소규모(40M) 매개변수 GPT-2 초안 모델, Katz 백오프 트라이그램 모델(Katz, 1987)은 draft² 모델입니다. Katz 백오프 모델은 초안 모델을 1.5의 샘플링 온도에서 2시간 동안 실행하여 120M 토큰을 생성하여 생성되었습니다. 모든 평가는 최고급 소비자 하드웨어인 정지된 RTX 4090 GPU(NVIDIA, 2022)에서 수행되었습니다. 우리는 두 가지 대체 추론 방법에 대해 평가합니다. 첫째, 우리의 표준 기준선은 오라클을 사용한 간단한 토큰별 디코딩입니다. 둘째, 우리는 또한 (Leviathan et al., 2022)에서 제안한 추측적 디코딩에 대해서도 평가하여 개선 사항의 효과를 분리합니다. 평가를 위해 우리는 HumanEval(Chen et al., 2021)의 164개 프롬프트를 비추측적, 추측적 및 우리의 단계적 추측적 방법을 사용하고 결정적 및 topk 샘플링(Radford et al., 2019)을 모두 사용하여 실행했습니다. 배치 크기와 내부 휴리스틱의 세부 정보는 코드에서 찾을 수 있습니다. 우리는 먼저 각 방법의 메모리 대역폭 요구 사항을 측정하여 우리의 접근 방식이 상당한 대역폭을 절약하는지 확인했습니다. 우리는 표 1에 결과를 자세히 설명하는데, 이는 단계적 추측적 디코딩이 두 가지 대안 방법보다 상당히 적은 메모리 대역폭을 사용한다는 것을 보여줍니다. 둘째, 우리는 각 접근 방식에 대한 순차적 디코딩 처리량을 측정합니다. 결과는 표 2에 요약되어 있으며 그림 2에 자세히 나와 있습니다. 결정적 샘플링을 통해 구현은 기준 토큰/초 토큰/초에 비해 평균 3.16배의 성능 향상을 제공합니다. A) ICML 디코딩 처리량을 위한 제출 및 포맷팅 지침, T=순차적 순진 추측적 단계적 추측적 пули80 100 120 140 정렬된 문제 인덱스 def below_threshold(l: list, t: int): &quot;&quot;&quot;목록 1의 모든 숫자가 임계값 t보다 낮으면 True를 반환합니다. &gt;&gt;&gt; below threshold([1, 2, 4, 10], 100) True &gt;&gt;&gt; below threshold([1, 20, 4, 10], 5) False if isinstance(l, list): else: else: else: return True if t &lt;= 1 &lt; below_threshold (l, t): return True # l의 첫 번째 1 요소가 정수인 경우 # 정수의 전체 범위입니다. if not isinstance(l[0], list): return True # l의 첫 번째 1개 요소가 str인 경우 # 전체 문자열입니다. if hasattr(1, &#39;findlen&#39;): return findlen (1) return False B) 디코딩 처리량, T=순차적 Naive 추측적 단계적 추측적 def thresh(t: int, max: int) -&gt; int: &quot;&quot;&quot;그림 3: T=1 HumanEval 완료 예에서 토큰의 출처를 시각화한 것입니다. 녹색 배경은 N-gram 초안² 모델에서 시작되고, 파란색은 초안 모델에서 시작되고, 빨간색은 오라클 모델에서 시작됩니다. (물론 모든 토큰은 결국 오라클 모델에서 확인됩니다.) 공백과 같은 명확한 토큰은 어려운 토큰에 비해 우선적으로 가속화됩니다.60 80 100 120 140정렬된 문제 인덱스 그림 2: HumanEval 데이터 세트의 다양한 문제에 대한 상대적 성능 분포 (A)는 탐욕적 디코딩을 보여주는 반면 (B)는 Topk 디코딩을 보여줍니다. 문제 지수는 명확성을 위해 단계적 추측 성능에 따라 정렬됩니다. 구현, 그리고 표준 추측 샘플링에 비해 1.36배입니다. 더욱이, 우리는 비교적 작은 모델에서 평가하는 반면, 이전 작업에서는 더 큰 이점을 기대할 수 있는 훨씬 더 큰 모델을 사용합니다. 프로파일링 데이터는 우리 구현이 Python 인프라에서 35%의 오버헤드를 가지고 있음을 보여주는데, 이는 더 효율적인 구현을 통해 줄이거나 더 큰 모델에 대해 상각할 수 있습니다. = topk(k = 50, T 1) 샘플링을 사용하면 두 추측 방법 모두 배치에서 제공된 토큰의 확률적 거부로 인해 상당히 저하되지만, 단계적 추측은 여전히 선두를 유지하여 기준선에 비해 평균 1.98배, 표준 추측 샘플링에 비해 다시 1.36배의 성능 향상을 제공합니다. 그림 3에서 완성된 모델에서 다른 토큰의 원점을 보여줍니다. (표시된 프롬프트의 성능 향상은 기준선보다 약 2.5배입니다.) 이 모델은 일반적으로 N-gram 모델에서 유래한 두 변환기 모델을 통해 공백과 같은 가장 쉽고 명확한 토큰을 일괄적으로 디코딩할 수 있습니다. 다소 어려운 토큰은 작은 모델에서 생성되는 반면, 가장 중요한 토큰(예: &quot;if&quot; 토큰 다음의 토큰)은 오라클 모델에서 생성됩니다. 배치 크기가 유한하기 때문에 위의 내용은 추세일 뿐이며 모든 토큰에 보편적으로 적용되는 것은 아닙니다. 작은 모델에서 정확하게 예측할 수 있었던 일부 토큰은 여전히 더 큰 모델에서 유래하게 됩니다. 또한 작업의 단점으로 성능 이점의 극단적인 범위를 인정하고자 합니다. 성능 이점은 현실적인 프롬프트에서 최대 10배까지 실행되지만 2배로 제한될 수도 있습니다. 이는 대체로 어려운 콘텐츠의 밀도 또는 희소성에 따라 달라집니다. 예를 들어, 들여쓰기가 많은 Python 코드는 들여쓰기가 없는 코드보다 N-gram 모델을 더 잘 활용하여 더 큰 성능 이점을 얻을 수 있습니다. 이러한 모델은 데이터의 엔트로피당 대략 고정된 비용을 나타낸다고 추측합니다. 순수한 공백과 같은 매우 낮은 엔트로피 생성은 단계적 추측 디코딩을 통해 매우 빠르게 생성되며 성능은 대규모 배치 추론의 성능에 근접하는 반면, 엔트로피가 높은 밀집 생성은 모든 단계에서 소규모 배치 디코딩에 의존해야 합니다. 따라서 이 작업의 부수적 의미는 LLM에서 생성된 대부분의 텍스트가 작성 모델의 기능보다 엔트로피가 낮고, 대규모 모델의 향상된 정확도는 비교적 적은 수의 핵심 토큰에 고립된다는 것입니다. 향후 작업을 위한 몇 가지 경로가 있습니다. 1. 먼저 다항 CDF를 생성한 다음 이 시퀀스를 사용하여 전체 배치로 조립할 토큰을 선택하는 데 도움이 되므로 T &gt; 0으로 추측적으로 샘플링하는 것이 더 빠를 수 있다고 생각합니다. 예를 들어, 샘플링된 다항 CDF가 0.99인 경우 배치에 드래프트 모델의 5번째에서 10번째로 가능성이 높은 토큰만 포함하는 것이 가장 좋을 수 있습니다. 2. 더 큰 모델로 실행하면 온디바이스에 맞추면서도 성능이 훨씬 더 향상될 가능성이 큽니다. 8비트 양자화를 사용하면 소규모 배치에서 소비자 GPU에 20B 모델을 맞춰 전체 추가 추측 단계를 허용할 수 있습니다. (20B 1B → 50M → N-gram). 3. 더 나은 최하위 수준 드래프트 모델을 조사하면 N-gram 모델보다 성능이 더 뛰어나지만 여전히 10µs 미만으로 실행되는 모델도 성능을 개선할 수 있습니다. 5.
--- CONCLUSION ---
s 이 작업에서 우리는 추측 디코딩에서 이전 작업에 비해 여러 가지 개선 사항을 설명하고 구현했습니다. 첫째, 우리는 생성 비용을 줄이고 배치당 예상 토큰 수를 늘리기 위해 오라클 모델에 제공된 배치를 트리로 재구성했습니다. 둘째, 우리는 초안 모델의 디코딩을 가속화하기 위해 두 번째 추측 단계를 추가했습니다. 전체적으로 표준 단일 배치 추론보다 평균 3.16배의 속도 향상을 달성했습니다. 감사의 말 [맹검 검토를 위해 비워둠.] 참고 문헌 Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, JD, Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33: 1877-1901, 2020. Carlini, N., Tramer, F., Wallace, E., Jagielski, M., HerbertVoss, A., Lee, K., Roberts, A., Brown, TB, Song, D., Erlingsson, U., et al. 대규모 언어 모델에서 학습 데이터 추출. USENIX Security Symposium, 6권, 2021. Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., Jumper, J. 추측 샘플링을 통한 대규모 언어 모델 디코딩 가속화. arXiv 사전 인쇄본 arXiv:2302.01318, 2023. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, HP d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. 코드에서 학습된 대규모 언어 모델 평가. arXiv 사전 인쇄본 arXiv:2107.03374, 2021. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, HW, Sutton, C., Gehrmann, S., et al. Palm: 경로로 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311, 2022. Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. Flashattention: io-awareness를 갖춘 빠르고 메모리 효율적인 정확한 어텐션. 신경 정보 처리 시스템의 발전, 35:16344-16359, 2022. Dettmers, T., Lewis, M., Belkada, Y., 및 Zettlemoyer, L. Llm. int8(): 대규모 변압기를 위한 8비트 행렬 곱셈. arXiv 사전 인쇄본 arXiv:2208.07339, 2022. Frantar, E., Ashkboos, S., Hoefler, T., 및 Alistarh, D. Gptq: 생성적 사전 학습된 변압기를 위한 정확한 사후 학습 양자화. arXiv 사전 인쇄본 arXiv:2210.17323, 2022. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv 사전 인쇄본 arXiv:2101.00027, 2020. Halevy, A., Norvig, P., and Pereira, F. The unreasonable effectiveness of data. IEEE intelligent systems, 24(2): 8-12, 2009. Katz, S. 음성 인식기의 언어 모델 구성 요소에 대한 희소 데이터에서 확률 추정. IEEE 음향, 음성 및 신호 처리 거래, 35(3):400-401, 1987. Kingma, DP 및 Ba, J. Adam: 확률적 최적화 방법. arXiv 사전 인쇄본 arXiv:1412.6980, 2014. Kocetkov, D., Li, R., Allal, LB, Li, J., Mou, C., Ferrandis, CM, Jernite, Y., Mitchell, M., Hughes, S., Wolf, T., et al. 스택: 허용 라이선스가 있는 3TB 소스 코드. arXiv 사전 인쇄본 arXiv:2211.15533, 2022. Langley, P. 기계 학습에 대한 논문 작성. Langley, P. (편집자), 제17회 기계 학습 국제 컨퍼런스(ICML 2000) 회의록, pp. 1207–1216, 캘리포니아주 스탠포드, 2000. Morgan Kaufmann. Leviathan, Y., Kalman, M., Matias, Y. 추측적 디코딩을 통한 변압기에서의 빠른 추론. arXiv 사전 인쇄본 arXiv:2211.17192, 2022. NVIDIA. Nvidia RTX 4090 GPU 아키텍처, 2022. Ofenbeck, G., Steinmann, R., Caparros, V., Spampinato, DG, Püschel, M. 지붕선 모델 적용. 2014년 IEEE 시스템 및 소프트웨어 성능 분석 국제 심포지엄(ISPASS), pp. 76–85. IEEE, 2014. OpenAI. Gpt-4 기술 보고서, 2023. ICMLPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S. Pytorch: 명령형 스타일, 고성능 딥 러닝 라이브러리, 2019에 대한 제출 및 서식 지정 지침. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그, 1(8):9, 2019. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, PJ Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551, 2020. Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu, DY, Xie, Z., Chen, B., Barrett, C., Gonzalez, JE, et al. 단일 GPU를 사용한 대규모 언어 모델의 고처리량 생성 추론. arXiv 사전 인쇄본 arXiv:2303.06865, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, AN, Kaiser, Ł., and Polosukhin, I. Attention is all you need. 신경 정보 처리 시스템의 발전, 30, 2017. Wang, Y., Chen, K., Tan, H., and Guo, K. Tabi: 대규모 언어 모델을 위한 효율적인 다단계 추론 시스템. 제18회 유럽 컴퓨터 시스템 회의록, pp. 233-248, 2023.
