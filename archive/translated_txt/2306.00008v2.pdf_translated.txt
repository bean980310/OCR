--- ABSTRACT ---
Transformers는 자연어 처리 및 컴퓨터 비전 분야의 최근 성공에 핵심적인 역할을 합니다. Transformers는 대부분 균일한 백본을 가지고 있으며, 레이어는 피드포워드와 셀프 어텐션을 번갈아가며 딥 네트워크를 구축합니다. 여기서 우리는 이 설계 선택을 조사하고 레이어 기본 요소의 다른 순열을 가진 더 복잡한 블록이 더 효율적일 수 있음을 발견했습니다. 이 통찰력을 사용하여 우리는 Brainformer라는 복잡한 블록을 개발합니다. 이 블록은 희소 게이트 피드포워드 레이어, 밀집 피드포워드 레이어, 어텐션 레이어, 다양한 형태의 레이어 정규화 및 활성화 함수와 같은 다양한 레이어 세트로 구성됩니다. Brainformer는 품질과 효율성 측면에서 최첨단 밀집 및 희소 Transformers보다 지속적으로 우수한 성능을 보입니다. 토큰당 80억 개의 활성화된 매개변수가 있는 Brainformer 모델은 GLAM 대응 모델에 비해 2배 더 빠른 학습 수렴과 5배 더 빠른 단계 시간을 보여줍니다. 다운스트림 작업 평가에서 Brainformer는 유사한 수의 활성화된 매개변수를 가진 GLAM에 비해 미세 조정을 통해 3% 더 높은 SuperGLUE 점수를 보여줍니다. 마지막으로, Brainformer는 토큰당 비슷한 계산량을 가진 NAS로 도출한 Primer 밀집 모델보다 fewshot 평가에서 훨씬 더 우수한 성능을 보였습니다. 1.
--- INTRODUCTION ---
최근 몇 년 동안 Transformer 아키텍처에서 파생된 대규모 신경망(Vaswani et al., 2017)은 언어 이해 및 생성 작업에서 우수한 결과를 보였습니다. Transformer 변형에 대한 많은 개선은 모델 크기 확장(Raffel et al., 2020; Brown et al., 2020a; Shoeybi et al., 2019; Chowdhery et al., 2022), 학습 토큰 확장(Hoff&#39;Google Deepmind. 연락처: Yanqi Zhou<yanqiz@google.com> . 미국 하와이 호놀룰루에서 열린 제40회 국제 머신러닝 컨퍼런스 논문집. PMLR 202, 2023. 저작권 2023년 저자(들) 소유. 로그 퍼플렉시티 스케일링 2.2.2.2.2.2. Branformer 퍼플렉시티 2. GLAM 퍼플렉시티 Brainformer 초당 단계 수 GLAM 초당 단계 수 2.2.2.50 2.75 3.00 3.25 3.50 3.75 4. 로그 스케일의 활성 매개변수(백만 개) 2.1.1.1.1.0.0. 초당 단계 수 그림 1: 스케일링에서 Brainformer 대 GLaM. Brainformer는 훨씬 빠른 학습 단계 시간으로 모델 품질을 개선합니다. mann et al., 2022; Shoeybi 등, 2019), 더 나은 학습 데이터 품질(Du 등, 2022), 희소하게 활성화된 모델 아키텍처(Du 등, 2022; Lepikhin 등, 2021; Roller 등, 2021; Lewis 등, 2021). 효율적인 변환기 언어 모델(Wang 등, 2020; Choromanski 등, 2020; Tay 등, 2021; Hua 등, 2022) 중에서는 저랭크 접근법이나 근사를 사용하여 어텐션 계층 효율성을 개선하는 데 중점을 두고 있습니다. 그러나 최근 연구에서는 밀집 피드포워드 계층이 일반적인 시퀀스 길이(&lt;2048)에 대한 대부분의 계산 비용을 구성한다는 사실도 확인되었으며, 특히 모델이 큰 경우 그렇습니다(Du 등, 2022; Zhou 등, 2022). 학습 중에 수렴에 도달하기 위해 사용된 총 FLOPS와 같은 컴퓨팅 효율성을 더욱 개선하기 위해 희소하게 게이트화된 Mixture-of-Experts(Lepikhin 등, 2021; Fedus 등, 2021; Du 등, 2022; Zhou 등, 2022; Roller 등, 2021; Lewis 등, 2021; Jaszczur 등, 2021)가 널리 사용되어 모델의 전반적인 용량이 커지면서도 계산 비용은 고정된 채로 품질을 개선할 수 있게 되었습니다. 희소하게 활성화된 모델은 계산 비용을 줄일 뿐만 아니라 라우팅 함수를 사용하여 각 전문가의 효과적인 학습 시간을 줄이지 않고도 다양한 데이터 분포에 대한 다양한 전문가를 학습시켜 더 나은 전문화를 이룹니다. 이 분야의 MoE 아키텍처는 균일한 변환기 블록 또는 밀집 및 희소 계층을 인터리빙(Du 등, 2022)하고 고정된 상위 k 라우팅을 기반으로 합니다. 바닐라 트랜스포머 브레인포머: 효율성을 위한 단순성 트레이딩 afafafafafafafafafafafafa 샌드위치 트랜스포머 aaaaaafafafafafafaffffff GLAM agafagafagafagafagafagafagaf 스택형 브레인포머 agfgfgagfgfgagfgfgagfgfg 그림 2: 상위 수준 비교
--- RELATED WORK ---
. &#39;a&#39;: 어텐션, &#39;f&#39;: 피드포워드, &#39;g&#39;: 희소 게이트 피드포워드. GLAM은 밀집된 트랜스포머 블록을 희소 트랜스포머 블록과 인터리빙합니다. Brainformer는 어텐션의 빈도를 줄이고 레이어 유형과 함께 레이어 너비를 변경합니다. EfficientNet(Tan &amp; Le, 2019)의 레이어별 아키텍처 스태킹과 샌드위치 트랜스포머(Press et al., 2019)의 레이어 재정렬과 공명하여 그림 2의 바닐라 트랜스포머와 같이 엄격한 레이어 인터리빙이 없는 희소성이 있는 비균일 아키텍처를 제안합니다. 검색 공간이 다른 하위 레이어를 다른 순서로 구성하도록 허용하여 아키텍처 규칙성을 희생합니다. 더 나은 스케일링을 위해 다른 게이팅 메커니즘과 결합된 희소 게이트 피드포워드 레이어(MoE 레이어)로 검색 공간에 희소성을 도입합니다. 우리는 희소 계층에서 아키텍처, 희소성, 라우팅 메커니즘을 최적화하는 것이 품질에서 거의 완벽한 로그 스케일 확장을 달성하는 데 중요하다는 것을 발견했습니다. 그림 1은 Brainformer가 GLaM(수동으로 제작된 I 희소 변환기)보다 훨씬 더 잘 확장됨을 보여줍니다. Brainformer는 모델 용량을 늘릴 때 예제 비율을 거의 일정하게 유지하면서 지속적으로 훈련 복잡도를 개선하지만 GLAM은 확장 시 훨씬 더 나쁜 예제 비율을 보입니다. 우리는 MoE 계층만 일반적인
--- CONCLUSION ---
Brainformers: 효율성을 위한 단순성 트레이딩 진화적 검색 알고리즘을 사용하여, 희소하게 게이트된 피드포워드 계층을 포함한 다양한 계층 시퀀스로 구성된 Brainformer라는 복잡한 아키텍처 블록을 개발하고 평가했습니다. 새로운 블록과 함께, 모델 패밀리 간에 공정한 비교를 가능하게 하는 고정된 학습 시간 검색을 사용하여 평가하는 것도 제안합니다. Brainformer는 GLAM 대응 제품에 비해 최대 2배 더 빠른 학습 수렴과 5배 더 빠른 단계 시간을 보여줍니다. 다운스트림 작업 평가에서 Brainformer는 GLaM에 비해 미세 조정을 통해 3% 더 높은 SuperGLUE 점수를 보여주며, 5가지 생성 작업에 대한 원샷 평가에서 Primer보다 훨씬 우수한 성능을 보입니다. 8. 제한 사항 연구 범위 측면에서, 우리의 경험적 결과는 주로 NLP 도메인에 있으며, 광범위한 NLU 및 NLG 작업에 대한 것입니다. 그러나 Brainformer를 컴퓨터 비전에 적용하는 것은 향후 작업에 맡깁니다. 다양한 하드웨어 플랫폼을 타겟으로 하는 Brainformer를 채택할 때 잠재적인 복잡성이 있을 수 있습니다. 예를 들어, 에지 디바이스는 Brainformer 모델의 표현을 제한하는 엄격한 하드웨어 제약을 부과할 수 있습니다. 실용적인 방법은 대상 하드웨어의 단계 시간을 시뮬레이션하거나 학습된 성능 모델을 사용하여 대상 하드웨어의 추론 속도를 예측하는 동안 GPU 또는 TPU와 같은 더 빠른 가속기에서 모델 학습 및 품질 평가를 실행하는 것입니다. 또 다른 문제는 일부 기본 연산자가 온칩 메모리가 충분하지 않은 디바이스에서 지원되지 않을 수 있다는 것입니다. 예를 들어, 글로벌 풀링은 에지 TPU에서 지원되지 않습니다. 그러나 Brainformer는 실행 가능한 연산자에서 컴퓨팅 효율적인 모델 아키텍처를 구축하는 것을 목표로 하기 때문에 이 논문의 범위를 벗어날 수 있습니다. 또 다른 제한은 많은 리소스 소비일 수 있습니다. Brainformer 검색에서 최상의 솔루션에 도달하기 위해 일주일 동안 512개의 TPU v4를 사용했습니다. 그러나 훨씬 더 큰 모델 규모에서 작업하고 있으며 더 작은 모델 크기와 MoE 계층에서 더 적은 수의 전문가를 사용하면 완화될 것이라는 점을 언급할 가치가 있습니다. 또한 검색은 500회 시도만으로도 더 나은 모델 아키텍처를 식별했습니다. 실제로, 더 좋지만 최적이 아닌 모델을 식별하기만 하면 리소스 소비가 작을 수 있습니다. 참고문헌 Berant, J., Chou, A., Frostig, R., and Liang, P. 질문-답변 쌍을 통한 Freebase에서의 의미 분석. 2013년 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, pp. 1533–1544, 미국 워싱턴 시애틀, 2013년 10월. Association for Comp-cational Linguistics. URL https://www.aclweb. org/anthology/D13-1160. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, JD, Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. 언어 모델은 few-shot 학습자입니다. Larochelle, H., Ranzato, M., Hadsell, R., Balcan, MF, Lin, H. (편집자), 신경 정보 처리 시스템의 발전, 33권, 1877-1901쪽. Curran Associates, Inc., URL https://proceedings. neurips.cc/paper/2020/file/ 2020a. 1457c0d6bfcb4967418bfb8ac142f64a-Paper. pdf. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, JD, Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33: 1877-1901, 2020b. Cho, K. 및 Bengio, Y. 심층 학습에서 조건부 계산을 위한 용량 대 계산 비율을 기하급수적으로 증가시킵니다. arXiv 사전 인쇄본 arXiv:1406.7362, 2014. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. 공연자와 함께 주의 재고. arXiv 사전 인쇄본 arXiv:2009.14794, 2020. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, HW, Sutton, C., Gehrmann, S., et al. Palm: 경로로 언어 모델링 확장. arXiv 사전 인쇄본 arXiv:2204.02311, 2022. Dai, AM 및 Le, QV 반지도 시퀀스 학습. Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., 및 Garnett, R.(편집자), 신경 정보 처리 시스템의 발전, 28권. Curran Associates, Inc., 2015. URL https://proceedings. neurips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper. pdf. Dai, Z., Liu, H., Le, QV, and Tan, M. CoAtNet: 모든 데이터 크기에 대한 합성곱과 주의 결합. 신경 정보 처리 시스템의 발전, 2021. Du, N., Huang, Y., Dai, AM, Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, AW, Firat, O., et al. Glam: 혼합 전문가를 사용한 언어 모델의 효율적인 스케일링. 기계 학습 국제 컨퍼런스, 5547-5569쪽. PMLR, 2022. Brainformers: 효율성을 위한 단순성 거래 Dua, D., Bhosale, S., Goswami, V., Cross, J., Lewis, M., and Fan, A. 희소 변환 모델 훈련을 위한 트릭. arXiv 사전 인쇄본 arXiv:2110.08246, 2021. Fedus, W., Zoph, B., and Shazeer, N. 스위치 변압기: 간단하고 효율적인 희소성을 갖춘 1조 매개변수 모델로 확장, 2021. Ghiasi, G., Lin, T.-Y., and Le, QV Nas-fpn: 객체 감지를 위한 확장 가능한 피처 피라미드 아키텍처 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 7036-7045쪽, 2019. Gross, S., Ranzato, M., Szlam, A. 대규모 약한 감독 비전을 위한 전문가의 하드 혼합. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 6865-6873쪽, 2017. He, K., Zhang, X., Ren, S., Sun, J. 이미지 인식을 위한 딥 레지듀얼 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 770-778쪽, 2016a. He, K., Zhang, X., Ren, S., Sun, J. 딥 레지듀얼 네트워크에서의 아이덴티티 매핑. 유럽 컴퓨터 비전 컨퍼런스, 630-645쪽. Springer, 2016b. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, LA, Welbl, J., Clark, A., et al. 컴퓨팅 최적화 대규모 언어 모델 학습. arXiv 사전 인쇄본 arXiv:2203.15556, 2022. Hua, W., Dai, Z., Liu, H., and Le, Q. 선형 시간에서의 변압기 품질. International Conference on Machine Learning, pp. 9099-9117. PMLR, 2022. Jaszczur, S., Chowdhery, A., Mohiuddin, A., Kaiser, L., Gajewski, W., Michalewski, H., and Kanerva, J. Sparse is enough in scaling transformers. 신경 정보 처리 시스템의 발전, 34:9895-9907, 2021. Joshi, M., Choi, E., Weld, DS, Zettlemoyer, L. Triviaqa: 독해를 위한 대규모 원격 감독 챌린지 데이터 세트. 2017년 7월 캐나다 밴쿠버에서 열린 제55회 전산 언어학 협회 연례 회의록. 전산 언어학 협회. Kaplan, J., McCandlish, S., Henighan, T., Brown, TB, Chess, B., Child, R., Gray, S., Radford, A., Wu, J., Amodei, D. 신경 언어 모델을 위한 스케일링 법칙. arXiv 사전 인쇄본 arXiv:2001.08361, 2020. Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin, J., Lee, K., Toutanova, KN, Jones, L., Chang, M.-W., Dai, A., Uszkoreit, J., Le, Q., and Petrov, S. 자연스러운 질문: 질문에 답하는 연구의 벤치마크. Transactions of the Association of Computational Linguistics, 2019. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. GShard: 조건부 계산 및 자동 샤딩을 사용한 거대 모델 확장. International Conference on Learning Representations, 2021. Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pp. 6265–6274. PMLR, 2021. Lin, M., Fu, J., and Bengio, Y. Conditional computing for Continual Learning. arXiv 사전 인쇄본 arXiv:1906.06635, 2019. Liu, H., Dai, Z., So, D., 및 Le, QV mlps에 주의하세요. 신경 정보 처리 시스템의 발전, 34: 9204-9215, 2021. Mikolov, T., Karafiát, M., Burget, L., Cernockỳ, J., 및 Khudanpur, S. 순환 신경망 기반 언어 모델. Interspeech, 2권, 1045-1048쪽. Makuhari, 2010. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, QN, Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R. The lambada dataset: Word prediction requiring a wide discourse context, 2016. URL https://arxiv.org/abs/1606.06031. Press, O., Smith, NA, and Levy, O. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. Puigcerver, J., Riquelme, C., Mustafa, B., Renggli, C., Pinto, AS, Gelly, S., Keysers, D., and Houlsby, N. Scalable transfer learning with expert models. arXiv 사전 인쇄본 arXiv:2009.13239, 2020. Radford, A., Narasimhan, K., Salimans, T., 및 Sutskever, I. 생성적 사전 학습을 통한 언어 이해 향상. 2018. Rae, JW, Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., et al. 언어 모델 확장: 고퍼 학습에서 얻은 방법, 분석 및 통찰력. arXiv 사전 인쇄본 arXiv:2112.11446, 2021. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, PJ, et al. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계를 탐구합니다.J. Mach. Learn. Res., 21(140):1-67, 2020. Brainformers: 효율성을 위한 단순성 거래 Rajpurkar, P., Jia, R., and Liang, P. Know what you don&#39;t know: Unanswerable questions for squad, 2018. URL https://arxiv.org/abs/1806.03822. Roller, S., Sukhbaatar, S., Weston, J., et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:17555-17566, 2021. Shazeer, N. and Stern, M. Adafactor: 비선형 메모리 비용이 있는 적응 학습 속도. International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv 사전 인쇄본 arXiv:1701.06538, 2017. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-lm: 모델 병렬 처리를 사용하여 수십억 매개변수 언어 모델 학습. arXiv 사전 인쇄본 arXiv: 1909.08053, 2019. So, D., Mańke, W., Liu, H., Dai, Z., Shazeer, N., and Le, QV 언어 모델링을 위한 효율적인 변환기 검색. 신경 정보 처리 시스템의 발전, 34:6010-6022, 2021. Sutskever, I., Martens, J., Hinton, GE 순환 신경망을 사용한 텍스트 생성. ICML, 2011. Tan, M., Le, Q. Efficientnet: 합성곱 신경망을 위한 모델 스케일링 재고. 기계 학습 국제 컨퍼런스, 6105-6114쪽. PMLR, 2019. Tay, Y., Bahri, D., Metzler, D., Juan, D.-C., Zhao, Z., Zheng, C. Synthesizer: 변압기 모델을 위한 자기 주의 재고. 기계 학습 국제 컨퍼런스, 10183-10192쪽. PMLR, 2021. Tolstikhin, IO, Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., 외. Mlp-mixer: 비전을 위한 모든 MLP 아키텍처입니다. 신경 정보 처리 시스템의 발전, 34:24261-24272, 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, AN, Kaiser, Ł. 및 Polosukhin, I. 주의만 기울이면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, SR Glue: 자연어 이해를 위한 다중 작업 벤치마크 및 분석 플랫폼. arXiv 사전 인쇄본 arXiv:1804.07461, 2018. Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S. Superglue: 범용 언어 이해 시스템을 위한 더 끈적한 벤치마크. 신경 정보 처리 시스템의 발전, 32, 2019. Wang, S., Li, BZ, Khabsa, M., Fang, H., Ma, H. Linformer: 선형 복잡도를 가진 자기 주의. arXiv 사전 인쇄본 arXiv: 2006.04768, 2020. Wu, L., Liu, M., Chen, Y., Chen, D., Dai, X., 및 Yuan, L. 전문가의 잔여 혼합물. arXiv 사전 인쇄본 arXiv:2204.09636, 2022. Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A., Chen, Z., Le, Q., 및 Laudon, J. 전문가 선택 라우팅을 사용한 전문가 혼합물, 2022. URL https://arxiv.org/abs/2202.09368. Zuo, S., Liu, X., Jiao, J., Kim, YJ, Hassan, H., Zhang, R., Zhao, T., and Gao, J. Taming sparsely activate transformer with stochastic experts. arXiv preprint arXiv:2110.04260, 2021. Brainformers: Trading Simplicity for Efficiency A. 여기에 부록을 넣을 수 있습니다. 원하는 만큼 많은 텍스트를 넣을 수 있습니다. 본문은 최대 8페이지 길이여야 합니다. 최종 버전에는 한 페이지를 더 추가할 수 있습니다. 원하시면 이와 같은 부록을 사용할 수 있으며, 1열 형식을 사용할 수도 있습니다.
