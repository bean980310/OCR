--- ABSTRACT ---
출처를 명시하는 것은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org로 허가를 요청하세요. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW주 시드니 © 2023 저작권은 소유자/저자에게 있습니다. 출판권은 ACM에 라이선스되었습니다. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.초록 신경장은 뷰 합성 및 장면 재구성에서 인상적인 발전을 이루었습니다. 그러나 이러한 신경장을 편집하는 것은 기하학 및 텍스처 정보의 암묵적 인코딩으로 인해 여전히 어렵습니다. 이 논문에서는 사용자가 텍스트 프롬프트를 사용하여 신경장을 제어하여 편집할 수 있도록 하는 새로운 프레임워크인 DreamEditor를 제안합니다. DreamEditor는 장면을 메시 기반 신경 필드로 표현하여 특정 영역 내에서 로컬 편집을 허용합니다. DreamEditor는 사전 학습된 텍스트-이미지 확산 모델의 텍스트 인코더를 활용하여 텍스트 프롬프트의 의미론을 기반으로 편집할 영역을 자동으로 식별합니다. 그런 다음 DreamEditor는 편집 영역을 최적화하고 점수 증류 샘플링을 통해 해당 지오메트리와 텍스처를 텍스트 프롬프트에 맞춥니다[Poole et al. 2022]. 광범위한 실험을 통해 DreamEditor는 관련 없는 영역에서 일관성을 보장하면서 주어진 텍스트 프롬프트에 따라 실제 세계 장면의 신경 필드를 정확하게 편집할 수 있음이 입증되었습니다. DreamEditor는 매우 사실적인 텍스처와 지오메트리를 생성하여 정량적 및 정성적 평가에서 이전 작업을 크게 능가합니다. CCS 개념 . 컴퓨팅 방법론 → 렌더링; 신경망. ACM 참조 형식: ☑ Jingyu Zhuang*, Chen Wang*, Liang Lin Lingjie Liu, Guanbin Li 2023. DreamEditor: 신경장을 사용한 텍스트 기반 3D 장면 편집. SIGGRAPH Asia 2023 컨퍼런스 논문(SA 컨퍼런스 논문 &#39;23), 2023년 12월 12일~15일, 호주 NSW 시드니. ACM, 뉴욕, 뉴욕, 미국, 10페이지. https://doi.org/10.1145/3610548.
--- INTRODUCTION ---
신경 복사장[Mildenhall et al. 2021], NeuS[Wang et al. 2021] 및 후속 연구[Liu et al. 2020; Müller et al. 2022; Wang et al. 2022c](총칭하여 신경장)는 장면 재구성 및 새로운 뷰 합성에서 상당한 진전을 이루었습니다. 3D 장면의 다중 뷰 이미지를 캡처하고 기성품 구조-모션 모델을 사용하여 카메라 포즈를 추정하면 신경망을 학습하여 장면의 기하학과 텍스처를 암묵적으로 나타내는 신경장을 학습할 수 있습니다. 지루한 3D 매칭 및 복잡한 후처리 단계를 포함하는 기존 파이프라인과 비교할 때 신경장은 일반 사용자를 위한 컴퓨터 그래픽 자산으로 실제 세계 객체와 장면을 재구성하는 데 보다 효율적이고 접근 가능한 접근 방식을 제공합니다. 그러나 모양과 텍스처 정보가 고차원 신경망 피처 내에 암묵적으로 인코딩되기 때문에 신경장을 편집하는 것은 간단한 작업이 아닙니다. 기존의 3D 모델링 기술은 명시적 지오메트리를 사용할 수 없기 때문에 수동 조각 및 재텍스처링에 효과적이지 않습니다. 이전 연구에서는 장면에서 객체를 이동[Chen et al. 2021], 텍스처 수정[Xiang et al. 2021], 객체 모양 변경[Yang et al. 2022]과 같은 신경장 편집 기술을 탐구했습니다. 그러나 이러한 편집 절차에는 여전히 광범위한 사용자 입력이 필요합니다. 최근 연구에서는 텍스트 프롬프트를 사용하여 NeRF 편집이 가능해졌지만[Haque et al. 2023], 명령어의 다양성이 제한되어 정확하고 고품질의 편집을 달성하는 데 어려움을 겪고 있습니다. 결과적으로 기존 3D 자산의 재생성을 개선할 수 있는 사용하기 쉽고 정확한 3D 편집 방법을 개발하기 위한 추가 연구가 필요합니다. 이 논문에서는 사용자가 텍스트 프롬프트를 사용하여 직관적이고 편리하게 신경장을 수정할 수 있는 프레임워크인 DreamEditor를 제시합니다. 그림 1에 도시된 바와 같이, 개나 복잡한 야외 환경과 같이 신경장으로 표현된 주어진 장면에 대해 텍스트 설명을 사용하여 텍스처링, 객체 대체, 객체 삽입을 포함한 다양한 객체 중심 편집을 달성하는 동시에 텍스트 프롬프트와 관련 없는 영역을 보존할 수 있습니다. 이는 방법에서 두 가지 핵심 설계를 통해 가능합니다. (1) 메시 기반 신경장 표현, (2) 3D 편집을 위해 사전 학습된 확산 모델을 활용하는 단계적 프레임워크. Zhuang et al. 암묵적 표현과 비교하여 명시적 메시 기반 신경장은 역투영을 통해 2D 편집 마스크를 3D 편집 영역으로 효율적으로 변환하여 마스크된 영역만 수정하여 정확한 로컬 편집을 용이하게 합니다. 또한 메시 표현은 지오메트리와 텍스처를 풀어서 모양 변경만 예상되는 경우 불필요한 지오메트리 변형을 방지합니다. 메시 표현의 장점을 활용하여 마스크된 영역 내에서 점수 증류 샘플링을 통해 간단한 텍스트 프롬프트를 사용하여 3D 장면을 효율적이고 정확하게 편집하는 단계별 미세 조정-지역화-최적화 프레임워크를 제안합니다. 동물, 인간 얼굴 및 야외 장면을 포함한 다양한 합성 및 실제 장면에서 DreamEditor를 광범위하게 평가합니다. 전체 이미지에서 작동하는 방법과 달리 편집 방식은 무관한 영역을 자연스럽게 보존하면서 정확한 국소 변형을 가능하게 합니다. 예를 들어, 그림 1에서 입에 장미를 물고 있을 때 개의 입만 수정됩니다. 또한 간단한 텍스트 프롬프트로 편집을 수행할 수 있으므로 절차가 사용자 친화적이고 신경 필드 편집을 크게 단순화하여 실용적인 응용 분야에서 큰 잠재력을 보여줍니다. 정성적 및 정량적 비교는 편집 정밀도, 시각적 충실도 및 사용자 만족도 측면에서 DreamEditor가 이전 방법보다 우수함을 보여줍니다. 이 논문의 기여는 다음과 같이 요약할 수 있습니다. (1) 우리는 광범위한 실제 세계 장면에 대해 매우 사실적인 편집 결과를 달성하는 텍스트 가이드 3D 장면 편집을 위한 새로운 프레임워크를 소개합니다. (2) 우리는 메시 기반 신경장을 사용하여 장면의 로컬 수정을 가능하게 하고 유연한 편집을 위해 텍스처와 기하학적 특징을 분리하는 것을 제안합니다. (3) 우리는 텍스트 프롬프트에 따라 편집이 필요한 특정 영역을 먼저 식별한 다음 선택한 영역 내에서만 수정을 수행하는 단계적 편집 프레임워크를 고안합니다. 이 체계적인 절차는 불필요한 영역에서 불필요한 수정을 최소화하면서 정확한 3D 편집을 보장합니다. 2
--- RELATED WORK ---
S 2.1 텍스트 가이드 이미지 생성 및 편집 노이즈 제거 확산 확률 모델[Ho et al. 2020; Song et al. 2020]은 고품질 이미지를 생성하는 능력으로 큰 주목을 받았습니다. 나중에 대규모 이미지-텍스트 쌍 데이터 세트에서 학습된 확산 모델[Ramesh et al. 2022; Rombach et al. 2022; Saharia et al. 2022]은 텍스트 프롬프트(명사, 형용사, 동사 등 포함)에서 복잡한 의미를 이해하고 해당 고품질 이미지를 생성하는 데 놀라운 성능을 보였습니다. 사전 학습된 텍스트-이미지 확산 모델의 풍부한 의미와 높은 제어 가능성으로 인해 일련의 연구[Avrahami et al. 2022; Couairon et al. 2022; Hertz et al. 2022; Kawar et al. 2022]는 텍스트 가이드 이미지 편집에 이를 사용했습니다. 우리 작업과 가장 관련이 있는 것은 텍스트-이미지 확산 모델을 사용한 주제 중심 생성입니다[Gal et al. 2022a; Ruiz et al. 2022]. 이를 통해 사용자는 특정 주제와 주어진 개념에 대한 이미지 생성을 개인화할 수 있습니다. DreamBooth[Ruiz et al. 2022]는 희귀 토큰을 사용하여 언어-비전 사전을 확장하고 정규화를 위해 보존 손실로 모델을 미세 조정합니다. 마찬가지로 텍스트 역전[Gal et al. 2022a]은 사전 학습된 확산 모델의 임베딩 공간에서 새로운 &quot;단어&quot;를 최적화하여 입력 객체를 나타냅니다. DreamEditor: 신경망을 사용한 텍스트 중심 3D 장면 편집 SA 학술 대회 논문 &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니. 이러한 작업은 특정 이미지를 편집하거나 새로운 개념으로 이미지를 생성하는 작업을 다루지만 이러한 2D
--- METHOD ---
ologies → 렌더링; 신경망. ACM 참조 형식: ☑ Jingyu Zhuang*, Chen Wang*, Liang Lin Lingjie Liu, and Guanbin Li 2023. DreamEditor: 신경장을 사용한 텍스트 기반 3D 장면 편집. SIGGRAPH Asia 2023 컨퍼런스 논문(SA 컨퍼런스 논문 &#39;23), 2023년 12월 12일-15일, 호주 NSW주 시드니. ACM, 뉴욕, 뉴욕, 미국, 10페이지. https://doi.org/10.1145/3610548. 서론 신경 복사장[Mildenhall et al. 2021], NeuS[Wang et al. 2021] 및 후속 연구[Liu et al. 2020; Müller et al. 2022; Wang et al. 2022c](집합적으로 신경장이라고 함)는 장면 재구성 및 새로운 뷰 합성에서 상당한 진전을 이루었습니다. 3D 장면의 멀티뷰 이미지를 캡처하고 기성품 구조-모션 모델을 사용하여 카메라 포즈를 추정하면 신경망을 훈련하여 장면의 기하학과 텍스처를 암묵적으로 나타내는 신경장을 학습할 수 있습니다. 지루한 3D 매칭과 복잡한 후처리 단계를 포함하는 기존 파이프라인과 비교할 때 신경장은 일반 사용자를 위한 컴퓨터 그래픽 자산으로 실제 세계 객체와 장면을 재구성하는 데 더 효율적이고 접근 가능한 접근 방식을 제공합니다. 그러나 모양과 텍스처 정보가 고차원 신경망 피처 내에 암묵적으로 인코딩되기 때문에 신경장을 편집하는 것은 간단한 작업이 아닙니다. 기존의 3D 모델링 기술은 명시적 기하학을 사용할 수 없기 때문에 수동 조각 및 재텍스처링에는 효과적이지 않습니다. 이전 연구에서는 장면에서 객체를 이동[Chen et al. 2021]하고 텍스처를 수정[Xiang et al. 2021], 그리고 객체 모양 변경[Yang et al. 2022]. 그러나 이러한 편집 절차는 여전히 광범위한 사용자 입력이 필요합니다. 최근 연구에서는 텍스트 프롬프트를 사용하여 NeRF 편집이 가능해졌지만[Haque et al. 2023], 명령어 다양성이 제한되어 정확하고 고품질의 편집을 달성하는 데 어려움을 겪고 있습니다. 결과적으로 사용하기 쉽고 정확한 3D 편집 방법을 개발하여 기존 3D 자산의 재생성을 개선하기 위한 추가 연구가 필요합니다. 이 논문에서는 사용자가 텍스트 프롬프트를 사용하여 직관적이고 편리하게 신경 필드를 수정할 수 있는 프레임워크인 DreamEditor를 제시합니다. 그림 1에서 볼 수 있듯이 개나 복잡한 야외 환경과 같이 신경 필드로 표현된 주어진 장면의 경우 텍스트 설명을 사용하여 텍스처링, 객체 교체, 객체 삽입을 포함한 다양한 객체 중심 편집을 달성하는 동시에 텍스트 프롬프트와 관련 없는 영역을 보존할 수 있습니다. 이는 우리 방법의 두 가지 핵심 설계를 통해 가능합니다.(1) 메시 기반 신경장 표현, (2) 3D 편집을 위해 사전 학습된 확산 모델을 활용하는 단계적 프레임워크.Zhuang et al. 암묵적 표현과 비교할 때 명시적 메시 기반 신경장은 역투영을 통해 2D 편집 마스크를 3D 편집 영역으로 효율적으로 변환하여 마스크된 영역만 수정하여 정확한 로컬 편집을 용이하게 합니다.또한 메시 표현은 지오메트리와 텍스처를 풀어서 모양 변경만 예상되는 경우 불필요한 지오메트리 변형을 방지합니다.메시 표현의 장점을 활용하여 마스크된 영역 내에서 점수 증류 샘플링을 통해 간단한 텍스트 프롬프트를 사용하여 3D 장면을 효율적이고 정확하게 편집하는 단계적 미세 조정-지역화-최적화 프레임워크를 제안합니다.동물, 인간 얼굴 및 야외 장면을 포함한 다양한 합성 및 실제 장면에서 DreamEditor를 광범위하게 평가합니다.전체 이미지에서 작동하는 방법과 달리 편집 방식은 관련 없는 영역을 자연스럽게 보존하면서 정확한 로컬 변형을 가능하게 합니다. 예를 들어, 그림 1에서 개가 장미를 입에 물고 있을 때 개의 입만 수정됩니다. 또한 간단한 텍스트 프롬프트로 편집을 수행할 수 있으므로 절차가 사용자 친화적이며 신경 필드 편집을 크게 단순화하여 실제 응용 분야에서 큰 잠재력을 보여줍니다. 정성적 및 정량적 비교는 모두 편집 정밀도, 시각적 충실도 및 사용자 만족도 측면에서 DreamEditor가 이전 방법보다 우수함을 보여줍니다. 이 논문의 기여는 다음과 같이 요약할 수 있습니다. (1) 광범위한 실제 장면에 대해 매우 사실적인 편집 결과를 얻는 텍스트 가이드 3D 장면 편집을 위한 새로운 프레임워크를 소개합니다. (2) 메시 기반 신경 필드를 사용하여 장면의 로컬 수정을 가능하게 하고 유연한 편집을 위해 텍스처와 기하학적 특징을 분리합니다. (3) 텍스트 프롬프트에 따라 편집이 필요한 특정 영역을 먼저 식별한 다음 선택한 영역 내에서만 수정을 수행하는 단계적 편집 프레임워크를 고안합니다. 이 체계적인 절차는 불필요한 영역에서 불필요한 수정을 최소화하면서 정밀한 3D 편집을 보장합니다. 2 관련 연구 2.1 텍스트 가이드 이미지 생성 및 편집 노이즈 제거 확산 확률 모델[Ho et al. 2020; Song et al. 2020]은 고품질 이미지를 생성하는 능력으로 큰 주목을 받았습니다. 나중에 대규모 이미지-텍스트 쌍 데이터 세트에서 학습된 확산 모델[Ramesh et al. 2022; Rombach et al. 2022; Saharia et al. 2022]은 텍스트 프롬프트(명사, 형용사, 동사 등 포함)에서 복잡한 의미를 이해하고 해당 고품질 이미지를 생성하는 데 놀라운 성능을 보였습니다. 사전 학습된 텍스트-이미지 확산 모델의 풍부한 의미와 높은 제어 가능성으로 인해 일련의 연구[Avrahami et al. 2022; Couairon et al. 2022; Hertz et al. 2022; Kawar et al. 2022]는 텍스트 가이드 이미지 편집에 이를 사용했습니다. 우리 작업과 가장 관련이 있는 것은 텍스트-이미지 확산 모델을 사용한 주제 중심 생성입니다[Gal et al. 2022a; Ruiz et al. 2022]. 이를 통해 사용자는 특정 주제와 주어진 개념에 대한 이미지 생성을 개인화할 수 있습니다. DreamBooth[Ruiz et al. 2022]는 희귀 토큰을 사용하여 언어-시각 사전을 확장하고 정규화를 위해 보존 손실로 모델을 미세 조정합니다. 마찬가지로 텍스트 역전[Gal et al. 2022a]는 사전 훈련된 확산 모델의 임베딩 공간에서 새로운 &quot;단어&quot;를 최적화하여 입력 객체를 표현합니다. DreamEditor: 신경 필드를 사용한 텍스트 기반 3D 장면 편집 SA 컨퍼런스 논문 &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니. 이러한 작업은 특정 이미지를 편집하거나 새로운 개념으로 이미지를 생성하는 작업을 다루지만 이러한 2D 방법을 3D로 확장하는 것은 사소한 일이 아닙니다. 2. 텍스트-3D 생성 텍스트-이미지 생성 모델이 개발되면서 텍스트-3D 생성에 대한 관심이 커지고 있습니다. 일부 작업에서는 CLIP 모델을 사용하여 메시[Chen et al. 2022; Michel et al. 2022; Mohammad Khalid et al. 2022] 또는 신경 필드[Jain et al. 2022]를 최적화합니다. 세미나 작업 DreamFusion[Poole et al. 2022]은 먼저 다음을 제안합니다. 영어: 점수 증류 샘플링(SDS) 손실을 사용하여 텍스트에서 3D로 생성하기 위한 사전 학습된 2D 텍스트에서 이미지로의 확산 모델에서 지식을 증류합니다. SDS 손실을 기반으로 하는 일련의 연구[Chen et al. 2023; Lin et al. 2022; Metzer et al. 2022; Raj et al. 2023]는 3D 표현을 사전에 도입하거나 변경하여 생성 결과를 더욱 개선합니다. 점수 야코비안 체이닝[Wang et al. 2022b]은 3D 점수를 2D 점수로 근사화하는 관점에서 유사한 훈련 목표에 도달합니다. 그러나 이러한 모든 방법은 기존 3D 장면을 편집하는 기능이 없습니다. 주된 이유 중 하나는 기존 3D 장면을 텍스트와 완전히 정렬하는 데 어려움이 있어 이러한 방법은 새 장면을 생성하고 편집 전후의 일관성이 깨지는 경향이 있기 때문입니다. 이러한 한계를 극복하기 위해 텍스트 프롬프트를 기반으로 기존 3D 장면을 편집할 수 있는 새로운 텍스트 가이드 3D 편집 프레임워크를 제안합니다.2.3 신경장 편집 신경장을 편집하는 것은 얽힌 모양과 모양으로 인해 본질적으로 어렵습니다.EditNeRF[Liu et al. 2021]는 잠재 코드를 조건으로 신경장의 모양과 색상을 편집하는 것을 지원하는 최초의 연구입니다.일부 연구[Bao et al. 2023; Gao et al. 2023; Wang et al. 2022a, 2023]는 CLIP 모델을 더욱 활용하여 텍스트 프롬프트나 참조 이미지로 편집할 수 있도록 합니다.또 다른 연구 라인은 미리 정의된 템플릿 모델이나 스켈레톤을 사용하여 재배치나 렌더링을 지원하지만[Noguchi et al. 2021; Peng et al. 2021], 특정 범주에 제한이 있습니다. 3D 편집은 인페인팅과 같은 2D 이미지 조작을 신경장 훈련과 결합하여 달성할 수도 있습니다 [Kobayashi et al. 2022; Liu et al. 2022]. 기하 기반 방법 [Li et al. 2022; Xu and Harada 2022; Yang et al. 2022; Yuan et al. 2022]은 신경장을 메시로 내보내고 메시의 변형을 암묵적 필드로 다시 동기화합니다. TEXTure [Richardson et al. 2023]는 반복적 확산 기반 프로세스를 사용하여 텍스트 프롬프트를 사용하여 메시의 텍스처를 생성합니다. 우리와 가장 유사한 작업은 Instruct-NeRF2NeRF [Haque et al. 2023]와 Vox-E [Sella et al. 2023]로, 신경장 텍스트 프롬프트를 자유롭게 편집합니다. Instruct-NeRF2NeRF는 이미지 기반 확산 모델 [Brooks et al. 2022] 신경장을 최적화하기 위한 지침으로 입력 이미지를 편집합니다. 그럼에도 불구하고 전체 이미지를 조작하기 때문에 일반적으로 원치 않는 영역도 변경됩니다. Vox-E는 SDS 손실을 채택하고 2D 교차 주의 맵을 통해 3D 공간에서 로컬 편집을 수행합니다. 그러나 Vox-E의 체적 표현에 내재된 제약으로 인해 실제 장면의 편집 품질은 여전히 최적이 아닙니다. 3 배경 SDS 손실을 사용한 신경장 최적화. DreamFusion[Poole et al. 2022]은 3D 생성을 위한 사전 텍스트-이미지(T2I) 확산 모델을 증류하기 위해 점수 증류 샘플링(SDS) 손실을 제안했습니다. 먼저 t 수준에서 임의의 렌더링된 뷰 Î에 임의의 가우시안 노이즈를 추가하여 Ît를 얻습니다. 사전 학습된 확산 모델 ø는 Ît와 입력 텍스트 조건 y가 주어졌을 때 추가된 노이즈를 예측하는 데 사용됩니다. SDS 손실은 다음과 같이 픽셀당 그래디언트로 계산됩니다. VLSDs (4,Î = g(0)) = E€‚t aÎ дв (1) 여기서 w(t)는 노이즈 레벨 t에 따라 달라지는 가중치 함수이고, O는 신경장의 매개변수이며, g는 렌더링 프로세스입니다. 학습하는 동안 확산 모델이 동결되고 그래디언트가 0으로 역전파되어 신경장의 렌더링이 텍스트 조건 y를 사용하여 확산 모델에서 생성된 이미지와 유사하도록 강제합니다. DreamBooth [Ruiz et al. 2022]는 T2I 모델을 기반으로 하는 주체 중심 이미지 생성 방법입니다. 동일한 주체의 이미지 몇 개가 주어지면 DreamBooth는 주체를 고유 식별자(*로 표시)에 바인딩하여 T2I 확산 모델에 포함합니다. L 재구성 손실을 사용하여 입력 이미지의 확산 모델을 미세 조정하고 클래스 사전 보존 손실을 사용하여 과적합을 방지합니다. 자세한 훈련 내용은 Ruiz et al [2022]에서 확인할 수 있습니다. 이 논문에서는 또한 특정 장면을 표현하기 위한 T2I 확산 모델을 미세 조정하기 위해 DreamBooth를 채택합니다. 4. 방법 개요 방법의 입력은 편집할 3D 장면의 포즈 이미지 세트와 편집을 위한 텍스트 프롬프트입니다. 목표는 텍스트 프롬프트에 따라 원래 3D 장면에서 관심 객체의 모양과 모양을 변경하는 것입니다. 그림 3은 말 조각품을 실제 기린으로 바꾸는 예를 보여줍니다. 이 작업에서는 텍스트 프롬프트와 관련 없는 3D 콘텐츠를 편집 전후에 변경하지 않아야 합니다. DreamEditor의 프레임워크는 그림 3에 나와 있으며 세 단계로 구성되어 있습니다. 먼저 원래 신경 광도장을 메시 기반 신경장(섹션 4.2)으로 변환하여 공간 선택적 편집을 달성합니다. 4.3절에서 T2I 모델을 입력 장면에 맞게 사용자 정의하고 교차 주의 맵을 사용하여 텍스트 프롬프트의 키워드에 따라 3D 공간에서 편집 영역을 찾습니다.마지막으로 T2I 확산 모델을 통해 텍스트 프롬프트의 제어를 받는 신경 필드에서 대상 객체를 편집합니다(4.4절).4.2 신경 필드 추출 [Yang et al. 2022]에서 영감을 얻어 먼저 입력 이미지에서 신경 광도장을 학습하고 명시적 메시로 구성된 여러 로컬 암묵적 필드로 분해합니다.여기서 메시는 행진 큐브를 사용하여 신경 광도장에서 추출됩니다[Lorensen and Cline 1987].장면을 메시 기반 신경 필드로 표현하면 두 가지 이점이 있습니다.첫째, 메시 기반 신경 필드를 사용하면 장면의 특정 영역을 정확하게 편집할 수 있습니다.배경 및 무관한 객체와 같은 영역은 특정 암묵적 필드를 수정하여 편집 중에 변경되지 않을 수 있습니다. 둘째, 추출된 메시는 장면에서 객체의 표면과 윤곽을 명시적으로 표현할 수 있습니다.폭셀[Liu et al. 2020] 및 포인트 클라우드[Ost et al. 2022]와 같은 다른 명시적 표현과 비교하면 2023년 12월 12-15일, 호주 NSW 시드니에서 개최된 SA Conference Papers &#39;23에서 원본 신경장 메시 기반 신경장 메시 Distill DreamBooth &quot;a + 기린&quot; 1. 증류 샘플 포인트 레이 캐스팅 특징 표현 2. 편집 영역 찾기 주의 얻기 ← 역투영 메시 기반 신경장 SDS 손실 DreamBooth 3. &quot;a + 기린&quot; 최적화 ➡렌더링 -&gt; 역전파 Zhuang et al. 그림 2: 방법 개요.우리의 방법은 기존 신경장을 최적화하여 대상 텍스트 프롬프트에 맞게 3D 장면을 편집합니다.편집 프로세스는 세 단계로 구성됩니다.(1) 원본 신경장을 메시 기반 신경장으로 증류합니다. (2) 텍스트 프롬프트를 기반으로, 우리의 방법은 자동으로 메시 기반 신경 필드의 편집 영역을 식별합니다. (3) 우리의 방법은 SDS 손실을 활용하여 편집 영역의 색상 특징 fc, 기하 특징 fg 및 정점 위치를 최적화하여 해당 영역의 질감과 기하를 변경합니다. 색상으로 보는 것이 가장 좋습니다. 메시로 편집 영역의 범위를 결정하는 것이 더 편리합니다. 확산 모델의 어텐션 스킴을 결합하여 편집 영역을 자동으로 결정하는 방법을 제안하는데, 이는 입력 텍스트에 따라 메시에서 편집 영역을 정확하게 찾을 수 있습니다. 구체적으로, 신경 광도장을 얻은 후, 교사-학생 기반 훈련 프레임워크를 채택하여 증류를 수행합니다. 여기서 신경 광도장은 학생 모델을 안내하는 교사 모델, 즉 메시 기반 신경 필드로 간주됩니다. 우리는 각 메시 정점 va에 색상 특징 fc와 기하 특징 fg를 할당하여 각각 v 근처의 로컬 모양과 질감 정보를 나타내는 메시 기반 신경 필드를 정의합니다. 볼륨 렌더링 프로세스 동안 샘플링된 지점 x에 대해 먼저 x의 가장 가까운 상위 K 정점의 피처를 역거리(Vk x)로 가중하여 보간하여 집계된 피처 fc 및 fg를 구합니다[Qi et al. 2017]: ft(x) = Σk-1Wkft.k ΣΚ¯, Wk = k=1Wk , t = {g, c} ||vkx|| (2) 그런 다음 fg 및 fe는 x의 s-밀도 s 및 색상 c로 디코딩됩니다: S= DG (fg, h), C = Dc (fc, h, d, Vxs) (3) 여기서 DG 및 DC는 각각 기하 디코더 및 색상 디코더이고, h는 x에서 Vk로 보간된 부호 거리이고, d는 광선 방향이며 Vxs는 지점 x에서 s-밀도 s의 그래디언트입니다. 네트워크의 프레임워크는 그림 9에 나와 있습니다.증류 프로세스 동안 장면에서 광선 r을 무작위로 샘플링하고 r이 주어진 교사 모델의 출력을 기준 진실로 사용합니다.여기에는 이 광선의 각 샘플링 지점 x의 렌더링된 픽셀 색상 Ĉ(r), s-밀도 ŝ; 및 포인트 색상 ĉ;이 포함됩니다.증류 손실은 다음과 같이 계산됩니다.Ldis = ŝi-Si| + ||ĉi - Ci||) + Σ ||Ĉ(r) – C(r)||₁₂, (4) reRieN rЄR 여기서 교사 및 학생 모델(즉, Ĉ 및 C)의 볼륨 렌더링 공식은 NeuS [Wang et al. 2021]와 동일합니다.또한 샘플링된 지점에 Eikonal 손실 [Gropp et al. 2020]을 추가하여 가중치 λreg = 0.Lreg = reRieN x Si - - 1 || 2. (5) 우리 프레임워크에서 모든 카메라 포즈 샘플링은 구면 좌표계를 기반으로 합니다. 대상 객체를 원점으로 변환하고 y축을 위쪽으로 향하게 합니다. 다음 위치 지정 및 최적화 단계에서 고도 및 방위각 범위를 설정하여 샘플링된 뷰의 범위를 제한하여 편집 효율성을 개선합니다. 4.3 편집 영역 찾기 그림 2의 중간 부분에서 설명한 대로 텍스트 프롬프트가 주어지면 DreamEditor는 먼저 렌더링된 뷰에서 대상 편집 영역을 결정합니다. 준비 단계로 먼저 샘플링된 뷰를 사용하여 DreamBooth로 안정적 확산 모델을 미세 조정하여 모델의 지식을 특정 장면에 적용합니다. 그런 다음 미세 조정된 확산 모델을 사용하여 각 렌더링된 뷰에 대한 2D 마스크를 얻습니다. 마지막으로 마스크된 대상 영역을 다른 뷰에서 메시로 역투영하여 3D 편집 영역을 얻습니다. 위치 지정은 T2I 확산 모델의 교차 어텐션 레이어가 생성된 이미지의 레이아웃과 각 단어 간의 관계를 제어한다는 사실에 의해 동기가 부여됩니다[Hertz et al. 2022]: M = Softmax(QK√), 여기서 Q는 차원 q의 노이즈 이미지의 공간적 특징에서 투영된 쿼리 특징이고, K는 텍스트 임베딩에서 투영된 키 행렬이고, M은 각 픽셀에 대한 토큰의 가중치를 정의하는 어텐션 맵입니다. 따라서 M은 픽셀이 텍스트 프롬프트의 단어에 해당하고 편집 영역을 찾는 데 사용될 수 있는 확률을 나타냅니다. 구체적으로, 렌더링된 뷰의 노이즈 이미지 it와 텍스트 프롬프트는 노이즈 제거를 위해 확산 모델에 입력됩니다. 의도된 편집 결과를 나타내는 키워드(예: 그림 3의 &quot;앞치마&quot;, &quot;기린&quot;, &quot;모자&quot;)를 선택하고 생성 프로세스 중에 생성된 모든 어텐션 맵을 추출합니다. 실제로, 확산 모델의 백본은 일반적으로 L개의 합성곱 블록으로 구성되며, 여기에는 H개의 멀티헤드 어텐션 레이어가 장착되어 있습니다[Vaswani et al. 2017]. 따라서 T라운드의 노이즈 제거 후, 최종 어텐션 맵 M 세트는 {Mt,1,h}로 표현될 수 있습니다. 여기서 t, l, h는 각각 시간 단계, 합성곱 블록, 어텐션 헤드의 인덱스를 나타냅니다. 선형 보간을 통해 모든 어텐션 맵의 크기를 조정하고 이를 집계하여 집계된 어텐션 맵 M을 얻습니다. M은 [0,1]로 추가로 정규화되고 임계값 7 = 0.75로 이진화됩니다. 여기서 값이 1인 영역은 편집 영역입니다. 마스크의 편집 영역에 속하는 모든 픽셀을 메시에 역투영하고 교차된 메시 면을 편집 영역으로 표시합니다. 키워드가 초기 장면의 객체에 국한되지 않는다는 점을 강조할 가치가 있습니다. 키워드의 어텐션 맵은 생성된 이미지에서 키워드가 존재할 가능성이 매우 높은 영역을 구분하기 때문입니다. 그림 6에서 볼 수 있듯이 &quot;선글라스&quot;가 원래 장면의 일부가 아니더라도 장면 메시에서 합리적인 영역을 식별하는 것은 여전히 가능합니다. 이 단계에서는 샘플링된 뷰의 범위 내에서 45° 간격으로 모든 고도 및 방위각을 횡단하여 모든 잠재적 편집 영역의 적용 범위를 보장합니다. 그런 다음 모든 샘플링된 뷰의 마스크를 가져와 메시에 역투영합니다. 역투영 결과를 병합한 후 두 단계를 사용하여 마스크된 영역을 정제합니다.(1) 삭제: 전체 투영 면적의 10% 미만인 면의 수가 있는 편집 영역 내의 작은 조각을 삭제합니다.이는 일반적으로 부정확한 2D 마스크(예: 대상 객체보다 큰 마스크가 객체 외부에 투영됨)에서 발생합니다.(2) 채우기: 너비 우선 탐색을 사용하여 편집 영역의 &quot;구멍&quot;, 즉 편집 영역으로 둘러싸인 비편집 영역을 채웁니다.이러한 &quot;구멍&quot;은 일반적으로 가려진(예: 말의 엉덩이) 또는 오목한 영역에서 발생합니다.이러한 영역을 편집 영역에 통합하여 편집 영역의 완전성을 높입니다.최종 편집 영역을 V = {ve}=4로 표시합니다.4 편집 영역 최적화 이 단계에서는 DreamFusion의 SDS 손실을 채택합니다[Poole et al. 2022] T2I 확산 모델을 사용하여 신경 필드에서 편집 영역의 최적화를 안내하여 장면을 텍스트 프롬프트에 맞춥니다. 무작위로 렌더링된 뷰와 텍스트 프롬프트를 T2I 확산 모델에 공급하여 SDS 손실을 계산하고 그래디언트를 신경 필드로 역전파합니다. DreamFusion의 Imagen [Saharia et al. 2022]은 독점적이므로 안정적 확산 [Rombach et al. 2022] 다음과 같습니다. VwLSDS(0,9(w)) = Ee,tt[w(1)(€¢ (Zz;y, 1) — €) дл aÎ aw(6) 여기서 w = {fg,k, fc,k, Vk}k는 V의 모든 메시 정점에 대한 기하 특징, 색상 특징 및 위치의 집합이고, zt는 노이즈가 있는 잠재 값을 나타내고, z는 안정 확산 모델의 인코더에서 생성된 원래 잠재 값입니다. 방정식 6에서 볼 수 있듯이 학습 중에 편집 영역의 정점의 색상 특징 fc 및 기하 특징 fg를 최적화하는 것 외에도 정점의 위치도 포함됩니다. 이는 메시의 구조도 최적화 중에 동적으로 조정됨을 의미하며, 이는 접근 방식의 중요한 부분입니다. 로컬 암시적 필드에서 기하 특징은 주로 정점 근처의 모양 세부 정보를 나타냅니다. 정점에서 떨어진 지점의 s-밀도에 상당한 변화가 있으면 객체 표면의 매끄러움이 깨집니다.따라서 정점 위치와 기하적 특징을 동시에 최적화하는 보완적 최적화 접근 방식을 제안합니다.정점 위치를 최적화하면 메시의 전체 모양이 텍스트 프롬프트에 맞게 조정되고 기하적 특징을 최적화하면 객체의 로컬 기하가 정제됩니다.이 최적화 접근 방식을 사용하면 DreamEditor에서 장미 꽃잎과 같은 복잡한 모양을 생성할 수 있습니다.5절의 절제 연구는 정점 위치와 기하적 특징의 공동 최적화가 필요함을 보여줍니다.정점 위치 최적화 중에 매끄러운 표면을 유지하고 자연스러운 변형을 장려하기 위해 라플라시안 손실과 ARAP(as-rigid-as-possible) 손실을 포함하여 널리 사용되는 메시 정규화 용어를 도입합니다[Sumner et al. 2007]: ELlap Vi E Σνί ||vi |je, vi² V jЄNi (7) , i=E LARAP = |||V; Vj2-V; - Vj2, i=1 jЄNi (8) 여기서 N;은 정점 vi에 대한 원링 이웃의 집합이고, v&#39;는 마지막 반복에서의 정점 위치를 나타냅니다. 각각 균형을 맞추기 위해 lap = 10-4 및 λARAP = 10-4로 설정했습니다. 각 반복에서 최적화하는 동안 SDS 손실과 메시 정규화 항을 모두 수행합니다. SDS 및 정규화 항을 별도로 최적화하면 경험적으로 더 나은 결과를 얻을 수 있음을 발견했습니다. 렌더링된 뷰가 주어지면 먼저 SDS 손실을 사용하여 편집 영역의 fc, fg, v를 최적화합니다. 그런 다음 fc와 fg가 고정되고 메시 정규화 항을 사용하여 v만 최적화됩니다.
--- EXPERIMENT ---
s는 DreamEditor가 관련 없는 영역에서 일관성을 보장하면서 주어진 텍스트 프롬프트에 따라 실제 세계 장면의 신경 필드를 정확하게 편집할 수 있음을 보여주었습니다. DreamEditor는 매우 사실적인 텍스처와 지오메트리를 생성하여 양적 및 질적 평가에서 이전 작업을 크게 능가합니다. CCS 개념. 컴퓨팅 방법론 → 렌더링; 신경망. ACM 참조 형식: ☑ Jingyu Zhuang*, Chen Wang*, Liang Lin Lingjie Liu 및 Guanbin Li 2023. DreamEditor: 신경 필드를 사용한 텍스트 기반 3D 장면 편집. SIGGRAPH Asia 2023 컨퍼런스 논문(SA 컨퍼런스 논문 &#39;23), 2023년 12월 12일-15일, 호주, NSW, 시드니. ACM, 뉴욕, 뉴욕, 미국, 10페이지. 한국어: https://doi.org/10.1145/3610548. 서 론 신경 복사장[Mildenhall et al. 2021], NeuS[Wang et al. 2021] 및 후속 연구[Liu et al. 2020; Müller et al. 2022; Wang et al. 2022c](총칭하여 신경장)는 장면 재구성 및 새로운 뷰 합성에서 상당한 진전을 이루었습니다. 3D 장면의 다중 뷰 이미지를 캡처하고 기성품 구조-모션 모델을 사용하여 카메라 포즈를 추정하면 장면의 기하학과 텍스처를 암묵적으로 나타내는 신경장을 학습하도록 신경망을 훈련할 수 있습니다. 지루한 3D 매칭 및 복잡한 후처리 단계를 포함하는 기존 파이프라인과 비교할 때 신경장은 일반 사용자를 위한 컴퓨터 그래픽 자산으로 실제 세계 객체와 장면을 재구성하는 데 더 효율적이고 접근 가능한 접근 방식을 제공합니다. 그러나 신경장을 편집하는 것은 모양과 텍스처 정보가 고차원 신경망 피처 내에 암묵적으로 인코딩되어 있기 때문에 간단한 작업이 아닙니다.기존의 3D 모델링 기술은 명시적인 지오메트리를 사용할 수 없기 때문에 수동 조각 및 재텍스처링에는 효과적이지 않습니다.이전 연구에서는 장면에서 객체를 이동[Chen et al. 2021], 텍스처 수정[Xiang et al. 2021], 객체 모양 변경[Yang et al. 2022]과 같은 신경장 편집 기술을 탐구했습니다.그러나 이러한 편집 절차에는 여전히 광범위한 사용자 입력이 필요합니다.최근 작업에서는 텍스트 프롬프트를 사용하여 NeRF 편집이 가능해졌지만[Haque et al. 2023], 명령어의 다양성이 제한되어 정확하고 고품질의 편집을 달성하는 데 어려움을 겪고 있습니다.결과적으로 기존 3D 자산의 재생성을 개선하여 사용하기 쉽고 정확한 3D 편집 방법을 개발하기 위한 추가 연구가 필요합니다.이 논문에서는 사용자가 텍스트 프롬프트를 사용하여 직관적이고 편리하게 신경장을 수정할 수 있는 프레임워크인 DreamEditor를 제시합니다. 그림 1에 도시된 바와 같이, 개나 복잡한 야외 환경과 같이 신경장으로 표현된 주어진 장면에 대해 텍스트 설명을 사용하여 텍스처링, 객체 대체, 객체 삽입을 포함한 다양한 객체 중심 편집을 달성하는 동시에 텍스트 프롬프트와 관련 없는 영역을 보존할 수 있습니다. 이는 방법에서 두 가지 핵심 설계를 통해 가능합니다. (1) 메시 기반 신경장 표현, (2) 3D 편집을 위해 사전 학습된 확산 모델을 활용하는 단계적 프레임워크. Zhuang et al. 암묵적 표현과 비교하여 명시적 메시 기반 신경장은 역투영을 통해 2D 편집 마스크를 3D 편집 영역으로 효율적으로 변환하여 마스크된 영역만 수정하여 정확한 로컬 편집을 용이하게 합니다. 또한 메시 표현은 지오메트리와 텍스처를 풀어서 모양 변경만 예상되는 경우 불필요한 지오메트리 변형을 방지합니다. 메시 표현의 장점을 활용하여 마스크된 영역 내에서 점수 증류 샘플링을 통해 간단한 텍스트 프롬프트를 사용하여 3D 장면을 효율적이고 정확하게 편집하는 단계별 미세 조정-지역화-최적화 프레임워크를 제안합니다. 동물, 인간 얼굴 및 야외 장면을 포함한 다양한 합성 및 실제 장면에서 DreamEditor를 광범위하게 평가합니다. 전체 이미지에서 작동하는 방법과 달리 편집 방식은 무관한 영역을 자연스럽게 보존하면서 정확한 국소 변형을 가능하게 합니다. 예를 들어, 그림 1에서 입에 장미를 물고 있을 때 개의 입만 수정됩니다. 또한 간단한 텍스트 프롬프트로 편집을 수행할 수 있으므로 절차가 사용자 친화적이고 신경 필드 편집을 크게 단순화하여 실용적인 응용 분야에서 큰 잠재력을 보여줍니다. 정성적 및 정량적 비교는 편집 정밀도, 시각적 충실도 및 사용자 만족도 측면에서 DreamEditor가 이전 방법보다 우수함을 보여줍니다. 이 논문의 기여는 다음과 같이 요약할 수 있다.(1) 다양한 실제 세계 장면에 대해 매우 사실적인 편집 결과를 얻는 텍스트 기반 3D 장면 편집을 위한 새로운 프레임워크를 소개한다.(2) 메시 기반 신경장을 사용하여 장면의 로컬 수정을 가능하게 하고 유연한 편집을 위해 텍스처와 기하학적 특징을 분리하는 것을 제안한다.(3) 텍스트 프롬프트에 따라 편집이 필요한 특정 영역을 먼저 식별한 다음 선택한 영역 내에서만 수정을 수행하는 단계적 편집 프레임워크를 고안한다. 이러한 체계적인 절차는 불필요한 영역에서 불필요한 수정을 최소화하면서 정확한 3D 편집을 보장한다.2 관련 연구 2.1 텍스트 기반 이미지 생성 및 편집 잡음 제거 확산 확률 모델[Ho et al. 2020; Song et al. 2020]은 고품질 이미지를 생성하는 능력으로 큰 주목을 받았다.나중에 확산 모델[Ramesh et al. 2022; Rombach et al. 2022; Saharia et al. 2022] 대규모 이미지-텍스트 쌍 데이터 세트에서 학습된 것은 명사, 형용사, 동사 등을 포함한 텍스트 프롬프트에서 복잡한 의미를 이해하고 해당 고품질 이미지를 생성하는 데 놀라운 성능을 보였습니다. 사전 학습된 텍스트-이미지 확산 모델의 풍부한 의미와 높은 제어 가능성으로 인해 일련의 연구[Avrahami et al. 2022; Couairon et al. 2022; Hertz et al. 2022; Kawar et al. 2022]에서 이를 텍스트 가이드 이미지 편집에 사용했습니다. 우리 작업과 가장 관련이 있는 것은 텍스트-이미지 확산 모델을 사용한 주제 중심 생성[Gal et al. 2022a; Ruiz et al. 2022]으로, 사용자는 특정 주제와 주어진 개념에 맞게 이미지 생성을 개인화할 수 있습니다. DreamBooth[Ruiz et al. 2022]는 희귀 토큰을 사용하여 언어-비전 사전을 확장하고 정규화를 위한 보존 손실로 모델을 미세 조정합니다. 마찬가지로 텍스트 역전[Gal et al. 2022a]는 사전 훈련된 확산 모델의 임베딩 공간에서 새로운 &quot;단어&quot;를 최적화하여 입력 객체를 표현합니다. DreamEditor: 신경 필드를 사용한 텍스트 기반 3D 장면 편집 SA 컨퍼런스 논문 &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니. 이러한 작업은 특정 이미지를 편집하거나 새로운 개념으로 이미지를 생성하는 작업을 다루지만 이러한 2D 방법을 3D로 확장하는 것은 사소한 일이 아닙니다. 2. 텍스트-3D 생성 텍스트-이미지 생성 모델이 개발되면서 텍스트-3D 생성에 대한 관심이 커지고 있습니다. 일부 작업에서는 CLIP 모델을 사용하여 메시[Chen et al. 2022; Michel et al. 2022; Mohammad Khalid et al. 2022] 또는 신경 필드[Jain et al. 2022]를 최적화합니다. 세미나 작업 DreamFusion[Poole et al. 2022]은 먼저 다음을 제안합니다. 영어: 점수 증류 샘플링(SDS) 손실을 사용하여 텍스트에서 3D로 생성하기 위한 사전 학습된 2D 텍스트에서 이미지로의 확산 모델에서 지식을 증류합니다. SDS 손실을 기반으로 하는 일련의 연구[Chen et al. 2023; Lin et al. 2022; Metzer et al. 2022; Raj et al. 2023]는 3D 표현을 사전에 도입하거나 변경하여 생성 결과를 더욱 개선합니다. 점수 야코비안 체이닝[Wang et al. 2022b]은 3D 점수를 2D 점수로 근사화하는 관점에서 유사한 훈련 목표에 도달합니다. 그러나 이러한 모든 방법은 기존 3D 장면을 편집하는 기능이 없습니다. 주된 이유 중 하나는 기존 3D 장면을 텍스트와 완전히 정렬하는 데 어려움이 있어 이러한 방법은 새 장면을 생성하고 편집 전후의 일관성이 깨지는 경향이 있기 때문입니다. 이러한 한계를 극복하기 위해 텍스트 프롬프트를 기반으로 기존 3D 장면을 편집할 수 있는 새로운 텍스트 가이드 3D 편집 프레임워크를 제안합니다.2.3 신경장 편집 신경장을 편집하는 것은 얽힌 모양과 모양으로 인해 본질적으로 어렵습니다.EditNeRF[Liu et al. 2021]는 잠재 코드를 조건으로 신경장의 모양과 색상을 편집하는 것을 지원하는 최초의 연구입니다.일부 연구[Bao et al. 2023; Gao et al. 2023; Wang et al. 2022a, 2023]는 CLIP 모델을 더욱 활용하여 텍스트 프롬프트나 참조 이미지로 편집할 수 있도록 합니다.또 다른 연구 라인은 미리 정의된 템플릿 모델이나 스켈레톤을 사용하여 재배치나 렌더링을 지원하지만[Noguchi et al. 2021; Peng et al. 2021], 특정 범주에 제한이 있습니다. 3D 편집은 인페인팅과 같은 2D 이미지 조작을 신경장 훈련과 결합하여 달성할 수도 있습니다 [Kobayashi et al. 2022; Liu et al. 2022]. 기하 기반 방법 [Li et al. 2022; Xu and Harada 2022; Yang et al. 2022; Yuan et al. 2022]은 신경장을 메시로 내보내고 메시의 변형을 암묵적 필드로 다시 동기화합니다. TEXTure [Richardson et al. 2023]는 반복적 확산 기반 프로세스를 사용하여 텍스트 프롬프트를 사용하여 메시의 텍스처를 생성합니다. 우리와 가장 유사한 작업은 Instruct-NeRF2NeRF [Haque et al. 2023]와 Vox-E [Sella et al. 2023]로, 신경장 텍스트 프롬프트를 자유롭게 편집합니다. Instruct-NeRF2NeRF는 이미지 기반 확산 모델 [Brooks et al. 2022] 신경장을 최적화하기 위한 지침으로 입력 이미지를 편집합니다. 그럼에도 불구하고 전체 이미지를 조작하기 때문에 일반적으로 원치 않는 영역도 변경됩니다. Vox-E는 SDS 손실을 채택하고 2D 교차 주의 맵을 통해 3D 공간에서 로컬 편집을 수행합니다. 그러나 Vox-E의 체적 표현에 내재된 제약으로 인해 실제 장면의 편집 품질은 여전히 최적이 아닙니다. 3 배경 SDS 손실을 사용한 신경장 최적화. DreamFusion[Poole et al. 2022]은 3D 생성을 위한 사전 텍스트-이미지(T2I) 확산 모델을 증류하기 위해 점수 증류 샘플링(SDS) 손실을 제안했습니다. 먼저 t 수준에서 임의의 렌더링된 뷰 Î에 임의의 가우시안 노이즈를 추가하여 Ît를 얻습니다. 사전 학습된 확산 모델 ø는 Ît와 입력 텍스트 조건 y가 주어졌을 때 추가된 노이즈를 예측하는 데 사용됩니다. SDS 손실은 다음과 같이 픽셀당 그래디언트로 계산됩니다. VLSDs (4,Î = g(0)) = E€‚t aÎ дв (1) 여기서 w(t)는 노이즈 레벨 t에 따라 달라지는 가중치 함수이고, O는 신경장의 매개변수이며, g는 렌더링 프로세스입니다. 학습하는 동안 확산 모델이 동결되고 그래디언트가 0으로 역전파되어 신경장의 렌더링이 텍스트 조건 y를 사용하여 확산 모델에서 생성된 이미지와 유사하도록 강제합니다. DreamBooth [Ruiz et al. 2022]는 T2I 모델을 기반으로 하는 주체 중심 이미지 생성 방법입니다. 동일한 주체의 이미지 몇 개가 주어지면 DreamBooth는 주체를 고유 식별자(*로 표시)에 바인딩하여 T2I 확산 모델에 포함합니다. L 재구성 손실을 사용하여 입력 이미지의 확산 모델을 미세 조정하고 클래스 사전 보존 손실을 사용하여 과적합을 방지합니다. 자세한 훈련 내용은 Ruiz et al [2022]에서 확인할 수 있습니다. 이 논문에서는 또한 특정 장면을 표현하기 위한 T2I 확산 모델을 미세 조정하기 위해 DreamBooth를 채택합니다. 4. 방법 개요 방법의 입력은 편집할 3D 장면의 포즈 이미지 세트와 편집을 위한 텍스트 프롬프트입니다. 목표는 텍스트 프롬프트에 따라 원래 3D 장면에서 관심 객체의 모양과 모양을 변경하는 것입니다. 그림 3은 말 조각품을 실제 기린으로 바꾸는 예를 보여줍니다. 이 작업에서는 텍스트 프롬프트와 관련 없는 3D 콘텐츠를 편집 전후에 변경하지 않아야 합니다. DreamEditor의 프레임워크는 그림 3에 나와 있으며 세 단계로 구성되어 있습니다. 먼저 원래 신경 광도장을 메시 기반 신경장(섹션 4.2)으로 변환하여 공간 선택적 편집을 달성합니다. 4.3절에서 T2I 모델을 입력 장면에 맞게 사용자 정의하고 교차 주의 맵을 사용하여 텍스트 프롬프트의 키워드에 따라 3D 공간에서 편집 영역을 찾습니다.마지막으로 T2I 확산 모델을 통해 텍스트 프롬프트의 제어를 받는 신경 필드에서 대상 객체를 편집합니다(4.4절).4.2 신경 필드 추출 [Yang et al. 2022]에서 영감을 얻어 먼저 입력 이미지에서 신경 광도장을 학습하고 명시적 메시로 구성된 여러 로컬 암묵적 필드로 분해합니다.여기서 메시는 행진 큐브를 사용하여 신경 광도장에서 추출됩니다[Lorensen and Cline 1987].장면을 메시 기반 신경 필드로 표현하면 두 가지 이점이 있습니다.첫째, 메시 기반 신경 필드를 사용하면 장면의 특정 영역을 정확하게 편집할 수 있습니다.배경 및 무관한 객체와 같은 영역은 특정 암묵적 필드를 수정하여 편집 중에 변경되지 않을 수 있습니다. 둘째, 추출된 메시는 장면에서 객체의 표면과 윤곽을 명시적으로 표현할 수 있습니다.폭셀[Liu et al. 2020] 및 포인트 클라우드[Ost et al. 2022]와 같은 다른 명시적 표현과 비교하면 2023년 12월 12-15일, 호주 NSW 시드니에서 개최된 SA Conference Papers &#39;23에서 원본 신경장 메시 기반 신경장 메시 Distill DreamBooth &quot;a + 기린&quot; 1. 증류 샘플 포인트 레이 캐스팅 특징 표현 2. 편집 영역 찾기 주의 얻기 ← 역투영 메시 기반 신경장 SDS 손실 DreamBooth 3. &quot;a + 기린&quot; 최적화 ➡렌더링 -&gt; 역전파 Zhuang et al. 그림 2: 방법 개요.우리의 방법은 기존 신경장을 최적화하여 대상 텍스트 프롬프트에 맞게 3D 장면을 편집합니다.편집 프로세스는 세 단계로 구성됩니다.(1) 원본 신경장을 메시 기반 신경장으로 증류합니다. (2) 텍스트 프롬프트를 기반으로, 우리의 방법은 자동으로 메시 기반 신경 필드의 편집 영역을 식별합니다. (3) 우리의 방법은 SDS 손실을 활용하여 편집 영역의 색상 특징 fc, 기하 특징 fg 및 정점 위치를 최적화하여 해당 영역의 질감과 기하를 변경합니다. 색상으로 보는 것이 가장 좋습니다. 메시로 편집 영역의 범위를 결정하는 것이 더 편리합니다. 확산 모델의 어텐션 스킴을 결합하여 편집 영역을 자동으로 결정하는 방법을 제안하는데, 이는 입력 텍스트에 따라 메시에서 편집 영역을 정확하게 찾을 수 있습니다. 구체적으로, 신경 광도장을 얻은 후, 교사-학생 기반 훈련 프레임워크를 채택하여 증류를 수행합니다. 여기서 신경 광도장은 학생 모델을 안내하는 교사 모델, 즉 메시 기반 신경 필드로 간주됩니다. 우리는 각 메시 정점 va에 색상 특징 fc와 기하 특징 fg를 할당하여 각각 v 근처의 로컬 모양과 질감 정보를 나타내는 메시 기반 신경 필드를 정의합니다. 볼륨 렌더링 프로세스 동안 샘플링된 지점 x에 대해 먼저 x의 가장 가까운 상위 K 정점의 피처를 역거리(Vk x)로 가중하여 보간하여 집계된 피처 fc 및 fg를 구합니다[Qi et al. 2017]: ft(x) = Σk-1Wkft.k ΣΚ¯, Wk = k=1Wk , t = {g, c} ||vkx|| (2) 그런 다음 fg 및 fe는 x의 s-밀도 s 및 색상 c로 디코딩됩니다: S= DG (fg, h), C = Dc (fc, h, d, Vxs) (3) 여기서 DG 및 DC는 각각 기하 디코더 및 색상 디코더이고, h는 x에서 Vk로 보간된 부호 거리이고, d는 광선 방향이며 Vxs는 지점 x에서 s-밀도 s의 그래디언트입니다. 네트워크의 프레임워크는 그림 9에 나와 있습니다.증류 프로세스 동안 장면에서 광선 r을 무작위로 샘플링하고 r이 주어진 교사 모델의 출력을 기준 진실로 사용합니다.여기에는 이 광선의 각 샘플링 지점 x의 렌더링된 픽셀 색상 Ĉ(r), s-밀도 ŝ; 및 포인트 색상 ĉ;이 포함됩니다.증류 손실은 다음과 같이 계산됩니다.Ldis = ŝi-Si| + ||ĉi - Ci||) + Σ ||Ĉ(r) – C(r)||₁₂, (4) reRieN rЄR 여기서 교사 및 학생 모델(즉, Ĉ 및 C)의 볼륨 렌더링 공식은 NeuS [Wang et al. 2021]와 동일합니다.또한 샘플링된 지점에 Eikonal 손실 [Gropp et al. 2020]을 추가하여 가중치 λreg = 0.Lreg = reRieN x Si - - 1 || 2. (5) 우리 프레임워크에서 모든 카메라 포즈 샘플링은 구면 좌표계를 기반으로 합니다. 대상 객체를 원점으로 변환하고 y축을 위쪽으로 향하게 합니다. 다음 위치 지정 및 최적화 단계에서 고도 및 방위각 범위를 설정하여 샘플링된 뷰의 범위를 제한하여 편집 효율성을 개선합니다. 4.3 편집 영역 찾기 그림 2의 중간 부분에서 설명한 대로 텍스트 프롬프트가 주어지면 DreamEditor는 먼저 렌더링된 뷰에서 대상 편집 영역을 결정합니다. 준비 단계로 먼저 샘플링된 뷰를 사용하여 DreamBooth로 안정적 확산 모델을 미세 조정하여 모델의 지식을 특정 장면에 적용합니다. 그런 다음 미세 조정된 확산 모델을 사용하여 각 렌더링된 뷰에 대한 2D 마스크를 얻습니다. 마지막으로 마스크된 대상 영역을 다른 뷰에서 메시로 역투영하여 3D 편집 영역을 얻습니다. 위치 지정은 T2I 확산 모델의 교차 어텐션 레이어가 생성된 이미지의 레이아웃과 각 단어 간의 관계를 제어한다는 사실에 의해 동기가 부여됩니다[Hertz et al. 2022]: M = Softmax(QK√), 여기서 Q는 차원 q의 노이즈 이미지의 공간적 특징에서 투영된 쿼리 특징이고, K는 텍스트 임베딩에서 투영된 키 행렬이고, M은 각 픽셀에 대한 토큰의 가중치를 정의하는 어텐션 맵입니다. 따라서 M은 픽셀이 텍스트 프롬프트의 단어에 해당하고 편집 영역을 찾는 데 사용될 수 있는 확률을 나타냅니다. 구체적으로, 렌더링된 뷰의 노이즈 이미지 it와 텍스트 프롬프트는 노이즈 제거를 위해 확산 모델에 입력됩니다. 의도된 편집 결과를 나타내는 키워드(예: 그림 3의 &quot;앞치마&quot;, &quot;기린&quot;, &quot;모자&quot;)를 선택하고 생성 프로세스 중에 생성된 모든 어텐션 맵을 추출합니다. 실제로, 확산 모델의 백본은 일반적으로 L개의 합성곱 블록으로 구성되며, 여기에는 H개의 멀티헤드 어텐션 레이어가 장착되어 있습니다[Vaswani et al. 2017]. 따라서 T라운드의 노이즈 제거 후, 최종 어텐션 맵 M 세트는 {Mt,1,h}로 표현될 수 있습니다. 여기서 t, l, h는 각각 시간 단계, 합성곱 블록, 어텐션 헤드의 인덱스를 나타냅니다. 선형 보간을 통해 모든 어텐션 맵의 크기를 조정하고 이를 집계하여 집계된 어텐션 맵 M을 얻습니다. M은 [0,1]로 추가로 정규화되고 임계값 7 = 0.75로 이진화됩니다. 여기서 값이 1인 영역은 편집 영역입니다. 마스크의 편집 영역에 속하는 모든 픽셀을 메시에 역투영하고 교차된 메시 면을 편집 영역으로 표시합니다. 키워드가 초기 장면의 객체에 국한되지 않는다는 점을 강조할 가치가 있습니다. 키워드의 어텐션 맵은 생성된 이미지에서 키워드가 존재할 가능성이 매우 높은 영역을 구분하기 때문입니다. 그림 6에서 볼 수 있듯이 &quot;선글라스&quot;가 원래 장면의 일부가 아니더라도 장면 메시에서 합리적인 영역을 식별하는 것은 여전히 가능합니다. 이 단계에서는 샘플링된 뷰의 범위 내에서 45° 간격으로 모든 고도 및 방위각을 횡단하여 모든 잠재적 편집 영역의 적용 범위를 보장합니다. 그런 다음 모든 샘플링된 뷰의 마스크를 가져와 메시에 역투영합니다. 역투영 결과를 병합한 후 두 단계를 사용하여 마스크된 영역을 정제합니다.(1) 삭제: 전체 투영 면적의 10% 미만인 면의 수가 있는 편집 영역 내의 작은 조각을 삭제합니다.이는 일반적으로 부정확한 2D 마스크(예: 대상 객체보다 큰 마스크가 객체 외부에 투영됨)에서 발생합니다.(2) 채우기: 너비 우선 탐색을 사용하여 편집 영역의 &quot;구멍&quot;, 즉 편집 영역으로 둘러싸인 비편집 영역을 채웁니다.이러한 &quot;구멍&quot;은 일반적으로 가려진(예: 말의 엉덩이) 또는 오목한 영역에서 발생합니다.이러한 영역을 편집 영역에 통합하여 편집 영역의 완전성을 높입니다.최종 편집 영역을 V = {ve}=4로 표시합니다.4 편집 영역 최적화 이 단계에서는 DreamFusion의 SDS 손실을 채택합니다[Poole et al. 2022] T2I 확산 모델을 사용하여 신경 필드에서 편집 영역의 최적화를 안내하여 장면을 텍스트 프롬프트에 맞춥니다. 무작위로 렌더링된 뷰와 텍스트 프롬프트를 T2I 확산 모델에 공급하여 SDS 손실을 계산하고 그래디언트를 신경 필드로 역전파합니다. DreamFusion의 Imagen [Saharia et al. 2022]은 독점적이므로 안정적 확산 [Rombach et al. 2022] 다음과 같습니다. VwLSDS(0,9(w)) = Ee,tt[w(1)(€¢ (Zz;y, 1) — €) дл aÎ aw(6) 여기서 w = {fg,k, fc,k, Vk}k는 V의 모든 메시 정점에 대한 기하 특징, 색상 특징 및 위치의 집합이고, zt는 노이즈가 있는 잠재 값을 나타내고, z는 안정 확산 모델의 인코더에서 생성된 원래 잠재 값입니다. 방정식 6에서 볼 수 있듯이 학습 중에 편집 영역의 정점의 색상 특징 fc 및 기하 특징 fg를 최적화하는 것 외에도 정점의 위치도 포함됩니다. 이는 메시의 구조도 최적화 중에 동적으로 조정됨을 의미하며, 이는 접근 방식의 중요한 부분입니다. 로컬 암시적 필드에서 기하 특징은 주로 정점 근처의 모양 세부 정보를 나타냅니다. 정점에서 떨어진 지점의 s-밀도에 상당한 변화가 있으면 객체 표면의 매끄러움이 깨집니다.따라서 정점 위치와 기하적 특징을 동시에 최적화하는 보완적 최적화 접근 방식을 제안합니다.정점 위치를 최적화하면 메시의 전체 모양이 텍스트 프롬프트에 맞게 조정되고 기하적 특징을 최적화하면 객체의 로컬 기하가 정제됩니다.이 최적화 접근 방식을 사용하면 DreamEditor에서 장미 꽃잎과 같은 복잡한 모양을 생성할 수 있습니다.5절의 절제 연구는 정점 위치와 기하적 특징의 공동 최적화가 필요함을 보여줍니다.정점 위치 최적화 중에 매끄러운 표면을 유지하고 자연스러운 변형을 장려하기 위해 라플라시안 손실과 ARAP(as-rigid-as-possible) 손실을 포함하여 널리 사용되는 메시 정규화 용어를 도입합니다[Sumner et al. 2007]: ELlap Vi E Σνί ||vi |je, vi² V jЄNi (7) , i=E LARAP = |||V; Vj2-V; - Vj2, i=1 jЄNi (8) 여기서 N;은 정점 vi에 대한 원링 이웃의 집합이고, v&#39;는 마지막 반복에서의 정점 위치를 나타냅니다. 각각 균형을 맞추기 위해 lap = 10-4 및 λARAP = 10-4로 설정했습니다. 각 반복에서 최적화하는 동안 SDS 손실과 메시 정규화 항을 모두 수행합니다. SDS 및 정규화 항을 별도로 최적화하면 경험적으로 더 나은 결과를 얻을 수 있음을 발견했습니다. 렌더링된 뷰가 주어지면 먼저 SDS 손실을 사용하여 편집 영역의 fc, fg, v를 최적화합니다. 그런 다음 fc와 fg는 고정되고 메시 정규화 항을 사용하여 v만 최적화됩니다.실험 5.1 실험 설정 데이터 세트. 영어: 다양한 장면에서 우리 방법의 효과를 검증하기 위해, 우리는 네 가지 데이터 세트에서 복잡도 수준이 다른 여섯 가지 장면을 선택했습니다: DTU [Jensen et al. 2014], BlendedMVS [Yao et al. 2020], Co3D [Reizenstein et al. 2021], GL3D [Shen et al. 2018]. 이러한 장면에는 간단한 배경의 객체, 인간의 얼굴, 복잡한 배경이 있는 야외 장면이 포함됩니다. 우리는 각 데이터 세트에서 고해상도 이미지와 해당 카메라 포즈를 사용하여 원래의 신경 필드를 학습합니다. 그런 다음 텍스트 프롬프트에 따라 원래 장면을 편집합니다. 기준선. 우리는 세 가지 기준선과 비교합니다. (1) D-DreamFusion*: Instruct-N2N에서 지적했듯이, DreamFusion은 장면과 일치하는 정확한 텍스트 설명을 찾는 것이 어려워서 신경 필드를 편집하지 못합니다. 특정 장면에 대한 더 나은 신경 표현을 학습하기 위해, 우리는 Stable-DreamFusion을 DreamBooth [Ruiz et al. 2022]를 또 다른 기준으로 사용합니다.(2) Instruct-NeRF2NeRF(InstructN2N): 또한 최근 연구인 Instruct-NeRF2NeRF와 비교하고 논문 [Haque et al. 2023]에서 제공하는 텍스트 지침을 사용하여 3D 장면을 편집합니다.(3) NeRF-Art [Wang et al. 2023]: NeRF-Art는 스타일화된 편집만 지원하므로 스타일화 작업에서 비교합니다.평가 기준.[Haque et al. 2023]에 따라 CLIP 텍스트-이미지 방향 유사성을 사용하여 이미지와 텍스트 프롬프트의 변경 간의 정렬 정도를 평가합니다.SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 원래 Instruct-N2N Zhuang et al. D-DreamFusion* Ours &quot;Give it a blue apron&quot; &quot;A doll wearing a blue apron&quot; &quot;A doll wearing a blue apron&quot; &quot;Turn the stone horse into a giraffe&quot; &quot;A* giraffe&quot; &quot;A * giraffe&quot; &quot;Give him a cowboy hat&quot; &quot;A man wearing a cowboy hat&quot; &quot;A man wearing a cowboy hat&quot; 그림 3: 세 가지 다른 장면에서 두 개의 기준선과 비교한 우리 방법의 시각적 결과. 결과는 DreamEditor가 관련 영역을 정확하게 찾고, 텍스트에 충실한 편집을 수행하고, 기준선 방법으로는 달성하기 어려운 바람직하지 않은 수정을 방지할 수 있음을 분명히 보여줍니다. 자세한 정의는 [Gal et al. 2022b]에서 찾을 수 있습니다. 각 편집 결과에 대해 편집 영역 주변의 50개 관점을 균일하게 샘플링하고 평균값을 결과로 취합니다. CLIP 방향 유사성은 편집 품질을 대략적으로만 평가할 수 있으므로, 우리는 또한 사용자 연구를 수행하여 인간의 평가를 얻습니다. 우리는 설문지 50부를 나눠주고, 모든 방법의 회전 비디오 결과를 나란히 제시하고 사용자에게 최상의 편집 결과를 선택하도록 요청합니다.투표율은 각 방법에 대해 계산됩니다.우리는 총 20개의 개별 편집 작업을 포함하는 4개의 선택된 장면에서 우리의 방법을 앞서 언급한 기준선과 비교합니다.양식화된 편집만 지원하기 때문에 NeRF-Art는 정량적 비교에서 제외합니다.구현 세부 정보.우리의 실험에서는 Neus를 채택하여 원래 신경장을 학습합니다.학습 매개변수는 [Wang et al. 2021]에서 찾을 수 있습니다.확산 모델의 경우 공개 사전 학습된 안정적 확산 모델 V2를 사용합니다.각 원래 신경장에 대해 위치 지정 단계에서 렌더링된 이미지를 사용하고 DreamBooth를 적용하여 500번의 반복에 걸쳐 안정적 확산 모델을 미세 조정합니다.증류 단계에서 lr = 10−4인 Adam 옵티마이저를 사용하여 100K 반복에 대해 로컬 필드를 최적화합니다. 최적화 단계에서는 렌더링된 이미지의 크기가 96×96에서 192×192로 점차 증가합니다. lr = : 10¯²인 Adam 최적화기를 설정하여 편집 영역의 정점 fc, fg, v를 2K 반복에 대해 최적화합니다. Pytorch에서 편집 프레임워크를 구현합니다. 5.2 정성적 결과 3D 장면 편집 결과. 그림 1과 그림 10에서 방법의 정성적 결과를 제공합니다. 결과는 우리 방법이 다양한 장면에서 신경장의 표적 편집을 효과적으로 수행할 수 있음을 보여줍니다. 그림 1의 가운데 행에 나와 있듯이 야외 정원과 같은 복잡한 장면에서도 우리 방법은 말 조각품을 편집 영역으로 정확하게 결정하여 고품질 텍스처와 기하학을 갖춘 사슴이나 기린으로 바꿀 수 있습니다. 또한, 우리의 방법은 그림 1 하단의 개에게 선글라스를 씌우는 것과 같은 로컬 편집이 가능합니다. 특히, 그림 7에서 볼 수 있듯이, 우리의 방법으로 생성된 편집 결과는 추출된 메시에서 직관적으로 관찰할 수 있듯이 3D 지오메트리에서 뛰어난 일관성을 보여줍니다. 그림 3은 우리의 방법의 결과를 기준선과 비교한 것입니다. Instruct-N2N은 추상적인 작업(예: 인형에게 앞치마를 입히기)을 실행하는 데 어려움이 있으며 일부 장면에서는 최적이 아닌 결과를 생성합니다. 이는 주로 Instruct-Pix2Pix 모델이 항상 신뢰할 수 있는 것은 아니며 전체 이미지에서 작동하기 때문입니다. 따라서 Instruct-N2N은 전체 장면을 변경하며 Instruct-Pix2Pix 학습 세트를 넘어서는 명령을 실행할 때 성능이 저하될 수 있습니다. D-DreamFusion*의 DreamBooth 미세 조정을 통해 T2I 확산 모델은 첫 번째 행의 장난감과 세 번째 행의 남자와 같은 입력 객체의 표현을 대략적으로 학습할 수 있습니다. 그러나 실제 세계 장면의 복잡성과 다양성으로 인해 D-DreamFusion*은 특정 장면을 정확하게 표현할 수 없어 D-DreamFusion*에서 편집한 장면의 무관한 영역이 크게 변경됩니다(예: 첫 번째 행의 인형 변형, 두 번째 행의 배경).게다가 모든 비교 기준선은 복잡한 장면(예: 두 번째 행의 정원)에서 편집 전후 장면의 일관성을 보장할 수 없으며, 편집 프로세스로 인해 전체 장면이 변경될 수 있습니다.반면에 저희 방법은 세부 정보가 더 많고 텍스트 프롬프트의 내용을 충실하게 생성하는 동시에 편집 전후 입력 객체와 장면의 일관성을 성공적으로 유지합니다.스타일화 작업의 결과.그림 8에서 볼 수 있듯이 저희 방법을 NeRF-Art 및 Instruct-N2N과 비교합니다.이 작업에서는 DreamEditor: 신경장을 사용한 텍스트 기반 3D 장면 편집을 생략합니다.표 1: CLIP 텍스트-이미지 방향 손실 및 사용자 연구 결과. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 방법 CLIP 텍스트-이미지 방향 유사성 ↑ 편집 성능 투표 비율 ↑ D-DreamFusion* 12.Instruct-N2N 10.Ours 18.Original (1) w/o locating 12.1% 6.8% 81.1% (2) Ours 그림 4: locating step의 절제 연구. locating step 없이 편집하면 인형이 변형되어 객체의 일관성이 깨집니다. locating step을 사용하여 전체 장면을 스타일링합니다. 스타일링 편집은 주관적인 작업이기 때문에 정성적 결과만 참고로 제공하고 정량적 분석은 수행하지 않습니다. 편집 영역 찾기 결과. 그림 6에서 편집 영역 찾기 방법의 결과도 보여줍니다. 이 방법이 합리적인 편집 영역을 정확하게 찾을 수 있음을 알 수 있습니다. 5.3 정량적 결과 표 1에서 CLIP 텍스트-이미지 방향 손실의 결과를 제시합니다. 결과는 우리 방법이 훨씬 더 높은 점수를 달성한다는 것을 분명히 보여주며, 우리 방법이 더 선명하고 편집된 텍스트 프롬프트와 더 일치하는 모양과 질감을 생성한다는 것을 나타냅니다. 또한 우리 방법은 81.1% 이상의 투표를 받아 다른 방법을 상당한 차이로 앞지릅니다. 이는 DreamEditor가 다양한 장면에서 훨씬 더 높은 사용자 만족도를 달성할 수 있음을 더욱 보여줍니다. 5. 절제 연구 위치 지정 단계의 효과. 위치 지정 단계의 필요성을 보여주기 위해 두 가지 변형을 설계합니다. (1) 위치 지정 없음: 위치 지정 단계를 생략하고 메시의 모든 로컬 암시적 필드를 최적화합니다. (3) 우리 방법: 위치 지정 단계를 통해 편집 영역을 결정하고 최적화에서 편집하지 않는 영역을 수정합니다. 그림 4(1)에서 볼 수 있듯이, 위치 지정 단계 없이 편집하면 인형의 팔을 짧게 하는 등 장면의 무관한 영역이 실수로 변경되어 객체의 일관성이 손상됩니다.반면에 위치 지정 단계를 사용하면 프레임워크에서 관심 영역만 최적화할 수 있습니다.최적화 접근 방식의 효과.최적화 중에 최적화 접근 방식이 더 자세한 3D 모양을 생성할 수 있는지 평가하기 위해 다음과 같이 DreamEditor의 세 가지 변형을 사용하여 절제합니다.(1) 고정 v: 업데이트 프로세스 중에 메시 구조를 고정하고 지오메트리 피처만 최적화합니다.(2) 고정 fg: 지오메트리 피처를 최적화하지 않고 메시 구조만 변경합니다.(3) 저희 방법: v와 fg를 동시에 최적화합니다.평가하기 어려운 장면을 선택합니다.컵 위의 장미를 생성합니다.(1) 고정 v (2) 고정 fg (3) 저희 방법 그림 5: 최적화 접근 방식에 대한 절제 연구. 분명히, 기하학적 특징과 정점 위치(저희의 것)를 동시에 최적화하고 더욱 자세하고 사실적인 3D 모양의 빨간 장미를 생성합니다. 생성된 결과의 렌더링된 이미지와 마칭 큐브 알고리즘을 사용하여 추출된 3D 모양을 그림 5에 표시합니다. 그림 5(1)은 스파이크로 가득 찬 정점 위치를 고정하여 생성된 장미를 표시합니다. 이는 메시 표면에서 멀리 떨어진 영역에서 암묵적 필드에 걸쳐 샘플링 포인트의 s-밀도의 부드러움을 제한하는 것이 매우 어렵기 때문입니다. 그림 5(2)에 표시된 것처럼 기하학적 특징을 고정하면 거친 모양을 생성할 수 있지만 세부 정보가 부족합니다. 반면에 저희의 방법은 기하학적 특징과 정점 위치를 동시에 최적화하여 스파이크를 제거하고 더욱 자세한 꽃봉오리와 꽃잎을 생성합니다.
--- CONCLUSION ---
및 제한 사항 이 논문에서는 신경 필드로 표현된 3D 장면을 편집하기 위한 텍스트 기반 프레임워크인 DreamEditor를 제시합니다. 신경 필드와 원하는 편집 내용을 설명하는 텍스트 프롬프트가 주어지면 DreamEditor는 장면 내의 편집 영역을 자동으로 식별하고 그에 따라 지오메트리와 텍스처를 수정합니다. 얼굴, 객체, 대형 야외 장면을 포함한 다양한 장면에 대한 실험은 DreamEditor의 강력한 편집 기능을 보여주며, 다른 기준선과 비교하여 고품질 텍스처와 모양을 생성하는 동시에 편집된 장면이 입력 텍스트 프롬프트와 일관성을 유지하도록 보장합니다. DreamEditor의 제한 사항에는 DreamFusion에서 상속된 문제인 Janus 문제가 포함되며, 생성된 객체는 다른 관점에서 정면도로 나타납니다. 또한 DreamEditor는 환경 조명을 직접 모델링하지 않아 조명 조건에 대한 제어가 제한됩니다. DreamEditor는 일반적으로 잘 작동하지만 편집 시 렌더링된 뷰의 종속성으로 인해 장면에 상당한 자체 폐색이 있는 경우 성능이 저하되어 최종 합성 결과에 영향을 미칠 수 있습니다. NeuS가 경계 없는 장면에서 배경을 효과적으로 재구성하는 데 어려움을 겪는다는 점을 고려할 때, 현재는 장면의 전경에서 객체 중심 편집에 중점을 두고 있습니다. 향후 작업에서는 BakedSDF[Yariv et al. 2023]와 같은 최근의 경계 없는 실제 세계 장면 메시 재구성 방법을 결합하여 전체 장면 편집으로 방법을 확장할 수 있습니다. 감사의 말 이 연구는 중국 국가자연과학기금(NO. 62322608, 61976250), 베이항대학 가상현실 기술 및 시스템 국가중점실험실 오픈프로젝트 프로그램(No. VRLAB2023A01), 광둥 기초 및 응용 기초 연구 기금(NO. 2020B1515020048)의 지원을 받았습니다. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 &quot;콧수염이 있는 남자&quot; &quot;웃는 * 개&quot; &quot;선글라스를 낀 인형&quot; 원본 &quot;사슴&quot; Zhuang et al. &quot;기린 *&quot; 그림 6: 편집 영역의 시각화. 굵은 글씨는 키워드를 나타내고 메시의 빨간색 영역은 편집 영역을 나타냄. 그림 7: 편집 결과에서 추출한 메시의 시각화. 우리의 Instruct-N2N NeRF-Art 원본 &quot;빈센트 반 고흐&quot; &quot;야수주의&quot; &quot;톨킨 엘프&quot; &quot;볼드모트 경&quot; 그림 8: NeRF-Art 및 Instruct-NeRF2NeRF와 비교한 스타일 편집 결과의 시각화. x-&gt; fg 256Sy(h) 집계 기능 Fc 256 →→→→256 256 — C→ y(d) 그림 9: 메시 기반 신경망 네트워크. 샘플링된 지점 x와 광선 방향 d를 입력으로 하여 s-밀도 s와 색상 c를 출력합니다.y(.)는 NeRF에서 채택한 위치 인코딩을 나타냅니다[Mildenhall et al. 2021].DreamEditor: 신경장을 사용한 텍스트 기반 3D 장면 편집 SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW주 시드니 원래 신경장 &quot;A * zebra&quot; * &quot;A robot horse&quot; * &quot;A horse with rainbow hair&quot; &quot;A * giraffe&quot; &quot;A * deer&quot; 원래 신경장 * &quot;A Ironman doll&quot; * &quot;A doll wearing sunglasses&quot; * &quot;A doll wearing a red top hat&quot; &quot;A * gold doll&quot; * &quot;A doll with tiger pattern&quot; 원래 신경장 &quot;A * * man wearing sunglasses&quot; &quot;A * * man wearing a navy hat&quot; &quot;A * * man wearing a cowboy hat&quot; &quot;A * * man wearing a clown man&quot; * 그림 10: 추가 편집 결과. &quot;콧수염을 기른 남자&quot; * SA Conference Papers &#39;23, 2023년 12월 12-15일, 시드니, NSW, 호주 참고 문헌 Omri Avrahami, Dani Lischinski 및 Ohad Fried. 2022. 자연 이미지의 텍스트 중심 편집을 위한 혼합 확산. CVPR 2022. 18208–18218. Chong Bao, Yinda Zhang 및 Bangbang 외. 양. 2023. Sine: 사전 안내 편집 필드를 사용한 의미 기반 이미지 기반 너프 편집. CVPR 2023. 20919-20929. 팀 브룩스, 알렉산더 홀린스키, 알렉세이 A 에프로스. 2022. Instructpix2pix: 이미지 편집 지침을 따르는 방법을 학습합니다. arXiv 사전 인쇄 arXiv:2211.09800 (2022). Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia 및 Huchuan Lu. 2021. 단안 RGB 비디오에서 애니메이션 가능한 신경 방사 필드. arXiv 사전 인쇄 arXiv:2106.13629 (2021). Rui Chen, Yongwei Chen, Ningxin Jiao 및 Kui Jia. 2023. Fantasia3D: 고품질 텍스트-3D 콘텐츠 제작을 위한 기하학 및 모양 분리. arXiv 사전 인쇄 arXiv:2303.13873 (2023). Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang 및 Kui Jia. 2022. Tango: 조명 분해를 통한 텍스트 기반의 사실적이며 강력한 3D 스타일화. arXiv 사전 인쇄 arXiv:2210.11277 (2022). Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2022. Diffedit: 마스크 가이드를 사용한 확산 기반 의미적 이미지 편집. arXiv 사전 인쇄본 arXiv:2210.11427 (2022). Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022a. 이미지는 한 단어의 가치가 있다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618 (2022). Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022b. StyleGAN-NADA: 이미지 생성기의 CLIP 가이드 도메인 적응. ACM Transactions on Graphics(TOG) 41, 4(2022), 1-13. William Gao, Noam Aigerman, Thibault Groueix, Vladimir G Kim, Rana Hanocka. 2023. TextDeformer: 텍스트 가이드를 사용한 기하 조작. arXiv 사전 인쇄본 arXiv:2304.13348(2023). Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman. 2020. 도형 학습을 위한 암묵적 기하 정규화. arXiv 사전 인쇄본 arXiv:2002.10099(2020). Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, Angjoo Kanazawa. 2023. Instruct-NeRF2NeRF: 지침이 있는 3D 장면 편집. arXiv 사전 인쇄본 arXiv:2303.12789(2023). Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. 2022. 교차 주의 제어를 통한 프롬프트 간 이미지 편집. arXiv 사전 인쇄본 arXiv:2208.01626(2022). Jonathan Ho, Ajay Jain, Pieter Abbeel. 2020. 확산 확률적 모델의 노이즈 제거. NeurIPS 2020 33(2020), 6840-6851. Ajay Jain, Ben Mildenhall, Jonathan T Barron, et al. 2022. 꿈 필드를 사용한 제로샷 텍스트 가이드 객체 생성. CVPR 2022에서. 867-876. Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola 및 Henrik Aanæs. 2014. 대규모 다시점 입체시 평가. CVPR 2014. 406-413. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri 및 Michal Irani. 2022. Imagic: 확산 모델을 사용한 텍스트 기반 실제 이미지 편집. arXiv 사전 인쇄 arXiv:2210.09276 (2022). 고바야시 소스케, 마츠모토 에이이치, 빈센트 시츠만. 2022. 기능 필드 증류를 통한 편집을 위한 너프 분해. arXiv 사전 인쇄 arXiv:2205.15585 (2022). Yuan Li, Zhi-Hao Lin, David Forsyth, Jia-Bin Huang, Shenlong Wang. 2022. ClimateNeRF: 극한 기후 합성을 위한 물리적 기반 신경 렌더링. arXiv e-prints (2022), arXiv-2211. Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin. 2022. Magic3D: 고해상도 텍스트-3D 콘텐츠 생성. arXiv 사전 인쇄본 arXiv:2211.10440 (2022). Hao-Kang Liu, I Shen, Bing-Yu Chen, et al. 2022. NeRF-In: RGB-D 사전을 사용한 자유형 NeRF 인페인팅. arXiv 사전 인쇄본 arXiv:2206.04901 (2022). Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, et al. 2020. Neural sparse voxel fields. NeurIPS 2020 33 (2020), 15651-15663. Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell. 2021. Editing conditional radiance fields. In ICCV 2021. 5773-5783. William E Lorensen and Harvey E Cline. 1987. Marching cubes: A high resolution 3D surface construction algorithm. ACM siggraph computer graphics 21, 4 (1987), 163-169. Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2022. 3D 모양과 텍스처의 모양 기반 생성을 위한 Latent-NeRF. arXiv 사전 인쇄본 arXiv:2211.07600 (2022). Oscar Michel, Roi Bar-On, Richard Liu, and et al. 2022. Text2mesh: 메시를 위한 텍스트 기반 신경 스타일화. CVPR 2022에서. 13492-13502. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2021. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현. Commun. ACM 65, 1 (2021), 99–106. Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. 2022. CLIP-Mesh: 사전 학습된 이미지-텍스트 모델을 사용하여 텍스트에서 텍스처 메시 생성. SIGGRAPH Asia 2022에서. 1-8. Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 원시. ACM Transactions on Graphics (ToG) 41, 4 (2022), 1-15. Zhuang et al. Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. 2021. 신경 관절 광도 필드. ICCV 2021에서. 5762-5772. Julian Ost, Issam Laradji, Alejandro Newell, and et al. 2022. 신경 점 광 필드. CVPR 2022. 18419-18429. Sida Peng, Yuanqing Zhang, Yinghao Xu, et al. 2021. 신경체: 동적 인간의 새로운 관점 합성을 위한 구조화된 잠재 코드를 사용한 암묵적 신경 표현. CVPR 2021. 9054-9063. Ben Poole, Ajay Jain, Jonathan T Barron, Ben Mildenhall. 2022. Dreamfusion: 2D 확산을 사용한 텍스트-3D. arXiv 사전 인쇄본 arXiv:2209.14988(2022). Charles R Qi, Hao Su, Kaichun Mo, Leonidas J Guibas. 2017. Pointnet: 3D 분류 및 분할을 위한 포인트 세트에 대한 딥 러닝. CVPR 2017. 652-660. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron 등 2023. DreamBooth3D: 주제 기반 텍스트를 3D로 생성. arXiv 사전 인쇄 arXiv:2303.(2023). Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu 및 Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄 arXiv:2204.06125 (2022). Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler 등 2021. 3D의 공통 객체: 실제 3D 카테고리 재구성에 대한 대규모 학습 및 평가. ICCV 2021. 10901-10911. Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or. 2023. Texture: 3D 모양의 텍스트 가이드 텍스처링. arXiv 사전 인쇄본 arXiv:2302.01721(2023). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR 2022. 10684-10695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. 2022. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. arXiv 사전 인쇄 arXiv:2208.12242 (2022). Chitwan Saharia, William Chan, Saurabh 외. 색세나. 2022. 깊은 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. NeurIPS 2022(2022), 36479-36494. Etai Sella, Gal Fiebelman, Peter Hedman 및 Hadar Averbuch-Elor. 2023. Vox-E: 3D 개체의 텍스트 안내 복셀 편집. arXiv 사전 인쇄 arXiv:2303.12048 (2023). Tianwei Shen, Zixin Luo, Lei Zhou 등. 2018. 표면 재구성을 통해 학습하여 일치 가능한 이미지 검색. ACCV 2018. Springer, 415-431. Jiaming Song, Chenlin Meng, Stefano Ermon. 2020. Denoising diffusion implicit models. arXiv 사전 인쇄본 arXiv:2010.02502(2020). Robert W Sumner, Johannes Schmid, Mark Pauly. 2007. 모양 조작을 위한 임베디드 변형. ACM siggraph 2007 논문. 80-es. Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. 2017. Attention is all you need. NeurIPS 2017 30(2017). Can Wang, Menglei Chai, Mingming He, et al. 2022a. Clip-nerf: 신경 광도장의 텍스트 및 이미지 기반 조작. CVPR 2022. 3835-3844. Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. 2023. Nerf-art: 텍스트 기반 신경 복사 필드 스타일화. IEEE Transactions on Visualization and Computer Graphics(2023). Chen Wang, Xian Wu, Yuan-Chen Guo, and et al. 2022c. NeRF-SR: 슈퍼샘플링을 사용한 고품질 신경 복사 필드. ACM MM 2022. 6445-6454. Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. 2022b. Score Jacobian Chaining: 3D 생성을 위한 사전 학습된 2D 확산 모델 리프팅. arXiv 사전 인쇄본 arXiv:2212.00774(2022). Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. 2021. Neus: Learning neural implicit surface by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689 (2021). Fanbo Xiang, Zexiang Xu, Milos Hasan, and et al. 2021. Neutex: Neural texture mapping for volumetric neural rendering. CVPR 2021에서. 7119-7128. Tianhan Xu and Tatsuya Harada. 2022. Deforming radiance fields with cages. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXIII. Springer, 159–175. Bangbang Yang, Chong Bao, and Junyi et al. Zeng. 2022. Neumesh: 기하학 및 텍스처 편집을 위한 얽힘이 풀린 신경망 기반 암묵적 필드 학습. ECCV 2022에서. Springer, 597-614. Yao Yao, Zixin Luo, Shiwei Li, and et al. 2020. BlendedMVS: 일반화된 다중 뷰 스테레오 네트워크를 위한 대규모 데이터 세트. CVPR 2020에서. Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. 2023. BakedSDF: 실시간 뷰 합성을 위한 신경망 SDF 메싱. arXiv 사전 인쇄본 arXiv:2302.14859(2023). Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, and et al. 2022. NeRF 편집: 신경 복사장의 기하학적 편집. CVPR 2022에서. 18353-18364.
