--- ABSTRACT ---
모바일 기기에 NMT 모델을 배포하는 것은 프라이버시, 낮은 지연 시간 및 오프라인 시나리오에 필수적입니다. 높은 모델 용량의 경우 NMT 모델은 다소 큽니다. 이러한 모델을 기기에서 실행하는 것은 제한된 저장 공간, 메모리, 계산 및 전력 소비로 인해 어렵습니다. 기존 작업은 FLOPS와 같은 단일 메트릭이나 자동 회귀 디코딩에 적합하지 않은 일반 엔진에만 초점을 맞춥니다. 이 논문에서는 기기에서 15MB 및 30ms로 변환할 수 있는 시스템인 MobileNMT를 제시합니다. 양자화와 결합할 때 모델 압축에 대한 일련의 원칙을 제안합니다. 또한 INT8 및 디코딩에 친화적인 엔진을 구현합니다. 모델과 엔진의 공동 설계를 통해 기존 시스템과 비교하여 47.0배 속도를 높이고 BLEU 손실은 11.6%에 불과하여 메모리를 99.5% 절약합니다. 코드는 https://github.com/zjersey/Lightseq-ARM에서 공개적으로 사용할 수 있습니다. 1
--- INTRODUCTION ---
자연어 처리의 고전적인 하위 분야인 신경망 기계 번역(NMT)은 최근 몇 년 동안 큰 성공을 거두었습니다. 대부분의 연구는 이러한 모델이 실제 시나리오에 쉽게 배포될 수 있는지 여부를 무시한 채 대규모 기계 번역 시스템의 정확도를 개선하는 데 초점을 맞춥니다. 여기서는 NMT 모델이 배포에 친화적인지 평가하기 위해 네 가지 지표를 채택합니다. (1) 모델 크기는 모델 압축에서 가장 중요한 지표입니다(Han et al., 2016). (2) 부동 소수점 연산(FLOP)은 일반적으로 신경 구조 설계에서 계산 복잡도를 평가하는 데 사용됩니다. (3) 메모리 또는 메모리 매핑 I/O(MMI/O)는 실제 실행 시스템의 메모리 요구 사항을 반영합니다. (4) 디코딩 속도는 엔진 구현 및 사용 가능한 프로세서의 성능과 같은 여러 현실적인 요인에 따라 달라집니다. *이 작업은 ByteDance에서 인턴십을 하는 동안 수행됩니다. *연락 저자. Big-TFLiteBase-TFLite20MB-Ours10MB-Ours400 600 800 1, 모델 크기(MB) Big-TFLite Base-TFLite 20MB-Ours 10MB-Ours 14-2,886.908.25.1,2,3, 메모리(MB) 1,281.332.Big-TFLite Base-TFLite 20MB-Ours 46.10MB-Ours 27.Big-TFLite Base-TFLite 20MB-Ours 10MB-Ours1,1, 대기 시간(ms) 28.27.27.25.BLEU [%]그림 1: 이러한 메트릭은 Google Pixel 4에서 측정되었습니다. 각 결과는 src/tgt 길이가 30인 샘플에서 200번 실행한 평균입니다. 이 논문에서는 Transformer 기반 머신인 MobileNMT를 제안합니다. 15MB와 30ms로 번역할 수 있는 번역 시스템입니다. 먼저, 매개변수 제한 MT 모델을 설계하기 위한 세 가지 원칙을 제안합니다. 1) 임베딩을 압축하기 위해 어휘 크기를 줄이는 것은 임베딩 인수분해에 비해 간단하고 효과적입니다. 2) 인코더와 디코더를 압축하기 위해 모델 너비를 줄이는 것은 교차 계층 매개변수 공유보다 계산과 메모리 측면에서 훨씬 더 효율적입니다. 3) 인코더 깊이는 정확도를 보장하는 데 매우 중요합니다. 더 높은 정확도를 달성하기 위해 새로 설계된 구조에 따라 학습 하이퍼파라미터를 조정하고 시퀀스 수준 지식 증류를 채택합니다. 산업 배포의 경우 자체 추론 엔진에서 일반 행렬 곱셈(GEMM)과 메모리를 최적화하고 저장 및 계산에 8비트 정수를 사용합니다. 표 1에서 볼 수 있듯이 10MB MobileNMT는 88.4%47Scaling E- Scaling V Scaling E Scaling V BLEU [%]Scaling E Scaling V1447#Base의 매개변수(M) #Small의 매개변수(M) #Tiny의 매개변수(M)23.공유 너비22.공유 너비 BLEU [%]28.2 35.564.12.2 14.1 15.21.#Base의 매개변수(M) #Small의 매개변수(M)공유 너비5.7 6.1 6.#Tiny의 매개변수(M)그림 2: 섹션 2와 섹션 3의 다양한 방법에 대한 모델 성능(Scaling E: 임베딩 차원 확장; Scaling V: 어휘 크기 확장; 공유: 교차 계층 매개변수 공유; 너비: 모델 너비 감소). 스케일링 V는 스케일링 E보다 성능이 우수합니다. 너비는 공유와 거의 동일한 성능을 보이며 Transformer-big의 성능은 1.1% 크기에 불과하고 디코딩에서 47.0배 더 빠르며 쉽게 배포하고 사용할 수 있습니다. 저희의 기여는 다음과 같이 요약됩니다. • 계산 및 메모리 리소스를 보다 효율적으로 사용하기 위해 매개변수 제한 MT 모델에 대한 세 가지 원칙을 제안합니다. • 새로 설계된 구조에 따라 학습 전략을 조정하여 더 높은 변환 정확도를 달성합니다. • 산업 실무와 이론 연구 간의 격차를 메우기 위해 모바일 추론 엔진을 개발합니다. 2 아키텍처 설계 원칙 모델 압축 및 가속의 경우 대부분 연구는 실제 응용 프로그램을 고려하지 않고 모델 크기나 FLOP와 같은 단일 메트릭에 초점을 맞춥니다. 이 섹션에서는 모델 크기, FLOP, 메모리 사용량, 디코딩 속도를 포함한 네 가지 메트릭을 고려한 다음 매개변수 제한 MT 모델에 대한 세 가지 설계 원칙을 제안합니다. 저희는 기계 번역에서 큰 성공을 거두었기 때문에 Transformer(부록 A)를 기준으로 선택했습니다. 2.1 임베딩 압축 어휘 크기 V는 일반적으로 NMT 모델에서 수만에 이릅니다(Akhbardeh et al., 2021). 매개변수는 수천만에 이를 수 있으며 전체 매개변수 효율성에 큰 영향을 미칩니다.임베딩 인수분해(스케일링 E). 모델 압축의 경우 임베딩 인수분해가 널리 연구되었습니다(Lan et al., 2020; Grave et al., 2017; Baevski and Auli, 2019). 모듈 Dim Vocab Embed Embed Hidden Hidden Encoder Head Base 40,000¬ Small 40,000· Tiny 40,[ &quot;N/A&quot;] × [ &quot;N/A&quot; × [ &quot;N/A&quot; | ×[28] × 6 [ 2404 ] ×[ FFN HiddenDecoder HeadFFN22 ] ×Params 64.5M ×621.5M ××8.0M 표 1: Base, Small 및 Tiny의 세부 설정. 임베딩 차원 E와 은닉 차원 H에서 추가로 학습 가능한 변환 가중치 WT = REXH를 도입합니다. 여기서 E ≤ H입니다. 인수 분해 후 임베딩 매개변수는 O(V × H)에서 O(V × E+E×H)로 감소합니다. 어휘 크기 줄이기(V 스케일링). 임베딩을 압축하는 보다 직접적인 방법은 어휘 크기 V를 줄이는 것입니다. 어휘에서 벗어난 단어의 위험을 줄이기 위해 여기에서는 바이트 쌍 인코딩(BPE)을 채택합니다(Sennrich et al., 2016; Ott et al., 2018; Ding et al., 2019; Liu et al., 2020). 기계 번역에 대한 대부분의 연구에서 채택된 BPE 병합 작업은 30~40K 범위입니다(Ding et al., 2019). Volt는 더 높은 BLEU와 더 작은 BPE 병합 작업으로 성능이 좋은 어휘를 찾을 수 있음을 증명합니다(Xu et al., 2021). Lin et al.(2021)의 작업에서 수행한 실험에서도 더 작은 어휘가 더 나을 수 있음을 보여줍니다. 어휘 크기를 줄이면 더 나은 성능이 나타납니다. 두 가지 임베딩 압축 방법을 비교하기 위해 여기서는 크기가 다른 세 가지 기준 모델을 선택합니다. 모델 설정은 표 1에 나와 있습니다. 표 2에서 볼 수 있듯이 이 두 방법의 매개변수와 FLOP는 거의 동일합니다. 표시된 대로 WeightSharing Width OutputSharing Width0 1 2 3 4 5 6 7 8 9#Layer ID 0 1 2 3 4 5 6 7 8 9#Layer ID BLEU [%] 그림 3: 왼쪽 두 그림은 각 계층의 가중치 및 출력 범위를 보여줍니다.오른쪽 그림은 모델 너비를 줄이는 것에 비해 교차 계층 매개변수 공유에서 사후 학습 양자화(PTQ)의 모델 성능을 보여줍니다.이 그림은 모델 너비를 줄이는 것이 교차 계층 매개변수 공유보다 양자화에 더 친화적임을 보여줍니다.WBase Small Tiny En→De Metric Scaling E Base Small Tiny Scaling V Base Small TinyParams(M) FLOPS(G) MMI/O(M) BLEU 47 121.41 0.38 0.48 1525.46 21.03 14.VS. 47 121.41 0.38 0.47 1426.17 22.49 17. 지표 공유 기본 소형 소형 너비 BLEU [%] 기본 소형 소형 매개변수(M) FLOPS(G) MMI/O(M) BLEU 28 121.95 0.65 0.66 2425.41 22.37 17. 대 28 120.85 0.38 0.30 1525.18 22.62 18. 표 2: 매개변수, FLOP 및 모델 성능(FLOP 및 MMI/O는 src/tgt 길이가 30인 샘플에서 추정됨). 임베딩 압축의 경우 어휘 크기를 줄이는 것(스케일링 V가 더 간단하고 효과적입니다. 인코더/디코더 압축의 경우 모델 너비(폭)를 줄이는 것이 계산 및 메모리에서 더 효율적입니다. 그림 2의 첫 번째 행에서 어휘 크기를 줄이는 것과 비교했을 때 임베딩 인수분해를 사용한 모델은 대부분의 경우 성능이 좋지 않으며, 특히 매개변수가 제한적일 때 그렇습니다. 2.2 인코더/디코더 압축 인코더 및 디코더 압축의 경우 여기서는 교차 계층 매개변수 공유와 모델 너비 감소를 사용한 모델을 비교합니다. 교차 계층 매개변수 공유(공유). 매개변수 공유의 가장 널리 퍼진 용도는 합성곱 신경망(Long et al., 2015)입니다. 최근 몇 년 동안 NLP 및 NLU 작업에서도 조사되었습니다. 그 중에서도 교차 계층 매개변수 공유는 매개변수를 변경하지 않고도 모델 깊이를 따라 더 강력한 비선형성을 제공할 수 있습니다(Dehghani et al., 2019; Takase and Kiyono, 2021; Lan et al., 2020). 모델 줄이기 너비(Width). 모델 깊이가 기계 번역(Devlin et al., 2019; Liu et al., 2020; Wang et al., 2022; Liu et al., 2020)과 같은 자연어 처리 작업에서 중요한 것으로 입증되었으므로 여기서는 OSmall Baseline ▲ Vocab Size Encoder Depth ◆ Decoder Depth Hidden Size ◆ FFN Dim#Params (M)그림 4: 성능(BLEU) 대 매개변수(M). 다른 표시는 다른 차원을 나타냅니다. 큰 빨간색 원 근처의 점은 작은 빨간색 원 근처의 점보다 모델 성능에 더 큰 영향을 미칩니다. 인코더 깊이는 가장 중요한 차원으로 간주될 수 있습니다. 깊이를 변경하지 않고 모델 너비를 줄이십시오. 모델 너비를 줄이는 것이 더 효율적이고 양자화 친화적입니다. 그림 2의 두 번째 행에서 이 두 방법은 거의 동일한 성능을 보입니다. 그러나 표 2는 FLOPS와 MMI/O에 큰 차이가 있음을 보여주며, 이는 모델 폭을 줄이는 것이 계산과 메모리 측면에서 훨씬 더 효율적임을 의미합니다. 이러한 모델을 더 큰 압축을 위해 양자화할 필요가 있으므로 그림 3에서 두 방법의 가중치와 출력 범위를 추가로 비교합니다. 매개변수 공유가 있는 모델은 가중치와 출력 모두에 대한 값 범위가 더 넓은 것을 분명히 알 수 있으며, 이는 양자화에 친화적이지 않습니다. 오른쪽 그림도 이를 확인합니다. 이 두 방법에 사후 학습 양자화(PTQ)(Sung et al., 2015; Banner et al., 2019; Choukroun et al., 2019)를 적용하면 교차 계층 매개변수 공유의 성능이 저하됩니다. 2.3 딥 인코더와 얕은 디코더 그림 4는 서로 다른 차원이 Transformer 성능에 어떤 영향을 미치는지 연구합니다. 각 차원의 영향을 개별적으로 분석하기 위해 여기서는 특정 차원 하나만 변경하고 다른 차원은 그대로 둡니다.모듈 Dim MobileNMT-10MB MobileNMT-20MB Vocab 8,Embed Embed N/A Hidden L8.xN/A xXHiddenEncoder Head FFN Hidden×xDecoder Head FFNx] ×Params ≈10M ≈20M BLEU [%] Base -Ours w/o E w/ PTQ0.0.Dropout 0.1LR:0.01 LR:0.표 3: MobileNMT의 세부 설정.Ya 그림 6: 왼쪽 부분은 기본 모델과 MobileNMT에서 다양한 드롭아웃의 성능을 보여줍니다.오른쪽 부분은 PTQ 전과 후의 성능을 보여줍니다.MobileNMT에서 드롭아웃을 제거하면 성능이 크게 향상될 수 있습니다.더 큰 학습 속도도 모델 성능을 향상시킬 수 있지만 모델이 양자화에 적합하지 않게 됩니다. bo Vi bVi Wo WSoft Max ReLU 스케일 bi bqkv W₁ W qkv LN LN LR: 0.- LR: 0.가중치0 2 4 6 8#계층 ID 출력0 2 4 6 8#계층 ID 그림 7: 각 계층의 가중치 및 출력 범위. 학습률이 높을수록 값 범위가 커집니다. (a) FFN 양자화기 (b) 어텐션 양자화기 그림 5: FFN 및 어텐션 양자화기의 실행 예제. 여기서 빨간색 선은 양자화될 값을 나타내고 검은색 선은 전체 정밀도의 값을 나타냅니다. 변경되지 않음. Small Baseline의 왼쪽에 있는 점은 한 차원을 축소하는 것을 나타내고 오른쪽에 있는 점은 한 차원을 확대하는 것을 나타냅니다. 인코더 깊이가 다른 차원보다 더 중요하다는 것을 알 수 있으며, 이는
--- RELATED WORK ---
대규모 모델에서(Wang et al., 2019, 2022). 위의 논의를 바탕으로, 우리는 어휘 크기와 모델 폭을 줄이면서 최종적으로 딥 인코더와 얕음 디코더를 구축했습니다. 여기서는 서로 다른 크기의 두 MobileNMT 모델을 구축하고 자세한 설정은 표 3에 나와 있습니다. 3 학습 전략 3.1 지식 증류를 통한 사전 학습 압축 모델의 성능을 개선하기 위해 최근 연구에서는 잘 훈련된 전체 정밀도 교사 네트워크에서 학생 네트워크로 지식을 증류하거나(Mishra and Marr, 2018) 양자화된 교사 네트워크를 직접 사용합니다(Kim et al., 2019). 여기서는 NMT 작업에 효과적인 것으로 나타났기 때문에 시퀀스 수준 지식 증류를 채택했습니다. 가장 기본적인 전체 정밀도 Transformerbase 모델을 교사로 채택했습니다. 3.2 양자화 변압기 모델을 양자화하는 프로세스는 두 단계로 나눌 수 있습니다. 1) 양자화기 구성; 2) 섹션 3.1에서 얻은 사전 학습된 모델을 기반으로 양자화 인식 학습(QAT)(Courbariaux et al., 2015)을 적용합니다.FFN 및 어텐션 양자화기. 원래의 Transformer 계층에는 어텐션 하위 계층과 피드포워드 네트워크(FFN)의 두 가지 유형의 하위 계층이 포함됩니다(Vaswani et al., 2017). 여기서 어텐션과 FFN의 각 선형에 대한 양자화기를 구성하고 그림 5에서와 같이 가중치와 활성화를 모두 양자화합니다. 대부분의 계산이 행렬 곱셈에 소요되므로 모든 편향과 잔차는 정확도를 유지하기 위해 전체 정밀도로 유지됩니다. 양자화는 네트워크 출력 범위를 변경하므로 여기서는 i번째 하위 계층에 학습 가능한 가중치 Yi를 추가하여 출력과 이를 둘러싼 잔차를 결합하는 방법을 학습합니다. 양자화 인식 학습. MobileNMT에는 10M/20M 매개변수만 있으므로 이렇게 작은 모델을 양자화하면 필연적으로 성능 손실이 발생하므로 WeightAttn(L2 없음) Attn(L2 있음) FFN(L2 없음) FFN(L2 있음) OutputBLEU [%] 0 1 2 3 4 5 6 7 8 9#Layer ID 0 1 2 3 4 5 6 7 8 9#Layer ID 그림 8: 왼쪽 두 그림은 각 계층의 가중치 및 출력 범위를 보여줍니다.오른쪽 그림은 PTQ 전과 후의 다양한 L2 정규화의 성능을 보여줍니다.실험 결과 L2 정규화가 모델을 양자화에 더 친화적으로 만들 수 있음이 밝혀졌습니다.w/o ■w/ PTQ 0.0.0.Weight Decay 양자화기. QAT 이전에, 우리는 섹션 3.1에서 얻은 사전 훈련된 증류 모델에서 순방향 실행을 기반으로 모든 스케일링 매개변수를 사전 계산합니다.거의 추가 비용이 들지 않지만 좋은 초기화를 제공합니다.엔지니어링 개발의 경우 하드웨어 친화적이기 때문에 균일 양자화 체계를 선택합니다(Liu et al., 2022).8비트 양자화의 경우 요소별 양자화를 사용합니다(Lee et al., 2021).4비트 정수와 같은 하위 비트 양자화의 경우 행별 양자화를 사용합니다(Faraone et al., 2018).3.3 훈련 하이퍼 매개변수 원래 Transformer 모델과 비교하여 섹션 2에서 소개한 Mobile NMT는 매개변수가 적고 아키텍처가 다르기 때문에 다른 훈련 하이퍼 매개변수가 필요합니다.드롭아웃 제거.모델의 매개변수가 적기 때문에 강력한 정규화를 적용할 필요가 없으며 전체 모델에서 드롭아웃을 제거합니다. 그림 6의 왼쪽 부분은 드롭아웃을 제거하면 거의 두 개의 BLEU 포인트가 향상됨을 보여줍니다.더 큰 학습률.여기서 우리는 더 큰 학습률(0.01 → 0.02), 더 큰 학습 배치(4096 → 8192), 더 많은 워밍업 단계(4000 → 8000)를 사용하여 Wang et al.(2019)에서 제공된 구성을 따릅니다.그림 6의 오른쪽 부분에서 볼 수 있듯이 모델 성능을 0.5 BLEU 포인트 이상 향상시킬 수 있습니다(빨간색 막대).그러나 PTQ 후에 학습률이 0.02인 모델은 0.01보다 성능이 현저히 떨어집니다(파란색 막대).그림 7에서 볼 수 있듯이 더 큰 학습률을 사용하면 네트워크 가중치와 출력이 더 커져 양자화에 친화적이지 않습니다.L2 정규화.위의 문제를 해결하기 위해 이 논문에서는 가중치에 적용되는 L2 정규화(가중치 감소라고도 함)를 채택합니다. 네트워크 가중치의 제곱 크기를 원래 손실 함수에 페널티 항으로 추가하고 enkdo * n wo WWWThread(3)128비트 레지스터 1 | do | d1 | d2 | dΤΟWo SIMD 명령어 W1 W2 W그림 9: 단일 SIMD 명령어에서 여러 정수를 처리하는 예. 가중치를 더 작게 만들 것을 권장합니다.그림 8의 왼쪽 두 부분에서 볼 수 있듯이 L2 정규화를 사용하면 네트워크 가중치와 출력 값이 모두 상당히 작아집니다.그림 8의 오른쪽 부분은 다양한 수준의 L2 정규화를 적용할 때 PTQ의 성능을 보여줍니다.빨간색과 파란색 막대는 PTQ 전후의 모델 성능을 나타냅니다.L2 정규화가 PTQ 후에 모델 성능을 개선하는 것을 볼 수 있습니다.4 엔진 이 섹션에서는 추론 엔진의 자세한 구현을 소개합니다. 4.1 GEMM 최적화 ONNX 런타임 플랫폼의 통계에 따르면 일반 행렬 곱셈(GEMM)은 전체 디코딩 시간의 80.44%를 차지합니다. 즉, GEMM을 최적화하는 것이 디코딩 속도를 향상시키는 핵심이라는 것을 보여줍니다. 우리는 세 가지 측면에서 GEMM을 최적화합니다.(1) 32비트 부동 소수점 교체 En-Fr En-De 시스템 Transformer-big 218 ↑1× Transformer-base 65 13× Transformer-small 22 ↑10x Transformer-tiny 8127× MobileNMT-20MB 20 ↑11× MobileNMT-10MB 10 122× 매개변수(M) 크기(MB) 메모리(MB) 872 ↑1x 2886.6 +1.0x 대기 시간(ms) 1281.5 +1.0× 테스트 260 ↑3× 908.5 13.2× 332.3 ↑3.9x 28.36 A-0.27.40 -0.Valid 26.75 A-0.25.81 A-0.88 ↑10× 759.5 13.8x 158.0 18.1x 24.20 -4.23.91 A-2.32 127× 398.9 17.2× 73.0 ↑17.6x 20.97 A-7.21.53 A-5.20144× 26.0 ↑111.2x 46.3 127.7x 27.09 A-1.25.72 A-1.10 +87× 14.9 +194.0x 27.3 147.0x 25.08 A-3.24.85 A-1.트랜스포머-빅 259 ↑1x 1036 ↑1x 2987.6 +1.0x 1345.6 ↑1.0x 39.05 A-0.44.12 A-0.변압기-베이스 8613× 344 13× 944.8 ↑3.2x 358.9 +3.7x 38.64 A-0.43.80 A-0.변압기-소형 22 ↑12× 88 ↑12× 782.3 13.8× 178.5 17.5× 34.76 A-4.40.01 A-4.변압기-소형 8132× 32 132x 418.8 17.1× 80.3 ↑16.8× 30.36 A-8.36.01 A-8.모바일NMT-20MB 20 +13× 모바일NMT-10MB 10 ↑26x 20 ↑52× 10 ↑104× 26.7 ↑111.9x 53.7 125.1x 15.8 ↑189.1× 28.9 +46.6x 37.67 A-1.36.00 A-3.05 41.87 A-2.43.81 A-0.표 4: WMT14 En-De 및 WMT14 En-Fr 작업의 결과. 이러한 지표는 Google Pixel 4에서 측정되었습니다. Transformer-big/base/small/tiny 결과는 TFLite에서 테스트되었고 MobileNMT-20MB/10MB는 엔진에서 테스트되었습니다. 모든 결과는 모델 양자화를 위해 GEMM에서 8비트 정수를 사용하여 src/tgt 길이가 30인 샘플을 기반으로 합니다. (2) 우리가 사용하는 Arm 명령어 세트는 여러 정수를 단일 명령어에서 병렬로 처리할 수 있게 해주므로 프로세서 처리량을 최대한 활용할 수 있습니다. (3) 캐시 적중률과 레지스터 사용을 개선하기 위해 메모리에서 텐서의 레이아웃을 조정하여 명령어가 연속 공간에서 데이터를 읽도록 합니다. 구체적으로 원래 레이아웃의 각 4 × 4 블록을 크기가 16인 연속 벡터로 변환합니다. 그림 9에서 예를 볼 수 있습니다. 4.2 메모리 최적화 부록 C의 그림 10에서 볼 수 있듯이 GEMM을 제외한 다른 작업은 디코딩 시간의 19.56%에 불과하지만 자주 수행되어 많은 양의 임시 메모리가 발생합니다. 메모리 효율성을 개선하기 위해 두 가지 전략을 사용합니다. (1) 빈번한 메모리 맵 I/O와 풋프린트를 피하기 위해 엔진은 두 GEMM 작업 간의 모든 인접한 세분화된 작업을 하나의 융합된 작업으로 통합합니다. (2) 임시 메모리를 절약하기 위해 서로 다른 작업이 동시에 서로 간섭하지 않는 한 동일한 공간을 공유할 수 있습니다. 메모리 공유를 통해 Transformer 인코더에 중간 결과를 보관하기 위해 8비트 메모리 버퍼 2개와 32비트 버퍼 1개만 미리 할당하면 됩니다. 5. 실험 5.1 설정 우리는 다음을 평가합니다.
--- METHOD ---
2절과 3절(스케일링 E: 임베딩 차원 스케일링; 스케일링 V: 어휘 크기 스케일링; 공유: 교차 계층 매개변수 공유; 너비: 모델 너비 줄이기)에서 스케일링 V가 스케일링 E보다 성능이 우수합니다. 너비는 공유와 거의 동일한 성능을 보입니다. Transformer-big의 성능은 1.1%에 불과하고 디코딩에서 47.0배 더 빠르며 쉽게 배포하고 사용할 수 있습니다. 저희의 기여는 다음과 같이 요약할 수 있습니다. • 매개변수 제한 MT 모델에 대한 세 가지 원칙을 제안하여 계산 및 메모리 리소스를 보다 효율적으로 사용합니다. • 새롭게 설계된 구조에 따라 학습 전략을 조정하여 더 높은 변환 정확도를 달성합니다. • 산업 실무와 이론적 연구 간의 격차를 메우는 모바일 추론 엔진을 개발합니다. 2 아키텍처 설계 원칙 모델 압축 및 가속의 경우 대부분의 연구는 실제 적용을 고려하지 않고 모델 크기나 FLOP과 같은 단일 메트릭에 초점을 맞춥니다. 이 섹션에서는 모델 크기, FLOP, 메모리 사용량 및 디코딩 속도를 포함한 네 가지 메트릭을 고려한 다음 매개변수 제한 MT 모델에 대한 세 가지 설계 원칙을 제안합니다.기계 번역에서 큰 성공을 거두었기 때문에 Transformer(부록 A)를 기준으로 선택했습니다.2.1 임베딩 압축 어휘 크기 V는 일반적으로 NMT 모델에서 수만에 이릅니다(Akhbardeh et al., 2021).매개 변수는 수천만에 이를 수 있으며 전체 매개변수 효율성에 큰 영향을 미칩니다.임베딩 인수 분해(스케일링 E).모델 압축의 경우 임베딩 인수 분해가 널리 연구되었습니다(Lan et al., 2020; Grave et al., 2017; Baevski and Auli, 2019).모듈 Dim Vocab Embed Embed Hidden Hidden Encoder Head Base 40,000¬ Small 40,000· Tiny 40,[ &quot;N/A&quot;] × [ &quot;N/A&quot; × [ &quot;N/A&quot; | ×[28] × 6 [ 2404 ] ×[ FFN HiddenDecoder HeadFFN22 ] ×Params 64.5M ×621.5M ××8.0M 표 1: Base, Small 및 Tiny의 세부 설정. 임베딩 차원 E와 은닉 차원 H에서 추가로 학습 가능한 변환 가중치 WT = REXH를 도입합니다. 여기서 E ≤ H입니다. 인수 분해 후 임베딩 매개변수는 O(V × H)에서 O(V × E+E×H)로 감소합니다. 어휘 크기 줄이기(V 스케일링). 임베딩을 압축하는 보다 직접적인 방법은 어휘 크기 V를 줄이는 것입니다. 어휘에서 벗어난 단어의 위험을 줄이기 위해 여기에서는 바이트 쌍 인코딩(BPE)을 채택합니다(Sennrich et al., 2016; Ott et al., 2018; Ding et al., 2019; Liu et al., 2020). 기계 번역에 대한 대부분의 연구에서 채택된 BPE 병합 작업은 30~40K 범위에 있습니다(Ding et al., 2019). Volt는 더 높은 BLEU와 더 작은 BPE 병합 작업으로 성능이 좋은 어휘를 찾을 수 있음을 증명합니다(Xu et al., 2021).
--- EXPERIMENT ---
Lin et al. (2021)의 연구에서도 어휘가 작을수록 더 나을 수 있음을 보여줍니다.어휘 크기를 줄이면 성능이 더 좋습니다.두 가지 임베딩 압축 방법을 비교하기 위해 여기서는 크기가 다른 세 가지 기준 모델을 선택합니다.모델 설정은 표 1에 나와 있습니다.표 2에서 볼 수 있듯이 이 두 방법의 매개변수와 FLOP은 거의 동일합니다.그림 3: 왼쪽 두 그림은 각 계층의 가중치 및 출력 범위를 보여줍니다.오른쪽 그림은 모델 너비를 줄이는 것과 비교하여 교차 계층 매개변수 공유에서 사후 학습 양자화(PTQ)의 모델 성능을 보여줍니다. 이 그림은 모델 너비를 줄이는 것이 교차 계층 매개변수 공유보다 양자화에 더 친화적임을 보여줍니다.WBase Small Tiny En→De Metric Scaling E Base Small Tiny Scaling V Base Small TinyParams(M) FLOPS(G) MMI/O(M) BLEU 47 121.41 0.38 0.48 1525.46 21.03 14.VS. 47 121.41 0.38 0.47 1426.17 22.49 17.Metric Sharing Base Small Tiny Width BLEU [%]Base Small Tiny Params(M) FLOPS(G) MMI/O(M) BLEU 28 121.95 0.65 0.66 2425.41 22.37 17.VS. 28 120.85 0.38 0.30 1525.18 22.62 18.표 2: 매개변수, FLOP 및 모델 성능(FLOP 및 MMI/O는 src/tgt 길이가 30인 샘플에서 추정됨). 임베딩 압축의 경우 어휘 크기를 줄이는 것(스케일링 V가 더 간단하고 효과적입니다. 인코더/디코더 압축의 경우 모델 너비(폭)를 줄이는 것이 계산 및 메모리에서 더 효율적입니다. 그림 2의 첫 번째 행에서 어휘 크기를 줄이는 것과 비교했을 때 임베딩 인수분해를 사용한 모델은 대부분의 경우 성능이 좋지 않으며, 특히 매개변수가 제한적일 때 그렇습니다. 2.2 인코더/디코더 압축 인코더 및 디코더 압축의 경우 여기서는 교차 계층 매개변수 공유와 모델 너비 감소를 사용한 모델을 비교합니다. 교차 계층 매개변수 공유(공유). 매개변수 공유의 가장 널리 퍼진 용도는 합성곱 신경망(Long et al., 2015)입니다. 최근 몇 년 동안 NLP 및 NLU 작업에서도 조사되었습니다. 그 중에서도 교차 계층 매개변수 공유는 매개변수를 변경하지 않고도 모델 깊이를 따라 더 강력한 비선형성을 제공할 수 있습니다(Dehghani et al., 2019; Takase and Kiyono, 2021; Lan et al., 2020). 모델 줄이기 너비(Width). 모델 깊이가 기계 번역(Devlin et al., 2019; Liu et al., 2020; Wang et al., 2022; Liu et al., 2020)과 같은 자연어 처리 작업에서 중요한 것으로 입증되었으므로 여기서는 OSmall Baseline ▲ Vocab Size Encoder Depth ◆ Decoder Depth Hidden Size ◆ FFN Dim#Params (M)그림 4: 성능(BLEU) 대 매개변수(M). 다른 표시는 다른 차원을 나타냅니다. 큰 빨간색 원 근처의 점은 작은 빨간색 원 근처의 점보다 모델 성능에 더 큰 영향을 미칩니다. 인코더 깊이는 가장 중요한 차원으로 간주될 수 있습니다. 깊이를 변경하지 않고 모델 너비를 줄이십시오. 모델 너비를 줄이는 것이 더 효율적이고 양자화 친화적입니다. 그림 2의 두 번째 행에서 이 두 방법은 거의 동일한 성능을 보입니다. 그러나 표 2는 FLOPS와 MMI/O에 큰 차이가 있음을 보여주며, 이는 모델 폭을 줄이는 것이 계산과 메모리 측면에서 훨씬 더 효율적임을 의미합니다. 이러한 모델을 더 큰 압축을 위해 양자화할 필요가 있으므로 그림 3에서 두 방법의 가중치와 출력 범위를 추가로 비교합니다. 매개변수 공유가 있는 모델은 가중치와 출력 모두에 대한 값 범위가 더 넓은 것을 분명히 알 수 있으며, 이는 양자화에 친화적이지 않습니다. 오른쪽 그림도 이를 확인합니다. 이 두 방법에 사후 학습 양자화(PTQ)(Sung et al., 2015; Banner et al., 2019; Choukroun et al., 2019)를 적용하면 교차 계층 매개변수 공유의 성능이 저하됩니다. 2.3 딥 인코더와 얕은 디코더 그림 4는 서로 다른 차원이 Transformer 성능에 어떤 영향을 미치는지 연구합니다. 각 차원의 영향을 개별적으로 분석하기 위해 여기서는 특정 차원 하나만 변경하고 다른 차원은 그대로 둡니다.모듈 Dim MobileNMT-10MB MobileNMT-20MB Vocab 8,Embed Embed N/A Hidden L8.xN/A xXHiddenEncoder Head FFN Hidden×xDecoder Head FFNx] ×Params ≈10M ≈20M BLEU [%] Base -Ours w/o E w/ PTQ0.0.Dropout 0.1LR:0.01 LR:0.표 3: MobileNMT의 세부 설정.Ya 그림 6: 왼쪽 부분은 기본 모델과 MobileNMT에서 다양한 드롭아웃의 성능을 보여줍니다.오른쪽 부분은 PTQ 전과 후의 성능을 보여줍니다.MobileNMT에서 드롭아웃을 제거하면 성능이 크게 향상될 수 있습니다.더 큰 학습 속도도 모델 성능을 향상시킬 수 있지만 모델이 양자화에 적합하지 않게 됩니다. bo Vi bVi Wo WSoft Max ReLU 스케일 bi bqkv W₁ W qkv LN LN LR: 0.- LR: 0.가중치0 2 4 6 8#계층 ID 출력0 2 4 6 8#계층 ID 그림 7: 각 계층의 가중치 및 출력 범위. 학습률이 클수록 값의 범위가 커집니다. (a) FFN 양자화기 (b) 어텐션 양자화기 그림 5: FFN 및 어텐션 양자화기의 실행 예. 여기서 빨간색 선은 양자화될 값을 나타내고 검은색 선은 전체 정밀도의 값을 나타냅니다. 변경되지 않음. 작은 기준선의 왼쪽에 있는 점은 한 차원을 축소하는 것을 나타내고, 오른쪽에 있는 점은 한 차원을 확대하는 것을 나타냅니다. 인코더 깊이가 다른 차원보다 더 중요하다는 것을 알 수 있으며, 이는 대규모 모델에 대한 관련 작업(Wang et al., 2019, 2022)과 일치합니다. 위의 논의를 바탕으로, 우리는 최종적으로 어휘 크기와 모델 폭을 줄이면서 딥 인코더와 셸로우 디코더를 구축한다.여기서는 서로 다른 크기의 두 MobileNMT 모델을 구축하고 자세한 설정은 표 3에 나와 있다.3 학습 전략 3.1 지식 증류를 통한 사전 학습 압축 모델의 성능을 개선하기 위해 최근 연구에서는 잘 훈련된 완전 정밀도 교사 네트워크에서 학생 네트워크로 지식을 증류하거나(Mishra 및 Marr, 2018) 양자화된 교사 네트워크를 직접 사용한다(Kim et al., 2019).여기서는 NMT 작업에 효과적인 것으로 밝혀진 시퀀스 수준 지식 증류를 채택한다.가장 기본적인 완전 정밀도 Transformerbase 모델을 교사로 채택한다.3.2 양자화 변압기 모델을 양자화하는 프로세스는 두 단계로 나눌 수 있다.1) 양자화기 구성; 2) 섹션 3.1에서 얻은 사전 학습된 모델을 기반으로 양자화 인식 학습(QAT)(Courbariaux et al., 2015)을 적용합니다.FFN 및 어텐션 양자화기. 원래의 Transformer 계층에는 어텐션 하위 계층과 피드포워드 네트워크(FFN)의 두 가지 유형의 하위 계층이 포함됩니다(Vaswani et al., 2017). 여기서 어텐션과 FFN의 각 선형에 대한 양자화기를 구성하고 그림 5에서와 같이 가중치와 활성화를 모두 양자화합니다. 대부분의 계산이 행렬 곱셈에 소요되므로 모든 편향과 잔차는 정확도를 유지하기 위해 전체 정밀도로 유지됩니다. 양자화는 네트워크 출력 범위를 변경하므로 여기서는 i번째 하위 계층에 학습 가능한 가중치 Yi를 추가하여 출력과 이를 둘러싼 잔차를 결합하는 방법을 학습합니다. 양자화 인식 학습. MobileNMT에는 10M/20M 매개변수만 있으므로 이렇게 작은 모델을 양자화하면 필연적으로 성능 손실이 발생하므로 WeightAttn(L2 없음) Attn(L2 있음) FFN(L2 없음) FFN(L2 있음) OutputBLEU [%] 0 1 2 3 4 5 6 7 8 9#Layer ID 0 1 2 3 4 5 6 7 8 9#Layer ID 그림 8: 왼쪽 두 그림은 각 계층의 가중치 및 출력 범위를 보여줍니다.오른쪽 그림은 PTQ 전과 후의 다양한 L2 정규화의 성능을 보여줍니다.실험 결과 L2 정규화가 모델을 양자화에 더 친화적으로 만들 수 있음이 밝혀졌습니다.w/o ■w/ PTQ 0.0.0.Weight Decay 양자화기. QAT 이전에, 우리는 섹션 3.1에서 얻은 사전 훈련된 증류 모델에서 순방향 실행을 기반으로 모든 스케일링 매개변수를 사전 계산합니다.거의 추가 비용이 들지 않지만 좋은 초기화를 제공합니다.엔지니어링 개발의 경우 하드웨어 친화적이기 때문에 균일 양자화 체계를 선택합니다(Liu et al., 2022).8비트 양자화의 경우 요소별 양자화를 사용합니다(Lee et al., 2021).4비트 정수와 같은 하위 비트 양자화의 경우 행별 양자화를 사용합니다(Faraone et al., 2018).3.3 훈련 하이퍼 매개변수 원래 Transformer 모델과 비교하여 섹션 2에서 소개한 Mobile NMT는 매개변수가 적고 아키텍처가 다르기 때문에 다른 훈련 하이퍼 매개변수가 필요합니다.드롭아웃 제거.모델의 매개변수가 적기 때문에 강력한 정규화를 적용할 필요가 없으며 전체 모델에서 드롭아웃을 제거합니다. 그림 6의 왼쪽 부분은 드롭아웃을 제거하면 거의 두 개의 BLEU 포인트가 향상됨을 보여줍니다.더 큰 학습률.여기서 우리는 더 큰 학습률(0.01 → 0.02), 더 큰 학습 배치(4096 → 8192), 더 많은 워밍업 단계(4000 → 8000)를 사용하여 Wang et al.(2019)에서 제공된 구성을 따릅니다.그림 6의 오른쪽 부분에서 볼 수 있듯이 모델 성능을 0.5 BLEU 포인트 이상 향상시킬 수 있습니다(빨간색 막대).그러나 PTQ 후에 학습률이 0.02인 모델은 0.01보다 성능이 현저히 떨어집니다(파란색 막대).그림 7에서 볼 수 있듯이 더 큰 학습률을 사용하면 네트워크 가중치와 출력이 더 커져 양자화에 친화적이지 않습니다.L2 정규화.위의 문제를 해결하기 위해 이 논문에서는 가중치에 적용되는 L2 정규화(가중치 감소라고도 함)를 채택합니다. 네트워크 가중치의 제곱 크기를 원래 손실 함수에 페널티 항으로 추가하고 enkdo * n wo WWWThread(3)128비트 레지스터 1 | do | d1 | d2 | dΤΟWo SIMD 명령어 W1 W2 W그림 9: 단일 SIMD 명령어에서 여러 정수를 처리하는 예. 가중치를 더 작게 만들 것을 권장합니다.그림 8의 왼쪽 두 부분에서 볼 수 있듯이 L2 정규화를 사용하면 네트워크 가중치와 출력 값이 모두 상당히 작아집니다.그림 8의 오른쪽 부분은 다양한 수준의 L2 정규화를 적용할 때 PTQ의 성능을 보여줍니다.빨간색과 파란색 막대는 PTQ 전후의 모델 성능을 나타냅니다.L2 정규화가 PTQ 후에 모델 성능을 개선하는 것을 볼 수 있습니다.4 엔진 이 섹션에서는 추론 엔진의 자세한 구현을 소개합니다. 4.1 GEMM 최적화 ONNX 런타임 플랫폼의 통계에 따르면 일반 행렬 곱셈(GEMM)은 전체 디코딩 시간의 80.44%를 차지합니다. 즉, GEMM을 최적화하는 것이 디코딩 속도를 향상시키는 핵심이라는 것을 보여줍니다. 우리는 세 가지 측면에서 GEMM을 최적화합니다.(1) 32비트 부동 소수점 교체 En-Fr En-De 시스템 Transformer-big 218 ↑1× Transformer-base 65 13× Transformer-small 22 ↑10x Transformer-tiny 8127× MobileNMT-20MB 20 ↑11× MobileNMT-10MB 10 122× 매개변수(M) 크기(MB) 메모리(MB) 872 ↑1x 2886.6 +1.0x 대기 시간(ms) 1281.5 +1.0× 테스트 260 ↑3× 908.5 13.2× 332.3 ↑3.9x 28.36 A-0.27.40 -0.Valid 26.75 A-0.25.81 A-0.88 ↑10× 759.5 13.8x 158.0 18.1x 24.20 -4.23.91 A-2.32 127× 398.9 17.2× 73.0 ↑17.6x 20.97 A-7.21.53 A-5.20144× 26.0 ↑111.2x 46.3 127.7x 27.09 A-1.25.72 A-1.10 +87× 14.9 +194.0x 27.3 147.0x 25.08 A-3.24.85 A-1.트랜스포머-빅 259 ↑1x 1036 ↑1x 2987.6 +1.0x 1345.6 ↑1.0x 39.05 A-0.44.12 A-0.변압기-베이스 8613× 344 13× 944.8 ↑3.2x 358.9 +3.7x 38.64 A-0.43.80 A-0.변압기-소형 22 ↑12× 88 ↑12× 782.3 13.8× 178.5 17.5× 34.76 A-4.40.01 A-4.변압기-소형 8132× 32 132x 418.8 17.1× 80.3 ↑16.8× 30.36 A-8.36.01 A-8.모바일NMT-20MB 20 +13× 모바일NMT-10MB 10 ↑26x 20 ↑52× 10 ↑104× 26.7 ↑111.9x 53.7 125.1x 15.8 ↑189.1× 28.9 +46.6x 37.67 A-1.36.00 A-3.05 41.87 A-2.43.81 A-0.표 4: WMT14 En-De 및 WMT14 En-Fr 작업의 결과. 이러한 지표는 Google Pixel 4에서 측정되었습니다. Transformer-big/base/small/tiny 결과는 TFLite에서 테스트되었고 MobileNMT-20MB/10MB는 엔진에서 테스트되었습니다. 모든 결과는 모델 양자화를 위해 GEMM에서 8비트 정수를 사용하여 src/tgt 길이가 30인 샘플을 기반으로 합니다. (2) 우리가 사용하는 Arm 명령어 세트는 여러 정수를 단일 명령어에서 병렬로 처리할 수 있게 해주므로 프로세서 처리량을 최대한 활용할 수 있습니다. (3) 캐시 적중률과 레지스터 사용을 개선하기 위해 메모리에서 텐서의 레이아웃을 조정하여 명령어가 연속 공간에서 데이터를 읽도록 합니다. 구체적으로 원래 레이아웃의 각 4 × 4 블록을 크기가 16인 연속 벡터로 변환합니다. 그림 9에서 예를 볼 수 있습니다. 4.2 메모리 최적화 부록 C의 그림 10에서 볼 수 있듯이 GEMM을 제외한 다른 작업은 디코딩 시간의 19.56%에 불과하지만 자주 수행되어 많은 양의 임시 메모리가 발생합니다. 메모리 효율성을 개선하기 위해 두 가지 전략을 사용합니다. (1) 빈번한 메모리 맵 I/O와 풋프린트를 피하기 위해 엔진은 두 GEMM 작업 간의 모든 인접한 세분화된 작업을 하나의 융합된 작업으로 통합합니다. (2) 임시 메모리를 절약하기 위해 서로 다른 작업이 동시에 서로 간섭하지 않는 한 동일한 공간을 공유할 수 있습니다. 메모리 공유를 통해 Transformer 인코더에 중간 결과를 보관하기 위해 8비트 메모리 버퍼 2개와 32비트 버퍼 1개만 미리 할당하면 됩니다. 5. 실험 5.1 설정 두 가지 WMT 벤치마크에서 방법을 평가합니다. WMT14 En-De 작업(4.5M 쌍)의 경우 검증 세트로 newstest-2013을 선택하고 Transformer-base DeLighT System Params(M) w/ w/o FLOPS (G) BLEU 651.27.37 31.27.Universal Transformer Lite Transformer(소형) Lite Transformer(중형) 없음 11.Lite Transformer(대형) 없음 17.EdgeFormer w/o LA 없음 8.EdgeFormer(Adapter-LA) 없음 9.EdgeFormer(Prefix-LA) MobileNMT-10MB MobileNMT-20MB 없음 7.1.26.N/A 2.0.22.0.25.1.26.1.26.1.26.N/A 8.1.26.10 7.0.25.20 17.0.6 27.Table 5: MobileNMT와 DeLighT(Mehta 등, 2021), Universal Transformer(Dehghani 등, 2019), Lite Transformer(Wu 등, 2020) 및 EdgeFormer(Ge 등, 2022)를 포함한 다른 매개변수 효율적 Transformers의 비교(임베딩 레이어가 있는 매개변수 또는 없는 매개변수가 모두 제공됨. FLOPs는 src/tgt 길이가 30인 샘플에서 추정됨). newstest-2014를 테스트 세트로 사용. WMT14 EnFr 작업(35M 쌍)의 경우 newstest-2012와 newstest2013의 조합으로 시스템을 검증하고 newstest-2014에서 테스트합니다. 아키텍처의 세부 사항은 섹션 2에서 소개되었고, 훈련 하이퍼파라미터는 섹션 3에서 소개되었습니다. 모델 압축률과 디코딩 속도 향상을 위해 벤치마크로 Transformer-big(1.0×)를 선택했습니다. 실험 설정의 다른 세부 사항은 부록 D에 소개되어 있습니다.5.2 결과 표 4는 WMT14 En-De 및 En-Fr에서 다양한 시스템의 결과를 보여줍니다.표는 MobileNMT와 Transformer에 기반한 다른 매개변수 효율적 방법을 비교한 내용을 보여줍니다.MobileNMT-10MB와 MobileNMT-20MB는 서로 다른 크기로 구축한 두 가지 모델로, 표 3에 소개되어 있습니다.WMT14 En-De에서 MobileNMT-10MB는 Transformer-big의 88.4% 성능을 유지하는 데 매개변수의 4.6%만 필요하지만 압축률은 87.2배, 속도는 47.0배 향상됩니다.MobileNMT-20MB는 9.2%의 매개변수만으로 Transformer-big의 95.5% 성능을 유지할 수 있지만 압축률은 43.6배, 속도는 27.7배 향상됩니다.En-Fr에서의 실험도 비슷한 결과를 보여줍니다. 또한, 우리 엔진에서 채택한 메모리 최적화 전략 덕분에 MobileNMT는 다른 모델보다 훨씬 적은 실행 메모리를 필요로 합니다(Transformer-big의 0.5%~0.9%). 이 모든 실험은 MobileNMT가 매개변수, 계산 및 메모리 측면에서 효율적이며 모바일 기기에 쉽게 배포할 수 있음을 보여줍니다. 6
--- CONCLUSION ---
우리는 15MB와 30ms로 번역할 수 있는 Transformer 기반 기계 번역 시스템인 MobileNMT를 제안합니다. 이는 기존 리소스를 효율적으로 사용하고 실제 시나리오에 쉽게 배포할 수 있습니다. 우리는 GEMM과 메모리 최적화를 갖춘 모바일 추론 엔진을 개발하여 효율적인 기계 번역에 대한 이론적 연구와 실제 응용 프로그램 간의 격차를 메울 수 있기를 바랍니다. 감사의 말 본 연구는 중국 국가과학기금(번호 62276056), 중국 국가중점연구개발계획, 중국 HTRD 센터 프로젝트(번호 2020AAA0107904), 중국 랴오닝성 자연과학기금(2022-KF-16-01), 윈난성 중점과학기술특별계획 프로젝트(번호 202103AA080015), 중앙대학 기초연구자금(번호 N2216016, N2216001, N2216002), 대학 학문 인재 도입 계획 111(번호 B16009)의 지원을 받아 수행되었습니다. 한계점 다국어 번역. 여기서는 주로 이중 언어 기계 번역을 위한 효율적인 아키텍처의 설계 원칙에 대해 논의합니다. 영어: 이중 언어 번역과 비교할 때 다중 언어 번역 작업은 성능이 우수하려면 훨씬 더 많은 매개변수와 계산이 필요하며, 모델 규모가 다르면 설계 고려 사항이 달라질 수 있습니다. 이는 향후 탐색을 위해 남겨두겠습니다. 지식 증류. 10MB/20MB의 저장 공간만 필요한 작은 모델인 MobileNMT는 다른 Transformer 기반 모델에 비해 필연적으로 성능 저하를 겪을 것입니다. 성능 저하를 줄이기 위해 여기서는 지식 증류를 채택하고 Transformer 기반 모델을 교사로 선택합니다. 교육 효율성 관점에서 교사 모델은 MobileNMT가 성능을 개선하는 데 도움이 될 수 있지만, 추가적인 교육 비용도 발생합니다. 호환성. 여기서 추론 엔진은 ARM CPU에 대한 구현만 제공합니다. 향후 모바일 기기에서 다른 AI 가속기(예: NPU)에도 사용할 수 있도록 할 것입니다. 참고 문헌 Farhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ondrej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussà, Cristina España-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth 히필드, 크리스토퍼 호만, 마티아스 허크, 콰베나 암폰사-카카이레, 카사이 준고, 다니엘 카샤비, 케빈 나이트, 톰 코미, 필립 쿤, 니콜라스 루리, 크리스토프 몬즈, 모리시타 마코토, 나가타 마사아키, 아제시, 나카자와 도시아키, 마테오 네그리, 산타누 팔, 알라세라 오귀스트 타포, 마르코 투르치, 발렌틴 비드린, 그리고 Marcos Zampieri. 2021. 2021년 기계 번역 컨퍼런스(WMT21) 결과. 제6회 기계 번역 컨퍼런스 회의록, WMT@EMNLP 2021, 온라인 이벤트, 2021년 11월 10-11일, 1-88페이지. Association for Computational Linguistics. Lei Jimmy Ba, Jamie Ryan Kiros, Geoffrey E. Hinton. 2016. 계층 정규화. CORR, abs/1607.06450. Alexei Baevski와 Michael Auli. 2019. 신경 언어 모델링을 위한 적응형 입력 표현. 제7회 국제 학습 표현 컨퍼런스, ICLR 2019, 미국 루이지애나주 뉴올리언스, 2019년 5월 6-9일. OpenReview.net. Ron Banner, Yury Nahshan, Daniel Soudry. 2019. 빠른 배포를 위한 합성 신경망의 포스트 트레이닝 4비트 양자화. 신경 정보 처리 시스템의 발전 32: 신경 정보 처리 시스템 연례 컨퍼런스 2019, NeurIPS 2019, 2019년 12월 8일-14일, 캐나다 브리티시 컬럼비아 주 밴쿠버, 7948-7956쪽. Yoni Choukroun, Eli Kravchik, Fan Yang, Pavel Kisilev. 2019. 효율적인 추론을 위한 신경망의 저비트 양자화. 2019 IEEE/CVF 컴퓨터 비전 워크숍 국제 컨퍼런스, ICCV 워크숍 2019, 한국 서울, 2019년 10월 27일-28일, 3009-3018쪽. IEEE. Matthieu Courbariaux, Yoshua Bengio, Jean-Pierre David. 2015. Binaryconnect: 전파 중 이진 가중치를 사용한 딥 신경망 학습. 신경 정보 처리 시스템 28의 발전: 신경 정보 처리 시스템 2015 연례 컨퍼런스, 2015년 12월 7일-12일, 캐나다 퀘벡주 몬트리올, 3123-3131쪽. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser. 2019. 범용 변환기. 2019년 5월 6일-9일 미국 루이지애나주 뉴올리언스에서 열린 제7회 국제 학습 표현 컨퍼런스, ICLR 2019. OpenReview.net. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, NAACL-HLT 2019, 미네소타주 미니애폴리스, 미국, 2019년 6월 2-7일, 1권(긴 논문과 짧은 논문), 4171-4186쪽. 컴퓨터 언어학 협회. Shuoyang Ding, Adithya Renduchintala, Kevin Duh. 2019. 신경망 기계 번역에서 서브워드 병합 작업의 신중한 선택에 대한 요청. 기계 번역 서밋 XVII 1권: 연구 트랙, MTSummit 2019, 아일랜드 더블린, 2019년 8월 19-23일, 204-213쪽. 유럽 기계 번역 협회. Julian Faraone, Nicholas J. Fraser, Michaela Blott, Philip HW Leong. 2018. SYQ: 효율적인 딥 신경망을 위한 대칭 양자화 학습. 2018 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2018, 미국 유타주 솔트레이크시티, 2018년 6월 18-22일, 4300-4309쪽. 컴퓨터 비전 재단/IEEE 컴퓨터 학회. 타오 게, 시칭 첸, 후루 웨이. 2022. Edgeformer: 온디바이스 seq2seq 생성을 위한 매개변수 효율적인 변환기. 2022 자연어 처리 경험적 방법 컨퍼런스 회의록, EMNLP 2022, 아랍에미리트 아부다비, 2022년 12월 7-11일, 10786-10798쪽. 계산 언어학 협회. Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, Hervé Jégou. 2017. GPU를 위한 효율적인 소프트맥스 근사. 34회 기계 학습 국제 컨퍼런스 회의록, ICML 2017, 시드니, NSW, 호주, 2017년 8월 6일, 기계 학습 연구 회의록 70권, 1302-1310쪽. PMLR. Song Han, Huizi Mao, William J. Dally. 2016. 심층 압축: 가지치기, 훈련된 양자화 및 허프만 코딩을 사용한 심층 신경망 압축. 4회 학습 표현 국제 컨퍼런스, ICLR 2016, 산후안, 푸에르토리코, 2016년 5월 2-4일, 컨퍼런스 트랙 회의록. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. 2016a. 이미지 인식을 위한 심층적 잔여 학습. 2016 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2016, 미국 네바다주 라스베이거스, 2016년 6월 27-30일, 770-778쪽. IEEE 컴퓨터 학회. Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio. 2016. 이진화된 신경망. 신경 정보 처리 시스템의 발전 29: 신경 정보 처리 시스템 연례 컨퍼런스 2016, 2016년 12월 5-10일, 스페인 바르셀로나, 4107-4115쪽. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew G. Howard, Hartwig Adam, Dmitry Kalenichenko. 2018. 효율적인 정수-산술-전용 추론을 위한 신경망의 양자화 및 훈련. 2018 IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2018, 미국 유타주 솔트레이크시티, 2018년 6월 18-22일, 2704-2713쪽. 김장호, 야시 발갓, 이진원, 치라그 파텔, 곽노준. 2019. QKD: 양자화 인식 지식 증류. CoRR, abs/1911.12491. 란 전중, 첸 밍다, 세바스찬 굿맨, 케빈 짐펠, 피유시 샤르마, 라두 소리컷. 2020. ALBERT: 언어 표현의 자기 지도 학습을 위한 가벼운 BERT. 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Junghyup Lee, Dohyung Kim, and Bumsub Ham. 2021. Network quantization with element-wise gradient scaling. IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 6448-6457. Computer Vision Foundation/IEEE. Ye Lin, Yanyang Li, Tong Xiao, and Jingbo Zhu. 2021. Bag of tricks for optimizing transformer efficiency. Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event/Punta Cana, Dominican Republic, 16-20 November, 2021, pages 4227-4233. Association for Computational Linguistics. Xiaodong Liu, Kevin Duh, Liyuan Liu, Jianfeng Gao. 2020. 신경 기계 번역을 위한 매우 깊은 변환기. CoRR, abs/2008.07772. Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P. Xing, Zhiqiang Shen. 2022. 비균일-균일 양자화: 일반화된 직접 추정을 통한 정확한 양자화를 향해. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2022, 미국 루이지애나주 뉴올리언스, 2022년 6월 18-24일, 4932-4942페이지. IEEE. Jonathan Long, Evan Shelhamer, Trevor Darrell. 2015. 의미 분할을 위한 완전 합성 네트워크. IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, 2015년 6월 7-12일, 3431-3440페이지. IEEE Computer Society. Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi. 2021. Delight: Deep and light-weight transformer. 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, 2021년 5월 3-7일. OpenReview.net. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, Hao Wu. 2018. Mixed precision training. 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, 2018년 4월 30일 - 5월 3일, Conference Track Proceedings. Asit K. Mishra와 Debbie Marr. 2018. Apprentice: Using knowledge distillation techniques to improve low-precision network accuracy. 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, 2018년 4월 30일 - 5월 3일, Conference Track Proceedings. OpenReview.net. Toan Q. Nguyen과 Julian Salazar. 2019. Transformers without tears: Improving the normalization of self-attention. CORR, abs/1910.05895. Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. 2018. Scaling neural machine translation. 기계 번역에 대한 제3차 회의록: 연구 논문, WMT 2018, 벨기에, 브뤼셀, 2018년 10월 31일 - 11월 1일, 1-9페이지. 전산 언어학 협회. Matt Post. 2018. BLEU 점수 보고의 명확성 요구. 기계 번역에 대한 제3차 회의록: 연구 논문, WMT 2018, 벨기에, 브뤼셀, 2018년 10월 31일 - 11월 1일, 186-191페이지. 전산 언어학 협회. Jerry Quinn과 Miguel Ballesteros. 2018. 8개 조각: 8비트 신경망 기계 번역. 2018년 북미 학회 컴퓨터 언어학 회의록: 인간 언어 기술, NAACLHLT 2018, 미국 루이지애나주 뉴올리언스, 2018년 6월 1-6일, 3권(산업 논문), 114-120쪽. Rico Sennrich, Barry Haddow, Alexandra Birch. 2016. 하위 단어 단위가 있는 희귀 단어의 신경망 기계 번역. 54회 학회 컴퓨터 언어학 연례 회의록, ACL 2016, 2016년 8월 7-12일, 독일 베를린, 1권: 장문 논문. 컴퓨터 언어학 협회. Wonyong Sung, Sungho Shin, Kyuyeon Hwang. 2015. 양자화 하에서 딥 신경망의 복원력. CoRR, abs/1511.06488. Sho Takase와 Shun Kiyono. 2021. 변압기에서 계층 간 매개변수 공유에 대한 강의. CORR, abs/2104.06022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 2017. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전 30: 신경 정보 처리 시스템 연례 컨퍼런스 2017, 2017년 12월 4-9일, 미국 캘리포니아주 롱비치, 5998-6008쪽. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei. 2022. Deepnet: 변압기를 1,000개 계층으로 확장. CORR, abs/2203.00555. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. 기계 번역을 위한 딥 트랜스포머 모델 학습. 2019년 7월 28일-8월 2일 이탈리아 피렌체에서 열린 제57회 전산언어학 협회 회의록, ACL 2019, Volume 1: Long Papers, pages 1810-1822. 전산언어학 협회. Ephrem Wu. 2020. 정확한 정수 트랜스포머 기계 번역 모델 학습. CORR, abs/2001.00926. Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. 2020. 장거리-단거리 주의를 가진 라이트 트랜스포머. 8th International Conference on Learning Representations, ICLR 2020, 에티오피아 아디스아바바, 2020년 4월 26-30일. OpenReview.net. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 2020년 7월 13-18일, Virtual Event, Proceedings of Machine Learning Research의 119권, 10524-10533쪽. PMLR. Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng, Lei Li. 2021. Vocabulary learning via optimal transport for neural machine translation. 59회 연례 총회 및 자연어 처리에 관한 11회 국제 공동 학술 대회의 회의록, ACL/IJCNLP 2021, (제1권: 장문 논문), 가상 이벤트, 2021년 8월 1-6일, 7361-7373페이지. 전산 언어학 협회. 변압기 아키텍처 우리는 기계 번역을 위한 가장 성공적인 신경 모델 중 하나이기 때문에 Transformer를 연구 대상으로 선택했습니다. N-계층 인코더와 M-계층 디코더로 구성되어 있으며, 원래 Transformer-base 및 Transformer-big에서 N=M=6입니다. 각 인코더 계층은 셀프 어텐션 및 피드 포워드 네트워크(FFN)를 포함한 두 개의 하위 계층으로 구성되어 있습니다. 각 디코더 계층에는 인코더와 디코더를 연결하는 추가 교차 어텐션 하위 계층이 있습니다. 셀프 어텐션은 이전 하위 계층의 출력 X를 입력으로 사용합니다. 교차 어텐션은 셀프 어텐션과 비슷하지만, 인코더 출력을 추가 입력으로 사용합니다. 두 유형의 어텐션 모두 먼저 어텐션 분포 Ax를 계산한 다음 X를 Ax로 평균화합니다. Q, K, V의 변환 행렬을 Wq, Wk, Wv로, 그 이후의 변환 행렬을 Wo로, 어텐션을 Ya = Attn(X)로 표시하면 다음과 같습니다. Ax Soft Max( q XW WTXT √d Ya=AxXWW. (1) (2) FFN은 입력 X에 비선형 변환을 적용합니다. FFN을 Yƒ = FFN(X)로 표시합니다. (3) Yƒ = ReLU(XW1 + b1)W2 +b여기서 W₁과 b₁은 첫 번째 선형 변환의 가중치와 바이어스를 나타내고 W2와 b₂는 두 번째 변환의 매개변수입니다. 여기서 각 하위 계층 입력을 계층 정규화(Ba et al., 2016)로 전처리합니다. 모든 하위 계층은 잔여 연결(He et al., 2016a)로 결합됩니다. B PTQ 및 QAT 압축을 모델화하는 매력적인 솔루션으로, 양자화를 통해 모델이 하위 비트 값(예: 8비트 정수)을 사용할 수 있습니다. 더 빠르게 계산하고 저장 공간을 덜 소모합니다(Hubara et al., 2016; Micikevicius et al., 2018; Quinn and Ballesteros, 2018; Jacob et al., 2018). 훈련 후 양자화(PTQ)는 양자화 인식 훈련(QAT)의 기반으로 볼 수 있으며, 잘 훈련된 부동 소수점 모델에 양자화 노드를 추가합니다. 부동 소수점 텐서 r을 n비트 텐서로 양자화하기 위해 이 두 유형의 값을 매핑하는 스케일 s가 도입되었습니다(Wu, 2020): max(r) — min(r) S = 2n-(4) 시스템 변압기 기반 매개변수(M) 크기(MB) BLEU27.+ 어휘 감소 + 너비 감소 + 기타 차원 + 증류26.22.22.23.23.25.25.+ 양자화 + 하이퍼 매개변수 + 탐욕적 탐색 표 6: MobileNMT-10MB에서의 절제 연구. 색상은 섹션 2의 모델 아키텍처, 섹션 3의 학습 전략 및 탐욕적 탐색을 나타냅니다. 더 빠른 계산 속도를 얻기 위해 가중치와 활성화 모두 n비트로 양자화됩니다. rm = = min(r)이라고 가정하면 양자화 함수는 다음과 같습니다. Q(r) = [(r = rm)/s] × s + rm 여기서 는 가장 가까운 정수로 반올림하는 것을 나타냅니다. 그러나 PTQ에서 부동 소수점 네트워크에 직접 양자화를 적용하면 상당한 성능 손실이 발생합니다. PTQ를 기반으로 QAT는 학습 중에 양자화 오류를 최소화하여 n비트 계산의 동작을 시뮬레이션하여 모델이 더 높은 정확도를 달성하는 데 도움이 됩니다. 모델 자체의 학습 가능한 가중치 외에도 s도 학습 가능합니다. C GEMM을 제외한 연산 일반 행렬 곱셈(GEMM)은 전체 디코딩 시간의 80.44%를 차지하므로 섹션 4에서 GEMM을 최적화하는 것이 디코딩 속도를 높이는 핵심이라는 결론을 내렸습니다. GEMM을 제외한 연산의 경우 그림 10은 디코딩 프로세스에서 실행 시간의 비율을 보여줍니다. 해당 데이터는 ONNX 런타임에서 32비트 부동 소수점 형식으로 측정됩니다. D 설정 모든 문장은 하위 단어 단위의 시퀀스로 분할되었습니다(Sennrich et al., 2016). 구현에서 레이어 전 정규화를 채택했습니다(Baevski and Auli, 2019; Xiong et al., 2020; Nguyen and Salazar, 2019). 대부분의 이전 작업은 En-De 작업에서 공유 소스 및 대상 어휘만 사용했습니다. MobileNMT에서 En-De와 En-Fr은 모두 효율성을 높이기 위해 공유 어휘를 채택하여 성능을 희생하고 더 큰 압축 이득을 얻었습니다. 마지막 5개 체크포인트의 평균을 내어 모델 앙상블에서 테스트하고 SacreBLEU 점수를 보고합니다(Post, 2018). 아들들, ENCODER_SELF_ATTENTION DECODER SELF ATTENTION DECODER ATTENTION ARGMAX LAYER NORM BINARY ADD CONVERTER GATHER BINARY UNKNOWN REDUCE BINARY MUL MULTI_HEAD_TRANS CONSTANT_OF_SHAPE 5.364.88-2.99-4.634.132.01-10 -5.893.761.96-1.883.572-1.91-CONCAT1.52UNSQUEEZE 1.13-SPLIT 9.49SHAPE 7.1-HOST CONVERTER 6.28-10-MULTI_HEAD 5.52WHERE2.45EXPAND 1.22CUMSUM-4.79SLICE IDENTITY 2.552.09Percentage [%]-10-그림 10: Transformer-base 모델에서 다양한 연산(GEMM 제외)의 비율.표 6은 섹션 2와 섹션 3의 각 부분이 전체 성능에 어떤 영향을 미치는지 요약한 것입니다.표 6의 각 행은 이전 시스템에서 얻은 시스템에 현재 부분을 적용한 결과를 나타냅니다.Transformer-base 매개변수 비트 크기(M) (WEA) (MB) 65 32-32-32BLEU E 분석32-32-3227.25.E.절제 연구8-8-MobileNMT-10MB4-8-3-8-2-8-32-32-3225.25.3.75 24.2.5 21.27.8-8-27.MobileNMT-20MB4-8-26.3-8-7.26.2-8-24.표 7: 가중치를 하위 비트로 양자화한 결과. 표 4의 MobileNMT 실험의 경우, 엔진에서 탐욕적 탐색 알고리즘을 사용합니다. 빔 탐색과 비교할 때 탐욕적 탐색은 더 효율적인 디코딩으로 이어질 수 있습니다. 표 4의 TFLite 실험의 경우, TFLite는 모든 루프 서브그래프를 확장하기 때문에 제한된 메모리(Google Pixel 4의 경우 6GB)로 Transformerbig/base 모델의 전체 디코딩 프로세스(30단계)를 지원하기 어렵습니다. 이 두 모델의 메모리의 경우, step의 실행 메모리만 기록합니다. 해당 대기 시간의 경우, 1단계 및 5단계 대기 시간에 따라 30단계 대기 시간을 추정합니다. Transformer-big/base의 메모리와 대기 시간을 제외한 다른 모든 데이터 통계는 실제 행에서 측정된다는 점에 주목할 가치가 있습니다. 모델 매개변수를 65M에서 10M으로 줄이기 위해 모델 성능이 27.에서 22.54로 감소했는데, 이는 모델 용량에 대한 네트워크 매개변수의 중요성을 보여줍니다. 지식 증류와 튜닝 하이퍼파라미터 모두 상당한 성능 개선(22.54에서 25.48로)을 가져올 수 있으며, 이는 매개변수 감소로 인한 성능 손실을 효과적으로 보상합니다.E.2 양자화 연구 표 7은 모델을 더 낮은 비트(예: 4비트, 3비트 및 2비트)로 양자화할 때 성능이 어떻게 변하는지 연구합니다.섹션 3.2에서 소개한 대로 8비트 양자화의 경우 요소별 양자화 방법(Lee et al., 2021)을 사용합니다.더 낮은 비트 양자화의 경우 정확도 유지를 위해 행별 양자화를 사용합니다(Faraone et al., 2018).표 7에서 볼 수 있듯이 8비트 및 4비트 양자화는 모델 성능에 거의 부정적인 영향을 미치지 않습니다.3비트 및 2비트 정수와 같이 모델을 더 낮은 비트로 양자화하면 모델 성능이 급격히 떨어집니다.
