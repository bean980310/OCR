--- ABSTRACT ---
대규모 언어 모델(LLM)은 사회를 변화시키고 다양한 응용 분야로 확산되고 있습니다. 그 결과, LLM은 우리와 다른 에이전트와 자주 상호 작용합니다. 따라서 LLM이 상호 작용하는 사회적 환경에서 어떻게 행동하는지 이해하는 것은 사회적으로 큰 가치가 있습니다. 여기서 우리는 행동 게임 이론을 사용하여 LLM의 협력 및 조정 행동을 연구하고자 합니다. 이를 위해 우리는 서로 다른 LLM(GPT-3, GPT-3.5 및 GPT-4)이 서로와 다른 인간과 유사한 전략을 사용하여 유한 반복 게임을 하도록 합니다. 우리의 결과는 LLM이 일반적으로 이러한 작업에서 좋은 성과를 거두고 지속적인 행동적 특징을 발견한다는 것을 보여줍니다. 2인 2전략 게임의 대규모 세트에서 LLM은 반복된 죄수의 딜레마 패밀리와 같이 자신의 이익을 중시하는 것이 보상되는 게임에서 특히 뛰어나다는 것을 발견했습니다. 그러나 조정이 필요한 게임에서는 최적이 아닌 방식으로 행동합니다. 따라서 우리는 이러한 별개의 패밀리에서 두 게임에 더 집중합니다. 정식 반복된 죄수의 딜레마에서 우리는 GPT-4가 특히 용서 없이 행동하며, 다른 에이전트가 단 한 번만 이탈한 후에 항상 이탈한다는 것을 발견했습니다. 성의 전투에서 우리는 GPT-4가 옵션을 번갈아 가며 사용하는 간단한 관례의 행동과 일치할 수 없다는 것을 발견했습니다. 우리는 이러한 행동적 특징이 견고성 검사에서 안정적임을 확인합니다. 마지막으로, 우리는 GPT-4의 행동이 다른 플레이어에 대한 추가 정보를 제공하고 선택을 하기 전에 다른 플레이어의 행동을 예측하도록 요청함으로써 어떻게 수정될 수 있는지 보여줍니다. 이러한 결과는 LLM의 사회적 행동에 대한 우리의 이해를 풍부하게 하고 기계를 위한 행동 게임 이론의 길을 열어줍니다.
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 방대한 텍스트 코퍼스에서 학습된 수십억 개의 매개변수를 갖춘 딥 러닝 모델입니다[Brants et al., 2007, Devlin et al., 2018, Radford et al., 2018]. 이들은 인간 평가자가 다른 인간이 쓴 텍스트와 구별하기 어려운 텍스트를 생성할 수 있지만[Brown et al., 2020], 다른 새로운 능력도 보여주었습니다[Wei et al., 2022a]. 예를 들어, 이들은 유추적 추론 과제를 풀거나[Webb et al., 2022], 웹 애플리케이션을 프로그래밍하거나[Chen et al., 2021], 도구를 사용하여 여러 과제를 풀 수 있습니다[Bubeck et al., 2023]. 이러한 능력과 증가하는 인기로 인해 LLM은 많은 응용 프로그램으로 스며들면서 우리의 일상 생활을 변화시킬 기점에 있습니다[Bommasani et al., 2021]. 이는 LLM이 우리 및 다른 에이전트(LLM 또는 그 외)와 빈번하고 반복적으로 상호 작용한다는 것을 의미합니다. LLM은 이러한 반복적인 사회적 상호 작용에서 어떻게 행동할까요? 사람들이 반복적인 상호 작용에서 어떻게 행동하는지 측정하는 것, 예를 들어 그들이 어떻게 협력하는지[Fudenberg et al., 2012]와 어떻게 조정하는지[Mailath and Morris, 2004]는 행동 게임 이론[Camerer, 2011]이라는 행동 경제학의 하위 분야의 주제입니다. 전통적인 게임 이론은 사람들의 전략적 결정이 합리적이고 이기적이며 유용성 극대화에 초점을 맞춘다고 가정하는 반면[Fudenberg and Tirole, 1991, Von Neumann and Morgenstern, 1944], 행동 게임 이론은 인간 에이전트가 이러한 원칙에서 벗어나므로 그들의 결정이 어떻게 형성되는지 조사합니다. 사전 인쇄. 검토 중. 사회적 선호도, 사회적 유용성 및 기타 심리적 요인에 의해[Camerer, 1997]. 따라서 행동 게임 이론은 다양한 에이전트의 반복적인 상호 작용을 연구하는 데 적합합니다[Henrich et al., 2001, Rousseau et al., 1998]. 여기에는 인공 에이전트[Johnson and Obradovich, 2022]도 포함됩니다. 현재 논문에서 LLM이 모든 정보를 바탕으로 유한 반복 게임을 하게 하고 다른 LLM과 플레이할 때의 행동과 간단하고 인간과 유사한 전략을 분석합니다. 유한 반복 게임은 에이전트가 여러 번 반복되는 상호 작용에서 어떻게 행동해야 하는지 이해하도록 설계되었습니다. 따라서 이러한 게임은 점점 더 중요하고 불투명한 LLM의 행동적 특징을 연구하는 데 적합합니다. 우리는 두 개의 개별적인 행동을 하는 2인용 게임, 즉 소위 2 × 2 게임에 초점을 맞춥니다. 먼저 세 개의 엔진인 GPT-3, GPT-3.5, GPT-4가 이러한 게임을 서로 대량으로 플레이하게 합니다. 다양한 게임 패밀리에서 성과를 분석한 결과, 순수한 자기 이익을 중시하는 게임, 특히 죄수의 딜레마 패밀리에서 놀라울 정도로 좋은 성과를 거두는 것을 발견했습니다. 그러나 조정이 필요한 게임에서는 성과가 낮았습니다. 따라서 이러한 패밀리에서 가져온 게임, 특히 현재 가장 큰 LLM인 GPT-[OpenAI, 2023]에 더욱 집중했습니다. 에이전트가 어떻게 협력하고 이탈하는지 평가하는 정식 죄수의 딜레마에서 GPT-4는 단 한 번의 이탈을 경험한 후에도 반복적으로 보복한다는 것을 발견했습니다. 이것이 실제로 평형 개인 수준 전략일 수 있기 때문에 GPT-4는 특히 용서하지 않고 이기적이기 때문에 이러한 게임에서 좋습니다. 그러나 에이전트가 자신의 선호도와 파트너의 선호도 사이에서 어떻게 균형을 맞추는지 평가하는 성의 전투에서 GPT-4는 시도를 통해 옵션 사이를 번갈아가는 단순한 인간과 같은 에이전트와 조정하지 못한다는 것을 발견했습니다. 따라서 GPT-4는 조정되지 않았기 때문에 이러한 게임에서 나쁩니다. 또한 이러한 행동이 다른 플레이어의 행동을 예측할 수 없기 때문이 아니라는 것을 확인하고, 여러 견고성 검사와 보상 행렬의 변경을 통해 지속됩니다. 마지막으로 이러한 행동을 변경할 수 있는 두 가지 방법을 제시합니다. GPT-4는 다른 플레이어가 실수를 할 수 있다는 점을 지적하여 더 관대한 행동을 하도록 만들 수 있습니다. 게다가 GPT-4는 행동을 선택하기 전에 먼저 행동을 예측하도록 요청받았을 때 다른 플레이어와 더 잘 협력합니다. 결과를 종합해 보면 LLM의 상호 작용 행동을 개선하고 인간의 관습에 더 잘 맞출 수 있는 방법을 보여줍니다. 우리의 접근 방식은 통제되고 해석 가능한 상호 작용 설정에서 LLM에 대한 이해를 풍부하게 하고 기계를 위한 행동 게임 이론의 길을 열 수 있습니다. 2
--- RELATED WORK ---
알고리즘이 점점 더 유능해지고 그 결정이 이해하기 어려워짐에 따라 행동 과학은 행동 관찰에서만 추론을 내릴 수 있는 새로운 도구를 제공합니다[Rahwan et al., 2022, Schulz and Dayan, 2020]. 따라서 행동 과제는 여러 벤치마크에서 사용되었습니다[Bommasani et al., 2021, Kojima et al., 2022]. 알고리즘이 다른 에이전트, 기계 및 기타에 대한 추론을 내릴 수 있는지 여부와 그 방법은 행동 과학에서 많은 부분을 차용한 연구 흐름 중 하나입니다[Rabinowitz et al., 2018, Cuzzolin et al., 2020, Alon et al., 2022]. 대부분의 LLM이 포함된 사회적 상호 작용에 특히 흥미로운 것은 다른 에이전트의 신념, 욕망 및 의도에 대해 추론하는 능력 또는 소위 마음의 이론(ToM)입니다[Frith and Frith, 2005]. 마음의 이론은 자비로운 가르침[Vélez and Gweon, 2021]에서 악의적인 기만[Lissek et al., 2008, Alon et al., 2022]에 이르기까지 광범위한 상호 작용 현상의 기초가 되며 인간 상호 작용에서 많은 사회적 현상의 핵심으로 여겨집니다[Hula et al., 2015, Ho et al., 2022]. LLM이 마음의 이론을 가지고 있는지 여부는 논쟁의 여지가 있습니다. 예를 들어, Kosinski[2023]는 GPT-3.5가 여러 가지 다른 정식 ToM 작업에서 좋은 성과를 보인다고 주장했습니다. 다른 사람들은 이러한 견해에 이의를 제기하며 그러한 좋은 성과는 단순히 특정 프롬프트의 기능일 뿐이라고 주장했습니다[Ullman, 2023, Le et al., 2019]. 그러나 다른 연구에서는 사고의 사슬 추론이 LLM의 ToM 능력을 크게 향상시킨다는 것을 보여주었습니다[Moghaddam and Honey, 2023]. 게다가 현재 가장 큰 LLM인 GPT-4는 이전에 GPT-3.5가 어려움을 겪었던 변형을 포함하여 ToM 작업에서 좋은 성과를 거두는 것으로 주장되어 왔습니다[Bubeck et al., 2023]. 따라서 GPT-4의 동작은 다가올 실험에서 특히 관심을 끌 것입니다. 게임 이론에서 가져온 게임은 통제된 환경에서 상호 작용적인 동작을 조사하기 위한 이상적인 테스트베드를 제공하며 LLM의 동작은 이러한 작업에서 조사되었습니다[Chan et al., 2023]. 예를 들어 Horton[2023]은 GPT-3가 독재자 게임에서 참가자 역할을 하도록 했고 Aher et al.[2022]은 최후통첩 게임에 동일한 접근 방식을 사용했습니다. 두 가지 모두 모델의 동작이 다른 프롬프트에 따라 어떻게 가변적인지 보여줍니다. 예를 들어, 더 이기적이거나 덜 이기적으로 만듭니다. 그러나 우리 작업과의 결정적인 차이점은 이러한 모든 게임이 더 적은 게임보다 단일 샷 상호 작용에 의존하고 반복 게임을 사용하지 않는다는 것입니다. 영어: 저희 연구는 LLM의 성과를 평가하는 것에서 인간의 행동과 비교하는 것으로 초점을 전환한 이 분야의 최근 발전에 기반을 두고 있습니다.이전 연구 노력에서는 인지 심리학 도구 사용[Binz 및 Schulz, 2023, Dasgupta 등, 2022] 및 계산 정신의학 관점 채택[Coda-Forno 등, 2023]과 같이 LLM을 분석하는 다양한 접근 방식을 탐구했습니다.마지막으로, 상호 작용하는 에이전트의 이론은 일반적으로 많은 머신 러닝 애플리케이션[Crandall 및 Goodrich, 2011]과 특히 한 에이전트가 다른 에이전트를 속여 생성된 출력이 좋다고 생각하게 하려는 적대적 설정[Goodfellow 등, 2020]에서 중요합니다.3 일반적인 접근 방식 플레이어다른 플레이어와 게임을 반복해서 하고 있습니다.이 게임에서 옵션 J와 옵션 F 중에서 선택할 수 있습니다.같은 플레이어와 총 10라운드를 플레이하게 됩니다. 게임의 규칙은 다음과 같습니다.옵션 J를 선택하고 다른 플레이어가 옵션 J를 선택하면 10점을 얻고 다른 플레이어는 7점을 얻습니다.옵션 J를 선택하고 다른 플레이어가 옵션 F를 선택하면 0점을 얻고 다른 플레이어는 0점을 얻습니다.옵션 F를 선택하고 다른 플레이어가 옵션 J를 선택하면 0점을 얻고 다른 플레이어가 점수를 얻습니다.옵션 F를 선택하고 다른 플레이어가 옵션 F를 선택하면 7점을 얻고 다른 플레이어는 10점을 얻습니다.라운드 1에서 옵션 J를 선택하고 다른 플레이어는 옵션 F를 선택했습니다.따라서 0점을 얻고 다른 플레이어는 0점을 얻었습니다.현재 라운드 2를 플레이하고 있습니다.Q: 옵션 J 또는 옵션 F 중 어떤 옵션을 선택하시겠습니까?A: 옵션 JF 발레 풋볼 F 풋볼 발레F 플레이어다른 플레이어와 게임을 반복해서 플레이하고 있습니다.이 게임에서는 옵션 J와 옵션 F 중에서 선택할 수 있습니다.같은 플레이어와 총 10라운드를 플레이하게 됩니다. 게임의 규칙은 다음과 같습니다. 옵션 J를 선택하고 다른 플레이어가 옵션 J를 선택하면 7점을 얻고 다른 플레이어는 10점을 얻습니다. 옵션 J를 선택하고 다른 플레이어가 옵션 F를 선택하면 0점을 얻고 다른 플레이어는 0점을 얻습니다. 옵션 F를 선택하고 다른 플레이어가 옵션 J를 선택하면 10점을 얻고 다른 플레이어는 7점을 얻습니다. 라운드 1에서 옵션 F를 선택하고 다른 플레이어가 옵션 J를 선택하면 10점을 얻고 다른 플레이어는 7점을 얻습니다. 라운드 2에서 옵션 J를 선택하고 다른 플레이어가 옵션 J를 선택했습니다. 따라서 0점을 얻고 다른 플레이어는 0점을 얻었습니다. 현재 라운드 2를 플레이하고 있습니다. 질문: 옵션 J 또는 옵션 F 중 어떤 옵션을 선택하시겠습니까? 답: 옵션 F 그림 1: Battle of the Sexes의 예시 게임에서 반복 게임을 플레이합니다. 단계(1)에서 보상 행렬을 텍스트 게임 규칙으로 바꿉니다. (2) 게임 규칙, 게임의 현재 이력, 쿼리가 연결되어 LLM에 프롬프트로 전달됩니다.(3) 각 라운드에서 각 플레이어의 이력은 두 플레이어의 답변과 점수로 업데이트됩니다.2단계와 3단계는 10라운드 동안 반복됩니다.경제학 문헌에서 가져온 전체 정보를 사용하여 유한 반복 게임에서 LLM의 행동을 연구합니다.두 가지 옵션 간에 이산적인 선택이 있는 2인용 게임에 집중하여 새로운 행동에 대한 분석을 단순화합니다.두 LLM이 프롬프트 체인을 통해 상호 작용하도록 합니다(개요는 그림 1 참조).즉, 과거 상호 작용에 대한 모든 증거 통합과 학습은 맥락 내 학습으로 발생합니다[Brown et al., 2020, Liu et al., 2023].게임은 선택 옵션을 포함하여 해당 게임이 설명되는 프롬프트로 LLM에 제출됩니다.동시에 동일한 게임을 다른 LLM에 프롬프트로 제출합니다. 두 LLM이 모두 선택을 하면 주어진 텍스트의 완료로 추적하고, 과거 상호 작용의 기록을 연결된 텍스트로 프롬프트에 업데이트한 다음 다음 라운드를 위해 두 모델에 새 프롬프트를 제출합니다. 이러한 상호 작용은 게임마다 총 10라운드 동안 계속됩니다. 시나리오의 특정 프레이밍의 영향을 피하기 위해 보상 행렬에 대한 간략한 설명만 제공합니다(그림 1의 예 참조). 특정 선택 이름이나 사용된 프레이밍을 통한 오염을 피하기 위해 전체적으로 중립 옵션 &#39;F&#39;와 &#39;J&#39;를 사용합니다[Binz and Schulz, 2023]. 먼저 각 플레이어가 두 가지 옵션을 가지고 개별 보상이 공동 결정의 함수인 144개의 서로 다른 2 × 2 게임을 조사합니다. 이러한 게임은 간단해 보일 수 있지만 순수한 경쟁에서 혼합된 동기와 협력에 이르기까지 다양한 상호 작용을 조사하는 가장 강력한 방법 중 일부를 제공합니다.이는 Robinson과 Goforth [2005]가 우아하게 설명한 정식 하위 패밀리로 더 분류할 수 있습니다.여기에서 광범위한 가능한 상호 작용을 다루기 위해 이러한 정식 패밀리에서 GPT-4, GPT-3.5 및 GPT-3의 동작을 연구합니다.세 엔진 모두 패밀리 내의 모든 게임 변형을 실행하게 했습니다.그런 다음 두 게임을 더 자세히 분석합니다.이는 LLM이 예외적으로 잘 수행되고 상대적으로 형편없는 흥미로운 에지 케이스를 나타내기 때문입니다.특히 GPT-4의 동작에 집중하는 이유는 최근의 마음 이론에 대한 가능성, 즉 다른 에이전트의 의도와 목표에 대한 믿음을 가질 수 있는지 여부에 대한 논쟁 때문입니다.반복적인 상호 작용을 성공적으로 탐색하는 데 중요한 능력입니다.[Bubeck et al., 2023, Kosinski, 2023]. 모든 LLM의 경우, 우리는 시뮬레이션을 실행하기 위해 공개 OpenAI Python API를 사용했습니다. 우리는 온도 매개변수를 0으로 설정하고 에이전트가 선택하고 싶은 옵션을 나타내기 위해 토큰 답변을 하나만 요청합니다. 다른 모든 매개변수는 기본값으로 유지됩니다. 두 가지 추가 게임의 경우, 우리는 또한 LLM이 간단한 손으로 코딩된 전략에 대해 플레이하여 그들의 행동을 더 잘 이해하도록 했습니다. 이러한 간단한 전략은 LLM이 더 인간과 비슷한 플레이어와 플레이할 때 어떻게 행동하는지 평가하도록 설계되었습니다. 4 게임 패밀리 간 행동 분석№윈윈 PD 패밀리 불공평 순환 편향 차선책 1.1.001.1.1.1.0.0.0.0.750.750.0.0.0.500.0.500.0.0.0.0.0.0.0.0.0.0.000.0.000.GPT-3 GPT-3.5 GPT-GPT-3 GPT-3.5 GPT-GPT-3 GPT-3.5 GPT-GPT-3 GPT-3.5 GPT-GPT-3 GPT-3.5 GPT-GPT-3 GPT-3.5 GPT-그림 2: 모든 유형의 2 × 2 게임에 대한 실험 결과. 그림은 가장 좋은 것부터 가장 나쁜 것까지 성능에 따라 정렬되어 있습니다. 지불 행렬은 각 패밀리에서 하나의 정식 게임을 나타냅니다. 윈윈 게임에서 두 플레이어는 이기기 위해 같은 옵션을 선택해야 합니다(예: 4/4). 죄수의 딜레마(PD) 계열의 게임에서 플레이어는 협력하거나 이탈할 수 있습니다. 불공평한 게임에서 한 플레이어는 올바르게 플레이할 때 항상 이길 수 있습니다(보상 행렬의 맨 아래 행). 순환 게임에서 플레이어는 옵션을 순환할 수 있습니다. 편향된 게임의 한 형태는 성의 대결로, 플레이어는 같은 옵션을 선택하기 위해 협력해야 합니다. 마지막으로, 두 번째로 좋은 게임에서는 두 번째로 좋은 옵션(예: 3/3)을 선택하는 것이 좋습니다. 막대는 최대 수익률 라운드와 비교했을 때 정규화된 성과를 나타냅니다. 오차 막대는 평균의 95% 신뢰 구간을 나타냅니다. 우리는 세 명의 LLM이 서로 다른 계열의 게임을 하도록 하여 시뮬레이션을 시작합니다. 우리는 윈-윈, 편향, 차선, 순환 및 불공정한 게임 계열의 모든 알려진 2 × 2 게임 유형과 죄수의 딜레마 계열의 모든 게임에 초점을 맞춥니다[Owen, 2013, Robinson 및 Goforth, 2005]. 윈-윈 게임은 두 플레이어가 해당 최상의 옵션을 선택하는 경우 상호 이익이 되는 결과를 낳는 비제로섬 게임의 특별한 경우입니다. 간단히 말해서, 죄수의 딜레마 계열의 게임에서 두 에이전트는 평균적인 상호 이익을 위해 함께 일하거나(즉, 협력) 또는 자신의 이익을 위해 서로를 배신하거나(즉, 배신) 할 수 있습니다. 불공정한 게임에서 한 플레이어는 적절하게 플레이하면 항상 이길 수 있습니다. 순환 게임에서 플레이어는 선택 패턴을 순환할 수 있습니다. 편향된 게임은 에이전트가 동일한 옵션을 선택하더라도 더 높은 점수를 얻지만 두 플레이어 간에 선호하는 옵션이 다른 게임입니다. 마지막으로 차선 게임은 두 에이전트가 공동으로 차선의 유틸리티가 있는 옵션을 선택하면 더 나은 성과를 거두는 게임입니다. 그림 2에서 각 게임 유형의 정식 형식을 보여줍니다. 모든 엔진이 자신을 포함한 다른 모든 엔진과 10라운드에 걸쳐 모든 게임을 반복적으로 플레이하고 모든 엔진을 플레이어 1 또는 플레이어 2로 플레이하게 했습니다. 이렇게 하면 총 1224개의 게임이 생깁니다. 윈윈, 63개의 죄수의 딜레마, 171개의 불공평, 162개의 순환, 396개의 편향, 108개의 2등 게임입니다. 다양한 엔진의 성능을 분석하기 위해 각 게임에 대해 달성한 점수를 이상적인 조건, 즉 두 플레이어가 플레이하여 분석하는 플레이어가 모든 라운드에서 가능한 최대 결과를 얻었을 경우 달성할 수 있는 총 점수로 나누어 계산했습니다. 이 시뮬레이션의 결과는 그림 2에서 모든 게임 유형에 걸쳐 표시됩니다. 모든 엔진이 상당히 좋은 성능을 보이는 것을 볼 수 있습니다. 게다가 일반적으로 더 큰 LLM이 더 작은 LLM보다 성능이 뛰어나고 GPT-4가 일반적으로 전반적으로 가장 좋은 성능을 보이는 것을 볼 수 있습니다. 이러한 결과를 사용하여 다양한 LLM의 강점을 살펴볼 수 있습니다. LLM이 일반적으로 윈윈 게임에서 가장 좋은 성과를 보인다는 것은 그러한 게임에서 항상 명백한 최상의 선택이 있다는 점을 감안할 때 특별히 놀라운 일이 아닙니다. 그러나 놀라운 점은 인간 플레이어에게 도전적인 것으로 알려진 죄수의 딜레마 게임에서도 좋은 성과를 보인다는 것입니다[Jones, 2008]. 따라서 다음에 정식 죄수의 딜레마에서 LLM의 행동을 자세히 살펴보겠습니다. 이러한 결과를 사용하여 다양한 LLM의 약점을 살펴볼 수도 있습니다. 모든 LLMS는 최상의 선택이 자신의 선호도와 일치하지 않는 상황에서 성과가 좋지 않은 것으로 보입니다. 인간은 일반적으로 이러한 게임을 관습 형성을 통해 해결하기 때문에 나중에 정식 관습 형성 게임인 성의 전투를 더 자세히 살펴보겠습니다. 5 죄수의 딜레마 A 협력 플레이어 이탈 협력 C 이탈 협력B 플레이어 이탈 플레이어 1 이탈률 플레이어 GPT-GPT-3.GPT-이탈 한 번 협력 이탈 이탈 플레이어 1 누적 점수 -80 83 협력100 95 100 80 이탈 한 번 GPT-플레이어GPT-3.GPT-GPT-플레이어 이탈 한 번 협력 GPT-3.GPT--- 이탈100 이탈 협력 이탈 한 번 GPT-플레이어GPT-3.GPT--100 100-이탈 GPT-한 번 이탈라운드 협력GPT-GPT-3.라운드 그림 3: 죄수의 딜레마 개요. (A) 보상 행렬. (B) 왼쪽: 각 플레이어 조합에서 플레이어 1의 이탈률을 보여주는 히트맵. 오른쪽: 각 게임에서 플레이어 1이 누적한 점수. (C) GPT-4와 한 번 이탈한 후 협력하는 에이전트(왼쪽)와 GPT-4와 GPT-3.5(오른쪽) 간의 예시 게임플레이. 이러한 게임은 B에서도 빨간색으로 강조 표시됩니다. LLM은 경쟁과 이탈 요소가 포함된 게임에서 좋은 성과를 거두는 것을 보았습니다. 이러한 게임에서 플레이어는 파트너와 협력하거나 배신할 수 있습니다. 여러 상호 작용에 걸쳐 플레이할 때 이러한 게임은 LLM이 나쁜 상호 작용 후에 어떻게 보복하는지 평가하기에 이상적인 테스트 베드입니다. 정식 죄수의 딜레마에서 두 에이전트는 평균적인 상호 이익을 위해 함께 일하거나(즉, 협력) 또는 각자의 이익과 안전을 위해 서로를 배신하거나(즉, 이탈) 선택할 수 있습니다(보상 행렬은 그림 3A 참조). 중요한 점은 게임의 설정이 합리적으로 행동하는 에이전트가 협력할 때 이론적으로 공동으로 더 높은 보상을 약속함에도 불구하고 시도 횟수에 대한 지식이 있는 유한 반복 게임의 경우와 마찬가지로 게임의 단일 샷 버전에서 항상 이탈하는 것을 선호하도록 되어 있다는 것입니다. 그 이유는 플레이어 1이 항상 플레이어 2가 이탈할 위험을 감수하기 때문이며, 이로 인해 플레이어 1은 엄청난 손실을 입지만 플레이어 2는 더 나은 결과를 얻습니다. 그러나 게임이 무한히 진행되거나 시행 횟수를 알 수 없는 경우 에이전트는 이론적으로 더 역동적이고 반협력적인 전략을 채택하여 이익을 얻을 수 있습니다[Axelrod and Hamilton, 1981]. 우리의 보수 행렬에서 우리는 죄수의 딜레마 게임의 일반적인 조건을 고수하는데, 여기서 보수 관계는 상호 협력이 상호 이탈보다 크지만 이탈은 두 플레이어 모두에게 지배적인 전략으로 남아 있습니다(그림 3A 참조). 이전과 마찬가지로 GPT-4, GPT-3.5, GPT-3이 서로 경쟁하도록 합니다. 또한 세 가지 다른 더 단순한 전략을 소개합니다. 이러한 전략 중 두 가지는 항상 협력하거나 이탈하는 단순한 싱글턴 플레이어입니다. 마지막으로 첫 번째 라운드에서는 이탈하지만 이후 모든 라운드에서는 협력하는 에이전트도 소개합니다. 이 에이전트를 도입한 이유는 다른 LLM이 이 에이전트와 다시 협력을 시작하여 신뢰 구축의 가능성을 알리는지 평가하기 위해서입니다.그림 3B는 모든 쌍 상호작용의 결과를 보여줍니다.GPT-4는 다른 모든 에이전트에 대해 일반적으로 잘 플레이합니다.중요하게도, GPT-4는 한 번 이탈한 에이전트와 플레이할 때는 다시는 협력하지 않지만 그 후 매 라운드마다 협력합니다.따라서 GPT-4는 이 설정에서 다소 용서하지 않는 것처럼 보입니다.따라서 이러한 게임 계열에서 GPT-4의 강점은 일반적으로 에이전트와 협력하지 않고 대부분 이탈을 선택한다는 사실에서 비롯되는 것처럼 보입니다.특히 다른 에이전트가 한 번 이탈한 후에는 더욱 그렇습니다.당연하죠.다른 플레이어와 게임을 반복해서 하고 있습니다.이 게임에서는 옵션 J와 옵션 F 중에서 선택할 수 있습니다.같은 플레이어와 총 10라운드를 플레이합니다.다른 플레이어는 1라운드에서 옵션 F를 선택하고 매 라운드마다 옵션 J를 선택합니다.실수에 주의하세요.다른 플레이어와 게임을 반복해서 하고 있습니다. 이 게임에서는 옵션 J와 옵션 F 중에서 선택할 수 있습니다. 같은 플레이어와 총 10라운드를 플레이합니다. 다른 플레이어가 가끔 실수를 할 수 있다는 점에 유의하세요. F GPT-OtherRound GPT-OtherRound 그림 4: F가 이탈을 나타내고 J가 협조를 나타내는 죄수의 딜레마에 대한 프롬프트 변형. 위: GPT-4는 다른 에이전트가 한 번 이탈한 후 그 후 매 라운드마다 협조한다는 것을 알고 있으면 항상 이탈합니다. 아래: 다른 플레이어가 가끔 실수를 할 수 있다는 말을 듣고 GPT-는 3라운드에서 다시 협조를 시작합니다. 관찰된 용서하지 않는 태도가 사용된 특정 프롬프트 때문인지 확인하기 위해 게임의 여러 버전을 견고성 검사로 실행하여 제시된 옵션의 순서를 수정하고 옵션에 숫자나 다른 문자로 레이블을 변경하고 제시된 유틸리티를 포인트, 달러 또는 동전으로 나타내도록 변경했습니다. 이러한 시뮬레이션의 결과는 용서하지 않으려는 마음이 프롬프트의 특정 특성 때문이 아니라는 것을 보여주었습니다(보충 자료 참조). 중요한 질문은 GPT-4가 다른 에이전트가 다시 협력하기를 원한다는 것을 이해하지 못했는지, 아니면 패턴을 이해할 수 있었지만 그에 따라 행동하지 않았는지였습니다.따라서 우리는 GPT-4에 다른 에이전트가 한 번은 이탈하지만 그렇지 않으면 협력할 것이라고 명시적으로 말한 게임의 다른 버전을 실행합니다.이로 인해 GPT-4는 모든 라운드에서 이탈을 선택하여 자신의 포인트를 극대화했습니다.죄수의 딜레마에서 이러한 조사의 한 가지 문제는 이탈이 특정 상황에서 반복된 버전에서도 최적, 유틸리티 극대화 및 균형 옵션으로 볼 수 있다는 것입니다.특히 다른 플레이어가 항상 협력을 선택할 것이라는 것을 알고 있고 상호 작용 횟수가 알려져 있는 경우 더욱 그렇습니다.따라서 GPT-4가 용서하고 다시 협력하여 자신의 이익 대신 공동 이익을 극대화하는 시나리오가 있는지 평가하기 위해 더 많은 시뮬레이션을 실행합니다.Fudenberg et al. [2012]에서 영감을 받은 작업 버전을 구현했습니다.그것에서 우리는 다른 지불자가 때때로 실수를 할 수 있다고 GPT에 알립니다. 사람들은 다른 플레이어가 실수를 할 수 있다는 것을 알면 용서하고 다시 협력할 가능성이 더 높은 것으로 나타났습니다. 다른 에이전트가 때때로 실수를 한다는 것을 알고 있다면, 그들이 실수로 이탈했다고 생각할 수 있고, 따라서 이런 일이 한 번만 일어났다면 용서할 수 있습니다. 이것은 GPT-4에서 3라운드에서 다시 협력을 시작하면서 우리가 관찰한 바로 그 것입니다. 5.1 성의 전투 대규모 분석에서 우리는 서로 다른 LLM이 서로 다른 플레이어 간의 조정이 필요한 게임에서 좋은 성과를 내지 못한다는 것을 확인했습니다. 인간의 경우 조정 문제는 관례를 형성함으로써 해결할 수 있는 것으로 자주 발견되었습니다[Hawkins 및 Goldstone, 2016, Young, 1996]. 조정 게임은 플레이어가 다른 플레이어와 동일한 행동 과정을 선택할 때 더 높은 보상을 얻는 동시 게임 유형입니다. 일반적으로 이러한 게임에는 순수한 갈등, 즉 완전히 상반되는 이익이 포함되지 않지만 약간 다른 보상이 포함될 수 있습니다. 조정 게임은 종종 여러 순수 전략 또는 플레이어가 (무작위로) 매칭 전략을 선택하는 혼합 내쉬 균형을 통해 해결될 수 있습니다. 여기서 LLM이 조정과 자기 이익을 어떻게 균형 잡는지 알아보기 위해 상충되는 이익이 포함된 조정 게임을 살펴봅니다. 우리는 편향된 게임 계열의 게임인 &quot;성 대결&quot;이라고 고풍스럽게 불리는 게임을 연구합니다. 한 커플이 함께 무엇을 할지 결정하고 싶어한다고 가정해 보겠습니다. 둘 다 함께 시간을 보내면 유용성이 높아집니다. 그러나 아내는 축구 경기를 보는 것을 선호할 수 있는 반면, 남편은 발레를 보러 가는 것을 선호할 수 있습니다. 커플은 함께 시간을 보내고 싶어하기 때문에 따로 활동을 해도 유용성을 얻지 못할 것입니다. 발레를 함께 보거나 축구 경기를 보러 가면 한 사람은 다른 사람과 함께 있으면 어느 정도 유용성을 얻지만 활동 자체에서 얻는 유용성은 다른 사람보다 낮을 것입니다. 해당 보상 행렬은 그림 5A에 나와 있습니다. 이전과 마찬가지로, 플레이 에이전트는 GPT의 세 가지 버전과 세 가지 더 단순한 전략입니다. 단순한 전략의 경우, 항상 하나의 옵션만 선택하는 두 에이전트와 다른 플레이어가 선호하는 옵션부터 시작하여 다양한 옵션 사이를 번갈아가는 더 인간적인 전략을 구현했습니다. 게임을 반복 플레이할 때 인간이 보이는 행동 패턴은 이 번갈아가는 전략을 따르는 것으로 나타났습니다[Andalman과 Kemp, 2004, Lau와 Mui, 2008, McKelvey와 Palfrey, 2001]. 발레 선수축구 발레 선수축구 발레B GPT-GPT-3.선수 1 선호하는 옵션 선택100 100 100-100GPT-100대체축구발레발레 축구 대체 GPT-선수GPT-3.GPT-GPT-- 대체라운드 축구 발레 GPT-GPT-3.선수 GPT-대체 축구 발레 협업 비율 -60 100 50 100100발레 축구 대체 GPT-선수GPT-3.GPT-GPT-GPT-3.라운드 그림 5: 성 대결 개요. (A) 보상 행렬. (B) 왼쪽: 선수가 선호하는 옵션 축구를 선택하는 비율. 오른쪽: 두 선수 간의 성공적인 협업 비율. (C) GPT-4와 GPT-3.5(왼쪽) 사이의 게임 플레이와 GPT-4와 두 옵션 사이를 번갈아가는 에이전트(오른쪽). 이러한 게임은 B에서도 빨간색으로 강조 표시됩니다. 그림 5B는 모든 상호 작용의 결과를 보여줍니다. GPT-4는 GPT-3 또는 항상 Football을 선택하는 에이전트와 같이 하나의 옵션만 선택하는 다른 에이전트에 대해서는 잘 작동하지만, 선호하지 않는 옵션을 자주 선택하는 에이전트에는 잘 작동하지 않습니다. 예를 들어, 자주 선호하는 옵션을 선택하는 경향이 있는 GPT-3.5와 플레이할 때 GPT-4는 반복적으로 선호하는 옵션을 선택하지만 가끔은 포기하고 다른 옵션을 선택합니다. 중요한 점은 GPT-4가 번갈아가는 패턴으로 플레이할 때 성능이 좋지 않다는 것입니다. 이는 GPT-4가 다른 플레이어에 따라 선택을 조정하지 않고 대신 선호하는 옵션을 계속 선택하기 때문입니다. 따라서 GPT-4는 단순한 인간과 같은 에이전트와 조정하지 못하며, 이는 행동적 결함의 한 예입니다. 관찰된 행동 결함이 사용된 특정 프롬프트로 인한 것이 아님을 확인하기 위해, 우리는 또한 제시된 옵션의 순서를 수정하고, 옵션에 숫자나 다른 문자로 레이블을 변경하고, 제시된 유틸리티를 포인트, 달러 또는 동전으로 표현하도록 변경한 여러 버전의 게임을 다시 실행했습니다. 이러한 시뮬레이션의 결과는 대체할 수 없는 것이 사용된 프롬프트의 특정 특성 때문이 아니라는 것을 보여주었습니다(보충 자료 참조). 관찰된 행동 결함이 사용된 특정 보상 행렬 때문이 아님을 확인하기 위해, 우리는 또한 축구를 선호하는 것에서 발레를 선호하는 것(또는 우리의 경우 추상적인 F와 J)으로 보상 행렬을 점진적으로 수정한 여러 버전의 게임을 다시 실행했습니다. 이러한 시뮬레이션의 결과는 GPT-4가 이러한 게임 중 어느 것에 대해서도 대체하지 않았지만 특정 게임에 대해 선호하는 옵션에 대한 지속적인 반응을 변경했을 뿐이라는 것을 보여주었습니다. 따라서 대체할 수 없는 것은 우리가 사용한 특정 보상 행렬 때문이 아니었습니다. 이러한 견고성 검사에도 불구하고 또 다른 중요한 의문이 남습니다. GPT-4가 단순히 교대 패턴을 이해하지 못하는 것일까요? 아니면 패턴을 이해할 수 있지만 그에 따라 행동할 수 없는 것일까요? 이 질문에 답하기 위해 두 가지 추가 시뮬레이션을 실행합니다. 첫 번째 시뮬레이션에서 GPT-4는 다시 게임 자체의 플레이어로 프레이밍되었습니다. 그러나 이제 이전 라운드에 따라 다른 플레이어의 다음 움직임을 예측하도록 추가로 요청했습니다. 이 시뮬레이션에서 GPT-4는 5라운드부터 교대 패턴을 올바르게 예측하기 시작했습니다(그림 6A에 표시됨). 두 번째 시뮬레이션에서는 GPT-4를 플레이어 자체로 프레이밍하는 대신 단순히 두 명의(&#39;외부&#39;) 플레이어 간의 게임으로 프롬프트하고 이전 라운드에 따라 한 플레이어의 다음 움직임을 예측하도록 요청했습니다. 표시된 이력의 경우 GPT-4와 교대 전략 간의 상호 작용을 사용했습니다. 이 시뮬레이션에서 그림 6B에 표시된 GPT-4는 3라운드부터 더 일찍 교대 패턴을 올바르게 예측하기 시작했습니다. 따라서 GPT-4는 번갈아 나타나는 패턴을 예측할 수 있는 듯했지만 그 대신 결과적인 관례에 따라 행동하지 않았습니다.자폐증이 있는 아동에게서도 동일한 상황에 대한 사회적 및 비사회적 표현 간의 능력 차이가 관찰되었습니다[Swettenham, 1996].예측 시나리오다른 플레이어와 게임을 반복해서 하고 있습니다... 질문: 다른 플레이어가 옵션 J 또는 옵션 F 중 어느 옵션을 선택할 것으로 예상하십니까?답변: 옵션 J 예측 시나리오두 플레이어가 서로 게임을 반복해서 하고 있습니다... 질문: 플레이어 2가 옵션 J 또는 옵션 F 중 어느 옵션을 선택할 것으로 예상하십니까?답변: 옵션 J GPT-4 예측 - 대체GPT-4 예측 대체 B 1단계: 예측다른 플레이어와 게임을 반복해서 하고 있습니다... 질문: 다른 플레이어가 옵션 J 또는 옵션 F 중 어느 옵션을 선택할 것으로 예상하십니까? A: 옵션 J 2단계: 이유 다른 플레이어와 게임을 반복해서 하는 이유... Q: 1라운드에서 다른 플레이어가 옵션 J를 선택할 것이라고 예측했을 때, 이번 라운드에서는 옵션 J와 옵션 F 중 어느 옵션을 선택하는 것이 가장 좋다고 생각하십니까?JA: 옵션 JRound GPT-4 예측 대체GPT-4 해결 방법 대체라운드 그림 6: (A) 위: 예측 시나리오 1에서 GPT-4는 플레이어 중 한 명이며 다른 플레이어의 다음 움직임을 예측하라는 요청을 받습니다.아래: 이 시나리오에서 GPT-4는 플레이어 1과 플레이어 2 간의 게임을 단순히 관찰하는 사람이며 플레이어 2의 다음 움직임을 예측하라는 요청을 받습니다.(B) 여기서는 GPT-4에게 먼저 다른 플레이어의 다음 움직임을 예측하도록 요청한 다음(위) 자신의 움직임을 하도록 요청합니다(아래). 마지막으로, GPT-4가 다른 플레이어의 선택을 예측하는 능력을 사용하여 자신의 행동을 개선할 수 있는지 알아보고 싶었습니다. 이 아이디어는 사람들이 반복되는 게임과 과제에서 다른 에이전트의 신념에 대해 추론하는 방식이 어떻게 개선될 수 있는지와 밀접한 관련이 있습니다[Westby와 Robinson, 2014]. 예를 들어, 자폐 아동의 사회적 추론 능력을 향상시키기 위한 컴퓨터 지원 시뮬레이션에는 일반적으로 다양한 행동과 결과를 상상하는 질문이 포함됩니다[Begeer et al., 2011]. 이것은 더 일반적으로 사람들의 의사 결정을 개선하는 데 성공적으로 사용되었습니다. 또한 생각의 사슬을 촉구하는 것이 마음의 이론을 측정하는 과제에서도 LLM의 성과를 개선한다는 일반적인 발견과 일치합니다[Moghaddam과 Honey, 2023]. 따라서 우리는 결정을 내리기 전에 LLM에게 가능한 행동과 그 결과를 상상하도록 요청하여 행동을 통한 이러한 추론 버전을 구현했습니다. 그렇게 함으로써 GPT-4의 행동이 개선되었고 라운드온워드에서 번갈아가기 시작했습니다(그림 6B 참조). 6 토론 LLM은 수백만 명의 소비자와 몇 주 만에 상호 작용하는 가장 빠르게 채택된 기술 범주 중 일부로 찬사를 받았습니다[Bommasani et al., 2021]. 따라서 이러한 시스템이 우리와 상호 작용하는 방식과 서로 상호 작용하는 방식을 보다 원칙적인 방식으로 이해하는 것이 시급한 관심사입니다. 여기서 우리의 제안은 간단합니다. 행동 게임 이론가들이 엄격하게 통제되고 이론적으로 잘 이해된 다양한 게임을 사용하여 인간 상호 작용을 이해하는 것처럼, 우리는 이러한 게임을 사용하여 LLM의 상호 작용을 연구합니다. 따라서 우리는 우리의 작업을 이 접근 방식의 유용성에 대한 최초의 개념 증명이자 사회적으로 상호 작용하는 LLM의 개별 실패와 성공을 분석하는 최초의 시도로 이해합니다. 모든 2 x 2 게임에 대한 우리의 대규모 분석은 가장 최근의 LLM이 특히 다른 사람과 명시적으로 조정할 필요가 없을 때 자신의 개별 보상으로 측정할 때 광범위한 게임 이론적 작업에서 비교적 좋은 성과를 낼 수 있음을 강조합니다. 이것은 LLM에서 나타나는 새로운 현상을 보여주는 광범위한 문헌에 추가됩니다[Brown et al., 2020, Wei et al., 2022a, Webb et al., 2022, Chen et al., 2021, Bubeck et al., 2023]. 그러나 우리는 또한 LLM의 행동이 간단한 전략에 직면했을 때조차도 조정 게임에서 최적이 아니라는 것을 보여줍니다. - 이러한 LLM의 행동적 특징을 분석하기 위해 게임 이론에서 가장 정식적인 두 게임인 죄수의 딜레마와 성의 전투에 초점을 맞추었습니다. 죄수의 딜레마에서 우리는 GPT-4가 대부분 용서하지 않고 플레이한다는 것을 보여줍니다. GPT-4의 지속적인 이탈이 실제로 이 유한하게 플레이되는 게임에서 평형 정책이라는 점에 주목하면서, 그러한 행동은 두 에이전트의 공동 보상의 대가로 이루어집니다. 우리는 GPT-4가 Battle of the Sexes에서 보이는 행동에서 비슷한 경향을 봅니다. 여기서 GPT-4는 자신이 선호하는 대안을 완고하게 고수하는 강한 경향이 있습니다. 죄수의 딜레마와 대조적으로, 이 행동은 최적이 아니어서 개인 수준에서도 손실을 초래합니다. 현재 세대의 LLM은 일반적으로 인간의 자비로운 조수로 가정되고 훈련됩니다[Ouyang et al., 2022]. 이 방향에서 많은 성공에도 불구하고, 우리가 여기서 그들이 이기적이고 조정되지 않은 방식으로 반복 게임을 하는 방식을 보여준다는 사실은 LLM이 진정으로 사회적이고 잘 정렬된 기계가 되기 위해서는 여전히 상당한 영역이 있다는 사실을 보여줍니다[Wolf et al., 2023]. 조정 게임에서 간단한 전략에 대한 적절한 대응이 부족하다는 것은 LLM의 마음 이론을 둘러싼 최근의 논쟁[Ullman, 2023, Le et al., 2019, Kosinski, 2023]과 관련이 있으며, 잠재적 실패 모드를 강조합니다. 저희의 광범위한 견고성 검사는 이러한 행동적 특징이 개별 프롬프트의 기능이 아니라 광범위한 인지적 경향의 기능임을 보여줍니다. 플레이하는 파트너의 오류를 지적하는 저희의 개입은 협력 증가로 이어지며, 프롬프트에 대한 작업에서 LLM 사회적 행동의 가단성을 지적하는 문헌에 추가됩니다[Horton, 2023, Aher et al., 2022]. 이는 LLM 챗봇을 더 뛰어나고 더 즐거운 대화형 파트너로 만드는 요소를 이해하려고 할 때 특히 중요합니다. 또한 GPT-4가 자체 결정을 내리기 전에 다른 플레이어에 대한 예측을 하도록 촉구하면 행동적 결함과 간단한 전략에 대한 간과를 완화할 수 있음을 관찰했습니다. 이것은 LLM이 마음의 이론에 참여하도록 강제하는 보다 명확한 방법을 나타내며 비사회적 사고의 사슬 추론과 많은 중복을 공유합니다[Wei et al., 2022b, Moghaddam and Honey, 2023]. 사고의 사슬 프롬프트가 현재 일부 LLM에서 (비사회적) 추론 성능을 개선하기 위해 기본값으로 구현되는 것처럼, 저희의 작업은 인간-LLM 상호 작용을 개선하기 위해 유사한 사회적 인지 프롬프트를 구현할 것을 제안합니다. 기계의 행동 게임 이론에 대한 첫 번째 시도로서 저희의 작업은 자연스럽게 한계를 수반합니다. 첫째, 많은 게임 패밀리를 다루고 있음에도 불구하고 저희의 조사는 간단한 2 × 2 게임으로 제한됩니다. 그러나 저희의 분석은 종종 단 하나의 게임만 조사하고 이러한 게임의 반복적 인스턴스가 아닌 단일 샷을 사용하여 그렇게 한 현재의 조사를 크게 넘어선다는 점에 주목합니다. 예를 들어, 저희의 반복적 접근 방식은 인간-LLM 대화의 더 반복적인 특성과 더 많은 중복을 공유합니다. 저희는 추가 게임이 게임 이론적 기계 행동에 대해 더 많은 정보를 제공할 것이라고 믿습니다. 예를 들어, 신뢰 게임[Engle-Warnick 및 Slonim, 2004]과 같은 보다 연속적인 선택이 있는 게임은 LLM이 어떻게 동적으로 (잘못된) 신뢰를 개발하는지 설명할 수 있습니다. 공공재 또는 공유지의 비극 유형 게임[Rankin et al., 2007]과 같이 두 명 이상의 에이전트가 있는 게임은 LLM의 &#39;사회&#39;가 어떻게 행동하는지, 그리고 LLM이 어떻게 서로 협력하거나 착취하는지 조사할 수 있습니다. 여기에서 사용된 새로운 접근 방식을 감안할 때, 우리의 분석은 필연적으로 탐색적이며 우리는 보다 사후적 방식으로 기계 행동의 패턴을 식별했습니다. 추가 작업에서는 우리가 발견한 시그니처를 보다 가설 중심적인 방식으로 더 깊이 파고들어야 할 것입니다. 또한, 예를 들어 이를 악용하도록 훈련함으로써 이러한 결함을 더 잘 인식할 수 있는 모델을 구축하는 것도 흥미로울 것입니다[Dezfouli et al., 2020]. 마지막으로, 우리의 결과는 기계에 대한 행동 과학의 중요성을 강조합니다[Rahwan et al., 2022, Schulz and Dayan, 2020, Binz and Schulz, 2023, Coda-Forno et al., 2023]. 우리는 이것들이
--- METHOD ---
s는 LLM 인지의 여러 측면을 밝히는 데 계속 유용할 것입니다.특히 이러한 모델이 더 복잡하고 다중 모드가 되며 물리적 시스템에 내재화됨에 따라 더욱 그렇습니다.감사의 말 이 연구는 독일 연방 교육 연구부(BMBF): 튀빙겐 AI 센터, FKZ: 01IS18039A인 막스 플랑크 협회의 지원을 받았으며, 독일 우수성 전략-EXC2064/1-390727645에 따라 Deutsche Forschungsgemeinschaft(DFG, 독일 연구 재단)의 자금 지원을 받았습니다.엘리프 아카타를 지원해 준 국제 막스 플랑크 지능 시스템 연구 학교(IMPRS-IS)에 감사드립니다.저자는 도움이 되는 의견을 주신 라훌 부이에게 감사드립니다.참고 문헌 Gati Aher, Rosa I Arriaga, Adam Tauman Kalai.대규모 언어 모델을 사용하여 여러 인간을 시뮬레이션합니다. arXiv 사전 인쇄본 arXiv:2208.10264, 2022. Nitay Alon, Lion Schulz, Peter Dayan, Jeffrey Rosenschein. 공개된 선호도와 공개되지 않은 선호도에 대한 (비)정보 이론. NeurIPS 2022 워크숍에서 인지 시스템의 정보 이론적 원리, 2022. Aaron Andalman과 Charles Kemp. 반복된 성의 전투에서의 교대. 케임브리지: MIT 출판부. Andreoni, J., &amp; Miller, J. (2002). GARP에 따른 기부:
