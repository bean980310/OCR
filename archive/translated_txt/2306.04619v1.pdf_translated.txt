--- ABSTRACT ---
단안 이미지에서 동물의 몸과 같은 3D 관절 모양을 추정하는 것은 카메라 시점, 포즈, 질감, 조명 등의 모호성으로 인해 본질적으로 어렵습니다. 우리는 야생에서 희소한 이미지 컬렉션에서 인스턴스별 3D 모양을 재구성하는 자체 감독 프레임워크인 ARTIC3D를 제안합니다. 구체적으로, ARTIC3D는 골격 기반 표면 표현을 기반으로 하며 안정적 확산의 2D 확산 사전 확률에 의해 더욱 안내됩니다. 먼저, 우리는 2D 확산을 통해 폐색/절단으로 입력 이미지를 향상시켜 더 깨끗한 마스크 추정치와 의미적 특징을 얻습니다. 둘째, 우리는 확산 유도 3D 최적화를 수행하여 입력 이미지에 충실하고 고충실도인 모양과 질감을 추정합니다. 또한 기존 대안에 비해 확산 모델을 통해 보다 안정적인 이미지 수준 그래디언트를 계산하는 새로운 기술을 제안합니다. 마지막으로, 우리는 강체 부분 변환에서 렌더링된 모양과 질감을 미세 조정하여 사실적인 애니메이션을 생성합니다. 여러 기존 데이터 세트와 오클루전 및 절단이 있는 새로 도입된 노이즈가 있는 웹 이미지 컬렉션에 대한 광범위한 평가는 ARTIC3D 출력이 노이즈가 있는 이미지에 더 견고하고, 모양과 텍스처 세부 정보 측면에서 더 높은 품질이며, 애니메이션을 적용할 때 더 사실적임을 보여줍니다. 프로젝트 페이지: https://chhankyao.github.io/artic3d/
--- INTRODUCTION ---
관절형 3D 동물 모양은 AR/VR, 게임, 콘텐츠 제작과 같은 애플리케이션에서 널리 사용됩니다. 그러나 관절형 모델은 일반적으로 수동으로 만드는 데 노동 집약적이고 실험실 환경에서 실제 동물을 3D 스캐닝하는 것이 매우 불가능하기 때문에 얻기 어렵습니다. 이 작업에서 우리는 희소하고 노이즈가 많은 웹 이미지 컬렉션에서 직접 고품질 3D 관절형 동물 모양을 자동으로 추정하는 것을 목표로 합니다. 이는 다양한 배경, 조명, 카메라 관점, 동물 포즈, 모양, 질감 등이 있는 이미지 간의 차이로 인해 매우 잘못된 문제입니다. 또한 우리는 야생 환경에서 키포인트 및 카메라 관점과 같은 3D 모양 모델이나 이미지당 주석에 대한 액세스를 가정하지 않습니다. 여러 최근 방법[39, 33, 38]은 골격 기반 신경 표면 또는 사전 정의된 메시 템플릿을 사용하여 애니메이션 가능한 3D 모양을 생성할 수 있지만 성공은 주로 훈련 또는 최적화를 위한 대규모 이미지 데이터 세트 또는 수동으로 필터링된 깨끗한 이미지에 달려 있습니다. 또한 출력된 3D 모양과 텍스처는 새로운 관점이나 포즈 관절에서 볼 때 일반적으로 비현실적입니다. 반면, 생성 확산 모델[25, 28, 27]의 최근 성공은 주어진 텍스트 프롬프트에 대해 고품질 이미지를 생성할 수 있음을 보여줍니다. 여러 최근 연구[21, 15, 18, 23]는 다중 뷰 감독으로 2D 확산을 사용하여 3D 객체/장면을 생성할 수 있는 가능성을 더욱 잘 보여줍니다. 이 연구에서는 3D 관절 모양을 학습하기 전에 강력한 2D 확산을 활용하여 2D 또는 3D 주석 없이 희소한 노이즈가 있는 온라인 이미지에서 3D 동물을 재구성하고 애니메이션화하는 것을 목표로 합니다. 직관적으로, *Google에서 학생 연구원으로 수행한 작업. 사전 인쇄. 검토 중. 노이즈가 있는 웹 이미지 관절 모양(입력 및 새로운 뷰) 질감이 있는 출력(입력 및 새로운 뷰) 애니메이션 미세 조정된 애니메이션 그림 1: 노이즈가 있는 웹 이미지에서 관절이 있는 3D 모양 학습. 야생에서 희소하고 노이즈가 있는 이미지에서 관절이 있는 동물 신체의 3D 모양과 질감을 추정하기 위한 확산 유도 최적화 프레임워크인 ARTIC3D를 제안합니다. 결과에 따르면 ARTIC3D 출력은 자세하고 애니메이션이 가능하며 폐색이나 잘림에 강합니다. 확산 사전 손실은 DreamFusion[21]에서 제안된 점수 증류 샘플링(SDS) 손실과 유사합니다. 그럼에도 불구하고 실험에서 3D 표면 최적화에 SDS 손실을 순진하게 적용하면 불안정하고 비효율적인 학습으로 이어져 노이즈가 있는 표면이나 모호한 질감과 같은 바람직하지 않은 아티팩트가 생성됩니다. 이 연구에서 우리는 희소한 노이즈 이미지 컬렉션에서 관절형 3D 모양을 학습하기 위한 확산 유도 최적화 프레임워크인 ARTIC3D(ARTiculated Image Collections in 3D)를 제시합니다. 우리는 Hi-LASSIE [38]의 관절형 부품 표면과 골격을 사용하여 명시적인 부품 조작과 애니메이션을 허용합니다. 우리는 3D 최적화를 위해 Stable Diffusion [27]의 2D 확산 모델 사전 확률을 효과적으로 활용할 수 있는 새로운 디코더 기반 누적 점수 샘플링(DASS) 모듈을 제안합니다. 잠재 인코더를 통해 이미지 그래디언트를 역전파하는 기존 작업과 달리, 우리는 DASS에서 디코더 기반 다단계 전략을 제안하는데, 이는 3D 최적화를 위해 더 안정적인 그래디언트를 제공하는 것으로 밝혀졌습니다. 노이즈가 있는 입력 이미지를 처리하기 위해 우리는 가려지거나 잘린 영역에 대해 추론하기 위해 확산 모델을 사용하는 입력 전처리 방식을 제안합니다. 또한 포즈 관절에서 사실적인 애니메이션을 만드는 기술도 제안합니다. 우리는 Pascal-Part [5] 및 LASSIE [39] 데이터 세트에서 ARTIC3D를 분석합니다. 노이즈가 있는 이미지에 대한 견고성을 더 잘 보여주기 위해 동물이 가려지고 잘린 노이즈가 있는 웹 동물 이미지로 LASSIE 동물 데이터 세트 [39]를 확장합니다. 정성적 및 정량적 결과 모두 ARTIC3D가 세부적이고 입력 이미지에 충실하며 부분적 관찰에 견고한 3D 모양과 텍스처를 생성한다는 것을 보여줍니다. 더욱이, 우리의 3D 관절 표현은 신경 체적 표현을 사용하는 기존 확산 유도 방법에서는 실행 가능하지 않은 명시적 포즈 전송과 사실적인 애니메이션을 가능하게 합니다. 그림 1은 ARTIC3D의 샘플 3D 재구성 및 응용 프로그램을 보여줍니다. 이 작업의 주요 기여는 다음과 같습니다. • 우리는 ARTIC3D라는 확산 유도 최적화 프레임워크를 제안합니다. 여기서는 사전 정의된 모양 템플릿이나 카메라 시점 또는 키포인트와 같은 이미지별 주석을 사용하지 않고 희소한 노이즈가 있는 온라인 이미지에서 3D 관절 모양과 텍스처를 재구성합니다. • 입력 전처리, 이미지 대상으로 확산된 잠재 데이터 디코딩, 포즈 탐색, 애니메이션 미세 조정을 포함하여 3D 표면 최적화에 2D 확산 사전 확률을 효율적으로 통합하기 위한 여러 전략을 설계합니다. • 모델 견고성을 평가하기 위해 폐색 또는 절단이 있는 노이즈가 있는 웹 이미지를 수집하고 주석을 달아 확장된 LASSIE 데이터 세트[39]인 E-LASSIE를 소개합니다. 정성적 및 정량적 결과 모두 ARTIC3D 출력이 3D 모양 세부 정보, 텍스처 및 애니메이션 측면에서 기존 기술에 비해 더 높은 충실도를 가지고 있음을 보여줍니다. 2
--- RELATED WORK ---
동물 모양 및 포즈 추정. 동물 모양 추정에 대한 이전 기술은 동물 인형이나 많은 수의 주석이 달린 동물 이미지를 사용하여 학습하는 통계적 신체 모델[46, 45]을 사용했습니다. 다른 일부 작업[35, 34, 36, 37]은 비디오 입력을 사용하여 비디오의 밀집된 대응 정보를 활용하여 관절 모양을 학습합니다. 그러나 이러한
--- METHOD ---
s [39, 33, 38]는 골격 기반 신경 표면 또는 사전 정의된 메시 템플릿을 사용하여 애니메이션 가능한 3D 모양을 생성할 수 있으며, 성공 여부는 주로 훈련 또는 최적화를 위한 대규모 이미지 데이터 세트 또는 수동으로 필터링된 깨끗한 이미지에 달려 있습니다. 더욱이 출력된 3D 모양과 텍스처는 새로운 관점이나 포즈 관절에서 볼 때 일반적으로 비현실적입니다. 반면, 생성 확산 모델 [25, 28, 27]의 최근 성공은 주어진 텍스트 프롬프트에 대해 고품질 이미지를 생성할 수 있음을 보여줍니다. 여러 최근 작업 [21, 15, 18, 23]은 다중 뷰 감독으로 2D 확산을 간단히 사용하여 3D 개체/장면을 생성할 수 있는 가능성을 더욱 잘 보여줍니다. 이 작업에서 우리는 3D 관절 모양을 학습하기 전에 강력한 2D 확산을 활용하여 2D 또는 3D 주석 없이 희소한 노이즈가 있는 온라인 이미지에서 3D 동물을 재구성하고 애니메이션화하는 것을 목표로 합니다. 직관적으로, *Google에서 학생 연구원으로 수행한 작업. 사전 인쇄본. 검토 중. 노이즈가 있는 웹 이미지 관절 모양(입력 및 새로운 뷰) 질감이 있는 출력(입력 및 새로운 뷰) 애니메이션 미세 조정된 애니메이션 그림 1: 노이즈가 있는 웹 이미지에서 관절이 있는 3D 모양 학습. 야생에서 희소하고 노이즈가 있는 이미지에서 관절이 있는 동물 신체의 3D 모양과 질감을 추정하기 위한 확산 유도 최적화 프레임워크인 ARTIC3D를 제안합니다. 결과에 따르면 ARTIC3D 출력은 자세하고 애니메이션이 가능하며 폐색이나 잘림에 강합니다. DreamFusion [21]에서 제안한 점수 증류 샘플링(SDS) 손실과 유사한 확산 사전. 우리의
--- EXPERIMENT ---
s, 그럼에도 불구하고, 우리는 순진하게 3D 표면 최적화에 SDS 손실을 적용하면 불안정하고 비효율적인 학습으로 이어져 노이즈가 많은 표면이나 모호한 질감과 같은 바람직하지 않은 아티팩트를 생성합니다. 이 작업에서 우리는 희소한 노이즈가 많은 이미지 컬렉션에서 관절이 있는 3D 모양을 학습하는 확산 유도 최적화 프레임워크인 ARTIC3D(ARTiculated Image Collections in 3D)를 제시합니다. 우리는 Hi-LASSIE [38]의 관절이 있는 부분 표면과 골격을 사용하여 명시적인 부분 조작과 애니메이션을 허용합니다. 우리는 3D 최적화를 위해 Stable Diffusion [27]의 2D 확산 모델 사전 확률을 효과적으로 활용할 수 있는 새로운 디코더 기반 누적 점수 샘플링(DASS) 모듈을 제안합니다. 잠재 인코더를 통해 이미지 그래디언트를 역전파하는 기존 작업과 달리, 우리는 DASS에서 디코더 기반 다단계 전략을 제안하는데, 이는 3D 최적화를 위해 더 안정적인 그래디언트를 제공하는 것으로 밝혀졌습니다. 노이즈가 있는 입력 이미지를 처리하기 위해 우리는 가려지거나 잘린 영역에 대해 추론하기 위해 확산 모델을 사용하는 입력 전처리 방식을 제안합니다. 또한 포즈 관절에서 사실적인 애니메이션을 만드는 기술도 제안합니다. 우리는 Pascal-Part [5] 및 LASSIE [39] 데이터 세트에서 ARTIC3D를 분석합니다. 노이즈가 있는 이미지에 대한 견고성을 더 잘 보여주기 위해 동물이 가려지고 잘린 노이즈가 있는 웹 동물 이미지로 LASSIE 동물 데이터 세트 [39]를 확장합니다. 정성적 및 정량적 결과 모두 ARTIC3D가 세부적이고 입력 이미지에 충실하며 부분적 관찰에 견고한 3D 모양과 텍스처를 생성한다는 것을 보여줍니다. 더욱이, 우리의 3D 관절 표현은 신경 체적 표현을 사용하는 이전의 확산 유도 방법에서는 실행 가능하지 않은 명시적 포즈 전송과 사실적인 애니메이션을 가능하게 합니다. 그림 1은 ARTIC3D의 샘플 3D 재구성 및 응용 프로그램을 보여줍니다. 이 연구의 주요 기여는 다음과 같습니다. • 우리는 ARTIC3D라는 확산 유도 최적화 프레임워크를 제안합니다. 여기서 우리는 사전 정의된 모양 템플릿이나 카메라 시점 또는 키포인트와 같은 이미지별 주석을 사용하지 않고 희소한 노이즈가 있는 온라인 이미지에서 3D 관절 모양과 질감을 재구성합니다. • 우리는 입력 전처리, 이미지 대상으로 확산된 잠재 객체 디코딩, 포즈 탐색 및 애니메이션 미세 조정을 포함하여 3D 표면 최적화에 2D 확산 사전 확률을 효율적으로 통합하기 위한 여러 전략을 설계합니다. • 우리는 모델 견고성을 평가하기 위해 폐색 또는 잘림이 있는 노이즈가 있는 웹 이미지를 수집하고 주석을 달아 확장된 LASSIE 데이터 세트[39]인 E-LASSIE를 소개합니다. 정성적 및 정량적 결과 모두 ARTIC3D 출력이 3D 모양 세부 정보, 질감 및 애니메이션 측면에서 기존 기술에 비해 더 높은 충실도를 가지고 있음을 보여줍니다. 2 관련 연구 동물 모양 및 포즈 추정. 동물 모양 추정에 대한 이전 기술은 동물 인형이나 많은 수의 주석이 달린 동물 이미지를 사용하여 학습하는 통계적 신체 모델[46, 45]을 사용했습니다. 다른 작업[35, 34, 36, 37]은 비디오 입력을 사용하여 비디오의 밀집된 대응 정보를 활용하여 관절 모양을 학습합니다. 그러나 이러한 방법은 비디오 프레임 간의 광학 흐름 대응에 의존하는데, 이는 우리의 문제 설정에서 사용할 수 없습니다. 다른 기술[12, 11]은 매개 변수 메시 모델을 활용하고 이미지에서 선형 블렌드 스키닝을 학습하여 다양한 동물 범주에 대한 포즈 메시를 얻습니다. 우리 작업과 가장 관련이 있는 것은 LASSIE[39]와 Hi-LASSIE[38]로, 수동으로 주석이 달린 골격 템플릿을 사용하거나 이미지 컬렉션에서 범주별 템플릿을 발견하여 야생에서 희소한 동물 이미지 컬렉션에서 3D 모양과 질감을 복구하는 동일한 문제 설정을 다룹니다. MagicPony[33]는 범주별 이미지 컬렉션에서 동물 인스턴스의 하이브리드 3D 표현을 학습합니다. 그러나 이러한 접근 방식은 신중하게 큐레이팅된 입력 데이터가 필요하고 부분적 폐색, 잘림 또는 노이즈가 있는 이미지 컬렉션을 처리하지 못합니다. 확산 모델의 최근 발전을 활용하여 더 다양한 입력 이미지에 대한 재구성을 지원합니다. 희소 이미지에서 3D 재구성. 여러 최근 연구[30, 42, 41, 32, 24, 2, 43, 3]는 암시적 표현[19]을 사용하여 범주별 방식으로 학습하거나 추론 중에 다중 뷰 일관된 희소 이미지에 대한 액세스를 가정하여 희소 이미지 컬렉션에서 기하학과 모양을 학습했습니다. 그러나 이러한 접근 방식의 대부분은 단단한 물체에서만 강력한 결과를 보여줍니다. Zhang et al.[42]은 희소 이미지 컬렉션에서 신경 표면 표현을 찾는 또 다른 밀접하게 관련된 연구이지만 거친 카메라 초기화가 필요합니다. 부분 기반 메시 모양과 텍스처를 학습함으로써 프레임워크는 카메라 매개변수에 대한 추가 요구 사항 없이 야생의 동물과 같은 관절형 범주를 모델링하고 애니메이션화하는 데 자연스럽게 적합합니다. 3D에 대한 사전 확산. 영어: 확산 모델[27, 28, 44]은 최근 다양한 종류의 컨디셔닝 입력에 의해 안내되는 고해상도 이미지를 생성하는 데 인기를 얻었습니다.확산 모델은 실제 데이터의 분포를 포착하여 DreamFusion[21]에서 처음 설명된 대로 스코어 증류 샘플링(SDS) 손실을 사용하여 3D 생성을 안내하는 스코어 함수로 사용할 수 있습니다.여러 최근 접근 방식[18, 15, 17, 29, 26, 23]은 SDS 손실을 활용하여 텍스트 또는 단일 또는 희소 이미지 컬렉션에서 3D 표현을 생성합니다.이러한 작업 라인에서 영감을 얻어 디코더가 합성한 고품질 이미지를 활용하고 순진한 SDS 손실보다 향상된 성능을 보여주는 새로운 디코더 기반 누적 스코어 샘플링(DASS)을 제안합니다.3 접근 방식 동물 종의 노이즈가 있는 웹 이미지 10~30개가 주어지면 ARTIC3D는 먼저 2D 확산을 통해 이미지를 사전 처리하여 더 깨끗한 실루엣 추정치, 의미적 특징 및 3D 골격 초기화를 얻습니다. 그런 다음 각 인스턴스에 대한 카메라 시점, 포즈 관절, 부분 모양 및 텍스처를 공동으로 최적화합니다. 마지막으로 강체 뼈 변환을 통해 3D 모양을 애니메이션화한 다음 확산 유도 미세 조정을 수행합니다. 3D 출력의 품질을 개선하기 위한 확산 기반 전략을 소개하기 전에 [39, 38]과 유사한 골격 기반 표면 표현과 확산 사전으로 사용하는 안정적 확산 [27]을 간략히 검토합니다. 3. 예비 단계 대부분의 3D 생성 방법은 3D 강체 객체/장면을 나타내기 위해 체적 신경장을 최적화하는 반면, 우리는 관절이 있고 애니메이션이 가능한 3D 모양을 생성하는 것을 목표로 합니다. 명시적 부분 조작과 사실적인 애니메이션을 가능하게 하기 위해 LASSIE [39] 및 Hi-LASSIE [38]와 같은 골격 기반 표면 표현을 채택합니다. 이미지에서 표면 텍스처를 직접 샘플링하는 [39, 38]과 달리 새로운 뷰에서 사실적인 인스턴스 텍스처를 얻기 위해 부분별 텍스처 이미지를 최적화합니다. 3D 골격. 컬렉션에서 사용자가 지정한 참조 이미지가 주어지면 Hi-LASSIE[38]는 DINO-ViT[4] 피처 클러스터의 기하학적 및 의미적 단서를 기반으로 3D 골격을 자동으로 검색합니다. 골격은 3D 관절과 기본 부분 모양 세트를 초기화하여 부분 변환 및 연결에 대한 좋은 제약 조건을 제공합니다. 우리 프레임워크에서 입력 이미지를 확산한 다음 Hi-LASSIE를 기성형 골격 검색 방법으로 적용하여 더 깨끗한 피처 클러스터를 얻습니다. 기존 작업과 공정하게 비교하기 위해 실험에서 [38]과 동일한 참조 이미지를 골격 검색에 사용합니다. 골격 검색에 대한 자세한 내용은 [38]을 참조하십시오. 신경 부분 표면. [38]에 따라 검색된 3D 골격을 사용하여 변형 가능한 신경 표면[42]을 통해 각 골격 뼈에 해당하는 3D 부분을 재구성합니다. 신경 표면은 다층 퍼셉트론 네트워크(MLP)에 의해 매개변수화되어 단위 구의 3D 표면 점을 xyz 변형에 매핑합니다. 구면 표면에서 m개의 균일하게 샘플링된 3D 점 X Є R³×m이 주어지면, X ⇒ F¿(X)로 부분 MLP를 통해 i번째 부분의 3D 모양을 변형할 수 있습니다. 그런 다음, 부분 표면은 각 골격 부분 i의 스케일링 s¿ Є R, 회전 R¿ € R³×³, 이동 ti Є R³에 의해 강체 변환됩니다. 전역 좌표에서 변환된 부분 표면 점 V₂는 다음과 같이 쓸 수 있습니다. V₁ = siRiFi(X) + ti . 자세한 내용은 [38]을 참조하십시오. 입력 전처리(3.3절) 3D 관절 모양 및 텍스처 최적화(3.4절) 애니메이션(3.5절) (x, y, z) Hi-LASSIE 손실 텍스트 재구성 손실 강체 부분 동작(u,v). 부분 MLP 부분 표면 DASS 손실 시간적 일관성 T-DASS 표면 포인트 노이즈가 있는 웹 이미지 및 노이즈가 있는 마스크 DASS 이미지 x 향상된 이미지 및 피처 클러스터 텍스처 맵(r, g, b) 렌더링된 이미지 DASS: 디코더 기반 누적 점수 샘플링(3.2절) &quot;*의 사진&quot; 점수 증류 V₂LSDS 정지 그래디언트 프레임당 대상 데이터 흐름 그래디언트 흐름 인코더 디코더 잠재 z 이미지 대상 x&#39; 이미지 그래디언트 그림 2: ARTIC3D 개요. 동물 종의 희소 웹 이미지가 주어지면 ARTIC3D는 각 인스턴스에 대한 카메라 시점, 관절 포즈, 3D 부분 모양 및 표면 텍스처를 추정합니다. 우리는 1) 입력 전처리, 2) 모양 및 텍스처 최적화, 3) 애니메이션에 적용되는 안정적인 확산에서 이미지 수준 그래디언트를 효율적으로 계산하는 새로운 DASS 모듈을 제안합니다. 안정적인 확산 아키텍처. 안정적 확산(SD)[27]은 텍스트 프롬프트가 주어지면 고품질 이미지를 합성할 수 있는 최첨단 텍스트-이미지 생성 모델입니다.SD는 주로 3가지 구성 요소로 구성됩니다.주어진 이미지 x를 잠재 코드 z로 인코딩하는 이미지 인코더 &amp;, 잠재 코드를 이미지 픽셀로 다시 변환하는 디코더 네트워크 D, 노이즈가 있는 잠재 코드의 노이즈를 반복적으로 제거할 수 있는 U-Net 노이즈 제거기 Еф.우리는 프레임워크에서 SD를 확산 사전으로 사용합니다.3.2 디코더 기반 누적 점수 샘플링(DASS) 3D 모양 학습을 위해 2D 확산 사전을 활용하기 위해 DreamFusion[21]은 무작위 뷰에서 렌더링된 이미지를 증류하고 이미지 수준 그래디언트를 NeRF(Neural Radiance Field) 매개변수로 전파하는 점수 증류 샘플링(SDS) 손실을 제안합니다. 계산 비용을 줄이고 학습 안정성을 개선하기 위해 Latent-NeRF[18] 및 Magic3D[15]와 같은 최근 연구는 SD에서 저해상도 잠복 코드에 대한 증류를 수행하고 SD 이미지 인코더 E를 통해 그래디언트를 역전파합니다.형식적으로 x가 3D 모델에서 렌더링된 이미지이고 z가 SD 이미지 인코더 E에서 나온 잠복 코드를 나타내도록 합니다.각 점수 증류 반복에서 잠복 코드 z는 zt로 표시된 무작위 시간 단계 t로 노이즈가 적용되고 확산 모델의 U-Net 노이즈 제거기 ε로 노이즈가 제거됩니다.이미지 수준 SDS 그래디언트는 다음과 같이 표현할 수 있습니다.VxLSDS = W₁ (€4 (zt; y, t) — €) oz, 여기서 y는 안내 텍스트 임베딩을 나타내고, &amp;는 잠복 코드에 추가된 무작위 노이즈이고, wt는 확산 시간 단계 t에 따라 달라지는 상수 배수입니다. 디노이저 &amp;는 텍스트 가이드와 무조건 모델의 분류기 없는 가이드[8]의 균형을 맞추기 위해 가이드 스케일 wg를 사용합니다. 이 일반적인 SDS 손실은 텍스트에서 NeRF를 생성하는 데 효과적이지만, 프레임워크에 순진하게 적용하면 불안정하고 비효율적인 학습이 이루어진다는 것을 관찰했습니다. 그림 3(b)에서 볼 수 있듯이 인코더를 통해 역전파된 SDS 그래디언트는 종종 매우 노이즈가 많아 3D 모양과 텍스처에 바람직하지 않은 아티팩트를 발생시킵니다. 게다가 그래디언트 역전파를 위해 추가 계산과 메모리 사용이 필요하여 학습 배치 크기가 제한되고 안정성이 감소합니다. 이러한 문제를 완화하기 위해 더 깨끗하고 효율적인 픽셀 그래디언트를 계산하는 대안인 새로운 디코더 기반 누적 점수 샘플링(DASS) 모듈을 제안합니다. 그림 2는 제안된 DASS 모듈을 보여줍니다. 높은 수준에서 입력 이미지 x가 주어지면 디코딩된 출력이 일반적으로 노이즈가 적다는 관찰에 따라 디코더에서 노이즈가 제거된 이미지 x&#39;를 재구성 대상으로 얻습니다. 그림 2에서 보듯이, 렌더링된 이미지를 인코더 Ɛ에 통과시켜 저해상도 잠복 코드를 얻고, 스코어 증류를 통해 잠복 코드를 n단계 동안 업데이트한 다음, 디코더 D를 이미지 대상으로 하여 업데이트된 잠복 코드를 디코딩합니다. 형식적으로, 편미분 Əz/əx를 명시적으로 계산하는 대신, Vz가 n단계에 걸쳐 누적된 잠복 그래디언트인 Vx에 대한 프록시로 x - D(z - Vz)를 사용합니다. 이는 잠복 코드 z를 중심으로 D에 대한 선형 가정을 만드는데, 이는 경험적으로 픽셀 그래디언트를 근사하는 데 효과적이라는 것을 알게 되었습니다. 대상 이미지 x&#39; D(z - Vz)는 업데이트된 입력(섹션 3.3)으로 직접 사용하거나 픽셀 수준 DASS 손실을 계산하는 데 사용할 수 있습니다.=(a) 깨끗한 배경 대 노이즈가 있는 배경 (b) 인코더 대 디코더 이미지 입력(깨끗함) 확산 입력(노이즈 있음) 확산 인코더(SDS) (c) 축적 단계 n (d) 확산 시간 단계 t 디코더(DASS) (e) 유도 가중치 wg n =(DSS) n =(DASS) n =(DASS) t = 0.t = 0.t = 0.W =g W =Wg Wg =그림 3: DASS 방법의 절제 시각화. 예시 입력 이미지(왼쪽 상단)에서 이미지 수준 그래디언트 또는 매개변수 설정을 얻기 위해 다양한 방법을 사용하여 한 번의 최적화 반복 후 업데이트된 이미지를 보여줍니다.(a)는 입력 이미지의 노이즈가 있는 배경이 DASS가 누락된 부분을 환각하도록 하는 것을 보여줍니다. (b) 표준 SDS(인코더를 통한 역전파 그래디언트)와 DASS(디코더 기반) 손실을 비교합니다. (c) 더 깨끗한 디코딩된 출력으로 이어지는 누적 잠재 그래디언트 접근 방식을 정당화합니다. (d) 작은 타임스텝은 대부분 텍스처를 수정하는 반면, 큰 타임스텝은 지오메트리를 더 많이 변경합니다(때로는 신체 부위를 제거하거나 만듭니다). (e) 높은 안내 가중치로 고대비 색상과 약간 불균형한 신체를 보여줍니다(확산 사전은 더 큰 머리와 정면 보기에 편향됨). (b)는 더 나은 시각화를 위해 (a)의 깨끗한 입력을 사용하는 반면 (c), (d), (e)는 노이즈가 있는 입력에서 얻습니다. = Ldass ||(x − D(z – Vz))||² 3D 최적화(섹션 3.4). DASS 모듈은 인코더와 디코더의 순방향 패스를 한 번만 포함하므로 원래 SDS 손실에 비해 학습 중에 메모리 소비가 약 절반입니다. 그림 3의 시각화는 DASS가 한 번의 학습 단계에서 원래 SDS 손실보다 더 깨끗한 이미지를 생성한다는 것(b)과 누적된 그래디언트가 효과적으로 노이즈를 줄이고 누락된 부분을 채울 수 있다는 것(c)을 보여줍니다. 게다가, 배경 픽셀에 랜덤 노이즈를 추가하면 DASS에 의한 모양 완성을 용이하게 할 수 있다는 것을 보여줍니다(a). 또한, 그림 3에서 노이즈 타임스텝(d)과 가이드 가중치(e)와 같은 다른 확산 매개변수에 대한 절제 분석을 수행합니다. 일반적으로 ARTIC3D는 높은 분산으로 인해 입력 이미지에 충실하지 않은 3D 결과가 생성될 수 있으므로 적당한 누적 단계 n = (3, 10)과 낮은 타임스텝 t = (0.2, 0.5)를 선호합니다. 또한, SDS 손실에서 높은 가이드 스케일로 인해 이전 작업에서 흔히 발생하는 과포화 효과가 결과에 영향을 미치지 않도록 낮은 가이드 가중치 wg € (10, 30)를 사용합니다. 3.3 노이즈가 있는 이미지에 대한 입력 전처리 실제 세계 이미지의 동물 신체는 종종 그림 4에서와 같이 노이즈가 있는 질감, 어두운 조명, 폐색 또는 잘림으로 인해 모호한 모양을 갖습니다. 노이즈가 있거나 부분적인 관찰을 더 잘 처리하기 위해 이미지 품질을 향상시키고 누락된 부분을 완성하는 새로운 방법을 제안합니다. 동물 종의 희소 이미지 컬렉션 {I; € RH×W×³} (j = {1, ..., N}이고 N은 일반적으로 10-30 사이)이 주어지면 각 인스턴스에 대해 정확한 실루엣 추정치 {Â; € Rª×W}와 깨끗한 의미적 특징 {K; Є Rhxwxd}를 얻는 것을 목표로 합니다. 그림 2에서와 같이 훈련된 DINO-ViT [4] 네트워크에서 추출한 눈에 띄는 특징을 클러스터링하여 전경 마스크를 대략적으로 추정합니다. 그런 다음 DASS를 적용하여 배경이 마스크된 이미지를 확산시켜 질감이 더 깨끗하고 모양이 완전한 동물 신체를 얻습니다. 형식적으로, 우리는 D(z - Vz)에 의해 업데이트된 이미지 I&#39;를 얻습니다.여기서 z = E(I)입니다.여기서 DASS는 이미지 노이즈 제거 및 인페인팅 모듈 역할을 하며, n개의 잠재 업데이트와 D의 단일 전방 패스를 통해 노이즈가 있는 입력의 고품질 버전을 효과적으로 생성할 수 있습니다.확산 모델의 노이즈 및 노이즈 제거 특성에 따라, 그림 3(a)에서 입력 이미지의 배경 픽셀에 가우시안 노이즈를 수동으로 추가하면 DASS가 대부분 가시 영역을 보존하면서 가려진 부분을 환각하도록 장려한다는 것을 보여줍니다.마지막으로, 우리는 확산된 이미지에 DINO-VIT 특징 추출 및 클러스터링[1]을 다시 적용하여 더 깨끗하고 완전한 마스크와 의미적 특징을 얻습니다.그림 2(왼쪽)는 샘플 노이즈가 있는 입력 이미지와 해당 출력 강화 이미지 및 특징 클러스터를 보여줍니다. 참고로 Farm3D[9]는 3D 훈련을 위해 텍스트에서 동물 이미지를 생성하는 데 SD[27]를 사용하지만, 이러한 이미지에는 종종 불규칙한 모양(예: 다리가 5개인 말)이 포함되어 있습니다.반면에, 우리의 전처리된 이미지는 우리의 목표가 입력 이미지에 사실적이고 충실한 3D 모양과 질감을 재구성하는 것이므로 희소 이미지 최적화 프레임워크에 더 적합합니다.3.4 모양과 질감의 확산 유도 최적화 전처리된 이미지, 실루엣 추정치 및 의미적 특징이 주어지면 카메라 시점, 포즈 관절, 3D 부분 모양 및 질감을 공동으로 최적화합니다.2D 또는 3D 주석을 가정하지 않으므로 Hi-LASSIE[38]를 따르고 입력 이미지에 충실한 3D 모양과 질감을 재구성하기 위해 분석-합성 접근 방식을 채택합니다.즉, 미분 가능한 렌더러[16]를 사용하여 3D 부분을 렌더링하고 이를 2D 이미지, 가상 기준 진실 실루엣 및 DINO-ViT 특징과 비교합니다. 그림 2(위)는 모양과 텍스처 최적화를 보여줍니다.LASSIE 및 Hi-LASSIE 손실.인스턴스 j의 렌더링된 실루엣 M³와 가상 기준 진실 Mi가 주어지면 실루엣 손실 Lil은 다음과 같이 쓸 수 있습니다.Lsil = Σ;||M³ – M³ ||².LASSIE[39]와 Hi-LASSIE[38]는 동일한 동물 종류의 이미지 간 DINO 특징의 2D 대응 관계를 더욱 활용하여 의미적 일관성 손실 Lsem을 정의합니다.Lsem은 3D 표면 포인트와 2D 픽셀 간의 챔퍼 거리로 해석할 수 있으며, 집계된 3D 포인트 특징이 모든 이미지에서 유사한 픽셀 특징에 더 가깝게 투영되도록 합니다.포즈 관절과 부분 모양을 정규화하기 위해 [39, 38]은 부분 회전 손실 Crot, 라플라시안 메시 정규화 Llap 및 표면 법선 손실 Lnorm도 적용합니다. 부분 회전 손실 Crot = Σ;||R³ – ¯||²는 정지 포즈로부터의 각도 오프셋을 제한합니다. 여기서 R³는 인스턴스 j의 부분 회전이고 Ŕ는 공유 정지 포즈의 부분 회전을 나타냅니다. Llap 및 Lnorm은 각 정점을 이웃의 중심으로 당기고 이웃 면이 각각 유사한 법선을 갖도록 하여 부드러운 3D 표면을 촉진합니다. 자세한 내용은 생략하고 독자는 [39, 38]을 참조하세요. 재구성(Lsil, Lsem) 및 정규화(Crot, Llap, Lnorm) 손실이 관절 모양에 일반적이고 효과적이라는 점을 고려하여 ARTIC3D에서 새로운 텍스처 재구성 및 DASS 모듈과 함께 사용합니다. 텍스처 재구성. [39, 38] 모두 입력 RGB에서 텍스처를 직접 샘플링하여 가려진 영역에 비현실적인 텍스처가 생성됩니다. 보다 사실적인 텍스처를 얻기 위해 각 부분에 대한 텍스처 이미지 T¿도 최적화합니다. 정점 색상 C = R³×m은 표면 점 X의 사전 정의된 UV 매핑 S를 통해 샘플링됩니다. 형식적으로, 부분 i의 표면 색상 샘플링은 C₁ = Ti(S(X))로 표현할 수 있습니다. 샘플링된 표면 텍스처는 3D 스켈레톤에 정의된 대칭 평면에 따라 대칭됩니다. 웹 이미지의 동물은 다양한 텍스처를 가질 수 있으므로 텍스처 이미지는 인스턴스별로 최적화됩니다. Llap과 유사하게 추정된 입력 뷰에서 렌더링할 때 표면 텍스처가 입력 이미지에 가깝도록 적용합니다. 텍스처 재구성 손실은 다음과 같이 정의됩니다. Ltext = Σ¡||Û³ © (Î³ – Ĩ³)||² 여기서 Î³는 입력 전처리 후 인스턴스 j의 깨끗한 입력 이미지를 나타내고 M³는 해당 동물 마스크를 나타냅니다. Ĩ는 추정된 3D 모양과 텍스처에서 렌더링된 RGB 이미지입니다. ○는 요소별 곱을 나타냅니다. 재구성 손실은 추정된 전경 실루엣에 의해 마스크 처리되어 표면 텍스처 최적화는 가시적인 비폐쇄 동물 픽셀에 의해서만 영향을 받습니다.3D 재구성 증류.앞서 언급한 손실 외에도 3D 재구성을 증류하여 모양과 텍스처 세부 정보를 늘리는 것을 제안합니다.여기서 DASS를 비평자로 사용하여 3D 재구성이 2D 렌더링에서 얼마나 잘 보이는지 평가하고 이미지 대상에서 픽셀 그래디언트를 계산합니다.이전의 확산 기반 방법[21, 18, 15]과 유사하게 훈련 중에 임의의 관점, 조명 및 배경색으로 3D 표면을 렌더링합니다.또한 희소 이미지 시나리오에서 관절 공간을 밀집시키기 위해 포즈 탐색 계획을 설계합니다. 특히, 두 인스턴스(j₁, j2)의 추정된 뼈 회전(R³¹, R³2)을 무작위로 보간하고, 렌더링을 위해 새로운 포즈 R&#39; = αR³¹ + (1 − a) R³²를 갖는 새로운 인스턴스를 생성합니다. 여기서 a = (0, 1)은 무작위 스칼라입니다. 이와 같이, 확산 사전으로 파트 변형을 더 잘 제한하고 불규칙한 모양이나 파트 간의 단절을 방지할 수 있습니다. 그림 2와 같이 렌더링된 이미지의 잠재 코드를 확산시키고 DASS 모듈에서 픽셀 그래디언트를 얻습니다. 결과 그래디언트는 역전파되어 파트 표면 텍스처, 변형 MLP, 뼈 변환 및 카메라 시점을 업데이트합니다. 실험에서 RGB 그래디언트가 SoftRas [16] 블렌딩 함수를 통해 잘 전파되지 않는다는 것을 관찰했으며, 따라서 [20]에서 제안한 계층 블렌딩 접근 방식으로 수정합니다. 최적화 세부 정보. 전반적인 최적화 목표는 위에서 설명한 대로 모든 손실의 가중 합 L = Σ α11로 표현할 수 있습니다. 여기서 {sil, sem, rot, lap, norm, text, dass}입니다. 공유 및 인스턴스별 모양을 두 단계로 최적화합니다. 즉, 먼저 카메라 시점 및 포즈 매개변수와 함께 공유 부분 MLP를 업데이트합니다. 그런 다음 인스턴스별 부분 MLP를 미세 조정하고 각 인스턴스의 텍스처 이미지를 최적화합니다. 모든 모델 매개변수는 Adam 최적화 도구[10]를 사용하여 업데이트합니다. 이미지를 512×512 해상도로 렌더링하고 부분 텍스처 이미지의 경우 128×로 렌더링합니다. 추가 최적화 세부 정보는 보충 자료에 설명되어 있습니다. =그림 4: E-LASSIE 샘플. 동물 클래스당 15개의 가려지거나 잘린 이미지로 LASSIE[39] 이미지 세트를 확장하고 평가를 위해 2D 키포인트에 주석을 달았습니다. 이러한 노이즈가 많은 이미지는 인스턴스별 3D 모양이 눈에 보이는 부분에 쉽게 과적합되고 나머지는 무시될 수 있기 때문에 희소 이미지 최적화에 큰 과제를 제기합니다.3.5 애니메이션 미세 조정 점진적으로 골격 뼈와 해당 부분 표면을 회전하여 생성된 3D 관절 동물을 쉽게 애니메이션화할 수 있습니다.그러나 강체 부분 변환으로 인해 종종 관절 주변의 모양이나 질감이 분리됩니다.2D에서 렌더링된 애니메이션을 개선하기 위해 관절 모양의 시퀀스에서 프레임별로 DASS를 순진하게 사용할 수 있습니다.그러나 이렇게 하면 프레임 전체에서 색상 깜박임 및 모양 불일치와 같은 아티팩트가 생성될 수 있습니다.이를 해결하기 위해 ARTIC3D 출력을 기반으로 고품질의 시간적으로 일관된 2D 애니메이션을 생성하기 위해 Temporal-DASS(T-DASS)라는 미세 조정 단계를 추가로 제안합니다. 인스턴스 간 단순 보간 또는 모션 리타게팅에서 부분 변환 시퀀스가 주어지면 3D 표면을 비디오 프레임 {Jk € Rª×W×³(k = {1, ..., K})}으로 렌더링하고 SD 인코더 Ɛ를 통해 잠재 코드 {zk Є Rhxwx3}로 인코딩합니다. 그런 다음 잠재 공간에서 애니메이션을 미세 조정하기 위해 재구성 손실 recon 및 시간적 일관성 손실 temp를 설계합니다. DASS와 유사하게 여러 단계에 대해 잠재 SDS 그래디언트 Vzk를 누적하여 재구성 대상 {z}을 얻습니다. z = Zk — Vzk. 그런 다음 재구성 손실은 다음과 같이 쓸 수 있습니다. Crecon = Σt||(Zk — Zk)||². 시간적 일관성을 강제하기 위해 3D 표면 출력을 활용하고 인접 프레임에서 정확한 2D 대응 관계를 계산합니다. 구체적으로, 프레임 zk의 각 잠재 픽셀에 대해 메시 래스터화를 통해 가장 가까운 가시 3D 표면을 찾은 다음, 프레임 Zk−1에서 2D 투영을 백트래킹하여 밀도가 높은 2D 흐름장 Fk Є Rhxwx²를 형성합니다. 직관적으로, 해당 픽셀은 유사한 잠재 코드를 가져야 합니다. 따라서 F½를 사용하여 잠복 코드 zk−1에 대한 시간적 워핑을 수행합니다. 이를 warp(zk−1, Fk)로 표시하고 Ltemp를 다음과 같이 정의합니다. Ltemp = Σ ±2||(zk – warp(zk−1, Fk) ||². 잠복 코드 {k}를 recon 및 Ltemp로 미세 조정합니다. 여기서 {F}는 미리 계산되고 {%}는 각 반복에서 업데이트됩니다. 마지막으로 최적화된 잠복 코드를 SD 디코더 {D(zk)}로 전달하여 간단히 RGB 비디오 프레임을 얻을 수 있습니다. 제안된 Lrecon은 각 프레임에서 더 나은 모양과 질감 세부 정보를 장려하고 temp는 잠복 업데이트를 시간적으로 효과적으로 정규화할 수 있습니다. T-DASS는 잠복 코드를 최적화하고 시간적 일관성을 고려하는데, 이는 각 이미지에서 개별적으로 작동하는 DASS와 다릅니다. 4 실험 K 데이터 세트. [39, 38]에 따라 Pascal-Part에서 ARTIC3D를 평가합니다. [5] 및 LASSIE [39] 이미지. Pascal-Part에서 말, 소, 양의 이미지와 실제 2D 파트 마스크를 사용하여 자동으로 계산된 2D 키포인트를 얻습니다. LASSIE 데이터 세트에는 다른 동물 종(얼룩말, 호랑이, 기린, 코끼리, 캥거루, 펭귄)의 웹 이미지와 2D 키포인트 주석이 포함됩니다. 각 이미지 컬렉션에는 다양한 모양의 서로 다른 인스턴스의 약 30개 이미지가 포함되어 있으며, 이는 수동으로 필터링되어 이미지에서 동물의 몸체가 완전히 보입니다. 보다 실용적인 설정에서 모델 견고성을 평가하기 위해 동물이 가려지거나 잘린 여러 노이즈 이미지로 LASSIE 이미지 세트를 확장합니다. 특히 클래스당 15개의 추가 웹 이미지(CC 라이선스)를 수집하고 평가를 위해 2D 키포인트에 주석을 달았습니다. 우리는 확장된 이미지 세트를 E-LASSIE라고 부르고 그림 4에 몇 가지 예를 보여줍니다.E-LASSIE에 대한 실험의 경우 각 세트의 모든 45개 이미지에서 최적화하고 평가합니다.기준선.우리는 주로 관절이 있는 동물 모양에 대한 희소 이미지 최적화라는 동일한 문제 설정을 다루기 때문에 ARTIC3D를 LASSIE[39] 및 Hi-LASSIE[38]와 비교합니다.참고로, 우리는 또한 A-CSM[11], MagicPony[15], Farm3D[9]와 같은 여러 학습 기반 방법과 결과를 비교합니다.이러한 접근 방식은 대규모 이미지 세트(우리 시나리오에서는 사용할 수 없음)에서 피드포워드 네트워크를 학습하기 때문에 ARTIC3D와 직접 비교할 수 없습니다.관련이 있기는 하지만 3D 표면 재구성에 대한 다른 최근 작업 중 일부는 관절을 처리할 수 없거나[12, 14, 7, 31, 40] 다른 입력이 필요합니다[13, 34, 36]. 더 강력한 기준선으로서, 우리는 Hi-LASSIE+를 구현하고, [27, 15, 18]에서와 같이 표준 SDS 손실을 통합합니다(인코더를 통한 잠재 그래디언트 역전파).Hi-LASSIE [38]에서 모양과 질감을 최적화하는 동안.입력 이미지 Hi-LASSIE Hi-LASSIE+ Artic3D(저희) 그림 5: ARTIC3D와 다른 기준선의 시각적 비교.각 입력 이미지에 대해 입력(위)과 새로운(아래) 뷰의 3D 질감 출력을 보여줍니다.결과는 ARTIC3D가 폐색이나 잘림이 있는 노이즈가 있는 이미지에 더 강력하여 입력 이미지에 자세하고 충실한 3D 모양과 질감을 생성한다는 것을 보여줍니다.평가 지표.데이터 세트에 기준 진실 3D 주석이 부족하다는 점을 고려하여, 우리는 일반적인 관행[45, 11, 39, 38]을 따르며, 키포인트 전송 정확도를 정량적 지표로 사용하여 3D 재구성을 평가합니다. 각 이미지 쌍에 대해 소스 이미지의 주석이 달린 2D 키포인트를 표준 3D 표면에 매핑하고 추정된 카메라, 포즈, 모양을 통해 대상 이미지로 다시 투영한 다음 전송된 키포인트를 대상 주석과 비교합니다.텍스처가 있는 출력의 품질을 추가로 평가하기 위해 밀도 있게 샘플링된 관점에서 3D 출력 렌더링의 CLIP [22] 기능을 계산하고 텍스트 프롬프트와 입력 이미지에 대한 기능 유사도를 계산합니다.3D 모양 생성에 대한 대부분의 기존 기술[21, 23]은 이미지-텍스트 유사도만 평가하는 반면, 출력은 범주 수준 텍스트 설명과 인스턴스별 입력 이미지에 모두 충실해야 하므로 이미지-이미지 유사도도 평가합니다.실험에서 각 동물 종류 &quot;*&quot;에 대해 텍스트 프롬프트 &quot;A photo of *&quot;를 사용합니다.CLIP ViT-B/32 모델을 사용하여 30도의 고정 고도에서 균일하게 샘플링된 36개의 방위각 렌더링에 대한 평균 기능 유사도를 계산합니다. 여기서는 주요 결과를 보여주고 애니메이션 비디오, 사용자 연구, 보다 자세한 절제 연구를 포함한 보충 자료에서 보다 정량적이고 정성적인 비교를 제공합니다.정성적 결과.그림 1은 ARTIC3D의 일부 샘플 출력을 보여줍니다.그림 5에서는 E-LASSIE 이미지에서 Hi-LASSIE, Hi-LASSIE+ 및 ARTIC3D의 시각적 결과를 비교합니다.Hi-LASSIE와 Hi-LASSIE+는 모두 보이지 않는 부분에 불규칙한 포즈와 모양을 생성합니다.표면 텍스처와 관련하여 Hi-LASSIE는 입력 뷰에서 충실한 텍스처를 재구성하지만 입력 이미지에서 정점 색상을 순진하게 샘플링하기 때문에 새로운 뷰에서는 노이즈가 있습니다.Hi-LASSIE+의 출력 텍스처는 일반적으로 SDS 손실 덕분에 노이즈가 적습니다.이에 비해 ARTIC3D는 폐색이나 잘림이 있어도 카메라 시점, 포즈, 모양 및 텍스처를 정확하게 추정합니다.ARTIC3D 출력은 자세하고 입력 이미지에 충실하며 입력 뷰와 새로운 뷰 모두에서 사실적입니다. 양적 비교. 표 1에서 키포인트 전송 정확도(PCK)의 비교를 보여줍니다. LASSIE와 E-LASSIE 이미지 세트 모두에서 Hi-LASSIE+는 SDS 손실을 순진하게 적용하여 Hi-LASSIE[38]보다 약간의 PCK 이득을 생성합니다. 반면 ARTIC3D는 특히 노이즈가 있는 E-LASSIE 이미지에서 기준선보다 일관되게 더 높은 PCK를 달성합니다. 결과는 확산 유도 전략이 더 자세하고 정확하며 견고한 3D 모양을 효과적으로 학습할 수 있음을 보여줍니다. 표 2의 Pascal-Part 결과는 ARTIC3D가 최첨단 최적화 기반 방법에 비해 유리한 성능을 보이며 학습 기반 접근 방식과 유사함을 보여줍니다. 표 3에서는 E-LASSIE 이미지에 대한 CLIP 유사도 비교를 보여줍니다. 이는 텍스처 출력이 대부분 동물 클래스에 대해 입력 이미지(인스턴스 수준)와 텍스트 프롬프트(클래스 수준)에 더 충실함을 나타냅니다. 애니메이션 및 텍스처 전송. 그림 6에서 T-DASS를 통한 미세 조정 단계 전과 후의 애니메이션을 비교합니다. 골격 기반 표현은 강체 부분 변환을 통해 쉬운 애니메이션을 허용하지만 출력 부분 모양과 질감은 종종 분리되어 있고 관절 주변이 불규칙합니다. 결과에 따르면 T-DASS는 모양과 질감이 세부적으로 표현되고 프레임 간에 시간적으로 일관된 고품질 애니메이션을 효과적으로 생성할 수 있습니다. 애니메이션 외에도 3D 부분 표면은 질감 전송 및 포즈 전송과 같은 편리한 제어 가능한 합성도 가능하게 합니다. 미세 조정된 강체 변환 표 1: LASSIE [39] 및 E-LASSIE 이미지 세트의 키포인트 전송 평가. 모든 이미지 쌍에서 평균 PCK@0.(†)를 보고합니다. ARTIC3D는 모든 동물 클래스에서 최적화 기반 기존 기술에 비해 유리하게 수행됩니다. E-LASSIE에서 더 큰 성능 격차는 ARTIC3D가 노이즈가 있는 이미지에 강하다는 것을 보여줍니다. 방법 이미지 집합 코끼리 기린 캥거루 펭귄 호랑이 얼룩말 LASSIE [39] Hi-LASSIE [38] LASSIE 40.60.31.LASSIE 42.61.35.40.6 62.4 63.44.4 63.1 64.Hi-LASSIE+ LASSIE 43.61.35.44.6 63.4 64.ARTIC3D LASSIE 44.61.36.45.64.0 64.Hi-LASSIE [38] E-LASSIE 37.54.31.41.Hi-LASSIE+ E-LASSIE ARTIC3D 38.54.32.41.57.60.57.7 61.E-LASSIE 39.58.35.43.8 59.3 63.표 2: Pascal-Part [6]의 주요 지점 전송 결과. 모든 쌍의 이미지에 대한 평균 PCK@0.1(†)을 보고합니다. *는 대규모 이미지 세트에서 학습된 학습 기반 모델을 나타냅니다. 방법 UMR* [14] A-CSM* [11] Magic Pony [33] Farm3D* [9] LASSIE [39] Hi-LASSIE [38] Hi-LASSIE+ ARTIC3D 말 소 양 24.32.9 26.3 28.42.9 42.5 26.42.5 40.2 32.42.2 37.5 27.43.7 42.1 29.43.3 42.3 30.44.4 43.0 31.표 3: E-LASSIE 이미지에 대한 CLIP 유사도(↑) 평가. 각 동물 종류에 대해 코사인 유사도 s1/s2를 계산합니다. 여기서 s1은 이미지 간 유사도(마스크된 입력 이미지에 대한)이고 s2는 이미지 간 유사도(텍스트 프롬프트에 대한)입니다. 방법 코끼리 기린 캥거루 펭귄 호랑이 얼룩말 Hi-LASSIE [38] 80.0/ 26.85.2/29.77.4/25.Hi-LASSIE+ ARTIC3D 79.0/27.82.6/28.84.7/30.78.3/29.85.3/30.81.6/29.85.8/30.82.9/32.85.5 / 33.79.7/25.75.3/25.80.0/27.83.8/27.81.9/27.84.1/29.Target 소스 전송 전송 그림 6: 애니메이션 미세 조정. 비교 그림 7: 텍스처 전송. 영어: 우리의 부분 표면은 원래 애니메이션 출력을 강체 변환을 통해 표현하여 포즈나 텍스처 형성(위)과 같은 응용 프로그램을 가능하게 하고, 애니메이션 미세 조정(bot-전송. 소스 모양과 대상 텍스처가 주어지면, 우리는 특히 동물 관절 주변의 인스턴스와 동물 종(오른쪽) 간에 모양과 텍스처 세부 정보를 효과적으로 개선합니다. 텍스처 전송의 몇 가지 예가 그림 7에 나와 있습니다. 이러한 응용 프로그램의 비디오 결과와 함께 더 많은 시각적 결과는 보충 자료에 나와 있습니다. 한계. ARTIC3D는 Hi-LASSIE[38]에서 발견한 3D 골격에 의존하여 부분을 초기화합니다. 대부분의 이미지에서 동물의 몸이 가려지거나 잘린 경우 골격 초기화가 부정확해지는 경향이 있어 ARTIC3D가 사실적인 부분을 형성하는 능력이 제한됩니다. 우리의 입력 전처리 방법은 이 문제를 어느 정도 완화할 수 있지만, 모호한 골격 구성을 가진 솜털 같은 동물(예: 양)은 여전히 골격 발견에 어려움을 줄 수 있습니다. 또한, 확산 모델의 정면 편향은 때때로 여러 얼굴과 같은 비현실적인 질감을 초래하여 재구성 품질에도 영향을 미칩니다. 실패 사례는 보충 자료를 참조하세요. 5
--- CONCLUSION ---
우리는 희소하고 노이즈가 많은 웹 이미지에서 3D 관절 모양과 질감을 재구성하기 위한 확산 유도 프레임워크인 ARTIC3D를 제안합니다. 구체적으로, 우리는 3D 표면 최적화를 위한 스코어 증류에서 픽셀 그래디언트를 효율적으로 계산하고 노이즈가 많은 이미지의 입력 전처리, 모양 및 질감 최적화, 애니메이션 미세 조정에 사용하기 위한 새로운 DASS 모듈을 설계합니다. 기존 데이터 세트와 새로 도입된 노이즈가 많은 웹 이미지에 대한 결과는 ARTIC3D가 기존 기술에 비해 더 견고하고 자세하며 사실적인 재구성을 생성한다는 것을 보여줍니다. 참고문헌 [1] Shir Amir, Yossi Gandelsman, Shai Bagon 및 Tali Dekel. Deep ViT features as dense visual descriptors. arXiv 사전 인쇄본 arXiv:2112.05814, 2021.[2] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Barron, Ce Liu 및 Hendrik Lensch. NeRD: 이미지 컬렉션의 신경 반사 분해. CVPR, 12684-12694페이지, 2021.[3] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T Barron, Hendrik Lensch, Varun Jampani. SAMURAI: 제약 없는 실제 세계 임의 이미지 컬렉션의 모양과 재료. NeurIPS, 2022.[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin. 자기 감독 비전 변환기의 새로운 속성. ICCV, 9650-9660페이지, 2021. 3,[5] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, Alan Yuille. 감지할 수 있는 것 감지: 전체론적 모델과 신체 부위를 사용하여 객체 감지 및 표현. CVPR, 1971-1978페이지, 2014. 2,[6] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, Andrew Zisserman. PASCAL 시각적 객체 클래스(VOC) 챌린지. IJCV, 88(2):303–338, 2010.[7] Shubham Goel, Angjoo Kanazawa, Jitendra Malik. 주요 포인트가 없는 모양과 관점. ECCV, 88-104페이지, 2020.[8] Jonathan Ho 및 Tim Salimans. 분류자 없는 확산 안내. arXiv 사전 인쇄본 arXiv:2207.12598, 2022.[9] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht 및 Andrea Vedaldi. Farm3d: 2D 확산을 증류하여 관절이 있는 3D 동물 학습. arXiv 사전 인쇄본 arXiv:2304.10535, 2023. 5, 7,[10] Diederik P Kingma 및 Jimmy Ba. Adam: 확률적 최적화 방법. arXiv 사전 인쇄본 arXiv:1412.6980, 2014.[11] Nilesh Kulkarni, Abhinav Gupta, David F Fouhey 및 Shubham Tulsiani. 관절 인식 표준 표면 매핑. CVPR, 452-461페이지, 2020. 3, 7, 8,[12] Nilesh Kulkarni, Abhinav Gupta 및 Shubham Tulsiani. 기하학적 사이클 일관성을 통한 표준 표면 매핑. ICCV, 2202-2211페이지, 2019. 3,[13] Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang 및 Jan Kautz. 야생에서 일관된 메시 재구성을 위한 온라인 적용. NeurIPS, 33:15009-15019, 2020.[14] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani, Ming-Hsuan Yang 및 Jan Kautz. 의미적 일관성을 통한 자체 감독 단일 보기 3D 재구성. ECCV, 677-693페이지, 2020. 7,[15] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin. Magic3d: 고해상도 텍스트-3D 콘텐츠 생성. arXiv 사전 인쇄본 arXiv:2211.10440, 2022. 1, 3, 4, 6,[16] Shichen Liu, Tianye Li, Weikai Chen, Hao Li. 소프트 래스터라이저: 이미지 기반 3D 추론을 위한 미분 가능 렌더러. CVPR, 7708-7717페이지, 2019.[17] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi. Realfusion: 단일 이미지에서 모든 객체의 360도 재구성. arXiv 사전 인쇄본 arXiv:2302.10663, 2023.[18] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or. 3D 모양 및 텍스처의 모양 가이드 생성을 위한 Latent-nerf. arXiv 사전 인쇄본 arXiv:2211.07600, 2022. 1, 3, 4, 6,[19] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, Ren Ng. NeRF: 뷰 합성을 위한 신경 광도 필드로 장면 표현. ECCV, 405-421페이지, 2020.[20] Tom Monnier, Matthew Fisher, Alexei A Efros, Mathieu Aubry. 이웃과 공유하세요: 교차 인스턴스 일관성을 통한 단일 뷰 재구성. ECCV, 285-303페이지, 2022.[21] Ben Poole, Ajay Jain, Jonathan T Barron, Ben Mildenhall. Dreamfusion: 2d 확산을 사용한 텍스트-3d. arXiv 사전 인쇄본 arXiv:2209.14988, 2022. 1, 2, 3, 4, 6,[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. ICML, 8748-8763페이지, 2021.[23] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: 주제 중심 텍스트-3D 생성. arXiv 사전 인쇄본 arXiv:2303.13508, 2023. 1, 3,[24] Amit Raj, Michael Zollhofer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, Stephen Lombardi. 픽셀 정렬된 체적 아바타. CVPR, 11733-11742페이지, 2021.[25] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. 제로샷 텍스트-이미지 생성. ICML, 8821-8831페이지, 2021.[26] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or. 텍스처: 3D 모양의 텍스트 가이드 텍스처링. arXiv 사전 인쇄본 arXiv:2302.01721, 2023.[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR, 10684-10695페이지, 2022. 1, 2, 3, 4, 5,[28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적인 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. NeurIPS, 35:36479-36494, 2022. 1,[29] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. 텍스트-4D 동적 장면 생성. arXiv 사전 인쇄본 arXiv:2301.11280, 2023.[30] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, Federico Tombari. Sparf: 희소하고 노이즈가 많은 포즈의 신경 광도장. arXiv 사전 인쇄본 arXiv:2211.11738, 2022.[31] Shubham Tulsiani, Nilesh Kulkarni, Abhinav Gupta. 주석이 없는 이미지 컬렉션의 암묵적 메시 재구성. arXiv 사전 인쇄본 arXiv:2007.08504, 2020.[32] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser. Ibrnet: 다중 뷰 이미지 기반 렌더링을 학습합니다. CVPR, 페이지 4690–4699, 2021.[33] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht 및 Andrea Vedaldi. Magicpony: 야생에서 관절로 연결된 3D 동물을 학습합니다. arXiv 사전 인쇄 arXiv:2211.12497, 2022. 1, 3,[34] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva Ramanan, William T Freeman 및 Ce Liu. LASR: 단안 비디오를 통해 관절 모양 재구성을 학습합니다. CVPR, 페이지 15980–15989, 2021. 2,[35] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Ce Liu 및 Deva Ramanan. VISER: 관절로 연결된 3D 모양 재구성을 위한 비디오별 표면 임베딩입니다. NeurIPS, 34, 2021.[36] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, Hanbyul Joo. BANMO: 다양한 캐주얼 비디오에서 애니메이션 가능한 3D 신경 모델을 구축합니다. arXiv 사전 인쇄 arXiv:2112.12761, 2021. 2,[37] Gengshan Yang, Chaoyang Wang, N Dinesh Reddy 및 Deva Ramanan. 비디오에서 애니메이션 가능한 카테고리를 재구성합니다. arXiv 사전 인쇄본 arXiv:2305.06351, 2023.[38] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, Varun Jampani. Hi-lassie: 희소 이미지 앙상블에서 고충실도 관절 모양 및 골격 발견. arXiv 사전 인쇄본 arXiv:2212.11042, 2022. 1, 2, 3, 6, 7, 8,[39] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, Varun Jampani. Lassie: 3D 파트 발견을 통해 희소 이미지 앙상블에서 관절 모양 학습. arXiv 사전 인쇄본 arXiv:2207.03434, 2022. 1, 2, 3, 6, 7, 8,[40] Yufei Ye, Shubham Tulsiani, Abhinav Gupta. 야생에서의 선반 지도 메시 예측. CVPR, 8843-8852페이지, 2021.[41] Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa. pixelnerf: 하나 또는 몇 개의 이미지에서 얻은 신경 광도장. CVPR, 4578-4587페이지, 2021.[42] Jason Zhang, Gengshan Yang, Shubham Tulsiani, Deva Ramanan. NeRS: 야생에서의 희소 뷰 3D 재구성을 위한 신경 반사 표면. NeurIPS, 34, 2021.[43] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, Noah Snavely. PhySG: 물리 기반 소재 편집 및 재조명을 위한 구면 가우시안을 사용한 역 렌더링. CVPR, 5453–5462페이지, 2021.[44] Lvmin Zhang 및 Maneesh Agrawala. 텍스트-이미지 확산 모델에 조건부 제어 추가. arXiv 사전 인쇄본 arXiv:2302.05543, 2023.[45] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, Michael J Black. 3D Safari: &quot;야생&quot; 이미지에서 얼룩말 포즈, 모양 및 질감을 추정하는 방법 학습. ICCV, 5359-5368페이지, 2019. 2,[46] Silvia Zuffi, Angjoo Kanazawa, David W Jacobs 및 Michael J Black. 3D 동물원: 동물의 3D 모양 및 포즈 모델링. CVPR, 6365-6373페이지, 2017.
