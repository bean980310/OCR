--- ABSTRACT ---
- 오디오-텍스트 프롬프트를 기반으로 하는 생성 음성 모델의 최근 발전으로 고품질 제로샷 텍스트-음성과 같은 놀라운 혁신이 가능해졌습니다. 그러나 기존 모델은 여전히 입력 음성을 변환하고 불리한 음향 조건에서 캡처된 오디오를 처리하는 다양한 오디오-텍스트 음성 생성 작업을 처리하는 데 한계가 있습니다. 이 논문에서는 제로샷 TTS와 다양한 음성 변환 작업을 수행할 수 있는 다재다능한 음성 생성 모델인 SpeechX를 소개하며, 깨끗한 신호와 노이즈가 있는 신호를 모두 처리합니다. SpeechX는 신경 코덱 언어 모델링과 작업 종속 프롬프트를 사용하는 멀티태스크 학습을 결합하여 통합되고 확장 가능한 모델링을 가능하게 하고 음성 향상 및 변환 작업에서 텍스트 입력을 활용하는 일관된 방법을 제공합니다. 실험 결과에 따르면 제로샷 TTS, 노이즈 억제, 대상 화자 추출, 음성 제거 및 배경 노이즈가 있거나 없는 음성 편집을 포함한 다양한 작업에서 SpeechX의 효능이 나타나 작업 전반에 걸쳐 특수 모델과 동등하거나 더 우수한 성능을 달성합니다. 데모 샘플은 https://aka.ms/speechx를 참조하세요. 색인 용어-음성 생성, 오디오-텍스트 입력, 멀티태스크 학습, 제로샷 텍스트-음성 변환, 잡음 제거, 대상 화자 추출, 음성 편집, 음성 제거 I.
--- INTRODUCTION ---
생성 모델 기술은 텍스트 [1], [2], 비전 [3] 및 오디오 [4]를 포함하는 다양한 머신 러닝 애플리케이션에서 빠르고 혁신적인 발전을 거쳤습니다. 이러한 발전은 산업과 사회 전반에 상당한 영향을 미쳤습니다. 특히, 다중 모달 입력을 사용하는 생성 모델이 놀라운 혁신으로 등장했습니다 [5]–[10]. 음성 영역에서 오디오-텍스트 입력을 활용하는 두드러진 음성 생성 작업 중 하나는 제로샷 텍스트-음성(TTS)입니다. 제로샷 TTS는 해당 사람의 짧은 오디오 샘플만을 사용하여 주어진 텍스트를 원하는 화자의 음성 특성과 말하기 스타일을 가진 음성으로 변환하는 것을 포함합니다. 제로샷 TTS의 초기 연구에서는 고정 차원 화자 임베딩을 사용했습니다 [11]–[14]. 이 접근 방식은 TTS에만 사용이 제한되었고 화자 복제 기능을 적절히 지원하지 못했습니다. 대조적으로, 최근의 접근 방식은 마스크 음성 예측[15]이나 신경 코덱 언어 모델링[16]–[19]과 같은 보다 일반적인 공식을 채택했습니다. 이러한 새로운 접근 방식은 고정 차원 표현으로 압축하지 않고 대상 화자의 오디오를 직접 활용합니다. 결과적으로 이러한 X. Wang은 해당 저자입니다(이메일: xiaofei.wang@microsoft.com). X. Wang, M. Thakker, Z. Chen, N. Kanda, SE Eskimez, M. Tang, J. Li 및 T. Yoshioka는 미국 워싱턴주 레드먼드 98052에 있는 Microsoft Corporation에 있습니다. S. Chen 및 S. Liu는 중국 베이징에 있는 Microsoft Research Asia에 있습니다. 모델은 놀라운 제로샷 TTS 성능을 달성했을 뿐만 아니라 음성 변환[15], [18] 및 음성 편집[15]을 포함한 추가 기능도 보여주었습니다. 이 향상된 유연성은 음성 생성 모델에서 새로운 가능성을 열어줄 엄청난 가능성을 가지고 있습니다. 그러나 인상적인 성과에도 불구하고 이러한 최근 생성 모델은 여전히 특정 한계를 가지고 있으며, 특히 입력 음성을 변환하는 다양한 오디오-텍스트 기반 음성 생성 작업을 처리할 때 그렇습니다. 예를 들어, 기존 음성 편집 모델[20], [21]은 깨끗한 신호만 처리하는 데 제한되어 있어 배경 소리를 보존하면서 말한 내용을 수정할 수 있는 기능이 없습니다. 또한, [15]에서 논의한 모델은 잡음 제거를 수행하기 위해 잡음이 있는 신호를 깨끗한 음성 세그먼트로 둘러싸야 하므로 실제 응용 프로그램에 상당한 제약이 있습니다. 깨끗하지 않은 음성을 변환하는 맥락에서 특히 유용한 또 다른 작업은 대상 화자 추출[22]–[24]입니다. 대상 화자 추출에는 여러 화자가 포함된 음성 믹스에서 원하는 화자의 음성을 추출하는 것이 포함됩니다. 원하는 화자는 해당 개인의 짧은 음성 녹음을 사용하여 지정할 수 있습니다. [25]에서 논의한 대로 잠재적 중요성에도 불구하고 이 작업은 기존 생성 음성 모델에서 다루지 않습니다. 잡음 제거 및 대상 화자 추출과 같은 음성 향상 작업에 대한 기존 접근 방식은 충실한 신호 복구를 위해 회귀 모델에 의존해 왔다는 점이 주목할 만합니다. 그러나 이러한 이전 방법은 일반적으로 각 작업에 대해 별도의 전문가 모델이 필요했으며, 음향 교란의 잠재적 다양성을 고려할 때 이상적이지 않습니다[26]. 또한 특정 음성 향상 작업에만 초점을 맞춘 제한된 연구를 제외하고 참조 전사본을 활용하여 이해할 수 있는 음성을 생성하는 포괄적인 오디오텍스트 기반 음성 향상 모델이 부족했습니다[27], [28]. 앞서 언급한 고려 사항과 다른 도메인의 성공적인 선례를 감안할 때 생성 및 변환 기능을 통합하는 오디오-텍스트 기반 생성 음성 모델을 만드는 것은 매우 중요한 연구적 중요성을 갖습니다. 이러한 모델은 다양한 음성 생성 작업을 처리할 수 있는 포괄적인 기능을 가져야 합니다. 이러한 모델은 다음과 같은 주요 속성을 갖추어야 한다고 제안합니다. 다재다능성: 다른 머신 러닝 도메인에서 개발된 통합 또는 기초 모델과 유사하게 통합 오디오-텍스트 기반 생성 음성 모델은 오디오 및 텍스트 입력에서 음성을 생성하는 광범위한 작업을 처리해야 합니다. 이러한 작업에는 제로샷 TTS뿐만 아니라 다양한 형태가 포함되어야 합니다.SpeechX 생성된 오디오 신경 코덱 언어 모델 음소 변환 작업 기반 프롬프트 오디오 코덱 인코더 입력 텍스트 입력 오디오 작업 입력 텍스트 입력 오디오 오디오 코덱 디코더 잡음 억제 필사(선택 사항) 잡음이 있는 음성 출력 오디오 깨끗한 음성 음성 제거 필사(선택 사항) 잡음이 있는 음성 잡음 대상 화자 추출 제로숏 TTS 필사(선택 사항) 합성용 텍스트 음성 혼합, 등록 음성 등록 음성 깨끗한 음성 편집 잡음이 있는 음성 편집 편집된 필사 편집된 필사 깨끗한 음성 대상 화자의 깨끗한 음성 대상 화자를 모방한 합성된 음성 편집된 음성 잡음이 있는 음성 원래 배경 잡음이 있는 편집된 음성그림 1. SpeechX 개요. SpeechX는 텍스트 및 음향 토큰 스트림을 조건으로 하는 신경 코덱 언어 모델을 사용하여 잡음 억제, 음성 제거, 대상 화자 추출, 제로샷 TTS, 깨끗한 음성 편집 및 잡음이 있는 음성 편집을 포함한 여러 오디오-텍스트 기반 음성 생성 작업을 처리합니다. 일부 작업의 경우 텍스트 입력은 선택 사항입니다. • 음성 향상 및 음성 편집을 포함한 음성 변환의 몇 가지 예를 들면, • 견고성: 통합 모델은 음향적으로 어려운 환경에 적용될 가능성이 높기 때문에 다양한 음향 왜곡에 대한 견고성을 보여주는 것이 필수적입니다. 신뢰할 수 있는 성능을 보장함으로써 이러한 모델은 배경음이 널리 퍼진 실제 시나리오에서 매우 유용하다고 간주될 수 있습니다. 확장성: 통합 모델은 유연한 아키텍처를 채택하여 작업 지원을 원활하게 확장할 수 있어야 합니다. 이를 달성하기 위한 한 가지 접근 방식은 입력 토큰이나 추가 모듈과 같은 추가 요소를 수용하는 것입니다. 이러한 유연성은 모델이 향후 음성 생성 작업에 효율적으로 적응할 수 있도록 합니다. 이 목표를 추구하기 위해 이 논문에서는 제로샷 TTS, 선택적 대본 입력을 사용한 노이즈 억제, 음성 제거, 선택적 대본 입력을 사용한 대상 화자 추출, 조용하고 시끄러운 음향 환경 모두에 대한 음성 편집(그림 1)을 포함한 여러 작업을 수행할 수 있는 다재다능한 음성 생성 모델을 소개합니다. 제안된 모델을 SpeechX¹라고 합니다. VALL-E와 마찬가지로 SpeechX는 텍스트 및 음향 입력을 기반으로 신경 코덱 모델 코드 또는 음향 토큰을 생성하는 언어 모델링 접근 방식을 채택합니다. 다양한 작업을 처리할 수 있도록 멀티태스크 학습 설정에 추가 토큰을 통합합니다. 여기서 토큰은 실행할 작업을 집합적으로 지정합니다. LibriLight[29]의 60,000시간 음성 데이터를 학습 세트로 사용한 실험 결과는 SpeechX의 효능을 보여주며, 앞서 언급한 모든 작업에서 전문가 모델과 동등하거나 더 우수한 성능을 보여줍니다. 특히 SpeechX는 음성 편집 중 배경음을 보존하고 노이즈 억제 및 대상 화자 추출을 위해 참조 전사를 활용하는 것과 같은 새롭거나 확장된 기능도 보여줍니다. 음성 향상 작업의 관점에서 SpeechX는 간단하고 효과적인 1X는 변환을 의미하며, 모델이 제로샷 TTS 외에도 다양한 음성 변환 작업을 수행한다는 것을 강조합니다. 제로샷 TTS와 같은 비음성 향상 작업을 통합하여 이해도를 향상시키기 위한 참조 전사를 활용하는 프레임워크입니다. 제안된 SpeechX 모델의 기능을 보여주는 오디오 샘플은 https://aka.ms/speechx에서 제공됩니다. II.
--- RELATED WORK ---
A. 자기 회귀 생성 모델 자기 회귀 변환기(decoderonly Transformer라고도 함)를 사용하는 언어 모델링 접근 방식을 기반으로 하는 생성 모델은 다양한 응용 분야에서 상당한 성공을 거두었습니다. 이러한 모델의 주목할 만한 예로는 GPT 시리즈[1], [2] 및 DALL-E[30]가 있습니다. 자기 회귀 접근 방식은 오디오 및 음성 영역으로도 확장되었습니다. AudioLM[4] 및 MusicLM[10]은 각각 고유한 시간 척도와 의미적 세분성을 가진 여러 유형의 토큰을 활용하여 계층적 토큰 생성을 허용하는 선구적인 노력입니다. 거친 토큰과 세분된 토큰을 모두 포함하는 이 계층적 구조는 미묘한 세부 사항과 장기적인 규칙성을 모두 갖춘 사운드의 합성을 가능하게 합니다. 제로샷 TTS의 경우 VALL-E[16] 및 SPEAR-TTS[17]는 텍스트(의미적) 및 음향 토큰을 단일 데이터 스트림으로 표현하여 자기 회귀 변환기를 사용합니다. 이 접근 방식을 사용하면 모델이 제로샷 화자 적응을 수행하여 특정 사람의 음성을 모방하는 TTS 음성을 생성할 수 있습니다. 이러한 모델은 3초만큼 짧은 음성 클립에서 제로샷 TTS를 수행할 수 있음이 입증되었습니다. 이러한 자기 회귀 음성 생성 모델의 주목할 만한 장점은 별도의 지속 시간 모델이 필요 없이 TTS를 수행할 수 있다는 것입니다. 이 간소화된 아키텍처는 학습 프로세스를 간소화하고 다양한 음성 생성 작업을 포함하는 데 필요한 유연성을 높일 수 있습니다. 이러한 이유로 자기 회귀 Transformers를 사용하여 SpeechX 모델을 구축하기로 했습니다.B. 다중 작업 생성 음성 모델 논문 최근 여러 논문에서 제로샷 TTS 및 여러 관련 작업을 지원하는 오디오-텍스트 기반 음성 생성 모델을 개발하려는 노력을 보고했습니다. 이러한 작업에는 음성 또는 스타일 변환(Make-A-Voice [18], NaturalSpeech2 [31] 및 Voicebox [15]), 음성 편집(Mega-TTS [21] 및 Voicebox) 및 잡음 제거(NaturalSpeech2 및 Voicebox)가 포함됩니다. Voicebox는 마스크된 음성 예측 원리를 통해 다양한 작업을 용이하게 함으로써 주목할 만한 발전을 보여주었습니다. 그럼에도 불구하고, 그 기능은 여전히 깨끗한 음성 생성에만 국한되어 있어 잡음이 있는 음성을 효과적으로 처리하거나 잡음 억제 및 대상 화자 추출과 같은 기존 오디오 향상 작업을 포괄하지 못합니다. 이 연구에서는 깨끗한 음성과 잡음이 있는 음성을 모두 처리하고 생성 및 변환 작업을 통합합니다. 이를 달성하기 위해 작업 종속 프롬프트로 다중 작업 학습을 수행하여 VALL-E를 확장합니다. SpeechX라고 부르는 결과 모델은 다양한 음성 처리 작업에서 다재다능함을 보여줍니다. 이 모델은 제로샷 TTS 및 음성 편집과 같은 음성 생성 작업에서 탁월할 뿐만 아니라 잡음 억제 및 대상 화자 추출과 같은 향상 작업에서도 효과적으로 수행합니다. 또한 배경 소음을 유지하면서 말한 내용을 편집하거나 향상 작업을 위해 필사본을 효과적으로 활용하는 것과 같은 새로운 기능을 실현합니다. A. 개요 III.
--- METHOD ---
영어: s는 일반적으로 각 작업에 대해 별도의 전문가 모델을 필요로 하는데, 음향 교란의 잠재적 다양성을 고려할 때 이상적이지 않습니다[26]. 또한 특정 음성 향상 작업에만 초점을 맞춘 제한된 연구를 제외하고 참조 전사를 활용하여 이해할 수 있는 음성을 생성하는 포괄적인 오디오텍스트 기반 음성 향상 모델이 부족했습니다[27], [28]. 앞서 언급한 고려 사항과 다른 도메인의 성공적인 선례를 감안할 때 생성 및 변환 기능을 통합하는 오디오-텍스트 기반 생성 음성 모델을 만드는 것은 결정적인 연구 중요성을 가정합니다. 이러한 모델은 다양한 음성 생성 작업을 처리할 수 있는 포괄적인 기능을 가져야 합니다. 이러한 모델은 다음과 같은 주요 속성을 갖추어야 한다고 제안합니다. 다재다능함: 다른 머신 러닝 도메인에서 개발된 통합 또는 기초 모델과 유사하게 통합 오디오-텍스트 기반 생성 음성 모델은 오디오 및 텍스트 입력에서 음성을 생성하는 광범위한 작업을 처리해야 합니다. 이러한 작업에는 제로샷 TTS뿐만 아니라 다양한 형태가 포함되어야 합니다.SpeechX 생성된 오디오 신경 코덱 언어 모델 음소 변환 작업 기반 프롬프트 오디오 코덱 인코더 입력 텍스트 입력 오디오 작업 입력 텍스트 입력 오디오 오디오 코덱 디코더 잡음 억제 필사(선택 사항) 잡음이 있는 음성 출력 오디오 깨끗한 음성 음성 제거 필사(선택 사항) 잡음이 있는 음성 잡음 대상 화자 추출 제로숏 TTS 필사(선택 사항) 합성용 텍스트 음성 혼합, 등록 음성 등록 음성 깨끗한 음성 편집 잡음이 있는 음성 편집 편집된 필사 편집된 필사 깨끗한 음성 대상 화자의 깨끗한 음성 대상 화자를 모방한 합성된 음성 편집된 음성 잡음이 있는 음성 원래 배경 잡음이 있는 편집된 음성그림 1. SpeechX 개요. SpeechX는 텍스트 및 음향 토큰 스트림을 조건으로 하는 신경 코덱 언어 모델을 사용하여 잡음 억제, 음성 제거, 대상 화자 추출, 제로샷 TTS, 깨끗한 음성 편집 및 잡음이 있는 음성 편집을 포함한 여러 오디오-텍스트 기반 음성 생성 작업을 처리합니다. 일부 작업의 경우 텍스트 입력은 선택 사항입니다. • 음성 향상 및 음성 편집을 포함한 음성 변환의 몇 가지 예를 들면, • 견고성: 통합 모델은 음향적으로 어려운 환경에 적용될 가능성이 높기 때문에 다양한 음향 왜곡에 대한 견고성을 보여주는 것이 필수적입니다. 신뢰할 수 있는 성능을 보장함으로써 이러한 모델은 배경음이 널리 퍼진 실제 시나리오에서 매우 유용하다고 간주될 수 있습니다. 확장성: 통합 모델은 유연한 아키텍처를 채택하여 작업 지원을 원활하게 확장할 수 있어야 합니다. 이를 달성하기 위한 한 가지 접근 방식은 입력 토큰이나 추가 모듈과 같은 추가 요소를 수용하는 것입니다. 이러한 유연성은 모델이 향후 음성 생성 작업에 효율적으로 적응할 수 있도록 합니다. 이 목표를 추구하기 위해 이 논문에서는 제로샷 TTS, 선택적 대본 입력을 사용한 노이즈 억제, 음성 제거, 선택적 대본 입력을 사용한 대상 화자 추출, 조용하고 시끄러운 음향 환경 모두에 대한 음성 편집(그림 1)을 포함한 여러 작업을 수행할 수 있는 다재다능한 음성 생성 모델을 소개합니다. 제안된 모델을 SpeechX¹라고 합니다. VALL-E와 마찬가지로 SpeechX는 텍스트 및 음향 입력을 기반으로 신경 코덱 모델 코드 또는 음향 토큰을 생성하는 언어 모델링 방식을 채택합니다. 다양한 작업을 처리할 수 있도록 멀티태스크 학습 설정에 추가 토큰을 통합합니다. 여기서 토큰은 실행할 작업을 집합적으로 지정합니다.
--- EXPERIMENT ---
영어: 모든 결과에 따르면 SpeechX는 제로샷 TTS, 노이즈 억제, 대상 화자 추출, 음성 제거 및 배경 노이즈가 있거나 없는 음성 편집을 포함한 다양한 작업에서 효과적이며 작업 전반에 걸쳐 전문 모델과 동등하거나 더 우수한 성능을 달성했습니다. 데모 샘플은 https://aka.ms/speechx에서 확인하세요. 색인 용어-음성 생성, 오디오-텍스트 입력, 멀티태스크 학습, 제로샷 텍스트-음성 변환, 노이즈 억제, 대상 화자 추출, 음성 편집, 음성 제거 I. 서론 생성 모델 기술은 텍스트[1], [2], 비전[3] 및 오디오[4]를 포함하는 다양한 머신 러닝 애플리케이션에서 빠르고 혁신적인 발전을 거쳤습니다. 이러한 발전은 산업과 사회 전반에 상당한 영향을 미쳤습니다. 특히 멀티모달 입력을 사용하는 생성 모델이 놀라운 혁신으로 등장했습니다[5]–[10]. 음성 도메인에서 오디오-텍스트 입력을 활용하는 두드러진 음성 생성 작업 중 하나는 제로샷 텍스트-음성 변환(TTS)입니다. 제로샷 TTS는 주어진 텍스트를 해당 화자의 음성 특성과 말하는 스타일을 가진 음성으로 변환하는 것을 포함하며, 해당 화자의 짧은 오디오 샘플만을 사용합니다. 제로샷 TTS의 초기 연구에서는 고정 차원 화자 임베딩[11]–[14]을 사용했습니다. 이 접근 방식은 TTS에만 사용이 제한되었고 화자 복제 기능을 적절히 지원하지 못했습니다. 반면, 최근 접근 방식은 마스크 음성 예측[15]이나 신경 코덱 언어 모델링[16]–[19]과 같은 보다 일반적인 공식을 채택했습니다. 이러한 새로운 접근 방식은 고정 차원 표현으로 압축하지 않고 대상 화자의 오디오를 직접 활용합니다. 결과적으로 이러한 X. Wang은 해당 저자입니다(이메일: xiaofei.wang@microsoft.com). X. Wang, M. Thakker, Z. Chen, N. Kanda, SE Eskimez, M. Tang, J. Li 및 T. Yoshioka는 Microsoft Corporation, Redmond, WA 98052, USA에 있습니다. S. Chen과 S. Liu는 중국 베이징에 있는 Microsoft Research Asia에 있습니다. 모델은 놀라운 제로샷 TTS 성능을 달성했을 뿐만 아니라 음성 변환[15], [18] 및 음성 편집[15]을 포함한 추가 기능도 보여주었습니다. 이 향상된 유연성은 음성 생성 모델에서 새로운 가능성을 열어줄 엄청난 가능성을 가지고 있습니다. 그러나 인상적인 성과에도 불구하고 이러한 최근 생성 모델은 여전히 특정 한계가 있는데, 특히 입력 음성을 변환하는 다양한 오디오-텍스트 기반 음성 생성 작업을 처리할 때 그렇습니다. 예를 들어, 기존 음성 편집 모델[20], [21]은 깨끗한 신호만 처리하는 데 제한되어 배경 소리를 보존하면서 말한 내용을 수정할 수 있는 기능이 없습니다. 또한, [15]에서 논의된 모델은 잡음 제거를 수행하기 위해 잡음이 있는 신호를 깨끗한 음성 세그먼트로 둘러싸야 하므로 실제 응용 프로그램에 상당한 제약이 있습니다. 깨끗하지 않은 음성을 변환하는 맥락에서 특히 유용한 또 다른 작업은 대상 화자 추출[22]–[24]입니다. 대상 화자 추출은 여러 화자가 포함된 음성 믹스에서 원하는 화자의 음성을 추출하는 것을 포함합니다. 원하는 화자는 해당 개인의 짧은 음성 녹음을 사용하여 지정할 수 있습니다. [25]에서 논의한 잠재적 중요성에도 불구하고 이 작업은 기존 생성 음성 모델에서 다루지 않은 채로 남아 있습니다. 잡음 제거 및 대상 화자 추출과 같은 음성 향상 작업에 대한 기존 접근 방식은 충실한 신호 복구를 위해 회귀 모델에 의존해 왔다는 점이 주목할 만합니다. 그러나 이러한 이전 방법은 일반적으로 각 작업에 대해 별도의 전문가 모델이 필요했는데, 음향 교란의 잠재적 다양성을 감안할 때 이상적이지 않습니다[26]. 또한 특정 음성 향상 작업에만 초점을 맞춘 제한된 연구를 제외하고는 참조 전사본을 활용하여 이해할 수 있는 음성을 생성하는 포괄적인 오디오텍스트 기반 음성 향상 모델이 부족했습니다[27], [28]. 앞서 언급한 고려 사항과 다른 도메인의 성공적인 선례를 감안할 때 생성 및 변환 기능을 통합하는 오디오텍스트 기반 생성 음성 모델을 만드는 것은 매우 중요한 연구 중요성을 가정합니다. 이러한 모델은 다양한 음성 생성 작업을 처리할 수 있는 포괄적인 기능을 보유해야 합니다. 우리는 이러한 모델이 다음과 같은 주요 속성을 갖추어야 한다고 제안합니다.다재다능함: 다른 머신 러닝 도메인에서 개발된 통합 또는 기초 모델과 유사하게 통합 오디오-텍스트 기반 생성 음성 모델은 오디오 및 텍스트 입력에서 음성을 생성하는 광범위한 작업을 처리해야 합니다.이러한 작업에는 제로샷 TTS뿐만 아니라 다양한 형태가 포함되어야 합니다.SpeechX 생성된 오디오 신경 코덱 언어 모델 음소 변환 작업 기반 프롬프팅 오디오 코덱 인코더 입력 텍스트 입력 오디오 작업 입력 텍스트 입력 오디오 오디오 코덱 디코더 잡음 억제 필사(선택 사항) 잡음이 있는 음성 출력 오디오 깨끗한 음성 음성 제거 필사(선택 사항) 잡음이 있는 음성 잡음 대상 화자 추출 제로숏 TTS 필사(선택 사항) 합성용 텍스트 음성 혼합, 등록 음성 등록 음성 깨끗한 음성 편집 잡음이 있는 음성 편집 편집된 필사 편집된 필사 깨끗한 음성 대상 화자의 깨끗한 음성 대상 화자를 모방한 합성된 음성 편집된 음성 잡음이 있는 음성 원래 배경 소음이 있는 편집된 음성그림 1. SpeechX 개요. SpeechX는 텍스트와 음향 토큰 스트림에 따라 조건화된 신경 코덱 언어 모델을 사용하여 잡음 억제, 음성 제거, 대상 화자 추출, 제로샷 TTS, 깨끗한 음성 편집 및 잡음이 있는 음성 편집을 포함한 여러 오디오-텍스트 기반 음성 생성 작업을 처리합니다. 일부 작업의 경우 텍스트 입력은 선택 사항입니다. • 음성 향상 및 음성 편집을 포함한 음성 변환의 경우를 예로 들 수 있습니다. • 견고성: 통합 모델은 음향적으로 어려운 환경에 적용될 가능성이 높기 때문에 다양한 음향 왜곡에 대한 견고성을 보여주는 것이 필수적입니다. 이러한 모델은 신뢰할 수 있는 성능을 보장함으로써 배경음이 널리 퍼진 실제 시나리오에서 매우 유용하다고 간주될 수 있습니다. 확장성: 통합 모델은 유연한 아키텍처를 채택하여 작업 지원을 원활하게 확장할 수 있어야 합니다. 이를 달성하기 위한 한 가지 접근 방식은 입력 토큰이나 추가 모듈과 같은 추가 요소를 수용하는 것입니다. 이러한 유연성을 통해 모델은 향후 음성 생성 작업에 효율적으로 적응할 수 있습니다. 이러한 목표를 추구하기 위해 이 논문에서는 제로샷 TTS, 선택적 대본 입력을 사용한 노이즈 억제, 음성 제거, 선택적 대본 입력을 사용한 대상 화자 추출, 조용하고 노이즈가 많은 음향 환경 모두에 대한 음성 편집을 포함한 여러 작업을 수행할 수 있는 다재다능한 음성 생성 모델을 소개합니다(그림 1). 제안된 모델을 SpeechX¹라고 합니다. VALL-E와 마찬가지로 SpeechX는 텍스트 및 음향 입력을 기반으로 신경 코덱 모델의 코드 또는 음향 토큰을 생성하는 언어 모델링 방식을 채택합니다. 다양한 작업을 처리할 수 있도록 다중 작업 학습 설정에 추가 토큰을 통합합니다. 여기서 토큰은 실행할 작업을 집합적으로 지정합니다. LibriLight[29]의 60,000시간 음성 데이터를 훈련 세트로 사용한 실험 결과는 SpeechX의 효능을 보여주며 앞서 언급한 모든 작업에서 전문가 모델과 비교했을 때 동등하거나 더 우수한 성능을 보여줍니다. 특히 SpeechX는 음성 편집 중에 배경음을 보존하고 노이즈 억제 및 대상 화자 추출을 위해 참조 대본을 활용하는 등 새로운 기능이나 확장된 기능도 보여줍니다. 음성 향상 작업의 관점에서 SpeechX는 간단하고 효과적인 1X(변환)를 제공하여 모델이 제로샷 TTS 외에도 다양한 음성 변환 작업을 수행한다는 것을 강조합니다. 제로샷 TTS와 같은 비음성 향상 작업을 통합하여 이해도를 향상시키기 위한 참조 전사를 활용하는 프레임워크입니다. 제안된 SpeechX 모델의 기능을 보여주는 오디오 샘플은 https://aka.ms/speechx에서 사용할 수 있습니다. II. 관련 연구 A. 자기 회귀 생성 모델 디코더 전용 변환기라고도 하는 자기 회귀 변환기를 사용하는 언어 모델링 접근 방식을 기반으로 하는 생성 모델은 다양한 응용 분야에서 상당한 성공을 거두었습니다. 이러한 모델의 주목할 만한 예로는 GPT 시리즈[1], [2] 및 DALL-E[30]가 있습니다. 자기 회귀 접근 방식은 오디오 및 음성 영역으로도 확장되었습니다. AudioLM[4]과 MusicLM[10]은 각각 고유한 시간 척도와 의미적 세분성을 가진 여러 유형의 토큰을 활용하는 선구적인 노력으로, 계층적 토큰 생성을 가능하게 합니다. 거친 토큰과 세분된 토큰을 모두 포함하는 이 계층적 구조는 미묘한 세부 사항과 장기적인 규칙성을 모두 갖춘 사운드 합성을 가능하게 합니다. 제로샷 TTS의 경우 VALL-E[16]와 SPEAR-TTS[17]는 텍스트(의미적) 토큰과 음향 토큰을 단일 데이터 스트림으로 표현하여 자기 회귀 변환기를 사용합니다. 이 접근 방식을 통해 모델은 제로샷 화자 적응을 수행하여 특정 사람의 음성을 모방하는 TTS 음성을 생성할 수 있습니다. 이러한 모델은 단 3초의 짧은 음성 클립에서 제로샷 TTS를 수행할 수 있음이 입증되었습니다. 이러한 자기 회귀 음성 생성 모델의 주목할 만한 장점은 별도의 지속 시간 모델이 필요 없이 TTS를 수행할 수 있다는 것입니다. 이 간소화된 아키텍처는 학습 프로세스를 간소화하고 다양한 음성 생성 작업을 포함하는 데 필요한 유연성을 높일 수 있습니다. 이러한 이유로 우리는 자기 회귀 Transformers를 사용하여 SpeechX 모델을 구축하기로 했습니다.B. 다중 작업 생성 음성 모델 논문 여러 논문에서 최근에 제로샷 TTS 및 여러 관련 작업을 지원하는 오디오-텍스트 기반 음성 생성 모델을 개발하는 데 노력을 기울였다고 보고했습니다. 이러한 작업에는 음성 또는 스타일 변환(Make-A-Voice[18], NaturalSpeech2[31] 및 Voicebox[15]), 음성 편집(Mega-TTS[21] 및 Voicebox) 및 잡음 제거(NaturalSpeech2 및 Voicebox)가 포함됩니다. Voicebox는 마스크된 음성 예측 원리를 통해 다양한 작업을 용이하게 함으로써 주목할 만한 발전을 보여주었습니다. 그럼에도 불구하고 그 기능은 여전히 깨끗한 음성 생성에만 국한되어 잡음이 있는 음성을 효과적으로 처리하거나 잡음 억제 및 대상 화자 추출과 같은 기존 오디오 향상 작업을 포괄하지 못합니다. 이 연구에서 우리는 깨끗한 음성과 잡음이 있는 음성을 모두 다루고 생성 및 변환 작업을 통합합니다. 이를 달성하기 위해 우리는 작업 종속 프롬프트로 다중 작업 학습을 수행하여 VALL-E를 확장합니다. SpeechX라고 부르는 결과 모델은 다양한 음성 처리 작업에서 다재다능함을 보여줍니다. 이 모델은 제로샷 TTS 및 음성 편집과 같은 음성 생성 작업에서 탁월할 뿐만 아니라 잡음 억제 및 대상 화자 추출과 같은 향상 작업에서도 효과적으로 수행합니다. 또한 배경 소음을 유지하면서 말한 내용을 편집하거나 향상 작업을 위해 필사본을 효과적으로 활용하는 것과 같은 새로운 기능을 실현합니다. A. 개요 III. 방법 그림 1은 SpeechX 아키텍처의 개요를 보여줍니다. VALL-E에서 도입한 원칙을 기반으로 SpeechX는 Transformers를 기반으로 하는 신경 코덱 언어 모델을 사용합니다. 이 모델은 텍스트 프롬프트 T와 음향 프롬프트 A의 두 입력 프롬프트를 기반으로 O로 표시된 신경 코드 시퀀스의 조건부 생성을 수행하는 방법을 학습합니다. 신경 코드는 음향 토큰이라고도 합니다. 텍스트 프롬프트 T는 입력 텍스트에 자소-음소 변환²을 적용하여 얻은 음소 시퀀스입니다. 텍스트 프롬프트는 의미 정보를 전달하므로 의미 토큰이라고 합니다. 반대로 음향 프롬프트 A는 입력 음성 신호의 음향 정보를 캡슐화합니다. 신경 코덱 모델의 인코더를 사용하여 입력 오디오를 음향 토큰 시퀀스로 변환하여 얻습니다. 또한 실행할 작업 또는 동등하게 원하는 출력을 지정하기 위해 음향 프롬프트에 추가 토큰을 통합합니다. 자세한 내용은 섹션 III-C에서 설명합니다. 출력 O는 원하는 신호의 신경 코드 시퀀스로, 코덱 디코더를 사용하여 파형 신호로 변환됩니다. 이전 작업에 따라 EnCodec[32]을 신경 코덱 모델로 사용합니다. EnCodec은 L개의 양자화 계층이 있는 인코더-디코더 아키텍처를 기반으로 합니다. 실험에서 [16]의 구성과 일치하도록 L = 8을 사용합니다. EnCodec 모델의 각 계층은 75Hz의 샘플링 속도에서 1024개 항목으로 구성된 이산 코드를 생성합니다.2https://github.com/Kyubyong/g2p 제안된 간단한 아키텍처는 신경 언어 모델링 접근 방식의 종단 간 모델링 기능을 활용한다는 점을 강조합니다.다른 제로샷 TTS 또는 음성 생성 방법과 달리 이 접근 방식은 신경 코덱 모델과 별도로 스피커 임베딩 모델이나 지속 시간 모델과 같은 별도의 모델이 필요 없습니다.이 핵심 속성을 통해 Speech는 다양한 요구 사항과 입출력 관계가 있는 다양한 작업에 대한 지식을 습득하여 다재다능하고 확장성이 뛰어난 음성 생성 프로세스를 용이하게 합니다.B. 신경 코덱 언어 모델 VALL-E [16]와 마찬가지로 SpeechX는 자기 회귀(AR) 및 비 자기 회귀(NAR) 변환기 모델을 사용합니다.특히 AR 모델은 EnCodec의 첫 번째 양자화 계층에 해당하는 신경 코드를 출력하는 데 사용됩니다. 반면, NAR 모델은 첫 번째 계층 위의 모든 계층, 즉 두 번째 계층에서 여덟 번째 계층의 신경 코드를 생성합니다.AR 및 NAR 모델을 결합하면 [16]에서 설명한 대로 생성 유연성과 추론 속도 간에 합리적인 균형을 이룰 수 있습니다.= 출력을 구체적으로 행렬 O [0,1] € NTXL로 표현합니다.여기서 ot는 시간 프레임 t에서 1번째 코덱 계층의 코드를 나타내며 다음 값 중 하나를 취할 수 있습니다.출력 시퀀스 길이는 T로 표시합니다.AR 모델은 Transformer 디코더 계층[33]의 스택으로 구성되며 다음과 같이 정의되는 원하는 출력의 첫 번째 계층 코드의 음의 로그 가능도를 최소화하여 최적화됩니다.T LAR t=log P(0t,1|T, A, 0 <t,1; (AR), (1) where o<t,1 = [01,1, · · ·, Ot−1,1], While AR represents the AR Transformer model parameters. Different embedding projections are applied to the textual and acoustic tokens, and they are superimposed by sinusoidal positional embeddings. Note that the AR model in SpeechX is conditioned on three elements: the acoustic prompt A, the textual prompt T, and the past acoustic history o<t,1. This formulation differs from that of VALL-E, where the AR model is conditioned only on and o<t,1 (Eq. (1) of [16]), and the audio prompt is represented as part of o<t,1 during inference. This difference provides a practical benefit during the inference time of zeroshot TTS, where SpeechX no longer requires a transcription of the audio prompt. More specifically, in the case of VALL-E inference, we need to construct T from the concatenation of the transcription of the audio prompt and the text prompt. The audio is then generated by setting the codec sequence of the audio prompt to o<t, 1. In contrast, for SpeechX inference, T is simply the text prompt. The model can generate the sequence without requiring the transcription of the audio codec prompt. After obtaining the first layer codes with the AR model, the NAR model is used to generate the 1-th layer codes based on the text and acoustic prompts as well as the output codes for the first - 1 layers, which have already been produced. The model is used repeatedly for 1 = 2,, 8. Since we Task TABLE I TASK-BASED PROMPTING: PROMPTS AND DESIRED OUTPUT FOR INDIVIDUAL TASKS. G2P(·) DENOTES GRAPHEME-TO-PHONEME CONVERSION. Textual prompt T Noise suppression Speech removal Target speaker extraction Zero-shot TTS Clean speech editing Noisy speech editing Acoustic prompt A G2P(text) null <ns>, C(s+n) G2P(텍스트) null<sr> , C(s+n) G2P(텍스트) null C(s),<tse> , C(S1 + 82) G2P(텍스트) C(s) G2P(텍스트) C(스프레),<soe> ,<mask> ,<eoe> , C(Spost) 원하는 출력 OC(s) C(n) C(81) C(s&#39;) C(Spre), C(Sedit), C(Spost) G2P(text) C(spre pre),<soe> , C(Smid + mid),<eoe> , C(Spost + n post) C(Spre + pre), C(Sedit + nmid), C(Spost + npost)나머지 7개 계층에 동일한 NAR 모델을 사용하고, NAR 모델은 다음의 음의 로그 가능도 함수를 최소화하도록 학습됩니다: LNAR -Σ log P(0,1|T, A, 0:,&lt;1; Onar), 1=(2) 여기서 ONAR은 NAR 모델 매개변수를 나타내고, o.는 I번째 계층의 전체 시퀀스를 나타내고, o:,&lt;1 = Ot,l [0:1, · · · ‚ 0:,1–1]. 이 공식에서 단일 NAR 모델이 7개 계층 각각을 처리하기 위해, 첫 번째에서 (1 − 1)번째 계층까지의 음향 토큰 0.,&lt;1이 내장되고 합산됩니다. C. 작업 기반 프롬프팅 _ SpeechX는 하나의 모델로 여러 작업을 처리하는 것을 목표로 합니다. 이를 위해 우리는 표 I에 예시되어 있고 아래에서 자세히 설명된 대로 작업 기반 프롬프팅을 채택합니다. = 노이즈 억제는 노이즈로 손상된 관찰 s + n에서 깨끗한 음성 신호를 추출하는 작업입니다. 여기서 In은 노이즈를 나타냅니다. 노이즈 억제 작업의 경우 특수 토큰을 통합합니다.<ns> , 음향 프롬프트를 형성하여 A를 생성합니다. [<ns> ,C(s + n)]. 여기서 C()는 오디오 신호를 신경 코덱 토큰 시퀀스로 변환하는 데 사용되는 함수를 나타냅니다. 텍스트 프롬프트 T는 사용자가 참조 필사로 제공해야 하지만, 우리는 인간 필사를 사용할 수 없는 시나리오를 수용하기 위해 텍스트 프롬프트의 사용을 선택 사항으로 둡니다. 원하는 출력은 깨끗한 오디오의 음향 토큰 시퀀스인 C(s)입니다. 음성 제거는 배경 잡음을 보존하면서 잡음이 있는 음성 신호에서 음성을 제거하는 것을 포함합니다. 녹음에서 원치 않는 음성만 제거하는 데 유용합니다. 이 작업을 처리하기 위해 특수 토큰을 사용합니다.<sr> , 음향 프롬프트를 A = [로 구성합니다.<sr> ,C(s + n)]. 원하는 출력은 잡음 신호 C(n)의 음향 토큰 시퀀스입니다. 잡음 억제의 경우와 마찬가지로 텍스트 프롬프트는 생략할 수 있습니다. 대상 화자 추출은 2차 화자의 s₁와 간섭 음성 82의 혼합에서 대상 화자의 깨끗한 음성 S₁을 분리하는 것을 목표로 합니다. 대상 화자는 해당 개인의 짧은 등록 오디오 s½을 통해 식별되며, 여기서 등록에 3초가 필요하다고 가정했습니다. 이 작업의 경우 등록 오디오 C(s)에서 추출한 음향 토큰과 혼합 음성 C(81 +82)의 음향 토큰을 작업 지정 토큰으로 연결하여 음향 프롬프트를 형성합니다.<tse> 즉, A = [C(s1),<tse> , C(81+82)]. 원하는 출력은 C(81)입니다. 이전 작업과 마찬가지로 텍스트 프롬프트를 포함하는 것은 선택 사항입니다. 제로샷 TTS는 제공된 입력 텍스트와 등록 음성 s를 모두 활용하여 음성 신호 s&#39;를 생성하는 것을 목표로 합니다. 목표는 s&#39;의 음성 특성이 s의 음성 특성과 매우 유사하면서도 입력 텍스트를 정확하게 반영하도록 하는 것입니다. 이 작업의 경우 등록 오디오에서 추출한 음향 토큰인 C(s)를 음향 프롬프트로 사용합니다. 이 모델은 입력 텍스트를 기반으로 합성된 음성 C(s&#39;)에 대한 음향 토큰을 생성합니다. 그런 다음 이러한 음향 토큰을 해당 파형으로 변환합니다. 깨끗한 음성 편집은 입력 음성의 세그먼트를 수정하여 입력 텍스트에 맞추는 것으로 정의됩니다. s는 편집할 입력 음성 신호를 나타냅니다. 우리는 일반성을 잃지 않고 편집을 위한 대상 세그먼트인 Smid를 Spre, Smid, Spost의 세 개의 뚜렷한 부분으로 나눕니다(Spre와 Spost는 비어 있을 수 있음). 우리는 음향 프롬프트를 [C(Spre),<soe> ,<mask> ,<eoe> , C(Spost)], 여기서 새로운 토큰<soe> ,<mask> ,<eoe> 작업과 편집을 위해 지정된 음성 세그먼트를 지정하기 위해 도입되었습니다. 원하는 출력은 신경 코드 시퀀스 [C(Spre), C(Sedit), C(Spost)]이며, 여기서 [Spre, Sedit, Spost]의 음성 내용은 입력 텍스트와 일치합니다. Sedit의 화자 특성은 Spre 및 Spost의 화자 특성과 일치해야 합니다.⚫ 이와 대조적으로 잡음이 있는 음성 편집은 잡음이 있는 음성을 입력으로 사용하여 기본 배경 잡음을 그대로 유지하면서 세그먼트 내의 음성 내용을 수정하는 것을 목표로 합니다. 따라서 이 작업은 모델이 편집 프로세스 중에 음성과 잡음을 구별해야 하기 때문에 깨끗한 음성 편집 작업보다 더 어려울 것입니다. 이 목표를 달성하려면 편집을 위해 세그먼트를 마스크하는 대신 완전한 입력 음성 신호를 모델에 제공하는 것이 중요합니다.<mask> 토큰. 따라서 우리는 음향 프롬프트를 [C(Spre + Npre),<soe> , C(스미드+나미드),<eoe> , C(Spost +npost)], 여기서 아래 첨자는 이전에 정의된 대로 pre, mid 또는 post에 해당합니다. 원하는 출력은 신경 코드 시퀀스 [C(Spre + npre), C(Sedit + nmid), C(Spost + npost)]로 구성됩니다. 이 공식은 모델이 nmid를 유지하면서 텍스트 입력을 기반으로 Smid를 Sedit로 변환해야 함을 명확히 합니다. 실제 음성 편집 시나리오에서 입력 텍스트는 종종 먼저 입력 음성에 자동 음성 인식(ASR)을 적용한 다음 사용자가 필사본을 편집하여 얻습니다. 이러한 상황에서는 다음을 식별하는 것이 간단합니다.<soe> 그리고<eoe> 삽입해야 합니다. 또한 깨끗한 음성 편집에서 다음을 사용하는 것이 주목할 만합니다.<mask> 모델이 출력 음성 길이를 적응적으로 변경하여 출력 음성이 말하는 속도 측면에서 자연스럽게 들리도록 할 수 있습니다. 설명된 작업 기반 프롬프트 전략은 SpeechX 모델이 추론 중에 원하는 출력을 고유하게 결정할 수 있는 기능을 제공합니다. 이 접근 방식은 추가 작업을 통합하는 데 유연성을 제공합니다. 새 작업을 추가하려면 해당 프롬프트 체계를 통합하고 기존 검사점에서 모델 학습을 계속해야 하며, 새로 도입된 작업별 토큰에 대한 임베딩만 무작위로 초기화됩니다. 이는 기본 모델 아키텍처를 변경하지 않고도 수행할 수 있습니다. D. 모델 학습 학습하는 동안 각 모델 업데이트에 대해 동일한 확률로 작업을 무작위로 샘플링합니다. 이는 모델이 특정 작업을 지나치게 선호하지 않도록 하기 위한 것입니다. 노이즈 억제, 음성 제거 및 대상 화자 추출 작업의 경우 텍스트 프롬프트를 50% 확률로 포함하여 모델이 텍스트와 텍스트 없는 시나리오를 모두 동등하게 경험하도록 합니다. 모델이 기본 생성 기능을 습득하도록 돕기 위해 먼저 모델을 제로샷 TTS에 대해서만 학습시킨 다음 모든 작업을 사용하여 멀티태스크 학습을 수행하는 학습 프로세스를 계속합니다.즉, 기존 VALL-E 모델 체크포인트로 모델을 초기화합니다.정확히 말해서, 제로샷 TTS에 대해서만 학습된 SpeechX 모델은 VALL-E와 약간의 차이를 보입니다.이 차이는 전자가 각 학습 샘플에 대해 동일한 화자에서 나온 별도의 등록 오디오를 명시적으로 통합하는 반면 후자는 통합하지 않는다는 사실에서 발생합니다.그럼에도 불구하고 단순화를 위해 이 초기화 접근 방식을 VALLE 초기화라고 합니다.멀티태스크 학습 단계를 시작할 때 작업 종속 프롬프트와 관련된 특수 토큰에 대해 무작위로 초기화된 임베딩이 추가됩니다.이 2단계 학습 전략은 실험 결과에서 입증되었듯이 모든 작업에서 성능을 크게 향상시킵니다.IV.평가 설정SpeechX와 같은 다재다능한 음성 생성 모델을 평가하려면 각각 개별 작업에 초점을 맞춘 일련의 테스트를 수행해야 합니다. 실험을 관리하기 쉽게 유지하고 작업 전체에서 일관성을 유지하기 위해 모든 평가에 Libri Speech의 테스트 클린 분할에서 파생된 평가 데이터 세트를 사용했습니다. 이 섹션에서는 평가 설정에 대한 세부 정보를 제공합니다. 이전에 확립된 관행 [15], [16]에 따라 4~10초 사이의 지속 시간을 가진 테스트 샘플을 선택했습니다. A. 평가 데이터 제로샷 TTS: 각 테스트 샘플에 대해 참조 전사본을 사용하여 텍스트 프롬프트를 만들었습니다. 음향 프롬프트는 같은 화자의 다른 발화를 무작위로 선택하고 3초 길이의 클립을 추출하여 생성했습니다. 노이즈 억제: 각 테스트 샘플을 MUSAN 데이터 세트 [34]에서 무작위로 선택한 노이즈 샘플과 혼합했습니다. 신호 대 잡음비(SNR)는 0dB~20dB 범위에서 무작위로 결정되었습니다. 작업은 노이즈가 있는 음성에서 손상되지 않은 음성을 복구하는 것이었습니다. 음향 프롬프트는 노이즈가 있는 신호에 EnCodec을 적용하여 얻었습니다. 텍스트 프롬프트와 관련하여 텍스트 없는(즉, 의미적 프롬프트 사용 안 함) 및 텍스트 가이드 노이즈 억제를 모두 고려했으며, 텍스트 가이드 설정에 참조 전사본을 사용했습니다.대상 화자 추출: 각 테스트 샘플을 0dB와 20dB 사이의 범위에서 무작위로 결정된 신호 대 간섭 비율(SIR)에서 다른 화자의 발화와 혼합했습니다.또한 동일한 화자의 다른 발화 하나 이상을 무작위로 선택하여 모델이 원하는 화자가 누구인지 식별하는 데 도움이 되는 3초 길이의 등록 클립을 만들었습니다.혼합 신호와 등록 신호는 모두 섹션 III-C에서 설명한 대로 음향 프롬프트를 도출하는 데 사용되었습니다.과제는 대상 화자의 손상되지 않은 원래 음성을 복구하는 것이었습니다.노이즈 억제 작업과 마찬가지로 텍스트 없는 설정과 텍스트 가이드 설정을 모두 고려했습니다.클린 음성 편집: 각 테스트 샘플에 대해 전체 발화의 10%에서 50% 사이의 길이의 기간을 무작위로 선택했습니다. 우리는 선택된 기간의 음성을 같은 화자의 다른 무작위로 선택된 음성 샘플로 대체했습니다. 부분적으로 대체된 화자 동질 음성과 참조 전사본을 감안할 때, 과제는 화자 특성과 입력 신호의 대체되지 않은 부분을 변경하지 않고 전사본을 따르는 음성 신호를 생성하는 것이었습니다. 우리의 실험에서 우리는 올바른<soe> 그리고<eoe> 대체된 세그먼트의 지식에 기반한 위치.잡음이 있는 음성 편집: 깨끗한 음성 편집 작업의 각 테스트 샘플에 무작위로 선택한 MUSAN 잡음 샘플을 추가했습니다.SNR은 0dB에서 20dB 범위에서 선택되었습니다.잡음이 손상된 부분적으로 대체된 음성과 참조 전사본을 감안하여, 작업은 배경 잡음, 화자 특성 및 입력 음성의 대체되지 않은 부분을 변경하지 않고 전사본을 따르는 잡음이 있는 음성 신호를 생성하는 것이었습니다.음성 제거: 잡음 억제에 사용된 것과 동일한 데이터 세트를 사용했습니다.잡음이 있는 음성 신호가 주어지면, 작업은 음성을 제거하여 잡음 신호를 추출하는 것이었습니다.텍스트가 없는 경우만 고려했습니다.결과적으로 입력은 잡음이 있는 음성에 해당하는 음향 프롬프트로만 구성되었습니다.B. 지표 일관성과 재현성을 위해 아래에 설명된 대로 개별 작업에 대해 객관적인 지표를 사용하기로 했습니다.단어 오류율(WER): 제공된 전사본을 준수하여 생성된 오디오의 충실도를 평가하기 위한 지표로 WER을 사용했습니다. 실험에 사용된 ASR 시스템은 NeMo의 stt_en_conformer_transducer_large 모델³로, Conformer Transducer 아키텍처[35]를 기반으로 합니다. 우리는 예비 실험에서 관찰된 것처럼 다른 공개적으로 사용 가능한 ASR 모델과 비교했을 때 노이즈 및 처리 아티팩트에 대한 뛰어난 안정성과 견고성을 기반으로 이 특정 ASR 모델을 선택했습니다. ASR의 견고성은 노이즈 억제 및 노이즈가 있는 음성 편집과 같은 작업에 특히 중요합니다. WER 메트릭은 음성 제거를 제외한 모든 작업에 사용되었습니다. 화자 유사도 점수(SIM): 화자 유사도 점수는 생성된 음성의 일관성을 화자의 특성과 관련하여 평가하는 메트릭으로 사용되었습니다. 이 점수는 생성된 음성의 화자 임베딩과 원하는 음성 신호 간의 코사인 유사도로 계산되었습니다. 화자 임베딩의 계산은 NeMo의 TitaNet-Large를 사용하여 수행되었습니다. 우리는 화자 유사도 측정을 위해 EnCodec 처리된 신호를 활용하는 대신 원래 오디오 데이터를 사용하여 EnCodec 모델 사용으로 인해 발생할 수 있는 잠재적인 음성 변형 효과를 포착하고 반영했습니다. SIM은 zeroshot TTS, 깨끗한 음성 편집 및 노이즈가 있는 음성 편집에 사용되었습니다. DNSMOS: 노이즈 억제 및 대상 화자 추출 작업에서 평가를 위해 음향적으로 손상된 음성의 지각된 품질을 예측하기 위한 잘 확립된 모델 기반 메트릭인 DNSMOS[36]를 사용했습니다³. 구체적으로, 우리는 DNSMOS P.model의 OVRL 점수를 사용했습니다. 대상 화자 추출의 성능을 평가하기 위해, 우리는 이 특정 작업에 맞게 조정되었고 동일한 웹페이지에서 사용할 수 있는 개인화된 DNSMOS 모델을 사용했습니다. 음성 품질의 지각적 평가(PESQ): 노이즈 억제 및 대상 화자 추출 작업의 경우 PESQ[37]도 사용했습니다. DNSMOS와 달리 PESQ는 깨끗한 참조 신호가 필요한 침입적 메트릭입니다. 결과적으로 PESQ는 생성된 오디오의 충실도를 원래 깨끗한 데이터와 비교하여 평가할 것으로 예상됩니다. 멜셉스트럼 왜곡(MCD): MCD는 두 멜셉스트럼 시퀀스 간의 차이점을 정량화하는 데 사용되는 메트릭입니다. 이 메트릭을 사용하여 추정된 노이즈를 기준 진실 노이즈 오디오와 비교하여 음성 제거 정확도를 객관적으로 측정했습니다. A. 훈련 데이터 V. 실험 우리는 LibriLight에서 깨끗한 음성 데이터를 얻었는데, 여기에는 7,000명 이상의 화자로부터 나온 60,000시간 분량의 전사되지 않은 영어 독해 음성이 포함되어 있습니다[29]. 이는 VALL-E[16]를 사용한 zeroshot TTS 실험에서 수행한 것과 같습니다. 각 작업에 대한 특정 훈련 요구 사항을 충족하기 위해 아래에 자세히 설명된 대로 평가 데이터를 만드는 데 사용된 방법에 따라 데이터 시뮬레이션을 수행했습니다. 섹션 III-D에서 설명한 대로 각 반복에 대해 무작위로 선택된 작업을 기반으로 개별 훈련 미니 배치를 구성했습니다. 한국어: https://huggingface.co/nvidia/speakerverification_en_titanet_large Shttps://github.com/microsoft/DNS-Challenge/tree/master/DNSMOS 6https://pypi.org/project/pymcd 잡음 억제 및 음성 제거 작업의 경우, 깨끗한 음성을 SNR이 -5dB~20dB인 DNS 챌린지 코퍼스[38]의 잡음 샘플과 혼합했습니다. 모델은 잡음 억제 및 음성 제거를 위해 깨끗한 음성과 잡음의 음향 토큰을 복구하도록 훈련되었습니다. 대상 화자 추출 작업의 경우, 개별 깨끗한 음성 샘플을 SIRS 범위가 5dB~20dB인 다른 무작위로 선택한 화자의 음성 샘플과 혼합했습니다. 깨끗한 음성 편집과 관련하여, 각 깨끗한 발화에 대해 길이가 10%~70%인 하위 세그먼트를 무작위로 선택한 다음, 다른 내용이 있는 동일한 화자의 다른 오디오 세그먼트로 대체했습니다. 교체된 세그먼트의 시작 및 종료 시간을 저장하여 삽입하는 데 사용했습니다.<soe> 그리고<eoe> 훈련 중에 음향 프롬프트에서 토큰을 올바른 위치에 배치합니다. 또한, 잡음이 있는 음성 편집을 위한 훈련 샘플을 만들기 위해 잡음 억제 작업에 사용된 잡음 샘플을 부분적으로 대체된 깨끗한 오디오에 추가했습니다. 그 결과, 잡음이 있는 부분적으로 대체된 음성과 해당 원래 잡음이 있는 음성의 쌍을 얻었고, 이는 잡음이 있는 음성 편집 작업의 훈련 데이터로 사용되었습니다. 잡음이 있는 음성 편집 훈련에 사용된 SNR 범위도 -dB에서 20dB였습니다. LibriLight는 참조 전사본을 제공하지 않으므로 [15], [16]에 따라 개별 훈련 샘플의 음소 시퀀스인 의미 프롬프트를 도출하기 위해 의사 레이블링 방식을 채택했습니다. 구체적으로, 3배 속도 섭동으로 960시간 분량의 Librispeech 데이터에서 훈련된 기성품 Kaldi 모델로 LibriLight 훈련 데이터를 전사했습니다. B. 모델 및 학습 구성 SpeechX AR과 NAR 모델은 모두 동일한 Transformer 아키텍처를 공유하며, 12개 레이어, 16개 어텐션 헤드, 1024의 임베딩 차원, 4096의 피드포워드 레이어 차원, 0.1의 드롭아웃 비율을 특징으로 합니다. 무작위 초기화와 VALL-E 초기화의 두 가지 초기화 방법을 사용하여 실험을 수행했습니다(자세한 내용은 섹션 III-D 참조). 무작위 초기화 시나리오에서 SpeechX 모델을 800k 반복으로 학습했습니다. 모델 최적화는 AdamW 최적화 도구를 사용했으며, 학습률은 초기 32K 업데이트에 대한 워밍업 단계를 거쳐 5 × 10−4에서 정점을 이룬 후 선형 감쇠 단계로 전환되었습니다. 반대로 VALL-E 초기화의 경우 초기 모델이 이미 400k 반복에 걸쳐 제로샷 TTS 학습을 거쳤으므로 400k 반복을 선택했습니다. 이 경우 학습률 스케줄러는 유지되었지만 워밍업 기간은 처음 20k 업데이트로 단축되었습니다.C. 기준선 전문가 모델 우리는 비교 기준선을 확립하기 위해 다양한 작업에 전문가 모델을 사용했습니다.제로샷 TTS의 경우 원래 논문 [16]에 설명된 모델 구성을 따라 VALL-E를 활용했습니다.노이즈 억제 작업의 경우 비인과적 Deep Complex Convolutional Recurrent Network https://kaldi-asr.org/models/m을 사용했습니다.표 II 개별 작업에 대한 전문가 모델과 비교한 SPEECHX의 다양한 음성 생성/변환 작업에 대한 결과.노이즈 억제 및 대상 화자 추출에는 텍스트 프롬프트가 사용되었습니다.제로샷 TTS에서 &quot;처리 없음&quot; 행은 원하는 음성 신호의 결과를 보여줍니다. 잡음 제거 모델 WER 처리 없음 3.DNSMOS↑ PESQ↑ 2.1.대상 화자 추출 WER DNSMOS↑ PESQ↑ 12.55 3.2.제로샷 TTS WER↓ SIM↑ 1.71 1.WER↓ 0.42.깨끗한 음성 편집 잡음이 많은 음성 편집 음성 제거 WER↓ SIM↑ 38.SIM↑ MCD↓↓ 0.12.전문가 모델 DCCRN [39], [40] VoiceFilter [22] VALL-E [16] A³T [20] A³T [20] 없음 6.3.3.5.3.2.5.90 0.17.0.32.0.SpeechX(무작위 초기화) 2.3.2.3.3.2.SpeechX(VALL-E 초기화) 2.3.2.2.3.2.5.40 0.4.66 0.8.0.15.0.3.5.0.13.0.3.표 III 텍스트 프롬프트가 있거나 없는 소음 억제 및 대상 화자 추출 결과.소음 억제 프롬프트 WER DNSMOS↑ PESQ↑ 텍스트 있음 텍스트 없음 2.6.3.3.2.2.대상 화자 추출 WER DNSMOS↑ PESQ↑ 2.3.2.5.3.2.(DCRN) [39]은 소음 억제를 위한 널리 알려진 모델입니다. DCCRN에 대한 우리의 훈련 데이터는 Microsoft의 내부 데이터 세트에서 가져왔고, 우리는 [40]의 훈련 레시피를 기반으로 하는 ASR 목적을 사용하여 모델을 더욱 미세 조정했습니다.대상 화자 추출을 위해, 우리는 양방향 LSTM 구성을 채택한 VoiceFilter [22]를 활용했습니다.우리는 VoiceFilter³의 공개적으로 사용 가능한 구현에 의존했습니다. 마지막으로, 음성 편집을 위해 우리는 A³T[20]를 기준으로 사용했습니다. 우리가 사용한 A³T의 구현도 공개적으로 접근 가능합니다⁹. D. 결과 1) 결과 개요: 표 II는 다양한 작업에서 개별 전문가 모델과 비교한 SpeechX의 성능 분석을 보여줍니다. 기존 VALL-E 모델 체크포인트를 사용하여 모델 매개변수를 초기화하는 것이 모든 작업에서 유익했으며, 특히 WER 측면에서 유익했음을 알 수 있습니다. 잡음 억제 및 대상 화자 추출에서 SpeechX는 해당 전문가 모델과 비교하여 WER 측면에서 더 우수한 성능을 보였습니다. 기존의 회귀 기반 잡음 억제 및 대상 화자 추출 모델은 처리 아티팩트가 발생하는 것으로 알려져 있으며, 이는 WER 결과에서 확인되었습니다. SpeechX는 오디오-텍스트 기반 생성 기능 덕분에 이러한 부정적인 영향을 피할 수 있었습니다. 반면 DNSMOS 및 PESQ 점수 측면에서는 전문가 모델보다 뒤처졌습니다. 이는 섹션 V-D6에서 자세히 설명한 대로 사용된 코덱 모델의 영향에 크게 기인할 수 있습니다. 음성 제거 작업에 대한 조사 결과 SpeechX는 MCD에서 상당한 개선을 보였으며, 음성 제거에 대한 효능을 보여주었습니다. 이러한 결과는 SpeechX 모델이 향상 관련 작업을 처리하는 데 있어 다재다능하다는 것을 강조하는 동시에 SpeechX가 제공하는 오디오-텍스트 기반 음성 생성 기능의 유용성을 강조합니다. 제로샷 TTS 작업에서 SpeechX는 WER 측면에서 기준 VALL-E 모델보다 약간의 이점을 보였으며 동시에 비슷한 화자 유사도 점수를 달성했습니다.10 또한 깨끗한 음성 편집 작업의 경우 SpeechX는 기준 A³T 모델보다 상당한 개선을 보였습니다. 음성 편집 작업에서 관찰된 WER은 제로샷 TTS 작업에서 얻은 WER보다 약간 높았지만, 두 작업이 동일한 범위에 속할 것으로 예상할 수 있습니다. 이러한 불일치는 편집되지 않은 음성의 길이가 3초보다 짧은 특정 테스트 샘플에 기인할 수 있습니다. 이러한 결과는 SpeechX가 변환 능력보다는 주로 음성 생성 능력에 초점을 맞춘 작업에서 동일하게 효과적임을 강조합니다.2) 깨끗하고 잡음이 있는 음성의 음성 편집: 표 II는 또한 WER과 SIM 측면에서 깨끗하고 잡음이 있는 음성의 음성 편집 결과를 비교합니다.잡음이 있는 음성을 편집하는 것은 배경 잡음을 보존하면서 말한 내용을 수정해야 하기 때문에 깨끗한 음성보다 더 큰 과제를 안겨줍니다.편집할 깨끗한 오디오 신호와 잡음이 있는 오디오 신호 간의 WER 격차가 38.29% 대 42.48%인 것과 A³T의 WER 개선이 42.48%에서 32.17%로 제한적이라는 것에서 이러한 어려움이 분명하게 드러납니다.그럼에도 불구하고 SpeechX 모델은 잡음이 있는 음성을 성공적으로 편집하여 처리 후 WER을 13.95%로 줄였습니다.이는 입력 신호의 음향 잡음에 대한 모델의 견고성을 보여줍니다.0.65의 높은 SIM 점수는 모델이 잡음이 있는 경우에도 화자 특성을 크게 보존했음을 보여줍니다. 우리의 관찰에 따르면 모델은 배경 잡음을 유지했으며, 이는 제공된 데모 샘플에서 확인되었습니다.그림 2는 두 쌍의 입력 및 생성된 음성 신호에 대한 멜 스펙트로그램을 비교합니다.첫 번째 예에서 입력 음성은 중간 주파수 범위에서 주기적 잡음을 포함했습니다.SpeechX는 2초에서 시작하는 기간 동안 전경 음성만 선택적으로 수정하면서 전체 입력 기간 동안 이 배경 잡음을 유지했습니다.두 번째 예에서도 비슷한 관찰이 가능한데, 여기서 변경 사항은 음성 콘텐츠의 전반부에 적용되었습니다.요약하면, 결과는 SpeechX 모델이 화자 신원과 배경 잡음을 유지하면서 잡음이 있는 음성을 편집하는 데 효과적임을 보여줍니다.미래 10 잠재적인 혼란을 피하기 위해, 우리의 실험 설정은 원래 VALL-E 작업에서 사용된 비연속적 평가 구성과 일치한다는 점에 유의해야 합니다.표 IV 다중 작업 학습의 영향.무작위 초기화가 있는 모델의 경우, 우리는 800K 반복을 위해 모델을 학습했습니다. VALL-E 초기화가 있는 모델의 경우, 모델은 먼저 ZERO-SHOT TTS 작업으로 400K 반복으로 훈련된 다음, 다양한 작업으로 400K 반복으로 추가로 업데이트되었습니다.ZS: ZERO-SHOT, CSE: 깨끗한 음성 편집, NSE: 잡음이 있는 음성 편집, NS: 잡음 억제, SR: 음성 제거, TSE: 대상 화자 추출.ZERO-SHOT TTS 초기화.훈련 작업 WER↓ SIM↑ 깨끗한 음성 편집 WER↓ SIM↑ 잡음이 있는 음성 편집 WER↓ SIM↑ 잡음 억제 WER↓ DNSMOS↑ 음성 제거 MCD↓ 대상 화자 추출 WER DNSMOS↑ Rand. Rand. ZS-TTS 5.0.VALL-E ZS-TTS + CSE + NSE + NS + SR + TSE CSE 5.0.8.0.15.0.2.3.3.3.3.5.0.VALL-E NSE 12.0.VALL-E NS 4.3.VALL-E SR 3.VALL-E TSE 3.3.VALL-E ZS-TTS+CSE + NSE + NS + SR + TSE 4.0.5.0.13.0.2.3.3.2.3.사전 편집 우리는 함께 정자로 나갈 것입니다.내 창문에서 안뜰로 내려가는 길이 있습니다.주파수(kHz)사전 편집 주파수(kHz)0.1.2 2.시간(초)3.우리는 함께 정자로 나갈 것입니다.안뜰로 내려가는 길이 있습니다.0.1.2 2.시간(초)3.사전 편집 그는 형식에 대한 광신자이며 그는 지루해질 정도로 3인칭으로 나에게만 말을 걸었다 주파수(kHz) 시간(초) 편집 후 그는 의전에 집착했고 항상 지루해질 정도로 3인칭으로 나에게 말했다 주파수(kHz) 8 42 시간(초) 그림 2. 편집 전 및 편집 후 노이즈 신호의 Mel 스펙트로그램. 편집 전 신호는 SpeechX의 신경 코덱 언어 모델에 의해 이루어진 변경 사항을 강조하기 위해 중간 처리 없이 EnCodec 압축 및 압축 해제를 적용하여 얻었습니다. 코덱의 영향에 대한 논의는 섹션 V-D6을 참조하십시오. 이 작업은 노이즈 복제 기능을 정량적으로 평가하는 지표를 개발해야 합니다. 3) 노이즈 억제 및 대상 화자 추출에서 텍스트 입력의 효과: SpeechX를 사용하면 음향 프롬프트만을 입력으로 사용하여 노이즈 억제 및 대상 화자 추출을 수행하는 것이 가능합니다. SpeechX 모델에 추가 텍스트 입력을 통합하는 효능을 평가하기 위해 음향 프롬프트만 모델 입력으로 사용한 잡음 억제 및 대상 화자 추출 실험을 수행했습니다. 구체적으로, 잡음 억제를 위한 입력은 잡음이 있는 음성으로 구성되었고, 대상 화자 추출을 위해 혼합된 음성과 대상 화자의 등록 오디오로 구성되었습니다. 실험 결과는 표 III에 나와 있습니다. 두 작업 모두 텍스트 입력을 생략하면 WER이 눈에 띄게 증가했지만 DNSMOS 및 PESQ 점수의 저하는 적당했습니다. 이러한 결과는 텍스트 입력을 활용하는 것이 출력 음성의 이해도를 높이는 데 특히 유익하다는 것을 시사합니다. 대상 화자 추출에서 DNSMOS 점수에 상당한 영향이 관찰되었는데, 이는 텍스트 입력이 대상 화자의 음성을 방해하는 화자로부터 분리하는 데 도움이 된다는 것을 나타냅니다. 특히 음향 프롬프트에만 의존하면 WER이 저하되었지만 달성된 WER은 여전히 기준 전문가 모델의 WER과 유사했습니다. 4) 멀티태스크 훈련의 효과: 표 IV는 멀티태스크 훈련의 영향을 평가하기 위한 실험 결과를 보여줍니다. 처음 두 행에서는 무작위 초기화에서 훈련된 두 모델의 결과를 제시합니다. 또한, 우리는 각각 깨끗하고 잡음이 있는 음성 편집, 잡음 억제, 음성 제거 및 대상 화자 추출 작업을 위해 단일 작업 훈련 데이터로 SpeechX 모델을 훈련했습니다. 이러한 단일 작업 모델은 멀티태스크 훈련의 영향을 평가하기 위한 추가 전문가 모델 역할을 했습니다. 새로 훈련된 모델은 모두 400k 반복을 위한 제로샷 TTS 작업으로 훈련된 VALL-E 체크포인트에 의해 초기화되었습니다. 그런 다음 단일 작업 훈련 데이터로 400k 반복을 위해 추가로 업데이트되었습니다. 처음 두 행을 비교하면 멀티태스크 훈련이 추가 작업을 처리하는 능력을 습득하는 것과 함께 제로샷 TTS 작업에서 WER을 크게 개선했음을 알 수 있습니다. 이는 모델을 다양한 데이터에 노출시키면 단일 작업 성능을 개선할 수 있는 멀티태스크 훈련의 효능을 강조합니다. 표의 아랫부분에서, 우리는 먼저 잡음 억제 및 대상 화자 추출 작업을 위해 훈련된 단일 작업 모델이 다중 작업 모델(마지막 행)에 비해 상당히 더 나쁜 WERS를 보였다는 것을 관찰했습니다. 반면에 음성 편집 작업을 위해 훈련된 단일 작업 모델은 다중 작업 모델보다 약간 더 나은 WER을 보였습니다. SIM, DNSMOS, MCD와 같은 다른 품질 지표는 거의 동일했습니다. 이러한 관찰 결과는 TTS 및 음성 편집 작업을 통해 학습한 텍스트 조건 음성 생성 기능이 음성 향상 작업에서 높은 명료도를 달성하는 데 특히 유익하다는 것을 시사합니다.표 V 훈련 중 작업 추가의 효과. ZS: ZERO-SHOT, SE: 음성 편집, NS: 잡음 억제, SR: 음성 제거, TSE: 대상 화자 추출. Zero-shot TTS 훈련 과제 WER↓↓ SIM↑ 음성 편집(클린/노이즈) WER↓ SIM↑ 노이즈 억제 WER↓↓ DNSMOS↑ 음성 제거 MCD↓ 대상 화자 추출 WER↓ DNSMOS↑ ZS-TTS 5.0.ZS-TTS + SE 4.0.5.79 1 13.ZS-TTS + SE + NS/SR 5.0.6.ZS-TTS + SE + NS/SR + TSE 4.0.5.13.13.0.76 / 0.0.77 / 0.0.76 / 0.2.3.2.3.3.3.2.3.표 VI BPE 기반 모델과 음소 기반 모델 간의 비교. 잡음 억제 대상 화자 추출 모델 WER DNSMOS↑ PESQ↑ BPE 2.3.2.Phoneme 2.3.2.WER DNSMOS↑ PESQ↑ 2.46 3.2.2.3.2.Zero-shot TTS WER SIM↑ 8.25 0.4.66 0.Clean speech editing WER↓ SIM↑ Noisy speech editing WER↓ SIM↑ Speech removal MCD↓ 7.5.0.13.0.3.0.13.0.3.우리는 훈련 중에 작업의 하위 집합을 사용하여 다른 작업 간의 잠재적인 상호 작용을 탐색하는 실험을 추가로 수행했습니다.이러한 실험의 결과는 표 V에 제시되어 있습니다.특히, VALLE 모델과 전체 작업 집합을 사용하는 완전히 훈련된 SpeechX 모델 외에도 두 개의 추가 SpeechX 모델을 훈련했습니다.하나는 zero-shot TTS 및 음성 편집 작업을 위해 독점적으로 훈련되었고, 다른 하나는 zero-shot TTS, 음성 편집, 잡음 억제 및 음성 제거 데이터에서 훈련되었습니다. 새로 훈련된 모델은 400k 반복으로 VALLE 체크포인트에 의해 초기화된 다음, 훈련 세트의 하위 세트에 의해 업데이트되었습니다. 이 실험에서 반복 횟수는 작업 수에 따라 비례적으로 감소했으며, 전체 작업 세트에서 최대 반복 횟수는 400k였습니다. 결과에 따르면, 훈련 중에 음성 편집을 포함시키면 제로샷 TTS에 대한 WER이 향상되고 모델이 음성 편집 작업에 대해 학습할 수 있었습니다. 제로샷 TTS와 음성 편집 간의 강력한 유사점을 고려할 때, 이러한 개선은 음성 편집 훈련 작업이 훈련 데이터 분포에 추가적인 변형을 도입했기 때문일 수 있습니다. 훈련 중에 노이즈 억제 및 음성 제거 작업을 추가로 포함시키면 깨끗한 음성 편집 성능이 저하되는 반면, 노이즈가 있는 음성 편집의 성능은 동시에 향상되었습니다. 이는 이러한 추가 작업의 노이즈가 있는 음성 샘플에 모델을 노출시키면 깨끗한 음성 생성을 희생하고 음향 노이즈에 대한 모델의 견고성이 향상되었음을 시사합니다. 또한, 훈련 데이터에 대상 화자 추출 작업을 도입해도 모델의 잡음 억제 및 음성 제거 능력이 저하되지 않는다는 점이 주목할 만합니다.5) 음소 대 바이트 쌍 인코딩(BPE): 표 VI는 사용된 텍스트 프롬프트 유형, 즉 음소와 BPE를 기반으로 한 SpeechX 모델의 두 가지 변형 간의 성능 비교를 보여줍니다.BPE 기반 SpeechX는 잡음 억제 및 대상 화자 추출 작업에서 약간 더 나은 성능을 보인 반면, 음소 기반 SpeechX는 특히 WER에서 제로샷 TTS 및 깨끗한 음성 편집 작업에서 상당히 더 나은 성능을 보였습니다.잡음이 있는 음성 편집 및 음성 제거의 경우, 음소와 BPE는 유사한 결과를 보였습니다.표 VII 깨끗한 음성 및 잡음이 있는 음성에 대한 성능 지표에 대한 신경 코덱의 영향. 오디오 유형 WER↓ DNSMOS↑ PESQ↑ SIM↑ 원본 깨끗한 음성 → EnCodec 원본 노이즈가 있는 음성 EnCodec 1.3.4.1.1.2.2.0.3.2.1.0.5.2.1.0.성능 수준. 우리는 제로샷 TTS 및 깨끗한 음성 편집 작업에서 BPE로 관찰된 심각한 WER 저하가 모델 훈련 중에 ASR에 기반한 잘못된 전사본을 사용했기 때문이라고 추측합니다. BPE 단위는 음소보다 길기 때문에 ASR 오류율은 BPE에서 더 높은 경향이 있습니다. 결과적으로 이 모델이 텍스트 프롬프트에서 음성을 생성해야 하는 텍스트 기반 음성 생성 작업에 사용되었을 때 WER이 상당히 증가했습니다. 반면에 텍스트 프롬프트는 노이즈 억제 및 대상 화자 추출과 같은 음성 향상 작업을 위한 보충 정보로만 사용되었습니다. 결과적으로 이러한 경우 WER에 큰 영향을 미치지 않았습니다. BPE 단위가 실제 레이블을 사용하여 모델을 학습할 때 전반적인 성능이 더 좋을 수 있으며, 이는 향후 조사가 필요한 영역으로 남아 있습니다. 6) 현재 신경 코덱 모델의 한계: SpeechX의 성능은 음향 토큰화에 사용된 신경 코덱 모델의 정확도에 따라 본질적으로 제한됩니다. 이전 실험에서 SpeechX의 결과를 신경 코덱 처리 없이 얻은 참조(즉, 처리 없음 및 전문가 모델) 결과와 비교했다는 점에 유의해야 합니다. SpeechX의 결과를 보다 정확하게 해석하기 위해 중간 처리 없이 LibriSpeech 테스트 클린 데이터에 압축 및 압축 해제를 적용하여 성능 지표에 대한 EnCodec의 영향을 측정하는 추가 실험을 수행했습니다. 표 VII은 실험 결과를 보여줍니다. 코덱 모델로 신호를 처리하면 모든 지표에서 다양한 수준의 성능 회귀가 발생한다는 것이 분명합니다. 특히 PESQ 점수는 클린 음성 입력의 경우 4.64에서 2.69로 떨어졌습니다. 우리의 평가에 따르면 EnCodec은 약간 눈에 띄는 음성 품질 저하를 일으켰지만, 상당한 PESQ 저하가 부분적으로 PESQ 알고리즘과 EnCodec의 훈련 목표 간의 불일치 때문일 수 있습니다. 우리는 접근성과 이전 사용으로 인해 EnCodec을 사용했지만, 향후 연구에서는 다양한 음향 조건에서 음성을 처리하는 데 더 적합한 음향 토큰화 모델을 개발하여 이 문제를 해결해야 합니다. VI.
--- CONCLUSION ---
이 논문에서는 다양한 오디오텍스트 기반 음성 생성 작업(제로샷 TTS, 노이즈 억제, 음성 제거, 대상 화자 추출, 음성 편집 포함)을 처리할 수 있는 새로운 다재다능한 음성 생성 모델인 SpeechX를 설명했습니다. 노이즈 억제 및 대상 화자 추출을 위해 제안된 모델은 전사에 대한 지식을 통합하는 통합된 방법을 제공합니다. 또한 음성 편집과 관련하여 SpeechX는 상당한 양의 배경 노이즈가 포함된 음성 신호의 말한 내용을 수정할 수 있습니다. SpeechX는 텍스트 및 음향 프롬프트에 따라 음향 토큰을 생성하는 언어 모델링 방식을 채택하며, 여기서 추가 작업 종속 토큰은 멀티태스크 학습 프레임워크에 통합되어 제로샷 TTS를 넘어서는 다양한 음성 변환 기능을 지원합니다. 우리는 포괄적인 실험을 통해 SpeechX의 효능을 입증했습니다. 제안된 모델은 통합 생성 음성 모델을 향한 중요한 단계를 나타냅니다. 지원되는 작업 확장, 견고성 및 품질 향상(예: 지속 시간 제어를 통한 AR 생성 안정화[41] 및 개선된 신경 코덱 활용[42]) 및 보다 고급 컨디셔닝 메커니즘 개발(예: 풀린 음성 및 화자 정보 사용[43])을 통해 이 작업을 기반으로 추가 연구가 이루어질 수 있습니다. 참고문헌 [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, JD Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. HerbertVoss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei, &quot;언어 모델은 소수 샷 학습자입니다.&quot; 신경 정보 처리 시스템의 발전, 제33권, 2020년, 1877-1901쪽. [2] OpenAI, &quot;GPT-4 기술 보고서,&quot; arXiv 사전 인쇄본 arXiv:2204.06125, 2023. [3] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, B. Ommer, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성,&quot; IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 2022, 10674-10 685쪽. [4] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi 외, &quot;AudiOLM: 오디오 생성을 위한 언어 모델링 접근 방식,&quot; IEEE/ACM 오디오, 음성 및 언어 처리 저널, vol. 31, pp. 2523-2533, 2023. [5] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, JL Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. a. Bińkowski, R. Barreira, O. Vinyals, A. Zisserman, 및 K. Simonyan, &quot;Flamingo: few-shot learning을 위한 시각 언어 모델&quot;, 신경 정보 처리 시스템의 발전, 제 31권, 2523-2533쪽, 2023. 35, 2022, pp. 23 71623 736. [6] J. Wang, Z. Yang, X. Hu, L. Li, K. Lin, Z. Gan, Z. Liu, C. Liu, and L. Wang, “GIT: 비전 및 언어를 위한 생성적 이미지-텍스트 변환기,&quot; Transactions on Machine Learning Research, 2022. [7] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, &quot;DreamBooth: 주체 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정,&quot; IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 회의록, 2023, pp. 22 500-22510. [8] Z. Yang, M. Khademi, Y. Xu, R. Pryzant, Y. Fang, C. Zhu, D. Chen, Y. Qian, M. Gao, Y.-L. Chen, R. Gmyr, N. Kanda, N. Codella, B. Xiao, Y. Shi, L. Yuan, T. Yoshioka, M. Zeng 및 X. Huang, “i-Code V2: 비전, 언어 및 음성 데이터에 대한 자동 회귀 생성 프레임워크,&quot; arXiv 사전 인쇄 arXiv:2305.12311, 2023. [9] PK Rubenstein, C. Asawaroengchai, DD Nguyen, A. Bapna, Z. Borsos, F. d. Quitry, P. Chen, DE Badawy, W. Han, E. Kharitonov, H. Muckenhirn, D. Padfield, J. Qin, D. Rozenberg, T. Sainath, J. Schalkwyk, M. Sharifi, MT Ramanovich, M. Tagliasacchi, A. Tudor, M. Velimirović, D. 빈센트, J. 유, Y. Wang, V. Zayats, N. Zeghidour, Y. Zhang, Z. Zhang, L. Zilka 및 C. Frank, &quot;AudioPaLM: 말하고 들을 수 있는 대규모 언어 모델&quot;, arXiv 사전 인쇄 arXiv:2306.12925, 2023. [10] A. Agostinelli, TI Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, M. Sharifi, N. Zeghidour 및 C. Frank, &quot;MusicLM: 텍스트에서 음악 생성&quot;, arXiv 사전 인쇄 arXiv:2301.11325, 2023. [11] E. Casanova, J. Weber, CD Shulby, AC Junior, E. Gölge 및 MA Ponti, &quot;YourTTS: 제로샷을 향하여 모든 사람을 위한 다중 스피커 TTS 및 제로샷 음성 변환,&quot; 2022년 제39회 기계 학습 국제 컨퍼런스 논문집, 2709-2720쪽. [12] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen, P. Nguyen, R. Pang, I. Lopez Moreno, 및 Y. Wu, &quot;스피커 검증에서 다중 스피커 텍스트-음성 합성으로의 전이 학습&quot;, 신경 정보 처리 시스템의 발전, 2018. [13] E. Cooper, C.-I. Lai, Y. Yasuda, F. Fang, X. Wang, N. Chen, 및 J. Yamagishi, &quot;최신 신경 스피커 임베딩을 사용한 제로샷 다중 스피커 텍스트-음성&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스 논문집, 2020, 6184-6188쪽. [14] E. Casanova, C. Shulby, E. Gölge, NM Müller, FS de Oliveira, A. Candido Jr., A. da Silva Soares, SM Aluisio 및 MA Ponti, &quot;SCGlowTTS: 효율적인 제로샷 다중 스피커 텍스트 음성 변환 모델&quot;, ISCA INTERSPEECH 회보, 2021, pp. 3645–3649. [15] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar 등, &quot;VoiceBox: 규모에 따른 텍스트 기반 다국어 범용 음성 생성&quot;, 신경 정보 처리 시스템의 발전, vol. 36, 2024. [16] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao 및 F. Wei, &quot;신경 코덱 언어 모델은 음성 합성기에 대한 제로샷 텍스트입니다.&quot; arXiv 사전 인쇄 arXiv:2301.02111, 2023. [17] E. Kharitonov, D. Vincent, Z. Borsos, R. 마리니어, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour, &quot;말하고, 읽고, 촉구하세요: 최소한의 감독으로 고품질 텍스트 음성 변환&quot;, arXiv 사전 인쇄본 arXiv:2302.03540, 2023. [18] R. Huang, C. Zhang, Y. Wang, D. Yang, L. Liu, Z. Ye, Z. Jiang, C. Weng, Z. Zhao, and D. Yu, &quot;Make-A-Voice: 이산 표현을 통한 통합 음성 합성&quot;, arXiv 사전 인쇄본 arXiv:2305.19269, 2023. [19] Z. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei, &quot;외국어로 말하세요 자신의 음성을 가진 언어: 교차 언어 신경 코덱 언어 모델링,&quot; arXiv 사전 인쇄본 arXiv:2303.03926, 2023. [20] H. Bai, R. Zheng, J. Chen, M. Ma, X. Li 및 L. Huang, &quot;A³T: 음성 합성 및 편집을 위한 정렬 인식 음향 및 텍스트 사전 학습,&quot; 39회 기계 학습 국제 회의록, 2022, pp. 1399–1411. [21] Z. Jiang, Y. Ren, Z. Ye, J. Liu, C. Zhang, Q. Yang, S. Ji, R. Huang, C. Wang, X. Yin 외, &quot;Mega-TTS: 내재적 귀납적 편향이 있는 대규모 제로샷 텍스트 음성 변환,&quot; arXiv 사전 인쇄본 arXiv:2306.03509, 2023. [22] Q. Wang, H. Muckenhirn, K. Wilson, P. Sridhar, Z. Wu, J. Hershey, RA Saurous, RJ Weiss, Y. Jia, IL Moreno, &quot;VoiceFilter: 화자 조건 스펙트로그램 마스킹을 통한 대상 음성 분리,&quot; ISCA INTERSPEECH 회의록, 2019, 2728-2732쪽. [23] K. Žmolíková, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, L. Burget, 및 J. Černocký, &quot;SpeakerBeam: 음성 혼합에서 대상 화자 추출을 위한 화자 인식 신경망,&quot; IEEE Journal of Selected Topics in Signal Processing, 제13권, 제4호, 800-814쪽, 2019. [24] SE Eskimez, T. Yoshioka, H. Wang, X. Wang, Z. Chen, 및 X. Huang, &quot;개인화된 음성 향상: 새로운 모델 및 포괄적 평가,&quot; IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스 회의록, 2022, 356-360쪽. [25] K. Žmolíková, M. Delcroix, T. Ochiai, K. Kinoshita, J. Černocký, 및 D. Yu, &quot;신경 대상 음성 추출: 개요,&quot; IEEE Signal Processing Magazine, vol. 40, no. 3, pp. 8-29, 2023. [26] J. Serrà, S. Pascual, J. Pons, RO Araz, and D. Scaini, &quot;점수 기반 확산을 통한 범용 음성 향상,&quot; arXiv 사전 인쇄본 arXiv:2206.03065, 2022. [27] K. Kinoshita, M. Delcroix, A. Ogawa, and T. Nakatani, &quot;딥 신경망을 통한 텍스트 정보 음성 향상,&quot; ISCA INTERSPEECH 회의록, 2015, pp. 1760-1764. [28] K. Schulze-Forster, CSJ Doire, G. Richard 및 R. Badeau, &quot;심하게 손상된 음성에 대한 공동 음소 정렬 및 텍스트 정보 음성 분리&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스 논문집, 2020, 7274-7278쪽. [29] J. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazaré, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, E. Dupoux, &quot;Libri-Light: 제한적 또는 전혀 감독이 없는 ASR에 대한 벤치마크&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스 논문집, 2020, 7669-7673쪽. [30] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, I. Sutskever, &quot;제로샷 텍스트-이미지 생성&quot;, 제38회 기계 학습 국제 컨퍼런스 논문집, 2021, 8821-8831쪽. [31] K. Shen, Z. Ju, X. Tan, E. Liu, Y. Leng, L. He, T. Qin, S. Zhao, and J. Bian, &quot;NaturalSpeech 2: Latent diffusion models are natural and zero-shot speech and singer synthesizer,” in Proceedings of the 12th International Conference on Learning Representations, 2024. [32] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, &quot;High fidelity neural audio compression,&quot; Transactions on Machine Learning Research, 2023. [33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, AN Gomez, L. u. Kaiser, and I. Polosukhin, &quot;Attention is all you need,&quot; in Advances in Neural Information Processing Systems, vol. 30, 2017. [34] D. Snyder, G. Chen 및 D. Povey, &quot;MUSAN: 음악, 음성 및 소음 코퍼스,&quot; arXiv 사전 인쇄본 arXiv:1510.08484, 2015. [35] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, &quot;Conformer: Convolutionaugmented transformer for speech awareness,&quot; in Proceedings of ISCA INTERSPEECH, 2020, pp. 5036-5040. [36] CK Reddy, V. Gopal, and R. Cutler, &quot;DNSMOS: 노이즈 억제기를 평가하기 위한 비침투적 지각적 객관적 음성 품질 측정법,&quot; in Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, 2021, pp. 6493–6497. [37] AW Rix, JG Beerends, MP Hollier, and AP Hekstra, &quot;음성 품질의 지각적 평가(PESQ) - 전화 네트워크 및 코덱의 음성 품질 평가를 위한 새로운 방법,&quot; in Proceedings of IEEE International Conference on 음향, 음성 및 신호 처리, 제2권, 2001년, 749-752쪽. [38] H. Dubey, A. Aazami, V. Gopal, B. Naderi, S. Braun, R. Cutler, A. Ju, M. Zohourian, M. Tang, M. Golestaneh 외, &quot;ICASSP 2023 심층 노이즈 억제 챌린지,&quot; IEEE Open Journal of Signal Processing, 2024년. [39] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, L. Xie, &quot;DCCRN: 위상 인식 음성 향상을 위한 심층 복합 합성곱 순환 네트워크,&quot; ISCA INTERSPEECH 회의록, 2020년, 2472-2476쪽. [40] SE Eskimez, X. Wang, M. Tang, H. Yang, Z. Zhu, Z. Chen, H. Wang 및 T. Yoshioka, &quot;인간의 청취 및 실시간 자막: 음성 향상을 위한 다중 작업 훈련&quot;, ISCA INTERSPEECH 회의록, 2021, 2686-2690쪽. [41] Y. Song, Z. Chen, X. Wang, Z. Ma 및 X. Chen, “ELLA-V: 정렬 가이드 시퀀스 재정렬을 사용한 안정적인 신경 코덱 언어 모델링,&quot; arXiv 사전 인쇄본 arXiv:2401.07333, 2024. [42] H. Siuzdak, &quot;Vocos: 고품질 오디오 합성을 위한 시간 영역 및 푸리에 기반 신경 보코더 간 격차 해소,&quot; arXiv 사전 인쇄본 arXiv:2306.00814, 2023. [43] Z. Ju, Y. Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y. Liu, Y. Leng, K. Song, S. Tang 등, “NaturalSpeech 3: 인수화된 코덱 및 확산 모델을 사용한 제로샷 음성 합성,&quot; arXiv 사전 인쇄본 arXiv:2403.03100, 2024년.
