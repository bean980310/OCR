--- ABSTRACT ---
소프트웨어 개발은 현대 사회에서 혁신과 효율성을 주도하는 데 중요한 역할을 합니다. 이 역동적인 분야의 요구를 충족하기 위해 효과적인 소프트웨어 개발 도우미에 대한 필요성이 커지고 있습니다. 그러나 ChatGPT로 표현되는 기존의 대규모 언어 모델은 학습 데이터와 모델 가중치를 포함하여 접근성이 제한적입니다. LLAMA와 같은 다른 대규모 오픈소스 모델이 유망해 보였지만 여전히 인간의 의도를 이해하는 데 어려움을 겪습니다. 이 논문에서는 오픈소스 소프트웨어 개발 도우미인 SoTaNa를 소개합니다. SoTaNa는 ChatGPT를 활용하여 소프트웨어 엔지니어링 도메인을 위한 고품질 명령어 기반 데이터를 생성하고 매개변수 효율적인 미세 조정 방식을 사용하여 오픈소스 기반 모델인 LLAMA를 개선합니다. 우리는 SoTaNa가 Stack Overflow 질문에 답하는 데 효과적인지 평가하고 그 기능을 보여줍니다. 또한 코드 요약 및 생성 기능과 생성된 데이터의 양을 변화시키는 것이 모델 성능에 미치는 영향에 대해 논의합니다. 특히 SoTaNa는 단일 GPU에서 실행할 수 있으므로 더 광범위한 연구자가 액세스할 수 있습니다. 저희의 코드, 모델 가중치 및 데이터는 https://github.com/DeepSoftware Analytics/SoTaNa에 공개되어 있습니다. 1
--- METHOD ---
오픈소스 기반 모델, 즉 LLAMA(Touvron et al., 2023)를 강화합니다. 이 작업의 주요 목적은 기반 LLM(예: LLAMA)이 제한된 컴퓨팅 리소스를 활용하면서 개발자의 의도를 이해할 수 있도록 하는 것입니다. 구체적으로, 소프트웨어 엔지니어링(SE) 관련 데이터를 생성하기 위해 새로 생성된 인스턴스에 대한 요구 사항을 포함하는 특정 프롬프트를 사용하여 ChatGPT를 안내합니다(그림 2). ChatGPT가 원하는 출력 형식과 내용을 이해하도록 하기 위해 수동으로 주석이 달린 200개의 소프트웨어 엔지니어링 관련 인스턴스 시드 풀을 제공합니다. 이러한 인스턴스는 서로 다른 SE 작업에 속하며 각각은 (명령, 입력, 출력)으로 구성된 3튜플입니다. 생성 프로세스 동안 시드 풀에서 세 개의 인스턴스를 경험적으로 샘플링하여 데모로 만들고 이전에 생성된 데이터에서 두 개의 인스턴스를 추가하여 데모를 다양화합니다. 요구 사항과 데모를 포함한 전체 프롬프트는 그림 2에 나와 있습니다. 또한 인스턴스별 검사를 통해 요구 사항을 충족하지 않는 생성된 데이터를 자동으로 필터링하여 고품질 데이터를 보장합니다. 소프트웨어 엔지니어링을 위한 고품질 명령어 기반 데이터를 생성한 후, 매개변수 효율적 튜닝 방식인 Lora(Hu et al., 2021)를 채택하여 단일 A100 GPU를 사용하여 LLAMA를 미세 조정합니다. 이 미세 조정 프로세스를 통해 LLAMA는 제한된 컴퓨팅 리소스를 활용하면서 소프트웨어 엔지니어링 도메인에서 인간의 의도를 이해하고 의도와 관련된 응답을 생성할 수 있습니다. LLM 시드 풀 데모 데모 2 프롬프트 샘플 명령어 데모 쿼리 명령어 기반 데이터 생성 기초 모델 SoTaNa 그림 1: SoTaNa의 파이프라인. Stack Overflow 질문 답변 데이터 세트(Kou et al., 2022)에서 SoTaNa를 평가합니다. 인간 평가를 포함한 결과는 개발자를 지원하는 데 있어 모델의 효과를 보여줍니다. 또한 코드 요약(Shi et al., 2022a) 및 생성(Zan et al., 2022)에서 모델의 기능에 대해 간략하게 설명합니다. 또한 생성된 데이터의 다양한 볼륨이 모델 성능에 미치는 영향을 살펴봅니다. 이 작업의 주요 기여는 다음과 같이 요약할 수 있습니다. • 우리는 개발자의 의도를 이해하고 관련되고 유용한 응답을 생성할 수 있는 대규모 언어 모델을 기반으로 소프트웨어 개발 어시스턴트를 개발한 최초의 기업입니다. • 우리는 모델 가중치를 해제하고 소프트웨어 엔지니어링에 특별히 맞춤화된 고품질의 명령어 기반 데이터 세트를 제공합니다. 이러한 리소스의 가용성은 이 분야의 미래 연구와 발전을 촉진하는 것을 목표로 합니다. • 우리는 광범위한
--- EXPERIMENT ---
s는 Stack Overflow 질문에 효과적으로 답하고, 코드를 요약하고, 코드를 생성하는 SoTaNa의 역량을 입증합니다.2 배경 2.1 소프트웨어 개발 어시스턴트 소프트웨어 시스템에 대한 의존도가 높아짐에 따라 혁신적인 소프트웨어 솔루션에 대한 수요가 크게 증가했습니다(DRM Associates, 2002). 그러나 소프트웨어 개발 프로세스는 개발 라이프사이클 전반에 걸쳐 수많은 장애물에 직면하는 개발자에게 복잡하고 어려운 상태로 남아 있습니다. 소프트웨어 개발의 주요 과제 중 하나는 기술의 끊임없는 진화입니다(Nerur et al., 2005; Mikkonen et al., 2018; Cao and Ramesh, 2008). 새로운 기술이 등장하고 기존 기술이 발전함에 따라 개발자는 새로운 개념을 프로젝트에 지속적으로 적응하고 동화시켜야 합니다. 이러한 기술적 발전을 따라가는 것은 힘들고 시간이 많이 걸릴 수 있으며, 종종 프로젝트 일정이 지연되고 개발 비용이 증가합니다. 게다가 소프트웨어 아티팩트의 설계와 구현에는 세심한 계획과 세부 사항에 대한 주의가 필요합니다(Stone, 2010; Florac and Carleton, 1999). 개발자는 소프트웨어 구성 요소를 신중하게 설계하여 확장 가능하고 유지 관리 가능하며 프로젝트 목표와 일치하는지 확인해야 합니다. 추상적인 아이디어를 기능적 소프트웨어 솔루션으로 변환하는 프로세스에는 복잡한 의사 결정이 수반되므로 개발 단계는 소프트웨어 개발 수명 주기의 중요하고 리소스 집약적인 측면이 됩니다. 또 다른 중요한 과제는 개발 프로세스 중에 발생할 수 있는 예외와 오류를 처리하는 것입니다(Nuseibeh, 1996; Dellarocas and Klein, 2000). 소프트웨어의 복잡성이 증가함에 따라 버그와 문제가 발생할 가능성도 증가합니다. 이러한 문제를 효과적으로 식별, 디버깅 및 해결하는 데는 시간이 많이 걸리고 효율적으로 관리하지 않으면 진행을 방해할 수 있습니다. 이러한 과제를 해결하기 위해 개발 프로세스의 효율성과 효과성을 크게 개선할 수 있는 소프트웨어 개발 어시스턴트(Winograd, 1973)에 대한 긴급한 수요가 있습니다. 종종 인공 지능과 머신 러닝 알고리즘으로 구동되는 이러한 어시스턴트는 개발자의 작업 방식에 혁명을 일으킬 잠재력이 있습니다. 이러한 어시스턴트는 지능적이고 컨텍스트 인식 추천, 코드 제안 및 오류 분석을 제공함으로써 개발자의 능력을 향상시켜 개발 주기를 단축하고 소프트웨어 품질을 개선할 수 있습니다. 저희는 최근 강력한 대규모 언어 모델을 기반으로 소프트웨어 개발 어시스턴트를 개발한 최초의 기업입니다. 2.2 대규모 언어 모델 대규모 언어 모델(LLM)은 최근 자연어 처리(NLP)에서 강력한 도구로 부상하여 광범위한 작업에서 놀라운 성과를 보였습니다(Zhao et al., 2023; Brown et al., 2020; Zhang et al., 2022; Touvron et al., 2023; Workshop et al., 2022; Zeng et al., 2023). GPT-3(Brown 등, 2020), BLOOM(Workshop 등, 2022) 및 LLAMA(Touvron 등, 2023)를 포함한 이러한 모델은 일반적으로 수십억 개의 학습 매개변수가 있는 다층 Transformer 아키텍처(Vaswani 등, 2017)를 사용합니다. 이러한 모델은 종종 수천억 또는 심지어 수조 개의 토큰을 포함하는 레이블이 지정되지 않은 방대한 코퍼스에서 학습되므로 작업별 학습 데이터에 의존하지 않고도 상당한 도메인 지식을 포착할 수 있습니다. 자체 감독 사전 학습 접근 방식은 놀라운 성공에 기여한 중요한 요인이었습니다. 이러한 LLM 중에서 LLAMA는 7B에서 65B 매개변수에 이르는 개방적이고 효율적인 LLM 컬렉션이기 때문에 상당한 주목을 받았습니다. Transformer 디코더 아키텍처를 기반으로 하는 LLAMA는 수조 개의 토큰에서 학습되었으며 다양한 측면에서 뛰어난 성능을 보입니다(Touvron 등, 2023). 우리의 주요 목표는 LLAMA가 개발자의 의도를 이해하고 인간과 같은 응답을 생성할 수 있도록 하는 것입니다.2.3 LLM을 사용한 데이터 생성 인간이 주석을 단 명령어와 해당 응답으로 구성된 대규모 데이터 세트를 수집하는 것은 시간이 많이 걸리고 노동 집약적인 작업이 될 수 있습니다.이러한 과제를 극복하기 위해 연구자들은 LLM의 기능을 활용하여 이러한 데이터를 생성하는 대체 접근 방식을 모색했습니다.주목할 만한 방법 중 하나는 Self-Instruct(Wang et al., 2022a)로, 기존 명령어 컬렉션과 대규모 언어 모델을 활용하여 다양한 작업을 정의하는 보다 광범위한 명령어를 생성하고 종종 새로운 명령어를 도입하는 파이프라인을 제안합니다.이 아이디어를 바탕으로 Alpaca(Taori et al., 2023)는 Self-Instruct와 Text-Davinci-003(강력한 LLM)을 활용하여 52K 명령어 기반 데이터 세트 세트를 생성합니다.놀랍게도 이 데이터 세트를 사용하여 LLaMA-7B를 미세 조정할 때 Alpaca는 인간의 의도를 놀라울 정도로 잘 이해합니다. codealpaca(Chaudhary, 2023), alpacacot(Si et al., 2023), GPT4ALL(Anand et al., 2023), ShareGPT(Domeccleston, 2023), Dollyv2(Conover et al., 2023), BELLE(Ji et al., 2023a), Vicuna(Chiang et al., 2023), Koala(Geng et al., 2023), Baize(Xu et al., 2023b), Wizardlm(Xu et al., 2023a) 등의 후속 노력은 LLM을 사용한 데이터 증강을 더욱 탐구했습니다. 이전 작업이 범용 데이터 생성에 초점을 맞춘 반면, 우리 연구는 소프트웨어 엔지니어링 도메인을 위한 데이터를 생성하는 것을 목표로 합니다. 2.4 지침 미세 조정 지침 미세 조정의 주요 목적은 모델에 다양한 범위의 NLP 작업을 처리할 수 있는 기능을 제공하는 것입니다(Wei et al., 2021; Sanh et al., 2021; Mishra et al., 2021; Ji et al., 2023b; Wang et al., 2022b). 이러한 모델은 일반적으로 일정량의 NLP 작업을 통합된 형식으로 변환하고 다중 작업 학습 패러다임으로 훈련하여 작업 간 일반화를 용이하게 합니다. 결과적으로 종종 새로운 작업에서 유망한 결과를 얻습니다. 그러나 이러한 모델에서는 인간이 작성한 지침을 이해하는 것이 여전히 어렵습니다(Ouyang et al., 2022). OpenAI는 인간이 작성한 지침과 광범위한 작업에 걸쳐 해당 원하는 출력을 포함하는 상당한 양의 지침 기반 데이터 세트를 큐레이션하여 이러한 과제를 해결합니다(Ouyang et al., 2022). 이 데이터 세트와 인간 피드백(RLHF)을 통한 강화 학습(Ouyang et al., 2022; Ziegler et al., 2020)을 활용하여 모델이 인간 지침을 이해하고 인간과 유사한 응답을 생성할 수 있도록 합니다. 이러한 개발 라인은 ChatGPT(OpenAI, 2022) 및 GPT4(OpenAI, 2023)와 같은 인상적인 작업으로 이어졌습니다. Alpaca(Taori et al., 2023) 및 Baize(Xu et al., 2023b)와 같은 최신 모델은 ChatGPT를 활용하여 지침 기반 데이터를 생성하고 이에 따라 LLAMA를 미세 조정하여 LLAMA 모델이 인간의 의도와 일치하도록 합니다. 우리 모델의 주요 목표는 LLaMA가 개발자의 의도를 이해하도록 하여 소프트웨어 엔지니어링 영역으로 기능을 확장하는 것입니다.3 우리의 접근 방식 이 섹션에서는 접근 방식에 대한 자세한 개요를 제시합니다.이전 연구(Wang et al., 2022a; Taori et al., 2023)를 바탕으로 먼저 ChatGPT(Text-Davinci-003)를 활용하여 소프트웨어 엔지니어링 영역에 대한 명령어 기반 데이터를 자동으로 생성합니다.그런 다음 매개변수 효율적인 미세 조정 접근 방식인 Lora(Hu et al., 2021)를 채택하여 새로 생성된 데이터로 LLAMA(개방적이고 효과적인 대규모 언어 모델)를 조정합니다.목표는 제한된 컴퓨팅 리소스로 LLaMA가 인간의 명령어를 더 잘 이해하도록 하는 것입니다.3.1 자동 데이터 생성 소프트웨어 엔지니어링 관련 데이터를 효과적으로 생성하기 위해 작업 설명(파란색), 데이터 생성 요구 사항(노란색), 데모(녹색)로 구성된 프롬프트(그림 2)를 설계합니다. 데이터 생성 요구 사항은 Alpaca(Taori et al., 2023)에서 가져온 것이며, 새로 생성된 예제가 소프트웨어 엔지니어링 도메인과 관련성이 있는지 확인하기 위한 지침 역할을 합니다. 데모는 시드 풀에서 무작위로 샘플링됩니다. 시드 풀을 구성하기 위해 먼저 그림 2에 표시된 프롬프트를 사용하고 매번 Alpaca의 52K 데이터 세트에서 데모로 3개의 인스턴스를 무작위로 샘플링하여 프롬프트에 넣습니다. 그런 다음 20개의 다양한 소프트웨어 엔지니어링 관련 작업 지침 세트를 생각해 내라는 요청을 받습니다. 이러한 작업 지침은 GPT 모델에 제공되고 지침을 완료하기 위해 GPT 모델을 평가합니다. 요구 사항: 1. 코딩, 디버깅, 테스트, 문서화 등 다양한 유형의 소프트웨어 엔지니어링 작업을 포함합니다. 9. 지침은 1~2문장 길이여야 합니다. 필수 문장이나 질문이 허용됩니다. ### 예제 지침: 컴퓨터 과학에서 스택의 개념을 설명합니다. 입력:<noinput> 출력: 스택은 요소를 위에서만 추가하거나 제거할 수 있는 데이터 구조입니다(&quot;푸싱&quot; 및 &quot;팝핑&quot;이라고 함). 후입선출(LIFO) 데이터 구조이며 문자열 반전, 산술 표현식 평가 등과 같은 많은 작업에 사용할 수 있습니다. ### 예제 지침: 그림 2: 데이터 생성에 사용된 프롬프트. 위의 프롬프트를 사용하여 ChatGPT를 쿼리하여 500개의 인스턴스를 생성합니다. 다음으로, 저자 두 명이 생성된 인스턴스가 올바르고 소프트웨어 엔지니어링 도메인과 관련이 있는지 평가합니다. 마지막으로, 동의가 있는 200개의 인스턴스를 시드 인스턴스로 선택합니다. 데이터 생성 프로세스 동안, 시드 풀에서 3개의 인스턴스를 경험적으로 통합하여 데모로 사용하고 이전에 생성된 데이터에서 추가로 2개의 인스턴스를 포함하여 다양성을 향상시킵니다. 데이터 품질을 보장하기 위해 필터를 적용하여 3튜플 형식을 따르지 않거나 영어가 아닌 예를 제거합니다. 또한 3개 단어 미만의 지침이 있는 예는 삭제합니다. 이 엄격한 프로세스를 통해 소프트웨어 엔지니어링 분야에 특화된 100K 명령어 기반 예제의 고품질 데이터 세트를 성공적으로 얻었습니다.3. 매개변수 효율적 튜닝 대규모 언어 모델이 인간의 의도를 이해하고 관련 응답을 생성할 수 있도록 하기 위해 이전 연구(Taori et al., 2023; Chiang et al., 2023)는 일반적으로 명령어 기반 데이터 세트의 모든 매개변수를 미세 조정하여 대규모 계산 리소스를 필요로 했습니다.반면에, 우리의 접근 방식은 매개변수 효율적 튜닝 접근 방식(Hu et al., 2021; Shi et al., 2023)에 초점을 맞춰 더 적은 리소스를 사용하여 LLM을 미세 조정합니다.이러한 접근 방식 중에서, GPT-3(Brown et al., 2020)와 같은 대규모 언어 모델을 미세 조정하는 효율성으로 알려진 Lora(Hu et al., 2021)를 적용하여 기초 모델 LLAMA를 조정합니다.특히, Lora는 사전 훈련된 모델 매개변수를 동결하고 각 Transformer 계층에 추가 훈련 가능한 저랭크 분해 행렬을 도입합니다. 예를 들어, y Wx 방정식을 갖는 선형 계층에서 WЄ Rnk가 사전 학습된 매개변수를 나타낼 때, 저랭크 행렬 A = Rnr 및 BЄ R™×k를 통합하여 y를 다음과 같이 계산합니다. = y = (W)x+(AW) x = WxBAx (1) 여기서 r은 A와 B의 랭크에 해당하며, r « min(n, k)입니다. A와 B의 가중치만 업데이트하여 학습 매개변수의 수를 n × k에서 (n + k) × r로 크게 줄인다는 점에 유의해야 합니다. 일반적으로 (AW)x를 a로 조정합니다. 여기서 a는 상수입니다. LLaMA는 다층 Transformer(Vaswani et al., 2017)에 기반하므로 효율적인 매개변수 튜닝과 향상된 전체 성능을 달성하기 위해 각 계층의 모든 선형 가중치에 저랭크 분해 행렬을 적용합니다. 4 실험 설계 4.1 평가 데이터 세트 우리는 주로 Stack Overflow 질문에 답하는 데 있어 SoTaNa의 효과를 검증하는 데 중점을 둡니다. 또한 코드 이해 및 생성 기능을 평가합니다.Stack Overflow 질문 답변: Stack Overflow 질문에 답변하는 모델의 능력을 평가하기 위해 질문 제목, 질문 본문, 긍정적인 점수를 받은 답변 게시물과 게시물 요약이 포함된 SoSum 데이터 세트(Kou et al., 2022)를 사용합니다.이 데이터 세트는 원래 게시물 요약 모델을 평가하기 위한 것이었지만 질문 답변(QA) 기능을 평가하는 데 재활용했습니다.특히 질문 제목과 본문을 모델에 제공하고 모델은 답변을 생성해야 합니다.506개의 질문으로 구성된 원래 테스트 세트에서 큰 코드 조각이나 이미지가 BIGBLOCK으로 대체되어 이해할 수 없게 된 86개의 질문을 제외했습니다.필터링 후 나머지 420개의 질문을 사용하여 평가를 진행합니다.코드 생성: 모델의 코드 생성 효과를 평가하기 위해 널리 사용되는 HumanEval(Chen et al., 2021) 데이터 세트를 활용합니다.이 데이터 세트는 Python에서 164개의 함수 수준 프로그래밍 문제로 구성되어 있습니다. 이 작업에서는 모델이 함수의 서명과 영어 설명을 기반으로 함수 본문을 생성해야 합니다. 평가에는 생성된 코드를 평가하기 위한 테스트 사례가 포함되며 문제당 평균 7.7개의 테스트 사례가 있습니다. 코드 요약: 모델의 코드 이해 능력을 평가하기 위해 TLCodeSum(Hu et al., 2018) 데이터 세트를 사용합니다. 이 데이터 세트는 일반적으로 코드 요약 모델을 평가하는 데 사용됩니다. 구체적으로, 코드 조각이 주어지면 모델은 코드의 의미를 설명하는 하나의 자연어 문장을 생성해야 합니다. 모델의 코드 이해 능력을 확인하기 위해 테스트 세트의 처음 100개 예제에 대한 평가를 수행합니다. 4.2 기준선 접근 방식의 효과를 평가하기 위해 SoTaNa를 두 가지 관련 모델인 LLAMA(Touvron et al., 2023)와 Alpaca(Taori et al., 2023)와 비교합니다. LLAMA(Touvron et al., 2023)는 7B에서 65B 매개변수에 이르는 개방형 대규모 사전 학습된 언어 모델 모음입니다. 이 모델은 Transformer 디코더(Vaswani et al., 2017)를 기반으로 구축되었으며 책, GitHub, Wikipedia, arXiv 등 다양한 출처의 약 1T 토큰으로 사전 학습되었습니다. 크기가 크기 때문에 65B 모델은 80G 메모리의 단일 A100 GPU 카드에 로드할 수 없습니다. 따라서 다른 세 가지 크기(7/13/30B)에 초점을 맞춥니다. 각각 LLaMA-7B, LLaMA-13B, LLaMA-30B로 표시합니다. Alpaca(Taori et al., 2023)는 LLaMA-7B 모델에서 파생되었으며 Text-Davinci003에서 생성한 52K 명령어 기반 데이터로 미세 조정되었습니다. 또한 동일한 52K 명령어 기반 데이터에서 Lora를 사용하여 LLAMA13B 및 LLAMA-30B를 더욱 미세 조정합니다. 결과 모델은 각각 Alpaca-7B, Alpaca-13B 및 Alpaca-30B로 표시됩니다. 이러한 모델은 제안된 SoTaNa에 대한 비교 지점으로 사용됩니다. 4.3 실험 설정 이전 연구(Xu et al., 2023b; Taori et al., 2023)에 따라 입력 시퀀스의 최대 길이를 512로 설정했습니다. Lora의 랭크 r과 상수 a는 8과 16으로 설정되었습니다. 메모리 사용량을 줄이고 학습 프로세스 속도를 높이기 위해 LLAMA 가중치를 8비트 정수 형식으로 초기화합니다. Lora의 매개변수의 경우 이전 연구(Hu et al., 2021)에 따라 행렬 A에 대한 무작위 가우시안 초기화를 채택하고 행렬 B는 0으로 설정합니다. 그 결과 BA 모델 모델 #LLAMA Param의 값이 생성됩니다. #로라파람. 훈련 시간 BLEU Meteor Rouge-L Cider LLaMa-7B 0.8.8.0.SoTaNa-7B SoTaNa-13B SoTaNa-30B 7B 8.4M 25h35m LLaMa-13B 0.4.6.0.13B 13.1M 39h10m LLaMa-30B 0.4.5.0.30B 25.6M 48h02m Alpaca-7B 1.6.12.0.Alpaca-13B 1.7.13.0.표 1: SoTaNa의 통계. Alpaca-30B 1.7.13.0.SoTaNa-7B 1.7.12.0.SoTaNa-13B 1.7.12.0.SoTaNa-30B 1.8.13.0.학습 시작 시 0이 됩니다. LLAMA의 각 계층에 있는 모든 선형 가중치에 저랭크 분해 행렬을 주입합니다. Lora 매개변수의 수는 표 1에 나와 있습니다. Adam 옵티마이저를 사용하여 배치 크기 512와 학습 속도 1e-4로 Lora 매개변수를 업데이트합니다. Lora 매개변수의 드롭아웃 비율은 0.05로 설정됩니다. LLAMA-7B, LLaMA-13B 및 LLaMA-30B는 각각 5, 5 및 3 에포크에 대해 미세 조정됩니다. 모든 실험은 NVIDIA A10080GB GPU에서 수행됩니다. 우리는 7B, 13B 및 30B를 갖는 SoTaNa를 각각 SoTaNa-7B, SoTaNa-13B 및 SoTaNa30B로 표시합니다. 훈련 시간을 포함한 각 모델의 통계는 표 1에 나열되어 있습니다. 4.4 평가 지표 우리는 BLEU(Papineni et al., 2002), Meteor(Banerjee and Lavie, 2005), Rouge-L(Lin, 2004) 및 Cider(Vedantam et al., 2015)의 네 가지 지표를 통해 Stack Overflow 질문에 대한 생성된 답변의 품질과 주어진 코드 조각에 대한 생성된 요약의 품질을 평가합니다. 생성된 코드 요약을 측정하는 데 사용되는 BLEU의 많은 변형이 있습니다(Shi et al., 2022a). 우리는 인간의 지각과 가장 상관 관계가 있는 BLEU-DC(평활화 함수 4를 갖는 문장 수준 BLEU)를 선택했습니다(Shi et al., 2022a). 또한 코드 생성 모델을 평가하기 위해 이전 연구(Chen et al., 2021)에 따라 널리 사용되는 Pass @ 1을 평가 지표로 사용했습니다.5 실험 결과 5.1 Stack Overflow 질의 응답 SoTaNa가 Stack Overflow 질문에 답변하는 데 얼마나 효과적인지 평가하기 위해 인간 평가를 포함한 광범위한 실험을 수행했습니다.5.1. 자동 평가 음수 점수가 없는 답변을 기준 진실로 간주하고 4개의 자동 지표(BLEU, Meteor, Rouge-L 및 Cider)를 사용하여 생성된 답변의 품질을 평가했습니다.결과는 표 2에 나와 있습니다.SoTaNa와 Alpaca 모두 모든 지표에서 LLaMA보다 성능이 우수한 것을 알 수 있는데, 이는 명령어 미세 조정을 통해 모델이 인간의 의도를 이해하고 인간과 유사한 응답을 생성하는 데 도움이 될 것임을 나타냅니다.그러나 LLAMA에 비해 개선되었음에도 불구하고 Alpaca와 SoTaNa는 4가지 지표에서 비교적 낮은 점수를 받았습니다. 토큰 기반 유사성에 기반한 이러한 자동 메트릭은 생성된 답변의 품질을 완전히 반영하지 못할 수 있습니다. 예를 들어, 질문이 &quot;PHP에서 파일 확장자를 가져오는(추출하는) 방법&quot;이고 해당 참조 답변이 &quot; <code>pathinfo()</code> &quot;인 표 3의 예를 고려해 보겠습니다. 많은 모델(LLaMA-30B, Alpaca-7B, Alpaca30B, SoTaNa-7B 및 SoTaNa-30B)이 PHP에서 파일 확장자를 추출하기 위해 pathinfo() 함수를 사용할 것을 올바르게 제안합니다. 그러나 답변은 BLEU, Rouge-L 및 Cider에서 낮거나 일관되지 않은 점수를 받았으며, 이는 이러한 메트릭이 답변 품질을 평가하는 데 한계가 있음을 보여줍니다. 구체적으로, 모든 답변은 정답 여부에 관계없이 BLEU, Rouge-L 및 Cider 측면에서 0점을 받습니다. Alpaca-7B와 Alpaca30B의 답변은 관련 없는 문장을 피함으로써 LLAMA-30B보다 성능이 뛰어나지만, LLAMA-30B의 Meteor 점수는 Alpaca-7B와 Alpaca-30B보다 높습니다. 따라서 우리 접근 방식인 SoTaNa의 효과를 종합적으로 연구하기 위해서는 인간 평가를 실시하는 것이 필요합니다. 5.1.2 인간 평가 이전 연구(Shi et al., 2022a,b, 2021)에서 영감을 얻어, 우리는 인간 평가를 실시하여 SoTaNa의 효과를 평가합니다. 우리는 테스트 세트에서 무작위로 50개의 질문을 선택하고 표 2에 나열된 9개의 접근 방식에서 생성된 답변을 수집합니다. 그런 다음, 우리는 450개의 <question title, question body, answer pairs for scoring. Specifically, we invite 10 volunteers with excellent English abilities and over three years of software development experience. Each volunteer Reference Model Question Title Question Body How to get(extract) a file extension in PHP? This is a question you can read everywhere on the web with various answers etc. However, there is always "the best way" and it should be on Stack Overflow. <code>pathinfo() 생성된 답변 BLEU Met. Rou.L Cid. LLAMA-7B 질문 제목: PHP에서 파일 확장자를 가져오는(추출하는) 방법은? 질문 본문: 이 질문은 다양한 답변과 함께 웹의 모든 곳에서 읽을 수 있습니다: 등. 그러나 항상 &quot;가장 좋은 방법&quot;이 있으며 Stack Overflow에 있어야 합니다. 0.1.0.0.LLAMA-13B Stackoverflow에서 질문에 답하세요. ### 출력: Stackoverflow에서 질문에 답하세요. 0.1.0.0.LLAMA-30B PHP에서 파일 확장자를 가져오는 가장 좋은 방법은 pathinfo() 함수를 사용하는 것입니다. ### 출력: PHP에서 파일 확장자를 가져오는 가장 좋은 방법은 pathinfo() 함수를 사용하는 것입니다. 0.6.57 0.0.0.6.0.0.0.2.0.0.0.5.0.0.Alpaca-7B Alpaca-13B Alpaca-30B SoTaNa-7B SoTaNa-13B SoTaNa-30B PHP에서 파일 확장자를 가져오는 가장 좋은 방법은 pathinfo() 함수를 사용하는 것입니다. 이 함수는 파일 이름, 확장자, 경로 및 파일에 대한 기타 정보가 포함된 배열을 반환합니다. PHP에서 파일 확장자를 가져오는(추출하는) 가장 좋은 방법은 explosive() 함수를 사용하는 것입니다. 이 함수는 문자열과 구분 기호를 사용하여 문자열 배열을 반환합니다. 구분 기호는 파일 확장자를 나머지 문자열과 구분하는 데 사용할 수 있습니다. 예를 들어 문자열이 &quot;myfile.txt&quot;인 경우 구분 기호를 &quot;.&quot;로 설정할 수 있고 explosive() 함수는 &quot;myfile&quot;과 &quot;txt&quot;의 두 요소가 있는 배열을 반환합니다. PHP에서 파일 확장자를 가져오는 가장 좋은 방법은 pathinfo() 함수를 사용하는 것입니다. 이 함수는 파일 확장자를 포함하여 주어진 경로에 대한 정보 배열을 반환합니다. 예를 들어, &quot;myfile.txt&quot;라는 파일이 있는 경우 pathinfo() 함수는 파일 확장자로 &quot;txt&quot;를 반환합니다. PHP에서 파일 확장자를 가져오는 가장 좋은 방법은 pathinfo() 0.function을 사용하는 것입니다. PHP에서 파일 확장자를 가져오는(추출하는) 가장 좋은 방법은 explosive() 함수를 사용하는 것입니다. 이 함수는 문자열을 가져와 지정된 구분 기호를 기준으로 배열로 분할합니다. 이 경우 구분 기호는 점(&quot;.&quot;)이며, 이는 파일 확장자를 나머지 파일 이름과 구분하는 데 사용됩니다. explosive() 함수는 배열 끝에 파일 확장자가 있는 배열을 반환합니다. PHP에서 파일 확장자를 가져오는(추출하는) 가장 좋은 방법은 pathinfo() 함수를 사용하는 것입니다. 이 함수는 파일 확장자를 포함하여 주어진 경로에 대한 정보가 들어 있는 배열을 반환합니다. 예를 들어, 주어진 경로가 &quot;/path/to/file.txt&quot;인 경우 pathinfo() 함수는 파일 확장자가 &quot;txt&quot;인 배열을 반환합니다. 표 3: 다양한 모델에서 생성된 결과.6.0.0.0.3.6 0.0.0.7.0.0.은 정렬(질문을 이해하고 관련 답변을 제공하는 정도), 정확성(올바른 정보와 유효한 해결책을 제공하는 정도), 가독성(문법적 정확성, 유창함 및 형식 수준), 자신감(평가에 대한 자신감 정도)의 네 가지 측면을 기반으로 생성된 답변에 0~4점(점수가 높을수록 더 나은 품질)을 할당하도록 요청받았습니다.각 쌍은 두 명의 자원봉사자가 평가하고 최종 점수(자신감 제외)는 평가의 평균입니다.자세한 채점 기준, 예 및 해당 설명은 표 4에 나와 있습니다.인간 점수의 신뢰성을 보장하기 위해 자원봉사자 간의 일치도를 평가하기 위해 Krippendorff의 알파(Hayes 및 Krippendorff, 2007) 및 Kendall 순위 상관 계수(Kendall의 Tau)(Kendall, 1945) 값을 계산했습니다. 크리펜도르프의 알파 값은 약 0.9이고, 쌍별 켄달의 타우 값은 0.75에서 0.96까지로, 10명의 자원봉사자 간에 높은 수준의 일치도를 나타냅니다. 게다가, 신뢰성을 더욱 높이기 위해, 낮은 신뢰 점수(2 미만)를 가진 레이블이 지정된 결과를 다른 선임 자원봉사자가 다시 확인하도록 했습니다. 인간 평가 결과는 표 5에 나와 있습니다. 점수 범주 정렬 정확도 가독성 신뢰도 채점 기준 답변은 전혀 관련이 없으며, 질문의 주제와 관련이 없는 내용을 포함하고 있습니다. 답변은 주제와 어느 정도 관련이 있지만, 질문과의 연결이 약하고 문제에 직접적으로 초점을 맞추지 않았습니다. 답변은 관련이 있으며, 질문의 주제에 대한 이해를 보여주지만, 문제의 모든 측면이나 뉘앙스를 포함하지 않을 수 있습니다. 답변은 매우 관련이 있으며, 질문의 주제에 대한 깊은 이해를 보여주고 문제의 모든 측면과 긴밀하게 연결되어 있습니다. 답변은 전혀 틀렸거나, 잘못된 정보를 제공하거나, 잘못된 해결책을 제안합니다. 답변에는 일부 올바른 정보가 포함되어 있지만 상당한 부정확성이나 오해의 소지가 있습니다. 답변은 대체로 정확하며 사소한 오류나 누락만 있습니다. 답변은 완전히 정확하며 올바른 정보와 타당한 해결책을 제공합니다. 답변은 문법, 구조가 좋지 않거나 전문 용어가 지나쳐 이해하기 매우 어렵습니다. 답변은 다소 이해하기 어렵거나 문법 오류와 불분명한 설명이 있습니다. 답변은 명확하고 잘 구성되었으며 사소한 문법 오류나 개선의 여지가 있습니다. 답변은 매우 명확하고 잘 구성되었으며 문법 오류가 없어 이해하기 쉽습니다. 평가자는 답변 평가에 전혀 자신감이 없으며 할당된 점수에 대해 확신이 없습니다. 평가자는 평가에 대한 자신감이 낮으며 할당된 점수에 대해 의심할 수 있습니다. 평가자는 평가에 상당히 자신감이 있으며 할당된 점수에 대해 약간의 불확실성만 있습니다. 평가자는 평가에 매우 자신감이 있으며 할당된 점수에 대해 확신을 가지고 있습니다. / / 예 고양이는 관리가 간편하고 독립적이기 때문에 훌륭한 애완동물입니다. 파일 이름을 보면 파일 유형을 확인할 수 있습니다. PHP에서는 다음을 사용하여 파일 확장자와 이름을 찾을 수 있습니다. PHP에서 파일 확장자를 찾으려면 구분 기호로 파일 이름을 나누고 마지막 부분을 검색하면 됩니다. PHP에서 &#39;strlen()&#39; 함수를 사용하여 파일 확장자를 찾습니다. pathinfo() 함수를 사용합니다. 이 함수는 확장자를 직접 반환합니다. PHP에서 pathinfo()를 사용하여 확장자와 filedir을 포함한 파일 정보를 가져옵니다. PHP에서 pathinfo() 함수를 사용하여 파일 확장자를 추출합니다. $extension pathinfo( $filename, PATHINFO_EXTENSION); PHP 파일 확장자 가져오기 메서드 apply for find out. php use pathinfo get file info eg extenion,basenamee, filenme =pathinfo()를 사용하여 확장자 추출: $extension = pathinfo($filename, PATHINFO_EXTENSION); PHP에서 pathinfo() 함수를 사용하여 파일 확장자를 추출합니다. Sextension pathinfo($filename, PATHINFO_EXTENSION) 설명 이 답변은 애완동물을 논의하기 때문에 전혀 관련이 없습니다. 애완동물은 PHP에서 파일 확장자를 추출하는 주제와 관련이 없습니다. 이 답변은 파일 유형 결정을 언급하기 때문에 주제와 어느 정도 관련이 있지만 PHP에서 파일 확장자를 추출하는 직접적인 솔루션을 제공하지 않습니다. 이 답변은 파일 확장자를 언급하기 때문에 관련이 있지만 &quot;방법&quot;과 관련된 실질적인 솔루션이 없습니다. 이 답변은 PHP에서 파일 확장자를 찾는 방법을 제안하기 때문에 매우 관련이 있지만 전적으로 정확하지는 않을 수 있습니다. 이 답변은 &#39;strlen()&#39; 함수가 파일 확장자를 추출하는 것이 아니라 문자열의 길이를 찾는 데 사용되기 때문에 전적으로 틀렸습니다. 이 답변은 &#39;pathinfo()&#39;를 사용하도록 제안하기 때문에 부분적으로 맞지만 확장자가 아닌 배열을 반환합니다. 이 답변은 파일 정보를 가져오는 올바른 함수를 언급하기 때문에 대체로 정확합니다. 그러나 &#39;filedir&#39; 대신 &#39;dirname&#39;이어야 합니다. 답변은 완벽히 정확하며 올바른 PHP 함수와 예제를 제공합니다. 문법과 문장 구조가 좋지 않아 답변을 이해하기가 매우 어렵습니다. 구체적인 예와 적절한 문법이 부족하여 답변을 이해하기가 다소 어렵습니다. 답변은 코드 예를 제공하지만 불필요한 기호 &quot;==&quot;로 인해 가독성이 떨어집니다. 답변은 매우 명확하고 잘 구성되었으며 문법 오류가 없어 이해하기 쉽습니다. 표 4: 채점 기준. &quot;PHP에서 파일 확장자를 가져오는(추출하는) 방법?&quot;에 대한 예제. LLAMA는 질문을 이해하고 올바른 솔루션을 제공하는 데 어려움을 겪습니다. 생성된 답변은 일반적으로 이해하기 어렵고 문법 오류가 있으며 설명이 명확하지 않습니다. 반면에 SoTaNa와 Alpaca는 모두 질문을 이해하고 올바른 답변을 생성하는 측면에서 LLAMA보다 상당히 우수한 성과를 보입니다. SoTaNa와 Alpaca의 답변은 일반적으로 명확하고 잘 구성되었으며 문법 오류가 없어 이해하기 쉽습니다. 놀랍게도, 우리 모델(SoTaNa)은 세 가지 측정 항목 모두에서 모든 접근 방식 중에서 가장 뛰어난 성과를 보였으며, 문법적 정확성, 유창성, 좋은 형식을 보장하면서 질문을 이해하고 적절하고 정확한 답변을 제공하는 탁월한 능력을 보여줍니다.모델 LLaMa-7B LLaMa-13B LLaMa-30B Alpaca-7B 정렬 정확도 가독성 0.08(±0.27) 0.35(±0.61) 1.08(±1.21) 2.60(±0.63) 2.86(±0.40) 0.11(0.34) 0.02(±0.14) 0.20(±0.53) 0.14(±0.40) 0.95(±1.13) 0.70(±1.04) 1.97(±0.85) 1.36(±1.03) Alpaca-13B 2.52 (±0.71) 2.10 (±1.10) Alpaca-30B 2.52 (±0.67) 2.04 (±1.02) 2.90 (±0.30) SoTaNa-7B 2.20 (±0.69) 1.62 (±1.09) 2.69 (±0.48) SoTaNa-13B 2.42 (±0.80) 2.02 (±1.10) 2.71 (±0.59) SoTaNa-30B 2.52 (±0.74) 2.16 (±0.92) 2.90 (±0.30) 표 5: 인간 평가 결과.5.2 코드 요약 및 생성에 대한 실험 5.2.1 전체 결과 코드 요약 BLEU MET. Rou. Cid. 0.29 2.41 2.24 0.0.33 3.17 3.44 0.0.89 5.21 6.34 0.12.97 19.71 0.12.67 19.88 0.모델 코드 생성 P@LLAMA-7B 10.LLAMA-13B 15.LLAMA-30B 21.Alpaca-7B 10.3.Alpaca-13B 12.3.Alpaca-30B 18.4.14.51 22.25 0.SoTaNa-7B 10.3.SoTaNa-13B 18.3.SoTaNa-30B 23.4.14.32 19.96 0.13.02 19.52 0.15.29 22.93 0.표 6: 코드 요약 및 생성 결과. 코드 이해 및 생성에서 모델의 효과를 평가하기 위해 두 벤치마크에서 실험을 수행하고 모델 SoTaNa를 LLAMA 및 Alpaca의 모델과 비교했습니다. 실험 결과는 표 6에 나와 있습니다. 일반적으로 모델 크기가 클수록 코드 요약과 생성 모두에서 더 나은 성능을 보입니다. LLaMA에 비해 Alpaca와 SoTaNa는 코드 요약에서 상당한 개선을 보였습니다. 이는 인간의 지시에 따라 모델을 미세 조정하면 인간과 유사한 설명과 유사한 자연어 문장을 이해하고 생성하는 능력을 향상시킬 수 있음을 시사합니다. 또한 SoTaNa는 LLAMA의 코드 생성 기능이 향상된 반면 Alpaca의 미세 조정은 LLAMA의 코드 생성 기능을 감소시키는 것으로 보입니다. 이는 범용 데이터를 사용하여 모델을 미세 조정하면 코드 생성 기능이 잠재적으로 손상될 수 있음을 나타냅니다. 반면에 우리의 접근 방식에서와 같이 소프트웨어 엔지니어링 관련 데이터로 미세 조정하면 모델의 코드 생성 능력이 향상됩니다. 요약하자면, 실험 결과, 우리 모델은 명령어 기반 튜닝의 이점을 얻어 코드 요약과 생성 작업 모두에서 개선이 이루어짐을 보여줍니다. 또한, 소프트웨어 엔지니어링 도메인별 데이터를 미세 조정하면 코드 생성에 유리한 것으로 입증되었습니다. 5.2.2 데이터 볼륨의 영향 생성된 데이터 볼륨을 다양하게 하는 것이 모델 성능에 미치는 영향에 대해 추가 조사를 수행합니다. 구체적으로, 1k, 5k, 10k, 50k, 100k 생성된 예제의 다양한 크기의 데이터 세트를 사용하여 LLAMA 모델을 튜닝합니다. 그런 다음 코드 요약과 코드 생성 작업 모두에서 모델을 평가하고 그 결과를 그림 7에 표시합니다. 흥미롭게도, 코드 요약에서 개선을 보이는 SoTaNa-30B를 제외하고는 데이터 크기가 증가함에 따라 성능이 일관되게 증가하지 않는 것을 알 수 있습니다. 이러한 불일치에 대한 한 가지 가능한 이유는 평가 메트릭 문제일 수 있습니다. 5.1.1절에서 논의했듯이 자동 메트릭은 생성된 결과의 품질을 효과적으로 측정하지 못할 수 있습니다. 또한, 우리는 다양한 데이터 크기가 모델 성능에 미치는 영향이 다른 모델 크기에서 일관되지 않다는 것을 알아차렸습니다. 즉,
--- CONCLUSION ---
s 또는 한 모델 크기에 대해 도출된 결과는 다른 크기에 직접 적용할 수 없습니다. 예를 들어, SoTaNa-13B는 5K 데이터를 사용할 때 코드 요약에서 최고의 성능을 달성하는 반면, SoTaNa-7B와 SoTaNa-30B는 동일한 추세를 보이지 않았습니다. 코드 생성의 경우, SoTaNa-7B는 10K 데이터에서 학습했을 때 예외적으로 좋은 성능을 보이는 반면, SoTaNa-7B와 SoTaNa-30B는 동일한 데이터 세트 크기에서 최악의 성능을 보입니다. 이 결과는 특정 작업에 대한 데이터 크기와 모델 구성을 선택할 때 신중하게 고려하는 것이 중요함을 나타냅니다. 또한 일부 코드 관련 작업에서 모델의 성능을 정확하게 평가하기 위해 적절한 평가 지표를 사용해야 함을 강조합니다. 6 논의 우리 연구와 가장 밀접하게 관련된 작업은 StarChat(Lewis et al., 2023) 프로젝트입니다. 그들은 범용 데이터를 사용하여 특별히 코드용으로 설계된 StarCode라는 모델을 미세 조정하여 StarCoder(Li et al., 2023)가 대화를 처리할 수 있도록 합니다. 이와 대조적으로, 우리의 접근 방식은 소프트웨어 엔지니어링 관련 데이터를 사용하여 범용 대규모 언어 모델을 미세 조정하고 소프트웨어 개발 도우미를 만드는 것을 목표로 합니다. 점수 점수BLEU Meteor Rouge-L 21.19.2019.CiderBLEU-DC Meteor Rouge-L Cider 21.19.18.17.16.15.14.13.11.12.10 9.ScoreS 13.12.11.10.10-9.5.4.4.3.3.3.3.3.2.2.1.0.0.0.0.0.0.0.0.0.0.0.Number of Training Examples Number of Training Examples Figure 3: SoTaNa-7B on code summary Figure 4: SoTaNa-13B on code summary BLEU-DC 25Meteor Rouge-L Cider 27.SoTaNa-7B SoTaNa-13B 22.21.25.SoTaNa-30B 23.23.18.22.21.16.15.14.20.11.10.점수 18.17.16.15.1015.15.6.13.5.54.4.3.12.5 11,12/11.2.10.10.8:0.0.0.0.10.0 9. 학습 예제 수 그림 5: 코드 요약에 대한 SoTaNa-30B 7 유효성에 대한 위협 학습 예제 수 그림 6: 코드 생성에 대한 SoTaNa 그림 7: 다양한 데이터 크기의 영향. 데이터 품질. 또 다른 잠재적 우려 사항은 ChatGPT를 사용하는 데이터 생성 프로세스에 있습니다. 3단어 미만의 명령어와 같이 품질이 낮은 데이터 세트를 필터링했지만 생성된 데이터의 정확성에 대한 인간의 확인은 수행되지 않았음을 인정합니다. 생성된 데이터의 품질을 개선하기 위해 향후 작업에서는 인간 검사를 통합하거나 데이터 정확성과 신뢰성을 보장하기 위한 대체 접근 방식을 제안할 수 있습니다.평가 데이터 세트.실험은 널리 사용되는 데이터 세트에서 수행되었지만 각 다운스트림 작업에 사용할 수 있는 다른 데이터 세트가 있습니다.이러한 대체 데이터 세트는 구성 방법과 코퍼스 크기가 다를 수 있으며, 잠재적으로 모델 성능에 차이가 있을 수 있습니다.연구 결과와 결론의 견고성을 높이기 위해 더 광범위한 데이터 세트에 대한 추가 실험을 수행하여 결과의 일반화 가능성을 검증할 수 있습니다.평가 지표.우리는 일반적으로 사용되는 지표를 활용하여 모델의 성능을 평가했습니다.그러나 이러한 지표에는 고유한 한계가 있을 수 있음을 인식하는 것이 중요합니다.예를 들어, BLEU 및 METEOR와 같은 지표는 텍스트 유사성에 의존하며 두 문장 간의 의미적 유사성을 완전히 포착하지 못할 수 있습니다.이러한 한계를 해결하고 보다 포괄적인 평가를 얻기 위해 인간 평가도 수행했습니다.그러나 인간 평가는 노동 집약적이고 시간이 많이 걸릴 수 있다는 점에 유의하는 것이 좋습니다. 향후 연구에서는 인간의 지각과 더 일치하는 새로운 자동 평가 지표를 탐색할 것입니다.8 결론 이 논문에서는 효과적인 소프트웨어 개발 도구에 대한 증가하는 수요를 충족하도록 설계된 오픈소스 소프트웨어 개발 어시스턴트인 SoTaNa를 소개합니다.대규모 언어 모델(LLM)을 활용하여 SoTaNa는 소프트웨어 엔지니어링 작업에 맞게 조정된 고품질의 명령어 기반 데이터를 생성합니다.매개 변수 효율적인 미세 조정 방식을 사용하여 LLAMA 오픈소스 기반 모델의 기능을 향상시킵니다.인간 평가를 포함한 포괄적인 평가를 통해 SoTaNa는 다양한 Stack Overflow 쿼리에 대한 정확한 답변을 제공하여 개발자를 지원하는 데 효과적임을 보여줍니다.인간의 의도를 이해하고 소프트웨어 엔지니어링 작업에 특화된 상황에 맞는 적절한 응답을 생성하는 측면에서 기존 언어 모델보다 성능이 뛰어납니다.향후 작업에서는 LLM을 오픈소스 소프트웨어 개발 어시스턴트로 더욱 평가하기 위한 벤치마크를 도입하는 것을 목표로 합니다.이 벤치마크는 소프트웨어 엔지니어링 맥락에서 다양한 언어 모델의 성능을 평가하기 위한 표준화되고 체계적인 방식을 제공하여 더 나은 비교와 진행 상황 추적을 가능하게 합니다. 참고문헌 Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, Charles Sutton. 2018. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4):1–37. Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, Andriy Mulyar. 2023. Gpt4all: gpt-3.5-turbo에서 대규모 데이터 증류를 사용하여 어시스턴트 스타일 챗봇 훈련. https://github.com/nomic-ai/gpt4all. Satanjeev Banerjee와 Alon Lavie. 2005. METEOR: 인간 판단과의 상관관계가 향상된 MT 평가를 위한 자동 메트릭. IEEvaluation@ACL에서. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. 언어 모델은 few-shot 학습자입니다. 신경 정보 처리 시스템의 발전, 33:1877–1901. Lan Cao와 Balasubramaniam Ramesh. 2008. Agile 요구 사항 엔지니어링 관행: 경험적 연구. IEEE 소프트웨어, 25(1):60–67. Sahil Chaudhary. 2023. 코드 알파카: 코드 생성을 위한 명령어 따르기 라마 모델. https://github.com/sahil280114/ codealpaca. Mark Chen, Jerry Tworek, 전희우, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman 등 2021. 코드로 훈련된 대규모 언어 모델 평가. arXiv 사전 인쇄 arXiv:2107.03374. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica 및 Eric P. Xing. 2023. Vicuna: 90%* chatgpt 품질로 gpt-4를 감동시킨 오픈소스 챗봇. Mike Conover, Matt Hayes, Matt Mathur, Xiangrui Meng, Jianwei Xie, Jun Wan, Ali Ghodsi, Patrick Wendell, Patrick Zaharia. 2023. Hello Dolly: Open Models로 Chatgpt의 마법을 민주화합니다. Chrysanthos Dellarocas와 Mark Klein. 2000. 비즈니스 프로세스에서 예외를 처리하기 위한 지식 기반 접근 방식. Information Technology and Management, 1:155–169. Domeccleston. 2023. Sharegpt - 클릭 한 번으로 가장 거친 Chatgpt 대화를 공유하세요. 2023년 5월에 확인. DRM Associates. 2002. 신제품 개발 용어집. 2018년 7월 13일에 원본에서 보관됨. 2006년 10월 29일에 확인. William A Florac과 Anita D Carleton. 1999. 소프트웨어 프로세스 측정: 소프트웨어 프로세스 개선을 위한 통계적 프로세스 제어. Addison-Wesley Professional. Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, Dawn Song. 2023. Koala: 학술 연구를 위한 대화 모델. 블로그 게시물. Jonathan Grudin. 1994. 그룹웨어와 사회적 역학: 개발자를 위한 8가지 과제. ACM 커뮤니케이션, 37(1):92–105. Andrew F Hayes와 Klaus Krippendorff. 2007. 코딩 데이터를 위한 표준 신뢰도 측정에 대한 요청에 응답. 커뮤니케이션 방법 및 측정, 1(1):77–89. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2021. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685. Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu 및 Zhi Jin. 2018. API 지식을 전수받아 소스코드를 정리합니다. Ji Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma 및 Xiangang Li. 2023a. 대규모 언어 모델에 대한 명령 데이터 확장의 영향 탐색: 실제 사용 사례에 대한 실증적 연구. arXiv 사전 인쇄 arXiv:2303.14742. Ji Yunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang Niu, Baochang Ma 및 Xiangang Li. 2023b. 중국어에 대한 언어 모델에 따른 더 나은 교육을 위해: 교육 데이터 및 평가의 영향 조사. arXiv 사전 인쇄 arXiv:2304.07854. Wenxiang Jiao, Wenxuan Wang, JT Huang, Xing Wang 및 ZP Tu. 2023. chatgpt는 좋은 번역기인가요? 예, gpt-4를 엔진으로 사용. arXiv 사전 인쇄본 arXiv:2301.08745. Maurice G Kendall. 1945. 순위 문제에서 동점 처리. Biometrika, 33(3):239–251. Bonan Kou, Yifeng Di, Muhao Chen, Tianyi Zhang. 2022. Sosum: 스택 오버플로 게시물 요약 데이터 세트. 제19회 국제 채굴 소프트웨어 저장소 컨퍼런스 회의록, 247-251페이지. Tunstall Lewis, Lambert Nathan, Beeching Nazneen, Rajanian, Edward, Le Scao Teven, Han Sheon, Schmid Philipp, von Werra Leandro, Sasha Rush. 2023. starcoder로 코딩 어시스턴트 만들기. https://huggingface.co/blog/ starchat-alpha. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: 출처가 당신과 함께 하길! arXiv 사전 인쇄본 arXiv:2305.06161. Chin-Yew Lin. 2004. ROUGE: 요약 자동 평가 패키지. Text Summarization Branches Out에서. Tommi Mikkonen, Casper Lassenius, Tomi Männistö, Markku Oivo, Janne Järvinen. 2018. 지속적이고 협력적인 기술 이전: 실시간 산업 영향을 미치는 소프트웨어 엔지니어링 연구. 정보 및 소프트웨어 기술, 95:34–45. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi. 2021. 자연어 크라우드소싱 지침을 통한 교차 작업 일반화. arXiv 사전 인쇄본 arXiv:2104.08773. Sridhar Nerur, RadhaKanta Mahapatra, George Mangalaraj. 2005. 애자일 방법론으로 마이그레이션하는 과제. ACM 커뮤니케이션, 48(5):72-78. Bashar Nuseibeh. 1996. 존재와 존재하지 않음: 소프트웨어 개발에서 불일치 관리. 제8회 소프트웨어 사양 및 설계 국제 워크숍 회의록, 164-169페이지. IEEE. OpenAI. 2023. Gpt-4 기술 보고서. arXiv 사전 인쇄본 arXiv:2303.08774. TB OpenAI. 2022. Chatgpt: 대화를 위한 언어 모델 최적화. OpenAI. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. 인간의 피드백을 통해 지시를 따르도록 언어 모델 학습. 신경 정보 처리 시스템의 발전, 35:27730-27744. Kishore Papineni, Salim Roukos, Todd Ward, WeiJing Zhu. 2002. Bleu: 기계 번역의 자동 평가 방법. ACL에서. Victor Sanh, Albert Webson, Colin Raffel, et al. 2021. 멀티태스크 프롬프트 학습을 통해 제로샷 작업 일반화가 가능. arXiv:2110.08207 [cs]. Chenhui Shen, Liying Cheng, Yang You, Lidong Bing. 2023. 대규모 언어 모델은 추상적 요약을 평가하기에 좋은가? arXiv 사전 인쇄본 arXiv:2305.13091. Ensheng Shi, Yanlin Wang, Lun Du, Junjie Chen, Shi Han, Hongyu Zhang, Dongmei Zhang, Hongbin Sun. 2022a. 신경 코드 요약 평가에 관하여. 제44회 국제 소프트웨어 공학 컨퍼런스 회의록, 1597-1608쪽. Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun. 2021. 캐스트: 계층적 분할 및 추상 구문 트리 재구성을 통한 코드 요약 향상. 2021년 자연어 처리 경험적 방법 컨퍼런스 회의록, 4053-4062쪽. Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, Hongbin Sun. 2022b. Race: 검색 증강 커밋 메시지 생성. 2022년 자연어 처리 경험적 방법 컨퍼런스 회의록, 5520-5530쪽. Ensheng Shi, Yanlin Wang, Hongyu Zhang, Lun Du, Shi Han, Dongmei Zhang, Hongbin Sun. 2023. 사전 학습된 코드 모델의 효율적인 미세 조정을 향하여: 실험적 연구 및 그 이상. 제32회 ACM SIGSOFT 소프트웨어 테스트 및 분석 국제 심포지엄. Qingyi Si, Tong Wang, Naibin Gu, Rui Liu, Zheng Lin. 2023. Alpaca-cot: 명령어 데이터 수집 및 통합된 대규모 Inguage 모델 인터페이스를 갖춘 명령어 미세 조정 플랫폼. https://github.com/PhoebusSi/alpaca- CoT. Terry Stone. 2010. 디자인 프로세스 관리 디자인 구현: 실무 디자이너를 위한 필수 매뉴얼. Rockport Publishers. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto. 2023. Alpaca: 강력하고 재현 가능한 지시 따르기 모델. Stanford Center for Research on Foundation Models. https://crfm. stanford.edu/2023/03/13/alpaca. html. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar 등. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄본 arXiv:2302.13971. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 2017. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30. Ramakrishna Vedantam, C. Lawrence Zitnick, Devi Parikh. 2015. Cider: 합의 기반 이미지 설명 평가. CVPR에서. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi. 2022a. 자체 지시: 자체 생성 지침과 언어 모델 정렬. arXiv 사전 인쇄 arXiv:2212.10560. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap 등 2022b. 초자연적 지침: 1600개 이상의 nlp 작업에 대한 선언적 지침을 통한 일반화. 2022년 자연어 처리의 경험적 방법에 관한 컨퍼런스 진행, 페이지 5085-5109. Jason Wei, Maarten Bosma, Vincent Y. Zhao 등 2021. 미세 조정된 언어 모델은 제로샷 학습자입니다. arXiv:2109.01652 [cs]. 테리 위노그라드. 1973년. 복잡성의 장벽을 다시 무너뜨렸습니다. ACM Sigplan Notices, 10(1):13–30. BigScience Workshop, Teven Le Scao, Angela Fan, et al. 2022. Bloom: 176b-매개변수 오픈 액세스 다국어 언어 모델. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang. 2023a. Wizardlm: 복잡한 지침을 따르도록 대규모 언어 모델 강화. arXiv 사전 인쇄본 arXiv:2304.12244. Canwen Xu, Daya Guo, Nan Duan, Julian McAuley. 2023b. Baize: 셀프 채팅 데이터에 대한 매개변수 효율적 튜닝을 갖춘 오픈 소스 채팅 모델. arXiv 사전 인쇄본 arXiv:2304.01196. Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang 및 Jian-Guang Lou. 2022. 신경 모델이 nl2code를 만났을 때: 설문 조사. arXiv 사전 인쇄 arXiv:2212.09420. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong 및 Jie Tang. 2023. GLM-130b: 사전 훈련된 개방형 이중 언어 모델. 제11차 학습 표현에 관한 국제 회의(ICLR)에서. 수잔 장(Susan Zhang), 스티븐 롤러(Stephen Roller), 나만 고얄(Naman Goyal) 등 2022. Opt: 사전 훈련된 변환기 언어 모델을 공개합니다. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong 등 2023. 대규모 언어 모델에 대한 조사. arXiv 사전 인쇄 arXiv:2303.18223. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu 등 2020. 인간 선호도에 따라 언어 모델을 미세 조정합니다.
