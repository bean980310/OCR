--- ABSTRACT ---
작년에는 텍스트와 이미지 도메인이 공동으로 표현되는 교차 모달 표현 공간이라는 아이디어를 전제로 한 텍스트 프롬프트 이미지 생성에서 놀라운 진전이 있었습니다. ASR에서 이 아이디어는 쌍을 이루지 않은 음성과 텍스트 모두에서 학습하여 매우 큰 매개변수 모델의 용량까지 확장할 수 있는 공동 음성-텍스트 인코더로 적용되었습니다. 이러한 방법은 유망해 보이지만 업샘플링 휴리스틱이나 명시적 정렬 모델을 통해 음성과 텍스트에 내재된 시퀀스 길이 불일치를 특별히 처리해야 했습니다. 이 연구에서 우리는 공동 음성-텍스트 인코더가 시퀀스 길이를 무시하여 자연스럽게 모달리티 전반에 걸쳐 일관된 표현을 달성한다는 증거를 제시하고 일관성 손실이 길이 차이를 용서하고 단순히 최상의 정렬을 가정할 수 있다고 주장합니다. 우리는 이러한 손실이 대규모 매개변수 단일 언어 및 다중 언어 시스템에서 다운스트림 WER을 개선한다는 것을 보여줍니다. 1.
--- METHOD ---
s는 유망성을 보여주지만, 업샘플링 휴리스틱이나 명시적 정렬 모델을 통해 음성과 텍스트에 내재된 시퀀스 길이 불일치를 특별히 처리해야 했습니다. 이 연구에서 우리는 공동 음성-텍스트 인코더가 시퀀스 길이를 무시하여 자연스럽게 모달리티 간에 일관된 표현을 달성한다는 증거를 제시하고, 일관성 손실이 길이 차이를 용서하고 단순히 최상의 정렬을 가정할 수 있다고 주장합니다. 우리는 이러한 손실이 대규모 매개변수 단일 언어 및 다중 언어 시스템에서 다운스트림 WER을 개선한다는 것을 보여줍니다. 1. 서론 단일 모달리티에서 방대한 비지도 코퍼스에서 학습한 매우 큰 모델의 힘은 점점 더 명확해졌습니다. 이는 언어 모델이 전례 없는 제로샷 기능을 달성한 텍스트 도메인[1, 2]과 단일 모델이 놀라울 정도로 광범위한 음향 작업에 적응할 수 있는 것으로 나타난 오디오 도메인[3, 4]에서 입증되었습니다. 이러한 성공은 역사적으로 수동으로 페어링된 데이터에 의존해 온 두 가지 모달리티와 관련된 문제에 이러한 방법을 적용하는 방법에 대한 의문을 제기했습니다. 이 문제에 대한 매우 유망한 해결책 중 하나는 두 모달리티 모두에서 큰 인코더를 훈련하여 두 모달리티 중 하나를 짝이 없는 예로 제공하면서도 짝이 있는 예를 표현 공간의 유사한 지점에 매핑하도록 학습하는 것입니다.이미지/텍스트 도메인에서 이러한 표현은 달성 가능하고 단일 모델에서 많은 이미지 및 텍스트 이해 작업에서 최첨단 성능을 달성할 수 있는 것으로 입증되었습니다[5, 6].오디오/텍스트 도메인에서 공동 음성 및 텍스트 모델이 광범위한 작업에 활용되었습니다[7, 8, 9].음성 인식에서 지난 몇 년 동안 짝이 없는 음성 및 텍스트 데이터에 대한 사전 학습을 허용하는 공동 음성 및 텍스트 인코더가 있는 모델로의 추세가 나타났습니다[10, 11, 12, 13].그러나 음성 인식은 두 가지 시퀀스 모달리티의 특정 과제를 제시하는데, 그 중 하나(음성)는 일반적으로 다른 하나(텍스트)보다 훨씬 더 긴 시퀀스로 표현됩니다. 이것은 우리가 인코더의 음성 표현을 텍스트 표현과 프레임 단위로 직접 비교할 수 없기 때문에 동일한 임베딩 공간에서 두 모달리티를 표현하는 작업을 복잡하게 만듭니다. 이 복잡성은 대체로 업샘플링이나 명시적 정렬 모델로 처리되었습니다. 텍스트 입력의 고정 업샘플링은 [13]의 ASR과 [14]의 SLU에 성공적으로 적용되어 대략적인 정렬이 공동 표현을 학습하기에 충분하다는 것을 증명했습니다. 반면에 [15]는 완벽한 정렬을 목표로 하는 별도로 훈련된 정렬 모델로 문제를 해결합니다. [12]에서 이러한 정렬 모델은 해당 음성 및 텍스트에 대한 인코더의 출력을 프레임 단위로 비교하고 표현 공간에서 함께 푸시하는 &quot;일관성&quot; 정규화를 사용할 수 있음을 보여줍니다. [12]는 &quot;일관성&quot; 정규화가 더 긴밀하게 결합된 표현 공간을 생성하여 더 나은 WER을 생성함을 보여줍니다. 일관성 정규화 자체는 생성 모델에 대한 문헌에서 자연스럽게 따릅니다. 증강 데이터에 적용된 자동 인코더와 같은 시스템(예: [16])은 일치하는 예제의 표현을 명시적으로 함께 푸시하는 반면, [17]과 같은 대조 시스템은 암묵적으로 동일한 작업을 수행합니다.명시적 정렬을 사용하여 음성에서 동일한 아이디어의 성공은 동일한 작업이 암묵적 정렬로 수행될 수 있는지에 대한 의문을 제기합니다.즉, 음성과 텍스트 간의 특정 정렬을 알지 못한 채로 말입니다.이 논문에서는 [13]과 같은 업샘플링 시스템에서 학습한 암묵적 정렬을 사용하여 일관성 정규화를 적용하여 [12]의 명시적 정렬에서 볼 수 있는 성능 개선을 달성할 수 있는지 묻습니다.이를 위해 동적 시간 워핑 [18]에서 영감을 받은 알고리즘을 개발하여 쌍을 이룬 음성과 텍스트 예제의 인코더 표현 간에 가능한 최상의 정렬을 찾습니다.명시적 정렬 모델이 없는 시스템에서 이 최상의 정렬의 품질을 측정하고 이것이 학습 중에만 학습된 것이 아니라 실제로 네트워크의 더 깊은 계층에서 개선된다는 것을 보여줍니다. [15] 및 [12]에서 보여준 개선 사항에서 영감을 얻어, 우리는 일관성 정규화의 기준을 변경하여 일부 정렬에서 일관성을 장려함으로써 직접적인 프레임별 비교 대신 단일 언어 및 다중 언어 설정 모두에서 강력한 반지도 기준선에 대해 강력한 WER 개선을 달성할 수 있음을 보여줍니다. 이 모든 것은 학습된 정렬 모델 없이도 가능합니다. 우리의 결과는 교차 모달 표현에서 일관성을 강제하는 것은 단순히 오정렬을 용서함으로써 이루어질 수 있음을 시사합니다. 이 논문의 나머지 부분은 다음과 같이 진행됩니다. 섹션 2에서는 공동 음성/텍스트 모델링 및 일관성 정규화에 대한 설정과 최적 정렬 알고리즘에 대한 세부 정보를 지정합니다. 섹션 3에서는 데이터 및 훈련에 대한 세부 정보를 지정합니다. 이 섹션에서는 정규화되지 않은 모델에서 최상의 정렬에 대한 분석과 일관성 손실로 해당 정렬을 최적화한 결과를 제시합니다. 섹션 5에서 결론을 내립니다. 2. 방법 이 섹션에서는 주로 [13]을 따르는 공동 음성/텍스트 모델링에 의한 반지도 ASR에 대한 설정을 제시합니다. 그런 다음 제안된 최적 정렬 알고리즘을 제시하고 [15]에서 영감을 얻은 해당 일관성 손실을 정의합니다.2.1 모델 아키텍처 그림 1은 모델 아키텍처를 보여줍니다.기본적으로 스트리밍 및 비스트리밍 디코더로 감독 ASR을 수행하는데, 여기서 인코더는 텍스트 주입을 허용하기 위해 &quot;오디오 전용&quot;/&quot;텍스트 전용&quot; 및 &quot;공유&quot; 구성 요소로 분할됩니다.동시 ASR 및 텍스트 주입 작업은 공유 인코더에서 공동 표현을 발생시킵니다.특히 감독 예제 코퍼스 (x, y) = S와 페어링되지 않은 텍스트 코퍼스 y Є U가 주어지면 모델에는 다음 구성 요소가 포함됩니다.• Ea: 오디오 기능 x를 내장하는 오디오 인코더.Et: 텍스트 기능 y를 내장하는 텍스트 인코더.• • Estream: Ea(x) 또는 Et(y)를 소비하고 이를 공동 표현에 매핑할 수 있는 공유 스트리밍 인코더.이 인코더는 &quot;스트리밍&quot;이므로 과거 음향 프레임만 수신합니다. Efull-context: Estream의 출력을 소비하고 전방 음향 프레임이 제공되는 전체 컨텍스트 인코더.⚫ D stream: Estream의 출력을 소비하고 스트리밍 ASR 가설을 내보내는 스트리밍 디코더.Enon-streaming: Efull-context의 출력을 소비하고 스트리밍이 아닌 ASR 가설을 내보내는 스트리밍이 아닌 디코더.우리 모델은 ASR과 마스크된 텍스트 재구성이라는 두 가지 작업에서 동시에 학습됩니다.ASR의 경우 오디오가 오디오 인코더로 전달되고 가설은 기존 교차 엔트로피 손실을 사용하여 기준 진실 텍스트와 비교됩니다.마스크된 텍스트 재구성은 쌍이 없는 텍스트 데이터를 사용합니다.마스크(전사본의 15%)가 텍스트의 음소 표현에 적용된 다음 텍스트 인코더로 전달됩니다.가설은 교차 엔트로피 손실을 사용하여 다시 입력의 마스크된 부분과 비교됩니다.두 표현의 유사성을 측정하는 일관성(Ra, Rt). 오디오 x와 텍스트 y는 길이가 다른 시퀀스이므로 의미 있는 일관성 손실을 정의하려면 정렬에 대한 개념이 필요합니다. 정렬이란 각 오디오 프레임 x[i]가 일부 텍스트 프레임 y[j]에 해당하도록 y를 특정하게 업샘플링하는 것을 의미합니다. 이를 염두에 두고 정렬 A = (ao, a1,, an)을 y에 대한 인덱스 목록으로 정의합니다. 이때 모든 오디오 프레임 i에 대해 x[i]는 정렬에서 y[a¿]에 해당합니다. 또한 모든 i에 대해 a ≤ai+1이라는 제약 조건을 추가합니다. 즉, A가 단조롭게 증가하도록 제약하여 순차적인 오디오 프레임이 텍스트와 역방향으로 일치하지 않을 수 있습니다. 이 공식은 &quot;정렬&quot;을 정의하는 여러 가지 상상할 수 있는 방법 중 하나이며 최상의 정렬을 효율적으로 계산하는 데 실용성을 제공하기 때문에 선택했습니다(아래 섹션 2 참조). 이 공식에서 각 오디오 프레임은 정확히 한 번만 고려되는 반면 각 텍스트 프레임은 반복하거나 완전히 건너뛸 수 있습니다. 이 정의를 염두에 두고, 우리는 주어진 정렬에 대한 일관성 손실을 일관성 A n frame (Ra[x], Rt[A[x]]) n | (Ra, Rt) = Σ x =로 정의합니다.여기서 frame은 프레임별 유사도 측정값입니다(이 연구에서는 L2를 사용함).즉, 일관성(Ra, Rt)는 Ra와 A가 제공한 Rt의 특정 업샘플링 간의 평균 프레임별 유사도를 제공합니다.[15]와 [12]의 설정은 이러한 일관성 손실을 성공적으로 사용하여 신경 정렬 모델에서 A를 가져옵니다.우리는 대안으로 최상의 가능한 정렬에 대한 일관성을 최적화하는 것을 제안합니다: 일관성(Ra, Rt) 최소 일관성 &#39;A (Ra, Rt) A Hyps Hyps 이러한 손실로 훈련하기 위해서는 최상의 정렬을 계산하는 효율적인 알고리즘이 필요합니다.텍스트 텍스트 인코더 스트리밍 디코더 비스트리밍 디코더 2.3. 최상의 정렬 오디오 오디오 인코더 또는 스트리밍 공유 인코더 계산 전체 컨텍스트 공유 인코더 그림 1: [13] 및 [12]에서 수정한 반지도 ASR을 위한 아키텍처.2.2 일관성 손실 쌍으로 된 예제(x, y)를 고려합니다.여기서 x = (xo, ..., xn)은 음성 입력이고 y = (Yo, ym)은 텍스트 입력이며 n&gt; m입니다.오디오와 텍스트의 공유 표현을 Ra = Es(Ea(x)) Rt = Es(Et(y))로 정의하겠습니다.여기서 Es는 스트리밍 표현의 경우 Estream만 적용한 후 En E 비스트리밍(비스트리밍 표현의 경우)을 적용할 수 있습니다.[15] 및 [12]에서 개발한 &quot;일관성 손실&quot;은 손실 S 동적 시간 워핑[18]은 귀납적 규칙에 의존하여 비용 함수를 기반으로 두 시퀀스를 일치시키는 재귀적 알고리즘을 정의합니다. 우리는 동일한 작업을 수행하여 비용을 지정합니다.C(i, j) = min consistency AY (Ra[: i], Rt[: j]) 즉, 비용 C(i, j)는 인덱스 i까지의 오디오 표현 접두사와 인덱스 j까지의 텍스트 표현 접두사 간의 최적 정렬에서 일관성 손실을 제공합니다.그런 다음 귀납적 규칙을 지정할 수 있습니다.C(i, j) = min [C(i, k − 1) + Lframe (Ra[i], Rt[k])] k≤j 즉, 접두사 Ra[: i] 및 Rt[: j]에 대한 최적 정렬은 이전 i − 1 오디오 프레임을 더 짧은 접두사 Rt[: k]에 정렬한 다음 Ra[i]의 특정 정렬을 Rt[k]에 추가합니다.이 계산에서 최적 정렬의 인덱스를 백아웃할 수 있습니다.이 규칙은 O(nm²) 시간과 메모리에서 최적 정렬을 찾기 위한 동적 프로그래밍 알고리즘을 생성합니다. 모든 정렬에 대한 최소화가 정렬 찾기의 미분을 배제한다는 점에 유의합니다. 대신, 전방 전파 중에 최상의 정렬을 계산한 다음 정렬된 프레임에 적용된 프레임을 미분합니다. 즉, 그래디언트의 패스스루 근사를 사용합니다. 일관성(Ra, Rt) ǝe A* ≈ 일관성(Ra, Rt) ᎧᎾ 여기서 A* = arg min 일관성 A (Ra, Rt) A 3.
--- EXPERIMENT ---
s 이 섹션에서는 모델 설정과 데이터에 대한 세부 정보를 제공합니다.3.1. 모델 설정 섹션 2.1의 목록에 따라 구성 요소의 매개변수화를 지정합니다.• Ea: 오디오 인코더는 8개의 어텐션 헤드와 차원 2048을 갖춘 단일 컨포머[19] 계층으로 구성됩니다.오디오 인코더는 각각 32ms에 걸쳐 10ms 간격으로 배치된 128차원 로그-멜을 사용합니다.그런 다음 각 프레임을 앞의 프레임과 뒤의 두 프레임과 함께 쌓아 512차원 표현을 생성합니다.마지막으로 세 번째 프레임을 가져와 하위 샘플링하여 최종 프레임 속도를 30ms로 만듭니다.• Et: 텍스트 인코더는 임베딩 투영과 그 뒤에 오는 컨포머 계층으로 구성됩니다.[13]에서와 같이 텍스트 인코더에 텍스트 필사본의 음소 표현을 제공해야 합니다.그런 다음 정렬 휴리스틱으로 각 음소를 두 번 반복하여 [13]을 계속 따릅니다. • Estream: 공유 스트리밍 인코더는 5개의 컨포머 레이어로 구성되며, 마지막에 레이어 표준이 적용됩니다.Efull-context. 전체 컨텍스트 공유 인코더는 9개의 추가 컨포머 레이어로 구성되며, 마지막에 레이어 표준이 적용됩니다.⚫ D stream: 스트리밍 디코더는 예측 레이어와 조인트 레이어가 모두 차원이 640인 HAT 디코더[20]입니다.• Enon-streaming: 비스트리밍 디코더는 스트리밍 디코더와 동일합니다.이 모델에는 약 1억 6,500만 개의 매개변수가 포함됩니다.학습은 두 단계로 수행됩니다.먼저 오디오 인코더, 조인트 인코더 및 디코더는 모두 배치 크기가 2048인 800k 단계의 페어링된 데이터에서 학습됩니다.그런 다음 텍스트 인코더가 추가되고 모델은 섹션 2.1에 설명된 대로 동일하게 가중된 지도 및 비지도 손실로 추가로 학습되며, 섹션 2.3의 최상의 정렬 손실이 선택적으로 포함됩니다. 이 모델은 감독 및 비감독 데이터 모두에 대해 2048의 배치 크기로 100k 추가 단계에 대해 이런 방식으로 학습합니다. 모든 모델은 Tensorflow에서 구현되며, 최상의 정렬 알고리즘 자체는 CPU 커널로 구현됩니다. 최상의 정렬 계산을 추가해도 기준 모델에 비해 학습 시간이 크게 늘어나지 않는 것을 발견했습니다. 3.2. 데이터 세트 ASR의 텍스트 주입 방법은 역사적으로 두 가지 광범위한 설정에 적용되었습니다. 강력한 기준선은 종종 매우 큰 텍스트 코퍼스로 미세 조정되어 어려운 단어에 대한 성능을 개선합니다. 또는 텍스트 주입은 제한된 감독 데이터로 학습된 모델에 사용될 수 있으며, 내부 언어 모델을 개선하고 실행 가능한 시스템에 더 가까이 다가가는 데 사용될 수 있습니다. 이 두 가지 설정을 염두에 두고 두 가지 설정에서 최상의 정렬 손실을 연구합니다. • 약 200k 시간의 감독 음성으로 구성된 대규모 영어 코퍼스와 약 200B 개의 예제로 구성된 비감독 텍스트 데이터 세트. 우리는 훈련 예제와 동일한 분포에서 파생된 Main 테스트 세트와 특히 노이즈가 많은 예제의 Noisy 테스트 세트에 대한 결과를 보고합니다.• 다음 11개 언어로 구성된 다국어 코퍼스: 영어(En), 프랑스어(Fr), 스페인어(Sp), 아랍어(Ar), 포르투갈어(Po), 독일어(De), 러시아어(Ru), 힌디어(Hi), 이탈리아어(It), 중국어, 일본어.이 설정에는 비지도 텍스트가 포함되지 않으며 대신 MLM 목적이 지도된 필사본에 적용됩니다.데이터 세트는 약 1억 4천만 개의 쌍으로 된 예제로 구성됩니다.위에 굵은 글씨로 된 약어는 1에서 WER을 보고할 수 있는 언어에 대해 제공됩니다.테스트 세트의 수가 많으므로 단순화를 위해 이 모델에서 스트리밍되지 않는 WER만 보고합니다.모든 데이터 세트는 익명화되고 사람이 필사합니다.4. 결과 이 섹션에서는 일관성 정규화 없이도 모델이 쌍으로 된 음성 및 텍스트 예제 간의 정렬을 학습한다는 것을 보여주고자 합니다. 그런 다음 제안된 최적 정렬 일관성 정규화로 이 정렬을 최적화하면 WER이 개선됨을 보여주고자 합니다.4.1. 비정규화된 모델의 최적 정렬 이 분석에서는 섹션 3.1에서 설명한 대로 단일 언어 설정의 기준 모델을 사용합니다.우리의 목표는 스트리밍 조인트 인코더의 처음 5개 컨포머 계층 각각에서 수행된 Rs 및 Rt에 대한 작은 무작위 개발 예제 집합에 대한 일관성을 측정하는 것입니다.우리는 일관성에 대한 낮은 값이 음성과 텍스트 간의 더 강한 암묵적 정렬을 반영하는 것으로 해석합니다.각 계층 /에 대해 2000개의 무작위 오디오 및 텍스트 임베딩 쌍을 샘플링하고 쌍 간 거리 분포의 평균 μr 및 표준 편차 σ를 계산합니다.그런 다음 두 가지 정렬, 즉 나이브 프레임별 정렬과 계산된 최적 정렬을 비교합니다. 이러한 각 정렬 A에 대해 다음을 보고합니다.일관성 A μι (Rs, Rt) of 즉, 우리는 평균으로부터 떨어진 표준 편차 수의 관점에서 일관성을 보고하며, 결과는 정렬이 무작위보다 나을 것이 없음을 시사하고 0 미만의 결과는 정렬이 무작위보다 강함을 시사합니다.표 2는 이러한 측정값을 보여줍니다.프레임별 정렬의 일관성이 무작위 정렬의 일관성과 가까운 반면 최상의 정렬이 무작위보다 상당히 나은 것을 알 수 있습니다.또한 최상의 정렬의 품질은 모델이 더 깊어질수록 꾸준히 향상됩니다.즉, 텍스트와 음성이 프레임 수준에서 함께 모델링되지는 않지만 쌍을 이룬 음성과 텍스트 ☐ En Fr Sp| Ar Po De Ru Hi | It5.11.6.11.M_0 9.1 10.6 6.4 12.6 7.9 14.8 13.0 19.7 10.M_10 8.5 10.7.7 13.4 12.5 19.4 9.M_1 8.5 10.8.1 13.9 12.7 19.3 10.M_0.1 8.6 10.3 6.2 12.1 8.0 13.9 12.9 19.5 9.M_0.01 8.8 10.5 6.3 12.2 7.9 14.0 13.0 19.6 10.표 1: 다국어 설정에 대한 평가 결과. 표 2: 공유 인코더의 레이어에서 선형 및 최상의 정렬의 일관성.레이어 프레임별 정렬 최상의 정렬-0.-1.-0.-2.-0.-2.-0.-2.-0.-3.은 임베딩 공간의 유사한 지점에 매핑되며, 이 정렬은 네트워크의 깊이에 따라 개선됩니다. 이 정렬의 존재를 설명하기 위해 단일 테스트 예제에서 공유 인코더의 음성 및 텍스트 최종 표현 간의 관계를 시각화합니다. 그림 2a는 임베딩의 각 프레임 쌍 간의 거리를 표시하고 실제로 낮은 거리를 가진 단일 정렬임을 보여줍니다. 그림 2b는 최상의 정렬 알고리즘이 이 궤적을 복구하는 방법을 보여줍니다.(a) 거리Main Noisy Main NoisyE_5.40 8.E_10 5.37 8.E_1 5.35 8.E_0.1 5.27 8.E_0.01 5.32 8.E_O E_E_E_0.E_0.7.99 13.8.21 13.8.07 13.7.90 12.7.94 12.(b) 스트리밍 (a) 비스트리밍 표 3: 영어 전용 설정에 대한 평가 결과(b) 최상의 정렬 그림 2: 수평 축의 오디오 임베딩과 수직 축의 해당 텍스트 임베딩 간의 임베딩 거리(a)와 최상의 정렬(b)의 시각화. (a)의 어두운 점은 근처 임베딩이 있는 오디오 및 텍스트 프레임 쌍을 나타내고, (b)의 노란색 점은 복구된 최적 정렬의 쌍을 나타냅니다.4.2. 일관성 정규화 결과 우리는 서로 다른 보간 가중치에서 최적 정렬 손실과 섹션 3.2에 명시된 두 설정에 대한 결과를 제시합니다.읽기 편하도록 각 실험을 문자와 숫자로 지정합니다.문자는 영어 전용 설정의 경우 E이고 다국어 설정의 경우 M입니다.숫자는 최적 정렬 손실의 보간 가중치를 백분율로 나타낸 것입니다.예를 들어, E_0은 비정규화된 반지도 미세 조정이 적용된 기준 영어 전용 모델이고, M_0.1은 미세 조정 중에 0.1%로 보간된 최적 정렬 손실을 갖는 다국어 모델입니다.표 3은 리소스가 많은 영어 전용 설정에서의 결과를 보여줍니다.거기에서 최적 정렬 손실로 작지만 일관된 WER 개선을 확인했지만 올바른 보간 가중치를 선택해야 할 필요성에 주목합니다. 표 1은 다국어 환경에서의 결과를 보여주는데, 여기서 우리는 더 큰 개선을 보았습니다. 우리는 다국어 환경에서 이 방법의 강점이 문제의 난이도가 높아지고 데이터 세트가 작아서 모델이 개선할 여지가 더 많기 때문이라고 생각합니다. 5.
--- CONCLUSION ---
s 반지도 음성/텍스트 인코더가 최상의 정렬을 선택하여 관찰할 수 있는 두 모달리티의 공동 표현을 학습한다는 것을 보여주었습니다. 우리는 최상의 정렬을 위해 모달리티 매치를 최적화하는 추가 손실 항목으로 도메인 일관성을 강화하기 위해 이 사실을 활용했습니다. 우리는 매개변수를 추가하지 않고도 여러 설정에서 비정규화된 공동 모델에 비해 일관된 개선을 보여줍니다. 6. 참고문헌 [1] TB Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, DM Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, D. Amodei, &quot;언어 모델은 소수의 샷 학습자입니다.&quot; 신경 정보 처리 시스템의 발전, 2020. [2] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, HW Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, AM 다이, TS 필라이, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov 및 N. Fiedel, &quot;Palm: 경로를 사용한 언어 모델링 확장,&quot; 2022. [3] S. Yang, P. Chi, Y. Chuang, CJ Lai, K. Lakhotia, YY Lin, AT Liu, J. Shi, X. Chang, G. Lin, T. Huang, W. Tseng, K. Lee, D. Liu, Z. Huang, S. Dong, S. Li, S. Watanabe, A. Mohamed 및 H. Lee, &quot;SUPERB: 음성 처리 범용 성능 벤치마크&quot; 인터스피치, 2021. [4] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, O. Teboul, D. Grangier, M. Tagliasacchi 및 N. Zeghidour, &quot;Audiolm: 오디오 생성에 대한 언어 모델링 접근 방식&quot;, 2022. [5] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan, “Flamingo: a visual language model for fewshot learning,&quot; 신경 정보 처리 시스템의 발전, 2022. [6] J. Cho, J. Lei, H. Tan, and M. Bansal, “Unifying vision-and-language tasks via text generation,&quot; 기계 학습 국제 컨퍼런스, 2021. [7] A. Renduchintala, S. Ding, M. Wiesner, 및 S. Watanabe, &quot;종단 간 ASR을 위한 멀티모달 데이터 증강&quot;, INTERSPEECH, 2018. [8] Y. Huang, H. Kuo, S. Thomas, Z. Kons, K. Audhkhasi, B. Kingsbury, R. Hoory, 및 M. Picheny, &quot;종단 간 음성-의도 시스템 교육을 위한 비페어 텍스트 데이터 활용&quot;, IEEE 음향, 음성 및 신호 처리 국제 컨퍼런스, 2020. [9] S. Mariooryad, M. Shannon, S. Ma, T. Bagby, D. Kao, D. Stanton, E. Battenberg, 및 R. Skerry-Ryan, &quot;적거나 전혀 페어된 데이터를 사용하지 않고 두 시퀀스의 결합 분포 학습&quot;, 2022. [10] Y. Tang, JM Pino, C. Wang, X. Ma, 및 D. Genzel, &quot;음성 대 텍스트 작업을 위한 텍스트 데이터를 활용하기 위한 일반적인 멀티태스크 학습 프레임워크,&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2021. [11] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, A. Conneau, &quot;mslam: 음성 및 텍스트를 위한 대규모 다국어 공동 사전 훈련,&quot; 2020. [12] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno, A. Bapna, H. Zen, &quot;Maestro: 모달리티 매칭을 통한 매칭된 음성 텍스트 표현,&quot; INTERSPEECH, 2022. [13] TN Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen, B. Li, W. Wang, T. Strohman, “Joist: A joint speech and text streaming model for asr,” in IEEE Spoken Language Technology Workshop, 2022. [14] S. Thomas, H.-KJ Kuo, B. Kingsbury, and G. Saon, “Towards decrease the need for speech training data to build speaking language understanding systems,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. [15] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno, and G. Wang, “Tts4pretrain 2.0: Advancing the use of text and speech in asr pretraining with consistency and contrastive losses,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. [16] C. Chadebec, E. Thibeau-Sutre, N. Burgos, and S. Allassonniere, “Data augmentation in high dimensional low sample size setting using a 영어: 기하학 기반 변분 자동 인코더,&quot; IEEE 패턴 분석 및 머신 인텔리전스 트랜잭션, 2022. [17] T. Chen, S. Kornblith, M. Norouzi 및 G. Hinton, &quot;시각적 표현의 대조 학습을 위한 간단한 프레임워크,&quot; 국제 머신 러닝 컨퍼런스, 2020. [18] H. Sakoe 및 S. Chiba, &quot;말한 단어 인식을 위한 동적 프로그래밍 알고리즘 최적화,&quot; IEEE 음향, 음성 및 신호 처리 트랜잭션, 1978. [19] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, &quot;Conformer: Convolution-augmented transformer for speech awareness,&quot; INTERSPEECH, 2020. [20] E. Variani, D. Rybach, C. Allauzen, and M. Riley, &quot;Hybrid autoregressive transducer (hat),&quot; IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스, 2020.
