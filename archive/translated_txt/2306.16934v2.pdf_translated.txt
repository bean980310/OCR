--- ABSTRACT ---
이 논문에서는 생각을 텍스트로 변환할 필요 없이 뇌 전기뇌파(EEG) 신호에서 직접 고품질 이미지를 생성하는 새로운 방법인 DreamDiffusion을 소개합니다. DreamDiffusion은 사전 학습된 텍스트-이미지 모델을 활용하고 시간 마스크 신호 모델링을 사용하여 효과적이고 강력한 EEG 표현을 위해 EEG 인코더를 사전 학습합니다. 또한 이 방법은 CLIP 이미지 인코더를 더욱 활용하여 제한된 EEG-이미지 쌍으로 EEG, 텍스트 및 이미지 임베딩을 더 잘 정렬하기 위한 추가 감독을 제공합니다. 전반적으로 제안된 방법은 노이즈, 제한된 정보 및 개인 차이와 같은 이미지 생성에 EEG 신호를 사용하는 데 따르는 과제를 극복하고 유망한 결과를 얻습니다. 정량적 및 정성적 결과는 제안된 방법이 신경 과학 및 컴퓨터 비전에 잠재적으로 적용될 수 있는 휴대 가능하고 저렴한 &quot;생각-이미지&quot;를 향한 중요한 단계로서 효과적임을 보여줍니다. 코드는 여기에서 사용할 수 있습니다 https://github.com/bbaaii/DreamDiffusion. 1.
--- INTRODUCTION ---
이미지 생성[16, 22, 4]은 최근 몇 년 동안 큰 진전을 이루었는데, 특히 텍스트-이미지 생성[31, 12, 30, 34, 1]의 획기적인 발전 이후 더욱 그렇습니다. 최근의 텍스트-이미지 생성은 생성된 이미지의 품질을 극적으로 향상시킬 뿐만 아니라 사람들의 아이디어를 텍스트로 제어되는 절묘한 그림과 예술 작품으로 만들 수 있게 해줍니다. 우리는 생각을 생성하기 전에 텍스트로 변환하지 않고도 뇌 활동(예: 뇌파(EEG) 기록)에서 직접 이미지 생성을 제어할 수 있는지 매우 궁금합니다. 이런 종류의 &quot;생각-이미지&quot;는 광범위한 전망을 가지고 있으며 사람들의 상상력을 넓힐 수 있습니다. 예를 들어, 예술적 창작의 효율성을 크게 향상시키고 덧없는 영감을 포착하는 데 도움이 될 수 있습니다. 또한 밤에 꿈을 시각화하는 데 도움이 될 수 있는 잠재력이 있습니다(DreamDiffusion이라는 이름이 유래). 게다가 자폐증 아동과 언어 장애 아동을 도울 수 있는 잠재력이 있어 심리 치료에도 도움이 될 수 있습니다. 최근의 MinD-Vis [7] 및 [40]와 같은 일부 연구는 뇌 활동을 측정하는 또 다른 방법인 fMRI(기능적 자기공명영상) 신호를 기반으로 시각 정보를 재구성하려고 시도합니다. 그들은 뇌 활동으로부터 고품질 결과를 재구성하는 것이 가능하다는 것을 보여주었습니다. 그러나 그들은 여전히 뇌 신호를 사용하여 편리하고 효율적으로 창조한다는 우리의 목표와는 거리가 멉니다. 1) fMRI 장비는 휴대할 수 없고 전문가가 작동해야 하기 때문에 fMRI 신호를 포착하기 어렵습니다. 2) fMRI 획득 비용이 높습니다. 그들은 이 방법이 실용적인 예술적 세대에서 널리 사용되는 것을 크게 방해합니다. 반면에 EEG(뇌파)는 뇌의 전기적 활동을 기록하는 비침습적이고 저렴한 방법입니다. 현재 휴대형 상업용 제품이 출시되어 EEG 신호를 편리하게 획득할 수 있어 미래의 예술 세대에 큰 잠재력을 보여줍니다. 이 작업에서 우리는 사전 훈련된 텍스트-이미지 모델(즉, Stable Diffusion[32])의 강력한 생성 기능을 활용하여 뇌 EEG 신호에서 직접 고품질 이미지를 생성하는 것을 목표로 합니다. 그러나 이는 사소한 일이 아니며 두 가지 과제가 있습니다. 1) EEG 신호는 비침습적으로 수집되므로 본질적으로 노이즈가 있습니다. 또한 EEG 데이터는 제한적이며 개인 차이를 무시할 수 없습니다. 이렇게 많은 제약 조건이 있는 EEG 신호에서 효과적이고 견고한 의미 표현을 얻는 방법은 무엇입니까? 2) CLIP[28]을 사용하고 많은 수의 텍스트 이미지 쌍에 대한 학습 덕분에 Stable Diffusion의 텍스트 및 이미지 공간이 잘 정렬됩니다. 그러나 EEG 신호는 고유한 특성을 가지고 있으며 그 공간은 텍스트 및 이미지의 공간과 상당히 다릅니다. 제한적이고 노이즈가 많은 EEG-이미지 쌍으로 EEG, 텍스트 및 이미지 공간을 정렬하는 방법은 무엇입니까? 첫 번째 과제를 해결하기 위해 희귀한 EEG-이미지 쌍만 사용하는 대신 대량의 EEG 데이터를 사용하여 EEG 표현을 학습하는 것을 제안합니다. 구체적으로, 우리는 맥락적 단서를 기반으로 누락된 토큰을 예측하기 위해 마스크 신호 모델링을 채택합니다. 입력을 2차원 이미지로 처리하고 공간 정보를 마스크하는 MAE[18] 및 MinD-Vis[7]와 달리, 우리는 EEG 신호의 시간적 특성을 고려하고 사람들의 뇌에서 시간적 변화의 이면에 있는 의미를 깊이 파헤칩니다. 우리는 토큰의 일부를 무작위로 마스크한 다음 시간 영역에서 마스크된 토큰을 재구성합니다. 이런 식으로, 사전 훈련된 인코더는 다양한 사람과 다양한 뇌 활동에 걸쳐 EEG 데이터에 대한 심층적인 이해를 학습합니다. 두 번째 과제의 경우, 이전 방법[40, 7]은 일반적으로 소수의 노이즈가 있는 데이터 쌍을 사용하여 안정 확산(SD) 모델을 직접 미세 조정합니다. 그러나 최종 이미지 재구성 손실만을 사용하여 SD를 종단 간 미세 조정하여 뇌 신호(예: EEG 및 fMRI)와 텍스트 공간 간의 정확한 정렬을 학습하는 것은 어렵습니다. 따라서 우리는 EEG, 텍스트 및 이미지 공간의 정렬을 지원하기 위해 추가적인 CLIP [28] 감독을 채택할 것을 제안합니다. 구체적으로, SD 자체는 이전 단계의 마스크된 사전 학습된 EEG 임베딩과 상당히 다른 텍스트 임베딩을 생성하기 위해 CLIP의 텍스트 인코더를 사용합니다. 우리는 CLIP의 이미지 인코더를 활용하여 CLIP 텍스트 임베딩과 잘 맞는 풍부한 이미지 임베딩을 추출합니다. 그런 다음 이러한 CLIP 이미지 임베딩을 사용하여 EEG 임베딩 표현을 더욱 최적화합니다. 따라서 정제된 EEG 특징 임베딩은 CLIP 이미지 및 텍스트 임베딩과 잘 맞출 수 있으며 SD 이미지 생성에 더 적합하여 생성된 이미지의 품질이 향상됩니다. 위의 두 가지 섬세한 디자인을 갖춘 제안 방법인 DreamDiffusion은 EEG 신호로부터 고품질의 사실적인 이미지를 생성할 수 있습니다. 우리의 기여는 다음과 같이 요약될 수 있습니다. 1) 우리는 EEG 신호만으로 사실적인 이미지를 생성하기 위해 강력한 사전 훈련된 텍스트-이미지 확산 모델을 활용하는 DreamDiffusion을 제안합니다. 이는 휴대 가능하고 저렴한 &quot;생각-이미지&quot;를 향한 한 단계 더 나아간 것입니다. 2) 시간 마스크 신호 모델링을 사용하여 효과적이고 견고한 EEG 표현을 위해 EEG 인코더를 사전 훈련합니다. 3) 우리는 CLIP 이미지 인코더를 추가로 활용하여 제한된 EEG-이미지 쌍으로 EEG, 텍스트 및 이미지 임베딩을 더 잘 정렬하기 위한 추가 감독을 제공합니다. 4) 양적 및 질적 결과는 DreamDiffusion의 효과를 보여주었습니다. 2.
--- RELATED WORK ---
s 2.1. 뇌 활동에서 이미지 생성 fMRI 및 EEG를 포함한 뇌 신호를 사용하여 이미지를 생성하는 것은 활발한 연구 분야였습니다. fMRI를 사용하는 경우 기존
--- METHOD ---
생각을 텍스트로 변환할 필요 없이 뇌 전기뇌파(EEG) 신호에서 직접 고품질 이미지를 생성하는 DreamDiffusion은 사전 학습된 텍스트-이미지 모델을 활용하고 시간 마스크 신호 모델링을 사용하여 효과적이고 강력한 EEG 표현을 위해 EEG 인코더를 사전 학습합니다. 또한 이 방법은 CLIP 이미지 인코더를 더욱 활용하여 제한된 EEG-이미지 쌍으로 EEG, 텍스트 및 이미지 임베딩을 더 잘 정렬하기 위한 추가 감독을 제공합니다. 전반적으로 제안된 방법은 노이즈, 제한된 정보 및 개인 차이와 같은 이미지 생성에 EEG 신호를 사용하는 데 따르는 과제를 극복하고 유망한 결과를 얻습니다. 정량적 및 정성적 결과는 제안된 방법이 신경 과학 및 컴퓨터 비전에 잠재적으로 적용할 수 있는 휴대 가능하고 저렴한 &quot;생각-이미지&quot;를 향한 중요한 단계로서 효과적임을 보여줍니다. 코드는 여기에서 사용할 수 있습니다 https://github.com/bbaaii/DreamDiffusion. 1. 서론 이미지 생성[16, 22, 4]은 최근 몇 년 동안 큰 진전을 이루었는데, 특히 텍스트-이미지 생성[31, 12, 30, 34, 1]의 획기적인 발전 이후 더욱 그렇습니다. 최근의 텍스트-이미지 생성은 생성된 이미지의 품질을 극적으로 향상시킬 뿐만 아니라 사람들의 아이디어를 텍스트로 제어되는 절묘한 그림과 예술 작품으로 만들 수 있게 합니다. 우리는 생각을 생성하기 전에 텍스트로 변환하지 않고도 뇌 활동(예: 뇌파(EEG) 기록)에서 직접 이미지 생성을 제어할 수 있는지 매우 궁금합니다. 이런 종류의 &quot;생각-이미지&quot;는 광범위한 전망을 가지고 있으며 사람들의 상상력을 넓힐 수 있습니다. 예를 들어, 예술적 창작의 효율성을 크게 향상시키고 덧없는 영감을 포착하는 데 도움이 될 수 있습니다. 또한 밤에 꿈을 시각화하는 데 도움이 될 수 있는 잠재력이 있습니다(DreamDiffusion이라는 이름이 유래). 게다가 자폐증 아동과 언어 장애 아동을 도울 수 있는 잠재력이 있어 심리 치료에도 도움이 될 수 있습니다. 최근의 MinD-Vis [7] 및 [40]와 같은 일부 연구는 뇌 활동을 측정하는 또 다른 방법인 fMRI(기능적 자기공명영상) 신호를 기반으로 시각 정보를 재구성하려고 시도합니다. 그들은 뇌 활동으로부터 고품질 결과를 재구성하는 것이 가능하다는 것을 보여주었습니다. 그러나 그들은 여전히 뇌 신호를 사용하여 편리하고 효율적으로 창조한다는 우리의 목표와는 거리가 멉니다. 1) fMRI 장비는 휴대할 수 없고 전문가가 작동해야 하기 때문에 fMRI 신호를 포착하기 어렵습니다. 2) fMRI 획득 비용이 높습니다. 그들은 이 방법이 실용적인 예술적 세대에서 널리 사용되는 것을 크게 방해합니다. 반면에 EEG(뇌파)는 뇌의 전기적 활동을 기록하는 비침습적이고 저렴한 방법입니다. 현재 휴대형 상업용 제품이 출시되어 EEG 신호를 편리하게 획득할 수 있어 미래의 예술 세대에 큰 잠재력을 보여줍니다. 이 작업에서 우리는 사전 훈련된 텍스트-이미지 모델(즉, Stable Diffusion[32])의 강력한 생성 기능을 활용하여 뇌 EEG 신호에서 직접 고품질 이미지를 생성하는 것을 목표로 합니다. 그러나 이는 사소한 일이 아니며 두 가지 과제가 있습니다. 1) EEG 신호는 비침습적으로 수집되므로 본질적으로 노이즈가 있습니다. 또한 EEG 데이터는 제한적이며 개인 차이를 무시할 수 없습니다. 이렇게 많은 제약 조건이 있는 EEG 신호에서 효과적이고 견고한 의미 표현을 얻는 방법은 무엇입니까? 2) CLIP[28]을 사용하고 많은 수의 텍스트 이미지 쌍에 대한 학습 덕분에 Stable Diffusion의 텍스트 및 이미지 공간이 잘 정렬됩니다. 그러나 EEG 신호는 고유한 특성을 가지고 있으며 그 공간은 텍스트 및 이미지의 공간과 상당히 다릅니다. 제한적이고 노이즈가 많은 EEG-이미지 쌍으로 EEG, 텍스트 및 이미지 공간을 정렬하는 방법은 무엇입니까? 첫 번째 과제를 해결하기 위해 희귀한 EEG-이미지 쌍만 사용하는 대신 대량의 EEG 데이터를 사용하여 EEG 표현을 학습하는 것을 제안합니다. 구체적으로, 우리는 맥락적 단서를 기반으로 누락된 토큰을 예측하기 위해 마스크 신호 모델링을 채택합니다. 입력을 2차원 이미지로 처리하고 공간 정보를 마스크하는 MAE[18] 및 MinD-Vis[7]와 달리, 우리는 EEG 신호의 시간적 특성을 고려하고 사람들의 뇌에서 시간적 변화의 이면에 있는 의미를 깊이 파헤칩니다. 우리는 토큰의 일부를 무작위로 마스크한 다음 시간 영역에서 마스크된 토큰을 재구성합니다. 이런 식으로, 사전 훈련된 인코더는 다양한 사람과 다양한 뇌 활동에 걸쳐 EEG 데이터에 대한 심층적인 이해를 학습합니다. 두 번째 과제의 경우, 이전 방법[40, 7]은 일반적으로 소수의 노이즈가 있는 데이터 쌍을 사용하여 안정 확산(SD) 모델을 직접 미세 조정합니다. 그러나 최종 이미지 재구성 손실만을 사용하여 SD를 종단 간 미세 조정하여 뇌 신호(예: EEG 및 fMRI)와 텍스트 공간 간의 정확한 정렬을 학습하는 것은 어렵습니다. 따라서 우리는 EEG, 텍스트 및 이미지 공간의 정렬을 지원하기 위해 추가적인 CLIP [28] 감독을 채택할 것을 제안합니다. 구체적으로, SD 자체는 이전 단계의 마스크된 사전 학습된 EEG 임베딩과 상당히 다른 텍스트 임베딩을 생성하기 위해 CLIP의 텍스트 인코더를 사용합니다. 우리는 CLIP의 이미지 인코더를 활용하여 CLIP 텍스트 임베딩과 잘 맞는 풍부한 이미지 임베딩을 추출합니다. 그런 다음 이러한 CLIP 이미지 임베딩을 사용하여 EEG 임베딩 표현을 더욱 최적화합니다. 따라서 정제된 EEG 특징 임베딩은 CLIP 이미지 및 텍스트 임베딩과 잘 맞출 수 있으며 SD 이미지 생성에 더 적합하여 생성된 이미지의 품질이 향상됩니다. 위의 두 가지 섬세한 디자인을 갖춘 제안 방법인 DreamDiffusion은 EEG 신호로부터 고품질의 사실적인 이미지를 생성할 수 있습니다. 우리의 기여는 다음과 같이 요약될 수 있습니다. 1) 우리는 EEG 신호에서만 사실적인 이미지를 생성하기 위해 강력한 사전 훈련된 텍스트-이미지 확산 모델을 활용하는 DreamDiffusion을 제안합니다. 이는 휴대 가능하고 저렴한 &quot;생각-이미지&quot;를 향한 한 걸음 더 나아간 것입니다. 2) 시간 마스크 신호 모델링을 사용하여 효과적이고 견고한 EEG 표현을 위해 EEG 인코더를 사전 훈련합니다. 3) 우리는 CLIP 이미지 인코더를 추가로 활용하여 제한된 EEG-이미지 쌍으로 EEG, 텍스트 및 이미지 임베딩을 더 잘 정렬하기 위한 추가 감독을 제공합니다. 4) 정량적 및 정성적 결과는 DreamDiffusion의 효과를 보여주었습니다. 2. 관련 연구 2.1. 뇌 활동에서 이미지 생성 fMRI 및 EEG를 포함한 뇌 신호를 사용하여 이미지를 생성하는 것은 활발한 연구 분야였습니다. fMRI를 사용하기 위해 기존 방법은 fMRI-이미지 쌍 데이터에 의존하여 모델을 훈련하여 fMRI에서 이미지 특징을 예측합니다. 이러한 이미지 특징은 테스트 중 자극 재구성을 위해 GANS[36]에 공급됩니다. 그러나 최근 연구[3]에서는 재구성 가능한 자동 인코더 설계와 같은 비지도 접근 방식을 제안하여 페어링되지 않은 fMRI와 이미지에서 학습하고 회귀 모델[25, 27]을 활용하여 디코딩을 위해 사전 훈련된 조건부 BigGAN[5]을 미세 조정하는 데 사용할 수 있는 잠재 fMRI 표현을 추출합니다. 최근 작업 MinD-Vis[8]는 SC-MBM과 DC-LDM을 통합하여 더 잘 보존된 의미 정보가 있는 더욱 그럴듯한 이미지를 생성합니다. 마찬가지로 EEG 신호에서 이미지를 생성하는 것도 딥 러닝 기술을 사용하여 탐구되었습니다. Brain2image[23]는 LSTM과 생성적 방법을 사용하여 특정 뇌 반응을 유발하는 시각적 자극을 생성하기 위한 EEG 데이터의 보다 압축된 표현을 학습했습니다. ThoughtViz[41]는 제한된 훈련 데이터에서도 인코딩된 EEG 신호를 입력으로 사용하여 해당 이미지를 생성합니다. [9]는 의미적 특징 표현을 학습하고 의미적 이미지 편집과 비슷한 성능을 달성하기 위한 감독 신호로 EEG를 사용합니다. 전반적으로 이러한 접근 방식은 뇌 신호를 사용하여 이미지를 생성하고 뇌-컴퓨터 인터페이스 분야를 발전시킬 수 있는 잠재력을 보여줍니다. 1. 대규모 노이즈가 있는 EEG 데이터를 사용한 마스크 신호 모델링 EEG 신호 분할 및 마스크 2. 제한된 EEG-이미지 쌍으로 미세 조정 × (T-1) EEG 인코더 잠재 EEG 디코더 Am 재구성 3. EEG, 텍스트 및 이미지 공간 정렬 쌍 이미지 생성된 이미지 D 투영 계층 EEG 임베드 노이즈 제거 Zo ZT-노이즈 제거 U-Net ZT U-Net 사전 학습된 안정된 확산 CLIP 이미지 인코더 이미지 임베드 코사인 유사도 그림 2. DreamDiffusion 개요. 우리 방법은 세 가지 주요 구성 요소로 구성됩니다. 1) 효과적이고 견고한 EEG 인코더를 위한 마스크 신호 사전 학습, 2) 사전 학습된 안정 확산을 사용하여 제한된 EEG-이미지 쌍으로 미세 조정, 3) CLIP 인코더를 사용하여 EEG, 텍스트 및 이미지 공간 정렬. 2.2. 모델 사전 학습 사전 학습 모델은 컴퓨터 비전 분야에서 점점 더 인기를 얻고 있으며, 다양한 자기 감독 학습 접근 방식이 서로 다른 사전 텍스트 작업에 초점을 맞춥니다[13, 43, 26]. 이러한 방법은 종종 이미지 유사성과 비유사성을 모델링하는 대조 학습[2, 17]이나 마스크된 부분에서 원본 데이터를 복구하는 자동 인코딩[6]과 같은 사전 텍스트 작업을 활용합니다. 특히 마스크 신호 모델링(MSM)은 시각 신호의 높은 마스크 비율[18, 44]과 자연어의 낮은 마스크 비율[10, 29]에서 원본 데이터를 복구하여 다운스트림 작업에 대한 유용한 맥락 지식을 학습하는 데 성공했습니다. 영어: 또 다른 최근 접근 방식인 CLIP[28]은 인터넷의 다양한 소스에서 수집한 4억 개의 텍스트-이미지 쌍을 사전 학습하여 다중 모달 임베딩 공간을 구축합니다.CLIP에서 학습한 표현은 매우 강력하여 여러 데이터 세트에서 최첨단 제로샷 이미지 분류를 가능하게 하고 텍스트와 이미지 간의 의미적 유사성을 추정하는 방법을 제공합니다.2.3. 확산 모델 확산 모델은 고품질 콘텐츠를 제작하기 위한 생성 모델로 점점 더 인기를 얻고 있습니다[37].확산 모델의 기본 형태는 양방향 마르코프 상태 사슬로 정의되는 확률적 모델입니다[19].이러한 모델[11, 19, 33, 39]은 이미지와 같은 데이터의 귀납적 편향과 자연스럽게 맞아떨어지기 때문에 강력한 생성 능력을 보입니다.일반적으로 학습 중에 가중치가 재지정된 목표를 사용할 때 최상의 합성 품질이 달성되며[19] 이미지 품질과 압축 기능 간의 균형을 유지할 수 있습니다. 그러나 픽셀 공간에서 이러한 모델을 평가하고 최적화하는 것은 계산 비용이 많이 들고 시간도 많이 걸립니다[24, 35, 38, 20, 42]. 이러한 과제를 해결하기 위해 일부 확산 모델은 제안된 LDM[32]과 같이 차원이 낮은 압축된 잠재 공간에서 작동합니다. 벡터 양자화(VQ)[15] 정규화된 자동 인코더를 사용하여 이미지를 더 낮은 차원의 잠재 특징으로 압축한 다음 동일한 잠재 공간 기능을 사용하여 재구성함으로써 LDM은 합성 품질을 유지하면서 계산 비용을 줄입니다. 또한 어텐션 모듈이 있는 UNet 기반 노이즈 제거 모델은 마르코프 체인 전환 동안 키/값/쿼리 벡터를 통해 이미지 생성을 조절할 수 있는 유연성을 제공합니다. 이 접근 방식에는 계산 비용이 절감되고 이미지 합성 품질이 향상되는 등 여러 가지 장점이 있습니다. 3. 제안된 방법 본 방법은 세 가지 주요 구성 요소로 구성됩니다. 1) 효과적이고 견고한 EEG 인코더를 위한 마스크 신호 사전 학습, 2) 사전 학습된 안정 확산을 사용하여 제한된 EEG-이미지 쌍으로 미세 조정, 3) CLIP 인코더를 사용하여 EEG, 텍스트 및 이미지 공간 정렬. 먼저, 많은 노이즈가 있는 EEG 데이터로 마스크 신호 모델링을 활용하여 EEG 인코더를 학습하여 문맥적 지식을 추출합니다. 그런 다음 결과 EEG 인코더를 사용하여 조건을 제공합니다.----° 7.5.2.0.-2.-5.-7.-10.---EEG 신호 제목 Masked EEG SignalReconstructionV그림 3. 대규모 노이즈가 있는 EEG 데이터를 사용한 마스크 신호 모델링. EEG 데이터에서 한 채널의 재구성 결과를 시각화합니다. 전반적인 추세는 정확하지만 세부 정보는 데이터 세트의 영향을 받는 것을 알 수 있습니다. 이러한 데이터 세트의 EEG 신호는 비교적 노이즈가 많기 때문입니다. 영어: 교차 주의 메커니즘을 통한 안정적 확산을 위한 tional 특징. EEG 특징과 안정적 확산의 호환성을 향상시키기 위해 미세 조정 중에 EEG 임베딩과 CLIP 이미지 임베딩 간의 거리를 줄여 EEG, 텍스트 및 이미지 임베딩 공간을 추가로 정렬합니다. 그 결과 EEG 신호에서만 고품질 이미지를 생성할 수 있는 DreamDiffusion을 얻을 수 있습니다. 3.1. 효과적이고 견고한 EEG 표현을 위한 마스크 신호 사전 학습 EEG(뇌파) 데이터는 두피에 붙인 전극을 사용하여 측정한 인간의 뇌에서 생성된 전기적 활동을 기록한 것입니다. 뇌 활동을 측정하는 비침습적이고 저렴한 방법입니다. EEG 데이터에는 여러 가지 특징이 있습니다. 첫째, 데이터는 2차원으로, 한 차원은 두피에 붙인 채널 또는 전극을 나타내고 다른 차원은 시간을 나타냅니다. EEG의 시간적 해상도가 높기 때문에 밀리초 단위로 발생하는 뇌 활동의 빠른 변화를 포착할 수 있습니다. 그러나 EEG의 공간적 해상도가 낮아 뇌 내 활동의 근원을 정확하게 파악하기 어렵다. 둘째, EEG 신호는 나이, 수면, 인지 상태와 같은 요인의 영향을 받는 매우 가변적이다. 마지막으로, EEG 데이터는 종종 노이즈가 많고 의미 있는 정보를 추출하기 위해 신중한 처리와 분석이 필요하다. EEG 데이터의 본질적인 가변성과 노이즈로 인해 기존 모델링 방법은 종종 EEG 신호에서 의미 있는 정보를 추출하는 데 어려움을 겪는다. 결과적으로 노이즈가 많고 가변적인 데이터에서 맥락적 정보를 포착하는 데 효과적인 것으로 입증된 마스크 신호 모델링 기술[18, 7]을 채택하는 것은 대규모 노이즈가 있는 EEG 데이터에서 의미 있는 맥락적 지식을 도출하는 유망한 경로이다. 입력을 2차원 이미지로 처리하고 공간 정보를 마스크하는 MAE[18] 및 MinD-Vis[7]와 달리, 우리는 EEG 신호의 시간적 특성을 고려하고 사람들의 뇌에서 시간적 변화의 의미론을 깊이 파헤친다. EEG 신호의 높은 시간적 해상도를 고려하여, 우리는 먼저 이를 시간 영역에서 토큰으로 나누고, 토큰의 일정 비율을 무작위로 마스크합니다. 그런 다음, 이러한 토큰은 1차원 합성곱 계층을 사용하여 임베딩으로 변환됩니다. 그런 다음, MAE[18]와 같은 비대칭 아키텍처를 사용하여 주변 토큰의 맥락적 단서를 기반으로 누락된 토큰을 예측합니다. 마스크된 신호를 재구성함으로써, 사전 훈련된 EEG 인코더는 다양한 사람과 다양한 뇌 활동에 대한 EEG 데이터에 대한 심층적인 이해를 학습합니다. 3.2. 제한된 EEG-이미지 쌍에 대한 안정 확산을 사용한 미세 조정 마스크된 신호 사전 훈련에서 EEG 신호의 효과적인 표현을 얻은 후, 사전 훈련된 안정 확산(SD) 모델을 활용하여 이미지를 생성하는 데 사용합니다. 안정 확산은 데이터 분포를 학습하기 위해 정규 분포된 변수의 노이즈를 점진적으로 제거하는 것을 포함합니다. SD는 보다 유연한 조건부 이미지 생성을 위해 교차 주의 메커니즘으로 증강되며 가장 일반적인 조건은 텍스트 프롬프트입니다. Stable Diffusion은 레이블, 텍스트, 의미 맵과 같은 다양한 유형의 신호에서 고품질 이미지를 생성하는 데 큰 생성 능력을 보여주었습니다. = Stable Diffusion은 잠재 공간에서 작동합니다. 픽셀 공간에서 이미지 x가 주어지면, x는 VQ 인코더 E()에 의해 인코딩되어 해당 잠재 z E(x)를 얻습니다. 조건부 신호는 UNet의 교차 주의 메커니즘에 의해 도입됩니다. 이 교차 주의는 또한 EEG 데이터의 조건부 정보를 통합할 수 있습니다. 구체적으로, EEG 인코더 y의 출력은 프로젝터 70을 사용하여 임베딩 To(y) Є Rxdr에 추가로 투영됩니다. 그런 다음, 이 EEG 표현은 Attention (Q, K, V) = QKT softmax (2K)를 구현하는 교차 주의 계층에 의해 U-Net에 통합됩니다. V. Q = W(i) · αi (zt), K = W{²) · To(y), V = W() · To(y), . (1) 여기서 Xi (%t) Є R³×d는 U-Net의 중간 값을 나타냅니다. W(i) = Rdxd², W(²) € Rdxd 및 W Є Rdxd는 학습 가능한 매개변수가 있는 투영 행렬입니다. KE GT 샘플 샘플 샘플 샘플 melet wws Estou GT Diado 샘플 샘플 샘플 그림 4. 주요 결과. 왼쪽의 이미지는 쌍을 이룬 이미지 데이터를 나타내고 오른쪽의 세 이미지는 샘플링 결과를 나타냅니다. 우리 모델이 EEG 데이터에서 고품질 이미지를 생성하고 이러한 이미지가 EEG 데이터와 정확하게 일치한다는 것을 알 수 있습니다. 미세 조정 프로세스 동안 U-Net의 EEG 인코더와 교차 주의 헤드를 함께 최적화합니다. 안정 확산의 나머지 부분은 고정된 상태로 유지합니다. 미세 조정을 위해 다음 SD 손실 함수를 사용합니다. LSD = Ex‚¤~N(0,1),t [lle E- €0 (xt, t, To(y))||2}], 여기서 to는 UNet으로 구현된 노이즈 제거 함수입니다. (2) 3.3. CLIP 인코더를 사용하여 EEG, 텍스트 및 이미지 공간 정렬 다음으로 사전 학습에서 얻은 EEG 표현을 미세 조정하여 이미지 생성에 더 적합하게 만듭니다. 사전 학습된 안정 확산 모델은 텍스트-이미지 생성을 위해 특별히 훈련되었습니다. 그러나 EEG 신호는 고유한 특성을 가지고 있으며 잠재 공간은 텍스트 및 이미지와 상당히 다릅니다. 따라서 제한된 EEG-이미지 쌍 데이터를 사용하여 안정 확산 모델을 엔드투엔드로 직접 미세 조정해도 사전 학습된 SD의 기존 텍스트 임베딩과 EEG 기능을 정확하게 정렬할 가능성이 낮습니다. CLIP [28]을 사용하고 많은 수의 텍스트-이미지 쌍에 대한 훈련 덕분에 안정 확산의 텍스트 및 이미지 공간이 잘 정렬됩니다. 따라서 EEG, 텍스트 및 이미지 공간의 정렬을 지원하기 위해 추가적인 CLIP [28] 감독을 채택하는 것을 제안합니다. 구체적으로, 사전 훈련된 인코더에서 얻은 EEG 특징은 Brain2Image Ours Airliner Jack-o&#39;-Lantern 그림 5입니다. Brain2Image와 비교. DreamDiffusion에서 생성된 이미지의 품질은 Brain2Image에서 생성된 이미지보다 상당히 높습니다. Panda는 투영 계층을 통해 CLIP과 동일한 차원의 임베딩으로 변환했습니다. 그런 다음 손실 함수를 사용하여 EEG 임베딩과 CLIP 이미지 인코더에서 얻은 이미지 임베딩 간의 거리를 최소화합니다. CLIP 모델은 미세 조정 프로세스 동안 고정됩니다. 손실 함수는 다음과 같이 정의됩니다. Lclip =E₁(I) · h(To(y)) |E1(I)||h(To(y))|&#39; (3) 여기서 h는 투영 계층이고 E는 CLIP 이미지 인코더입니다. 이 손실 함수는 EEG 특징이 이미지와 더 밀접하게 정렬되고 따라서 텍스트 특징과 더 유사해지도록 장려할 수 있습니다. 이런 식으로, 우리는 EEG 신호, 텍스트 및 이미지를 하나의 통합된 공간에 정렬할 수 있습니다. 최적화된 EEG 임베딩 표현은 SD 이미지 생성에 더 적합하며, 이는 생성된 이미지의 품질을 개선합니다. 4.
--- EXPERIMENT ---
s 및 분석 4.1. 구현 세부 정보 EEG 표현 사전 훈련을 위한 데이터. EEG 사전 훈련을 위해 MOABB [21] 플랫폼에서 채널 범위가 30~128인 400명 이상의 피험자로부터 약 120,000개의 EEG 데이터 샘플을 수집했습니다. MOABB는 최신 알고리즘 모음과 함께 공통 형식으로 공개적으로 사용 가능한 EEG 데이터 세트 컬렉션을 제공하여 뇌-컴퓨터 인터페이스(BCI) 알고리즘 개발을 용이하게 하도록 설계된 소프트웨어 패키지입니다. 이 플랫폼을 사용하면 연구자가 자동화된 통계 분석을 사용하여 새로운 알고리즘을 쉽게 검증할 수 있으므로 시간이 많이 걸리고 신뢰할 수 없는 데이터 사전 처리가 필요 없습니다. 이러한 데이터에는 물체 보기, 운동 이미지, 비디오 시청과 같은 작업을 포함하여 다양한 EEG 데이터가 포함되어 있습니다. 우리의 목표는 EEG 데이터 유형에 대한 특정 요구 사항 없이 다양한 EEG 데이터에서 보편적인 표현을 학습하는 것입니다. 데이터 수집에 사용된 장비의 차이로 인해 모델 MSM 사전 학습 CLIP 미세 조정 마스크 비율 E + A 매개변수 Acc(%) 전체 0.E+ A 297M 45.E + A 297M 4.E+ A 18.3M 3.E+ A 297M 32.E+ A 18.3M 24.0.E + A 297M 19.0.E+ A 297M 38.0.E+A 297M 33.0.E+ A 458M 38.0.E + A 162M 36.0.E+A 74M 29.0.E+ A 18.3M 28.0.E만 297M 22.Х 0.E+ A 297M 28.✓ 0.A만 297M 20.표 1. 절제 연구의 정량적 결과. E와 A는 각각 인코더와 교차 어텐션 헤드의 미세 조정을 나타냅니다. 이러한 EEG 데이터 샘플은 상당히 다릅니다. 사전 학습을 용이하게 하기 위해 누락된 채널을 복제된 값으로 채워 모든 데이터를 채널에 균일하게 패딩했습니다. 사전 학습 프로세스 동안 4개의 인접한 시간 단계마다 토큰으로 그룹화하고 각 토큰은 후속 마스크 신호 모델링을 위해 투영 계층을 통해 1024차원 임베딩으로 변환됩니다. 손실 함수는 재구성된 EEG 신호와 원래 EEG 신호 간의 MSE를 계산합니다. 손실은 마스크된 패치에서만 계산됩니다. 재구성은 채널별로가 아니라 전체 채널 세트에서 수행됩니다. 디코더는 사전 학습 후 삭제됩니다. 페어링된 EEG 이미지 데이터. 우리는 &quot;생각에서 이미지로&quot; 실험에 ImageNetEEG [23] 데이터 세트를 채택했습니다. 이는 피험자에게 ImageNet 데이터 세트의 40가지 다른 범주에 속하는 2000개의 이미지를 보여주는 동안 얻은 EEG 기록 모음입니다. 각 범주는 50개의 이미지로 구성되었으며, 각 이미지는 0.5초 동안 제시된 다음 50개의 이미지마다 10초간 멈췄습니다. EEG 데이터는 128채널 Brainvision EEG 시스템을 사용하여 기록되었으며, 총 12000개의 128채널 EEG 시퀀스가 생성되었습니다. 데이터 세트에는 동물(개, 고양이, 코끼리 등), 차량(여객기, 자전거, 자동차 등), 일상 사물(컴퓨터, 의자, 머그잔 등)과 같은 다양한 사물의 이미지가 포함되어 있습니다. 자세한 내용은 관련 참고 문헌 [23]에서 확인할 수 있습니다. 기타 구현 세부 정보. 이미지 생성을 위해 Stable Diffusion의 1.5 버전을 사용합니다. EEG 신호의 마스크 비율은 75%로 설정됩니다. 모든 EEG 신호는 사전 학습 전에 5-95Hz의 주파수 범위 내에서 필터링됩니다. 그런 다음 신호는 공통 길이인 512로 잘립니다. 인코더는 500개 에포크 동안 사전 학습되고 Stable Diffusion으로 300개 더 미세 조정됩니다. EEG의 사전 학습 모델은 [14]의 ViT-Large와 유사합니다. 학습 및 테스트는 동일한 피험자에 대해 수행되었으며 논문에 제시된 모든 결과는 피험자 4의 데이터를 사용하여 생성되었습니다. 4.2. Brain2Image [23]와의 비교 이 섹션에서는 제안된 접근 방식을 최근 작업인 Brain2Image [23]와 비교합니다. Brain2Image는 기존 생성 모델, 즉 변분 자동 인코더(VAE) 및 생성적 적대 네트워크(GAN)를 사용하여 EEG-to-images를 달성합니다. 그러나 Brain2Image는 몇 가지 범주에 대한 결과만 제시하고 참조 구현을 제공하지 않습니다. 이에 따라 Brain2Image 논문에 소개된 몇 가지 범주(즉, 여객기, 잭오랜턴, 판다)에 대한 결과를 정성적으로 비교했습니다. 공정한 비교를 위해 Brain2Image에서 설명한 것과 동일한 주관적 평가 전략을 따랐고 그림 5에 다양한 방법의 생성된 인스턴스를 제시했습니다. 위쪽 행은 Brain2Image에서 생성된 결과를 나타내고 아래쪽 행은 제안하는 방법인 DreamDiffusion에서 생성된 결과를 나타냅니다. DreamDiffusion에서 생성된 이미지의 품질이 Brain2Image에서 생성된 이미지보다 상당히 높은 것을 관찰하여 제안하는 방법의 효능을 검증했습니다. 4.3. 절제 연구 이 섹션에서는 다양한 사례를 사용하여 제안된 프레임워크에 대한 여러 절제 연구를 수행합니다. 50가지 상위 1 정확도 분류 작업을 사용하여 다양한 방법의 효과를 평가합니다. 사전 학습된 ImageNet1K 분류기[14]를 사용하여 생성된 이미지의 의미적 정확성을 확인합니다. 기준 진실 이미지와 생성된 이미지가 모두 분류기에 입력됩니다. 그런 다음, GT 전체 w/o 사전 학습 &amp; CLIP 작은 w/o 사전 학습 &amp; CLIP 큰 Ave いし 인코더 고정 w 사전 학습 w 사전 학습 w/o CLIP only 최적화 인코더 W 사전 학습 &amp; CLIP 그림 6. 절제 연구의 정성적 결과. 생성된 이미지의 상위 1 분류가 선택된 클래스의 기준 진실 분류와 일치하는지 확인합니다. 생성된 이미지의 의미 분류 결과와 기준 진실이 일관되는 한 생성된 이미지는 올바른 것으로 간주됩니다. 사전 학습의 역할. 대규모 EEG 데이터로 사전 학습의 효과를 보여주기 위해 훈련되지 않은 인코더로 여러 모델을 훈련하여 검증을 수행합니다. 모델 중 하나는 전체 모델과 동일한 반면, 다른 모델은 데이터의 과적합을 방지하기 위해 두 개의 레이어만 있는 얕은 EEG 인코딩 레이어를 갖습니다. 훈련 프로세스 동안 두 모델은 클립 감독이 있거나 없이 훈련되었으며 결과는 표 1, 모델 14에 나와 있습니다. 사전 학습 없이 모델의 정확도가 감소한 것을 관찰할 수 있습니다. 마스크 비율. EEG 데이터로 MSM 사전 학습을 위한 최적의 마스크 비율을 결정하기 위해 조사했습니다. 표 1의 모델 5-7에서 볼 수 있듯이 지나치게 높거나 낮은 마스크 비율은 모델 성능에 부정적인 영향을 미칠 수 있습니다. 가장 높은 전체 정확도는 마스크 비율 0.75에서 달성되었습니다. 이 발견은 낮은 마스크 비율이 일반적으로 사용되는 자연어 처리와 달리 EEG에서 MSM을 수행할 때 높은 마스크 비율도 바람직한 옵션임을 시사하기 때문에 중요합니다. CLIP 정렬. 이 방법의 핵심 중 하나는 CLIP 인코더를 사용하여 사전 학습 없이 CLIP을 통해 EEG 표현을 이미지와 정렬하는 것입니다. 이 접근 방식의 효과를 검증하기 위해 표 1에 표시된 대로 실험 13-14를 수행했습니다. CLIP 감독을 사용하지 않으면 모델의 성능이 크게 저하되는 것을 확인할 수 있습니다. 실제로 그림 6의 오른쪽 하단에 표시된 것처럼 사전 훈련이 없더라도 CLIP을 사용하여 EEG 기능을 정렬하면 여전히 합리적인 결과를 얻을 수 있으며, 이는 우리 방법에서 CLIP 감독의 중요성을 강조합니다. GT 샘플 샘플 그림 7. DreamDiffusion의 실패 사례. 5.
--- CONCLUSION ---
이 논문은 비침습적이고 쉽게 얻을 수 있는 뇌 활동 소스인 EEG 신호에서 고품질 이미지를 생성하는 새로운 방법인 DreamDiffusion을 제안합니다. 제안된 방법은 대규모 EEG 데이터 세트에서 얻은 지식과 이미지 확산 모델의 강력한 생성 기능을 활용하여 EEG 기반 이미지 생성과 관련된 과제를 해결합니다. 사전 학습 및 미세 조정 방식을 통해 EEG 데이터는 Stable Diffusion을 사용하여 이미지 생성에 적합한 표현으로 인코딩할 수 있습니다. 저희의 방법은 뇌 활동에서 이미지를 생성하는 분야에서 상당한 진전을 나타냅니다. 한계. 현재 EEG 데이터는 실험 결과에서 범주 수준에서 대략적인 정보만 제공합니다. 그림 7은 일부 범주가 유사한 모양이나 색상을 가진 다른 범주에 매핑되는 몇 가지 실패 사례를 보여줍니다. 이는 인간의 뇌가 물체를 인식할 때 모양과 색상을 두 가지 중요한 요소로 간주하기 때문일 수 있다고 가정합니다. 그럼에도 불구하고 DreamDiffusion은 신경 과학, 심리학 및 인간-컴퓨터 상호 작용과 같은 광범위한 응용 분야에서 사용될 수 있는 잠재력이 있습니다. 참고문헌 [1] Yunpeng Bai, Cairong Wang, Shuzhao Xie, Chao Dong, Chun Yuan, and Zhi Wang. Textir: 텍스트 기반 편집 가능 이미지 복원을 위한 간단한 프레임워크. arXiv 사전 인쇄본 arXiv:2302.14736, 2023. [2] Suzanna Becker and Geoffrey E Hinton. 무작위 점 스테레오그램에서 표면을 발견하는 자기 조직화 신경망. Nature, 355(6356):161–163, 1992. [3] Chris M Bird, Samuel C Berens, Aidan J Horner, and Anna Franklin. 뇌의 색상에 대한 범주적 인코딩. 미국 국립 과학 아카데미 회보, 111(12):4590-4595, 2014. [4] Andrew Brock, Jeff Donahue, and Karen Simonyan. 고충실도 자연 이미지 합성을 위한 대규모 gan 훈련. arXiv 사전 인쇄본 arXiv:1809.11096, 2018. [5] Andrew Brock, Jeff Donahue, Karen Simonyan. 고충실도 자연 이미지 합성을 위한 대규모 GAN 훈련. 제7회 국제 학습 표현 컨퍼런스, ICLR 2019, 루이지애나주 뉴올리언스, 미국, 2019년 5월 6-9일. OpenReview.net, 2019. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020. [7] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, Juan Helen Zhou. 뇌 너머를 보는 것: 시각 디코딩을 위한 희소 마스크 모델링을 사용한 조건부 확산 모델. arXiv 사전 인쇄본 arXiv:2211.06956, 2022. [8] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, Juan Helen Zhou. 뇌 너머를 보는 것: 인간 시각 디코딩을 위한 마스크 모델링 조건부 확산 모델. arXiv, 2022년 11월. [9] Keith M Davis, Carlos de la Torre-Ortiz, Tuukka Ruotsalo. 뇌 감독 이미지 편집. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18480-18489페이지, 2022. [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee 및 Kristina Toutanova. Bert: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. arXiv 사전 인쇄본 arXiv:1810.04805, 2018. [11] Prafulla Dhariwal 및 Alexander Nichol. 확산 모델이 이미지 합성에서 gans를 이김. 신경 정보 처리 시스템의 발전, 34:8780-8794, 2021. [12] Ming Ding, Wendi Zheng, Wenyi Hong 및 Jie Tang. Cogview2: 계층적 변환기를 통한 더 빠르고 더 나은 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2204.14217, 2022. [13] Carl Doersch, Abhinav Gupta, Alexei A Efros. 컨텍스트 예측을 통한 비지도 시각적 표현 학습. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록, 1422-1430페이지, 2015. [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 이미지는 16x16 단어의 가치가 있습니다: 대규모 이미지 인식을 위한 변환기. arXiv 사전 인쇄본 arXiv:2010.11929, 2020. [15] Patrick Esser, Robin Rombach, Bjorn Ommer. 고해상도 이미지 합성을 위한 변환기 길들이기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 12873-12883페이지, 2021. [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. 생성적 적대 네트워크. ACM 커뮤니케이션, 63(11):139–144, 2020. [17] Raia Hadsell, Sumit Chopra, Yann LeCun. 불변 사상 학습을 통한 차원 감소. 2006년 IEEE 컴퓨터 학회 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR&#39;06), 2권, 1735–1742페이지. IEEE, 2006. [18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. 마스크 자동 인코더는 확장 가능한 비전 학습기입니다. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16000-16009페이지, 2022. [19] Jonathan Ho, Ajay Jain, Pieter Abbeel. 확산 확률적 모델의 잡음 제거. 신경 정보 처리 시스템의 발전, 33:6840-6851, 2020. [20] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, Tim Salimans. 고충실도 이미지 생성을 위한 계단식 확산 모델. J. Mach. 학습. Res., 23:47:1-47:33, 2022. [21] Vinay Jayaram 및 Alexandre Barachant. Moabb: bcis에 대한 신뢰할 수 있는 알고리즘 벤치마킹. 신경 공학 저널, 15(6):066011, 2018. [22] Tero Karras, Samuli Laine 및 Timo Aila. 생성적 적대 네트워크를 위한 스타일 기반 생성기 아키텍처. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 4401-4410페이지, 2019. [23] Isaak Kavasidis, Simone Palazzo, Concetto Spampinato, Daniela Giordano 및 Mubarak Shah. Brain2Image: 뇌 신호를 이미지로 변환. ACM 멀티미디어 컨퍼런스 논문집, MM 2017, 미국 캘리포니아주 마운틴 뷰, 2017년 10월 23-27일, 1809-1817쪽. ACM, 2017. [24] Zhifeng Kong 및 Wei Ping. 확산 확률적 모델의 빠른 샘플링에 관하여. CORR, abs/2106.00132, 2021. [25] Milad Mozafari, Leila Reddy 및 Rufin VanRullen. Bigbigan을 사용하여 FMRI 패턴에서 자연스러운 장면 재구성. 2020년 신경망 국제 공동 컨퍼런스(IJCNN), 1-8쪽. IEEE, 2020. [26] Mehdi Noroozi 및 Paolo Favaro. 퍼즐을 풀어 시각적 표현의 비지도 학습. Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI, pages 69–84. Springer, 2016. [27] Furkan Ozcelik, Bhavin Choksi, Milad Mozafari, Leila Reddy, and Rufin VanRullen. 인스턴스 조건부 GANS를 사용한 FMRI 패턴에서 인지된 이미지 재구성 및 의미적 뇌 탐색. 2022년 국제 신경망 공동 컨퍼런스(IJCNN), pages 1–8. IEEE, 2022. [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습에 관한 국제 컨퍼런스, 8748-8763페이지. PMLR, 2021. [29] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 언어 모델은 비지도 멀티태스크 학습자입니다. OpenAI 블로그, 1(8):9, 2019. [30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022. [31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 제로샷 텍스트-이미지 생성. 기계 학습에 관한 국제 컨퍼런스, 8821-8831페이지. PMLR, 2021. [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10684-10695페이지, 2022. [33] Olaf Ronneberger, Philipp Fischer, Thomas Brox. Unet: 생물의학 이미지 분할을 위한 합성 네트워크. Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 제18회 국제 컨퍼런스, 독일 뮌헨, 2015년 10월 5-9일, 회의록, Part III 18, 234-241페이지. Springer, 2015. [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. arXiv 사전 인쇄본 arXiv:2205.11487, 2022. [35] Robin San-Roman, Eliya Nachmani, and Lior Wolf. 생성 확산 모델을 위한 노이즈 추정. CORR, abs/2104.02600, 2021. [36] Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, and Yukiyasu Kamitani. 인간 뇌 활동으로부터 종단 간 심층 이미지 재구성. Frontiers Comput. Neurosci., 13:21, 2019. [37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli. 비평형 열역학을 사용한 심층적 비지도 학습. 기계 학습 국제 컨퍼런스, 2256-2265페이지. PMLR, 2015. [38] Jiaming Song, Chenlin Meng, Stefano Ermon. 확산 암시적 모델 잡음 제거. 2021년 5월 3-7일 오스트리아에서 열린 제9회 학습 표현 국제 컨퍼런스, ICLR 2021, 가상 이벤트. OpenReview.net, 2021. [39] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. 확률적 미분 방정식을 통한 점수 기반 생성 모델링. 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [40] Yu Takagi 및 Shinji Nishimoto. 인간 뇌 활동의 잠재 확산 모델을 사용한 고해상도 이미지 재구성. bioRxiv, 2022-11페이지, 2022. [41] Praveen Tirupattur, Yogesh Singh Rawat, Concetto Spampinato 및 Mubarak Shah. Thoughtviz: 생성적 적대적 네트워크를 사용하여 인간의 생각을 시각화. InACM Multimedia Conference on Multimedia Conference, MM 2018, 서울, 대한민국, 2018년 10월 22-26일, 950-958페이지. ACM, 2018. [42] Arash Vahdat, Karsten Kreis 및 Jan Kautz. 잠재 공간에서의 점수 기반 생성 모델링. Marc&#39;Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, Jennifer Wortman Vaughan 편집자, 신경 정보 처리 시스템의 발전 34: 신경 정보 처리 시스템 연례 컨퍼런스 2021, NeurIPS 2021, 2021년 12월 6-14일, 가상, 11287-11302페이지, 2021. [43] Xiaolong Wang 및 Abhinav Gupta. 비디오를 사용한 시각적 표현의 비지도 학습. IEEE 컴퓨터 비전 국제 컨퍼런스 회의록, 2794-2802페이지, 2015. [44] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, Han Hu. Simmim: 마스크 이미지 모델링을 위한 간단한 프레임워크. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 9653-9663페이지, 2022.
