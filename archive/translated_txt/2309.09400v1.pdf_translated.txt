--- ABSTRACT ---
인상적인 학습 기능을 갖춘 대규모 언어 모델(LLM) 개발의 원동력은 엄청난 모델 크기와 광범위한 교육 데이터 세트입니다. 자연어 처리의 발전과 함께 LLM은 보다 심층적인 조사와 응용을 촉진하기 위해 대중에게 자주 공개되었습니다. 그러나 이러한 LLM, 특히 최근의 최첨단 모델에 대한 교육 데이터 세트의 경우 종종 완전히 공개되지 않습니다. 고성능 LLM에 대한 교육 데이터를 생성하려면 필요한 수준의 품질을 보장하기 위해 광범위한 정리 및 중복 제거가 필요합니다. 따라서 교육 데이터에 대한 투명성이 부족하여 LLM에서 환각 및 편견 문제를 귀속하고 해결하는 연구가 방해를 받아 복제 노력과 커뮤니티의 추가 발전이 방해를 받았습니다. 이러한 과제는 사용 가능한 다국어 텍스트 데이터 세트가 종종 부적절하게 수집 및 정리되는 다국어 학습 시나리오에서 더욱 두드러집니다. 결과적으로 여러 언어로 LLM을 효과적으로 교육할 수 있는 오픈 소스 및 쉽게 사용할 수 있는 데이터 세트가 부족합니다. 이 문제를 해결하기 위해, 우리는 167개 언어로 6.3조 개의 토큰이 있는 상당한 다국어 데이터 세트인 CulturaX를 제시합니다. 이 데이터 세트는 LLM 개발에 맞게 조정되었습니다. 우리의 데이터 세트는 언어 식별, URL 기반 필터링, 메트릭 기반 정리, 문서 정제 및 데이터 중복 제거를 포함하여 모델 학습을 위한 최상의 품질을 달성하기 위해 여러 단계의 엄격한 파이프라인을 통해 세심한 정리 및 중복 제거를 거칩니다. CulturaX는 다국어 LLM의 연구와 발전을 촉진하기 위해 HuggingFace에서 대중에게 완전히 공개되었습니다: https://huggingface.co/datasets/uonlp/CulturaX. 1
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 자연어 처리(NLP) 연구와 응용 프로그램을 근본적으로 변화시켜 수많은 작업에 대한 최첨단 성능을 크게 향상시키고 새로운 떠오르는 능력을 보여주었습니다(Brown et al., 2020; Wei et al., 2022). 변압기 아키텍처(Vaswani et al., 2017)를 기반으로 문헌에서 세 가지 주요 LLM 변형이 탐구되었습니다. 입력 텍스트를 표현 벡터로 인코딩하는 인코더 전용 모델(예: BERT(Devlin et al., 2019) 및 ROBERTa(Liu et al., 2019)), 텍스트를 생성하는 디코더 전용 모델(예: GPT(Radford et al., 2019; Brown et al., 2020)) 그리고 시퀀스 대 시퀀스 생성을 수행하는 인코더-디코더 모델, 예를 들어 BART(Lewis et al., 2020) 및 T5(Raffel et al., 2020). LLM의 놀라운 기능은 주로 모델 크기와 교육 데이터 세트의 끊임없이 확장되는 규모에 의해 추진되었으며, 이는 스케일링 법칙에 의해 최적의 성능을 달성하는 데 필수적인 것으로 간주되었습니다(Hernandez et al., 2022). 예를 들어, 불과 수억 개의 매개변수를 가진 BERT 모델(Devlin et al., 2019)부터 최근의 GPT 기반 모델은 수천억 개의 매개변수를 포함하도록 확장되었습니다(Shoeybi et al., 2019; Scao et al., 2022; Lieber et al., 2021; Chowdhery et al., 2022). 마찬가지로 LLM에 대한 학습 데이터 세트는 기하급수적으로 증가하여 BERT(Devlin et al., 2019; Liu et al., 2019)에 사용된 Wikipedia와 책의 13GB에 불과한 텍스트 데이터에서 Falcon(Penedo et al., 2023), MPT(Mosaic ML, 2023), LLaMa(Touvron et al., 2023), PolyLM(Wei et al., 2023) 및 ChatGPT¹와 같은 최신 모델에 대한 테라바이트 규모의 데이터를 소비하게 되었습니다. 이 분야가 급속히 발전함에 따라 사전 학습된 LLM은 일반적으로 추가 연구와 발전을 촉진하기 위해 대중에게 공개되었습니다. 이러한 모델은 ChatGPT 및 GPT4에서 예시된 것처럼 상용 API를 통해 얻을 수 있거나 Falcon 및 LLaMa에서 예시된 것처럼 오픈 소스 이니셔티브를 통해 얻을 수 있습니다. 그럼에도 불구하고 LLM의 대중적 접근성과는 대조적으로 최첨단 모델을 뒷받침하는 훈련 https://openai.com/blog/chatgpt 데이터 세트는 BLOOM, LLaMa, MPT, Falcon과 같은 오픈 소스 LLM의 경우에도 대부분 엄격하게 보호되는 비밀로 남아 있습니다. 예를 들어 Falcon(Penedo et al., 2023)과 BLOOM(Scao et al., 2022)은 전체 훈련 데이터의 일부만 제공하는 반면 MPT, LLaMa 및 PolyLM의 데이터 세트(Touvron et al., 2023; Wei et al., 2023)는 대중이 접근할 수 없습니다. 한편, 투명성 부족으로 인해 LLM에 대한 심층 분석과 이해가 방해를 받아, 환각, 편견, 독성 콘텐츠(Tamkin et al., 2021; Weidinger et al., 2021; Kenton et al., 2021; Bommasani et al., 2021)와 같은 훈련 데이터에서 비롯된 근본적인 문제를 귀속하고 해결하기 위한 중요한 연구가 방해를 받았습니다. 반면에 훈련 데이터를 은폐하면 LLM 개발이 충분한 리소스를 가진 소수의 이해 관계자에게만 제한되어 기술의 민주화와 이점이 제약되고 더 광범위한 사회 내에서 편견이 심화됩니다. 따라서 LLM에 대한 투명성과 민주화를 달성하려면 고성능 LLM을 훈련하기 위한 대규모 고품질 데이터 세트를 만드는 동시에 더 심층적인 연구와 발전을 촉진하기 위해 대중의 접근성을 보장하는 것이 중요합니다. LLM 영역에서 고품질 학습 데이터 세트는 종종 광범위한 데이터 정리 및 중복 제거 프로세스를 적용하여 제작되며, 방대한 텍스트 컬렉션에서 노이즈가 많고 중복된 콘텐츠를 제거하는 것을 목표로 합니다(Allamanis, 2018; Penedo et al., 2023). 이를 위해 커뮤니티에서 LLM을 위한 이러한 오픈소스 데이터 세트를 개발하려는 최근의 노력이 있었습니다. 예를 들어 1.21T 토큰을 가진 RedPajama(Computer, 2023), 627B 토큰을 가진 SlimPajama², 3T 토큰을 가진 AI2 Dolma³가 있습니다. 그러나 LLM을 위한 기존 오픈소스 데이터 세트의 대부분은 영어에 맞게 조정되어 있어 결과 LLM을 비영어 언어, 특히 언어 리소스가 제한적인 언어에 적용할 때 활용 및 성능이 저하됩니다(Bang et al., 2023; Lai et al., 2023). 영어에 대한 이러한 강조는 또한 전 세계적으로 사용되는 7,개 이상의 다양한 언어에 걸쳐 LLM의 연구 과제와 민주화 문제를 포괄적으로 해결하기 위한 오픈 소스 데이터 세트의 용량을 제한합니다. 2https://www.cerebras.net/blog/slimpajama-a-27b-token-cleaned-and-deduplicated-version-of-r edpajama https://blog.allenai.org/dolma-3-trillion-to kens-open-11m-corpus-9a0ff4b8da 동시에 일부 다국어 데이터 세트가 개발되어 제공되어 여러 언어에 대한 텍스트 데이터를 제공합니다. 그럼에도 불구하고 그 품질과 규모는 고성능 LLM을 교육하는 데 필요한 요구 사항을 충족하지 못합니다. 구체적으로, 위키피디아에서 얻은 다국어 텍스트 데이터 세트는 고품질이지만 LLM을 교육하는 데는 비교적 작은 것으로 간주됩니다(Conneau et al., 2020). OSCAR 데이터 세트(Ortiz Suárez et al., 2019; Ortiz Suárez et al., 2020; Abadji et al., 2021, 2022)4는 160개 이상의 언어에 대한 CommonCrawl(CC)에서 텍스트 데이터를 추출합니다. 그러나 이러한 데이터 세트에는 문서 수준 중복 제거(즉, 데이터 세트에서 유사한 문서 제거)가 부족하여 중복 정보가 포함되고 생성 LLM의 성능이 저하됩니다(Lee et al., 2022). 마찬가지로 mC(Xue et al., 2021), CCAligned(Conneau et al., 2020), WikiMatrix(Schwenk et al., 2021), ParaCrawl(Bañón et al., 2020) 데이터 세트는 모두 100개 이상의 언어를 지원하지만 언어 식별 정확도가 낮아 데이터에 노이즈가 발생합니다(Kreutzer et al., 2022). 이러한 데이터 세트는 또한 MinHash(Broder, 1997)를 통해 퍼지 및 문서 수준에서 중복 제거되지 않습니다. 또한 100개 언어에 걸쳐 다국어 XLM-ROBERTa 모델을 훈련하는 데 사용된 CC100 데이터 세트(Wenzek et al., 2020; Conneau et al., 2020)는 2018년 CC 스냅샷만 고려하므로 크기와 고성능 LLM을 훈련하는 데 필요한 최신 정보의 가용성이 제한됩니다. 오픈소스 데이터 세트에 대한 앞서 언급한 문제를 해결하기 위해, 저희의 작업에서는 167개 언어로 LLM을 훈련하기 위한 CulturaX라는 새로운 다국어 데이터 세트를 소개합니다. CulturaX는 mC4의 최신 버전(버전 3.1.0)을 현재 연도까지 사용 가능한 모든 OSCAR 코퍼스와 병합하여 배포판 20.19, 21.09, 22.01 및 23.01을 포함합니다. 이 병합을 통해 27TB의 텍스트 데이터와 6조 개의 토큰으로 구성된 대규모 다국어 데이터 세트가 생성되고 LLM 개발을 위한 최신 데이터를 제공합니다. 저희 데이터 세트의 절반 이상은 영어가 아닌 언어에 전념하여 데이터 크기를 크게 늘리고 다국어 시나리오에서 훈련 모델의 실행 가능성을 향상시킵니다. 중요한 점은 CulturaX가 문서 수준에서 광범위하게 정리되고 중복 제거되어 여러 언어에 대한 LLM을 훈련하는 데 가장 높은 품질을 제공한다는 것입니다. 특히, 저희의 데이터 정리 프로세스에는 저품질 데이터를 제거하기 위해 서명된 포괄적인 파이프라인 dehttps://oscar-project.org가 포함됩니다. 여기에는 노이즈가 있는 텍스트, 비언어적 콘텐츠, 유해한 데이터, 잘못된 언어 식별 등이 제거됩니다. 저희의 데이터 정리 파이프라인은 사분위 범위(IQR) 방법(Dekking et al., 2007)의 변형을 사용하여 다양한 데이터 세트 메트릭(예: 불용어 비율, 데이터 복잡도, 언어 식별 점수)에 적합한 임계값을 선택합니다. 이는 데이터 세트의 노이즈가 있는 이상치를 필터링하는 데 사용할 수 있습니다. 따라서 저희는 대량의 데이터 샘플에 대해 계산된 분포의 백분위수를 활용하여 각 필터링 메트릭과 언어에 대한 임계값 선택 프로세스를 효과적으로 안내합니다. 마지막으로 저희는 근접 중복 제거 방법 MinHashLSH(Broder, 1997; Leskovec et al., 2020)와 URL을 기반으로 데이터 세트 내 언어의 데이터에 대한 광범위한 중복 제거를 수행하여 다국어 LLM을 훈련하기 위한 고품질 데이터를 얻습니다. 저희의 데이터 세트는 다국어 학습을 위한 추가 연구 및 개발을 촉진하기 위해 대중에게 완전히 공개될 것입니다. 저희가 아는 한, CulturaX는 LLM 및 NLP 애플리케이션을 위해 심층적으로 정리되고 중복이 제거된 지금까지 가장 큰 오픈 소스 다국어 데이터 세트입니다. 2 다국어 데이터 세트 생성 LLM을 위한 다국어 공개 데이터 세트를 개발하기 위해 저희의 전략은 저희가 사용할 수 있는 가장 큰 두 다국어 데이터 세트인 mC4(Xue et al., 2021)와 OSCAR(Ortiz Suárez et al., 2019; Abadji et al., 2021, 2022)를 결합하는 것입니다. 그런 다음 정리 및 중복 제거의 두 가지 주요 단계를 포함하는 광범위한 파이프라인으로 데이터를 처리하여 다국어 LLM을 위한 엄청나고 고품질의 데이터 세트를 생성합니다. mC4는 다국어 문서 수준 데이터 세트로, 원래 101개 언어에 대해 다국어 인코더디코더 모델 mT5(Xue et al., 2021)를 학습하기 위해 만들어졌습니다. 이 데이터 세트는 3줄 미만의 긴 줄이 있는 페이지(줄 길이 필터), 잘못된 단어가 있는 페이지, 문서 간에 중복된 줄을 제거하여 CC의 71개 월별 스냅샷에서 추출되었습니다. mC4의 페이지에 대한 언어 식별은 작은 피드포워드 네트워크(Xue et al., 2021)인 cld3 도구(Botha et al., 2017)5에 의해 수행됩니다. 언어 신뢰도가 0.95% 미만인 모든 페이지는 제외됩니다. mC4는 문서 수준에서 정확한 일치로 중복 제거되지만, 퍼지 문서 수준 중복 제거는 수행되지 않습니다. 우리는 mC4의 최신 버전(버전 3.1.0)을 활용합니다.https://github.com/google/cld&quot;https://huggingface.co/datasets/mcOSCAR 23.OSCAR 22.7% OSCAR 21.9% OSCAR 20.7% 11% MC66% 그림 1: 이 작업에서 AllenAI가 준비한 초기 데이터 세트에서 mCand OSCAR의 문서 수 분포. 데이터 세트의 주목할 만한 측면은 CC에서 추출한 선택한 데이터 세트인 mCand OSCAR의 웹 기반 출처와 관련이 있습니다. 이는 LLM을 훈련하기 위해 The Pile(Gao et al., 2020) 및 BookCorpus(Zhu et al., 2015)와 같은 큐레이트된 데이터 세트에 의존했던 이전 작업(Radford et al., 2019; Mosaic ML, 2023; Touvron et al., 2023)과 다릅니다. 더 높은 전반적인 품질. 그러나 다국어 설정의 맥락에서, 우리는 우수한 품질의 큐레이팅된 데이터 세트를 다양한 언어로 사용할 수 없을 수 있으므로 웹 스크래핑된 데이터 세트가 더 적합한 접근 방식이 될 수 있다고 주장합니다. 웹 스크래핑된 데이터를 사용하는 우리의 전략은 여러 언어에서 효율적인 데이터 수집을 용이하게 하여 향상된 교육 데이터 규모에 기여합니다. 더욱이, 최근 연구에서는 웹 스크래핑된 데이터를 정리하여 최첨단 LLM을 생성하는 효과가 입증되었습니다(Raffel et al., 2020; Almazrouei et al., 2023). 총 mC4와 OSCAR의 조합은 추가 처리를 위한 135억 개의 문서를 제공합니다. 그림 1은 우리의 초기 데이터 세트에서 mC4와 사용 가능한 네 가지 버전의 OSCAR에 대한 문서 수의 분포를 보여줍니다. 2.1 데이터 정리 mC4와 OSCAR 데이터세트의 조합을 고려하여, 먼저 언어 식별, ULR 기반 필터링, 메트릭 기반 정리, 문서 정제를 포함하여 데이터에서 노이즈가 많고 잘못된 콘텐츠를 제거하기 위한 포괄적인 데이터 정리 절차를 수행합니다.언어 식별: 특정 문제는 mC와 OSCAR(각각)에 대해 두 가지 다른 언어 식별 도구인 cld3와 FastText를 사용하는 것과 관련이 있습니다.이전 연구에서 cld3가 FastText보다 훨씬 나빠 mC4에 대해 훨씬 더 많은 언어 감지 오류를 유발하는 것으로 나타났습니다(Kreutzer et al., 2022).사실, 다른 여러 언어 감지기와 비교했을 때 FastText는 벤치마크 데이터세트에서 최첨단 성능을 보여주었습니다.이를 위해 첫 번째 데이터 정리 단계에서는 FastText를 적용하여 mC4의 문서에 대한 언어를 다시 예측합니다.예측된 언어가 mC4에서 제공된 언어와 다른 문서는 데이터세트에서 제거됩니다. 그 이유는 언어 감지기 cld3 및 FastText에 혼란을 주는 문서를 피하고, 따라서 데이터에 노이즈가 발생할 가능성이 있기 때문입니다. 마지막으로, 최상의 품질을 보장하기 위해 mC4에서 발견되지만 FastText에서 지원하지 않는 언어에 대한 데이터를 제거합니다. URL 기반 필터링: 다음 단계에서는 알려진 독성 및 유해 출처의 페이지를 제거하여 데이터에서 관련 위험을 줄이는 것을 목표로 합니다. 특히, 툴루즈 대학교에서 제공하는 최신 UT1 URL 및 도메인 블랙리스트를 활용하여 학교 관리자의 인터넷 사용 규제를 지원합니다. 이 목록에는 포르노, 불평, 해킹을 포함하여 LLM 교육에서 삭제해야 하는 다양한 주제의 사이트가 포함됩니다. 일주일에 두세 번 업데이트되는 블랙리스트에는 인간과 로봇(예: 검색 엔진, 알려진 주소 및 인덱스)이 기여한 370만 개 이상의 레코드가 포함됩니다(Abadji et al., 2022). 따라서 블랙리스트의 사이트와 연관된 URL이 일치하는 모든 페이지를 데이터 세트에서 제거합니다. 이 단계는 블랙리스트가 이전에 mCdataset에 사용되지 않았기 때문에 데이터 세트에 유용합니다. 또한 OSCAR가 이미 이 블랙리스트를 데이터 정리에 사용했지만, 우리의 접근 방식은 목록에서 최신 정보를 통합하는데, 이는 현재 OSCAR 배포판에서는 사용할 수 없을 수 있습니다. 지표 기반 정리: BigScience의 BLOOM용 ROOTS 코퍼스의 데이터 처리 파이프라인에서 동기를 얻어 데이터 세트의 품질을 향상시키기 위해(Laurençon et al., 2022; Scao et al., 2022), 다양한 데이터 세트 지표에 대한 분포를 추가로 활용하여 이상 문서를 식별하고 필터링합니다. 각 지표는 데이터 세트 내의 모든 문서에 대해 단일 값 7https://modelpredict.com/ language-identification-survey를 제공하여 각 문서에 대한 number_words, stopword_ratios, perplexity_score와 같은 특정 속성을 정량화합니다. 각 메트릭과 데이터 세트 내의 가능한 값 범위에 대해 임계값을 결정하여 범위를 정상 범위와 비정상 범위의 두 영역으로 분할합니다.비정상 범위는 표준에서 크게 벗어나는 메트릭 값을 보이는 문서에 지정되어 이를 이상치/노이즈로 분류하고 결과적으로 이러한 이상치는 데이터 세트에서 제거됩니다.따라서 아래에 설명된 대로 데이터 세트를 정제하는 데 집합적으로 사용될 포괄적인 데이터 세트 메트릭 배열을 사용합니다.• 단어 수 • • 문자 반복 비율 단어 반복 비율 • 특수 문자 비율 • • 불용어 비율 플래그가 지정된 단어 비율 언어 식별 신뢰도 • 복잡도 점수 • 문서 길이(문자 수) • • 줄 수 • 짧은 줄 길이 비율 짧은 줄 비율 마지막 네 가지 메트릭은 OSCAR 데이터 세트에서 제안하는 반면 다른 메트릭은 OSCAR 데이터를 처리하기 위한 BigScience ROOTS 코퍼스의 파이프라인에서 상속받습니다. 복잡도 점수의 경우 BigScience ROOTS 코퍼스를 따라 SentencePiece 토크나이저(Kudo, 2018)와 KenLM 라이브러리(Heafield, 2011)에서 제공하는 5그램 KneserNey 언어 모델을 Wikipedia의 20230501 덤프를 사용하여 훈련합니다. 이러한 KenLM 모델을 기반으로 높은 복잡도 점수를 표시하는 문서는 Wikipedia 문서와 현저히 다른 것으로 간주됩니다. 이는 데이터 세트에서 제외될 노이즈 수준을 나타냅니다(Wenzek et al., 2020). 토크나이저는 또한 메트릭을 위해 문서의 단어/토큰 수를 얻는 데 사용됩니다. 향후 탐색을 용이하게 하기 위해 HuggingFace에서 KenLM 모델을 공개적으로 릴리스합니다. 크롤링 오류와 낮은 품질의 소스로 인해 웹에서 큐레이팅된 데이터에 반복되는 정보(예: 단어, 문단)가 나타날 수 있으며, 이는 LLM 훈련에 부정적인 결과를 초래할 수 있습니다(Holtzman et al., 2019). 따라서 문자 및 단어 반복 비율은 지나치게 https://huggingface.co/uonlp/kenlm 반복되는 정보가 있는 문서를 피하도록 설계되었습니다. 특수 문자, 불용어 또는 플래그가 지정된 단어의 빈도가 높으면 노이즈가 많고 품질이 낮은 문서를 나타낼 수 있습니다. 따라서 다양한 언어의 불용어 및 플래그가 지정된 단어 목록을 활용하여 문서 제거 비율을 계산합니다. BigScience ROOTS에서 제공하는 13개 언어의 불용어 및 플래그가 지정된 단어 목록 외에도 다른 언어의 이러한 유형의 단어에 대한 사전을 추가로 수집합니다. 다양한 언어의 개인 GitHub 계정에서 공유된 목록을 우선시합니다. 이러한 목록은 종종 모국어 화자가 작성하고 더 높은 품질을 보이기 때문입니다. 또한 언어 식별 신뢰도가 낮으면 데이터에 노이즈가 많은 언어 구조가 있을 수도 있습니다. 따라서 데이터 세트의 각 문서에 대해 FastText가 데이터 필터링을 돕기 위해 해당 언어에 할당하는 확률을 통해 언어 식별 신뢰도를 얻습니다. 마지막으로 짧은 줄 기반 기준의 경우 OSCAR에서 사용하는 것처럼 줄을 짧은 것으로 분류하기 위해 100자의 임계값을 구현합니다. 짧은 줄이 과도하게 나타나는 문서는 데이터 세트에 보관되지 않습니다. 임계값 선택: 데이터 세트 메트릭 집합을 고려할 때 중요한 질문은 각 메트릭과 언어에 적합한 임계값을 선택하여 고품질 다국어 데이터를 생성하는 것입니다. BigScience ROOTS 프로젝트(Laurençon et al., 2022)에서 이 선택 프로세스는 13개 언어의 모국어 화자가 수행합니다. 그 결과 임계값은 나머지 46개 언어에 적용됩니다. 이 프로젝트는 언어당 수천 개의 문서 샘플을 색인화하는 시각화 인터페이스를 제공하여 사용자가 메트릭에 대한 임계값을 조정하면서 데이터 통계를 모니터링할 수 있도록 합니다. 그러나 이 프로세스는 경험이 풍부한 모국어 화자가 필요하기 때문에 다른 언어로 쉽게 확장할 수 없으며, 이로 인해 상당한 비용이 발생합니다. 게다가 제한된 샘플 크기로 인해 전체 데이터 세트에 대해 선택한 임계값의 대표성이 떨어집니다. 분석 결과 BigScience ROOTS 내의 특정 언어에 대해 선택한 일부 임계값이 전체 데이터 세트의 값 범위를 거의 벗어나 해당 메트릭이 비활성화되는 것을 확인했습니다. 이러한 문제를 해결하기 위해 우리는 사분위 범위(IQR) 방법의 변형(Dekking et al., 2007)을 활용하여 데이터 세트의 필터링 메트릭에 적합한 임계값을 선택합니다. 각 메트릭과 언어에 대해 전체 데이터 세트에서 해당 언어의 가능한 값 분포를 생성합니다. 스페인어와 러시아어와 같이 상당한 양의 데이터가 있는 언어의 경우 예외가 있습니다. 이 언어의 경우 데이터의 25%만 사용하여 이러한 분포를 계산합니다. 그런 다음 분포의 Q1 및 Q3 백분위수(Q1 Q3)를 계산하여 필터링 메트릭의 임계값으로 사용합니다. 특히 낮은 Q1 백분위수는 높은 값(예: 언어 식별 신뢰도)을 선호하는 메트릭에 대해 선택되고, 낮은 값(예: 복잡도 점수 및 문서 길이)을 선호하는 메트릭은 높은 Q3 백분위수를 활용합니다. 우리는 (25, 75), (20, 80), (15,85), (10,90), (5, 95)를 고려하여 (Q1, Q3)에 대한 다양한 값을 조사합니다. Q1 = 10 및 Q2 90을 선택하면 검토 대상 언어 샘플에 대한 최상의 데이터 품질을 얻을 수 있습니다. 임계값 선택에 백분위수를 사용하면 BigScience ROOTS 프로젝트에서 사용한 것보다 각 언어에 대해 더 광범위한 데이터 샘플을 효율적으로 추출할 수 있다는 점을 강조할 가치가 있습니다. 이를 통해 다양한 언어에 대한 전체 데이터 세트에 대해 더 신뢰할 수 있는 임계값이 생성됩니다. 특히, 메트릭의 값 분포를 계산하는 데 25%의 데이터 샘플만 사용되는 대규모 언어의 경우, 동일한 선택된 필터링 임계값을 적용할 때 전체 데이터 세트에 대한 삭제된 데이터의 비율이 데이터 샘플의 비율과 밀접하게 일치한다는 것을 관찰했습니다. 이는 방법론을 통해 선택된 임계값의 대표성을 강조합니다. 마지막으로, 주어진 언어의 메트릭에 대한 임계값이 결정되면 메트릭의 임계값을 초과하고 데이터의 불리한 범위에 들어가는 모든 문서를 제거합니다.문서 정제: 이전 정리 단계는 데이터 세트 수준에서 수행되며, 데이터 세트에서 품질이 낮은 문서를 제거하는 것을 목표로 합니다.이 단계에서는 보관된 문서를 추가로 정리하여 품질을 개선합니다.이전 메트릭 기반 필터링 단계는 노이즈가 많은 문서를 제거하는 데 중요한 역할을 하며, 이를 통해 이 단계에서 효과적인 문서 정리 규칙을 개발하는 프로세스가 간소화된다는 점에 유의하는 것이 중요합니다.특히, mC4와 OSCAR의 문서는 인터넷에서 크롤링된 HTML 페이지에서 추출되므로 상당 부분에 긴 JavaScript 줄과 외부 콘텐츠를 포함한 크롤링 및 추출 오류가 있을 수 있습니다.결과적으로 이러한 문서를 필터링하면 데이터 세트 내의 문서를 정리하기 위한 규칙을 설계하는 작업이 크게 간소화됩니다.따라서 각 문서에 대해 일련의 작업을 통해 노이즈가 많거나 관련성이 없는 부분을 제거합니다. 첫째, 각 문서의 끝에 있는 짧은 줄을 제거합니다. 이러한 줄에는 일반적으로 푸터 세부 정보나 웹사이트의 도움이 되지 않는 정보가 들어 있기 때문입니다. 둘째, JavaScript(JS) 키워드 목록에서 단어가 포함된 줄을 제거합니다(예: &quot;
--- RELATED WORK ---
영어: 다른 NLP 작업과 비교했을 때 언어 모델은 레이블이 지정되지 않은 데이터로 학습할 수 있으므로 효율적인 데이터 수집을 통해 https://github.com/ChenghaoMou/text-dedup/ tree/main에 대한 거대한 규모를 생성할 수 있습니다. #문서(M) #토큰 코드 언어 URL 초기 필터링 메트릭 필터링 en 영어 5783.5766.3586.MinHash URL 중복 제거 중복 제거 3308.30 3241.Filtering Rate (%) (B) (%) ru 러시아어 1431.1429.922.es 스페인어 844.842.530.de 독일어 863.861.515.845.64 799.479.65 450.447.06 420.fr 프랑스어 711.709.439.387.zh 중국어 444.444.258.it 이탈리아어 406.406.254.pt 포르투갈어 347.346.217.363.222.37 218.226.42 211.200.11 190.43.44.16 737.46.60 373.51.34 357.48.89 319.2846.97 45.11.5.5.5.50.80 227.48.06 165.3.2.45.24 136.2.pl 폴란드어 270.269.170.151.71 142.47.37 117.1.ja 일본어 247.247.137.114.64 111.55.11 107.1.vi 베트남어 182.182.118.108.102.44.98.1.nl 네덜란드어 238.238.148.125.117.50.80.1.ar 아랍어 132.132.84.77.74.44.69.1.tr 터키어 183.183.109.99.94.48.64.1.CS 체코어 136.136.80.69.65.52.56.0.fa 페르시아어 118.118.70.62.59.49.45.0.hu 헝가리어 88.88.53.46.44.50.43.0.el 그리스어 100.100.61.54.51.48.43.0.ro 루마니아어 89.89.45.42.40.54.39.0.SV 스웨덴어 103.102.58.52.49.51.38.0.uk 우크라이나어 81.81.50.47.44.45.38.0.fi 핀란드어 59.59.36.32.30.49.28.0.ko 한국어 46.45.25.21.20.55.24.0.da 덴마크어 53.52.28.26.25.52.22.0.bg 불가리아어 47.46.28.25.24.48.22.0.no 노르웨이어 40.40.20.19.18.52.18.0.hi 힌디어 35.35.22.20.19.44.16.0.sk 슬로바키아어 40.39.22.19.18.53.16.0.th 태국어 49.48.26.21.20.57.15.0.lt 리투아니아어 27.27.15.14.13.50.14.0.са 카탈로니아어 31.31.18.16.15.50.12.0.id 인도네시아어 48.48.25.23.23.51.12.0.bn 방글라어 20.20.13.13.12.40.9.0.et 에스토니아어 16.16.9.8.8.50.8.0.sl 슬로베니아어 15.15.8.7.7.52.8.0.lv 라트비아어 14.14.8.7.7.49.7.0.he 히브리어 10.10.5.4.4.56.4.0.sr 세르비아어 7.7.4.4.4.48.4.0.ta 타밀어 8.8.5.4.4.46.4.0.sq 알바니아어 9.9.5.5.5.44.3.0.az 아제르바이잔어 9.9.5.5.5.47.3.0.전체(42개 언어) 13397.13366.8254.7471.7181.46.6267.99.전체(167개 언어) 13506.76 13474.94 8308.7521.7228.46.48 6308.42 100.표 1: 데이터 통계 데이터 세트에서 토큰 비율이 0.05%를 넘는 42개 언어. &quot;#Documents (M)&quot; 레이블로 그룹화된 열은 해당 정리 및 중복 제거 단계 이후 각 언어의 문서 수를 나타냅니다. 토큰 수는 최종 데이터 세트(즉, 모든 정리 및 중복 제거 단계 이후)를 기반으로 합니다. 훈련 데이터. LLM 훈련에 일반적으로 사용되는 두 가지 주요 데이터 유형은 큐레이트된 데이터와 웹 크롤링 데이터입니다. 큐레이션된 데이터는 일반적으로 타깃 소스 및 도메인에서 잘 작성되고 잘 형식화된 텍스트로 구성됩니다. 예를 들어, 위키피디아 기사, 책, 뉴스 기사 및 과학 논문은 &quot;The Pile&quot;(Gao et al., 2020) 및 &quot;BookCorpus&quot;(Zhu et al., 2015) 데이터 세트에 사용됩니다. 반면, 웹 크롤링 데이터는 블로그, 소셜 미디어 게시물, 뉴스 기사 및 광고와 같이 형식 및 쓰기 스타일 측면에서 상당히 다양한 인터넷의 광범위한 소스에서 수집된 텍스트를 포함합니다. CommonCrawl(CC)은 12년 동안 인터넷에서 페타바이트 규모의 데이터를 수집한 널리 사용되는 웹 크롤링 저장소입니다. 이를 위해 큐레이션된 데이터는 종종 더 높은 품질을 가지고 있다고 간주되며, 이로 인해 BERT(Devlin et al., 2019) 및 GPT-2(Radford et al., 2019)와 같은 초기 LLM을 훈련하는 데 선호되었습니다. 그러나 더 큰 모델에 대한 수요가 성장함에 따라 웹 크롤링 데이터는 최근 LLM의 학습 데이터에 상당 부분을 기여하기 때문에 더 많은 주목을 받고 있습니다.예를 들어 ROBERTA(Liu 등, 2019), BART(Lewis 등, 2020), T5(Raffel 등, 2020), GPT-3(Rae 등, 2021), LLaMa(Touvron 등, 2023), MPT(MosaicML, 2023), Falcon(Almazrouei 등, 2023) 등이 있습니다. 따라서 이러한 LLM을 학습하기 위해 C4(Raffel 등, 2020), CC-News(Nagel), STORIES(Trinh and Le, 2018)를 포함하여 다양한 CC 추출물이 생성되었습니다. 학습 데이터의 접근성과 관련하여, 초기 LLM을 학습하는 데 사용된 데이터 세트는 종종 대중에게 공개됩니다(Devlin et al., 2019; Raffel et al., 2020). 그러나 가장 최근의 최첨단(SOTA) 생성 LLM의 경우, 학습 데이터 세트가 상업적 이익으로 인해 완전히 공개되지 않습니다. 이는 ChatGPT 및 GPT-4와 같은 독점 모델뿐만 아니라 LLaMa, MPT, Falcon, BLOOM과 같이 오픈소스 모델이라고 주장하는 모델에도 적용됩니다(Scao et al., 2022). 기존 LLM의 투명성 문제를 해결하기 위해, 최첨단 LLM, 즉 RedPajama(Computer, 2023), SlimPajama, AIDolma에 대한 학습 데이터 세트를 복제하여 공개하려는 최근의 노력이 있었습니다. 이러한 데이터 세트의 주요 차이점은 LLM 교육을 위한 고품질을 보장하기 위해 세심하게 정리되고 문서 수준에서 중복 제거된 대규모 텍스트 데이터와 관련이 있습니다. 그럼에도 불구하고 이러한 오픈소스 데이터 세트의 일반적인 단점은 주로 영어 데이터에 초점을 맞추고 다른 언어에 대한 데이터는 제한적이라는 것입니다. LLM 교육을 위한 다국어 대규모 데이터 세트를 얻으려면 CC와 같은 웹 스크레이프 데이터 세트를 활용하여 여러 언어로 최신 정보를 사용하여 효율적인 데이터 수집을 수행하는 것이 더 편리합니다. 또한 고성능 LLM의 고품질을 보장하려면 노이즈가 많고 관련성이 없는 콘텐츠(예: 품질이 낮은 기계 생성 텍스트 및 성인 콘텐츠)를 피하기 위해 다국어 데이터를 광범위하게 정리하고 중복 제거해야 합니다(Trinh 및 Le, 2018; Kreutzer et al., 2022; Raffel et al., 2020). 따라서 고품질 데이터 세트를 생성하기 위한 일반적인 데이터 처리 파이프라인에는 FastText(Joulin 등, 2016), CC-Net(Wenzek 등, 2020), BLOOM 모델을 위한 BigScience ROOTS 코퍼스(Laurençon 등, 2022; Scao 등, 2022), Falcon 모델을 위한 RefinedWeb 데이터 세트(Penedo 등, 2023; Almazrouei 등, 2023) 및 LLaMa 모델을 훈련하기 위한 데이터 세트(Touvron 등, 2023)에서 보여준 것처럼 여러 단계가 포함될 수 있습니다. 첫 번째 단계는 이러한 파이프라인에서 언어를 식별하여 데이터를 해당 언어에 적절히 할당하는 것이 필요합니다(Joulin 등, 2016). 다음 단계에서는 특수 문자, 짧은 줄, 나쁜 단어 등의 비율에 따라 바람직하지 않은 콘텐츠를 필터링하기 위한 다양한 데이터 세트별 규칙과 휴리스틱이 특징입니다(Grave et al., 2018; Laurençon et al., 2022). 또한 데이터는 가벼운 모델(예: KenLM 언어 모델(Heafield, 2011))을 통해 필터링하여 노이즈가 많은 문서를 피할 수 있습니다(Wenzek et al., 2020). 마지막으로 데이터 중복 제거를 수행하여 유사하거나 반복되는 정보를 제거해야 합니다(Laurençon et al., 2022; Penedo et al., 2023). 이와 관련하여 중요한 단계는 문서 수준에서 퍼지 중복 제거를 포함하는데, 예를 들어 MinHash(Broder, 1997)를 통해 유사한 문서를 제거하고, 이를 통해 기억을 완화하고 결과 LLM에 대한 일반화를 개선하는 것입니다(Lee et al., 2022). 이를 위해 mC4(Xue et al., 2021), OSCAR(Ortiz Suárez et al., 2019), CC100(Wenzek et al., 2020; Conneau et al., 2020) 및 BigScience ROOT 코퍼스(Laurençon et al., 2022)와 같이 여러 언어로 된 텍스트 데이터가 있는 다국어 오픈소스 데이터 세트가 있지만, 그 품질과 규모는 특히 GPT와 같은 생성 모델의 경우 LLM을 효과적으로 훈련하는 데 필요한 요구 사항을 충족하지 못합니다. 예를 들어, 서론에서 강조했듯이 mC4와 OSCAR는 모두 문서 수준의 데이터에 대한 퍼지 중복 제거가 부족합니다. mCalso는 cld3를 사용하기 때문에 언어 식별이 좋지 않습니다. BigScience ROOTS는 46개 언어에 대한 작은 샘플 데이터만 제공하는 반면 CC100은 2018년 이후의 정보가 없습니다. 따라서 저희 데이터 세트 CulturaX는 기존 데이터 세트의 문제를 포괄적으로 해결하여 LLM을 훈련하는 데 쉽게 사용할 수 있고 고품질의 데이터를 제공하는 다국어, 오픈 소스, 대규모 데이터 세트를 제공합니다. 5 결론 저희는 167개 언어에 대한 텍스트 데이터가 있는 새로운 다국어 데이터 세트인 CulturaX를 제시합니다. 저희 데이터 세트는 포괄적인 파이프라인을 통해 정리되고 중복 제거되어 6.3조 개의 토큰을 생성합니다. 따라서 CulturaX는 여러 언어에 대한 고성능 LLM을 훈련하는 데 쉽게 사용할 수 있는 대규모 고품질 데이터 세트입니다. 저희 데이터는 다국어 학습에 대한 추가 연구와 응용 프로그램을 촉진하기 위해 대중이 공개적으로 접근할 수 있습니다. 참고 문헌 Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, Benoît Sagot. 2022. 더 깨끗한 문서 지향 다국어 크롤링 코퍼스를 향해. 제13회 언어 자원 및 평가 컨퍼런스 회의록, 4344-4355페이지, 프랑스 마르세유. 유럽 언어 자원 협회. Julien Abadji, Pedro Javier Ortiz Suárez, Laurent Romary, Benoît Sagot. 2021. Ungoliant: 매우 대규모 다국어 웹 코퍼스 생성을 위한 최적화된 파이프라인. 대규모 코퍼스 관리의 과제에 대한 워크숍 회의록(CMLC-9) 2021. 리머릭, 2021년 7월 12일(온라인 이벤트). Miltiadis Allamanis. 2018. 코드의 기계 학습 모델에서 코드 중복의 부정적 영향. 2019년 ACM SIGPLAN 새로운 아이디어, 새로운 패러다임, 프로그래밍 및 소프트웨어에 대한 성찰에 대한 국제 심포지엄 회의록. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi et al. 2023. Falcon-40B: 최첨단 성능을 갖춘 개방형 대규모 언어 모델. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale Fung. 2023. 추론, 환각 및 상호 작용에 대한 chatgpt의 멀티태스크, 다국어, 멀티모달 평가. ArXiv, abs/2302.04023. Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William 웨이츠, 디온 위긴스, 하우메 사라고사. 2020. ParaCrawl: 웹 규모의 병렬 말뭉치 획득. 전산언어학협회 제58차 연차총회 진행, 4555-4567페이지, 온라인. 전산언어학협회. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli 등. 2021. 기초 모델의 기회와 위험. ArXiv, abs/2108.07258. Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, Slav Petrov. 2017. 소규모 피드포워드 네트워크를 사용한 자연어 처리. 경험적 연구 컨퍼런스 회의록에서
--- METHOD ---
(Dekking et al., 2007) 다양한 데이터 세트 메트릭(예: 불용어 비율, 데이터 복잡도, 언어 식별 점수)에 적합한 임계값을 선택하여 데이터 세트의 노이즈 이상치를 필터링하는 데 사용할 수 있습니다. 따라서 대량의 데이터 샘플에서 계산된 분포의 백분위수를 활용하여 각 필터링 메트릭과 언어에 대한 임계값 선택 프로세스를 효과적으로 안내합니다. 마지막으로 MinHashLSH(Broder, 1997; Leskovec et al., 2020)의 근접 중복 제거 방법과 URL을 기반으로 데이터 세트 내 언어의 데이터에 대한 광범위한 중복 제거를 수행하여 다국어 LLM을 훈련하는 데 사용할 수 있는 고품질 데이터를 제공합니다. 데이터 세트는 다국어 학습을 위한 추가 연구 개발을 촉진하기 위해 대중에게 완전히 공개됩니다. 저희가 아는 한, CulturaX는 LLM 및 NLP 애플리케이션을 위해 심층적으로 정리되고 중복 제거된 지금까지 가장 큰 오픈 소스 다국어 데이터 세트입니다. 2 다국어 데이터 세트 생성 LLM을 위한 다국어 공개 데이터 세트를 개발하기 위해, 우리의 전략은 우리가 사용할 수 있는 가장 큰 두 다국어 데이터 세트인 mC4(Xue et al., 2021)와 OSCAR(Ortiz Suárez et al., 2019; Abadji et al., 2021, 2022)를 결합하는 것입니다. 그런 다음 정리와 중복 제거의 두 가지 주요 단계를 포함하는 광범위한 파이프라인으로 데이터를 처리하여 다국어 LLM을 위한 엄청나고 고품질의 데이터 세트를 생성합니다. mC4는 다국어 문서 수준 데이터 세트로, 원래 101개 언어에 대한 다국어 인코더디코더 모델 mT5(Xue et al., 2021)를 학습하기 위해 만들어졌습니다. 이 데이터 세트는 3줄 미만의 긴 줄이 있는 페이지(줄 길이 필터), 잘못된 단어가 있는 페이지 및 문서 간에 중복된 줄을 제거하여 CC의 71개 월별 스냅샷에서 추출되었습니다. mC4의 페이지에 대한 언어 식별은 cld3 도구(Botha et al., 2017)5에 의해 수행되는데, 이는 작은 피드포워드 네트워크(Xue et al., 2021)입니다. 언어 신뢰도가 0.95% 미만인 모든 페이지는 제외됩니다. mC4는 문서 수준에서 정확한 일치로 중복 제거되지만, 퍼지 문서 수준 중복 제거는 수행되지 않습니다. 우리는 mC4의 최신 버전(버전 3.1.0)을 활용합니다.https://github.com/google/cld&quot;https://huggingface.co/datasets/mcOSCAR 23.OSCAR 22.7% OSCAR 21.9% OSCAR 20.7% 11% MC66% 그림 1: 이 작업에서 AllenAI가 준비한 초기 데이터 세트에서 mCand OSCAR의 문서 수 분포. 데이터 세트의 주목할 만한 측면은 CC에서 추출한 선택한 데이터 세트인 mCand OSCAR의 웹 기반 출처와 관련이 있습니다. 이는 LLM을 훈련하기 위해 The Pile(Gao et al., 2020) 및 BookCorpus(Zhu et al., 2015)와 같은 큐레이트된 데이터 세트에 의존했던 이전 작업(Radford et al., 2019; Mosaic ML, 2023; Touvron et al., 2023)과 다릅니다. 더 높은 전반적인 품질. 그러나 다국어 설정의 맥락에서, 우리는 우수한 품질의 큐레이팅된 데이터 세트를 다양한 언어로 사용할 수 없을 수 있으므로 웹 스크래핑된 데이터 세트가 더 적합한 접근 방식이 될 수 있다고 주장합니다. 웹 스크래핑된 데이터를 사용하는 우리의 전략은 여러 언어에서 효율적인 데이터 수집을 용이하게 하여 향상된 교육 데이터 규모에 기여합니다. 더욱이, 최근 연구에서는 웹 스크래핑된 데이터를 정리하여 최첨단 LLM을 생성하는 효과가 입증되었습니다(Raffel et al., 2020; Almazrouei et al., 2023). 총 mC4와 OSCAR의 조합은 추가 처리를 위한 135억 개의 문서를 제공합니다. 그림 1은 우리의 초기 데이터 세트에서 mC4와 사용 가능한 네 가지 버전의 OSCAR에 대한 문서 수의 분포를 보여줍니다. 2.1 데이터 정리 mC4와 OSCAR 데이터세트의 조합을 고려하여, 먼저 언어 식별, ULR 기반 필터링, 메트릭 기반 정리, 문서 정제를 포함하여 데이터에서 노이즈가 많고 잘못된 콘텐츠를 제거하기 위한 포괄적인 데이터 정리 절차를 수행합니다.언어 식별: 특정 문제는 mC와 OSCAR(각각)에 대해 두 가지 다른 언어 식별 도구인 cld3와 FastText를 사용하는 것과 관련이 있습니다.이전 연구에서 cld3가 FastText보다 훨씬 나빠 mC4에 대해 훨씬 더 많은 언어 감지 오류를 유발하는 것으로 나타났습니다(Kreutzer et al., 2022).사실, 다른 여러 언어 감지기와 비교했을 때 FastText는 벤치마크 데이터세트에서 최첨단 성능을 보여주었습니다.이를 위해 첫 번째 데이터 정리 단계에서는 FastText를 적용하여 mC4의 문서에 대한 언어를 다시 예측합니다.예측된 언어가 mC4에서 제공된 언어와 다른 문서는 데이터세트에서 제거됩니다. 그 이유는 언어 감지기 cld3 및 FastText에 혼란을 주는 문서를 피하고, 따라서 데이터에 노이즈가 발생할 가능성이 있기 때문입니다. 마지막으로, 최상의 품질을 보장하기 위해 mC4에서 발견되지만 FastText에서 지원하지 않는 언어에 대한 데이터를 제거합니다. URL 기반 필터링: 다음 단계에서는 알려진 독성 및 유해 출처의 페이지를 제거하여 데이터에서 관련 위험을 줄이는 것을 목표로 합니다. 특히, 툴루즈 대학교에서 제공하는 최신 UT1 URL 및 도메인 블랙리스트를 활용하여 학교 관리자의 인터넷 사용 규제를 지원합니다. 이 목록에는 포르노, 불평, 해킹을 포함하여 LLM 교육에서 삭제해야 하는 다양한 주제의 사이트가 포함됩니다. 일주일에 두세 번 업데이트되는 블랙리스트에는 인간과 로봇(예: 검색 엔진, 알려진 주소 및 인덱스)이 기여한 370만 개 이상의 레코드가 포함됩니다(Abadji et al., 2022). 따라서 블랙리스트의 사이트와 연관된 URL이 일치하는 모든 페이지를 데이터 세트에서 제거합니다. 이 단계는 블랙리스트가 이전에 mCdataset에 사용되지 않았기 때문에 데이터 세트에 유용합니다. 또한 OSCAR가 이미 이 블랙리스트를 데이터 정리에 사용했지만, 우리의 접근 방식은 목록에서 최신 정보를 통합하는데, 이는 현재 OSCAR 배포판에서는 사용할 수 없을 수 있습니다. 지표 기반 정리: BigScience의 BLOOM용 ROOTS 코퍼스의 데이터 처리 파이프라인에서 동기를 얻어 데이터 세트의 품질을 향상시키기 위해(Laurençon et al., 2022; Scao et al., 2022), 다양한 데이터 세트 지표에 대한 분포를 추가로 활용하여 이상 문서를 식별하고 필터링합니다. 각 지표는 데이터 세트 내의 모든 문서에 대해 단일 값 7https://modelpredict.com/ language-identification-survey를 제공하여 각 문서에 대한 number_words, stopword_ratios, perplexity_score와 같은 특정 속성을 정량화합니다. 각 메트릭과 데이터 세트 내의 가능한 값 범위에 대해 임계값을 결정하여 범위를 정상 범위와 비정상 범위의 두 영역으로 분할합니다.비정상 범위는 표준에서 크게 벗어나는 메트릭 값을 보이는 문서에 지정되어 이를 이상치/노이즈로 분류하고 결과적으로 이러한 이상치는 데이터 세트에서 제거됩니다.따라서 아래에 설명된 대로 데이터 세트를 정제하는 데 집합적으로 사용될 포괄적인 데이터 세트 메트릭 배열을 사용합니다.• 단어 수 • • 문자 반복 비율 단어 반복 비율 • 특수 문자 비율 • • 불용어 비율 플래그가 지정된 단어 비율 언어 식별 신뢰도 • 복잡도 점수 • 문서 길이(문자 수) • • 줄 수 • 짧은 줄 길이 비율 짧은 줄 비율 마지막 네 가지 메트릭은 OSCAR 데이터 세트에서 제안하는 반면 다른 메트릭은 OSCAR 데이터를 처리하기 위한 BigScience ROOTS 코퍼스의 파이프라인에서 상속받습니다. 복잡도 점수의 경우 BigScience ROOTS 코퍼스를 따라 SentencePiece 토크나이저(Kudo, 2018)와 KenLM 라이브러리(Heafield, 2011)에서 제공하는 5그램 KneserNey 언어 모델을 Wikipedia의 20230501 덤프를 사용하여 훈련합니다. 이러한 KenLM 모델을 기반으로 높은 복잡도 점수를 표시하는 문서는 Wikipedia 문서와 현저히 다른 것으로 간주됩니다. 이는 데이터 세트에서 제외될 노이즈 수준을 나타냅니다(Wenzek et al., 2020). 토크나이저는 또한 메트릭을 위해 문서의 단어/토큰 수를 얻는 데 사용됩니다. 향후 탐색을 용이하게 하기 위해 HuggingFace에서 KenLM 모델을 공개적으로 릴리스합니다. 크롤링 오류와 낮은 품질의 소스로 인해 웹에서 큐레이팅된 데이터에 반복되는 정보(예: 단어, 문단)가 나타날 수 있으며, 이는 LLM 훈련에 부정적인 결과를 초래할 수 있습니다(Holtzman et al., 2019). 따라서 문자 및 단어 반복 비율은 지나치게 https://huggingface.co/uonlp/kenlm 반복되는 정보가 있는 문서를 피하도록 설계되었습니다. 특수 문자, 불용어 또는 플래그가 지정된 단어의 빈도가 높으면 노이즈가 많고 품질이 낮은 문서를 나타낼 수 있습니다. 따라서 다양한 언어의 불용어 및 플래그가 지정된 단어 목록을 활용하여 문서 제거 비율을 계산합니다. BigScience ROOTS에서 제공하는 13개 언어의 불용어 및 플래그가 지정된 단어 목록 외에도 다른 언어의 이러한 유형의 단어에 대한 사전을 추가로 수집합니다. 다양한 언어의 개인 GitHub 계정에서 공유된 목록을 우선시합니다. 이러한 목록은 종종 모국어 화자가 작성하고 더 높은 품질을 보이기 때문입니다. 또한 언어 식별 신뢰도가 낮으면 데이터에 노이즈가 많은 언어 구조가 있을 수도 있습니다. 따라서 데이터 세트의 각 문서에 대해 FastText가 데이터 필터링을 돕기 위해 해당 언어에 할당하는 확률을 통해 언어 식별 신뢰도를 얻습니다. 마지막으로 짧은 줄 기반 기준의 경우 OSCAR에서 사용하는 것처럼 줄을 짧은 것으로 분류하기 위해 100자의 임계값을 구현합니다. 짧은 줄이 과도하게 나타나는 문서는 데이터 세트에 보관되지 않습니다. 임계값 선택: 데이터 세트 메트릭 집합을 고려할 때 중요한 질문은 각 메트릭과 언어에 적합한 임계값을 선택하여 고품질 다국어 데이터를 생성하는 것입니다. BigScience ROOTS 프로젝트(Laurençon et al., 2022)에서 이 선택 프로세스는 13개 언어의 모국어 화자가 수행합니다. 그 결과 임계값은 나머지 46개 언어에 적용됩니다. 이 프로젝트는 언어당 수천 개의 문서 샘플을 색인화하는 시각화 인터페이스를 제공하여 사용자가 메트릭에 대한 임계값을 조정하면서 데이터 통계를 모니터링할 수 있도록 합니다. 그러나 이 프로세스는 경험이 풍부한 모국어 화자가 필요하기 때문에 다른 언어로 쉽게 확장할 수 없으며, 이로 인해 상당한 비용이 발생합니다. 게다가 제한된 샘플 크기로 인해 전체 데이터 세트에 대해 선택한 임계값의 대표성이 떨어집니다. 분석 결과 BigScience ROOTS 내의 특정 언어에 대해 선택한 일부 임계값이 전체 데이터 세트의 값 범위를 거의 벗어나 해당 메트릭이 비활성화되는 것을 확인했습니다. 이러한 문제를 해결하기 위해 우리는 사분위 범위(IQR) 방법의 변형(Dekking et al., 2007)을 활용하여 데이터 세트의 필터링 메트릭에 적합한 임계값을 선택합니다. 각 메트릭과 언어에 대해 전체 데이터 세트에서 해당 언어의 가능한 값 분포를 생성합니다. 스페인어와 러시아어와 같이 상당한 양의 데이터가 있는 언어의 경우 예외가 있습니다. 이 언어의 경우 데이터의 25%만 사용하여 이러한 분포를 계산합니다. 그런 다음 분포의 Q1 및 Q3 백분위수(Q1 Q3)를 계산하여 필터링 메트릭의 임계값으로 사용합니다. 특히 낮은 Q1 백분위수는 높은 값(예: 언어 식별 신뢰도)을 선호하는 메트릭에 대해 선택되고, 낮은 값(예: 복잡도 점수 및 문서 길이)을 선호하는 메트릭은 높은 Q3 백분위수를 활용합니다. 우리는 (25, 75), (20, 80), (15,85), (10,90), (5, 95)를 고려하여 (Q1, Q3)에 대한 다양한 값을 조사합니다. Q1 = 10 및 Q2 90을 선택하면 검토 대상 언어 샘플에 대한 최상의 데이터 품질을 얻을 수 있습니다. 임계값 선택에 백분위수를 사용하면 BigScience ROOTS 프로젝트에서 사용한 것보다 각 언어에 대해 더 광범위한 데이터 샘플을 효율적으로 추출할 수 있다는 점을 강조할 가치가 있습니다. 이를 통해 다양한 언어에 대한 전체 데이터 세트에 대해 더 신뢰할 수 있는 임계값이 생성됩니다. 특히, 메트릭의 값 분포를 계산하는 데 25%의 데이터 샘플만 사용되는 대규모 언어의 경우, 동일한 선택된 필터링 임계값을 적용할 때 전체 데이터 세트에 대한 삭제된 데이터의 비율이 데이터 샘플의 비율과 밀접하게 일치한다는 것을 관찰했습니다. 이는 방법론을 통해 선택된 임계값의 대표성을 강조합니다. 마지막으로, 주어진 언어의 메트릭에 대한 임계값이 결정되면 메트릭의 임계값을 초과하고 데이터의 불리한 범위에 들어가는 모든 문서를 제거합니다. 문서 정제: 이전 정리 단계는 데이터 세트 수준에서 수행되며, 데이터 세트에서 품질이 낮은 문서를 제거하는 것을 목표로 합니다. 이 단계에서는 보관된 문서를 추가로 정리하여 품질을 개선합니다. 이전 메트릭 기반 필터링 단계는 노이즈가 많은 문서를 제거하는 데 중요한 역할을 하며, 이를 통해 이 단계에서 효과적인 문서 정리 규칙을 개발하는 프로세스가 간소화된다는 점에 유의하는 것이 중요합니다. 특히 mC4와 OSCAR의 문서는 인터넷에서 크롤링한 HTML 페이지에서 추출되므로 상당 부분에 긴 JavaScript 줄과 외부 콘텐츠를 포함한 크롤링 및 추출 오류가 있을 수 있습니다. 결과적으로 이러한 문서를 필터링하면 데이터 세트 내의 문서를 정리하기 위한 규칙을 설계하는 작업이 크게 간소화됩니다. 따라서 각 문서에 대해 일련의 작업을 통해 노이즈가 많거나 관련성이 없는 부분을 제거합니다. 첫째, 각 문서의 끝에 있는 짧은 줄을 제거합니다. 이러한 줄에는 일반적으로 푸터 세부 정보나 웹사이트의 도움이 되지 않는 정보가 들어 있기 때문입니다. 둘째, JavaScript(JS) 키워드 목록에서 단어가 포함된 줄을 제거합니다(예: &quot;
--- EXPERIMENT ---
s 모든 정리 및 중복 제거 단계를 완료한 후, 최종 데이터 세트는 167개 언어에 걸쳐 6.3조 개의 토큰으로 구성됩니다. 표 1은 각 처리 단계에 따른 CulturaX의 상위 42개 언어에 대한 문서 및 토큰 수에 대한 개요를 제공합니다. 알 수 있듯이, 데이터 정리 파이프라인은 각 언어에 대한 원래 mC4 및 OSCAR 데이터 세트의 문서 수를 상당히 줄일 수 있습니다. 제거된 문서의 총 수는 초기 문서의 46.48%를 차지하며, 이는 다국어 데이터 세트에 대한 노이즈 정보를 필터링하는 접근 방식의 효과를 시사합니다. 4 관련 작업 다른 NLP 작업과 비교했을 때 언어 모델은 레이블이 지정되지 않은 데이터로 학습할 수 있으므로 효율적인 데이터 수집을 통해 https://github.com/ChenghaoMou/text-dedup/ tree/main에 대한 거대한 규모를 생성할 수 있습니다. #문서(M) #토큰 코드 언어 URL 초기 필터링 메트릭 필터링 en 영어 5783.5766.3586.MinHash URL 중복 제거 중복 제거 3308.30 3241.Filtering Rate (%) (B) (%) ru 러시아어 1431.1429.922.es 스페인어 844.842.530.de 독일어 863.861.515.845.64 799.479.65 450.447.06 420.fr 프랑스어 711.709.439.387.zh 중국어 444.444.258.it 이탈리아어 406.406.254.pt 포르투갈어 347.346.217.363.222.37 218.226.42 211.200.11 190.43.44.16 737.46.60 373.51.34 357.48.89 319.2846.97 45.11.5.5.5.50.80 227.48.06 165.3.2.45.24 136.2.pl 폴란드어 270.269.170.151.71 142.47.37 117.1.ja 일본어 247.247.137.114.64 111.55.11 107.1.vi 베트남어 182.182.118.108.102.44.98.1.nl 네덜란드어 238.238.148.125.117.50.80.1.ar 아랍어 132.132.84.77.74.44.69.1.tr 터키어 183.183.109.99.94.48.64.1.CS 체코어 136.136.80.69.65.52.56.0.fa 페르시아어 118.118.70.62.59.49.45.0.hu 헝가리어 88.88.53.46.44.50.43.0.el 그리스어 100.100.61.54.51.48.43.0.ro 루마니아어 89.89.45.42.40.54.39.0.SV 스웨덴어 103.102.58.52.49.51.38.0.uk 우크라이나어 81.81.50.47.44.45.38.0.fi 핀란드어 59.59.36.32.30.49.28.0.ko 한국어 46.45.25.21.20.55.24.0.da 덴마크어 53.52.28.26.25.52.22.0.bg 불가리아어 47.46.28.25.24.48.22.0.no 노르웨이어 40.40.20.19.18.52.18.0.hi 힌디어 35.35.22.20.19.44.16.0.sk 슬로바키아어 40.39.22.19.18.53.16.0.th 태국어 49.48.26.21.20.57.15.0.lt 리투아니아어 27.27.15.14.13.50.14.0.са 카탈로니아어 31.31.18.16.15.50.12.0.id 인도네시아어 48.48.25.23.23.51.12.0.bn 방글라어 20.20.13.13.12.40.9.0.et 에스토니아어 16.16.9.8.8.50.8.0.sl 슬로베니아어 15.15.8.7.7.52.8.0.lv 라트비아어 14.14.8.7.7.49.7.0.he 히브리어 10.10.5.4.4.56.4.0.sr 세르비아어 7.7.4.4.4.48.4.0.ta 타밀어 8.8.5.4.4.46.4.0.sq 알바니아어 9.9.5.5.5.44.3.0.az 아제르바이잔어 9.9.5.5.5.47.3.0.전체(42개 언어) 13397.13366.8254.7471.7181.46.6267.99.전체(167개 언어) 13506.76 13474.94 8308.7521.7228.46.48 6308.42 100.표 1: 데이터 통계 데이터 세트에서 토큰 비율이 0.05%를 넘는 42개 언어. &quot;#Documents (M)&quot; 레이블로 그룹화된 열은 해당 정리 및 중복 제거 단계 이후 각 언어의 문서 수를 나타냅니다. 토큰 수는 최종 데이터 세트(즉, 모든 정리 및 중복 제거 단계 이후)를 기반으로 합니다. 훈련 데이터. LLM 훈련에 일반적으로 사용되는 두 가지 주요 데이터 유형은 큐레이트된 데이터와 웹 크롤링 데이터입니다. 큐레이션된 데이터는 일반적으로 타깃 소스 및 도메인에서 잘 작성되고 잘 형식화된 텍스트로 구성됩니다. 예를 들어, 위키피디아 기사, 책, 뉴스 기사 및 과학 논문은 &quot;The Pile&quot;(Gao et al., 2020) 및 &quot;BookCorpus&quot;(Zhu et al., 2015) 데이터 세트에 사용됩니다. 반면, 웹 크롤링 데이터는 블로그, 소셜 미디어 게시물, 뉴스 기사 및 광고와 같이 형식 및 쓰기 스타일 측면에서 상당히 다양한 인터넷의 광범위한 소스에서 수집된 텍스트를 포함합니다. CommonCrawl(CC)은 12년 동안 인터넷에서 페타바이트 규모의 데이터를 수집한 널리 사용되는 웹 크롤링 저장소입니다. 이를 위해 큐레이션된 데이터는 종종 더 높은 품질을 가지고 있다고 간주되며, 이로 인해 BERT(Devlin et al., 2019) 및 GPT-2(Radford et al., 2019)와 같은 초기 LLM을 훈련하는 데 선호되었습니다. 그러나 더 큰 모델에 대한 수요가 성장함에 따라 웹 크롤링 데이터는 최근 LLM의 학습 데이터에 상당 부분을 기여하기 때문에 더 많은 주목을 받고 있습니다.예를 들어 ROBERTA(Liu 등, 2019), BART(Lewis 등, 2020), T5(Raffel 등, 2020), GPT-3(Rae 등, 2021), LLaMa(Touvron 등, 2023), MPT(MosaicML, 2023), Falcon(Almazrouei 등, 2023) 등이 있습니다. 따라서 이러한 LLM을 학습하기 위해 C4(Raffel 등, 2020), CC-News(Nagel), STORIES(Trinh and Le, 2018)를 포함하여 다양한 CC 추출물이 생성되었습니다. 학습 데이터의 접근성과 관련하여, 초기 LLM을 학습하는 데 사용된 데이터 세트는 종종 대중에게 공개됩니다(Devlin et al., 2019; Raffel et al., 2020). 그러나 가장 최근의 최첨단(SOTA) 생성 LLM의 경우, 학습 데이터 세트가 상업적 이익으로 인해 완전히 공개되지 않습니다. 이는 ChatGPT 및 GPT-4와 같은 독점 모델뿐만 아니라 LLaMa, MPT, Falcon, BLOOM과 같이 오픈소스 모델이라고 주장하는 모델에도 적용됩니다(Scao et al., 2022). 기존 LLM의 투명성 문제를 해결하기 위해, 최첨단 LLM, 즉 RedPajama(Computer, 2023), SlimPajama, AIDolma에 대한 학습 데이터 세트를 복제하여 공개하려는 최근의 노력이 있었습니다. 이러한 데이터 세트의 주요 차이점은 LLM 교육을 위한 고품질을 보장하기 위해 세심하게 정리되고 문서 수준에서 중복 제거된 대규모 텍스트 데이터와 관련이 있습니다. 그럼에도 불구하고 이러한 오픈소스 데이터 세트의 일반적인 단점은 주로 영어 데이터에 초점을 맞추고 다른 언어에 대한 데이터는 제한적이라는 것입니다. LLM 교육을 위한 다국어 대규모 데이터 세트를 얻으려면 CC와 같은 웹 스크레이프 데이터 세트를 활용하여 여러 언어로 최신 정보를 사용하여 효율적인 데이터 수집을 수행하는 것이 더 편리합니다. 또한 고성능 LLM의 고품질을 보장하려면 노이즈가 많고 관련성이 없는 콘텐츠(예: 품질이 낮은 기계 생성 텍스트 및 성인 콘텐츠)를 피하기 위해 다국어 데이터를 광범위하게 정리하고 중복 제거해야 합니다(Trinh 및 Le, 2018; Kreutzer et al., 2022; Raffel et al., 2020). 따라서 고품질 데이터 세트를 생성하기 위한 일반적인 데이터 처리 파이프라인에는 FastText(Joulin 등, 2016), CC-Net(Wenzek 등, 2020), BLOOM 모델을 위한 BigScience ROOTS 코퍼스(Laurençon 등, 2022; Scao 등, 2022), Falcon 모델을 위한 RefinedWeb 데이터 세트(Penedo 등, 2023; Almazrouei 등, 2023) 및 LLaMa 모델을 훈련하기 위한 데이터 세트(Touvron 등, 2023)에서 보여준 것처럼 여러 단계가 포함될 수 있습니다. 첫 번째 단계는 이러한 파이프라인에서 언어를 식별하여 데이터를 해당 언어에 적절히 할당하는 것이 필요합니다(Joulin 등, 2016). 다음 단계에서는 특수 문자, 짧은 줄, 나쁜 단어 등의 비율에 따라 바람직하지 않은 콘텐츠를 필터링하기 위한 다양한 데이터 세트별 규칙과 휴리스틱이 특징입니다(Grave et al., 2018; Laurençon et al., 2022). 또한 데이터는 가벼운 모델(예: KenLM 언어 모델(Heafield, 2011))을 통해 필터링하여 노이즈가 많은 문서를 피할 수 있습니다(Wenzek et al., 2020). 마지막으로 데이터 중복 제거를 수행하여 유사하거나 반복되는 정보를 제거해야 합니다(Laurençon et al., 2022; Penedo et al., 2023). 이와 관련하여 중요한 단계는 문서 수준에서 퍼지 중복 제거를 포함하는데, 예를 들어 MinHash(Broder, 1997)를 통해 유사한 문서를 제거하고, 이를 통해 기억을 완화하고 결과 LLM에 대한 일반화를 개선하는 것입니다(Lee et al., 2022). 이를 위해 mC4(Xue et al., 2021), OSCAR(Ortiz Suárez et al., 2019), CC100(Wenzek et al., 2020; Conneau et al., 2020) 및 BigScience ROOT 코퍼스(Laurençon et al., 2022)와 같이 여러 언어로 된 텍스트 데이터가 있는 다국어 오픈소스 데이터 세트가 있지만, 그 품질과 규모는 특히 GPT와 같은 생성 모델의 경우 LLM을 효과적으로 훈련하는 데 필요한 요구 사항을 충족하지 못합니다. 예를 들어, 서론에서 강조했듯이 mC4와 OSCAR는 모두 문서 수준의 데이터에 대한 퍼지 중복 제거가 부족합니다. mCalso는 cld3를 사용하기 때문에 언어 식별이 좋지 않습니다. BigScience ROOTS는 46개 언어에 대한 작은 샘플 데이터만 제공하는 반면 CC100은 2018년 이후의 정보를 제공하지 않습니다. 따라서 저희 데이터 세트 CulturaX는 기존 데이터 세트의 문제를 종합적으로 해결하여 LLM을 훈련하는 데 쉽게 사용할 수 있고 고품질의 데이터를 제공하는 다국어, 오픈 소스, 대규모 데이터 세트를 제공합니다. 5
--- CONCLUSION ---
167개 언어의 텍스트 데이터가 있는 새로운 다국어 데이터 세트인 CulturaX를 소개합니다. 저희 데이터 세트는 포괄적인 파이프라인을 통해 정리되고 중복 제거되어 6.3조 개의 토큰을 생성합니다. 따라서 CulturaX는 대규모의 고품질 데이터 세트로, 다국어에 대한 고성능 LLM을 훈련하는 데 쉽게 사용할 수 있습니다. 저희 데이터는 다국어 학습에 대한 추가 연구와 응용 프로그램을 촉진하기 위해 대중이 공개적으로 접근할 수 있습니다. 참고문헌 Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. 2022. Towards a cleaner document-driven multilingual crawled corpus. 제13회 언어 자원 및 평가 컨퍼런스 회의록, 4344-4355페이지, 프랑스 마르세유. European Language Resources Association. Julien Abadji, Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. 2021. Ungoliant: 매우 대규모 다국어 웹 코퍼스 생성을 위한 최적화된 파이프라인. 2021년 대규모 코퍼스 관리의 과제에 대한 워크숍(CMLC-9) 회의록. 리머릭, 2021년 7월 12일(온라인 이벤트). Miltiadis Allamanis. 2018. 코드의 머신 러닝 모델에서 코드 중복의 부정적 영향. 2019년 ACM SIGPLAN 국제 심포지엄의 새로운 아이디어, 새로운 패러다임, 프로그래밍 및 소프트웨어에 대한 성찰 회의록. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi et al. 2023. Falcon-40B: 최첨단 성능을 갖춘 개방형 대규모 언어 모델. 방예진, 사무엘 카야위자야, 이나연, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu 및 Pascale Fung. 2023. 추론, 환각 및 상호 작용에 대한 chatgpt의 다중 작업, 다중 언어, 다중 모드 평가. ArXiv, ABS/2302.04023. Marta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Esplà-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William 웨이츠, 디온 위긴스, 하우메 사라고사. 2020. ParaCrawl: 웹 규모의 병렬 말뭉치 획득. 전산언어학협회 제58차 연차총회 진행, 4555-4567페이지, 온라인. 전산언어학협회. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli 등. 2021. 기초 모델의 기회와 위험. ArXiv, abs/2108.07258. Jan A. Botha, Emily Pitler, Ji Ma, Anton Bakalov, Alex Salcianu, David Weiss, Ryan McDonald, Slav Petrov. 2017. 소규모 피드포워드 네트워크를 사용한 자연어 처리. 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 2879-2885페이지, 덴마크 코펜하겐. 계산 언어학 협회. A. Broder. 1997. 문서의 유사성과 포함에 관하여. 시퀀스의 압축 및 복잡성 회의록. Tom Brown, Benjamin Mann, et al. 2020. 언어 모델은 few-shot 학습자입니다. Arxiv, abs/2005.14165. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin et al. 2022. Palm: 경로로 언어 모델링 확장. ArXiv, abs/2204.02311. Together Computer. 2023. Redpajama: 라마 훈련 데이터 세트를 재생성하는 오픈 소스 레시피. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. 2020. 규모에 따른 비지도 교차 언어 표현 학습. Association for Computational Linguistics의 제58회 연례 회의록, 84408451페이지, 온라인. Association for Computational Linguistics. Michel Dekking, Cornelis Kraaikamp, Hendrik Paul, Ludolf Erwin Meester. 2007. 확률과 통계에 대한 현대적 소개: 이유와 방법 이해. Springer Texts in Statistics에서. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. 2019년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 제1권(긴 논문과 짧은 논문), 4171-4186쪽, 미네소타주 미니애폴리스. 컴퓨터 언어학 협회. Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. 파일: 언어 모델링을 위한 다양한 텍스트의 800gb 데이터 세트. ArXiv, abs/2101.00027. Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, Tomas Mikolov. 2018. 157개 언어에 대한 단어 벡터 학습. 제11회 언어 자원 및 평가 국제 컨퍼런스(LREC 2018) 회의록, 일본 미야자키. 유럽 언어 자원 협회(ELRA). Kenneth Heafield. 2011. KenLM: 더 빠르고 더 작은 언어 모델 쿼리. 제6회 통계 기계 번역 워크숍 회의록, 187-197쪽, 스코틀랜드 에든버러. 계산 언어학 협회. Danny Hernandez, Tom B. Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, TJ Henighan, Tristan Hume, Scott Johnston, Benjamin Mann, Christopher Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan 및 Sam McCandlish. 2022. 반복 데이터 학습의 법칙 및 해석 가능성 확장. ArXiv, ABS/2205.10487. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes 및 최예진. 2019. 신경 텍스트 변성의 흥미로운 사례. ArXiv, ABS/1904.09751. Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou 및 Tomas Mikolov. 2016. Fasttext.zip: 텍스트 분류 모델 압축. ArXiv, abs/1612.03651. Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, Geoffrey Irving. 2021. 언어 에이전트 정렬. ArXiv, abs/2103.14659. Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoît Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey 오세이, 페드로 오르티즈 수아레스, 이로로 오리페, 켈레치 오구에지, 안드레 니용가보 루분고, 토안 Q. 응우옌, 마티아스 뮐러, 안드레 뮐러, 샴수딘 하산 무하마드, 난다 무하마드, 아얀다 음냐케니, 잠시드벡 미르자카로프, 타피와나셰 마탄기라, 콜린 레옹, 은제 로슨, 스네하 쿠두군타, 야신 제르나이트, Mathias Jenny, Orhan Firat, Bonaventure FP Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine Çabuk Ballı, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, Mofetoluwa Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50-72. Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidate. 56회 연례 총회 의사록(제1권: 장문 논문), 66-75쪽, 호주 멜버른. Association for Computational Linguistics. Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, Thien Huu Nguyen. 2023. Chatgpt beyond english: 다국어 학습에서 대규모 언어 모델에 대한 포괄적 평가를 향해. ArXiv, abs/2304.05613. Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, Jörg Frohberg, Mario Šaško, Quentin Lhoest, Angelina McMillan-Major, Gérard Dupont, Stella 비더만, 안나 로저스, 루브나 벤 알랄, 프란체스코 드 토니, 지아다 피스틸리, 올리비에 응우옌, 소마이에 닉푸어, 마라임 마수드, 피에르 콜롬보, 하비에르 데 라 로사, 파울로 빌레가스, 트리스탄 스러쉬, 셰인 롱프레, 세바스티안 나겔, 레온 웨버, 마누엘 로메로 무뇨스, 지안 주, 다니엘 반 스트리엔, 자이드 알리야페아이, 칼리드 알무바라크, 부민 Chien, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, Yacine Jernite. 2022. Bigscience ROOTS 코퍼스: 1.6TB 복합 다국어 데이터 세트. 제36회 신경 정보 처리 시스템 데이터 세트 및 벤치마크 트랙 컨퍼런스. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, Nicholas Carlini. 2022. 학습 데이터 중복 제거로 언어 모델이 개선됩니다. 60th Annual Meeting of the Association for Computational Linguistics(제1권: 장문 논문)의 회의록, 8424-8445페이지, 아일랜드 더블린. Association for Computational Linguistics. Jure Leskovec, Anand Rajaraman, Jeffrey David Ullman. 2020. Mining of massive datasets. Cambridge University Press에서. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer. 2020. BART: 자연어 생성, 번역 및 이해를 위한 시퀀스 간 사전 학습의 노이즈 제거. Association for Computational Linguistics의 제58회 연례 회의의 회의록, 7871-7880페이지, 온라인. Association for Computational Linguistics. Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: 기술적 세부 사항 및 평가. 백서. A121 Labs. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: 견고하게 최적화된 bert 사전 학습 접근법. ArXiv, abs/1907.11692. MosaicML. 2023. mpt-7b 소개: 오픈 소스, 상업적으로 사용 가능한 llms를 위한 새로운 표준. https://www.mosaicml.com/blog/mpt-7b. Sebastian http: Nagel. Cc-news. //web.archive.org/save/http: //commoncrawl.org/2016/10/news- dataset-available. Pedro Javier Ortiz Suárez, Laurent Romary, and Benoît Sagot. 2020. 중간 리소스 언어를 위한 문맥화된 단어 임베딩에 대한 단일 언어적 접근 방식. Association for Computational Linguistics의 제58회 연례 회의록, 1703-1714쪽, 온라인. Association for Computational Linguistics. Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. 2019. 중간에서 낮은 리소스 인프라에서 방대한 코퍼스를 처리하기 위한 비동기 파이프라인. 대규모 코퍼스 관리의 과제에 대한 워크숍(CMLC7) 2019의 회의록. 카디프, 2019년 7월 22일. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aimée Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay. 2023. Falcon LLM을 위한 정제된 웹 데이터 세트: 웹 데이터와 웹 데이터만을 사용한 큐레이션된 코퍼스보다 우수한 성과. Arxiv, abs/2306.01116. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. 언어 모델은 비지도 멀티태스크 학습기입니다. OpenAI 블로그. Jack Rae, Sebastian Borgeaud, et al. 2021. 언어 모델 확장: 고퍼 훈련에서 얻은 방법, 분석 및 통찰력. ArXiv, abs/2112.11446. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. 2020. 통합 텍스트-텍스트 변환기를 사용하여 전이 학습의 한계 탐색. Journal of Machine Learning Research에서. Teven Scao, Angela Fan, et al. 2022. Bloom: 176b-매개변수 오픈 액세스 다국어 언어 모델. ArXiv, abs/2211.05100. Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, Francisco Guzmán. 2021. WikiMatrix: 위키피디아에서 1620개 언어 쌍으로 135M개의 병렬 문장 마이닝. 제16차 유럽 지부 학회 회의록: 주요 권, 1351-1361페이지, 온라인. Association for Computational Linguistics. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro. 2019. Megatron-lm: 모델 병렬 처리를 사용하여 수십억 개의 매개변수 언어 모델 학습. ArXiv, abs/1909.08053. Alex Tamkin, Miles Brundage, Jack Clark, Deep Ganguli. 2021. 대규모 언어 모델의 역량, 한계 및 사회적 영향 이해. ArXiv, abs/2102.02503. Hugo Touvron, Thibaut Lavril, Gautier Izacard et al. 2023. Llama: 개방적이고 효율적인 기초 언어 모델. ArXiv, ABS/2302.13971. Trieu H. Trinh 및 Quoc V. Le. 2018. 상식 추론을 위한 간단한 방법. ArXiv, ABS/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser 및 Illia Polosukhin. 2017. 관심만 있으면 됩니다. 신경 정보 처리 시스템의 발전. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean 및 William Fedus. 2022. 대규모 언어 모델의 새로운 능력. 기계 학습 연구 거래. Xiangpeng Wei, Hao-Ran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, Jun Xie. 2023. Polylm: 오픈 소스 다국어 대규모 언어 모델. ArXiv, abs/2307.06018. Laura Weidinger, John FJ Mellor, Maribeth Rauh et al. 2021. 언어 모델로 인한 피해의 윤리적 및 사회적 위험. ArXiv, abs/2112.04359. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, Edouard Grave. 2020. CCNet: 웹 크롤링 데이터에서 고품질 단일 언어 데이터 세트 추출. 제12회 언어 자원 및 평가 컨퍼런스 회의록, 4003-4012페이지, 프랑스 마르세유. 유럽 언어 자원 협회. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel. 2021. mT5: 대규모 다국어 사전 학습된 텍스트-텍스트 변환기. 2021년 북미 컴퓨터 언어학 협회 회의록: 인간 언어 기술, 483-498페이지, 온라인. Association for Computational Linguistics. Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. 책과 영화 정렬: 영화 감상과 책 읽기를 통한 스토리와 같은 시각적 설명. IEEE International Conference on Computer Vision(ICCV) 회의록.
