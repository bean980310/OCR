--- ABSTRACT ---
강력한 대규모 언어 모델(LLM)을 기반으로 최근의 생성적 멀티모달 대규모 언어 모델(MLLM)은 핵심 연구 분야로 각광을 받고 있으며, 이해와 생성에 모두 뛰어난 역량을 보여줍니다. 이 연구에서는 SEED-Bench라는 벤치마크를 도입하여 생성 모델에 대한 포괄적인 평가를 향한 예비 단계로 MLLM에서 생성적 이해의 평가를 다룹니다. SEED-Bench는 정확한 인간 주석이 있는 19K개의 객관식 문제로 구성되어 있으며(기존 벤치마크보다 ×6 더 큼), 이미지와 비디오 모달리티의 이해를 포함한 12개의 평가 차원에 걸쳐 있습니다. 자동 필터링과 수동 검증 프로세스를 모두 통합하여 특정 평가 차원을 타겟으로 하는 객관식 문제를 생성하기 위한 고급 파이프라인을 개발합니다. 인간 주석에서 파생된 기준 진실 옵션이 있는 객관식 문제를 통해 모델 성능을 객관적이고 효율적으로 평가할 수 있으므로 평가 중에 인간이나 GPT가 개입할 필요가 없습니다. 공간적 이해와 시간적 이해를 모두 포괄하는 12개 차원 전체에 걸쳐 18개 모델의 성능을 추가로 평가합니다. 평가 결과를 통해 기존 MLLM의 한계를 밝혀냄으로써 SEED-Bench가 미래 연구를 촉진하기 위한 통찰력을 제공하는 것을 목표로 합니다. 우리는 리더보드를 출시하고 지속적으로 유지하여 커뮤니티가 모델 역량을 평가하고 조사할 수 있는 플랫폼을 제공할 것입니다.
--- INTRODUCTION ---
최근 몇 년 동안 대규모 언어 모델(LLM) [1, 2, 3, 4, 5]은 다양한 개방형 작업에서 텍스트를 이해하고 추론하고 생성하는 놀라운 능력을 보여주었습니다. LLM의 강력한 일반성을 활용하여 생성적 다중 모드 대규모 언어 모델(MLLM) [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]은 다중 모드 이해 및 생성에 대한 향상된 능력을 보여주었습니다. 그러나 현재의 MLLM은 주로 제한된 수의 정성적 예제로 성능을 평가하거나 개방형 출력을 가진 MLLM을 평가하는 데 맞춤화되지 않은 이전 벤치마크를 사용하여 성능을 평가합니다. 예를 들어, VQAv2[22]에서 답변은 모델의 출력이 일반적으로 하나 또는 두 단어로 구성된 기준 진실 답변과 정확히 일치하는 경우에만 올바른 것으로 간주됩니다.MLLM을 평가할 포괄적이고 객관적인 벤치마크가 부족하면 다양한 모델의 성능을 비교하고 조사하는 데 상당한 어려움이 있습니다.동시 작업[23, 24, 25, 26]은 표 1에 표시된 것처럼 MLLM을 특별히 평가하기 위한 벤치마크를 개발하기 위해 노력했습니다.예를 들어, LVLM-eHub[25]와 LAMM[24]은 다양한 컴퓨터 비전 작업에서 기존 공개 데이터 세트를 평가 샘플로 활용하고 인간 주석자 또는 GPT를 고용하여 모델 예측의 품질, 관련성 및 유용성을 평가합니다.그러나 참여 *동등한 기여.*yuyingge@tencent.com 및 yixiaoge@tencent.com으로의 서신. 절차 이해 동작 예측 장면 이해 인스턴스 ID 순위 모델 정확도(%) InstructBLIP Vicuna 53.InstructBLIP 52.BLIP46.인스턴스 속성 MiniGPT-42.VPGTrans 39.VideoChat 37.mPLUG-Owl 34.동작 인식(((1)) 인스턴스 위치 Otter 33.SEED-BenchLLaVa 33.GVT 33.MultiModal-GPT 33.OpenFlamingo 33.LLAMA-Adapter V32.OCR L 텍스트 인식 Video-ChatGPT 31.인스턴스 계산 Valley 30.Vicuna 28.시각적 추론 Dd 인스턴스 상호 작용 공간 관계 Flan-T5→ 27.LLAMA 26.: LLM ImageLLM :VideoLLM 그림 1: 왼쪽: 12개 평가 차원 개요 공간적 이해와 시간적 이해를 모두 포함하는 SEED-Bench에서 막대의 숫자는 각 차원에서 인간이 주석을 단 객관식 질문의 수를 나타냅니다.오른쪽: 12개 평가 차원에서 18개 모델의 평균 정확도를 표시하는 전체 리더보드.평가 중 인간과 GPT의 개입은 효율성을 떨어뜨릴 뿐만 아니라 주관성이 증가하고 평가의 정확도가 감소합니다.MME[23]와 MMBench[26]는 다양한 능력 차원을 포괄하는 참/거짓 질문 또는 객관식 질문을 구성하여 MLLM의 객관적 평가를 더욱 발전시킵니다.모델의 출력을 참/거짓 또는 A/B/C/D 옵션으로 제한하면 정확도를 편리하게 계산할 수 있으며, 이는 평가를 위한 객관적인 지표 역할을 합니다.그러나 이러한 벤치마크의 비교적 작은 규모(3,000개 샘플 미만)는 평가 통계에 불안정성을 초래합니다. 이 작업에서 우리는 SEED-Bench*라는 벤치마크를 도입하여 생성 모델의 포괄적 평가를 향한 예비 단계로 MLLM의 생성 이해 능력을 평가하는 데 중점을 둡니다.SEED-Bench는 그림 1에서 볼 수 있듯이 이미지와 비디오 모달리티에 걸쳐 12개의 평가 차원을 포괄합니다.SEED-Bench는 그림 2에서 볼 수 있듯이 인간 주석에서 파생된 기준 진실 답변이 있는 19K개의 객관식 문제로 구성됩니다(MME보다 ×9 크고 MMBench보다 ×6 큽니다).우리는 특정 차원을 평가하도록 맞춤화된 객관식 문제를 생성하기 위한 정교한 파이프라인을 설계합니다.또한 질문의 품질과 기준 진실 답변의 정확성을 보장하기 위해 자동화된 필터링 메커니즘과 수동 검증 프로세스를 통합합니다.특히 이미지의 경우 이미지 수준 캡션[6, 27], 인스턴스 수준 설명[28, 29, 30] 및 텍스트 요소[31]를 포함하여 다양한 기초 모델을 활용하여 시각적 정보를 추출합니다.비디오의 경우 원래 인간 주석을 활용하여 시각적 정보를 제공합니다. 그런 다음 특정 평가 차원에 해당하는 특별히 설계된 프롬프트와 함께 ChatGPT/GPT-4에 시각적 정보를 제공합니다. ChatGPT/GPT-4는 이후 하나의 정답이 있는 질문과 네 가지 후보 옵션을 생성합니다. 여러 LLM을 활용하여 시각적 입력 없이도 답할 수 있는 질문을 추가로 필터링합니다. 마지막으로 인간 주석자를 고용하여 각 객관식 질문의 올바른 옵션을 선택하고 각 질문을 하나의 평가 차원으로 분류하여 19,000개의 객관식 질문을 포함하는 깔끔하고 고품질의 벤치마크를 생성합니다. *인공 일반 지능(AGI)을 추구하면서 LLM은 상당한 진전을 목격했습니다. 우리는 다중 모드 기능의 출현에 대한 전제가 SEED[18]가 겸손한 단계를 밟은 자기 회귀 생성 모델 내에서 이해와 생성을 모두 통합하는 것이라는 대담한 가정을 했습니다. 모델 탐색 외에도 연구 방향을 동기를 부여하는 적절한 평가가 필수적입니다. 따라서 우리는 동시에 생성 모델의 이해 능력을 평가하기 위해 SEED-Bench를 제안한다.장면 이해 인스턴스 정체성 인스턴스 속성 이미지의 날씨는 어떻습니까?이미지에 어떤 동물이 보입니까?A. 맑은 날입니다.B. 안개가 낀 날입니다.C. 비가 많이 옵니다.D. 흐린 날입니다.A. 말 B. 소 C. 양 D. 염소 인스턴스 위치 인스턴스 계산 공간 관계 개는 거실의 어디에 있습니까?행사에 몇 명이 있습니까?A. 벽난로 위 AB 테이블 위 BC 의자 위 CD 양탄자 위 D.인스턴스 상호 작용 플레이어와 심판의 관계는 무엇입니까?A. 플레이어가 심판과 악수하고 있습니다.B. 플레이어가 심판과 논쟁하고 있습니다.시각적 추론 텍스트 인식 OCR 상황에 대해 무엇을 추론할 수 있습니까? 영어: A. 그들은 엔진을 감상하고 있습니다 B. 그들은 자동차 고장을 겪고 있습니다 KINGO C. 선수가 심판으로부터 상을 받고 있습니다 C. 그들은 피크닉을 하고 있습니다 D. 심판이 선수에게 카드를 보여줍니다 D. 그들은 자동차를 씻고 있습니다 동작 인식 ☑ 동작 예측 C 절차 이해 c 시간 시간 막다른 길 테이블의 재질은 무엇입니까? A. 대리석 B. 나무 C. 유리 D. 플라스틱 집과 관련하여 나무는 어디에 있습니까? A. 집 앞 B. 집 뒤 C. 집 안 D. 집 왼쪽 표지판의 주요 경고는 무엇입니까? A. 진입 금지 B. 막다른 길 C. 곰 조심 D. 길 폐쇄 비디오에서 수행되는 동작은 무엇입니까? A. 공중에 무언가를 던져 떨어뜨립니다. B. 공중에 무언가를 던져 떨어뜨립니다. C. 무언가의 한쪽 끝을 들어 올린 다음 떨어뜨립니다. D. 무언가를 찔러서 떨어뜨립니다. 이 비디오가 끝난 후 어떤 동작이 예상됩니까? A. 감자 저어주기 B. 감자 씻기 C. 감자 넣기 D. 감자 썰기 이 영상에서 일어나는 행동을 알아보고 순서대로 나열할 수 있나요? A. 아침 식사 만들기, 스토브 켜기, 냉장고 닫기, 우유 옮기기, 바나나 껍질 벗기기 B. 아이스크림 떠먹기, 초콜릿 시럽 짜기, 뿌리개 뿌리기, 냉장고 닫기 C. 냉장고 닫기, 우유 옮기기, 우유 뚜껑 돌리기, 우유 따르기, 우유 뚜껑 돌리기 D. 시리얼 상자 꺼내기, 그릇 잡기, 우유 따르기, 시리얼 저어주기, 냉장고 닫기 그림 2: 공간적 이해와 시간적 이해를 모두 포함한 12가지 평가 차원을 포괄하는 SEED-Bench의 데이터 샘플. 각 평가 차원에는 인간 주석에서 파생된 기준 진실 옵션이 있는 객관식 문제가 포함되어 있습니다. 표 1: 멀티모달 LLM에 대한 기존 벤치마크 비교. &quot;H/G 평가&quot;는 평가에 인간 또는 GPT를 사용하는지 여부를 나타냅니다.벤치마크 MME [23] 시각적 모달리티 이미지 이미지 및 포인트 클라우드 LAMM [24] LVLM-eHub [25] 이미지 MMBench [26] 당사 이미지 이미지 및 비디오 사용자 지정 질문 #답변 주석 답변 유형 H/G 평가 #모델 Y/N 자유형 N/AGPT자유형 인간자유형 A/B/C/D GPTN/A 당사 파이프라인은 여러 도메인에서 평가 데이터의 확장성을 지원하며 더 많은 평가 차원으로 벤치마크를 계속 확장할 것입니다. SEED-Bench를 기반으로 그림 1과 같이 모든 12개 차원에서 LLM, ImageLLM 및 VideoLLM을 포함한 18개 모델을 종합적으로 평가합니다. 다중 선택형 질문의 선택지 중 하나에 모델의 예측을 일치시키기 위해 ChatGPT를 사용하는 MMBench [26]와 달리(정렬 비율은 87.0%에 불과함) 다음을 따릅니다. GPT-3[32]는 각 후보 옵션에 대한 로그 우도를 계산하고 &quot;A&quot; 또는 &quot;B&quot; 또는 &quot;C&quot; 또는 &quot;D&quot;를 출력하는 모델의 지시 수행 기능에 의존하지 않고 최종 예측으로 가장 높은 값을 가진 것을 선택합니다. 12개 차원에 걸쳐 결과를 분석하여 공간 및 시간 이해 기능 모두에서 기존 멀티모달 모델을 포괄적으로 비교합니다. 대다수의 MLLM이 여전히 12개 평가 차원에서 제한된 성능을 보이는 것을 관찰하고 놀랍게도 VideoLLM이 ImageLLM과 비교하여 시간 이해에서 경쟁력 있는 성능을 달성하지 못한다는 것을 발견했습니다. 평가 결과를 통해 SEED-Bench가 보다 진보된 MLLM에 대한 향후 탐색을 동기를 부여하는 통찰력을 제공하는 것을 목표로 합니다. 평가 플랫폼을 출시하고 모델 성능을 평가하고 비교하기 위한 리더보드를 지속적으로 유지할 것입니다.
--- CONCLUSION ---
이 연구에서 우리는 생성 이해에 대한 다중 모달 대규모 언어 모델(MLLM)에 대한 포괄적이고 객관적인 평가를 제공하기 위해 대규모 벤치마크 SEED-Bench를 제안합니다. SEED-Bench는 정확한 인간 주석이 있는 19K개의 객관식 문제로 구성되어 있으며, 공간적 및 시간적 이해에 대한 12개의 평가 차원을 포함합니다. 우리는 특정 평가 차원을 타겟으로 하는 객관식 문제를 만드는 고급 파이프라인을 설계하여 다양한 도메인에서 평가 데이터의 확장성을 용이하게 합니다. 또한 자동 필터링과 수동 검증을 통합하여 생성된 질문과 답변의 품질을 개선합니다. 우리는 18개 모델에 대한 철저한 평가를 수행하고 성과를 분석하고 비교하여 향후 연구에 대한 통찰력을 제공합니다. 우리는 리더보드를 출시하고 지속적으로 유지하여 커뮤니티가 모델 성과를 평가할 수 있는 플랫폼을 제공할 계획입니다. 우리는 더 많은 데이터로 SEED-Bench의 평가 차원을 더욱 확대할 것입니다. 감사의 말 Junting Pan(CUHK MMLab)에게 통찰력 있는 제안을 해주셨고, Zhan Tong(난징 대학교)에게 데이터 처리를 해 주셨으며, Yi Chen(텐센트 AI 랩)에게 흥미로운 토론을 해 주셔서 진심으로 감사드립니다.참고문헌 [1] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv 사전 인쇄본 arXiv:2210.11416, 2022. [2] OpenAI. Gpt-4 기술 보고서, 2023. [3] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022. [4] FastChat. Vicuna. https://github.com/lm-sys/FastChat, 2023. [5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: 개방적이고 효율적인 기초 언어 모델. arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [6] Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. Blip-2: 동결된 이미지 인코더와 대규모 언어 모델을 사용한 언어 이미지 사전 학습 부트스트래핑. ICML, 2023. [7] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny. Minigpt-4: 고급 대규모 언어 모델을 사용한 시각 언어 이해 향상. arXiv 사전 인쇄 arXiv:2304.10592, 2023. [8] Haotian Liu, Chunyuan Li, Qingyang Wu 및 Yong Jae Lee. 시각적 지시 조정. arXiv 사전 인쇄 arXiv:2304.08485, 2023. [9] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: 모듈화는 다중 양식을 갖춘 대규모 언어 모델을 강화합니다. arXiv 사전 인쇄 arXiv:2304.14178, 2023. [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung 및 Steven Hoi. Instructblip: 명령 조정을 통한 범용 비전 언어 모델을 지향합니다. arXiv 사전 인쇄 arXiv:2305.06500, 2023. [11] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang 및 Ziwei Liu. Otter: 상황에 맞는 명령어 조정이 가능한 다중 모드 모델입니다. arXiv 사전 인쇄 arXiv:2305.03726, 2023. [12] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo 및 Kai Chen. Multimodal-gpt: 인간과의 대화를 위한 비전 및 언어 모델, 2023. [13] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang 및 Deng Cai. Pandagpt: 지시할 수 있는 하나의 모델을 모두 따르십시오. arXiv 사전 인쇄 arXiv:2305.16355, 2023. [14] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma 및 Furu Wei. Kosmos-2: 다중 모드 대형 언어 모델을 세계에 접지합니다. arXiv 사전 인쇄 arXiv:2306.14824, 2023. [15] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang 및 Yu Qiao. 영상채팅: 채팅 중심의 영상 이해. arXiv 사전 인쇄 arXiv:2305.06355, 2023. [16] Muhammad Maaz, Hanoona Rasheed, Salman Khan 및 Fahad Shahbaz Khan. Video-chatgpt: 대규모 비전 및 언어 모델을 통해 상세한 비디오 이해를 지향합니다. arXiv 사전 인쇄 arXiv:2306.05424, 2023. [17] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang 및 Zhongyu Wei. Valley: 대규모 언어 모델 강화 기능을 갖춘 비디오 어시스턴트입니다. arXiv 사전 인쇄 arXiv:2306.07207, 2023. [18] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang 및 Ying Shan. 대규모 언어 모델에 비전의 씨앗을 심습니다. arXiv 사전 인쇄 arXiv:2307.08041, 2023. [19] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang 및 Xinlong Wang. 다중 양식의 생성적 사전 훈련. arXiv 사전 인쇄본 arXiv:2307.05222, 2023. [20] Yu Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin, Golovneva Olga, Wang Tianlu, Babu Arun, Tang Binh, Karrer Brian, Sheynin Shelly, Ross Candace, Polyak Adam, Howes Russ, Sharma Vasu, Xu Jacob, Singer Uriel, Li (AI) Daniel, Ghosh Gargi, Taigman Yaniv, Fazel-Zarandi Maryam, Celikyilmaz Asli, Zettlemoyer Luke, Aghajanyan Armen. 자기 회귀 다중 모달 모델 확장: 사전 학습 및 지침 조정. 2023. [21] Jing Yu Koh, Daniel Fried, Ruslan Salakhutdinov. 다중 모달 언어 모델을 사용하여 이미지 생성. arXiv 사전 인쇄본 arXiv:2305.17216, 2023. [22] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh. VQA에서 v를 중요하게 만들기: 시각적 질문 답변에서 이미지 이해의 역할 강화. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 6904-6913페이지, 2017.[23] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji. Mme: 다중 모달 대규모 언어 모델을 위한 포괄적인 평가 벤치마크. arXiv 사전 인쇄 arXiv:2306.13394, 2023. [24] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang 등. Lamm: 언어 지원 다중 모드 명령 조정 데이터 세트, 프레임워크 및 벤치마크입니다. arXiv 사전 인쇄 arXiv:2306.06687, 2023. [25] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao 및 Ping Luo. Lvlm-ehub: 대규모 비전 언어 모델에 대한 포괄적인 평가 벤치마크입니다. arXiv 사전 인쇄 arXiv:2306.09265, 2023. [26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: 귀하의 다중 모드 모델은 다재다능한 플레이어입니까? arXiv 사전 인쇄 arXiv:2307.06281, 2023. [27] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui Feng, Yuejie Zhang, Yaqian Li, Yandong Guo 및 Lei Zhang. Tag2text: 이미지 태깅을 통해 비전 언어 모델을 안내합니다. arXiv 사전 인쇄본 arXiv:2303.05657, 2023. [28] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang. Grit: 객체 이해를 위한 생성적 영역-텍스트 변환기. arXiv 사전 인쇄본 arXiv:2212.00280, 2022. [29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, Ross Girshick. 무엇이든 분할하세요. arXiv:2304.02643, 2023. [30] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao. Vinyl: 시각 언어 모델의 시각적 표현 재검토. CVPR, 2021. [31] https://github.com/PaddlePaddle/Paddle OCR. Paddleocr. [32] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020. [33] Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankanhalli, 및 Ying Shan. 대규모 언어 모델을 위한 좋은 시각적 토크나이저를 만드는 요소는 무엇입니까?arXiv 사전 인쇄본 arXiv:2305.12223, 2023. [34] Piyush Sharma, Nan Ding, Sebastian Goodman, 및 Radu Soricut. 개념적 캡션: 자동 이미지 캡션을 위한 정리되고 하이퍼니밍된 이미지 대체 텍스트 데이터 세트. ACL, 2018. [35] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. 시각적 상식을 학습하고 평가하기 위한 &quot;something something&quot; 비디오 데이터베이스. ICCV, 2017. [36] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. 자기중심적 비전 재조정. arXiv 사전 인쇄본 arXiv:2006.13256, 2020. [37] Hilde Kuehne, Ali Arslan, Thomas Serre. 행동의 언어: 목표 지향적 인간 활동의 구문과 의미 회복. CVPR, 2014. [38] Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi. Blip: 통합된 시각-언어 이해 및 생성을 위한 언어-이미지 사전 학습 부트스트래핑. ICML, 2022. [39] Stephanie Lin, Jacob Hilton 및 Owain Evans. Truthfulqa: 모델이 인간의 거짓말을 어떻게 모방하는지 측정. arXiv 사전 인쇄본 arXiv:2109.07958, 2021. [40] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu 및 Tat-Seng Chua. llms에서 시각적 프롬프트 생성기 전송. abs/23045.01278, 2023. [41] ml_foundations. Openflamingo. https://github.com/mlfoundations/open_flamingo, 2023. [42] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li 및 Yu Qiao. Llama-adapter v2: 매개변수 효율적인 시각적 교육 모델. arXiv 사전 인쇄 arXiv:2304.15010, 2023.
