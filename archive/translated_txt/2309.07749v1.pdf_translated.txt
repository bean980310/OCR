--- ABSTRACT ---
비디오 매팅은 흥미로운 효과를 캐주얼하게 촬영한 영화에 추가하는 것부터 비디오 제작 전문가를 지원하는 것까지 광범위한 응용 분야가 있습니다. 그림자와 반사와 같은 관련 효과가 있는 매팅도 연구 활동을 증가시켰으며, Omnimatte와 같은 방법이 관심 있는 동적 전경 객체를 자체 레이어로 분리하는 데 제안되었습니다. 그러나 이전 연구에서는 비디오 배경을 2D 이미지 레이어로 표현하여 더 복잡한 장면을 표현하는 용량이 제한되어 실제 비디오에 적용하는 데 방해가 되었습니다. 이 논문에서는 동적 2D 전경 레이어와 3D 배경 모델을 결합하는 새로운 비디오 매팅 방법인 OmnimatteRF를 제안합니다. 2D 레이어는 피사체의 세부 정보를 보존하는 반면 3D 배경은 실제 비디오의 장면을 견고하게 재구성합니다. 광범위한 실험을 통해 우리 방법이 다양한 비디오에서 더 나은 품질로 장면을 재구성한다는 것이 입증되었습니다. (a) Omnimatte BG (b) Omnimatte FG (c) 우리의 BG (d) 우리의 FG 그림 1. 시차 효과가 있는 비디오. 2D 이미지 표현(a)에 의해 제한되어, Omnimatte와 같은 이전 작업들은 배경에 시차 효과가 있는 비디오를 처리하지 못했습니다. 전경 레이어(b)는 재구성 손실을 최소화하기 위해 (해제) 효과를 포착해야 합니다. 반면에, 우리의 방법은 3D 배경(c)을 사용하여 깨끗한 전경 레이어(d)를 얻을 수 있습니다. 1.
--- INTRODUCTION ---
비디오 매팅은 비디오를 여러 레이어로 분리하고 연관된 알파 매트를 사용하여 레이어를 다시 합성하여 원본 비디오로 되돌리는 문제입니다. 레이어를 대체하거나 다시 합성하기 전에 개별적으로 처리할 수 있으므로 비디오 편집에서 다양한 용도로 사용되며 수십 년 동안 연구되어 왔습니다. 비디오 제작의 로토스코핑 및 온라인 회의의 배경 흐림과 같은 일반적인 응용 프로그램에서 목표는 관심 있는 객체만 포함하는 마스크를 얻는 것입니다. 그러나 많은 경우 관심 있는 객체뿐만 아니라 그림자 및 반사와 같은 관련 효과도 포함하는 비디오 매트를 만드는 것이 선호됩니다. 이를 통해 종종 필요한 보조 효과의 추가 수동 분할을 줄이고 편집된 결과 비디오의 사실성을 높이는 데 도움이 될 수 있습니다. 전경 객체의 관련 효과를 요인 제거할 수 있으면 깨끗한 배경을 재구성하는 데 도움이 되며 이는 객체 제거와 같은 응용 프로그램에서 선호됩니다. 이러한 이점에도 불구하고 이 문제는 훨씬 더 잘못 제기되었으며 기존 매팅 문제보다 훨씬 덜 탐구되었습니다. 이 문제를 해결하기 위한 가장 유망한 시도는 Omnimatte[21]입니다. Omnimatte는 동적 전경 객체와 관련 효과를 캡처하는 RGBA 레이어입니다. 비디오와 관심 있는 전경 객체에 각각 해당하는 하나 이상의 거친 마스크 비디오가 주어지면 이 방법은 관심 있는 모든 객체와 관련 효과가 없는 정적 배경 외에도 각 객체에 대한 omnimatte를 재구성합니다. Omnimatte[21]는 많은 비디오에 잘 작동하지만 배경을 모델링하는 데 호모그래피를 사용해야 한다는 제한이 있습니다. 이를 위해서는 배경이 평면이거나 비디오에 회전 동작만 포함되어야 합니다. 카메라 동작으로 인해 시차가 발생하고 객체가 서로를 가리는 한 이는 해당되지 않습니다. 이러한 제한은 그림 1에서 볼 수 있듯이 많은 실제 비디오에 적용하는 데 방해가 됩니다. D2NERF[36]는 장면의 동적 및 정적 부분을 모델링하는 두 개의 광도장을 사용하여 이 문제를 해결하려고 합니다. 이 방법은 전적으로 3D에서 작동하며 상당한 카메라 동작이 있는 복잡한 장면을 처리할 수 있습니다. 또한 마스크 입력이 필요 없다는 의미에서 자체 감독 방식입니다. 그러나 모든 움직이는 객체를 정적 배경에서 분리하고 거친 마스크와 같은 비디오에 정의된 2D 안내를 통합하는 방법이 명확하지 않습니다. 또한 여러 전경 객체를 독립적으로 모델링할 수 없습니다. 각 전경 객체를 별도의 광도 필드로 모델링하는 간단한 솔루션은 과도한 학습 시간을 초래할 수 있지만 각 광도 필드에서 동작을 의미 있게 분리하는 방법은 명확하지 않습니다. 2D 전경 레이어와 3D 배경 모델을 결합하여 두 가지 이점을 모두 제공하는 방법을 제안합니다. 가벼운 2D 전경 레이어는 복잡한 객체, 동작 및 3D로 모델링하기 어려울 수 있는 효과를 포함하여 여러 객체 레이어를 나타낼 수 있습니다. 동시에 3D로 배경을 모델링하면 복잡한 기하학 및 비회전 카메라 동작의 배경을 처리할 수 있으므로 2D 방법보다 더 광범위한 비디오 세트를 처리할 수 있습니다. 이 방법을 OmnimatteRF라고 하며 실험을 통해 비디오별 매개변수 조정 없이 다양한 비디오에서 견고하게 작동한다는 것을 보여줍니다. D²NeRF는 3D 장면의 배경 분리를 정량적으로 평가하기 위해 Kubrics로 렌더링한 5개 비디오 데이터 세트를 공개했습니다.이것은 몇 개의 가구와 견고한 그림자를 드리우는 움직이는 물체가 있는 간단한 실내 장면입니다.또한 오픈 소스 Blender 영화[6]에서 더욱 사실적이고 도전적인 설정을 위한 정교한 동작과 조명 조건으로 5개 비디오를 렌더링합니다.저희의 방법은 두 데이터 세트 모두에서 이전 연구보다 우수한 성과를 거두었으며 향후 연구를 용이하게 하기 위해 비디오를 공개합니다.요약하면 저희의 기여는 다음과 같습니다.1. 저희는 광도장[22]을 사용하여 3D에서 정적 배경을 더 잘 모델링하여 Omnimatte[21]를 더욱 견고하게 만드는 새로운 방법을 제안합니다.2. Omnimatte 마스크를 활용하여 움직이는 피사체가 있는 비디오에서 깨끗한 정적 3D 재구성을 얻기 위한 간단하면서도 효과적인 재교육 단계를 제안합니다. 3. 우리는 오픈소스 블렌더 영화[6]에서 렌더링된 5개의 까다로운 비디오 시퀀스의 새로운 데이터 세트를 공개하여 관련 효과가 있는 비디오 매팅(일명 옴니매팅[21]) 문제의 개발 및 평가를 보다 용이하게 합니다. 2.
--- RELATED WORK ---
비디오 매팅. 비디오 편집에서 비디오 매팅이 중요하기 때문에 이를 탐구하는 작업이 많이 있습니다. 그린 스크리닝과 로토스코핑은 모든 시각 효과 파이프라인에서 중요한 첫 단계입니다. 매팅 문제는 전경 피사체를 자체 RGBA 레이어로 추출하여 배경 RGB 레이어에서 분리하는 것을 목표로 하는데, 이는 매우 제약이 적은 문제입니다. 많은 접근 방식에서 사용자 상호 작용을 통합하는 것 외에도 동작 및 깊이 신호를 활용했습니다[7, 3, 32, 16, 9]. 배경 비디오 매팅[18]은 특히 사람의 실시간 비디오 매팅과 가닥 수준의 머리카락 세부 정보를 보존하는 것을 다룹니다. 연관 효과가 있는 매팅. 전경 피사체에 그림자나 반사와 같은 연관 효과가 있을 수 있으므로 비디오 매팅만으로는 종종 충분하지 않으며, 이를 전경 RGBA 레이어로 추출해야 합니다. 이 문제는 광범위하게 탐구되지 않았으며 실제로는 고급 대화형 로토스코핑 도구를 사용하여 수동으로 처리하는 경우가 많습니다[15]. Omnimatte[21]는 모든 관련 효과를 학습할 수 있는 일반 프레임워크를 제안한 최초의 연구입니다. 이전 연구에서는 종종 그림자[34, 33]와 같은 관련 효과를 구체적으로 다루었습니다. 관련 효과가 있는 매트 레이어를 얻는 기능은 다양한 사람의 동작 재타이밍[20], 일관된 배경 편집[13, 14], 배경 뺄셈, 그린 스크리닝 및 기타 여러 비디오 효과[21]와 같은 많은 흥미로운 응용 프로그램을 가지고 있습니다. 최근 FactorMatte[12]는 데이터 증가 및 조건부 사전 확률로 품질을 개선하기 위해 제안되었습니다. 이러한 연구는 전경 객체를 암시하는 미리 정의된 마스크를 사용하고 각 비디오를 여러 레이어로 분해한다는 점에서 공통점이 있으며, 각 레이어에는 관련 효과가 있는 하나의 객체가 있습니다. 그런 다음 모든 프레임에서 공유하는 배경 레이어, 2D 정적 이미지 또는 변형 가능한 아틀라스가 있습니다. 배경은 각 프레임을 렌더링하기 위해 호모그래피를 통해 워핑되고 잘립니다. 전경 레이어는 역학을 포착하는 데 큰 잠재력을 보여주었지만 단일 이미지 배경으로 인해 이러한
--- METHOD ---
Omnimatte와 같은 s는 관심 있는 동적 전경 객체를 자체 레이어로 분리하기 위해 제안되었습니다. 그러나 이전 작업에서는 비디오 배경을 2D 이미지 레이어로 표현하여 더 복잡한 장면을 표현하는 용량이 제한되어 실제 비디오에 적용하는 데 방해가 되었습니다. 이 논문에서는 동적 2D 전경 레이어와 3D 배경 모델을 결합하는 새로운 비디오 매팅 방법인 OmnimatteRF를 제안합니다. 2D 레이어는 피사체의 세부 정보를 보존하는 반면 3D 배경은 실제 비디오의 장면을 견고하게 재구성합니다. 광범위한
--- EXPERIMENT ---
s는 우리 방법이 다양한 비디오에서 더 나은 품질로 장면을 재구성한다는 것을 보여줍니다.(a) Omnimatte BG (b) Omnimatte FG (c) 우리 BG (d) 우리 FG 그림 1. 시차 효과가 있는 비디오. 2D 이미지 표현(a)에 의해 제한되어 Omnimatte와 같은 이전 작업은 배경에 시차 효과가 있는 비디오를 처리하지 못했습니다. 전경 레이어(b)는 재구성 손실을 최소화하기 위해 (해제) 효과를 캡처해야 합니다. 반면에 우리 방법은 3D 배경(c)을 사용하여 깨끗한 전경 레이어(d)를 얻을 수 있습니다. 1. 소개 비디오 매팅은 비디오를 연관된 알파 매트가 있는 여러 레이어로 분리하여 레이어를 원본 비디오로 다시 합성하는 문제입니다. 레이어를 대체하거나 다시 합성하기 전에 개별적으로 처리할 수 있으므로 비디오 편집에서 다양한 응용 프로그램이 있으며 수십 년 동안 연구되어 왔습니다. 비디오 제작의 로토스코핑 및 온라인 회의의 배경 흐림과 같은 일반적인 응용 프로그램에서 목표는 관심 객체만 포함된 마스크를 얻는 것입니다.그러나 많은 경우 관심 객체뿐만 아니라 그림자 및 반사와 같은 관련 효과도 포함하는 비디오 매트를 만들 수 있는 것이 선호됩니다.이렇게 하면 종종 필요한 보조 효과의 추가 수동 분할을 줄이고 편집된 결과 비디오의 사실감을 높이는 데 도움이 될 수 있습니다.전경 객체의 관련 효과를 요인 제거할 수 있으면 깨끗한 배경을 재구성하는 데 도움이 되며 이는 객체 제거와 같은 응용 프로그램에서 선호됩니다.이러한 이점에도 불구하고 이 문제는 훨씬 더 잘못 제기되었으며 기존 매팅 문제보다 훨씬 덜 탐구되었습니다.이 문제를 해결하기 위한 가장 유망한 시도는 Omnimatte[21]입니다.Omnimatte는 동적 전경 객체와 관련 효과를 캡처하는 RGBA 레이어입니다. 주어진 비디오와 관심 있는 전경 객체에 각각 해당하는 하나 이상의 거친 마스크 비디오가 주어지면, 이 방법은 관심 있는 모든 객체와 관련 효과가 없는 정적 배경 외에도 각 객체에 대한 옴니매트를 재구성합니다.옴니매트[21]는 많은 비디오에 잘 작동하지만 배경을 모델링하는 데 호모그래피를 사용해야 한다는 제한이 있습니다.이를 위해서는 배경이 평면이거나 비디오에 회전 동작만 포함되어야 합니다.카메라 동작으로 인해 시차가 발생하고 객체가 서로 가려지는 한 이는 해당되지 않습니다.이러한 제한은 그림 1에서 볼 수 있듯이 많은 실제 비디오에 적용하는 데 방해가 됩니다.D2NERF[36]는 장면의 동적 및 정적 부분을 모델링하는 두 개의 광도장을 사용하여 이 문제를 해결하려고 합니다.이 방법은 전적으로 3D에서 작동하며 상당한 카메라 동작이 있는 복잡한 장면을 처리할 수 있습니다.또한 마스크 입력이 필요 없다는 의미에서 자체 감독 방식입니다. 그러나 모든 움직이는 객체를 정적 배경에서 분리하고 거친 마스크와 같은 비디오에 정의된 2D 안내를 통합하는 방법이 명확하지 않습니다. 또한 여러 전경 객체를 독립적으로 모델링할 수 없습니다. 각 전경 객체를 별도의 광도 필드로 모델링하는 간단한 솔루션은 과도한 학습 시간을 초래할 수 있지만 각 광도 필드에서 동작을 의미 있게 분리하는 방법은 명확하지 않습니다. 2D 전경 레이어와 3D 배경 모델을 결합하여 두 가지 이점을 모두 제공하는 방법을 제안합니다. 가벼운 2D 전경 레이어는 복잡한 객체, 동작 및 3D로 모델링하기 어려울 수 있는 효과를 포함하여 여러 객체 레이어를 나타낼 수 있습니다. 동시에 3D로 배경을 모델링하면 복잡한 기하학의 배경과 회전하지 않는 카메라 동작을 처리할 수 있으므로 2D 방법보다 더 광범위한 비디오 세트를 처리할 수 있습니다. 이 방법을 OmnimatteRF라고 하며 실험을 통해 비디오별 매개변수 조정 없이 다양한 비디오에서 견고하게 작동한다는 것을 보여줍니다. D²NeRF는 3D 장면의 배경 분리를 정량적으로 평가하기 위해 Kubrics로 렌더링한 5개 비디오 데이터 세트를 공개했습니다.이것은 몇 개의 가구와 견고한 그림자를 드리우는 움직이는 물체가 있는 간단한 실내 장면입니다.또한 오픈 소스 Blender 영화[6]에서 더욱 사실적이고 도전적인 설정을 위한 정교한 동작과 조명 조건으로 5개 비디오를 렌더링합니다.저희의 방법은 두 데이터 세트 모두에서 이전 연구보다 우수한 성과를 거두었으며 향후 연구를 용이하게 하기 위해 비디오를 공개합니다.요약하면 저희의 기여는 다음과 같습니다.1. 저희는 광도장[22]을 사용하여 3D에서 정적 배경을 더 잘 모델링하여 Omnimatte[21]를 더욱 견고하게 만드는 새로운 방법을 제안합니다.2. Omnimatte 마스크를 활용하여 움직이는 피사체가 있는 비디오에서 깨끗한 정적 3D 재구성을 얻기 위한 간단하면서도 효과적인 재교육 단계를 제안합니다. 3. 오픈소스 블렌더 영화[6]에서 렌더링한 까다로운 비디오 시퀀스 5개의 새로운 데이터 세트를 공개하여 관련 효과가 있는 비디오 매팅(일명 옴니매팅[21]) 문제의 개발 및 평가를 보다 용이하게 합니다.2. 관련 연구 비디오 매팅.비디오 편집에서 중요성 때문에 비디오 매팅을 탐구하는 연구가 많이 있습니다.그린 스크리닝과 로토스코핑은 모든 시각 효과 파이프라인에서 중요한 첫 단계입니다.매팅 문제는 전경 피사체를 자체 RGBA 레이어로 추출하여 배경 RGB 레이어에서 분리하는 것을 목표로 하며, 이는 매우 제약이 적은 문제입니다.많은 접근 방식에서 사용자 상호 작용을 통합하는 것 외에도 동작 및 깊이 신호를 활용했습니다[7, 3, 32, 16, 9].배경 비디오 매팅[18]은 특히 사람의 실시간 비디오 매팅과 가닥 수준의 머리카락 세부 정보를 보존하는 것을 다룹니다.관련 효과가 있는 매팅. 비디오 매팅은 종종 충분하지 않은데, 전경 피사체에 그림자나 반사와 같은 연관 효과가 있을 수 있고 이를 전경 RGBA 레이어로 추출해야 하기 때문입니다. 이 문제는 광범위하게 탐구되지 않았으며 실제로는 종종 고급 대화형 로토스코핑 도구[15]를 사용하여 수동으로 처리합니다. Omnimatte[21]는 모든 연관 효과를 학습할 수 있는 일반 프레임워크를 제안한 최초의 회사입니다. 이전 연구에서는 종종 그림자와 같은 연관 효과를 구체적으로 다루었습니다[34, 33]. 연관 효과가 있는 매트 레이어를 얻는 기능은 다양한 사람의 동작 재타이밍[20], 일관된 배경 편집[13, 14], 배경 뺄셈, 그린 스크리닝 및 기타 여러 비디오 효과[21]와 같은 많은 흥미로운 응용 분야가 있습니다. 최근 FactorMatte[12]는 데이터 증가 및 조건부 사전 확률로 품질을 개선하기 위해 제안되었습니다. 이러한 작업은 전경 객체를 암시하는 미리 정의된 마스크를 사용하고 각 비디오를 여러 레이어로 분해하며 각 레이어에 하나의 객체와 연관 효과를 둡니다. 그런 다음 모든 프레임에서 공유하는 배경 레이어, 2D 정적 이미지 또는 변형 가능한 아틀라스가 있습니다. 배경은 각 프레임을 렌더링하기 위해 호모그래피를 통해 워프되고 잘립니다. 전경 레이어는 역학을 포착하는 데 큰 잠재력을 보였지만 단일 이미지 배경으로 인해 카메라 동작으로 인한 시차 효과가 없는 평면 환경의 비디오에 이러한 방법을 적용하는 것이 제한됩니다. 광도 필드. 광도 필드(RF)는 기하학적 세부 사항과 사실적인 모습을 포착할 수 있는 3D 표현으로 등장했습니다[22]. 광도 필드는 3D 장면을 세계 공간의 모든 지점의 위치와 시야 방향을 색상과 불투명도에 매핑하는 연속 함수로 모델링합니다. 광선을 따라 볼륨 렌더링을 통해 새로운 뷰를 합성할 수 있습니다. 이 연속 함수는 렌더링된 이미지에서 재구성 손실을 사용하여 최적화하여 학습합니다. 이 뷰 종속 볼륨 표현은 이전의 표면 기반 방법에서는 처리하기 어려웠던 다양한 어려운 장면을 모델링할 수 있습니다. 예를 들어 금속과 같은 반짝이는 표면이나 머리카락이나 털과 같은 흐릿한 표면입니다. 그 이후로 여러 축으로 확장되었습니다.더 나은 모양 모델링(예: 반사 및 굴절[31, 5, 2, 1]), 더 빠른 최적화[8, 27, 23] 및 동적 장면 모델링[38, 17, 10, 19].MLP 기반 암묵적 RF 표현은 학습 속도가 느리기 때문에 우리는 폭셀 기반 명시적 광도장 표현[8] [27]을 사용합니다.특히, 우리는 [8]의 인수분해된 폭셀 그리드 표현을 사용합니다.자기 감독 비디오 역학 인수분해.또 다른 관련 작업은 미리 정의된 마스크가 필요 없는 비디오 역학 인수분해입니다.최근 작업 중 하나는 동작 신호에만 의존하는 변형 가능한 스프라이트[39]입니다.다른 비디오 매팅 작업과 유사하게 2D 전경 및 배경 레이어와 Omnimatte와 동일한 제한 사항이 있습니다.3D 모델링의 경우 D²NeRF[36]는 두 개의 광도장, 즉 영어: 동적 콘텐츠 및 정적 콘텐츠용 다른 하나.D²NeRF[36]는 단 하나의 전경 객체로 매팅하는 특수한 경우를 처리하며 다른 방법과 비교할 때 평면 배경에 국한되지 않습니다.그러나 자체 감독 방법은 비디오당 하이퍼 매개변수 조정이 필요한 휴리스틱에 의존하며 새 비디오에 강력하게 일반화되지 않습니다.전경 재구성의 품질은 큰 비강체 모션이 있는 객체의 경우 제한될 수도 있습니다.따라서 세부 정보가 뛰어난 여러 개별 객체를 지원하는 감독 2D 매트와 비평면 비디오에서 작동하는 3D 배경 분리의 이점이 있는 연관 효과가 있는 비디오 매팅 방법을 제안합니다.3. 방법 Lu et al. [21]은 옴니 매트의 개념을 제안하여 RGBA 비디오 매트를 확장하여 그림자 및 반사와 같은 관심 객체의 연관 효과를 캡처합니다. 혼란을 피하기 위해 다음 텍스트에서 해당 작업을 대문자 Omnimatte로, 결과 RGBA 레이어를 이탤릭 omnimatte라고 합니다.매팅 설정에서 사용자는 T 프레임 {It}}\/\±1과 N개의 순서가 지정된 마스크 레이어 {M}1의 비디오를 준비합니다.각각 관심 객체의 거친 마스크 비디오를 포함합니다.비디오의 카메라 매개변수도 {Pt}로 미리 계산됩니다.목표는 객체와 관련 효과를 포함하는 RGBA 전경 레이어 C와 a, 그리고 전경 객체에서 나오는 효과가 없고 깨끗한 배경 레이어 Bt를 예측하는 것입니다.입력 프레임 It은 배경 위에 전경 레이어를 알파 합성하여 재구성해야 합니다.Omnimatte에서 배경은 정적 2D 이미지와 호모그래피 변환 Pt로 표현됩니다.프레임을 구성하기 위해 추정된 호모그래피 Pt에 따라 정적 배경의 일부를 추출합니다. 우리 연구의 핵심 아이디어는 전경을 2D로 유지하면서 광도장을 사용하여 정적 배경을 3D로 표현하여 객체의 역학을 더 잘 포착하는 것입니다. 우리는 배경을 모델링하기 위해 명시적 인수분해된 폭셀 기반 광도장[8]을 사용합니다. 이 경우 Pt는 카메라 포즈를 나타내고 배경 프레임은 볼륨 렌더링으로 렌더링됩니다. 전경 레이어는 여전히 2D 비디오입니다. 이 조합을 OmnimatteRF 모델이라고 합니다. 3.1. OmnimatteRF 모델 우리 모델의 개요는 그림 2에 나와 있습니다. 이 모델은 전경과 배경이라는 두 개의 독립적인 분기가 있습니다. 주어진 프레임에 대해 전경 분기는 각 객체에 대한 RGBA 이미지(omnimatte)를 예측하고 배경 분기는 단일 RGB 이미지를 렌더링합니다. 전처리. 비슷한 연구에 따라 기성품 모델 RAFT[29]를 사용하여 이웃 프레임 간의 광학 흐름을 예측합니다. 흐름은 보조 입력 및 감독을 위한 기준 진실로 사용되며 {Ft}로 표시됩니다. 또한 기성품 깊이 추정기 MiDaS [26]를 사용하여 각 프레임의 단안 깊이 맵 {Dt}을 예측하고 단안 깊이 손실에 대한 기준 진실로 사용합니다.배경.배경 분기는 장면의 3D 표현을 인코딩하는 정적 신경 광도장 fbg로 구성됩니다.프레임 It의 픽셀을 렌더링하기 위해 추정된 카메라 포즈 Pt에 따라 광선을 추적하고 최종 RGB 색상은 체적 렌더링을 통해 생성됩니다.전체 프레임을 렌더링한 결과는 (Bt, Ôt) = fûg(Pt)입니다.여기서 B+는 RGB 이미지이고 D₁는 깊이 맵입니다.전경.전경 분기는 Omnimatte와 유사한 UNet 스타일의 합성 신경망 ffg입니다.네트워크의 입력은 세 가지 맵의 연결입니다.1. 거친 마스크 M².마스크는 사용자가 제공하며 관심 객체를 설명합니다.픽셀이 객체 내부에 있으면 마스크 값은 1입니다. 2. 광학 흐름 Ft. 네트워크에 동작 힌트를 제공합니다. 네트워크가 보조 작업으로 광학 흐름을 예측한다는 점에 유의하세요(섹션 3.2.2 참조). 3. 피처 맵 Et. 피처 맵의 각 픽셀(x, y)은 3-튜플(x, y, t)의 위치 인코딩입니다. 여러 전경 레이어가 개별적으로 처리됩니다. i번째 레이어의 경우 네트워크는 omnimatte 레이어(C, a)와 흐름 Ĥi를 예측합니다. 디테일 전송. 이미지 품질과 학습 시간 간의 균형을 위해 전경 네트워크는 일반적으로 알파 레이어가 충분한 관련 효과를 캡처했을 때 세부 정보가 누락된 색상 레이어를 생성합니다. 출력 품질을 높이기 위해 Omnimatte는 입력 프레임에서 세부 정보를 전송합니다. 파이프라인에 동일한 프로세스를 포함합니다. 이는 최종 결과를 생성하기 위한 후처리 단계이며 모델 최적화에는 적용되지 않습니다. 3.2. 모델 최적화 우리는 모델의 두 가지 분기가 비디오 전용이므로 모든 비디오에 대해 OmnimatteRF 모델을 최적화합니다. 학습을 감독하기 위해 이미지 재구성 손실과 여러 정규화 손실을 사용합니다. Pt M CH Ft 侧 ffg Et fbg Volumetric Render Fi Bt Alpha Composite Ît Precons Ĥt Dt F bg Ldepth Lflow It Dt 그림 2. 방법 개요. 우리는 2D 전경 레이어와 3D 배경 레이어를 결합하는 OmnimatteRF라는 비디오 매팅 방법을 제안합니다. 전경 분기(ffg, 녹색 상자)는 각 객체에 대한 RGBA 레이어(Ct, α)와 보조 흐름 출력(Ft)을 예측합니다. 배경 분기(fbg, 노란색 상자)는 깊이(Bt, Dt)가 있는 배경 레이어를 생성합니다. 최적화. 학습 중에 예측된 색상(It)과 흐름(Ĥt)은 알파 합성되며, 입력에는 각각 빨간색과 녹색 테두리가 있습니다. 가장 오른쪽 열은 손실 함수의 데이터 항을 설명하며, 이 그림에서는 정규화 항을 생략했습니다.3.2. 재구성 손실 전경 및 배경 레이어의 알파 합성을 통해 합성된 이미지 It로 재구성 손실을 계산합니다.N iN Î₁ = Σ(1)| + [[(1 − α²) Bt (1) i=1 j=i=그리고 재구성 손실은 예측 프레임과 입력 프레임 간의 평균 제곱 오차입니다.Lrecons = ||Ît — It ||(2) 재구성 손실은 파이프라인의 두 분기를 동시에 감독합니다.체적 렌더링의 계산 비용에 의해 제한되어 배경 레이어는 각 단계에서 희소한 무작위 위치에서만 렌더링되며, 여기서 재구성은 합성된 픽셀 값에 대해 계산됩니다.3.2.2 전경 손실 Omnimatte를 따르고 알파 정규화 손실 La-reg, 알파 워프 손실 La-warp 및 흐름 재구성 손실 흐름을 포함합니다. 또한 입력 마스크와 마스크 손실 마스크를 일치시키기 위해 초기 알파 예측을 부트스트랩합니다.마스크 손실 마스크는 값이 임계값 아래로 떨어지면 점차 감소하고 비활성화됩니다.Omnimatte의 대부분 정규화 항은 파이프라인에 직접 적용할 수 있지만 흐름 재구성 손실은 예외입니다.손실의 공식은 동일하게 유지됩니다.레이어당 흐름 예측 Ĥ와 배경 레이어 흐름 Fb%가 주어지면 전체 흐름 Ft는 알파 구성을 통해 구성됩니다(식 1).그러면 손실은 다음과 같이 정의됩니다.Lflow = ||(Êt − Ft) &amp; M³||² t bg (3) 여기서 M은 프레임 It에 대한 모든 전경 마스크({M}})의 합집합이고 손실은 입력 거친 마스크 위치에서만 평가됩니다.Omnimatte의 저자는 이 경우의 손실의 효과를 보여주었고, 우리는 또한 절제 연구에서의 중요성을 보여줍니다.그러나 Fog를 얻는 방법은 불분명합니다. Omnimatte에서 배경 흐름은 네트워크의 입력과 구성을 위한 배경으로 모두 사용되는 이미지 호모그래피에서 파생될 수 있습니다.반면에 3D 배경에는 알려진 카메라 포즈만 있고 깊이는 없으므로 배경 흐름을 직접 얻을 수 없습니다.대신 지상 진실 흐름 Ft를 네트워크 입력으로 사용하여 동작 신호를 제공하고 마스크된 버전의 Ft를 구성을 위한 배경 흐름으로 사용합니다.마스크된 흐름은 F™ = Ft × (1 – M³) M)이며, 이는 거친 마스크에 표시된 영역이 0으로 설정된 지상 진실 광학 흐름입니다.는 요소별 곱셈을 나타냅니다.후자의 경우 네트워크가 모든 곳에서 0인 빈 레이어를 생성하도록 장려하기 때문에 구성을 위해 Ft 대신 F를 사용하는 것이 중요하다고 생각합니다.(a) (b) 그림 3. 배경 레이어 학습 신호.시간에 따라 배경 레이어에 대한 학습 신호가 어떻게 변하는지 설명합니다.이것은 배경이 일부 관련 효과(이 예에서는 그림자)를 포착하는 이유를 설명합니다.빨간색으로 동그라미 친 픽셀을 예로 들어보겠습니다. (a) 학습 시작 시 전경 알파 값(연한 녹색)에는 그림자가 포함되지 않습니다. 따라서 a는 작고 이 픽셀에서 Ît(x,y) ≈ Bt(x,y)입니다. 재구성 손실 재구성은 배경 네트워크 fbg가 이 시야각에서 이 위치에서 어두운 예측을 생성하도록 장려합니다. (b) 학습이 진행됨에 따라 a는 그림자 영역에서 커지고 Ît(x, y) ≈ Cit(x, y)입니다. 즉, 안개는 이 픽셀에서 감독 신호를 거의 또는 전혀 받지 못합니다. 어떤 방식으로든 그림자를 모델링한 경우(이 경우 구멍) 제거할 인센티브가 거의 없어 (c)에 아티팩트가 남습니다. 3.2.3 배경 손실 재구성 손실 외에도 배경 네트워크는 TensoRF [8]에서와 같이 총 변동 정규화 손실 Lbg-reg에 의해 감독됩니다. 또한, 카메라 동작이 회전만으로 구성될 때 단안 깊이 감독을 사용하여 장면 재구성을 개선합니다.Ldepth = metric (Dt, Ôt), (4) 여기서 Dt는 볼륨 렌더링[22]에서 추정된 깊이이고, 메트릭 함수는 MiDaS[26]의 스케일 불변 손실입니다.또한, 깊이가 플로터를 도입할 수 있음을 경험적으로 발견했으며, Mip-NeRF 360[4]에서 제안된 왜곡 손실 왜곡을 사용하여 배경의 아티팩트를 줄입니다.3.2.4 요약 공동 최적화에 대한 결합 손실은 다음과 같습니다.L = L recons + La-reg + La-warp + Lflow + Lmask + Foreground Lbg-reg +Ldepth + distort Background (5) 모든 최적화 단계에서 재구성 및 배경 손실은 희소한 무작위 위치에서 평가됩니다.전경 손실은 전체 이미지에 대해 계산됩니다.3.3. 마스크 재학습을 통한 깨끗한 배경 위에서 설명한 대로 파이프라인을 공동으로 학습하는 경우 배경 광도 필드가 그림자와 같은 일부 전경 콘텐츠를 모델링하는 경우가 있습니다(그림 3(c) 참조). 2D 이미지에 비해 3D 광도 필드는 훨씬 더 유능하여 모델에 시간 정보가 제공되지 않더라도 일부 시간적 효과를 포착하기 위해 구멍과 플로터와 같은 왜곡된 기하학적 구조를 활용할 수 있습니다. 예를 들어, 카메라가 시간이 지남에 따라 움직이면 표면이 그림자로 덮여 있는지 여부와 표면을 보는 방향 사이에 상관 관계가 있을 수 있습니다. 그림 3에서 이 문제를 설명하고 직관적인 수준에서 원인을 설명합니다. 전경 브랜치는 연관된 효과 없이 개체만 포함하는 거친 마스크 입력과 일치하는 알파 값을 생성하도록 부트스트랩됩니다. 즉, 값은 개체에서 1에 가깝지만 그림자에서는 0입니다(단순화를 위해 그림 3과 같이 개체가 그림자를 드리우는 전경 레이어 하나를 고려합니다). 그림자로 덮인 픽셀(x, y)에서 Eq. 1은 단순히 It(x, y) ≈ Bt(x, y)로 축소됩니다. 따라서 재구성 손실은 Bt(x, y)가 이 위치를 향한 레이 샷의 그림자 색상과 일치하도록 합니다. 학습이 진행됨에 따라 ffg는 그림자가 있는 영역에서 예측된 알파 값을 점차 증가시킵니다. 그림자가 딱딱하고 a가 1에 가까워지면 Eq. 1은 Ît(x, y) ≈ C²(x, y)로 평가되고 재구성 손실은 픽셀의 배경색에 거의 제약을 주지 않습니다. 결과적으로 fbg는 (x, y)에서 프레임 It를 향한 레이에 대해 생성하는 그림자 색상을 제거하는 방법을 학습할 수 없습니다. 그림자가 부드럽고 a가 그 중간인 경우도 있습니다. 이러한 경우 문제는 모호합니다. 따라서 선택적 최적화 단계를 통해 깨끗한 배경 재구성을 얻는 것을 제안합니다. 공동 훈련에서 전경 omnimatte 레이어는 배경 레이어에 누출된 콘텐츠가 있는 부분을 포함하여 대부분의 연관된 효과를 캡처할 수 있습니다.그런 다음 알파 레이어를 사용하여 알파 값이 높은 전경 영역의 샘플 없이 처음부터 광도장 모델을 훈련할 수 있습니다.절제 연구(그림 7 참조)에서 이 단계가 야생 비디오에 대해 더 깨끗한 배경 재구성을 생성한다는 것을 보여줍니다.배경만 최적화되므로 프로세스가 빠르고 완료하는 데 1시간도 걸리지 않습니다.4. 평가 저희는 정량적 및 정성적 방법을 각각 2D 비디오 매팅 및 3D 비디오 분할에서 최첨단 방법인 Omnimatte 및 D2NeRF[21, 36]와 비교합니다.또한 Omnimatte의 정적 이미지와 대조적으로 변형 가능한 2D 배경을 사용하는 Layered Neural Atlas(LNA)[13]와 비교합니다.4.1. 영화 데이터 세트 배경 분할의 정량적 평가에는 입력 비디오와 기준 진실 배경 이미지가 모두 있는 데이터 세트가 필요합니다. 이전 작업에서는 주로 CDW-2014 [35]와 같은 데이터 세트를 사용했는데, 이는 대부분 정적 배경으로 제한되어 있으며 당사 설정에 적용할 수 없습니다. 최근 D²NeRF에서 3D 배경 합성을 평가할 수 있는 Kubrics가 제안되었습니다. 그러나 이러한 비디오는 비교적 간단한 장면과 조명을 가지고 있습니다. 어려운 시나리오에서 비디오 매팅과 배경 분할을 평가하기 쉽도록 Blender Studio [6]에서 3개의 Blender 영화에서 6개의 클립을 선택했습니다. Kubrics와 비교하여 더 복잡한 장면과 조명 조건, 캐릭터의 크고 비강체적인 움직임, 더 높은 해상도가 특징입니다. 사용성을 보장하기 위해 카메라 궤적을 수동으로 편집하여 충분한 카메라 움직임이 있고 배우의 크기가 적당하도록 합니다. 배경 재구성 평가 목적으로 입력과 기준 진실을 얻기 위해 배우가 있는 클립과 배우가 없는 클립을 렌더링합니다. 카메라 포즈도 내보냅니다. 4.2. 실험 설정 제안한 방법의 성능을 4개의 데이터 세트에서 평가합니다. 1. Movies: 새로운 도전적인 데이터 세트입니다.2. Kubrics: D²NeRF에서 생성 및 사용된 데이터 세트로, 3D Warehouse [30]에서 Kubric [11]으로 렌더링한 움직이는 물체의 5개 장면으로 구성되어 있습니다.3. DAVIS [24, 25]: 사람, 자동차, 동물과 같은 움직이는 전경 피사체가 있는 짧은 클립입니다.이 데이터 세트는 2D 배경 매팅 방법을 평가하는 데 널리 사용됩니다 [21, 13, 39].4. Wild: 인터넷에서 수집한 야생 시퀀스로, 자연스럽게 촬영한 비디오에 더 가깝고, 이동 및 회전을 포함한 자연스럽고 노이즈가 많은 카메라 동작과 카메라에서 다른 거리에 있는 물체가 있습니다.자연스럽게 이러한 비디오는 순수한 2D 방법에 도전적인 배경을 가지고 있습니다.Kubrics와 Movies는 깨끗한 배경 레이어 렌더링이 가능한 합성 데이터 세트입니다.새로운 뷰 합성이 방법의 초점이 아니므로 입력 뷰로 배경을 평가합니다. 두 데이터 세트 모두 훈련 및 평가에 사용되는 알려진 카메라 포즈와 객체 마스크를 가지고 있습니다.DAVIS와 Wild는 깨끗한 배경이 없는 실제 세계 비디오입니다.따라서 우리는 방법의 견고성을 보여주기 위해 정성적 평가만 수행합니다.Wild의 비디오에 대해 우리는 COLMAP으로 카메라 포즈를 복구합니다.DAVIS 비디오를 포함하여 COLMAP에서 안정적으로 처리할 수 없는 비디오의 경우 RoDynRF[19]의 포즈를 사용합니다.ku-bag ku-chair Sop dodge D²NeRF Omnimatte LNA Ours 그림 4. 배경 재구성.우리는 정량적 평가에서 제시된 결과의 예를 보여줍니다.시차 효과가 있는 비디오의 경우 D²NeRF와 우리의 방법과 같은 3D 방법은 Omnimatte 및 LNA보다 왜곡이 적은 배경을 재구성합니다.거친 객체 마스크를 얻기 위해 Detectron 2[37]의 사전 훈련된 객체 분할 모델로 추출하려고 시도합니다.작동하지 않는 경우 Adobe After Effects의 Roto Brush 도구를 사용합니다.자세한 절차는 보충 자료에 설명되어 있습니다. 약 분의 수동 작업이 필요하여 200프레임 마스크를 생성할 수 있습니다. 모든 비디오에 대해 LOFTR[28] 및 OpenCV를 사용하여 호모그래피를 추정하여 Omnimatte 처리를 활성화합니다. D²NeRF[36]에서 언급했듯이 이 방법은 하이퍼파라미터에 민감합니다. 저자는 다양한 비디오에 대해 5가지 구성 세트를 출시했습니다. 제공된 모든 구성을 사용하여 모든 비디오를 실험하고 가장 성능이 좋은 구성을 보고합니다. 4.3. 구현 세부 정보 네트워크는 공개적으로 사용 가능한 Omnimatte[21] 및 TensoRF[8]의 공식 구현을 기반으로 구축됩니다. Kubrics의 비디오는 해상도가 512x512이고 모든 방법은 해상도 256x256에서 실행됩니다. 해상도가 1920x1080인 다른 데이터 세트의 비디오의 경우 4배로 다운샘플링합니다. 최대 15,000단계에 대해 네트워크를 최적화합니다. ffg의 학습률은 0.001로 설정되고 10,000단계 후에 지수적으로 감소합니다. fúg의 경우 TensoRF의 학습률 스케줄링 방식을 사용합니다. 단일 RTX3090 GPU에서 학습하는 데 최대 6시간이 걸립니다. 자세한 네트워크 아키텍처, 하이퍼 매개변수 및 타이밍 데이터는 보충 자료에 나와 있습니다. 코드와 데이터 세트도 공개됩니다. 4.4. 양적 평가 두 개의 합성 데이터 세트에서 방법의 배경 재구성 품질을 양적으로 평가합니다. 우리 차 자동차 가방 의자 베개 쿠브릭스 LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ D²NeRF 0.0.34.0.Omnimatte 0.0.31.0.0.0.34.31.0.0.SSIM↑ PSNR↑ 0.880 33.0.796 23.LPIPS↓ 0.SSIM↑ PSNR↑ 0.916 33.LPIPS↓ SSIM↑ PSNR↑ 0.0.926 38.0.0.26.0.0.21.LNA 우리 0.0.39.0.0.961 39.0.0.0.835 27.0.972 39.0.0.21.0.0.923 31.0.0.42.0.0.982 43.영화 D²NeRF 옴니매트 0.LNA 우리 당나귀 LPIPS↓ SSIM PSNR↑ 0.19.0.0.0.18.0.990 38. 개 LPIPS↓ SSIM↑ PSNR↑ 0.370 0.694 22.0.279 0.706 21.0.154 0.828 26.0.030 0.976 31. 닭 LPIPS↓ SSIM↑ PSNR↑ 수탉 다지 LPIPS↓ SSIM↑ PSNR↑ LPIPS↓ SSIM↑ PSNR↑ 0.0.0.0.704 20.0.818 19.0.978 32.0.0.0.708 25.0.0.20.0.23.0.0.23.0.0.26.0.0.937 24.0.0.27.0.0.991 39.표 1. 양적 평가. Kubrics 및 Movies 데이터 세트에서 우리 방법과 기준선의 배경 재구성 비교를 제시합니다. 가장 좋은 결과는 굵은 글씨로 표시하고 2위는 밑줄로 표시합니다. -로 표시된 결과는 이 방법이 좋은 분리를 제공하지 못한 결과입니다(시각 자료는 보충 자료). 표 1의 모든 비디오에 대한 PSNR, SSIM 및 LPIPS와 그림 4의 일부 시각화를 보고합니다. D²NeRF의 경우 Movies의 모든 비디오에 대해 제공된 모든 사전 설정 구성을 시도했으며 Dog, Rooster 및 Dodge 비디오에 대해서만 좋은 결과를 얻었습니다. 2D 배경 레이어가 있는 Omnimatte 및 LNA는 두 데이터 세트 모두에서 어려움을 겪습니다. 우리 방법은 이러한 비디오를 잘 처리할 수 있습니다. 4.5. 정성적 평가 그림 5에서 방법에 대한 정성적 비교를 제시합니다. 공간 제한으로 인해 모든 데이터 세트에서 최소한 하나의 비디오를 제시하지만 그림에서는 선택한 모든 비디오의 프레임을 보여줍니다. 원본 비디오는 보충 자료로 제공되며 시청하는 것이 좋습니다.D²NeRF는 미세 조정된 비디오에는 잘 작동하지만 추가적인 하이퍼 매개변수 조정 없이는 새로운 입력에는 작동하지 않습니다.Omnimatte 배경은 객체 주변에 상당한 왜곡이 있으며, 전경 레이어는 모든 잔차를 캡처하여 제한을 보상해야 합니다.우리의 방법은 3D 배경 모델로 다양한 비디오에서 잘 수행할 만큼 다재다능합니다.4.6. Ablation Studies 4.6. Loss Terms 그림 6에 Ldepth 없이 배경 재구성 결과를 제시합니다.회전 카메라 포즈가 있는 비디오 시퀀스의 경우 모델은 3D 단서가 부족하여 입력 비디오에서 3D 정보를 추출하는 데 어려움을 겪습니다.이 손실은 우리 방법을 더 광범위한 비디오로 확장하는 데 중요합니다.Lflow의 효과도 그림 6에 나와 있습니다.보조 작업은 전경 품질을 개선하고 관련 없는 콘텐츠를 줄입니다.4.6.2 깨끗한 배경 재교육 실제 시퀀스에 대해 추가 단계를 사용하여 처음부터 깨끗한 배경을 최적화합니다. 그림 7에서 초기 조인트 최적화의 배경 레이어와 최종 결과를 비교합니다. 이는 더 나은 배경을 얻는 간단하면서도 강력한 방법입니다. 4.7. 제한 사항 향후 작업에서 탐색할 수 있는 몇 가지 제한 사항을 나열합니다. 1. 배경 영역이 거의 항상 그림자로 덮여 있는 경우 배경 모델은 색상을 올바르게 복구할 수 없습니다. 그림 8에는 영화 비디오의 예가 나와 있습니다. 이론적으로 omnimatte 레이어에는 알파 채널이 있으며 배경이 원래 색상을 가질 수 있도록 하는 가산 그림자만 캡처할 수 있습니다. 그러나 이 문제는 현재 설정에서 크게 제약이 부족하여 모호하고 배경이 만족스럽지 못한 솔루션으로 이어집니다. 2. 전경 레이어는 관련 없는 콘텐츠를 캡처합니다. 실제 비디오에서는 흔들리는 나무와 움직이는 자동차와 같이 관련 없는 동작이 배경에 종종 있습니다. 이러한 효과는 정적 광도 필드로 모델링할 수 없으며 개체와의 연관성에 관계없이 전경 레이어에서 캡처합니다. 가능한 방향에는 i) 더미 2D 레이어를 사용하여 이러한 콘텐츠를 포착하거나 ii) 배경과 전경 모두 동작을 모델링할 수 있으므로 모호성을 해결하기 위한 추가 정규화가 있는 변형 가능한 3D 배경 모델이 포함됩니다. 3. 전경 객체는 가려진 경우 omnimatte 레이어에서 누락된 부분이 있을 수 있습니다. 전경 네트워크는 알파 구성을 위한 픽셀 값을 예측하므로 가려진 부분을 항상 환각하지는 않습니다. 4. 비디오 해상도가 제한적입니다. 이는 주로 Omnimatte에서 상속된 전경 모델의 U-Net 아키텍처 때문입니다. 다른 경량 이미지 인코더를 사용하면 더 높은 해상도를 지원할 수 있습니다. 5. 가중치가 무작위로 다르게 초기화되면 전경 레이어가 다른 콘텐츠를 캡처할 수 있습니다. 보충 자료에 시각적 결과를 포함했습니다. 5.
--- CONCLUSION ---
우리는 객체와 연관된 효과를 포함하는 RGBA 레이어인 옴니매트를 얻는 방법을 제안합니다. 이는 자동차 수탉 쿠-필로우 입력 배경 전경 D²NeRF 옴니매트 우리의 D²NERF 옴니매트 우리의 그림 5. 정성적 비교. 우리는 각 데이터 세트의 비디오에 대한 우리의 방법과 기준 방법의 결과를 비교합니다. 독자는 보충 자료에서 제공되는 더 많은 시퀀스의 비디오 파일을 보는 것이 좋습니다. 처음 두 비디오는 Kubrics와 Movies에서 합성한 것이고, 그 다음에 세 개의 Wild 비디오가 나옵니다. 옴니매트는 3D 객체를 처리하지 못하고 왜곡된 배경을 생성합니다. D² NeRF는 적절한 하이퍼 매개변수가 있는 비디오에 작동하지만 새로운 비디오로 쉽게 일반화되지 않습니다. 우리의 방법은 여러 다른 설정의 비디오를 처리합니다. 공간 제약으로 인해 LNA 결과는 보충 자료로 미룹니다. 깊이가 있는 입력 깊이가 없는 입력 Lflow가 있는 입력 (a) 거친 마스크 (b) 배경 (c) 깊이 A 그림 6. 손실 용어 절제. 깊이가 없는 실제 세계 비디오의 배경과 흐름이 없는 전경은 실제 세계 비디오에서 저하될 수 있습니다.(f) 깊이 (d) 학습된 마스크 (e) 배경 그림 7. 깨끗한 배경 재학습. 공동으로 학습된 배경 레이어는 바닥에 구멍으로 그림자를 캡처할 수 있습니다(ac). 공동 학습 후, 전경 옴니매트는 깨끗한 배경을 학습하는 데 사용할 수 있는 더 나은 마스크를 제공합니다(df). 2D 전경 레이어와 3D 배경 모델을 결합합니다. 광범위한 실험을 통해 우리의 접근 방식이 이전 방법의 기능을 넘어 다양한 비디오에 적용 가능하다는 것이 입증되었습니다.(a) (b) (c) 그림 8. 제한 사항.(a), (b): 비디오의 대부분 프레임에서 영역이 그림자로 덮여 있을 때 배경에 구운 그림자가 생길 수 있습니다.(c): 전경 레이어는 배경에서 무관한 개체 동작(왼쪽 가운데)을 캡처합니다. 비디오에서 가장 잘 볼 수 있습니다. 참고문헌 [1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O&#39;Toole, Changil Kim. Hyperreel: 광선 조건 샘플링을 사용한 고충실도 6자유도 비디오. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023.[2] Benjamin Attal, Jia-Bin Huang, Michael Zollhöfer, Johannes Kopf, Changil Kim. 광선 공간 임베딩을 사용한 신경 광 필드 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2022.[3] Xue Bai, Jue Wang, David Simons, Guillermo Sapiro. 비디오 스냅컷: 국소화된 분류기를 사용한 견고한 비디오 객체 컷아웃. ACM Trans. Graph., 28(3):70, 2009.[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman. Mip-nerf 360: 무한한 앤티 앨리어싱 신경 광도 필드. CVPR, 2022.[5] Mojtaba Bemana, Karol Myszkowski, Jeppe Revall Frisvad, Hans-Peter Seidel, Tobias Ritschel. 굴절적 소설-뷰 합성을 위한 Eikonal 필드. Munkhtsetseg Nandigjav, Niloy J. Mitra, Aaron Hertzmann 편집자, SIGGRAPH &#39;22: 컴퓨터 그래픽 및 대화형 기술에 대한 특수 관심 그룹 컨퍼런스, 캐나다 브리티시 컬럼비아주 밴쿠버, 2022년 8월 7-11일, 39:1–39:9페이지. ACM, 2022.[6] Institute Blender, 2022년 10월. 2,[7] Gabriel J. Brostow 및 Irfan A. Essa. 비디오의 동작 기반 분해. 1999년 9월 20-25일, 그리스, 코르푸, 케르키라에서 열린 국제 컴퓨터 비전 컨퍼런스 회의록, 8-13페이지. IEEE Computer Society, 1999.[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu 및 Hao Su. Tensorf: Tensorial radiance fields. 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2022. 2, 3, 5, 6,[9] Yung-Yu Chuang, Aseem Agarwala, Brian Curless, David Salesin 및 Richard Szeliski. 복잡한 장면의 비디오 매팅. ACM Trans. Graph., 21(3):243-248, 2002.[10] Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang. 동적 단안 비디오에서 동적 뷰 합성. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 2021.[11] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, HsuehTi(Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi SM Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong 및 Andrea Tagliasacchi. Kubric: 확장 가능한 데이터세트 생성기입니다. 2022.[12] Zeqi Gu, Wenqi Xian, Noah Snavely, Abe Davis. Factormatte: 재구성 작업을 위한 비디오 매팅 재정의, 2022.[13] Yoni Kasten, Dolev Ofri, Oliver Wang, Tali Dekel. 일관된 비디오 편집을 위한 계층화된 신경 아틀라스. ACM Transactions on Graphics(TOG), 40(6):1-12, 2021. 2, 5, 6,[14] Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, Jia-Bin Huang. 모양 인식 텍스트 기반 계층화된 비디오 편집. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023.[15] Wenbin Li, Fabio Viola, Jonathan Starck, Gabriel J. Brostow, Neill DF Campbell. Roto++: 형상 매니폴드를 사용하여 전문 로토스코핑 가속화. ACM 그래픽스 트랜잭션(ACM SIGGRAPH&#39;16 진행 중), 35(4), 2016.[16] Yin Li, Jian Sun, Heung-Yeung Shum. 비디오 객체 잘라내기 및 붙여넣기. ACM Trans. Graph., 24(3):595-600, 2005.[17] Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang. 동적 장면의 공간-시간 뷰 합성을 위한 신경 장면 흐름 필드. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2021, 가상, 1925년 6월, 2021, 6498-6508페이지. 컴퓨터 비전 재단/IEEE, 2021.[18] Shanchuan Lin, Andrey Ryabtsev, Soumyadip Sengupta, Brian Curless, Steve Seitz, Ira KemelmacherShlizerman. 실시간 고해상도 배경 매팅. arXiv, 페이지 arXiv-2012, 2020.[19] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, Jia-Bin Huang. 견고한 동적 광도 필드. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023. 2,[20] Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T Freeman, Michael Rubinstein. 비디오에서 사람의 리타이밍을 위한 계층적 신경 렌더링. SIGGRAPH Asia, 2020.[21] Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T Freeman, Michael Rubinstein. Omnimatte: 비디오에서 객체와 그 효과 연관시키기. CVPR, 2021. 1, 2, 3, 5, 6,[22] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, Ren Ng. Nerf: 뷰 합성을 위한 신경 광도 필드로 장면 표현하기. ECCV, 2020. 2, 5,[23] Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽 원시. ACM Trans. Graph., 41(4):102:1– 102:15, 2022.[24] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, A. Sorkine-Hornung. 비디오 객체 분할을 위한 벤치마크 데이터 세트 및 평가 방법론. Computer Vision and Pattern Recognition, 2016.[25] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alexander Sorkine-Hornung, Luc Van Gool. 비디오 객체 분할에 대한 2017년 데이비스 챌린지. arXiv:1704.00675, 2017.[26] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun. 견고한 단안 깊이 추정을 향해: 제로 샷 교차 데이터 세트 전송을 위한 데이터 세트 혼합. IEEE 패턴 분석 및 머신 인텔리전스 저널, 44(3), 2022. 3,[27] Cheng Sun, Min Sun 및 Hwann-Tzong Chen. 직접 복셀 그리드 최적화: 광도장 재구성을 위한 초고속 수렴. CVPR, 2022.[28] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao 및 Xiaowei Zhou. LoFTR: 변압기를 사용한 검출기 없는 로컬 특징 매칭. CVPR, 2021.[29] Zachary Teed 및 Jia Deng. RAFT: 광학 흐름을 위한 순환 모든 쌍 필드 변환. Andrea Vedaldi, Horst Bischof, Thomas Brox 및 Jan-Michael Frahm 편집자, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, 2020년 8월 23-28일, Proceedings, Part II, Lecture Notes in Computer Science의 12347권, 402-419페이지. Springer, 2020.[30] Inc Trimble, 2022년 10월.[31] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd E. Zickler, Jonathan T. Barron 및 Pratul P. Srinivasan. Ref-nerf: 신경 광도장에 대한 구조화된 뷰 종속 모양. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2022, 루이지애나주 뉴올리언스, 미국, 2022년 6월 18일~24일, 5481~5490페이지. IEEE, 2022.[32] Jue Wang, Pravin Bhat, Alex Colburn, Maneesh Agrawala, Michael F. Cohen. 대화형 비디오 컷아웃. ACM Trans. Graph., 24(3):585~594, 2005.[33] Tianyu Wang, Xiaowei Hu, Chi-Wing Fu, Pheng-Ann Heng. 양방향 관계 학습을 통한 단일 단계 인스턴스 그림자 감지. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR) 회의록, 1~11페이지, 2021년 6월.[34] Tianyu Wang, Xiaowei Hu, Qiong Wang, Pheng-Ann Heng, Chi-Wing Fu. 인스턴스 그림자 감지. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 2020년 6월.[35] Yi Wang, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, Yannick Benezeth, Prakash Ishwar. Cdnet 2014: 확장된 변경 감지 벤치마크 데이터 세트. IEEE 컴퓨터 비전 및 패턴 인식 워크숍 컨퍼런스, 393-400페이지, 2014년.[36] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, Cengiz Oztireli. D² nerf: 단안 비디오에서 동적 및 정적 객체의 자체 감독 분리. Advances in Neural Information Processing Systems, 2022. 1, 3, 5, 6,[37] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019. 6,[38] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim. 자유 시점 비디오를 위한 시공간 신경 조도 필드. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, CVPR 2021, 가상, 2021년 6월 19-25일, 9421-9431페이지. Computer Vision Foundation/IEEE, 2021.[39] Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, Noah Snavely. 비지도 비디오 분해를 위한 변형 가능한 스프라이트.IEEE 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스, 2022년 6월.3,A. 추가 정성적 결과 본 논문에 제시된 결과의 비디오 파일(Movies, Kubrics, Wild 및 DAVIS 데이터 세트의 모든 비디오)은 보충 자료의 일부로 프로젝트 웹사이트에서 사용할 수 있습니다.다음에서 시청하는 것을 적극 권장합니다: https://omnimatte-rf.github.io 1. 우리 방법(OmnimatteRF)의 경우 모든 비디오에 대한 결과(마스크가 있는 입력, 전경 레이어, 배경 레이어, 배경 깊이 맵)를 포함합니다.2. D²NERF[36]의 경우 모든 비디오에 대해 작성자가 제공한 모든 구성 중에서 가장 좋은 결과를 사용합니다.어떤 구성도 비어 있지 않은 정적 및 동적 레이어를 성공적으로 재구성하지 못하면 비디오 파일을 삭제하고 그림 A2에 프레임만 표시합니다. 3. Omnimatte[21] 및 Layered Neural Atlas(LNA)[13]의 경우 Wild, Movies 및 Kubrics의 비디오를 포함했습니다.DAVIS의 결과는 이전 작업에서 찾을 수 있습니다.B. 임의 초기화 Omnimatte[21]에서도 논의했듯이 다른 임의 초기화로 인해 전경 레이어의 결과가 달라질 수 있습니다.그림 A1에 두 가지 예를 보여줍니다.모든 실험에서 임의 시드는 3으로 설정되었습니다.시드 =시드 =시드 =시드 =시드 =시드 =시드 =시드 =시드 =시드 =그림 A1. 임의 초기화의 효과.위: Wild/dogwalk 비디오의 경우 다른 시드가 사람의 환각 그림자의 양을 다르게 합니다.아래: Kubrics/cars 비디오의 경우 시드는 그림자가 객체에 연결되는 방식에 영향을 미칩니다.C. 추가 구현 세부 정보 C.1. 마스크 생성 우리의 방법과 Omnimatte는 관심 있는 모든 객체를 윤곽을 그리는 거친 마스크 비디오에 의존합니다. 합성 Kubrics 및 Movies 비디오에는 기준 진실 객체 마스크가 있으며 이를 직접 사용합니다.실제 비디오의 마스크를 얻기 위해 두 가지 워크플로 중 하나를 사용합니다.1. 먼저 Detectron 2[37]의 사전 학습된 Mask RCNN 모델(X101-FPN)로 비디오를 처리합니다.그런 다음 객체를 가장 잘 포착하는 모든 프레임에서 마스크를 수동으로 선택합니다.2. Adobe After Effects의 Roto Brush 도구를 사용하여 객체를 추적합니다.이 방법은 Mask R-CNN이 비디오에 적합한 마스크를 생성하지 못할 때 유용합니다.특히 Wild/dance 및 Wild/solo를 수동으로 처리했습니다.200프레임 비디오의 마스크 시퀀스를 생성하는 데 약 10분의 수동 작업이 걸립니다.C.2. 네트워크 아키텍처 전경 네트워크는 Omnimatte의 U-Net 아키텍처를 기반으로 하며, 이는 보충 자료[21]에 자세히 나와 있습니다.해당 네트워크를 OmnimatteRF에 적용하기 위해 배경 노이즈 입력을 2D 피처 맵 Et로 대체합니다. Et의 각 픽셀은 3D 벡터(x, y, t)의 위치 인코딩입니다.여기서 (x, y)는 픽셀 위치이고 t는 프레임 번호입니다.위치 인코딩 방식은 NeRF[22]에서 제안한 것과 동일하며 L = 10 주파수입니다.배경의 경우 MLP 피처 디코더가 있는 TensoRF[8]의 벡터-행렬 분해 모델을 사용합니다.초기 그리드는 동일한 해상도 No = 128을 갖고 최종 그리드는 N 640으로 제한됩니다.벡터와 행렬은 2000, 3000, 4000, 5500단계에서 업샘플링됩니다.= C.3.하이퍼 매개변수 모든 비디오에서 전경 네트워크에 0.001의 학습률을 사용하며, 이는 10,000단계에서 10,000단계당 0.1×의 속도로 지수적으로 감소합니다. 우리는 전경 훈련이 발산하는 것을 방지하는 데 감소가 중요하다는 것을 알았습니다. 마스크 부트스트래핑 손실 마스크는 초기 가중치가 50이며, 손실 값(가중치 전)이 0.02 미만으로 떨어지면 먼저 5로 줄어든 다음 같은 단계 수 후에 꺼집니다. 우리는 표 A1에 다른 손실 항목의 가중치를 기록합니다. 배경 네트워크 학습률 스케줄링 및 Lbg-reg 가중치는 원래 TensoRF[8]와 동일합니다. 일반적으로 대부분의 비디오에 동일한 하이퍼 매개변수 세트를 사용하고 아티팩트가 관찰될 때만 추가 항목을 추가합니다. 비디오 단계 Lrecons La-reg La-warp Lflow Ldepth Ldistort 모두 15,0.01(L1)/0.005(LO) 0.0.0.10, Wild/boldering DAVIS 표 A1. 하이퍼 매개변수. 우리는 실험에서 하이퍼 매개변수(단계 수와 손실 항목의 가중치)를 기록합니다. 첫 번째 행은 대부분의 비디오에서 공유하는 구성입니다. 나머지 행은 구성이 다른 비디오이고, -는 공유된 숫자에서 변경되지 않음을 의미합니다.FG 볼더링 워크 BG FG 자동차 개 BG 당나귀 그림 A2. 실패한 장면에 대한 D²NeRF 결과.방법 Omnimatte 12, 단계 학습(시간) 렌더링(초/이미지) 2.2.D²NeRF 100, 4.4.LNA 400, 8.0.우리 15, 3.3.Omnimatte 12, 1.0.D²NeRF 100, 3.2.LNA 400, 8.0.우리 15, 2.1.표 A2. 실행 시간 측정.OmnimatteRF와 기준 방법을 학습하는 데 걸리는 시간을 측정하여 비교합니다.위: 영화, 야생(480 x 270, DAVIS는 비슷한 428 x 240 해상도를 갖습니다).아래: Kubrics(256 x 256).C.4. 실행 시간 측정 우리는 표 A2에서 OmnimatteRF와 기준선 방법을 훈련하는 데 걸리는 시간을 측정하여 보고합니다. 모든 측정은 8코어 AMD R7-2700X CPU와 단일 NVIDIA RTXGPU가 있는 워크스테이션에서 수행됩니다. 우리의 방법은 3D 배경 광도 필드가 추가되어 Omnimatte보다 훈련하는 데 더 오래 걸립니다. 깨끗한 배경 재훈련 프로세스에서와 같이 배경 모델만 최적화하는 경우 비디오당 약 30분이 걸립니다. FG 솔로 치킨 BG
