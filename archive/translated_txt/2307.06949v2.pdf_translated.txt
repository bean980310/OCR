--- ABSTRACT ---
개인화는 생성 AI 분야에서 두드러진 측면으로 부상하여 다양한 맥락과 스타일에서 개인을 합성하면서도 정체성에 대한 높은 충실도를 유지할 수 있게 되었습니다. 그러나 개인화 프로세스는 시간과 메모리 요구 사항 측면에서 고유한 과제를 제시합니다. 각 개인화된 모델을 미세 조정하려면 상당한 GPU 시간 투자가 필요하고, 주제별로 개인화된 모델을 저장하는 것은 저장 용량 측면에서 까다로울 수 있습니다. 이러한 과제를 극복하기 위해, 우리는 한 사람의 단일 이미지에서 소규모의 개인화된 가중치 세트를 효율적으로 생성할 수 있는 하이퍼네트워크인 HyperDreamBooth를 제안합니다. 이러한 가중치를 확산 모델에 구성하고 빠른 미세 조정을 결합함으로써 HyperDreamBooth는 다양한 맥락과 스타일에서 높은 주제 세부 정보를 제공하는 동시에 다양한 스타일과 의미적 수정에 대한 모델의 중요한 지식을 보존하면서 사람의 얼굴을 생성할 수 있습니다. 저희의 방법은 DreamBooth보다 25배, Textual Inversion보다 125배 빠른 약 20초 만에 얼굴에 개인화를 구현하며, DreamBooth와 동일한 품질과 스타일 다양성으로 단 하나의 참조 이미지만 사용합니다. 또한 저희의 방법은 일반적인 DreamBooth 모델보다 10,000배 더 작은 모델을 생성합니다. 1.
--- INTRODUCTION ---
최근 텍스트-이미지(T2I) 개인화[25]에 대한 연구는 새로운 종류의 창의적 응용 프로그램을 위한 문을 열었습니다. 구체적으로 얼굴 개인화의 경우, 다양한 스타일로 특정 얼굴이나 사람의 새로운 이미지를 생성할 수 있습니다. 인상적인 스타일의 다양성은 사전 훈련된 확산 모델의 강력한 사전 지식 덕분이며, DreamBooth[25]와 같은 작업의 주요 속성 중 하나는 모델의 사전 지식을 손상시키지 않고 새로운 주제를 모델에 이식할 수 있는 능력입니다. 이러한 유형의 방법의 또 다른 주요 특징은 매우 다른 스타일을 적용하더라도 주제의 본질과 세부 정보가 보존된다는 것입니다. 예를 들어, 사람의 얼굴 사진으로 훈련할 때 애니메이션 만화 스타일로 그 사람의 새로운 이미지를 생성할 수 있으며, 그 사람의 본질의 일부가 보존되어 애니메이션 만화 인물에 표현됩니다. 이는 확산 모델에서 어느 정도 시각적 의미적 이해를 시사합니다. 이는 DreamBooth와 관련 방법의 두 가지 핵심 특성으로, 우리는 이를 그대로 두고 싶습니다. 그럼에도 불구하고 DreamBooth에는 크기와 속도라는 몇 가지 단점이 있습니다. 크기의 경우 원래 DreamBooth 논문은 확산 모델의 UNet 및 Text Encoder의 모든 가중치를 미세 조정하는데, 이는 안정적 확산의 경우 1GB 이상입니다. 속도 측면에서 확산 모델의 추론 속도 문제에도 불구하고 안정적 확산의 경우 DreamBooth 모델을 학습하는 데 약 5분이 걸립니다(학습 반복 1,000회). 이는 작업의 잠재적 영향을 제한합니다. 이 작업에서는 그림 1에 나와 있는 DreamBooth의 인상적인 주요 속성인 스타일 다양성과 주제 충실도를 변경하지 않고 이러한 단점을 해결하고자 합니다. 구체적으로, 모델 무결성을 보존하고 작은 모델로 빠르게 주제 본질에 근접하고자 합니다. 저희의 작업은 모델 무결성, 편집 가능성 및 주제 충실도를 보존하면서 DreamBooth의 크기와 속도 문제를 해결하는 것을 제안합니다. 우리는 다음과 같은 기여를 제안합니다. • • Lighweight DreamBooth(LiDB) - 사용자 정의된 부분의 크기가 약 100KB인 개인화된 텍스트-이미지 모델입니다. 이것은 저랭크 적응 [14] 가중치 공간 내부의 랜덤 직교 불완전 기저에 의해 생성된 저차원 가중치 공간에서 DreamBooth 모델을 학습하여 달성됩니다. • Lightweight DreamBooth 구성을 활용하고 텍스트-이미지 확산 모델에서 주어진 주제에 대한 가중치의 사용자 정의된 부분을 생성하는 새로운 HyperNetwork 아키텍처입니다. 이는 강력한 방향성 초기화를 제공하여 몇 번의 반복 내에서 강력한 주제 충실도를 달성하기 위해 모델을 더욱 미세 조정할 수 있습니다. 우리의 방법은 유사한 성능을 달성하면서 DreamBooth보다 25배 더 빠릅니다. • 우리는 LORA DreamBooth 모델의 순위를 최적화 중에 완화하여 더 높은 주제 충실도를 달성하는 순위 완화 미세 조정 기술을 제안합니다. 이를 통해 HyperNetwork를 사용하여 초기 근사치로 개인화된 모델을 초기화한 다음 순위 완화 미세 조정을 사용하여 고수준 주제 세부 정보를 근사할 수 있습니다. 하이퍼네트워크 접근 방식을 조사하게 된 한 가지 주요 측면은 주어진 생성 모델을 사용하여 높은 충실도로 특정 주제를 합성하기 위해서는 해당 출력 도메인을 &quot;수정&quot;하고 주제에 대한 지식을 모델에 삽입해야 한다는 것을 깨달은 것입니다. 즉, 네트워크 가중치를 수정하는 것입니다. 1단계 - 하이퍼네트워크 학습(대규모) &quot;A [V] face&quot; 하이퍼네트워크 2단계 - 빠른 미세 조정 &quot;A [V] face&quot; 재구성 손실 가중치 정규화 가중치 구성 텍스트-이미지 확산 모델 실제 가중치(1) 하이퍼네트워크(2) (2) 텍스트-이미지 확산 모델그림 2. HyperDreamBooth 학습 및 빠른 미세 조정. 1단계: 얼굴 이미지에서 네트워크 가중치를 예측하도록 하이퍼네트워크를 학습하여 예측된 가중치가 적용되면 텍스트-이미지 확산 네트워크가 문장 &quot;a [v] face&quot;에서 사람의 얼굴을 출력합니다. 미리 계산된 개인화된 L2 손실을 사용한 감독을 위한 가중치와 바닐라 확산 재구성 손실. 2단계: 얼굴 이미지가 주어지면, 하이퍼네트워크는 네트워크 가중치에 대한 초기 추측을 예측하고, 이는 재구성 손실을 사용하여 충실도를 높이기 위해 미세 조정됩니다. 2.
--- RELATED WORK ---
텍스트-이미지 모델 Imagen [26], DALL-E2 [22], Stable Diffusion (SD) [24], Muse [7], Parti [33] 등과 같은 최근의 여러 모델은 텍스트 프롬프트가 주어졌을 때 뛰어난 이미지 생성 기능을 보여줍니다. SD 및 Muse와 같은 일부 텍스트-이미지(T2I) 모델은 인코더 네트워크를 통해 주어진 이미지로 생성을 조절하는 것도 허용합니다. ControlNet [35]과 같은 기술은 깊이와 같은 새로운 입력 조절을 통합하는 방법을 제안합니다. 그러나 이러한 모델의 현재 텍스트 및 이미지 기반 조절은 충분한 피사체 세부 정보를 포착하지 못합니다. 실험의 용이성을 위해 비교적 작은 크기를 감안하여 SD 모델에서 HyperDreamBooth를 시연합니다. 그러나 제안된 기술은 일반적이며 모든 T2I 모델에 적용할 수 있습니다. 생성 모델의 개인화 개인화된 생성은 하나 또는 몇 개의 참조 이미지에서 특정 피사체의 다양한 이미지를 만드는 것을 목표로 합니다. 이전 접근 방식은 GAN을 사용하여 피사체 이미지를 새로운 맥락으로 조작했습니다. Pivotal tuning[23]은 역 잠재 코드를 사용하여 GAN을 미세 조정하는 반면, [20]은 개인화된 생성 사전을 위해 약 100개의 이미지로 StyleGAN을 미세 조정합니다.Casanova et al.[6]은 입력 이미지로 GAN을 조절하여 variHyperNetwork K 반복을 생성합니다.시각적 얼굴 weight_A Layer_weights weight_Transformer 인코더 0(초기) Transformer Decoder: weight_N weight_N A Layer_L weights 그림 3. 하이퍼네트워크 아키텍처: 하이퍼네트워크는 얼굴 이미지를 잠재 얼굴 특징으로 변환한 다음 0으로 시작되는 잠재 레이어 가중치 특징으로 연결하는 Visual Transformer(ViT) 인코더로 구성됩니다.Transformer Decoder는 연결된 특징의 시퀀스를 수신하고 델타 예측으로 초기 가중치를 정제하여 반복적으로 가중치 특징의 값을 예측합니다.확산 네트워크에 추가될 최종 레이어 가중치 델타는 디코더 출력을 학습 가능한 선형 레이어로 전달하여 얻습니다. 그러나 이러한 GAN 기반 기술은 생성된 이미지에서 종종 주제 충실도나 다양한 맥락이 부족합니다. 신경망의 가중치를 예측하는 보조 네트워크로 도입된 하이퍼네트워크[12]는 StyleGAN 역전[3]과 같은 개인화와 유사한 이미지 생성 작업에 적용되었습니다.
--- METHOD ---
둘 다 모델의 무결성과 스타일의 다양성을 보존하는 동시에 피사체의 본질과 세부 사항에 근접합니다. 초록 개인화는 생성 AI 분야에서 두드러진 측면으로 등장하여 다양한 맥락과 스타일의 개인을 합성하는 동시에 정체성에 대한 높은 충실도를 유지할 수 있게 합니다. 그러나 개인화 프로세스는 시간과 메모리 요구 사항 측면에서 고유한 과제를 제시합니다. 각 개인화된 모델을 미세 조정하려면 상당한 GPU 시간 투자가 필요하고, 피사체당 개인화된 모델을 저장하는 것은 저장 용량 측면에서 까다로울 수 있습니다. 이러한 과제를 극복하기 위해, 우리는 사람의 단일 이미지에서 소규모의 개인화된 가중치 세트를 효율적으로 생성할 수 있는 하이퍼네트워크인 HyperDreamBooth를 제안합니다. 이러한 가중치를 확산 모델에 구성하고 빠른 미세 조정을 결합함으로써 HyperDreamBooth는 다양한 맥락과 스타일에서 높은 주제 세부 정보를 사용하여 사람의 얼굴을 생성할 수 있으며, 다양한 스타일과 의미적 수정에 대한 모델의 중요한 지식을 보존할 수도 있습니다. 영어: 저희의 방법은 DreamBooth보다 25배, Textual Inversion보다 125배 빠른 약 20초 만에 얼굴에 대한 개인화를 달성하며, DreamBooth와 동일한 품질과 스타일 다양성으로 단 하나의 참조 이미지만 사용합니다. 또한 저희의 방법은 일반적인 DreamBooth 모델보다 10,000배 더 작은 모델을 생성합니다. 1. 서론 텍스트-이미지(T2I) 개인화에 대한 최근 연구[25]는 새로운 종류의 창의적 응용 프로그램을 위한 문을 열었습니다. 특히 얼굴 개인화의 경우 다양한 스타일로 특정 얼굴이나 사람의 새 이미지를 생성할 수 있습니다. 인상적인 스타일의 다양성은 사전 학습된 확산 모델의 강력한 사전 지식 덕분이며, DreamBooth[25]와 같은 작업의 주요 속성 중 하나는 모델의 사전 지식을 손상시키지 않고 새로운 주제를 모델에 이식할 수 있는 기능입니다. 이러한 유형의 방법의 또 다른 주요 특징은 매우 다른 스타일을 적용하더라도 주제의 본질과 세부 정보가 보존된다는 것입니다. 예를 들어, 사람의 얼굴 사진으로 학습할 때 애니메이션 만화 스타일로 그 사람의 새로운 이미지를 생성할 수 있는데, 그 사람의 본질의 일부가 보존되어 애니메이션 만화 인물에 표현되어 확산 모델에서 어느 정도 시각적 의미적 이해가 있음을 시사합니다. 이것들은 DreamBooth와 관련 방법의 두 가지 핵심 특성으로, 우리는 이를 그대로 두고 싶습니다. 그럼에도 불구하고 DreamBooth에는 크기와 속도라는 몇 가지 단점이 있습니다. 크기의 경우, 원래 DreamBooth 논문은 확산 모델의 UNet과 Text Encoder의 모든 가중치를 미세 조정하는데, 이는 안정적 확산의 경우 1GB 이상에 해당합니다. 속도 측면에서 확산 모델의 추론 속도 문제에도 불구하고, 안정적 확산의 경우 DreamBooth 모델을 학습하는 데 약 5분이 걸립니다(학습 반복 1,000회). 이는 작업의 잠재적 영향을 제한합니다. 이 작업에서 우리는 그림 1에 자세히 나와 있는 DreamBooth의 인상적인 주요 속성인 스타일 다양성과 주제 충실도를 변경하지 않고 이러한 단점을 해결하고자 합니다.특히, 우리는 모델 무결성을 유지하고 작은 모델로 빠른 방식으로 주제 본질에 근접하고자 합니다.우리의 작업은 모델 무결성, 편집 가능성 및 주제 충실도를 유지하면서 DreamBooth의 크기와 속도 문제를 해결하는 것을 제안합니다.우리는 다음과 같은 기여를 제안합니다.• • 경량 DreamBooth(LiDB) - 사용자 지정된 부분의 크기가 약 100KB인 개인화된 텍스트-이미지 모델입니다.이는 저랭크 적응[14] 가중치 공간 내부의 무작위 직교 불완전 기저에 의해 생성된 저차원 가중치 공간에서 DreamBooth 모델을 학습하여 달성됩니다.• 경량 DreamBooth 구성을 활용하고 텍스트-이미지 확산 모델에서 주어진 주제에 대한 가중치의 사용자 지정된 부분을 생성하는 새로운 하이퍼네트워크 아키텍처. 이러한 방법은 몇 번의 반복으로 강력한 주제 충실도를 달성하기 위해 모델을 더욱 미세 조정할 수 있는 강력한 방향성 초기화를 제공합니다. 저희 방법은 DreamBooth보다 25배 빠르지만 비슷한 성능을 달성합니다. • 저희는 랭크 완화 미세 조정 기술을 제안합니다. 여기서 LORA DreamBooth 모델의 랭크는 최적화 중에 완화되어 더 높은 주제 충실도를 달성하고, HyperNetwork를 사용하여 초기 근사로 개인화된 모델을 초기화한 다음 랭크 완화 미세 조정을 사용하여 고수준 주제 세부 정보를 근사할 수 있습니다. 하이퍼네트워크 접근 방식을 조사하게 된 한 가지 주요 측면은 주어진 생성 모델을 사용하여 높은 충실도로 특정 주제를 합성하기 위해서는 해당 출력 도메인을 &quot;수정&quot;하고 주제에 대한 지식을 모델에 삽입해야 한다는 것을 깨달은 것입니다. 즉, 네트워크 가중치를 수정하는 것입니다. 1단계 - 하이퍼네트워크 학습(대규모) &quot;A [V] face&quot; 하이퍼네트워크 2단계 - 빠른 미세 조정 &quot;A [V] face&quot; 재구성 손실 가중치 정규화 가중치 구성 텍스트-이미지 확산 모델 실제 가중치(1) 하이퍼네트워크(2) (2) 텍스트-이미지 확산 모델그림 2. HyperDreamBooth 학습 및 빠른 미세 조정. 1단계: 얼굴 이미지에서 네트워크 가중치를 예측하도록 하이퍼네트워크를 학습하여 예측된 가중치가 적용되면 텍스트-이미지 확산 네트워크가 문장 &quot;a [v] face&quot;에서 사람의 얼굴을 출력합니다. 미리 계산된 개인화된 감독을 위한 가중치, L2 손실 사용, 바닐라 확산 재구성 손실. 2단계: 얼굴 이미지가 주어지면, 하이퍼네트워크는 네트워크 가중치에 대한 초기 추측을 예측한 다음 재구성 손실을 사용하여 충실도를 높이기 위해 미세 조정합니다. 2. 관련 작업 텍스트-이미지 모델 Imagen [26], DALL-E2 [22], Stable Diffusion(SD) [24], Muse [7], Parti [33] 등과 같은 최근의 여러 모델은 텍스트 프롬프트가 주어졌을 때 뛰어난 이미지 생성 기능을 보여줍니다. SD 및 Muse와 같은 일부 텍스트-이미지(T2I) 모델은 인코더 네트워크를 통해 주어진 이미지로 생성을 조절하는 것도 허용합니다. ControlNet [35]과 같은 기술은 깊이와 같은 새로운 입력 조절을 통합하는 방법을 제안합니다. 그러나 이러한 모델의 현재 텍스트 및 이미지 기반 조절은 충분한 피사체 세부 정보를 포착하지 못합니다.
--- EXPERIMENT ---
생성 모델의 개인화 개인화된 생성은 하나 또는 몇 개의 참조 이미지에서 특정 피사체의 다양한 이미지를 만드는 것을 목표로 합니다.이전 접근 방식은 GAN을 사용하여 피사체 이미지를 새로운 맥락으로 조작했습니다.피벗 튜닝[23]은 역 잠복 코드로 GAN을 미세 조정하는 반면, [20]은 개인화된 생성 사전을 위해 약 100개의 이미지로 StyleGAN을 미세 조정합니다.Casanova et al. [6] 입력 이미지로 GAN을 조건화하여 다양한 HyperNetwork K 반복 수행 시각적 얼굴 weight_A 계층 가중치 weight_Transformer 인코더 0(초기) Transformer 디코더: weight_N weight_N A 계층_L 가중치 그림 3. 하이퍼네트워크 아키텍처: 하이퍼네트워크는 얼굴 이미지를 잠재 얼굴 특징으로 변환한 다음 0으로 시작되는 잠재 계층 가중치 특징으로 연결하는 Visual Transformer(ViT) 인코더로 구성됩니다. Transformer 디코더는 연결된 특징의 시퀀스를 수신하고 델타 예측으로 초기 가중치를 정제하여 반복적으로 가중치 특징의 값을 예측합니다. 확산 네트워크에 추가될 최종 계층 가중치 델타는 디코더 출력을 학습 가능한 선형 계층에 전달하여 얻습니다. 그러나 이러한 GAN 기반 기술은 생성된 이미지에서 종종 주체 충실도나 다양한 맥락이 부족합니다. 신경망의 가중치를 예측하는 보조 네트워크로 도입된 하이퍼네트워크[12]는 StyleGAN 역전[3]과 같은 개인화와 유사한 이미지 생성 작업에 적용되었으며, 이는 GAN 공간에서 편집을 위해 이미지의 잠재 코드를 역전하는 것을 목표로 하는 방법과 유사합니다[2]. 또한 언어 모델링[15, 19, 21]과 같은 다른 작업에도 사용되었습니다. 미세 조정을 통한 T2I 개인화 최근 기술은 향상된 주제 충실도와 다양한 텍스트 기반 재맥락화를 위해 T2I 모델을 향상시킵니다. 텍스트 역전[10]은 이미지 생성을 위해 주제 이미지의 텍스트 임베딩을 최적화하는 반면, [30]은 더 많은 주제 세부 정보를 포착하는 보다 풍부한 역전 공간을 탐색합니다. DreamBooth[25]는 주제 충실도를 위해 전체 네트워크 가중치를 조정합니다. CustomDiffusion[18], SVDiff[13], LoRa[1, 14], StyleDrop[28], DreamArtist[9]와 같은 다양한 방법은 특정 네트워크 부분을 최적화하거나 특수 튜닝 전략을 사용합니다. 이러한 기술의 대부분은 효과적이지만 느리며 고품질 결과를 얻으려면 피험자당 몇 분이 걸립니다. 빠른 T2I 개인화 최근 및 동시 작업 몇 가지는 더 빠른 T2I 모델 개인화를 목표로 합니다. E4T[11] 및 ELITE[31]와 같은 일부는 인코더 학습을 수행한 다음 전체 네트워크 미세 조정을 포함하는 반면, 우리의 하이퍼네트워크는 저순위 네트워크 잔차를 직접 예측합니다. SUTI[8]는 개인화된 이미지를 생성하기 위해 별도의 네트워크를 훈련하기 위한 데이터 세트를 생성하지만 피험자 충실도가 낮고 원래 모델의 무결성에 영향을 미칩니다. 동시 작업 InstantBooth[27] 및 Taming Encoder[16]는 확산 모델에 대한 컨디셔닝 분기를 도입하여 대규모 데이터 세트에 대한 훈련이 필요합니다. FastComposer[32]는 이미지 인코더를 사용하여 다중 피험자 생성에서 ID 블렌딩에 중점을 둡니다. [4], Face0 [29], Celebbasis [34]와 같은 기술은 효율적인 T2I 개인화를 위한 다양한 컨디셔닝 또는 안내 접근 방식을 탐구합니다. 그러나 다양성, 충실도 및 이미지 분포 준수의 균형을 맞추는 것은 여전히 어려운 일입니다. 제안하는 하이퍼네트워크 기반 접근 방식은 기존 기술과 달리 주제별 적응을 위한 저순위 네트워크 잔차를 직접 예측합니다. 3. 예비 χω 잠재 확산 모델(LDM). 텍스트-이미지(T2I) 확산 모델 Do(ε, c)는 주어진 노이즈 맵 € € Rhxw를 텍스트 프롬프트 T의 설명에 따라 이미지 I로 반복적으로 노이즈를 제거하고, 이는 텍스트 인코더를 사용하여 입력 텍스트 임베딩 c = (T)로 변환됩니다. 이 작업에서는 LDM [24]의 특정 구현인 안정적 확산 [24]을 사용합니다. 간단히 말해서 LDM은 3가지 주요 구성 요소로 구성됩니다. 주어진 이미지를 잠재 코드로 인코딩하는 이미지 인코더; 잠재 코드를 이미지 픽셀로 디코딩하는 디코더; 그리고 잡음이 있는 잠재 코드를 반복적으로 잡음을 제거하는 U-Net 잡음 제거 네트워크 D. 자세한 내용은 [24]를 참조하십시오. DreamBooth [25]는 주어진 T2I 잡음 제거 네트워크 De를 적응시켜 특정 피사체의 이미지를 생성하는 네트워크 미세 조정 전략을 제공합니다. 높은 수준에서 DreamBooth는 클래스별 사전 보존 손실이 있는 원래 모델의 일반화 능력을 유지하면서도 주어진 몇 가지 피사체 이미지에서 모든 확산 네트워크 가중치 0을 최적화합니다 [25]. 안정적 확산 [24]의 경우 이는 전체 잡음 제거 UNet을 미세 조정하는 것과 같으며 매개변수는 1GB가 넘습니다. 또한 단일 피사체에 대한 DreamBooth는 1K 학습 반복으로 약 5분이 걸립니다. 저순위 적응(LoRA) [1, 14]은 DreamBooth에 메모리 효율적이고 빠른 기술을 제공합니다. 구체적으로 LoRa는 전체 가중치 대신 네트워크 가중치 잔차를 미세 조정할 것을 제안합니다. 즉, 가중치 행렬 W Є Rnm을 갖는 계층의 경우 LoRa는 잔차 AW를 미세 조정할 것을 제안합니다.확산 모델의 경우 LoRa는 일반적으로 네트워크의 교차 및 셀프 어텐션 계층에 적용됩니다[1].LoRa의 핵심 측면은 AW 행렬을 저랭크 행렬 A = Rnxr 및 BERrxm으로 분해하는 것입니다.AW = AB.여기서 핵심 아이디어는 r &lt;&lt; n LORA DreamBooth Down (nxr) Up (rxm) AB Lightweight DreamBooth r =386k 변수 1.6MB a 100, b28k 변수 120KB Down Aux Down Train (nxa) Up Train (rxb) Up Aux (bxm) (axr) AABB 그림 4. Lightweight DreamBooth: LoRA 가중치 공간 내부의 무작위 직교 불완전 기저에 의해 생성된 모델 개인화를 위한 새로운 저차원 가중치 공간을 제안합니다. 이렇게 하면 약 100KB 크기(원래 DreamBooth의 0.01%, LORA DreamBooth 크기의 7.5%)의 모델을 얻을 수 있으며, 놀랍게도 견고한 편집 기능을 갖춘 강력한 개인화 결과를 얻기에 충분합니다. 그리고 A와 B의 결합된 가중치 수는 원래 잔여 AW의 매개변수 수보다 훨씬 적습니다. 사전 연구에 따르면 이 저순위 잔여 미세 조정은 원래 DreamBooth의 여러 유리한 속성을 보존하는 동시에 메모리 효율성과 속도가 뛰어나며, 특히 r = 1로 설정할 때에도 효과적인 기술입니다. 안정적인 확산 1.5 모델의 경우 r = 1인 LORADreamBooth는 약 386K 매개변수를 가지며 크기는 약 1.6MB에 불과합니다. 4. 방법 우리의 접근 방식은 이 섹션에서 설명하는 3가지 핵심 요소로 구성됩니다. 먼저 Lightweight DreamBooth(LiDB) 개념을 소개하고 가중치의 Low-Rank 분해(LoRa)를 어떻게 추가로 분해하여 모델 내의 개인화된 가중치 수를 효과적으로 최소화할 수 있는지 보여줍니다. 다음으로, HyperNetwork 학습과 모델이 수반하는 아키텍처에 대해 논의합니다. 이를 통해 단일 이미지에서 LiDB 가중치를 예측할 수 있습니다. 마지막으로, 몇 초 내에 출력 주제의 충실도를 크게 증폭할 수 있는 기술인 순위 완화 고속 미세 조정 개념을 제시합니다. 그림 2는 HyperDreamBooth 기술에서 하이퍼네트워크 학습과 고속 미세 조정 전략의 개요를 보여줍니다. 4.1. Lightweight DreamBooth(LiDB) HyperNetwork를 사용하여 직접 개인화된 가중치 하위 집합을 생성한다는 목표를 감안할 때 주제 충실도, 편집 가능성 및 스타일 다양성에 대한 강력한 결과를 유지하면서 가중치 수를 최소한으로 줄이는 것이 유익할 것입니다. 이를 위해, 우리는 DreamBooth 모델보다 10,000배 작고 LORA DreamBooth 모델보다 10배 이상 작은 개인화된 확산 모델을 허용하는 모델 개인화를 위한 새로운 저차원 가중치 공간을 제안합니다. 우리의 최종 버전은 30K 변수만 있고 120KB의 저장 공간만 차지합니다. Lightweight DreamBooth(LiDB)의 핵심 아이디어는 랭크 1 LoRa 잔차의 가중치 공간을 더욱 분해하는 것입니다. 구체적으로, 우리는 랭크 1 LoRA 가중치 공간 내에서 랜덤 직교 불완전 기반을 사용하여 이를 수행합니다. 그림 4에서 아이디어를 설명합니다. 이 접근 방식은 LoRA의 Down(A)과 Up(B) 행렬을 각각 두 개의 행렬로 더 분해하는 것으로 이해할 수도 있습니다. Aaux Atrain과 Aaux Є Rnxa, Atrain E Rax, B Btrain Baux와 Btrain Є Rrxb, Baux Є Rbxm입니다. 여기서 aux 계층은 행 방향 직교 벡터로 무작위로 초기화되고 고정됩니다. 그리고 학습 계층은 학습됩니다. 실험적으로 설정한 두 개의 새로운 하이퍼파라미터 a와 b가 도입되었습니다. 따라서 LiDB 선형 계층의 가중치 잔차는 다음과 같이 표현됩니다. A b = = AW x = Aaux Atrain Btrain Baux, (1) 여기서 r &lt;&lt; min(n, m), a &lt; n 및 b &lt; m입니다. Aaux와 Baux는 일정한 크기의 직교 행 벡터로 무작위로 초기화되고 고정되며 Btrain과 Atrain은 학습 가능합니다. 놀랍게도, a = 100 및 :50에서 학습 가능한 변수가 30K에 불과하고 크기가 120KB인 모델이 생성되고 개인화 결과가 강력하고 주제 충실도, 편집 가능성 및 스타일 다양성을 유지한다는 것을 발견했습니다. 실험 섹션에서 LiDB를 사용한 개인화에 대한 결과를 보여줍니다. = 4.2. 텍스트-이미지 모델의 빠른 개인화를 위한 하이퍼네트워크 사전 학습된 T2I 모델의 빠른 개인화를 위한 하이퍼네트워크를 제안합니다. 0이 T2I 모델의 교차 주의 및 자기 주의 계층 각각에 대한 모든 LiDB 잔차 행렬 집합인 Atrain 및 Btrain을 나타냅니다. 본질적으로 매개변수 ŋ을 갖는 하이퍼네트워크 Hn은 주어진 이미지 x를 입력으로 사용하고 LiDB 저랭크 잔차 Hn(x)를 예측합니다. HyperNetwork는 바닐라 확산 잡음 제거 손실과 가중치 공간 손실을 사용하여 도메인별 이미지 데이터 세트에서 학습됩니다.L(x) = a||D(x+e, c) − x||¾½ + ß||ô – 0 ||2, (2) 여기서 x는 참조 이미지이고, 0은 이미지 x에 대한 개인화된 모델의 사전 최적화된 가중치 매개변수이며, D는 노이즈가 있는 이미지 x + €와 감독 텍스트 프롬프트 c에 따라 조건화된 확산 모델이고, 마지막으로 a와 ẞ는 각 손실의 상대적 가중치를 제어하는 하이퍼매개변수입니다.그림 2(위)는 하이퍼네트워크 학습을 보여줍니다.초기 참조 출력 하이퍼네트워크 예측 하이퍼네트워크 + 하이퍼네트워크 없는 빠른 미세 조정 그림 5. 하이퍼네트워크 + 빠른 미세 조정은 강력한 결과를 얻습니다.각 행은 초기 하이퍼네트워크 예측(하이퍼네트워크 예측 열)과 빠른 미세 조정이 있는 하이퍼네트워크 예측 후(하이퍼네트워크 + 빠른 미세 조정)의 출력을 표시합니다. HyperNetwork 구성 요소가 없는 결과는 그 중요성을 강조합니다.감독 텍스트 프롬프트 이 작업을 위해 모든 유형의 학습된 토큰 임베딩을 피할 것을 제안하며, 하이퍼네트워크는 확산 모델의 LiDB 가중치를 예측하는 데만 작동합니다.모든 샘플에 대해 학습 프로세스를 &quot;[V] 얼굴&quot;로 조건지을 것을 제안합니다.여기서 [V]는 [25]에 설명된 드문 식별자입니다.추론 시간에 이 프롬프트의 변형을 사용하여 의미적 수정을 삽입할 수 있습니다(예: &quot;[V] 얼굴 in impressive style&quot;).HyperNetwork 아키텍처구체적으로 그림 3에서 설명한 것처럼 HyperNetwork 아키텍처를 ViT 이미지 인코더와 변환기 디코더의 두 부분으로 분리합니다.인코더 아키텍처에는 ViT-H를 사용하고 디코더 아키텍처에는 2개의 숨겨진 레이어 변환기 디코더를 사용합니다. 변환기 디코더는 이러한 유형의 가중치 예측 작업에 매우 적합합니다.확산 UNet 또는 텍스트 인코더의 출력은 계층의 가중치에 순차적으로 종속되므로 모델을 개인화하기 위해 다른 계층의 가중치 간에 상호 종속성이 있기 때문입니다.이전 작업[3, 12]에서는 이러한 종속성이 HyperNetwork에서 엄격하게 모델링되지 않았지만 위치 임베딩이 있는 변환기 디코더의 경우 이 위치 종속성이 언어 모델 변환기의 단어 간 종속성과 유사하게 모델링됩니다.저희가 아는 한 이것은 변환기 디코더를 HyperNetwork로 사용한 첫 번째 사례입니다.반복적 예측 저희는 HyperNetwork가 반복적 학습 및 예측 시나리오[3]에서 더 좋고 더 확실한 예측을 달성한다는 것을 발견했습니다.여기서 중간 가중치 예측은 HyperNetwork에 공급되고 네트워크의 작업은 해당 초기 예측을 개선하는 것입니다. 우리는 이미지 인코딩을 한 번만 수행하고, 이 추출된 특징 f는 HyperNetwork 디코딩 변환기 T에 대한 모든 라운드의 반복적 예측에 사용됩니다. 이를 통해 학습 및 추론 속도가 빨라지고 결과의 품질에 영향을 미치지 않는다는 것을 알 수 있습니다. 구체적으로, T의 순방향 패스는 다음과 같습니다. Ôk = T(f, Ôk−1), (3) 여기서 k는 가중치 예측의 현재 반복이고 k = s일 때 종료됩니다. 여기서 s는 최대 반복 횟수를 제어하는 하이퍼파라미터입니다. 가중치 0은 k = 0의 경우 0으로 초기화됩니다. 학습 가능한 선형 계층은 디코더 출력을 최종 계층 가중치로 변환하는 데 사용됩니다. HyperNetwork를 학습하기 위해 CelebAHQ 데이터 세트[17]를 사용하며 강력한 결과를 얻기 위해 15,000개의 ID만 필요하다는 것을 알 수 있습니다. 이는 다른 동시 방식보다 훨씬 적은 데이터입니다. 예를 들어 E4T[11]의 경우 100,000개의 ID, InstantBooth[27]의 경우 1,000,000개의 ID입니다. 4.3. 순위 완화 고속 미세 조정 = 초기 하이퍼네트워크 예측이 방향적으로 매우 정확하고 대상 얼굴과 유사한 의미적 속성(성별, 수염, 머리 색깔, 피부 색깔 등)을 가진 얼굴을 일관되게 생성하는 것을 발견했습니다. 그럼에도 불구하고 세부 사항은 충분히 포착되지 않았습니다. 이러한 세부 사항을 포착하기 위해 DreamBooth보다 훨씬 빠르지만 강력한 주제 충실도, 편집 가능성 및 스타일 다양성으로 사실상 동일한 결과를 얻는 최종 고속 미세 조정 단계를 제안합니다. 구체적으로, 먼저 개인화된 확산 모델 가중치 H(x)를 예측한 다음 확산 잡음 제거 손실 L(x) ||Ɗô(x + €, c) − x|| 2를 사용하여 가중치를 미세 조정합니다. 우리 작업의 핵심 기여는 순위 완화 미세 조정이라는 아이디어로, 빠른 미세 조정 전에 LORA 모델의 순위를 r = 1에서 r &gt;로 완화합니다. 구체적으로, 우리는 예측된 HyperNetwork 가중치를 모델의 전체 가중치에 추가한 다음, 새로운 상위 순위로 LoRA 미세 조정을 수행합니다. 이를 통해 피사체의 고주파 세부 정보를 근사화하는 방법의 기능이 확장되어 = 표 1의 하위 순위에 고정된 방법보다 더 높은 피사체 충실도를 제공합니다. 비교. 우리는 얼굴 신원 보존(Face Rec.), 피사체 충실도(DINO, CLIP-I) 및 프롬프트 충실도(CLIP-T)에 대한 방법을 DreamBooth 및 Textual Inversion과 비교합니다. 우리는 우리의 방법이 프롬프트 충실도에서 더 높은 점수를 달성하는 동시에 신원과 피사체 충실도를 더 밀접하게 보존한다는 것을 발견했습니다. 표 3. HyperNetwork Ablation. 우리는 구성 요소를 제거합니다: No Hyper(테스트 시 하이퍼네트워크 없음), Only Hyper(빠른 미세 조정 없이 하이퍼네트워크 예측 사용), 반복적 예측이 없는 전체 방법(k=1). 우리의 전체 방법은 모든 충실도 메트릭에서 가장 좋은 성능을 보이며, No Hyper가 약간 더 나은 프롬프트 추종을 달성합니다. 방법 Ours Face Rec.DINO ↑ CLIP-I↑ CLIP-T↑ 0.0.0.0.방법 Ours Face Rec.↑ DINO ↑ CLIP-I↑ CLIP-T↑ 0.0.0.0.DreamBooth 0.0.0.0.Hyper 없음 0.0.0.0.텍스트 반전 0.0.0.0.Hyper만 0.0.0.0.Ours (k=1) 0.0.0.0.표 2. DreamBooth와의 비교.최적화 시간을 최소화하는 다르게 조정된 DreamBooth 버전과 방법을 비교합니다.학습률을 높이고 반복 횟수를 줄여 하이퍼파라미터를 변경하면 DreamBooth에서 결과가 저하됩니다.DreamBooth-Agg-1은 400회 반복을 사용하고 DreamBooth-Agg-2는 바닐라 DreamBooth에서 사용하는 일반적인 1200회 반복과 달리 40회 반복을 사용합니다. 방법 DINO ↑ CLIP-I↑ CLIP-T↑ 0.0.Face Rec. ↑ Ours 0.0.0.DreamBooth 0.0.0.DreamBooth-Agg-1 0.0.0.0.DreamBooth-Agg-0.0.0.0.weight 업데이트. 저희가 아는 한 이러한 순위 완화 LORA 모델을 제안한 것은 저희입니다. 이 빠른 미세 조정 단계에서 동일한 감독 텍스트 프롬프트 &quot;[V] 얼굴&quot;을 사용합니다. HyperNetwork 초기화가 주어졌을 때 빠른 미세 조정이 40번의 반복으로 수행될 수 있으며, 이는 DreamBooth[25] 및 LORA DreamBooth[1]보다 25배 빠릅니다. 그림 5에서 초기, 중간 및 최종 결과의 예를 보여줍니다.5. 실험 Stable Diffusion v1.5 확산 모델에 HyperDreamBooth를 구현하고 확산 UNet의 모든 교차 및 셀프 어텐션 계층과 CLIP 텍스트 인코더에 대한 LoRa 가중치를 예측합니다.개인 정보 보호를 위해 시각적 요소에 사용된 모든 얼굴 이미지는 SFHQ 데이터 세트[5]에서 가져온 합성입니다.학습을 위해 CelebAHQ[17]의 15,000개 이미지를 사용합니다.5.1. 주체 개인화 결과 저희의 방법은 매우 다양한 얼굴에 대해 강력한 개인화 결과를 달성하며, 그 성능은 최첨단 최적화 기반 방법[10, 11, 25]과 동일하거나 그 이상입니다.또한 얼굴 정체성을 인형이나 애니메이션 캐릭터와 같은 매우 다른 도메인으로 의미적으로 변환하여 매우 강력한 편집성을 달성하며, 다양한 스타일 생성을 허용하는 모델의 강력한 스타일 사전을 보존합니다. 그림 6에 결과를 보여줍니다.5.2. 비교 정성적 비교 우리는 우리 방법을 Textual Inversion [10], DreamBooth [25] 및 E4T [11]와 비교합니다.결과는 그림 7에 나와 있습니다.우리는 우리 방법이 일반적으로 단일 입력 이미지 체제에서 Textual Inversion과 DreamBooth보다 훨씬 더 우수한 성능을 보이며 E4T와 비교하여 강력한 결과를 얻는다는 것을 관찰했습니다.특히 E4T가 참조 얼굴 포즈와 사실적인 모습에 과적합되는 경우에 출력이 매우 양식화되어야 하는 경우에도 그렇습니다.정량적 비교 및 절제 우리는 DINO, CLIP-I 및 CLIPT 메트릭과 함께 얼굴 인식 메트릭(VGGFaceInception ResNet의 &quot;Face Rec.&quot;)을 사용하여 우리 방법을 Textual Inversion 및 DreamBooth와 비교합니다[25]. 100개의 CelebAHQ 아이덴티티와 프롬프트(스타일 수정 및 재맥락화)를 사용하여 총 30,000개의 샘플을 얻은 표 1은 모든 지표에서 우리의 접근 방식이 우수한 성과를 거두었음을 보여줍니다. 그러나 네트워크 학습 제한(현실적인 얼굴 편향)으로 인해 얼굴 인식 지표는 상대적으로 약합니다. 이를 보완하기 위해 사용자 연구를 실시했습니다(자세한 내용은 아래 참조). 또한 보다 공격적인 표 4. 사용자 연구와 비교를 실시했습니다. 얼굴 인식 네트워크의 한계(스타일화된 얼굴은 OOD)를 감안하여 DB 및 TI와 스타일화된 세대를 비교하는 아이덴티티 충실도 사용자 연구를 실시했습니다. 우리의 접근 방식은 일반적으로 더 높은 사용자 선호도를 받았습니다. 우리의 ᎠᏴ 미정 우리의 TI 미정 7.8% 선호도 ↑ 64.8% 23.3% 11.9% 70.6% 21.6% 표 5. 사용자 스타일화 및 아이덴티티 선호도. 우리의 접근 방식과 SoTA 접근 방식 E4T 간의 스타일화 및 아이덴티티에 대한 사용자 선호도를 비교합니다. 사용자들은 일반적으로 우리의 방법을 선호합니다.선호도 ↑ 우리 E4T 60.0% 37.5% 미정 2.5% 입력 이미지 입력 이미지 입력 이미지 그림 6. 결과 갤러리: 우리의 방법은 다양한 피사체(왼쪽의 입력 이미지에 묘사됨)의 새로운 예술적이고 양식화된 결과를 생성할 수 있으며 피사체의 주요 얼굴 특징에 대한 무결성을 유지하면서 상당한 편집이 가능합니다.출력 이미지는 다음 캡션(왼쪽 위에서 오른쪽 아래로)과 함께 생성되었습니다. &quot;[V] 얼굴의 인스타그램 셀카&quot;, &quot;[V] 얼굴의 픽사 캐릭터&quot;, &quot;[V] 피부의 [V] 얼굴&quot;, &quot;[V] 얼굴을 록 스타로&quot;. 가장 오른쪽: &quot;[V] 얼굴의 전문가 사진&quot;. 반복 및 학습 속도를 변경한 DreamBooth 학습. 특히 DreamBooth-Agg-1(400회 반복)과 DreamBooth-Agg-2(40회 반복)는 1200회 반복의 바닐라 DreamBooth와 다릅니다. 표 2는 HyperNetwork 초기화 없이 공격적인 DreamBooth 학습을 수행하면 일반적으로 결과가 저하됨을 보여줍니다. 또한 HyperNetwork 제거(Hyper 없음), 미세 조정 없이 HyperNetwork만 활용(Hyper만), 반복 예측 없이 전체 설정(k=1) 등 방법의 구성 요소를 탐색하는 절제 연구를 보여줍니다. 표 3은 전체 설정이 프롬프트에 따른 메트릭이 약간 낮지만 더 우수한 피험자 충실도를 달성함을 보여줍니다. 사용자 연구 출력의 얼굴 정체성 보존에 대한 사용자 연구를 수행하고 방법을 DreamBooth 및 Textual Inversion과 비교합니다. 구체적으로 참조 얼굴 이미지와 방법 및 기준선에서 동일한 프롬프트를 사용하여 두 개의 무작위 세대를 제시하고 사용자에게 참조 얼굴 이미지와 얼굴 정체성이 가장 유사한 것을 평가하도록 요청합니다. 총 25개의 정체성을 테스트하고 질문당 5명의 사용자에게 쿼리를 보내 총 1,000개의 샘플 쌍을 평가합니다. 각 쌍에 대해 다수결을 취합니다. 표 4에 결과를 제시하며, 여기서 방법의 얼굴 정체성 보존에 대한 강한 선호도를 보여줍니다. 마지막으로, 우리는 주제 충실도와 스타일 충실도의 전반적인 선호도에 대한 사용자 연구를 제시하고, 저자가 친절하게 E4T를 실행한 SFHQ 데이터 세트의 신원 집합에 대해 우리의 접근 방식을 최신 E4T 방법[11]과 비교합니다.우리는 참조 주제 이미지와 참조 스타일 이미지를 모두 제시하고 사용자에게 신원 보존과 관련하여 어떤 출력을 선호하는지 묻습니다.참조 OO 출력 B HyperDreamBooth E4T 텍스트 반전 DreamBooth HyperDreamBooth E4T 텍스트 반전 DreamBooth 그림 7. 정성적 비교: 우리는 두 가지 다른 신원과 다섯 가지 다른 스타일 프롬프트에 대해 우리 방법(HyperDreamBooth), E4T[11], DreamBooth[25] 및 텍스트 반전[10]의 무작위 샘플을 비교합니다.우리는 우리의 방법이 일반적으로 신원을 보존하면서 매우 강력한 편집 가능성을 달성하여 일반적으로 단일 참조 체제에서 경쟁 방법을 능가한다는 것을 관찰합니다. E4T는 강력한 성능을 보이지만 이미지에 강력한 양식화가 필요한 경우에도 참조 머리 포즈와 사실적인 모양에 과적합되는 경향이 있습니다. 스타일 보존. 10개의 신원, 신원당 4개의 프롬프트, 질문당 15명의 사용자에게 쿼리를 보내 총 600개의 샘플을 테스트합니다. 결과는 표 5에 나와 있으며, 여기에서 우리는 사용자가 우리 방법에 대한 선호도를 관찰합니다. E4T는 강력한 결과를 얻고 신원을 잘 보존하는 방법이지만, 우리는 하드 프롬프트에서 약간 덜 정성적인 편집 가능성과 일부 일관성 오류를 관찰합니다. 우리 방법은 E4T의 경우 100,000개의 신원에 비해 15,000개의 신원에 대해 훈련된다는 점에 유의하세요. 6.
--- CONCLUSION ---
이 작업에서 우리는 확산 모델의 빠르고 가벼운 주제 개인화를 위한 새로운 방법인 HyperDreamBooth를 제시했습니다. 이 방법은 HyperNetwork를 활용하여 확산 모델에 대한 가벼운 DreamBooth(LiDB) 매개변수를 생성한 후 빠른 랭크 완화 미세 조정을 통해 DreamBooth 및 기타 최적화 기반 개인화 작업에 비해 크기와 속도가 크게 감소합니다. 우리는 이 방법이 주제 세부 정보와 모델 무결성을 보존하면서 다양한 스타일과 의미적 수정을 통해 고품질의 다양한 얼굴 이미지를 생성한다는 것을 보여주었습니다. 참고문헌 [1] 빠른 텍스트-이미지 확산 미세 조정을 위한 저랭크 적응. https://github.com/cloneofsimo/ lora, 2022. 3,[2] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle: 반복적 정제를 통한 잔여 기반 스타일간 인코더. IEEE/CVF 국제 컴퓨터 비전 컨퍼런스 회의록, 6711-6720페이지, 2021. 3. [3] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, Amit Bermano. Hyperstyle: 실제 이미지 편집을 위한 하이퍼네트워크를 사용한 Stylegan 반전. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 18511-18521페이지, 2022. 3,[4] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein. 확산 모델을 위한 범용 지침. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 843-852페이지, 2023.[5] David Beniaguev. 데이터 세트. 합성 얼굴 고품질(sfhq) https://github.com/SelfishGene/ SFHQ-dataset, 2022.[6] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal Drozdzal, Adriana Romero Soriano. Instanceconditioned gan. 신경 정보 처리 시스템의 발전, 34:27517-27529, 2021.[7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: 마스크 생성 변환기를 통한 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2301.00704, 2023.[8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, William W Cohen. 견습 학습을 통한 주제 중심 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2304.00186, 2023.[9] Ziyi Dong, Pengxu Wei, Liang Lin. Dreamartist: 대조적 프롬프트 튜닝을 통한 제어 가능한 원샷 텍스트-이미지 생성을 향해. arXiv 사전 인쇄본 arXiv:2211.11337, 2022.[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, Daniel CohenOr. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618, 2022. 3, 6,[11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, Daniel Cohen-Or. 텍스트-이미지 모델의 빠른 개인화를 위한 인코더 설계. arXiv 사전 인쇄본 arXiv:2302.12228, 2023. 3, 5, 6, 7,[12] David Ha, Andrew Dai, Quoc V Le. 하이퍼네트워크. arXiv 사전 인쇄본 arXiv:1609.09106, 2016. 3,[13] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, Feng Yang. Svdiff: 확산 미세 조정을 위한 컴팩트한 매개변수 공간. arXiv 사전 인쇄본 arXiv:2303.11305, 2023.[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685, 2021. 2,[15] Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh Hajishirzi, Matthew Peters. 힌트: 효율적인 제로샷 일반화를 위한 하이퍼네트워크 명령어 튜닝. arXiv 사전 인쇄본 arXiv:2212.10315, 2022.[16] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su. 텍스트-이미지 확산 모델을 사용하여 미세 조정이 필요 없는 이미지 사용자 지정을 위한 인코더 길들이기. arXiv 사전 인쇄본 arXiv:2304.02642, 2023.[17] Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen. 품질, 안정성 및 변형 개선을 위한 gan의 점진적 성장. arXiv 사전 인쇄본 arXiv:1710.10196, 2017. 5,[18] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu. 텍스트-이미지 확산의 다중 개념 사용자 지정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 1931-1941페이지, 2023.[19] Jesse Mu, Xiang Lisa Li, Noah Goodman. gist 토큰으로 프롬프트 압축하는 방법 배우기. arXiv 사전 인쇄본 arXiv:2304.08467, 2023.[20] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, Daniel Cohen-Or. Mystyle: 개인화된 생성 사전. ACM Transactions on Graphics(TOG), 41(6):1-10, 2022.[21] Jason Phang, Yi Mao, Pengcheng He, Weizhu Chen. 하이퍼튜닝: 역전파 없이 대규모 언어 모델을 적응시키는 방법. 기계 학습 국제 컨퍼런스, 27854-27875페이지. PMLR, 2023.[22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 2022.[23] Daniel Roich, Ron Mokady, Amit H Bermano, Daniel Cohen-Or. 실제 이미지의 잠재 기반 편집을 위한 피벗 튜닝. ACM 그래픽스 트랜잭션(TOG), 42(1):1–13, 2022.[24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10684-10695페이지, 2022. 2,[25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. 2022. 1, 3, 5, 6,[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적인 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479-36494, 2022.[27] Jing Shi, Wei Xiong, Zhe Lin, Hyun Joon Jung. Instantbooth: 테스트 시간 미세 조정 없이 개인화된 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2304.03411, 2023. 3,[28] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, Dilip Krishnan. Styledrop: 모든 스타일의 텍스트-이미지 생성. arXiv 사전 인쇄본 arXiv:2306.00983, 2023.[29] Dani Valevski, Danny Wasserman, Yossi Matias, Yaniv Leviathan. Face0: 얼굴에 텍스트-이미지 모델을 즉시 컨디셔닝. arXiv 사전 인쇄본 arXiv:2306.06638, 2023.[30] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: 텍스트-이미지 생성에서 확장된 텍스트 컨디셔닝. arXiv 사전 인쇄본 arXiv:2303.09522, 2023.[31] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: 사용자 지정 텍스트-이미지 생성을 위한 텍스트 임베딩으로 시각적 개념 인코딩. arXiv 사전 인쇄본 arXiv:2302.13848, 2023.[32] Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo Durand, and Song Han. Fastcomposer: 지역화된 주의를 통한 튜닝 없는 다중 주제 이미지 생성. arXiv 사전 인쇄본 arXiv:2305.10431, 2023.[33] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan 등. 콘텐츠가 풍부한 텍스트-이미지 생성을 위한 자동회귀 모델을 확장합니다. arXiv 사전 인쇄 arXiv:2206.10789, 2022.[34] Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan 및 Huicheng Zheng. 유명인 기반을 통해 확산 모델에 누구든지 삽입합니다. arXiv 사전 인쇄 arXiv:2306.00926, 2023.[35] Lvmin Zhang과 Maneesh Agrawala. 텍스트-이미지 확산 모델에 조건부 제어를 추가합니다. arXiv 사전 인쇄 arXiv:2302.05543, 2023.
