--- ABSTRACT ---
2022년 후반부터 대규모 언어 모델(LLM)이 매우 두드러지게 되었고 ChatGPT 및 Bard와 같은 LLM은 수백만 명의 사용자를 확보했습니다. 매주 수백 개의 새로운 LLM이 발표되고 그 중 다수가 기계 학습 모델 및 데이터 세트의 저장소인 Hugging Face에 저장됩니다. 지금까지 약 16,000개의 텍스트 생성 모델이 사이트에 업로드되었습니다. LLM의 엄청난 유입을 감안할 때 어떤 LLM 백본, 설정, 교육 방법 및 패밀리가 인기 있거나 트렌드인지 아는 것이 흥미롭습니다. 그러나 사용 가능한 LLM의 포괄적인 인덱스는 없습니다. 우리는 Hugging Face LLM의 비교적 체계적인 명명법을 활용하여 계층적 클러스터링을 수행하고 n-gram 및 용어 빈도-역문서 빈도를 사용하여 LLM 간의 커뮤니티를 식별합니다. 우리의 방법은 LLM 패밀리를 성공적으로 식별하고 LLM을 의미 있는 하위 그룹으로 정확하게 클러스터링합니다. 우리는 15,821개의 LLM에 대한 아틀라스인 Constellation을 탐색하고 탐색할 수 있는 공개 웹 애플리케이션을 제공합니다. Constellation은 덴드로그램, 그래프, 워드 클라우드, 산점도 등 다양한 시각화를 빠르게 생성합니다. Constellation은 다음 링크에서 사용할 수 있습니다: https://constellation.sites.stanford.edu/. 우리가 만든 데이터 세트는 Github의 @andrewgcodes(https://github.com/andrewgcodes)에서 공개적으로 공유됩니다.
--- CONCLUSION ---
대규모 언어 모델(LLM)의 수와 다양성이 증가함에 따라 이러한 모델을 구성, 분류 및 이해하기 위한 포괄적이고 체계적인 접근 방식이 필요합니다. 이 연구에서는 사용자 친화적인 웹 애플리케이션인 Constellation을 만들어 LLM 간의 계층적 관계를 시각화하여 눈에 띄는 LLM 패밀리와 기본 구조를 밝히는 데 도움이 되는 효과적인 솔루션을 제안했습니다. 저희의 접근 방식은 일반적으로 생물정보학과 시퀀스 유사성에서 영감을 받았습니다. 계층적 및 응집적 클러스터링을 덴드로그램, 워드 클라우드, 그래프 기반 표현과 같은 다양한 기술과 결합하여 활용합니다. 워드 클라우드는 눈에 띄는 모델 패밀리에 대한 상위 수준의 뷰를 제공하는 반면, 그래프 기반 시각화는 덴드로그램보다 더 유연한 형식으로 모델 간의 관계와 유사성을 묘사합니다. 저희 연구의 주요 한계는 LLM이 이름이 비슷할 때만 유사하다고 가정한다는 것입니다. 이는 전적으로 사실이 아닙니다. LLM은 Hugging Face에 저장하는 작성자가 원하는 대로 이름을 지정할 수 있습니다. 그러나 일반적으로 LLM은 구조적이고 논리적인 방식으로 명명되는 경향이 있음을 알 수 있습니다. 우리의 결과는 일반적으로 유사한 LLM은 유사한 이름을 공유한다는 우리의 가정이 타당하다는 것을 나타냅니다. 우리는 우리의 접근 방식이 유사한 LLM을 놓칠 수 있다는 것을 인정합니다. 특히 LLM 중 하나가 임의로 명명된 경우 더욱 그렇습니다. 또 다른 한계는 &quot;텍스트 생성&quot;이라는 레이블이 붙은 모든 모델이 반드시 LLM인 것은 아니라는 것입니다. 마지막으로, 덴드로그램이 진정한 &quot;진화적&quot; 트리가 아니라는 또 다른 경고가 있습니다. 동일한 저수준 클러스터의 모델은 일반적으로 신뢰할 수 있는 관련성을 갖지만, 이는 고수준 클러스터에는 해당되지 않습니다. Constellation을 공개적으로 제공함으로써 LLM과의 보다 체계적이고 정보에 입각한 참여를 장려하고자 합니다. LLM의 환경이 계속해서 빠르게 진화함에 따라 Constellation과 같은 도구는 연구자 및 개발자 커뮤니티가 이러한 발전에 발맞추는 데 도움이 될 것입니다. 참고문헌 1. Gao, A. (2023년 7월 8일). 대규모 언어 모델을 위한 신속한 엔지니어링. SSRN; SSRN. https://doi.org/10.2139/ssrn.2. Arancio, J. (2023년 4월 17일). 라마, 알파카 및 비쿠나: 노트북에서 실행되는 새로운 Chatgpt. Medium. https://medium.com/@jeremyarancio/exploring-llamas-family-models-how-we-achieved-running-llms-on-l aptops-16bf2539a1bb 3. Hiter, S. (2023, 6월 6일). What Is a Large Language Model? | Guide to LLMs. EWEEK. https://www.eweek.com/artificial-intelligence/large-language-model/ 4. Hugging Face. (nd). Hugging Face ― 한 번에 한 가지 커밋씩 NLP를 해결하는 임무. Huggingface.co. https://huggingface.co/ 5. Wei, D., Jiang, Q., Wei, Y., &amp; Wang, S. (2012). 유전자 시퀀스를 위한 새로운 계층적 클러스터링 알고리즘. BMC Bioinformatics, 13(1). https://doi.org/10.1186/1471-2105-13-6. Beautiful Soup 문서. (nd). Tedboy.github.io. 2023년 7월 19일 https://tedboy.github.io/bs4_doc/에서 검색됨 7. pandas 문서 - pandas 1.0.1 문서. (2023년 6월 28일). Pandas.pydata.org. https://pandas.pydata.org/docs/ 8. Streamlit Docs. (nd). Docs.streamlit.io. https://docs.streamlit.io/ 9. scipy. (2020년 2월 3일). scipy/scipy. GitHub. https://github.com/scipy/scipy 10. plotly.py. (2021년 9월 28일). GitHub. https://github.com/plotly/plotly.py 11. numpy/numpy. (2021년 10월 9일). GitHub. https://github.com/numpy/numpy 12. Scikit-Learn. (2019). 사용자 가이드: 목차 - scikit-learn 0.22.1 설명서. Scikit-Learn.org. https://scikit-learn.org/stable/user_guide.html 13. koonimaru. (2023년 6월 15일). radialtree. GitHub. https://github.com/koonimaru/radialtree 14. NLTK. (2009). Natural Language Toolkit NLTK 3.4.4 설명서. Nltk.org. https://www.nltk.org/ 15. Matplotlib. (nd). Matplotlib: Python 플로팅 - Matplotlib 3.3.4 설명서. Matplotlib.org. https://matplotlib.org/stable/index.html16. Aynaud, T. (2023년 7월 11일). Louvain Community Detection. GitHub. https://github.com/taynaud/python-louvain 17. Hapberg, A., Schult, D., &amp; Swart, P. (2008년 8월). NetworkX를 사용하여 네트워크 구조, 역학 및 기능 탐색. Conference.scipy.org. https://conference.scipy.org/proceedings/SciPy2008/paper_18. Mueller, A. (2020년 5월 7일). amueller/word_cloud. GitHub. https://github.com/amueller/word_cloud 19. Ahmad, Z. (2023년 7월 19일). ziishaned/learn-regex. GitHub. https://github.com/ziishaned/learn-regex 부록 단어 발생 gpt7b13bgptfinetunedllamagptqdistilgptPythiamodelwikitextsmallbaseinstructneooptvicuna4bitbloomv30b6balpaca125mcodeparrotrarityvfalcon8ksftlargedialogpttestallmediumloradsmergedsuperhotjhffpchatopenconcatowt350m70mchinese128gmptgpt3bmyawesomeeli5c lmtiny8bitheadlesscodegen33bdedupedshardedairoborosfinetunedgptvwizardlmgeneratorno560mrandomuncensoredfinetunexl1bmod27b65b elonmuskbloomzvbertguanacoftimdbgptj160mgptneoredpajamaneoxggml thedolly12bcutguten67bdelta표 1. 모든 Hugging Face LLM에서 가장 흔한 단어/구문
