--- ABSTRACT ---
오디오 초고해상도는 저해상도 오디오의 고주파 성분을 예측하여 디지털 애플리케이션에서 오디오 품질을 향상시키는 기본적인 작업입니다. 이전 방법에는 오디오 유형(예: 음악, 음성)의 범위가 제한적이고 처리할 수 있는 특정 대역폭 설정(예: 4kHz~8kHz)과 같은 한계가 있었습니다. 이 논문에서는 사운드 효과, 음악, 음성을 포함한 다양한 오디오 유형에 대해 강력한 오디오 초고해상도를 수행할 수 있는 확산 기반 생성 모델인 AudioSR을 소개합니다. 구체적으로 AudioSR은 2kHz~16kHz 대역폭 범위 내의 모든 입력 오디오 신호를 샘플링 속도가 48kHz인 24kHz 대역폭의 고해상도 오디오 신호로 업샘플링할 수 있습니다. 다양한 오디오 초고해상도 벤치마크에 대한 광범위한 객관적 평가는 제안된 모델이 달성한 강력한 결과를 보여줍니다. 또한 주관적인 평가에 따르면 AudioSR은 AudioLDM, Fastspeech2, MusicGen을 포함한 광범위한 오디오 생성 모델의 생성 품질을 향상시키는 플러그 앤 플레이 모듈 역할을 할 수 있습니다. 저희 코드와 데모는 https://audioldm.github.io/audiosr에서 볼 수 있습니다. 인덱스 용어 오디오 초고해상도, 확산 모델 1.
--- METHOD ---
s에는 제한된 오디오 유형(예: 음악, 음성)의 범위와 처리할 수 있는 특정 대역폭 설정(예: 4kHz~8kHz)과 같은 제한 사항이 있습니다. 이 논문에서는 사운드 효과, 음악, 음성을 포함한 다양한 오디오 유형에 대해 강력한 오디오 초고해상도를 수행할 수 있는 확산 기반 생성 모델인 AudioSR을 소개합니다. 구체적으로 AudioSR은 2kHz~16kHz의 대역폭 범위 내의 모든 입력 오디오 신호를 샘플링 속도가 48kHz인 24kHz 대역폭의 고해상도 오디오 신호로 업샘플링할 수 있습니다. 다양한 오디오 초고해상도 벤치마크에 대한 광범위한 객관적 평가는 제안된 모델이 달성한 강력한 결과를 보여줍니다. 또한 주관적인 평가에 따르면 AudioSR은 AudioLDM, Fastspeech2, MusicGen을 포함한 광범위한 오디오 생성 모델의 생성 품질을 향상시키는 플러그 앤 플레이 모듈 역할을 할 수 있습니다. 코드와 데모는 https://audioldm.github.io/audiosr에서 제공됩니다. 색인 용어 오디오 초고해상도, 확산 모델 1. 서론 오디오 초고해상도(SR)는 저해상도 오디오 신호의 고주파 정보를 추정하여 확장된 주파수 범위의 고해상도 오디오 신호를 생성하는 것을 목표로 합니다. 고해상도 오디오 신호는 일반적으로 더 나은 청취 경험을 제공하며, 이를 고충실도라고 합니다. 오디오 신호 품질을 향상시키는 기능으로 인해 오디오 초고해상도는 과거 녹음 복원과 같은 다양한 응용 분야에서 중요한 역할을 합니다[1]. 오디오 SR에 대한 이전 연구는 주로 특정 도메인에 초점을 맞추었으며, 특히 음성 SR에 중점을 두었습니다. 초기 연구에서는 음성 SR 작업을 스펙트럼 엔벨로프 추정 및 여기 생성으로 분해했습니다[2]. AECNN[3], NuWave[4], NVSR[5]과 같은 딥 러닝 기술을 사용한 최근 연구는 기존 방법에 비해 우수한 성능을 보였습니다. 청취자 선호도(%) 외에도 AudioSR 없음 명확한 차이 없음 AudioSRAudioLDM 있음 MusicGen FastSpeechFig. 1. 주관적 평가에 따르면 오디오 생성 모델의 출력에 오디오 초고해상도를 위해 AudioSR을 적용하면 지각적 품질을 크게 향상시킬 수 있습니다. 음성의 경우 일반 음악[6]과 특정 악기[7]에 대한 연구를 포함하여 음악 SR을 다루려는 노력이 있었습니다. 오디오의 제한된 범위 외에도 오디오 SR에 대한 기존 연구는 주로 통제된 환경에서 수행되었습니다.
--- EXPERIMENT ---
영어: al 설정으로 실제 시나리오에서의 적용성이 제한됩니다. [5]에서 강조된 것처럼 오디오 초고해상도의 중요한 과제는 대역폭 불일치 문제입니다. 이는 테스트 데이터의 대역폭이 훈련 데이터의 대역폭과 달라서 모델이 실패할 때 발생합니다. 그러나 이 문제는 이전 연구에서 일반적으로 훈련 및 테스트 데이터 모두에 대해 일관된 대역폭 설정을 가정하기 때문에 문헌에서 크게 주목받지 못했습니다. 실제로 테스트 오디오의 입력 대역폭은 녹음 장치, 사운드 특성 또는 적용된 압축 프로세스의 제한과 같은 요인으로 인해 달라질 수 있습니다. NVSR [5] 및 NuWave2 [8]를 포함하여 유연한 입력 대역폭을 탐구한 연구는 소수에 불과합니다. 그러나 이러한 방법은 여전히 더 넓은 영역으로 일반화하지 않고 주로 음성 SR에 초점을 맞춥니다. 이 논문에서는 제한된 오디오 유형과 제어된 샘플링 속도 설정에 대한 이전 연구의 한계를 해결하는 새로운 방법을 제안합니다. 음악, 음성 및 음향 효과와 같은 모든 가청 소리를 포함하여 오디오 SR을 일반 영역으로 확장하는 AudioSR이라는 방법을 소개합니다. 또한 AudioSR은 4kHz와 32kHz 사이의 유연한 입력 샘플링 속도를 처리할 수 있어 실제 시나리오에서 대부분의 사용 사례를 포괄합니다. 신경 보코더가 학습한 사전 지식은 오디오 SR 작업에서 더 높은 주파수 성분을 재구성하는 데 도움이 되는 것으로 밝혀졌습니다[5]. 따라서 AudioSR은 [5]에 따라 멜 스펙트로그램에서 오디오 SR을 수행하고 신경 보코더를 사용하여 오디오 신호를 합성합니다. 고해상도 멜 스펙트로그램을 추정하기 위해 AudioLDM[9]에 따라 저해상도 멜 스펙트로그램에서 고해상도 멜 스펙트로그램의 조건부 생성을 학습하는 잠재 확산 모델을 학습합니다. 실험 결과 AudioSR이 다양한 입력 샘플링 속도 설정으로 음성, 음악 및 음향 효과에 대해 유망한 SR 결과를 달성했음을 보여줍니다. 영어: 텍스트-오디오 모델 AudioLDM [9], 텍스트-음악 모델 MusicGen [10], 텍스트-음성 모델 Fastspeech2 [11]의 출력을 향상시키는 것에 대한 주관적인 평가는 AudioSR이 대부분의 오디오 생성 모델에서 청취 품질을 향상시키는 플러그 앤 플레이 모듈이 될 수 있음을 보여줍니다.저희의 기여는 다음과 같이 요약됩니다.• 제안한 AudioSR은 음악, 음성, 음향 효과와 같은 다양한 유형의 오디오를 포괄하는 일반 가청 오디오 도메인에서 오디오 SR을 달성한 최초의 시스템입니다.• AudioSR은 2kHz~16kHz 범위의 유연한 오디오 대역폭을 처리할 수 있으며 48kHz 샘플링 속도로 24kHz 대역폭까지 확장할 수 있습니다.• 오디오 SR 벤치마크에서 유망한 결과 외에도 AudioSR은 AudioLDM, MusicGen, FastSpeech2와 같은 다양한 오디오 생성 모델의 오디오 품질을 향상시키는 플러그 앤 플레이 모듈임이 검증되었습니다.이 논문은 다음과 같이 구성됩니다.섹션 2에서는 오디오 초고해상도 작업의 일반적인 공식을 제공합니다. 섹션 3에서는 AudioSR의 설계에 대한 자세한 설명을 제공합니다. 자세한 실험 설정은 섹션 4에서 논의합니다. 실험 결과는 섹션 5에 제시되고 섹션 6에서 논문을 마무리합니다. 2. 문제 공식화 초당 샘플 속도로 이산적으로 샘플링되어 저해상도 값 시퀀스 x₁ = [xi] 1,2,T1이 생성된 아날로그 신호가 주어지면 오디오 초고해상도(SR)의 목표는 초당 h 샘플 속도로 샘플링된 더 높은 해상도 신호 yh = [i]i=1,2,...Th를 추정하는 것입니다. 여기서 h1과 T는 초 단위의 총 지속 시간입니다. 나이퀴스트 이론에 따르면 xɩ와 yɩ는 각각 1/2Hz와 h/2Hz의 최대 주파수 대역폭을 갖습니다. 따라서 h/2 - 1/2Hz의 주파수 사이에 포함된 정보는 xɩ에서 누락되고 이 &quot;누락된&quot; 주파수 데이터를 추정하는 것이 SR 작업의 핵심 목표입니다. 이 논문에서 우리는 NVSR [5]에서 제안된 방법을 따라 원래 오디오 SR 작업을 두 단계로 분해합니다.여기에는 (i) 고해상도 멜 스펙트로그램 추정, (ii) 신경 보코더를 사용한 멜 스펙트로그램에서 파형 재구성이 포함됩니다.특히, 먼저 3차 보간을 사용하여 x1을 x로 리샘플링합니다.여기서 x는 더 높은 샘플링 속도 h를 갖지만 1/2Hz의 제한된 최대 대역폭을 갖습니다.우리는 [5]의 단계를 따라 xh와 yh의 멜 스펙트로그램을 계산하여 각각 Xmxn과 Ymxn을 얻습니다.여기서 m은 20000입니다.STFT 스펙트로그램저해상도 오디오 입력 xh STET 스펙트로그램 대체(더 높은 샘플링 속도로 리샘플링됨) 시간 Xh 대체 저해상도 멜 스펙트로그램 입력 고해상도 오디오 추정 신경 보코더 Vo Yh 고해상도 멜 스펙트로그램 추정 Ge 잠복 확산 모델 그림 2. AudioSR 아키텍처. 대체 기반 사후 처리의 목적은 모델 출력에서 원래 저주파 정보를 보존하는 것입니다. : 시간 프레임 수이고 n은 멜 주파수 빈의 수입니다. 그런 다음 생성 모델을 사용하여 X에 기반하여 Y를 추정하는 프로세스를 학습합니다. 이를 Go XŶ로 표시합니다. 여기서 0은 모델 G의 매개변수입니다. 마지막으로 신경 보코더를 사용하여 Y의 추정을 기반으로 높은 샘플링 레이트 오디오 신호를 재구성합니다. 이는 Vo : Ŷ → ŷh로 공식화할 수 있습니다. 여기서 V는 신경 보코더이고 는 학습 가능한 매개변수입니다. 3. 방법 제안된 AudioSR의 아키텍처는 그림 2에 나와 있습니다. 저해상도 오디오 x₁를 xh로 리샘플링한 후 시스템은 먼저 STFT 스펙트로그램과 xh의 멜 스펙트로그램을 모두 계산합니다. x에 고주파 정보가 없기 때문에 X의 고주파 빈은 비어 있습니다. X는 사전 훈련된 잠재 확산 모델을 안내하여 고해상도 멜 스펙트로그램 Ŷ를 추정하는 컨디셔닝 신호로 사용됩니다. X와 Ŷh 사이의 저주파 정보의 일관성을 보장하기 위해 Ŷ의 저주파 부분을 X의 저주파 부분으로 대체합니다. 저주파 대체 후의 멜 스펙트로그램은 신경 보코더의 입력으로 사용되며, 이 출력은 유사한 기술을 적용하여 저주파 정보를 입력 저해상도 오디오의 정보로 대체합니다. 3.1절에서 잠재 확산 모델과 신경 보코더의 학습을 소개합니다. 후처리 알고리즘은 3.2절에 자세히 설명되어 있습니다. 3.1. 고해상도 파형 추정 잠재 확산 모델(LDM)은 이미지 합성[12] 및 오디오 생성[9]을 포함한 다양한 도메인에서 유망한 결과를 보여주었습니다. 이 연구에서는 LDM을 사용하여 고해상도 멜 스펙트로그램을 추정합니다. LDM의 훈련은 사전 훈련된 변분 자동 인코더(VAE) F(.)가 학습한 잠재 공간 내에서 수행됩니다. VAE는 F : X ⇒ 20 → X로 표시되는 중앙에 작은 압축 잠재 공간을 사용하여 자동 인코딩을 수행하도록 훈련됩니다. 저차원 표현 zo를 활용함으로써 LDM은 X 대신 zo의 생성을 학습할 수 있어 계산 비용이 상당히 감소합니다. 재구성 손실, Kullback-Leibler 발산 손실, 판별 손실을 포함하여 VAE 모델을 최적화하기 위해 AudioLDM에서 제안한 방법론을 채택합니다. AudioLDM [9]에서 도입한 공식을 따라 훈련 목표, 노이즈 일정 및 컨디셔닝 메커니즘을 개선하여 LDM을 구현합니다. 확산 모델에서 사용되는 일반적인 노이즈 일정에는 결함이 있는 것으로 밝혀졌습니다[13]. 특히 LDM의 최종 확산 단계에서 노이즈 일정이 가우시안 분포와 일치하지 않기 때문입니다. 이 문제를 해결하기 위해 [13]에 따라 노이즈 일정을 코사인 일정으로 업데이트합니다. 이 조정을 통해 학습 중 최종 확산 단계에서 표준 가우시안 분포를 얻을 수 있습니다. 또한 새로운 노이즈 일정을 사용한 것을 반영하여 속도 예측 목표 [14]를 통합합니다. LDM의 최종 학습 목표는 argming Uk G(Zk, k, Fenc (X1); 0)||2, (1)입니다. 여기서 zk는 확산 단계 k에서 zo의 데이터 Є [1, ..., K]를 나타내고 || · ||2는 유클리드 거리를 나타내고 Fenc는 VAE 인코더를 나타내며 [13]에 설명된 대로 vk는 시간 단계 k에서 G의 예측 목표를 나타내는 zo를 기반으로 계산됩니다. 우리는 [15]에서 제안된 Transformer-UNet 아키텍처를 G로 채택합니다. G에 대한 입력은 zk를 저해상도 멜 스펙트로그램 X₁에서 추출한 VAE 잠재 음성인 Fenc(X1)와 연결하여 얻습니다. 분류기 없는 안내를 통합하기 위해 [9]의 공식에 따라 학습 중에 무작위 비율(예: 10%)로 Fenc(X₁)를 빈 텐서로 바꿉니다. 잠재 확산 모델을 학습한 후 DDIM 샘플러[16]를 사용하여 샘플링을 수행합니다. 신경 보코더. LDM은 고해상도 멜 스펙트로그램을 추정할 수 있습니다. 그러나 멜 스펙트로그램은 직접 들을 수 없기 때문에 HiFiGAN[17]에 기반한 신경 보코더를 사용하여 멜 스펙트로그램을 파형으로 변환합니다. 원래 HiFiGAN을 구현할 때 스펙트럼 누출 문제를 해결하기 위해 HiFiGAN 보코더에 다중 해상도 판별기[18]를 채택했습니다. 섹션 4에서 설명한 대로 다양한 오디오 데이터를 사용하여 보코더를 최적화하여 48kHz 샘플링 속도에서 작동하고 다양한 유형의 오디오에서 작동할 수 있는 보코더를 만들었습니다. 3.2. 사후 처리 및 사전 처리 사후 처리. 입력 저해상도 오디오 기능 Xh 및 xh는 추정 대상 Y₁ 및 y의 저주파 대역과 동일합니다. 결과적으로 X 및 x에서 사용 가능한 정보를 재사용하여 LDM 출력 Ŷ와 신경 보코더 출력 ŷ를 모두 향상시킬 수 있습니다. 이를 달성하기 위해 먼저 X와 yɲ의 STFT 스펙트로그램에 적용된 오픈 소스 방식¹을 기반으로 전체 입력 오디오의 0.99 롤오프 주파수 c를 결정합니다. 그 후, LDM 출력 Ŷ 및 보코더 출력 ŷ에서 차단 주파수 아래의 스펙트로그램 구성 요소를 각각 Xh 및 xh의 해당 정보로 대체합니다. 이 후처리 방법은 최종 출력이 저주파 정보를 크게 변경하지 않도록 보장할 수 있습니다. 전처리. 모델 학습과 평가 간의 불일치를 최소화하기 위해 저역 통과 필터링 작업으로 평가하는 동안 입력 오디오에 대한 전처리를 수행합니다. 후처리에서 동일한 방법을 사용하여 0.99 롤오프 주파수를 계산하고 8차 체비셰프 필터로 저역 통과 필터링을 수행합니다. 4. 실험 학습 데이터 세트. 이 논문에서 사용한 데이터 세트에는 MUSDB18-HQ [19], MoisesDB [20], MedleyDB [21], FreeSound [22]² 및 OpenSLR³의 음성 데이터 세트가 포함되며, 이는 VoiceFixer [1]에서 제공하는 링크를 따라 다운로드할 수 있습니다. 사용된 모든 오디오 데이터는 48kHz 샘플링 속도로 리샘플링됩니다. 학습 데이터의 총 기간은 약 7000시간입니다. 이 모든 데이터 세트를 활용하여 VAE, LDM 및 HiFi-GAN을 최적화합니다. 학습 데이터 시뮬레이션. NVSR [5]에서 도입한 방법을 따라 저해상도-고해상도 오디오 데이터 쌍을 시뮬레이션합니다. 고해상도 오디오 데이터 yh가 주어지면 먼저 2kHz와 16kHz 사이에서 균일하게 샘플링된 차단 주파수로 오디오에 저역 통과 필터링을 수행합니다. 필터 일반화 문제 [3]를 해결하기 위해 저역 통과 필터의 유형은 Chebyshev, Elliptic, Butterworth 및 Boxcar 내에서 무작위로 샘플링되고 저역 통과 필터의 순서는 2와 10 사이에서 무작위로 선택됩니다. 평가 데이터 세트. 주관적 및 객관적 평가를 모두 수행했습니다. 주관적인 평가를 위해 MusicGen(MusicCaps [23]의 캡션), AudiOLDM(AudioCaps [24]의 캡션), Fastspeech2(LJSpeech [25]의 전사)의 출력을 채택하여 AudioSR이 생성의 품질을 향상시킬 수 있는지 연구했습니다. MusicGen의 경우 오디오 태그 4를 사용하여 비음악적 생성 출력을 필터링했습니다. 마지막으로 MusicGen에서 50개 샘플, AudioLDM에서 샘플, FastSpeech2에서 20개 샘플을 수집하여 청취자 선호도에 대한 주관적 평가를 위해 AudioSR로 처리했습니다. 또한 ESC50(사운드 효과) [26], AudioStock(음악)5, VCTK(음성) [5]를 포함하여 객관적인 평가를 위한 세 가지 벤치마크를 큐레이션했습니다. AudioStock 데이터 세트는 10가지 장르의 고품질 음악 100개를 직접 선택하여 구축했습니다. 우리는 ESC에서 fold-5만 사용합니다.https://librosa.org/doc/main/generated/librosa. feature.spectral_rolloff.html 2 https://labs.freesound.org/ 3 https://openslr.org/ 4 https://github.com/kkoutini/PASST Shttps://audiostock.net/ 객관적 평가 주관적 평가 VCTK(음성) AudioStock(음악) ESC-50(음향 효과) ESC-50(4kHz 차단 주파수) 차단 주파수 4kHz 8kHz 12kHz GT-Mel 0.0.0.Unprocessed 5.4.3. 차단 주파수 4kHz 8kHz GT-Mel 16kHz 4kHz 8kHz 16kHz 시스템 전체 품질 0.0.0.0.0.0.GT-Mel 4.NuWave [4] 1.1.1.Unprocessed 4.3.1.3.3.2.Unprocessed 3.NVSR [5] 0.0.0.NVSR-DNN 1.1.1.1.1.1.NVSR-DNN 2.AudioSR 1.1.0.1.0.0.NVSR-ResUNet AudioSR 1.70 1.0.99 0.0.0.1.80 1.1.74 1.1.1.NVSR-ResUNet AudioSR 3.4.AudioSR-Speech 표 1. 입력 오디오에서 다양한 차단 주파수를 사용한 음성, 음악 및 음향 효과 데이터의 48kHz 오디오 SR에 대한 객관적 및 주관적 평가 결과. 평가에 사용된 객관적 지표는 LSD로, 값이 낮을수록 성능이 우수함을 나타냅니다. 주관적 지표는 전반적인 청취 품질을 측정하며, 값이 높을수록 성능이 우수함을 나타냅니다. 처리되지 않은 NVSR-DNN NVSR-ResUNet AudioSR Ground Truth 그림 3. 다양한 시스템 비교. AudioSR은 기준 NVSR 모델보다 상당히 우수한 성능을 보입니다. 데이터 세트를 평가 세트로 사용합니다.평가 지표 객관적인 평가를 위해 이전 연구[3,5]에서 사용된 LSD 지표를 채택합니다.[15]의 설정에 따라 Amazon Mturk6에서 두 가지 유형의 주관적 평가를 수행합니다.전체 품질 평가 및 선호도 비교.전체 품질 평가에서 평가자는 오디오 품질을 반영하여 1~5 사이의 점수를 할당합니다.선호도 비교에서 평가자는 두 오디오 파일을 비교하여 더 나은 소리가 나는 것을 선택합니다.5. 결과 평가를 위해 두 가지 버전의 AudioSR을 훈련했습니다.임의의 오디오 유형과 입력 샘플링 속도에서 작동하는 기본 AudioSR과 AudioSR-Speech라는 음성 데이터 미세 조정 변형입니다.비교를 위한 주요 기준선은 오디오 SR 작업을 위해 유사한 멜 스펙트로그램 및 보코더 기반 파이프라인을 사용하는 NVSR[5]입니다. AudioSR과 NVSR의 주요 차이점은 mel-spectrogram 추정 방식에 있습니다.AudioSR은 잠재 확산 모델을 활용하는 반면 NVSR은 다층 퍼셉트론(NVSR-DNN) 또는 잔여 UNet(NVSR-ResUNet)을 사용합니다.음성 SR의 경우 오디오 SR에 확산 모델을 사용하는 기준 모델인 NuWave[4]와도 비교합니다.표 1에서 볼 수 있듯이 AudioSR은 객관적 및 주관적 평가에서 모두 유망한 결과를 얻었습니다.음악 SR의 경우 AudioSR은 모든 차단 주파수 설정에서 최첨단 성능을 달성하여 기준 NVSR 모델보다 큰 차이로 성능이 우수합니다.음성 SR의 경우 AudioSR-Speech가 24kHz~48kHz 업샘플링 작업에서 최고의 성능을 달성합니다.또한 AudioSR과 AudioSR-Speech를 비교하면 작은 데이터 영역에서 미세 조정을 통해 LSD를 크게 개선할 수 있음을 알 수 있습니다. LSD 지표는 항상 지각적 품질과 일치하지 않습니다. ESC-50 데이터 세트에서 8kHz(즉, 4kHz 차단 주파수)에서 48kHz 업샘플링 작업에서 NVSR-DNN이 1.64의 LSD 점수로 가장 좋은 성능을 달성한 것을 관찰했습니다. 그러나 주관적인 평가에 따르면 https://www.mturk.com/ NVSR-DNN의 지각적 품질은 2.84의 점수로 최악이었으며 AudioSR의 점수 4.01보다 상당히 낮았습니다. 이러한 결과는 LSD가 음향 효과 데이터에 대한 오디오 SR 작업에 적합한 평가 지표가 아닐 수 있음을 시사하며 향후 연구에서 추가 조사가 필요합니다. 그림 1에서 볼 수 있듯이 주관적인 선호도 테스트는 AudioSR을 활용하면 AudioLDM, MusicGen 및 FastSpeech2 출력의 지각적 품질이 크게 향상됨을 보여줍니다. MusicGen의 출력은 이미 32kHz의 높은 샘플링 속도에 있으며, 이는 &quot;No Clear Difference&quot; 응답의 비교적 높은 속도에 기여할 수 있다는 점에 주목할 가치가 있습니다. 그러나 MusicGen은 AudioSR을 적용한 후에도 여전히 상당히 향상된 지각적 품질을 보여줍니다. 6.
--- CONCLUSION ---
이 논문에서는 다양한 오디오 유형과 임의의 샘플링 속도 설정으로 작동할 수 있는 48kHz 오디오 초고해상도 모델인 AudioSR을 제시합니다. 여러 오디오 초고해상도 벤치마크를 평가하여 AudioSR은 다양한 유형의 오디오와 샘플링 속도에서 우수하고 견고한 성능을 보여줍니다. 또한, 주관적인 평가에서는 AudioLDM, MusicGen, Fastspeech2를 포함한 오디오 생성 모델에 대한 플러그 앤 플레이 품질 개선을 가능하게 하는 AudioSR의 효과를 강조합니다. 향후 작업에는 실시간 애플리케이션을 위한 AudioSR 확장과 일반 오디오 도메인에서 오디오 초고해상도에 적합한 평가 프로토콜 탐색이 포함됩니다. 7. 감사의 말 이 연구는 부분적으로 British Broadcasting Corporation Research and Development, Engineering and Physical Sciences Research Council(EPSRC) Grant EP/T019751/1 &quot;AI for Sound&quot; 및 University of Surrey의 공학 및 물리 과학부(FEPS)의 시각, 음성 및 신호 처리 센터(CVSSP)의 박사 장학금의 지원을 받았습니다. 오픈 액세스의 목적으로 저자는 발생하는 모든 저자 승인 원고 버전에 Creative Commons Attribution(CC BY) 라이선스를 적용했습니다. 8. 참고문헌 [1] H. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, &quot;VoiceFixer: Toward general speech restoration with neural vocoder,&quot; arXiv preprint:2109.13731, 2021. [2] J. Kontio, L. Laaksonen, and P. Alku, 영어: “음성의 신경망 기반 인공 대역폭 확장,&quot; 오디오, 음성 및 언어 처리 논문, 제15권, 제3호, 873-881쪽, 2007년. [3] H. Wang 및 D. Wang, “강력한 음성 초해상도를 향하여,&quot; 오디오, 음성 및 언어 처리 논문, 제15권, 제3호, 873-881쪽, 2007년. 29, pp. 2058-2066, 2021. [4] J. Lee 및 S. Han, &quot;NuWave: 신경 오디오 업샘플링을 위한 확산 확률적 모델,&quot; arXiv 사전 인쇄:2104.02321, 2021. [5] H. Liu, W. Choi, X. Liu, Q. Kong, Q. Tian 및 D. Wang, &quot;신경 보코더만 있으면 음성 초고해상도에 충분합니다,&quot; INTERSPEECH, pp. 4227-4231, 2022. [6] S. Hu, B. Zhang, B. Liang, E. Zhao 및 S. Lui, &quot;생성적 적대 네트워크를 사용한 위상 인식 음악 초고해상도,&quot; INTERSPEECH, pp. 4074–4078, 2020. [7] NC Rakotonirina, 신호 처리를 위한 기계 학습에 관한 국제 워크숍에서 &quot;오디오 초고해상도를 위한 자기 주의&quot;. IEEE, 2021. [8] S. Han 및 J. Lee, &quot;NUWave 2: 다양한 샘플링 속도에 대한 일반 신경 오디오 업샘플링 모델,&quot; arXiv 사전 인쇄: 2206.08545, 2022. [9] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang 및 MD Plumbley, &quot;AudioLDM: 잠재 확산 모델을 사용한 텍스트-오디오 생성,&quot; 기계 학습 국제 컨퍼런스, 2023. [10] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi 및 A. Défossez, &quot;간단하고 제어 가능한 음악 생성,&quot; arXiv 사전 인쇄: 2306.05284, 2023. [11] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao 및 T. Liu, &quot;Fastspeech 2: 빠르고 고품질의 엔드투엔드 텍스트-음성 변환&quot;, International Conference on Learning Representations, 2021. [12] R. Rombach, A. Blattmann, D. Lorenz, P. Esser 및 B. Ommer, &quot;잠재 확산 모델을 사용한 고해상도 이미지 합성&quot;, Conference on Computer Vision and Pattern Recognition, 2022, pp. 10684–10695. [13] S. Lin, B. Liu, J. Li, 및 X. Yang, &quot;일반적인 확산 노이즈 일정 및 샘플 단계에는 결함이 있습니다.&quot; arXiv 사전 인쇄: 2305.08891, 2023. [14] T. Salimans 및 J. Ho, &quot;확산 모델의 빠른 샘플링을 위한 점진적 증류&quot;, International Conference on Learning Representations, 2022. [15] H. Liu, Q. Tian, Y. Yuan, X. Liu, X. Mei, Q. Kong, Y. Wang, W. Wang, Y. Wang, 및 MD Plumbley, &quot;AudioLDM 2: 자체 감독 사전 학습을 통한 전체적인 오디오 생성 학습&quot;, arXiv 사전 인쇄 arXiv: 2308.05734, 2023. [16] J. Song, C. Meng, 및 S. Ermon, &quot;확산 암시적 모델의 노이즈 제거&quot;, International Conference on Learning Representations, 2020. [17] J. Kong, J. Kim, and J. Bae, &quot;HiFi-GAN: 효율적이고 고충실도의 음성 합성을 위한 생성적 적대적 네트워크,&quot; 신경 정보 처리 시스템의 발전, vol. 33, pp. 17022-17033, 2020. [18] J. You, D. Kim, G. Nam, G. Hwang, and G. Chae, &quot;GAN Vocoder: 다중 해상도 판별기만 있으면 됩니다,&quot; arXiv 사전 인쇄:2103.05236, 2021. [19] Z. Rafii, A. Liutkus, F.-R. Stöter, SI Mimilakis 및 R. Bittner, &quot;MUSDB18-HQ - MUSDB18의 압축되지 않은 버전,&quot; 2019년 8월. [20] I. Pereira, F. Araújo, F. Korzeniowski 및 R. Vogl, &quot;MoisesDB: 4개 스템을 넘어서는 소스 분리를 위한 데이터 세트,&quot; arXiv 사전 인쇄:2307.15913, 2023. [21] RM Bittner, J. Salamon, M. Tierney, M. Mauch, C. Cannam 및 JP Bello, &quot;MedleyDB: 주석 집약적 mir 연구를 위한 다중 트랙 데이터 세트.&quot; ISMIR, 제14권, 2014년, 155-160쪽. [22] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, MD Plumbley, Y. Zou, W. Wang, &quot;WavCaps: 오디오-언어 멀티모달 연구를 위한 ChatGPT 지원 약하게 레이블이 지정된 오디오 캡션 데이터 세트,&quot; arXiv 사전 인쇄: 2303.17395, 2023. [23] A. Agostinelli, TI Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., &quot;MusicLM: 텍스트에서 음악 생성,&quot; arXiv 사전 인쇄: 2301.11325, 2023. [24] CD Kim, B. Kim, H. Lee, G. Kim, &quot;AudioCaps: 오디오에서 캡션 생성 야생,&quot; NAACLHLT, 2019, pp. 119–132. [25] K. Ito 및 L. Johnson, &quot;LJSpeech 데이터 세트,&quot; https://keithito.com/LJ-Speech-Dataset/, 2017. [26] KJ Piczak, &quot;ESC: 환경 소음 분류를 위한 데이터 세트,&quot; 국제 멀티미디어 컨퍼런스, 2015, pp. 1015–1018.
