--- ABSTRACT ---
동작 확대는 미묘하고 감지할 수 없는 동작을 시각화하는 데 도움이 됩니다. 그러나 이전 방법은 고정된 카메라로 촬영한 2D 비디오에만 적용됩니다. 우리는 움직이는 카메라로 촬영한 장면에서 미묘한 동작을 확대하고 새로운 뷰 렌더링을 지원하는 3D 동작 확대 방법을 제시합니다. 우리는 시간에 따라 변하는 광도장으로 장면을 표현하고 동작 확대를 위한 오일러 원리를 활용하여 시간에 따른 고정점의 임베딩 변화를 추출하고 증폭합니다. 우리는 기본 3D 장면 표현으로 암묵적 광도장과 3면 기반 광도장을 모두 사용하여 제안된 3D 동작 확대 원리를 연구하고 검증합니다. 우리는 다양한 카메라 설정에서 촬영한 합성 및 실제 장면에서 우리 방법의 효과를 평가합니다. 1.
--- INTRODUCTION ---
우리는 작은 움직임이 가득한 큰 세상에 살고 있습니다. 인간의 호흡이나 물체의 진동과 같은 이러한 움직임은 육안으로는 인지하기 어렵습니다. 비디오 처리 기술[29, 61, 56]은 2D 비디오에서 캡처한 미묘한 움직임을 추출하고 확대하여 해당 움직임을 강조하고 시각화하기 위해 개발되었습니다. 이러한 움직임 확대 기술은 물리적 접촉 없이도 비디오만 사용하여 건물의 진동을 감지하고 사람의 심박수를 측정하는 것과 같은 시각적 분석 도구를 강화합니다[58, 10, 46, 23]. 그러나 우리는 3D 움직임으로 가득 찬 3D 세상에 살고 있습니다. 그림 1에 표시된 것처럼 3D로 움직임을 확대하면 다른 관점에서 이러한 움직임을 인지할 수 있습니다. 또한 3D로 움직임을 모델링하면 자연스러운 분리가 제공됩니다.*Equal contribution32D Method[57] Ours Figure 2. Handheld Video에서의 움직임 확대. 이전 2D 동작 확대 접근 방식(예: [57])은 움직이는 카메라로 촬영한 비디오를 처리할 수 없어 심각한 아티팩트가 발생합니다. 반면, 저희 접근 방식은 자연스럽게 카메라 동작과 객체 동작을 분리하여 관심 대상의 동작만 확대할 수 있습니다. 확대된 출력은 그림 7을 참조하세요. 카메라 동작과 관심 대상의 동작 사이. 이를 통해 그림 2에서와 같이 핸드헬드 비디오의 동작을 확대할 수 있습니다. 반면, 이전 2D 동작 확대 방법은 이러한 시나리오에서 치명적으로 실패합니다. 이 논문에서는 표준 NeRF 백본과 교육 파이프라인을 최소한으로 수정하여 신경 복사장(NeRF)을 사용한 3D 동작 확대 방법을 제안합니다. 2D 비디오용으로 설계된 이전 방법은 종종 오일러 관점을 활용하여 시간에 따른 각 픽셀 위치의 색상 변화를 분석하고 증폭하여 동작을 확대합니다. 반면, 저희는 NeRF의 피처 임베딩에서 작동하는 확대 방법을 설계하여 오일러 분석을 색상 공간을 넘어 새로운 영역으로 가져갑니다. 실험 결과는 각 3D 포인트의 피처 임베딩에서 시간적 변화를 증폭하는 것이 미묘한 3D 동작을 확대하는 데 매우 효과적임을 보여줍니다. 포인트 임베딩을 확대하면 렌더링된 이미지에서 직접 수행한 오일러 확대보다 더 정확하고 견고한 확대 렌더링이 제공된다는 것을 관찰했습니다. 미묘한 동작만 보이는 시간 창 동안 캡처한 이미지를 사용하여 NeRF가 이러한 미묘한 시간적 변화가 있는 3D 장면을 재구성하도록 훈련합니다. 시간이 지남에 따라 변경되는 유일한 요소가 포인트 임베딩 함수인 반면 NeRF의 MLP 레이어는 시간이 지남에 따라 일정하게 유지되도록 합니다. 선형 오일러 접근 방식[61]은 데이터 차원과 무관하고 NeRF의 포인트 임베딩으로 확장 가능하지만 선형 접근 방식보다 우수한 속성을 보여준 위상 기반 오일러 접근 방식[56]의 경우 각 2D 이미지 프레임 위에 복잡한 조종 가능 피라미드를 구체적으로 구성하기 때문에 NeRF에 어떻게 적용할 수 있는지 불분명합니다. NeRF의 임베딩 함수에 대한 최근 도입된 3면 표현은 위상 기반 접근 방식[7]과 같은 2D 특정 확대 방법을 자연스럽게 허용합니다. 분석적 위치 인코딩을 사용하여 포인트 임베딩을 생성하는 대신, 관찰된 각 타임스텝에서 하나의 피처 3면 을 학습합니다. 이러한 3면은 2D 비디오 기반 확대 방법을 위한 피처 비디오로 자연스럽게 구성될 수 있습니다. 마지막으로, 이러한 동작 확대 피처 3면을 포인트 임베딩 함수로 사용하여 동작 확대 3D 장면을 렌더링합니다. NeRF를 사용한 3D 확대의 성능을 평가하기 위해 먼저 미묘한 동작이 있는 장면의 합성 데이터 세트를 만들고 합성적으로 확대된 기준 진실 비디오에 대한 확대 품질을 측정합니다. 3면 피처에서 작동하는 위상 기반 접근 방식은 실험에서 고려한 다른 대안적 접근 방식에 비해 최상의 성능을 제공합니다. 제안된 방법의 실용성을 더욱 검증하기 위해 파이프라인을 사용하여 다양한 카메라 설정, 장면 구성 및 피사체 동작으로 촬영한 여러 실제 장면을 처리합니다. 우리의 결과는 3D 동작 확대를 위한 우리의 제안된 접근 방식이 이미지 노이즈와 카메라 포즈가 있는 실제 캡처에 대해 견고한 성능을 달성한다는 것을 보여줍니다. 요약하자면, 우리의 기여는 다음과 같습니다. • 우리는 3D 동작 확대 문제를 소개합니다. 우리는 표준 NeRF 백본과 훈련 파이프라인을 사용하여 3D 동작 확대에 오일러 동작 분석을 적용하는 것의 타당성을 보여줍니다. • 우리는 오일러 분석을 색 공간을 넘어 새로운 영역으로 확장하여 포인트 임베딩을 수정하고 필터링하는 전략을 탐구하고 이들의 상충 관계를 비교합니다. • 우리는 다른 동작, 장면 구성, 심지어 이전 2D 방법에서는 지원되지 않는 핸드헬드 비디오가 있는 다양한 실제 장면에서 성공적인 3D 동작 확대 결과를 보여줍니다. 2.
--- RELATED WORK ---
비디오 동작 확대. 비디오 확대에 대한 기존 접근 방식은 유체 역학에서 영감을 받은 라그랑지안[29]과 오일러안[61, 56, 57, 37, 65]의 두 가지 범주로 나뉩니다. 라그랑지안 관점은 개별 픽셀을 유체 입자로 추적하고 해당 동작 벡터를 추정하여 이미지의 픽셀을 워프합니다. 동작 확대에 대한 라그랑지안 기반 접근 방식은 광학 흐름을 명시적으로 계산하고 추정된 흐름을 사용하여 픽셀의 동작을 확대합니다[29]. 그러나 성능은 흐름 추정의 정확도에 의해 제한됩니다. 반면, 오일러안 관점은 고정된 픽셀 위치에서의 변화를 분석하여 각 픽셀/위치에서 시간적 변화를 증폭하여 동작을 확대합니다. 이 접근 방식은 부정확하고 비용이 많이 들 수 있는 명시적 특징 추적 또는 광학 흐름 추정의 필요성을 우회합니다. 오일러안 접근 방식의 두 가지 변형은 선형[61]과 위상 기반[56, 57]입니다. 선형 오일러[61]는 비디오 프레임 위에 라플라시안 피라미드를 구성하고 각 픽셀의 색상 변화를 증폭합니다.(a) 3D 장면 표현 3D 공간의 점 → 좌표 변환 → 점 임베딩 → 투영 함수 → 출력 색상 c 임베딩 함수 E 분석적 \ PosEnc(p) E(p) Є R&quot; 불투명도 σ OMLP TriPlane(p) 학습된 방향 d (b) 시간 단계별로 학습된 미묘한 동작이 있는 3D 장면 고정 투영 함수 d MLPCtoto Ct ,0tEto MLPр Et₁ Etz MLPto t₁ t₂ ~ tt₂ (c) 3D 동작 확대 고정 점 → 시간 가변 임베딩 → 시간 필터링 → 변화 증폭 확대 렌더링 р tn 푸리에 변환 대역 통과 필터 nd MLP Ꮎ MLPCtoto Ct₁,σtMLPCt₂Ot₂ 그림 3.
--- METHOD ---
영어: s는 고정된 카메라로 촬영한 2D 비디오에만 작동합니다. 우리는 움직이는 카메라로 촬영한 장면에서 미묘한 동작을 확대하고 새로운 뷰 렌더링을 지원할 수 있는 3D 동작 확대 방법을 제시합니다. 우리는 시간에 따라 변하는 광도장으로 장면을 표현하고 동작 확대를 위한 오일러 원리를 활용하여 시간에 따른 고정점의 임베딩 변화를 추출하고 증폭합니다. 우리는 기본 3D 장면 표현으로 암시적 광도장과 3면 기반 광도장을 모두 사용하여 제안된 3D 동작 확대 원리를 연구하고 검증합니다. 다양한 카메라 설정에서 촬영한 합성 및 실제 장면에서 우리 방법의 효과를 평가합니다. 1. 서론 우리는 작은 동작의 큰 세상에 살고 있습니다. 인간의 호흡이나 물체의 진동과 같은 이러한 동작은 육안으로는 인식하기 어렵습니다. 비디오 처리 기술[29, 61, 56]은 2D 비디오에서 촬영한 미묘한 동작을 추출하고 확대하여 해당 동작을 강조하고 시각화하기 위해 개발되었습니다. 이러한 동작 확대 기술은 건물의 진동을 감지하고 물리적 접촉 없이 비디오만을 사용하여 사람의 심박수를 측정하는 것과 같은 시각적 분석 도구를 강화합니다[58, 10, 46, 23]. 그러나 우리는 3D 동작으로 가득 찬 3D 세계에 살고 있습니다. 그림 1에 표시된 것처럼 3D로 동작을 확대하면 다른 관점에서 이러한 동작을 인식할 수 있습니다. 더욱이 3D로 동작을 모델링하면 *Equal contribution32D 방법[57] 우리의 그림 2. 핸드헬드 비디오의 동작 확대. 이전의 2D 동작 확대 방식(예: [57])은 움직이는 카메라로 촬영한 비디오를 처리할 수 없어 심각한 아티팩트가 발생합니다. 이와 대조적으로 우리의 방식은 카메라 동작과 객체 동작을 자연스럽게 분리하여 관심 대상의 동작만 확대할 수 있습니다. 확대된 출력은 그림 7을 참조하세요. 카메라 동작과 관심 대상의 동작 사이. 이를 통해 그림 2에 표시된 것처럼 핸드헬드 비디오의 동작을 확대할 수 있습니다. 이와 대조적으로, 이전의 2D 동작 확대 방법은 이러한 시나리오에서 치명적으로 실패합니다. 이 논문에서는 표준 NeRF 백본과 훈련 파이프라인을 최소한으로 수정하여 NeRF(신경 복사장)를 사용하여 3D 동작 확대 방법을 제안합니다. 2D 비디오용으로 설계된 이전 방법은 종종 오일러 관점을 활용하여 시간이 지남에 따라 각 픽셀 위치에서 색상 변화를 분석하고 증폭하여 동작을 확대합니다. 이와 대조적으로, 우리는 NeRF의 피처 임베딩에서 작동하는 확대 방법을 설계하여 오일러 분석을 색상 공간을 넘어 새로운 영역으로 가져옵니다. 우리의
--- EXPERIMENT ---
모든 결과는 각 3D 포인트의 피처 임베딩에서 시간적 변화를 증폭하는 것이 미묘한 3D 동작을 확대하는 데 매우 효과적임을 보여줍니다. 우리는 포인트 임베딩을 확대하면 렌더링된 이미지에서 직접 수행한 오일러 확대보다 더 정확하고 견고한 확대 렌더링을 제공한다는 것을 관찰했습니다. 미묘한 동작만 보이는 시간 창 동안 캡처한 이미지를 사용하여 NeRF가 그러한 미묘한 시간적 변화가 있는 3D 장면을 재구성하도록 훈련합니다. 우리는 시간이 지남에 따라 변경되는 유일한 요소가 포인트 임베딩 함수이고 NeRF의 MLP 계층은 시간이 지남에 따라 일정하게 유지되도록 합니다. 선형 오일러 접근 방식[61]은 데이터 차원과 무관하고 NeRF의 포인트 임베딩으로 확장 가능하지만 선형 접근 방식보다 우수한 속성을 보여준 위상 기반 오일러 접근 방식[56]의 경우 각 2D 이미지 프레임 위에 복잡한 조종 가능 피라미드를 구체적으로 구성하기 때문에 NeRF에 어떻게 적용할 수 있는지 불분명합니다. NeRF의 임베딩 함수에 대한 최근 도입된 3면 표현은 위상 기반 접근 방식[7]과 같은 2D 특정 확대 방법을 자연스럽게 허용합니다. 분석적 위치 인코딩을 사용하여 포인트 임베딩을 생성하는 대신, 관찰된 각 타임스텝에서 하나의 피처 3면 을 학습합니다. 이러한 3면은 2D 비디오 기반 확대 방법을 위한 피처 비디오로 자연스럽게 구성될 수 있습니다. 마지막으로, 이러한 동작 확대 피처 3면을 포인트 임베딩 함수로 사용하여 동작 확대 3D 장면을 렌더링합니다. NeRF를 사용한 3D 확대의 성능을 평가하기 위해 먼저 미묘한 동작이 있는 장면의 합성 데이터 세트를 만들고 합성적으로 확대된 기준 진실 비디오에 대한 확대 품질을 측정합니다. 3면 피처에서 작동하는 위상 기반 접근 방식은 실험에서 고려한 다른 대안적 접근 방식에 비해 최상의 성능을 제공합니다. 제안된 방법의 실용성을 더욱 검증하기 위해 파이프라인을 사용하여 다양한 카메라 설정, 장면 구성 및 피사체 동작으로 촬영한 여러 실제 장면을 처리합니다. 우리의 결과는 우리가 제안한 3D 동작 확대 접근 방식이 이미지 노이즈와 카메라 포즈가 있는 실제 캡처에 대해 견고한 성능을 달성한다는 것을 보여줍니다.요약하자면, 우리의 기여는 다음과 같습니다.• 우리는 3D 동작 확대 문제를 소개합니다.우리는 표준 NeRF 백본과 훈련 파이프라인을 사용하여 3D 동작 확대에 오일러 동작 분석을 적용하는 것의 타당성을 보여줍니다.• 우리는 오일러 분석을 색 공간을 넘어 새로운 영역으로 확장하여 포인트 임베딩을 수정하고 필터링하는 전략을 탐구하고 이들의 상충 관계를 비교합니다.• 우리는 다른 동작, 장면 구성, 심지어 이전 2D 방법에서는 지원되지 않는 핸드헬드 비디오가 있는 다양한 실제 장면에서 성공적인 3D 동작 확대 결과를 보여줍니다.2. 관련 연구 비디오 동작 확대.비디오 확대에 대한 이전의 접근 방식은 유체 역학에서 영감을 받은 라그랑지안[29]과 오일러안[61, 56, 57, 37, 65]의 두 가지 범주로 나뉩니다. 라그랑주 관점은 개별 픽셀을 유체 입자로 추적하고 해당 픽셀의 동작 벡터를 추정하여 이미지의 픽셀을 워프합니다. 동작 확대에 대한 라그랑주 기반 접근 방식은 광학 흐름을 명시적으로 계산하고 추정된 흐름을 사용하여 픽셀의 동작을 확대합니다[29]. 그러나 성능은 흐름 추정의 정확도에 의해 제한됩니다. 반면, 오일러 관점은 고정된 픽셀 위치에서의 변화를 분석하여 각 픽셀/위치의 시간적 변화를 증폭하여 동작을 확대합니다. 이 접근 방식은 부정확하고 비용이 많이 들 수 있는 명시적 특징 추적 또는 광학 흐름 추정의 필요성을 우회합니다. 오일러 접근 방식의 두 가지 변형은 선형[61] 및 위상 기반[56, 57]입니다. 선형 오일러[61]는 비디오 프레임 위에 라플라시안 피라미드를 구성하고 각 픽셀의 색상 변화를 증폭합니다.(a) 3D 장면 표현 3D 공간의 점 → 좌표 변환 → 점 임베딩 → 투영 함수 → 출력 색상 c 임베딩 함수 E 분석적 \ PosEnc(p) E(p) Є R&quot; 불투명도 σ OMLP TriPlane(p) 학습된 방향 d (b) 시간 단계별로 학습된 미묘한 동작이 있는 3D 장면 고정 투영 함수 d MLPCtoto Ct ,0tEto MLPр Et₁ Etz MLPto t₁ t₂ ~ tt₂ (c) 3D 동작 확대 고정 점 → 시간 가변 임베딩 → 시간 필터링 → 변화 증폭 확대 렌더링 р tn 푸리에 변환 대역 통과 필터 nd MLP Ꮎ MLPCtoto Ct₁,σtMLPCt₂Ot₂ 그림 3. 방법 개요. (a) NeRF를 사용한 3D 장면 표현은 두 가지 주요 구성 요소로 구성됩니다. 1) 좌표 변환은 임베딩 함수 E를 사용하여 입력 지점 p Є R³를 고차원 임베딩 벡터 E(p) Є R&quot;로 매핑합니다. 임베딩 함수는 분석적(위치 인코딩)이거나 학습된(삼면) 함수일 수 있습니다. 2) 투영 함수 0(일반적으로 MLP)은 지점 임베딩 및 시야 방향을 입력하고 이를 p에서 출력 색상 c와 불투명도 σ로 회귀시킵니다. (b) 미묘한 움직임이 있는 장면을 연구합니다. NeRF로 작은 변화를 모델링하기 위해 투영 함수를 고정하는 동안 시간에 따라 E를 변경합니다. (c) 주어진 지점 p에서 시간에 따른 임베딩 변화를 분석합니다. [Eto(p), ..., Etr-1(p)]. 우리는 특정 주파수 범위 내의 임베딩 변화를 분리하고 증폭하기 위해 시간적 필터링을 수행한 다음 증폭된 임베딩을 MLP 0으로 보내서 동작이 확대된 3D 렌더링을 생성합니다. 시간. 위상 기반 오일러리안[56, 57]은 각 비디오 프레임의 복잡한 조종 가능 피라미드[50, 17] 분해에서 추출된 각 픽셀의 위상 변화에 대해 작동합니다. 이후 작업은 아핀 변환을 사용하여 더 큰 동작을 확대하고 매팅[12]을 사용하여 관심 영역을 분리하고, 수작업으로 설계한 필터[37] 대신 선형 기반 방법을 사용하고, 1차 방법[65] 대신 2차 근사(가속도 포함)를 채택하는 데 중점을 둡니다. 비디오 기반 동작 확대는 칩 봉지와 같은 물체를 녹화하는 비디오에서 음파와 같은 신호를 추출하는 데에도 적용되었으며, 변형되고 진동합니다[11, 48]. 영어: 저희의 작업은 고전적인 오일러 운동 확대를 기반으로 하지만 1) 2D에서 3D로, 2) 색상 공간에서 광도장의 점 임베딩 공간으로 확장합니다. 저희의 결과는 오일러 원리가 점 임베딩 공간에서도 여전히 유지됨을 보여줍니다. 정적 광도장. NeRF[35]는 3D 장면을 표현하는 주류 접근 방식이 되었으며 고품질 뷰 합성 결과를 보여줍니다. 훈련 및 렌더링 가속[24, 36, 2, 18, 42, 8], 앨리어싱 감소[3], 무제한 장면 모델링[4, 63], 포즈 최적화[27, 34]를 포함하여 여러 측면에서 NeRF를 개선하기 위해 다양한 기술이 도입되었습니다. Factor Fields [9]는 다양한 NeRF 변형과 기타 신경 신호 표현을 요약하는 통합 프레임워크를 제공하며, 이는 주로 (1) 입력 좌표를 임베딩 공간에 매핑하는 좌표 변환, (2) 임베딩을 필드의 값으로 매핑하는 투영 함수의 두 가지 구성 요소로 구성됩니다. 이 논문에서는 유사한 관점을 채택하고 포인트 임베딩과 미묘한 동작 간의 관계를 분석하는 데 중점을 둡니다. NeRF에서 포인트 임베딩의 오일러 확대를 통해 미묘한 동작을 확대하는 것을 제안합니다. 위치 인코딩 기반 임베딩[35, 24]과 3면 임베딩[7, 18]을 모두 사용하여 NeRF에서 이 접근 방식을 성공적으로 적용했음을 보여줍니다. 동적 장면 표현. NeRF를 확장하여 동적 장면을 모델링하는 데 광범위한 연구가 수행되었습니다. 한 작업 라인은 변형 필드를 학습하고 이를 사용하여 각 타임스텝에 대해 표준 NeRF를 워핑합니다[41, 39, 40, 55, 25]. 또는 시간을 추가 좌표로 사용하여 공간-시간 광도장을 직접 학습할 수 있습니다[26, 62, 19, 1, 52, 31, 18, 6, 47]. 동적 장면을 재구성하는 주요 과제는 시간 동기식 다중 뷰 관찰을 캡처하는 것입니다. 다중 카메라 설정이 고품질 데이터를 수집하는 데 이상적이지만[66, 5, 30], 연구자들은 일관된 깊이[62, 32, 21], 광학 흐름[26, 19, 31] 또는 인간의 사전 정보[20, 60]와 같은 사전 정보를 활용하여 단일 카메라 캡처의 더 어렵지만 실용적인 설정을 탐구했습니다. 우리는 다중 카메라 및 단일 카메라 설정 모두에서 3D 동작 확대의 적용 가능성을 보여줍니다. 암묵적 표현. 암묵적 표현은 신호를 모델링하기 위한 강력한 도구로 등장했습니다[51, 13, 38, 28, 43, 53, 54, 15, 16, 14, 49]. Mai와 Liu[33]는 암묵적 신경 표현을 사용하여 2D 비디오를 연구하고 위치 인코딩 함수에서 공간적으로 불변하는 위상 변화를 학습하여 비디오의 동작을 모델링합니다. 위치 인코딩 함수에 기반한 확대에 대한 우리의 실험은 유사한 아이디어를 공유합니다. 그러나 Mai와 Liu[33]와 달리 우리는 공간적으로 변하는 위상 변화를 학습하고 3D 동작을 확대합니다. 3. 예비 오일러 동작 확대. 오일러 기반 동작 분석은 특정 입자(픽셀)를 추적하는 대신 시간에 따른 고정된 공간 위치에서의 변화에 초점을 맞춥니다. 선형 오일러 접근 방식은 푸리에 변환을 사용하여 각 픽셀의 시간에 따른 색상 변화를 1D 벡터로 변환하고 시간 주파수 구성 요소를 얻고 원하는 동작에 해당하는 주파수를 필터링합니다. 원하는 주파수 범위 내에서 색상 강도가 변경되면 증폭되어 원래 값에 다시 추가되어 동작 확대 비디오(미묘한 동작이 더 눈에 띄게 됨)가 생성됩니다. 픽셀당 색상 강도를 증폭하면 프레임 전체에서 동작이 확대되는 이유에 대한 직관을 제공하기 위해 f(x,t) = g(x+8(t))가 이동(t)으로 설명되는 시간에 따른 동작이 있는 신호를 나타낸다고 가정합니다. 점 x에 대한 g(x+8(t))의 1차 테일러 급수 확장은 다음과 같이 작성할 수 있습니다. g(x)+g&#39;(x)(x+ɗ(t) −x) = g(x)+g′(x)§(t). 여러 시간 단계 t에서 관찰을 통해 정적 g(x) 항을 쉽게 필터링하고 동적 항 g&#39;(x)(t)를 유지할 수 있습니다. g&#39;(x) 8(t)를 α로 곱하고 원래 신호에 다시 더하면 g(x)+(1+α)g&#39;(x)♂(t) = ≈ g(x+(1+α)&amp;(t))가 되는데, 이는 동작을 α만큼 확대하는 것과 같습니다. 위상 기반 오일러 방식은 색상 진폭 변화 대신 시간에 따른 위상 변화를 증폭합니다. 여기서 위상은 원래 프레임에서 구성된 복잡한 조종 가능 피라미드에서 추출됩니다. 위상과 동작 간의 연결은 푸리에 이동 정리를 통해 확립할 수 있습니다. 함수 f(x)가 도메인에서 거리 S만큼 이동하면 푸리에 성분 F(k)를 위상 인자 e-i2лk로 곱하는 것과 같습니다. е -ί2πκδ F {f(x − 8)}(k) = F{f(x)}(k)e¯i 여기서 는 푸리에 변환 연산자를 나타내고 k는 주파수 성분을 나타냅니다. 다시 말해, 시간에 따른 위상 변화를 추출하면 S에 의한 공간에서의 동작으로 인한 픽셀 이동이 드러납니다. 위상 변화를 증폭한 후 역 푸리에 변환을 통해 동작 확대 신호를 생성할 수 있습니다. 3D 장면 표현으로서의 신경 광도장. NERF는 장면의 광도를 연속 함수로 모델링합니다. 이 함수는 3D 공간 좌표 pЄR³와 시야 방향 d = S²를 입력으로 받고 해당 지점의 광도 색상 c(시야 방향 d에서 관찰)와 밀도 σ를 출력합니다. 주목할 점은 공간 좌표 Р가 일부 임베딩 함수 E를 통해 피처 표현으로 변환된 후 투영 함수(MLP)가 이를 최종 예측으로 회귀시킵니다. f(p,d) = MLP(E(p),d) = (c,σ). 미묘하고 알려지지 않은 장면 동작의 경우 이 시간 가변 장면을 f(p+8(p,t),d)로 공식화할 수 있다고 가정합니다. MLP가 시간에 따라 고정되어 있다면, 알려지지 않은 동작(p,t)은 E(p,t)의 시간적 변화를 분석하여 복구할 수 있습니다. 하지만, E(p,t)에 어디서 접근할까요? 2D 비디오의 경우, 관심 있는 데이터는 카메라로 직접 기록되어 분석에 사용할 수 있는 반면, 여기서는 캡처 중에 3D의 미묘한 동작을 관찰했을 수 있는 2D 이미지 컬렉션에만 접근할 수 있습니다. 다음 하위 섹션에서는 함수 E(p,t)를 변경하여 미묘한 3D 동작을 모델링하도록 NeRF를 재구성하는 방법을 설명합니다. 4. 방법 다음을 사용할 수 있다고 가정합니다. 1) 정적 NeRF를 재구성하기 위한 다중 시점 관찰, 2) 시간 동기화된 다중 카메라 설정 또는 단일 이동 카메라를 사용하여 미묘한 장면 동작의 비디오 녹화. 이 방법의 일반적인 워크플로는 다음과 같습니다. 1) 정지 상태라고 가정할 수 있는 이미지 관찰에서 정적 NeRF를 학습합니다. 2) 비디오 관찰에서 각 타임스텝 t = [0,T - 1]에 대해 NeRF 렌더링이 t에서의 관찰과 일치하도록 임베딩 함수 Et를 미세 조정합니다. 3) 모든 7개의 임베딩 함수 Et를 미세 조정한 후 NeRF 렌더링에 사용된 각 샘플링된 포인트의 시간적 변화를 증폭하여 동작을 확대합니다. 이 섹션에서는 NeRF를 재활용하여 미묘한 동작을 캡처하고 NeRF에서 학습한 포인트 임베딩을 분석하여 확대를 수행하는 방법을 설명합니다. 위치 인코딩을 포인트 임베딩 함수로 사용하는 표준 NeRF의 기본 케이스로 논의를 시작합니다. 그런 다음 시간 관찰된 www www 위치 이동 선형 3면 시간 관찰된 위치 이동 선형 3면 렌더링된 뷰 지상 진실 인코딩 이동 위상 3면 렌더링된 뷰 지상 진실 인코딩 이동 위상 3면 렌더링된 뷰 시간 관찰된 위치 이동 선형 3면 지상 진실 인코딩 이동 위상 3면 시간 관찰된 위치 이동 선형 - 3면 렌더링된 뷰 지상 진실 인코딩 이동 위상 3면 그림 4. 합성 장면의 3D 동작 확대. 우리는 주기적으로 진동하는 객체 부분을 사용하여 각 합성 장면을 생성했습니다. 우리는 4절에서 논의한 접근 방식을 사용하여 NeRF 재구성에서 인코딩된 미묘한 동작을 확대하고, 여기에서 동작을 2D 시공간 슬라이스 이미지로 시각화합니다. 각 시공간 슬라이스의 해당 위치는 렌더링된 뷰에서 빨간색 선으로 표시됩니다. 네 가지 접근 방식 모두 성공적으로 동작을 포착하고 확대하지만 선형 오일러 접근 방식인 선형-삼면은 공간-시간 슬라이스에서 밝고 어두운 점으로 나타나는 강도 오버슈팅[56]에 더 취약합니다.SSIM 위치 인코딩 LPIPS↓↓ 삼면 SSIM LPIPS↓ 위치 이동 인코딩 이동-선형-비디오 0.0.위상-비디오 --위치 이동 인코딩 이동-선형-비디오 위상 비디오 --선형-삼면 선형 삼면 위상-삼면-선형-비디오 위상 비디오 0.0.위상 삼면 선형 비디오 위상-비디오 0.50인자 0.0.0.520 50인자 20 50인자 0.0.50인자 그림 5. 정량적 비교. 우리는 위치 인코딩(왼쪽)과 3면(오른쪽)을 포인트 임베딩 함수로 사용하여 사용된 확대 계수의 함수로서 동작 확대 렌더링의 품질을 평가합니다. 위치 인코딩을 사용하여 사인파의 위상 이동을 통해 포인트 임베딩을 변경하는 두 가지 접근 방식을 평가합니다. 위치 이동(각 3D 포인트 이동)과 인코딩 이동(각 주파수 이동). 3면으로 학습된 포인트 임베딩을 변경하는 두 가지 접근 방식을 평가합니다. 선형 - 3면(3면의 선형 확대) 및 위상 - 3면(3면의 위상 기반 확대). 두 임베딩 함수 모두 비디오 동작 확대를 위한 두 가지 기준 방식인 선형 - 비디오(NeRF 렌더링 비디오의 선형 확대) 및 위상-비디오(NeRF 렌더링 비디오의 위상 기반 확대)와 비교합니다. 두 개의 임베딩 함수의 결과를 분리하여 다양한 확대 접근 방식의 영향을 더 잘 평가하고 다양한 임베딩 함수와 MLP 아키텍처 간의 고유한 성능 격차와 혼동되는 것을 방지합니다. NeRF에 대한 임베딩 함수로 3면 평면을 사용하는 선호하는 접근 방식을 설명합니다. 이는 이전에 비디오를 위해 설계된 위상 기반 오일러 확대 기술과 자연스럽게 통합됩니다. 4.1. 위치 인코딩을 사용한 NeRF 먼저 위치 인코딩을 사용하여 표준 NeRF에서 임베딩 공간의 오일러 확대를 달성하는 방법을 설명합니다. 비디오에 대한 동작 조정 가능 신경 표현에 대한 이전 작업[33]에서 영감을 받아 NeRF의 주요 백본을 그대로 유지하고 위치 인코딩 함수에서 위상 이동을 적용하는 방법을 학습하는 작은 MLP g를 별도로 훈련합니다. 그러나 이전 작업[33]과 달리 장면의 비디오 관찰을 사용하여 캡처한 시간별로 이미지를 구성하고 각 시간 단계에 대해 별도의 MLP g를 훈련합니다. 실제로 g는 각 지점의 임베디드 표현을 조정하여 투영 함수(MLP 0)의 NeRF 출력이 시간에 따라 변하는 관찰과 일치하도록 학습합니다.MLP의 가중치는 모든 시간 단계에서 공유되며 유일한 차이점은 지점 임베딩에 있습니다.위치 인코딩 함수에 위상 이동을 유도하는 데는 위치 이동과 인코딩 이동이라는 두 가지 옵션이 있습니다.위치 이동.지점 임베딩을 변경하여 동작을 독점적으로 모델링하기 위해 g가 쿼리된 지점 p의 3D 위치 이동을 직접 예측하도록 합니다.g(p,t) = Ap R³.위치 en을 적용하기 전에 Ap를 p에 추가합니다.렌더링된 보기 시간 관찰 확대 렌더링된 보기 시간 관찰 확대 렌더링된 보기 시간 관찰 확대 그림 6. 실제 다중 보기 동작 확대.HumanNerf 데이터 세트[20]의 다중 보기 비디오를 사용하여 실제 3D 동작을 캡처하고 확대할 수 있습니다. 여기서 우리는 동작을 2D 시공간 이미지로 시각화합니다.각 시공간 슬라이스의 해당 위치는 렌더링된 뷰에 빨간색 선으로 표시됩니다.관찰된 관찰된 프레임 x 1Hz로 확대된 새로운 뷰 관찰된 프레임 관찰된 확대된 1.5Hz 새로운 뷰 그림 7. 실제 단일 카메라 동작 확대.동적 장면의 단안경 뷰에도 불구하고 미묘한 동작을 확대하면서 새로운 뷰를 렌더링할 수 있습니다.우리는 동작을 시간에 따른 2D 이미지 슬라이스로 시각화합니다.코딩, 시간 가변 포인트 임베딩 함수 E(p,t) = PosEnc(p+g(p,t)), .을 얻습니다.이는 위치 인코딩에 사용된 각 사인파 sin(@x) 내에서 @g(p,t)의 위상 이동을 적용하는 것과 동일합니다.모든 K 주파수의 위상 이동은 방향이 같고 크기만 다르며 이는 다른 @ = [1,..., K]로 조정됩니다.인코딩 이동. 동작은 3D 장면에서 기하학적 변화만 일으키는 것이 아니라 그림자와 반사와 같은 모양도 변화시킨다는 점에 유의하세요. 따라서 모든 장면 변화를 3D 위치의 이동에 기인시키는 것만으로는 충분하지 않습니다. 대신 g가 각 인코딩 주파수에 대해 별도의 이동을 학습하도록 할 수 있습니다. 즉, Ow Є R³인 경우 g(p,t) = [01, 02, ..., OK] Є R³×K이고 포인트 임베딩 함수는 E(p,t) = [sin(@1p+1), ..., sin(@kp+¢k)]가 됩니다. 이 설정은 위치 인코딩을 3K 채널이 있는 임베딩을 생성하는 피처 생성기로 취급합니다. 이후 섹션에서 볼 수 있듯이, 이 &quot;모션 독립적&quot; 접근 방식은 위치 이동만 고려하는 접근 방식보다 성능이 뛰어나면서도 실제 모션을 포착하는 방법을 학습합니다. 위치 인코딩이 있는 표준 NeRF에서 포인트 임베딩을 변경하는 이 두 가지 접근 방식 중 하나를 사용하면 g(p,t)의 시간적 변화를 선형적으로 증폭한 다음 NeRF로 포인트의 색상과 불투명도를 렌더링하여 확대된 모션을 렌더링할 수 있습니다. 4.2. 3면 학습 가능 임베딩을 사용한 NeRF 이제 NeRF에 대한 임베딩 함수로 3면 기반 접근 방식을 사용하는 선호하는 접근 방식을 설명합니다. 이후 실험 결과 3면 기반 접근 방식이 위치 인코딩 접근 방식보다 더 나은 확대 품질을 달성했습니다. 3면 [7, 18]은 최근 NeRF 렌더링에서 포인트에 대한 학습 가능 임베딩을 얻는 효율적인 방법으로 제안되었습니다. 분석적 위치 인코딩이 있는 NeRF와 비교할 때, 포인트 임베딩 함수로 3면 기반 NeRF는 훨씬 적은 MLP 레이어와 더 빠른 추론으로 유사한 표현 용량을 달성할 수 있습니다. NeRF 사용 사례에서 3면 공식화는 3D 장면을 2D 피처 평면 모음으로 축소하는 좋은 의미를 갖습니다. 이러한 분해는 무작위로 점을 피처로 해싱하는 대신 점 간의 상대적인 공간 관계를 보존합니다. 이 관찰 결과는 피처 평면을 직접 처리하고 위치 인코딩 함수 내에서 앞서 언급한 선형 확대보다 잠재적으로 더 나은 성능을 발휘하여 3D 모션 확대를 달성할 수 있음을 시사합니다. 구체적으로, MLP 기반 투영 함수가 시간에 따라 공유되는 동안 각 시간 단계에 대해 별도의 3면 임베딩 함수를 학습합니다. 이 설정으로 장면의 모든 미묘한 시간적 변화(모션 또는 모양)는 3면의 2D 피처 이미지의 시간적 변화에 캡슐화되어야 합니다. 점 임베딩SSIM ↑ LPIPS↓ 0.0.선형 3면 위상 - 3면 --선형 - 비디오 위상 비디오 0.0.0.0.-선형 3면 --위상 - 3면 --선형 - 비디오 --위상 - 비디오각도 각도 그림 8. 관찰된 뷰에서 편차 각도 변경. 편차 각도가 증가함에 따라 임베딩 공간을 확대하면 색상 공간에서 작동하는 기준 접근 방식보다 지속적으로 성능이 우수합니다. --선형 - 3면 위상 3면 SSIM ↑ LPIPS↓ 0.선형 3면 0.위상 3면 선형 비디오 0.--선형-비디오 위상 비디오 위상 비디오 0.0.0.0.0.0.0.0.0.0.0.0.0.0.그림 9. 트레이닝 뷰에서 다양한 노이즈 수준. 3면 임베딩 공간에서 오일러 확대를 수행할 때 노이즈가 있는 경우 선형 3면보다 위상 3면이 더 강력합니다. 이 결과는 색상 공간 확대에 대한 이전 분석[56]과 유사합니다(선형 - 비디오 및 위상 - 비디오의 두 가지 기준 결과에 의해 검증됨). 여기서 함수는 E(p,t) = Project(p, TriPlanet)로 작성할 수 있으며, 여기서 p의 임베딩은 p를 3면으로 투영하고 해당 피처를 집계하여 얻습니다. 각 시간 단계에 대해 별도의 3면이 구성되면 이전 비디오 확대 방법과의 주요 연결을 설정합니다. 기본적으로 각 3면 피처 채널에 대한 비디오를 얻고, 여기에 선형 확대를 적용하여 피처 이미지의 각 픽셀의 시간적 변화를 증폭하거나 2D 피처 이미지의 각 채널 위에 구성된 복잡한 조종 가능 피라미드를 사용하여 위상 기반 확대를 할 수 있습니다. 실험 결과는 이 2D에서 영감을 받은 접근 방식의 타당성을 확인합니다. 5.2절에서 선형 및 위상 기반 접근 방식 간의 성능 비교는 이 두 접근 방식이 색상 공간의 오일러 처리를 수행하는 데 적용되었을 때의 원래 결과를 검증합니다.5. 실험 결과 5.2절에서 합성 장면을 사용하여 제안된 방법의 성능을 평가합니다.확대된 동작에 대한 기준 진실 시퀀스를 만들고 3D 동작 확대를 위한 다양한 접근 방식을 정량적으로 비교합니다.그런 다음 5.3절에서 실제 캡처된 데이터에 대한 결과를 제시합니다.먼저 실제 다중 뷰 비디오 관찰에 방법을 적용합니다.실제 다중 뷰 데이터에서 효과를 검증한 후 단일 카메라를 사용하여 캡처한 실제 비디오 시퀀스에 방법을 추가로 적용합니다.동작 확대를 3D로 확장한 결과, 안정화된 2D 비디오에 초점을 맞춘 이전 작업에서는 달성할 수 없는 시나리오인 카메라 흔들림이 있는 핸드헬드 캡처 비디오에서 3D 동작을 성공적으로 확대합니다.5.1. 구현 세부 정보 포인트 임베딩으로서의 위치 인코딩. 우리는 g(p,t)를 예측하고 위치 인코딩에서 결과 위상 변화를 적용하기 위해 32개의 은닉 채널을 갖는 3계층 MLP를 훈련합니다. 우리는 nerface[24]로 네트워크를 구현합니다. 첫 번째 타임스텝에 대한 정적 NeRF를 훈련하기 위해 50,000단계를 최적화하고 나머지 타임스텝에 대해 10,000단계에 대해 임베딩 함수를 미세 조정합니다. 훈련 후, 각 지점 p에 대해 시간에 따라 변하는 위상 변화 g(p,t)를 얻습니다. 시간 창 [0,..., T1] 내에서 확대된 동작으로 렌더링하기 위해 시간 차원을 따라 [g(p,0),..., g(p,T – 1)]을 푸리에 변환하고, 대역 통과 필터를 사용하여 관심 주파수 범위 내에서 동작을 분리하고, 통과 대역 범위 내에서 구성 요소를 증폭한 다음 역 푸리에 변환을 적용하여 확대된 예측을 얻습니다. 확대된 예측은 위상 변화로 위치 인코딩 내부에 추가되고, 그 다음에 MLP 추론을 사용한 표준 NeRF 렌더링이 이어집니다.점 임베딩으로서의 3면.우리의 구현은 K-Planes [18]를 기반으로 합니다.다른 평면에서 임베딩을 집계하기 위해 기본 Hadamard 곱 대신 연결을 채택하고 단순성을 위해 3면만 단일 스케일로 설정합니다.첫 번째 타임스텝에 대한 정적 NeRF를 학습하기 위해 30,000단계로 최적화하고 나머지 타임스텝에 대해 10,000단계로 임베딩 함수를 미세 조정합니다.학습 후 [0,..., T – 1] 내의 각 피처 평면에 대한 비디오를 구성합니다.그런 다음 이러한 피처 비디오에 직접 2D 확대 방법을 적용합니다.선형 접근 방식의 경우 주파수 범위 내에서 피처 값 변화를 시간적으로 필터링하고 증폭합니다. 위상 기반 접근 방식의 경우, 특징 이미지 위에 복잡한 조종 가능 피라미드를 구성하고, 위상 변화를 시간적으로 필터링하고 증폭한 다음, 피라미드를 다시 이미지 공간으로 축소합니다.결과적인 특징 이미지는 확대된 동작을 보일 것입니다[56].동작 확대 렌더링을 생성하기 위해, 각 샘플 포인트를 처리된 삼면 특징에 임베딩한 다음, MLP를 사용하여 평소와 같이 임베딩을 색상 및 밀도 출력으로 투영합니다.5.2. 합성 장면 데이터 생성.표준 Blender 장면[35]을 사용하고 다른 객체 부분에서 동작을 시뮬레이션합니다.각 장면을 초당 30프레임으로 1초 동안 렌더링합니다.시뮬레이션된 동작은 주기적이며 3Hz에서 5Hz까지입니다.또한 요인 5, 10, 20, 50 및 100에서 기준 진실 확대 동작이 있는 시퀀스를 렌더링합니다.질적 평가.Sec.에서 설명한 접근 방식으로 Blender 장면의 동작을 확대합니다. 4. 그림 4의 결과는 네 가지 접근 방식 모두 관찰에서 작은 동작을 성공적으로 모델링하고 확대한다는 것을 보여줍니다.비디오 확대[56]에서 이전에 발견된 결과와 일관되게 선형 오일러 접근 방식은 잘린 색상 강도 및 노이즈 증폭과 같은 더 많은 아티팩트를 초래합니다.양적 평가.구조적 유사성 지수 측정(SSIM)[59] 및 AlexNet[22]을 백본으로 하는 LPIPS[64]를 사용하여 기준 진실 확대 프레임에 대한 결과를 평가합니다.또한 각 테스트 뷰에서 기준선 비교를 위해 확대 없이 훈련된 NeRF의 비디오를 렌더링합니다.그런 다음 미묘한 동작을 확대하기 위해 비디오에 직접 고전적인 2D 방법을 적용합니다.그림 5에서 볼 수 있듯이 3D 확대 방법은 기준 진실 렌더링과 일치하는 동작 확대 렌더링을 생성하는 데 2D 기준선 방법보다 성능이 우수합니다.감도 분석.그림 8에서 테스트 관점이 관찰된 관점에서 벗어날 때 확대 품질을 분석합니다. 그림 9에서 캡처된 프레임에서 노이즈 수준이 증가함에 따라 확대 품질을 표시합니다.색 공간 확대에 대한 이전 연구[56]에서 위상 기반 접근 방식이 선형 접근 방식보다 노이즈에 덜 민감하다는 것을 발견했습니다.임베딩 공간에서 확대하는 동안 유사한 현상을 관찰했습니다.5.3. 실제 장면 다양한 카메라 설정으로 캡처한 여러 실제 장면에서 방법을 테스트합니다.다중 카메라 설정.먼저 HumanNerf[66]의 공개적으로 사용 가능한 데이터 세트에서 방법을 검증합니다.이 데이터 세트는 중앙에 사람이 서 있는 장면을 동시에 캡처하는 6개 카메라의 짧은 비디오로 구성됩니다.사람이 비교적 움직이지 않지만 미묘한 신체 움직임이 있는 짧은 기간을 추출합니다.그림 6에서 다양한 관점에서 확대된 결과를 제시합니다.전체 비디오는 보충 자료에서 사용할 수 있습니다.단일 카메라 설정.다중 카메라 설정은 많은 사용자에게 엄청나게 불편하고 비쌀 수 있으므로 단일 카메라 설정에 방법을 추가로 배포합니다.두 단계로 구성된 캡처 절차를 설계합니다. 1단계: 먼저 정적 장면의 움직이는 카메라 비디오를 캡처하여 정적 NeRF를 훈련하는 데 사용합니다.2단계: 동적 장면의 단일 뷰 비디오를 캡처하여 NeRF에서 포인트 임베딩 함수를 미세 조정하여 시간 가변 장면을 모델링하는 데 사용합니다.2단계 캡처 후 NeRF 훈련을 수행하고 이전에 설명한 파이프라인을 사용하여 확대된 동작을 렌더링합니다.이 2단계 캡처 접근 방식은 사람들이 정상 조건에서 정적 토목 구조물에서 원치 않는 동작을 식별하는 비디오 기반 동작 확대 응용 프로그램 시나리오에서 널리 사용됩니다[46, 23].또한 그림 2에서 볼 수 있듯이 안정적인 캡처 없이는 실패할 이전의 2D 기반 접근 방식과 달리 저희 방법은 핸드헬드 캡처를 지원하고 삼각대가 필요하지 않다는 점을 강조합니다.이는 저희 방법이 동적 시퀀스에서 광도 필드를 업데이트할 때 각 프레임의 추정된 포즈를 독립적으로 사용하기 때문에 가능합니다.따라서 정확한 포즈 추정을 통해 완벽하게 정지된 캡처가 필요 없습니다. 그림 1에서는 핸드스탠드를 하는 체조 선수의 단안 캡처를 사용한 결과를 보여주고, 그림 7에서는 한쪽 다리로 균형을 맞추려 하고 호흡하는 사람의 결과를 보여줍니다. 또한 NeRF 재구성을 통해 확대한 것과 비교한 관찰된 시공간 슬라이스를 보여줍니다. 6. 제한 사항 실제 환경에서 캡처한 데이터는 초점 흐림과 카메라 흔들림으로 인해 흐릿해질 수 있으며, 이는 NeRF의 품질을 저하시킵니다. NeRF의 훈련은 카메라 포즈에 대한 지식을 가정하므로 성능은 포즈 추정의 정확도에 크게 좌우되며, 종종 RGB 기반 구조-모션(SfM) 알고리즘[44, 45]을 사용합니다. 추정은 대부분 신뢰할 수 있지만 특정 보정이 필요한 렌즈 왜곡으로 인해 완벽하지는 않습니다. 이는 NeRF 기반 3D 재구성을 위한 일반적인 파이프라인에서는 수행되지 않을 수 있습니다. 더 중요한 것은 부정확한 포즈 추정은 카메라 모션과 장면 모션 간의 모호성을 심화시켜 미묘한 장면 모션을 확대하는 데 방해가 되거나 잘못된 모션 확대로 이어질 수 있습니다. 따라서 실제 세계 데이터는 장면에 질감이 있는 표면이 있는 안정적인 RGB 기반 SfM이나 캡처 중에 6-DoF 추적을 지원하는 카메라를 통해 정확한 카메라 내부 및 외부 매개변수에 접근할 수 있는 조건에서 캡처해야 합니다.
--- CONCLUSION ---
우리는 시간에 따른 NeRF 임베딩을 분석하기 위해 Eulerain 처리 원리를 적용하는 3D 동작 확대 방법을 제시합니다. 고전적인 확대 방법(원래 2D 비디오용으로 개발됨)이 픽셀 색상을 직접 처리하는 반면, 우리는 NeRF의 포인트 임베딩을 처리하면 이러한 접근 방식이 성공적으로 일반화되고 3D 렌더링에서 동작을 확대할 수 있음을 보여줍니다. 우리는 우리의 작업이 기존 신호 처리 기술을 신경 분야에 통합하기 위한 추가 연구를 촉진할 것이라고 믿습니다.참고문헌 [1] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael Zollhoefer, Johannes Kopf, Matthew O&#39;Toole, and Changil Kim. HyperReel: RayConditioned Sampling을 사용한 고충실도 6-DoF 비디오. CVPR, 2023.[2] Benjamin Attal, Jia-Bin Huang, Michael Zollhöfer, Johannes Kopf, and Changil Kim. Ray-space Embedding을 사용한 신경 광 필드 학습. CVPR, 2022.[3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan. Mip-NeRF: 앤티 앨리어싱 신경 광도장을 위한 다중 스케일 표현. arXiv, 2021.[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman. Mip-NeRF 360: 무제한 앤티 앨리어싱 신경 광도장. CVPR, 2022.[5] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, et al. HuMMan: 다목적 감지 및 모델링을 위한 다중 모달 4D 인간 데이터 세트. ECCV, 2022.[6] Ang Cao 및 Justin Johnson. HexPlane: 동적 장면을 위한 빠른 표현. CVPR, 2023.[7] Eric R. Chan, Connor Z. Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J. Guibas, et al. 효율적인 기하 인식 3D 생성적 적대 네트워크. CVPR, 2022. 2, 3,[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu 및 Hao Su. TensoRF: 텐서 광도장. ECCV, 2022.[9] Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su 및 Andreas Geiger. 요인장: 신경장 및 그 이상을 위한 통합 프레임워크. arXiv, 2023.[10] Justin G. Chen, Abe Davis, Neal Wadhwa, Frédo Durand, William T. Freeman, Oral Büyüköztürk. 토목 인프라 애플리케이션을 위한 비디오 카메라 기반 진동 측정. Journal of Infrastructure Systems, 23(3), 2017.[11] Abe Davis, Michael Rubinstein, Neal Wadhwa, Gautham J. Mysore, Frédo Durand, William T. Freeman. 시각적 마이크로폰: 비디오에서 수동적으로 사운드 복구. ACM Transactions on Graphics, 33(4), 2014.[12] Mohamed Elgharib, Mohamed Hefeeda, Fredo Durand, William T. Freeman. 큰 동작이 있는 경우의 비디오 확대. CVPR, 2015.[13] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, J Zico Kolter. 곱셈 필터 네트워크. ICLR, 2020.[14] Brandon Y. Feng, Susmija Jabbireddy 및 Amitabh Varshney. VIINTER: 이미지의 암묵적 신경 표현을 사용한 뷰 보간. SIGGRAPH Asia, 2022.[15] Brandon Y. Feng 및 Amitabh Varshney. SIGNET: 광장에 대한 효율적 신경 표현. ICCV, 2021.[16] Brandon Y. Feng, Yinda Zhang, Danhang Tang, Ruofei Du 및 Amitabh Varshney. PRIF: 1차 광선 기반 암묵적 함수. ECCV, 2022.[17] William T. Freeman 및 Edward H. Adelson. 조종 가능 필터의 설계 및 사용. IEEE 패턴 분석 및 머신 인텔리전스 저널, 1991.K[18] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, Angjoo Kanazawa. Planes: Explicit Radiance Fields in Space, Time, and Appearance. arXiv, 2023. 3, 6,[19] Chen Gao, Ayush Saraf, Johannes Kopf, Jia-Bin Huang. Dynamic Monocular Video의 Dynamic View Synthesis. ICCV, 2021. 3,[20] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, Anurag Ranjan. NeuMan: 단일 비디오의 Neural Human Radiance Field. ECCV, 2022. 4,[21] Johannes Kopf, Xuejian Rong, Jia-Bin Huang. 견고하고 일관된 비디오 깊이 추정. CVPR, 2021.[22] Alex Krizhevsky, Ilya Sutskever 및 Geoffrey E. Hinton. 딥 합성곱 신경망을 사용한 ImageNet 분류. NIPS, 2012.[23] Ricard Lado-Roigé, Josep Font-Moré 및 Marco A. Pérez. 진동 기반 손상 감지를 위한 학습 기반 비디오 동작 확대 접근 방식. 측정, 206, 2023. 1,[24] Ruilong Li, Matthew Tancik 및 Angjoo Kanazawa. NerfAcc: 일반적인 NeRF 가속 도구 상자. arXiv, 2022. 3,[25] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Zhaoyang Lv. 신경 3D 비디오 합성. CVPR, 2022.[26] Zhengqi Li, Simon Niklaus, Noah Snavely, Oliver Wang. 동적 장면의 시공간 뷰 합성을 위한 신경 장면 흐름 필드. CVPR, 2021. 3,[27] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey. Barf: 번들 조정 신경 광도 필드. ICCV, 2021.[28] David B. Lindell, Dave Van Veen, Jeong Joon Park, Gordon Wetzstein. BACON: 다중 스케일 장면 표현을 위한 대역 제한 좌표 네트워크. CVPR, 2022.[29] Ce Liu, Antonio Torralba, William T. Freeman, Frédo Durand, Edward H. Adelson. 모션 확대. ACM Transactions on Graphics, 2005. 1,[30] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, Christian Theobalt. 신경 행위자: 포즈 제어를 통한 인간 행위자의 신경 자유 시점 합성. ACM Transactions on Graphics, 2021.[31] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, Jia-Bin Huang. 견고한 동적 광채 필드. CVPR, 2023. 3,[32] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen 및 Johannes Kopf. 일관된 비디오 깊이 추정. ACM Transactions on Graphics, 2020.[33] Long Mai 및 Feng Liu. 동작 조정 가능 신경 암묵적 비디오 표현. CVPR, 2022. 4,[34] Andreas Meuleman, Yu-Lun Liu, Chen Gao, Jia-Bin Huang, Changil Kim, Min H. Kim 및 Johannes Kopf. 견고한 뷰 합성을 위한 점진적으로 최적화된 국소 광도 필드. CVPR, 2023.[35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi 및 Ren Ng. NeRF: 뷰 합성을 위한 신경 광도 필드로 장면 표현. ECCV, 2020. 3,[36] Thomas Müller, Alex Evans, Christoph Schied, Alexander Keller. 다중 해상도 해시 인코딩을 사용한 인스턴트 신경 그래픽스 기본 요소. ACM 그래픽스 트랜잭션, 2022.[37] Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib, Frédo Durand, William T. Freeman, Wojciech Matusik. 학습 기반 비디오 모션 확대. ECCV, 2018. 2,[38] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, Steven Lovegrove. DeepSDF: 모양 표현을 위한 연속 부호 거리 함수 학습. CVPR, 2019.[39] 박근홍, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Steven M. Seitz, Ricardo Martin-Brualla. Nerfies: 변형 가능한 신경 방사 필드. ICCV, 2021.[40] 박근홍, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B. Goldman, Ricardo MartinBrualla 및 Steven M. Seitz. HyperNeRF: 위상적으로 변화하는 신경 복사장에 대한 고차원 표현. 그래픽에 대한 ACM 거래, 2021.[41] 알베르트 푸마롤라, 엔릭 코로나, 제라드 폰스-몰, 프란세스크 모레노-노게르. D-NeRF: 동적 장면을 위한 신경 복사장. CVPR, 2021.[42] Sara Fridovich-Keil 및 Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht 및 Angjoo Kanazawa. Plenoxels: 신경망이 없는 광채 필드. CVPR, 2022.[43] Vishwanath Saragadam, Jasper Tan, Guha Balakrishnan, Richard G Baraniuk 및 Ashok Veeraraghavan. MINER: 다중 스케일 암묵적 신경 표현. ECCV, 2022.[44] Johannes Lutz Schönberger 및 Jan-Michael Frahm. 구조에서 움직임으로 재검토. CVPR, 2016.[45] Johannes Lutz Schönberger, Enliang Zheng, Marc Pollefeys 및 Jan-Michael Frahm. 비구조화된 다중 뷰 스테레오를 위한 픽셀별 뷰 선택. ECCV, 2016.[46] Zhexiong Shang 및 Zhigang Shen. 비디오 기반 모션 처리를 사용한 토목 구조물의 다중 지점 진동 측정 및 모드 확대. 건설 자동화, 93, 2018. 1,[47] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang 및 Yebin Liu. Tensor4D: 고충실도 동적 재구성 및 렌더링을 위한 효율적인 신경망 4D 분해. CVPR, 2023.[48] Mark Sheinin, Dorian Chan, Matthew O&#39;Toole 및 Srinivasa G. Narasimhan. 이중 셔터 광학 진동 감지. CVPR, 2022.[49] Shayan Shekarforoush, David B. Lindell, David J. Fleet 및 Marcus A. Brubaker. 다중 스케일 재구성을 위한 잔차 곱셈 필터 네트워크. arXiv, 2022.[50] Eero P. Simoncelli 및 William T. Freeman. 조종 가능한 피라미드: 다중 스케일 미분 계산을 위한 유연한 아키텍처. ICIP, 1995.[51] Vincent Sitzmann, Julien Martel, Alexander Bergman, David B. Lindell 및 Gordon Wetzstein. 주기적 활성화 함수를 사용한 암묵적 신경 표현. NeurIPS, 2020.[52] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu 및 Andreas Geiger. NeRFPlayer: 분해된 신경 광도장을 사용한 스트리밍 가능한 동적 장면 표현. arXiv, 2022.[53] Kushagra Tiwary, Akshat Dave, Nikhil Behari, Tzofi Klinghoffer, Ashok Veeraraghavan 및 Ramesh Raskar. ORCA: Radiance-Field 카메라로서의 광택 객체.CVPR, 2023.[54] Kushagra Tiwary, Tzofi Klinghoffer 및 Ramesh Raskar. 그림자에서 신경 표현을 학습하기 위해.ECCV, 2022.[55] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhöfer, Christoph Lassner 및 Christian Theobalt. 비강체 신경 Radiance Fields: 단안 비디오의 동적 장면의 재구성 및 새로운 관점 합성.ICCV, 2021.[56] Neal Wadhwa, Michael Rubinstein, Frédo Durand 및 William T. Freeman. 위상 기반 비디오 모션 처리. ACM Transactions on Graphics(Proceedings SIGGRAPH 2013), 32(4), 2013. 1, 2, 3, 5, 7,[57] Neal Wadhwa, Michael Rubinstein, Frédo Durand, William T. Freeman. 빠른 위상 기반 비디오 확대를 위한 Riesz 피라미드. ICCP, 2014. 2,[58] Wenjin Wang, Sander Stuijk, Gerard de Haan. 모션에 견고한 rPPG를 위한 이미지 센서의 공간 중복성 활용. IEEE Transactions on Biomedical Engineering, 62, 2015.[59] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, Eero P. Simoncelli. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. IEEE Transactions on Image Processing, 13(4), 2004.[60] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, Ira Kemelmacher-Shlizerman. Humannerf: 단안 비디오에서 움직이는 사람의 자유 시점 렌더링. CVPR, 2022.[61] Hao-Yu Wu, Michael Rubinstein, Eugene Shih, John Guttag, Frédo Durand, William T. Freeman. 세상의 미묘한 변화를 드러내기 위한 오일러리안 비디오 확대. ACM Transactions on Graphics(SIGGRAPH 2012 회의록), 31(4), 2012. 1,[62] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, Changil Kim. 자유 시점 비디오를 위한 시공간 신경 복사장. CVPR, 2021. 3,[63] Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun. Nerf++: 신경 광휘장 분석 및 개선. arXiv, 2020.[64] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, Oliver Wang. 지각적 지표로서의 딥 피처의 비합리적 효과. CVPR, 2018.[65] Yichao Zhang, Silvia L. Pintea, Jan C. van Gemert. 비디오 가속 확대. CVPR, 2017. 2,[66] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, Lan Xu. HumanNeRF: 희소 입력에서 효율적으로 생성된 인간 광휘장. CVPR, 2022. 4,
