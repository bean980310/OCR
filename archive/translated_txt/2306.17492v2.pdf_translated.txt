--- ABSTRACT ---
대규모 언어 모델(LLM)은 종종 오해의 소지가 있는 내용을 포함하고 있어 안전한 AI 시스템을 보장하기 위해 인간의 가치와 일치시킬 필요성을 강조합니다. 이러한 일치를 달성하기 위해 인간 피드백을 통한 강화 학습(RLHF)이 사용되었습니다. 그러나 여기에는 두 가지 주요 단점이 있습니다. (1) RLHF는 SFT와 달리 복잡성, 불안정성 및 하이퍼 매개변수에 대한 민감성을 나타냅니다. (2) 방대한 시행착오에도 불구하고 다중 샘플링은 쌍별 대조로 축소되어 거시적 관점에서 대조가 부족합니다. 이 논문에서는 인간 정렬을 위해 LLM을 직접 미세 조정하는 효율적인 SFT 알고리즘으로 선호도 순위 최적화(PRO)를 제안합니다. PRO는 쌍별 대조를 확장하여 모든 길이의 선호도 순위를 수용합니다. 후보를 반복적으로 대조함으로써 PRO는 LLM에 최상의 응답을 우선 순위를 지정하는 동시에 나머지 응답을 점진적으로 순위를 매기도록 지시합니다. 이러한 방식으로 PRO는 인간 정렬을 LLM에서 생성된 n개 응답의 확률 순위를 이러한 응답에 대한 인간의 선호도 순위와 일치시키는 것으로 효과적으로 변환합니다. 실험 결과, PRO는 기준 알고리즘보다 성능이 뛰어나 자동 기반, 보상 기반, GPT-4 및 인간 평가를 통해 ChatGPT 및 인간의 반응과 비슷한 결과를 달성했습니다.
--- INTRODUCTION ---
대규모 언어 모델(LLM)은 사용자의 다양한 정보 요구를 충족하는 데 있어 놀라운 역량을 보여주었습니다(Brown et al. 2020; Chowdhery et al. 2022; Bubeck et al. 2023; Touvron et al. 2023; Li et al. 2023). 1조 개의 토큰으로 구성된 사전 학습 코퍼스에 인코딩된 광범위한 글로벌 지식과 인간 행동을 활용함에도 불구하고, LLM은 불가피하게 그 안에 포함된 오해의 소지가 있고, 독성이 강하고, 해로운 콘텐츠의 영향을 받습니다(Bai et al. 2022b; Ouyang et al. 2022). 결과적으로, 인간 피드백을 통한 강화 학습(RLHF)이 도입되어 인간이 순위를 매긴 후보 집합에 따라 LLM의 언어 공간을 인간의 가치에 맞춰 안전하고 관리하기 쉬운 AI 시스템을 구축합니다(Stiennon et al. 2020; Xue et al. 2023; Peng et al. 2023). 그럼에도 불구하고 RLHF는 지도 학습보다 복잡하고 최적화 불안정성에 취약하며 하이퍼 매개변수에 민감합니다. 이러한 제한은 주로 *책임 저자에서 발생합니다. 저작권 2024, 인공 지능 진흥 협회(www.aaai.org). 모든 권리 보유. ONE y LLM PAIRy₁ y² LLM PAIRS ya LLMy² PRO y³ y LLM y 그림 1: 다양한 SFT 패러다임 간 비교. PRO는 다중 위치 및 다차원 대조를 위해 다른 패러다임과 구별됩니다. 에이전트, 즉 LLM을 사용하여 인간의 선호도에 직접 맞추는 대신 반복적인 시행착오를 경험합니다. 따라서 지도 미세 조정(SFT)은 RLHF를 대체할 수 있는 보다 직접적인 최적화 가능성으로 예상됩니다. SFT는 처음에는 RLHF에 대한 워밍업 프로세스로만 사용되며, 여기서 최상의 후보가 선택되어 LLM을 인간이 선호하는 데이터에 맞게 조정합니다. 최근 연구에서는 SFT를 향상시키기 위한 보다 복잡한 전략을 제안했습니다(Rafailov et al. 2023; Wu et al. 2023; Yuan et al. 2023; Dong et al. 2023). 약간의 진전에도 불구하고 개선의 여지가 여전히 있습니다. (1) RLHF가 좋은 성능을 발휘하도록 하는 본질은 무시됩니다. 즉, 훈련 중에 광범위한 언어 공간에서 점수를 매기는 다중 샘플링입니다. 주어진 순위 길이에 의해 제약을 받는 대부분의 방법은 의미적 또는 스칼라 관점에서 쌍별 대조에 주의를 기울입니다. (2) 더 긴 순위가 사용 가능하더라도 쌍으로 자르는 경향이 있어 거시적 관점에서 후보를 구별하지 못합니다. 이 연구에서 우리는 언어 공간에서 샘플링을 확대하는 것이 인간의 정렬에 미치는 영향을 철저히 조사합니다. 이 시나리오를 기반으로 우리는 직접 정책 최적화의 효율적인 프레임워크로 선호도 순위 최적화(PRO)를 제안합니다. 그림 1은 PRO가 다른 SFT 기반 공식과 어떻게 다른지 보여줍니다. 구체적으로, 우리는 RLHF의 본질을 재고하고 Bradley-Terry 모델(Bradley 및 Terry 1952)의 쌍대 대조를 확장하여 임의 길이의 순위 내에서 1대 N 대조를 포함합니다. 그런 다음 입력 프롬프트 x와 인간이 순위를 매긴 응답 집합(y¹, y², …..‚y”로 표현)이 주어지면 제안된 PRO 알고리즘은 에이전트 LLM을 조정하여 최상의 응답 y¹을 양수로, 나머지를 음수로 처리합니다. 이 우선순위는 LLM이 인간이 열등하다고 생각하는 응답보다 더 높은 가능성으로 y¹을 생성한다는 것을 시사합니다. 그런 다음 현재 최상위 응답을 반복적으로 무시하고 다음 응답을 나머지에 대한 양수로 간주하여 순위에 더 이상 응답이 없을 때까지 진행합니다. 더 많은 후보를 얻는 데 집중하는 것 외에도 특히 다양한 품질과 인간 정렬의 정도를 가진 발화를 샘플링하기 위해 다양한 수준의 프록시를 배포합니다. RLHF에서 영감을 받아 수신자 LLM에서 새로운 후보를 동적으로 샘플링하고 추가 보상 모델로 레이블을 지정하는 셀프 부트스트래핑 방법도 추가합니다. 새로운 후보는 학습을 위해 원래 세트에 추가됩니다. 이러한 모든 확장된 순위는 텍스트의 양, 질 및 다양성의 영향을 확인하도록 설계되었습니다. 일반적으로 PRO는 LLM을 n 길이의 인간 선호도 순위에 직접 적용합니다. 후보 간의 다중 위치 및 다차원 대조를 LLM에 제공함으로써 PRO는 모든 길이의 주어진 순위 시퀀스를 완전히 활용합니다. n이 무한대에 가까워짐에 따라 수신자 LLM은 점수가 있는 점점 더 많은 샘플에 노출되고 지속적으로 인간 선호도와 완벽하게 일치해야 합니다. 특히 n = 2일 때 PRO는 쌍별 대조를 사용하여 LLM을 효과적으로 최적화합니다. 자동 보상 모델 점수 매기기, GPT 평가 및 인간 평가를 포함한 수많은 실험을 통해 PRO를 철저히 평가합니다. 주요 관찰 결과는 다음과 같습니다. (1) 2순위로 PRO는 현재 경쟁 기준선을 큰 차이로 능가합니다. 또한 선호도 순위에서 후보자의 높은 질과 다양성은 궁극적인 성과를 위해 중요할 수 있습니다.(2) 길이가 길수록 PRO가 획득할 수 있는 개선 사항이 더 두드러집니다. ChatGPT에서 생성된 응답을 추가하여 순위 길이를 지속적으로 늘리더라도 PRO는 ChatGPT와 유사한 보상 점수를 달성하지만 매개변수는 7B개에 불과합니다.(3) 이기종 후보자는 비교적 동질적인 후보자보다 PRO의 개선 사항을 더 잘 가져올 수 있습니다. 예비 RLHF에 대한 간략한 검토를 제공하는 것으로 시작합니다. LLM이 인간의 선호도와 일치하는 응답을 생성하도록 훈련하기 위해 RLHF는 세 단계로 구성됩니다.(1) 레이블러가 x로 표시된 주어진 프롬프트에 대해 y = Y1,...,t로 표시된 t 토큰을 사용하여 원하는 동작의 응답을 제공하는 지도 미세 조정(SFT). 그 후, 정책 LLM은 이 데이터에 대한 직접적인 미세 조정(최대 우도)을 거쳐 &quot;SFT&quot;로 표시되는 모델이 생성됩니다. (2) 보상 모델(RM) 훈련에서 TSFT는 프롬프트 x에서 응답 쌍을 생성하는 데 사용되고, 이후 인간 레이블러가 이를 다른 응답 y²에 비해 더 선호되는 답변 y¹로 표시합니다. 즉, y¹ &gt; y² | x입니다. 이러한 선호도를 예측하기 위해 이전 연구에서는 Bradley-Terry(BT) RM을 사용하는데, 이는 본질적으로 쌍별 대조를 구성합니다. LRM = - log exp (ro(x, y¹)) ` exp (rò(x, y¹)) + exp (rø(x, y²)) (1) (3) 강화 학습(RL) 단계에서 TSFT는 언어 공간에서 반복적인 샘플링과 RM 및 참조 정책에서 동시에 해당 피드백을 포함하는 시행착오 과정에서 더욱 최적화됩니다. 유감스럽게도 RLHF는 여러 가지 이유로 비판을 받고 있습니다. 단점으로는 감독 학습에 비해 복잡성 증가, 하이퍼파라미터에 대한 민감성, 보상 모델 및 가치 네트워크에 대한 추가 교육 요구 사항이 있습니다. 방법론 이 섹션에서는 먼저 쌍별 대조에서 다중 위치 1대 N 대조를 활용하는 PRO로 전환합니다. 후보 순위가 확장됨에 따라 PRO는 언어 공간에서 더 많은 샘플에 액세스할 수 있으며 긍정적 샘플의 인간이 선호하는 특징을 다른 부정적 샘플과 효율적으로 구별합니다. 전체 프로세스는 SFT 설정에서 완료되므로 RLHF와 관련된 수많은 단점을 피하는 데 도움이 됩니다. 또한 RM과 통합하는 능력에서 PRO의 유연성을 보여 저렴한 선호도 순위 및 차별화된 대조와 같은 장점을 얻습니다. RLHF에서 PRO로 LLM이 점수 대조를 통해 y¹ &gt; y²를 이해하는 데 도움이 되는 Bradley-Terry RM(방정식 1)의 목적을 다시 검토합니다. RM은 감독 환경에서 훈련되며 인간의 선호도를 포착할 것으로 예상됩니다. 주어진 프롬프트 x와 두 응답 y¹과 y²에 대해 RM은 y¹을 선호해야 합니다. 정책 모델, 즉 LLM을 직접 최적화하기 위해 마찬가지로 쌍별 대조를 이에 전송할 수 있습니다. 이런 방식으로 LLM은 RM과 정책 네트워크로 간주되며 r로 표시됩니다. exp (r(x, y¹)) (2) L = -log exp (r(x, y¹)) + exp (r(x, y²)) 당연히 후보 세트를 확장하면 즉 샘플링을 늘리면 r이 더 많은 샷에 도달할 수 있으므로 시행착오 경험이 대체됩니다. 후보 응답 {y}가 n개 있다고 가정할 때 인간이 주석을 단 순서 y¹,,n은 y¹ &gt; y² &gt; &gt;yn입니다. 먼저 y¹과 그 뒤에 있는 응답 간의 부분 순서를 y¹,2:n = y¹ &gt; {y², …..‚y&quot;}로 정의합니다. InfoNCE Loss(He et al. 2020)를 참조하여 다차원 1대 N 대비 방정식 2를 도출합니다. ... L = = = — log exp (r(x, y¹)) Σ²±₁ exp (r(x, y²)) (3) 그러나 y1, 2:n만을 특성화하고 n-2개의 가치 있는 순위를 무시하기 때문에 순위를 완전히 활용하지 못합니다. 맥락 인간: 나비를 정원으로 끌어들이려면 어떻게 해야 하나요? 조수: 어떻게 끌어들이기를 원하시나요? 인간: 나비는 예쁘네요. 조수: 긍정적 LLM G 후보 응답 y y² y³ y&quot; 000 000SFT + p1 &gt; p² ... pr + n 부정적 n-그림 2: 인간 피드백을 위한 PRO 파이프라인 정렬 학습. 각 후보는 먼저 프롬프트와 연결된 다음 LLM에서 처리하여 해당 보상을 추정하고 이는 방정식 5에 의해 최적화됩니다. 예를 들어 y²,3:n 및 yn−1,”. 대신, 재귀적 대조를 적용하여 다중 위치 패턴을 활용할 수 있으며, 이는 첫 번째 응답에서 시작하여 나머지 응답을 부정으로 처리하고 현재 응답을 삭제하고 다음으로 이동합니다. 이 절차는 후보가 더 이상 없을 때까지 반복됩니다. 결과적으로 방정식 3에 대한 추가 확장은 다음과 같습니다. nL = − log II nk=exp (r(x, y)) Σk exp (r(x, y²)) i=k (4) 방정식 4는 인간의 피드백과 포괄적인 정렬을 제공합니다. 인간의 선호도를 준수하는 것 외에도 모델이 유창한 답변을 생성하는 것이 바람직합니다. 따라서 인간이 가장 좋다고 생각하는 응답에 모델이 맞아야 하는 원래의 감독 손실도 통합할 수 있습니다. 그림 2에 표시된 것처럼, 위의 내용을 선호도 순위 최적화(PRO) 알고리즘으로 결론지었습니다. RM에 근접하도록 에이전트를 최적화하는 대신, PRO는 에이전트 LLM이 다음 목적에 의해 직접 훈련되도록 합니다. LPRO(y¹n | x) = L+ BLSFT(5) 여기서 LSFT는 상위 1개 후보의 NLL 손실이고 ẞ는 텍스트 품질과 인간의 선호도 간의 균형을 유지하기 위한 하이퍼 매개변수입니다. 정책 에이전트(RM이라고도 함) г는 PRO에서 &quot;PRO&quot;로 매개변수화됩니다. |yk|TπPRO(x, y) = |yk| log P(x, y) t=(6) PRO와 RLHF는 유사한 목표를 공유합니다. 즉, 레이블이 지정된 샘플에 더 많이 노출되어 인간의 선호도를 이해합니다. 차이점은 RLHF가 시행착오 경험과 쌍대 대조에 의존하는 반면 PRO는 여러 샘플을 긴 순위로 조립하여 학습한다는 것입니다. 이는 더 효율적일 수 있습니다. 놀랍게도 방정식 4는 순위 집계를 위한 고전적 알고리즘인 Plackett-Luce(PL) 모델(Plackett 1975; Luce 2012)과 유사한 공식을 가지고 있습니다. 우연이 아니라고 믿고 PL 모델과 PRO가 유사한 목표를 달성한다고 가정합니다. PL 모델은 이러한 후보에 해당하는 매개변수를 가진 여러 순위를 결합하여 고정된 후보의 글로벌 순위를 얻는 것을 목표로 하는 반면 PRO는 일반적인 인간의 선호도를 학습하는 것을 목표로 하지만 관련 순위에는 서로 다른 n개의 후보가 포함됩니다. 모델링 언어 공간에서 에이전트 LLM의 매개변수는 이론적으로 각 순위에 대한 무한 후보(즉, n = ∞)에 해당해야 합니다. n은 실제로 제한적이지만 클수록 LLM이 더 완벽해야 합니다. 따라서 다음 섹션에서 n에 대한 실험을 구현합니다. RLHF를 PRO에 접목하기 PRO는 RLHF의 보상 모델과 같은 개념을 도입하지 않고도 인간이 주석한 선호도 순위 시퀀스에서 직접 유효할 수 있지만 RLHF를 PRO에 접목하면 PRO에 더 많은 유연성을 제공할 수 있습니다. 다음과 같이 세 가지 가능한 업그레이드를 설명합니다. 저렴한 선호도 순위. PRO는 매우 유연하여 순위가 매겨진 선호도 시퀀스에만 의존합니다. 시퀀스의 소스는 제한이 없으므로 다양한 가능성이 허용됩니다. 한 가지 접근 방식은 주석자에게 서로 다른 품질의 여러 응답을 상상하도록 요청하는 것입니다. 또는 보다 효율적인 방법은 ChatGPT 및 Alpaca와 같은 다양한 기존 LLM을 활용하여 여러 응답을 생성하는 것입니다. 이러한 응답은 RLHF와 유사한 추가 보상 모델 ro를 사용하여 순위를 매길 수 있습니다. 차별화된 대조. 방정식 4에 표시된 공식은 y² &lt; yk인 모든 응답을 y의 부정적 예로 취급하고 동일한 페널티를 적용합니다. 그러나 이러한 접근 방식은 특히 다른 y²의 선호도 점수가 유사한 경우 합리적이지 않을 수 있습니다. 예를 들어, yk+1의 선호도가 yk보다 약간만 나쁜 반면 y&quot;가 yk보다 상당히 나쁜 경우, 모델은 y+1을 약간 처벌하고 yk에 비해 yn을 크게 처벌하는 등 다른 페널티 강도를 구분하여 적용해야 합니다. 이를 해결하기 위해 보상 모델 ro의 점수 ro(x, y²)를 사용하여 y²의 수치적 선호도를 나타내고 방정식 4를 다음과 같이 수정하는 것을 제안합니다. nc=-Σ 10g == k=여기서 Ti&gt;k =k exp TπPRO(x,y) Tk (7) Σε exp TAPRO(x,y)ro(2,gh) – ro(x,y) (8) (9) T = min Th i&gt;k r$(x, yk)와 rø(x, y²)의 차이가 증가하면 yk와 y²의 선호도 격차가 더욱 분명해집니다. 결과적으로 온도 TË가 감소하여 양의 예제 yk의 페널티가 y²인 반면 차이가 작을수록 감소합니다. T는 분자와 분모 사이의 균형을 유지하기 위해 모든 부정적 사례 중 최소 온도로 정의됩니다. 실험(§)에 따르면 동적 온도 설계는 LSFT를 제외하고 LPRO만 최적화할 때 성능을 크게 높입니다. 또한 LPRO와 LSFT를 공동으로 최적화할 때도 일부 성능 이득을 제공합니다. 데이터 준비 실험 실험 데이터 세트로 HH-RLHF Bai et al. (2022a)을 선택했습니다. 여기에는 Harmlessbase, Helpfulɓase, Helpfulonline 및 Helpfulrejection의 4개 하위 세트가 있습니다. 각 샘플에는 인간 주석자가 평가한 두 가지 다른 대화가 포함되어 있으며 훈련/테스트 분할로 그룹화됩니다. 더 긴 순위에 대한 다양한 방법의 성능을 추가로 평가하기 위해 다양한 LLM의 새로운 후보로 각 샘플을 증강하여 순위가 매겨진 응답 범위를 확장합니다. 이러한 증강된 데이터 세트는 HHRLHFLLM,i로 표시되며, 여기서 LLM은 사용된 언어 모델(예: Alpaca, ChatGPT)을 나타내고 i는 순위의 길이입니다. 수정되지 않은 데이터 세트는 HH-RLHFraw라고 합니다. 면책 조항: Curie/ChatGPT의 데이터 증강 및 추론과 다음 GPT-4 평가는 관련 서비스를 사용할 수 있는 경우 완료됩니다. 평가 지표 자동, 모델 기반 및 인간 기반 지표와 같은 다양한 평가 방법을 사용하여 결과를 제시합니다. 구체적으로, BLEU(Papineni et al. 2002)를 사용하여 텍스트 품질을 평가하고 RM을 사용하여 얻은 인간 선호도 수준을 측정합니다. 불공평함을 피하기 위해 훈련 및 평가를 위한 두 개의 다른 RM을 선택합니다. 이를 각각 RM train 및 RMeval이라고 합니다. 이러한 지표를 사용하면 수많은 모델을 자동으로 평가할 수 있습니다. 인간 평가는 인간 선호도를 평가하는 데 있어 황금 표준입니다(Zhou et al. 2023). 주석 작성자 심사위원은 질문과 두 가지 응답을 받고 더 나은 옵션을 결정하거나 동점을 선언하는 작업을 맡습니다.또한 최근 연구에 따르면 GPT-4(OpenAI 2023)는 채팅 도우미의 응답을 효과적으로 평가하고 인간의 선호도와 일치합니다(Zheng et al. 2023; Wang et al. 2023).결과적으로 GPT-4를 참여시켜 두 옵션 중에서 최상의 옵션을 선택합니다.위치 편향을 완화하기 위해(Zheng et al. 2023; Wang et al. 2023) 두 번의 별도 실행 동안 각 후보자를 두 위치에서 평가하고 최종 점수는 두 번의 실행의 평균으로 계산됩니다.구현 세부 정보 인기 있는 LLaMA-7B(Touvron et al. 2023)를 백본 모델로 선택하고 Transformers(Wolf et al. 2020)와 Accelerate(Gugger et al. 2022)를 사용하여 PRO를 구현합니다. 우리는 가중치 SFT 손실인 ẞ를 0.05*(−1)²에 할당합니다. 여기서 l은 순위 길이입니다. 시퀀스 길이, 에포크, 학습률은 각각 512, 2, 5e-6으로 설정되고 추론 중 생성되는 새 토큰의 최대 수는 128이고 전체 배치 크기는 112입니다. 게다가, 각 증강된 데이터 세트의 확장된 후보 순위는 다시 정렬해야 합니다. 그러나 수많은 수동 정렬은 시간이 많이 걸리고 비용이 많이 듭니다. 따라서 우리는 RM train을 사용하여 훈련의 전처리 단계에서 모든 후보 순위를 평가하고 재정렬합니다. 또한, RMeval의 값은 가끔 전체 성능에 과도한 영향을 미치는 극단적인 값을 제공하는 경우 시그모이드 함수를 통해 정규화됩니다. RM train과 RM eval은 모두 오픈 소스 체크포인트를 사용하여 구현됩니다. 자세한 내용은 코드¹에서 확인할 수 있습니다. 주요 실험 우리는 PRO를 여러 LLM(제로샷)과 비교하고 LLaMA-7B 미세 조정을 위한 기준선(Touvron et al. 2023)을 비교합니다. 표 1에 결과가 나와 있습니다. 미세 조정된 각 LLaMA-7B는 인간의 선호도와 특정하게 정렬되지 않은 초기 버전에 비해 BLEU 및 Reward에서 눈에 띄는 개선을 보였습니다. 또한 HH-RLHF에서 미세 조정 없이도 LLM은 여전히 특정 성능을 보일 수 있는 반면, RLHF 훈련을 받은 ChatGLM(Du et al. 2022)과 ChatGPT는 LLaMA-7B, Curie(Brown et al. 2020), Alpaca-7B(Taori et al. 2023)를 이겼습니다. 이 모든 것이 인간 정렬의 중요성을 증명합니다. 다음으로, SFT, RLHF, CoH(Liu, Sferrazza, and Abbeel 2023), DPO(Rafailov et al. 2023) 및 RRHF(Yuan et al. 2023)를 포함하여 LLaMA-7B를 사용하여 동일한 데이터 세트에서 PRO를 강력한 기준선과 비교합니다. 일반적으로 PRO는 모든 기준선보다 성능이 뛰어나거나 상당한 BLEU 점수를 유지하면서 보상 점수 측면에서 경쟁력 있는 성능을 보일 수 있습니다. 구체적으로, 2명의 후보에 대한 순위만 포함된 기본 HH-RLHF에서도 PRO는 SFT에 비해 보상에서 6.52, 성능이 좋은 DPO에 비해 2.6 향상을 달성합니다. CoH(Liu, Sferrazza, and Abbeel 2023)는 더 높은 BLEU 점수를 얻지만 보상에서 PRO에 미치지 못하며 이는 보통입니다. 원시 PRO는 도움성에 비해 무해성 측면에서 뚜렷한 이점을 보여줍니다. 우리는 Harmlessness를 달성하는 것이 비교적 더 쉽다는 사실에 이를 기인합니다. github.com/AlibabaResearch/DAMO-ConvAI/tree/main/PRO 학습 세트 방법 Harmlessbase 도움이 되는 기반 도움이 되는 온라인 도움이 되는 거부 총 BLEU 보상 BLEU 보상 LLAMA Zero-shot Curie 알파카 10.82 51.14.23 50.15.07 53.ChatGLM 15.39 63.ChatGPT 15.51 71.67.BLEU 보상 12.78 31.15.02 38.91 14.60 34.85 13.13 38.17.33 45.51 17.11 51.36 18.99 48.68 16.99 48.19.68 49.80 18.77 55.74 22.21 53.72 19.12 52.20.16 59.14 30.99 61.10 25.41 61.45 21.99 61.21.38 65.94 29.BLEU 리워드 BLEU 리워드 26.68.22.56 68.SFT 15.RLHF 14.COH 13.HH-RLHF 원시 DPO 16.RRHF PRO 55.96 20.55.05 19.45.47 23.54.43 21.13.49 53.98 18.12.05 62.96 20.41.36 29.42.16 28.39.03 33.84 52.50.13 27.73 54.48.23 30.68 56.48.54.25.47.21.80 48.53.25.47.21.19 48.29.79 46.24.06 45.26.53.04 22.52.24.52.20.52.28.75 59.27.53.21.55.BON 16.59.22.54.29.89 61.00 27.58.23.57.RLHF 16.56.23.54.COH 13.47.22.42.HH-RLHF 알파카,DPO 16.37 63.93 21.55.30.54 60.33.17 53.27.84 58.27.58.23.57.28.76 48.23.47.27.53 58.22.59.RRHF PRO 12.79 54.14.19.53.31.59.25.14 56.21.55.62.60 22.54.25.60.26.58.26 22.58.BON RLHF 15.05 67.13.63 61.97 20.20.60.31.64.26.55.28.59.78 24.HH-RLHF ChatGPT,COH DPO RRHF 프로 13.44 56.87 21.15.63 67.81 21.13.02 64.63 18.15.53 73.08 22.51.34.63.14 22.58.26 20.99 58.59.51 28.24 56.35 23.61.86 29.01 61.90 26.39 63.81 22.61.38 31.37 63.26 24.75 63.28 20.64.78 29.35 66.66 27.49 66.95 23.63.55.64.63.67.표 1: 주요 결과. PRO는 모든 미세 조정된 기준선보다 지속적으로 더 많은 보상을 획득하는 반면 ChatGLM 및 ChatGPT에 가깝거나 심지어 이를 초과합니다. 오름차순 67. 랜덤 알파카 63. ChatGPT 60.56. 그림 3: 다양한 길이에 대한 실험 결과. PRO의 경우 표현 스타일을 조정하고 대부분 대화에서 공손함을 유지하는 것과 같은 중요한 기능이 주로 포함되기 때문입니다. 반면에 도움성은 일반적으로 구체적인 제안을 요구하는데, 이는 제한된 세계 지식으로 인해 언어 모델에 더 큰 과제를 제기하여 이 측면에서 어려움을 증가시킵니다. ChatGPT를 사용하여 순위 시퀀스를 확장하고 RM 훈련으로 정렬하면 각 방법의 성능도 증가합니다. 확장된 시퀀스에서 BON(SFT에 대한 가장 높은 보상 모델 점수를 가진 응답 선택)이 경쟁력 있는 기준선이 되는 것을 관찰합니다. 이 발견은 RLHF가 BoN보다 튜닝 효율성이 낮다는 것을 관찰한 Rafailov et al. 2023과 일치합니다. RRHF의 효과성은 주어진 순위의 후보자 간의 쌍별 대조에 의존하기 때문에 덜 두드러집니다. 그것은 방정식 4를 통해 달성할 수 있는 긴 랭킹에서 인간의 선호도에 상응하는 글로벌 차이를 포착하지 못한다. 전반적으로 확장된 랭킹에서 PRO는 가장 경쟁력 있는 방법으로 남아 있으며, 랭킹 증강에 사용된 LLM이 강력할수록 PRO의 개선이 더욱 두드러진다. 이 놀라운 특징은 PRO의 미래 개발에 대한 기대감을 불러일으킨다. 선호도 랭킹 시퀀스 확장의 효과 표 1에서 LLM을 사용하여 HH-RLHF의 랭킹 시퀀스를 길이 2에서 3으로 확장하면 모든 모델의 성능이 향상되는 것을 관찰했습니다. 이를 통해 선호도 랭킹 시퀀스를 더 확장하면 효과가 어떻게 바뀔지 궁금해집니다. 구체적으로, 각각 3개의 추가 응답을 도입하여 선호도 시퀀스를 길이 5로 확장한 다음 보상 모델을 사용하여 다시 랭킹하는 4개의 확장 전략을 시뮬레이션합니다. raw Alpaca: Alpaca-7B를 사용하여 3개의 응답을 생성하고, 각각 1, 2, 3개의 응답을 더하여 길이 3, 4, 5의 랭킹 시퀀스를 형성합니다. ChatGPT: ChatGPT를 사용하여 3개의 응답을 생성하고, 각각 1, 2, 3개의 응답을 더하여 길이 3, 4, 5의 랭킹 시퀀스를 형성합니다. 오름차순: Curie, Alpaca7B, ChatGPT라는 세 가지 LLM을 활용합니다. 표 1의 제로샷 결과를 기준으로 응답의 품질을 ChatGPT &gt; Alpaca-7B &gt; Curie로 순위를 매길 수 있습니다. 이 설정에서 우리는 질의 오름차순으로 응답을 추가합니다.즉, 퀴리의 응답은 길이 3의 순위, 퀴리와 알파카-7B의 응답은 길이 4의 순위, 퀴리, 알파카-7B, ChatGPT의 응답은 길이 5의 순위입니다.무작위: 응답 추가 순서는 응답 질과 관련이 없으며 무작위로 수행됩니다.그림 3은 다양한 길이의 시퀀스를 확장한 후 PRO의 효과성에 대한 다양한 확장 전략의 영향을 보여줍니다.우리의 관찰 결과는 다음과 같습니다.더 긴 순위, 더 나은 결과: 전반적으로 긴 순위 시퀀스는 일반적으로 대부분의 전략에 대한 성과가 향상됩니다.더 긴 순위는 더 많은 후보를 포함하기 때문입니다.이는 피드백 레이블이 있는 언어 공간에서 더 많은 샘플링이 LLM이 인간의 선호도와 일치하도록 효과적으로 도움이 됨을 의미합니다.이는 비교적 쉽게 얻을 수 있는 잘 수행된 RM의 경우 순위를 확장하는 것이 새로운 프롬프트에 대한 브레인스토밍에 비해 간단한 작업이 될 수 있기 때문에 흥미로운 발견입니다. 더 나은 추가 응답, 더 나은 결과: 단일 모델을 사용하여 추가 응답을 생성하는 경우, Alpaca와 같이 품질이 평균일 때 하나의 응답을 보완하는 것으로 충분하며, 더 많은 응답을 추가해도 제한적인 개선이 제공됩니다. 그러나 ChatGPT와 같이 응답 품질이 높을 때 더 많은 응답을 추가하면 일관된 성능 향상이 이루어집니다. 이는 잠재적으로 미래의 Human Alignment 알고리즘 설계에 대한 새로운 통찰력을 제공할 수 있습니다. 더 다양한 추가 응답, 더 나은 결과: 낮은 품질의 응답을 통합하면 고품질 응답만 사용하는 것에 비해 LLM을 더 잘 개선할 수 있습니다. 흥미롭게도 시퀀스 길이가 4일 때 오름차순(파란색 선) = Curie + Alpaca는 Curie의 응답 품질이 Alpaca만큼 좋지 않음에도 불구하고 Alpaca(녹색 선) Alpaca Alpaca의 성능을 능가합니다. 우리는 다양한 응답이 부정적인 예일지라도 언어 모델이 피해야 할 행동을 더 잘 인식하도록 돕고, 그 결과 전반적인 성능을 향상시킨다는 사실에 기인합니다. 마지막으로 Curie, Alpaca, ChatGPT를 결합함으로써 세 개의 ChatGPT 응답을 사용하는 것과 가까운 성능을 달성하여 &quot;두 개의 머리가 하나보다 낫다&quot;는 속담이 사실임을 입증했습니다.인간과 GPT-4 평가 인간의 선호도를 포착하는 데 왜곡이 있을 수 있는 보상 모델과 비교할 때 인간 주석은 가장 정확한 평가 방법으로 간주되며 최근에는 GPT-4-as-a-judge가 인간의 선호도를 빠르게 평가하기 위한 확장 가능한 접근 방식으로 등장했습니다.PRO가 인간의 선호도를 실제로 포착하는지 확인하기 위해 GPT-4와 인간이 모두 수행한 포괄적인 평가를 제공합니다.여기서 우리는 데이터 세트에서 제공한 첫 번째 후보인 PRO 대 Golden의 성능을 조사합니다.자세히 말하면, HH-RLHF 원시에서 훈련된 PRO가 PRO의 기능을 완전히 활용하지 못하는 길이 2의 순위 시퀀스를 포함하는 원시 데이터 세트에서 제공하는 인간이 선호하는 응답을 달성하거나 능가할 수 있는지 확인하는 것을 목표로 합니다.반면에 이 비교는 우리가 평가에 사용하는 보상 모델의 타당성에 대한 어느 정도 증거 역할을 합니다. 평가자 하위 집합 동점 패배 승리 무해기반 도움됨 기반 GPT-도움됨 온라인 도움됨 거부 평균 60.00 5.00 35.77.50 0.00 22.27.50 12.50 60.55.00 0.00 45.55.00 4.37 40.인간 무해기반 도움됨 기반 도움됨 온라인 도움됨 거부 평균 20.00 60.00 20.20.00 50.00 30.30.00 60.00 10.22.50 56.25 21.20.00 55.00 25.표 2: GPT-4 및 인간 평가 결과. GPT-4 평가를 위해 먼저 테스트 세트에서 컨텍스트를 샘플링합니다. 우리는 PRO와 그 대응물의 두 가지 상응하는 응답을 Zheng et al. (2023)의 프롬프트 템플릿의 수정된 버전으로 조립하여 GPT-4 점수를 매겼습니다. 또한 Wang et al. (2023)을 참조하여 각각 이진 방향으로 두 후보를 제공하여 순서에 의해 유발되는 불공정성을 제거했습니다. 인간 평가의 경우, 우리는 3명의 주석자를 사용하여 GPT-4 평가로 동일한 샘플을 추정하고 두 개의 셔플링된 응답 간에 하나를 직접 구별했습니다. 표 2는 자세한 결과를 제공하며, GPT-4와 인간 모두 PRO를 전반적으로 더 많이 지원하여 PRO의 강점을 강조합니다. 이는 PRO가 주석이 달린 데이터에 반영된 인간의 선호도를 효과적으로 포착할 수 있음을 시사합니다. 또한 보상 모델을 사용한 평가는 일관된 결과를 얻었으며, 인간과 GPT-4 모두 PRO를 상당히 선호했습니다. 이는 PRO의 효과를 재확인할 뿐만 아니라 보상 모델이 인간의 선호도를 합리적으로 평가할 수 있음을 보여줍니다. 절제 연구 이 부분에서는 PRO의 각 부분의 효과를 조사합니다. 표 3에 결과가 나와 있습니다. SFT 손실 모델이 텍스트 품질을 희생하고 보상 모델에만 맞춰지는 것을 피하기 위해 LSFT를 도입했습니다.따라서 LSFT를 제거하면 BLEU 점수가 낮아집니다.PRO 손실 표 1은 또한 LPRO의 영향을 보여줍니다.PRO에서 LPRO를 제외하면 본질적으로 보상이 낮은 SFT(BON)와 같습니다.적절한 순위 랭킹 y¹¨*를 최대한 활용하기 위해 n― 1 손실 함수를 사용하여 y1, 2:n, y2,3:n yn-1,n을 모델링합니다.저희의 목표는 모든 순위 순서를 적절하게 모델링하고 LLM이 서로 다른 선호도의 샘플을 더 잘 구별할 수 있도록 하는 것입니다.이 아이디어를 검증하기 위해 첫 번째 항인 L½ PRO를 제외하고 LPRO를 비활성화합니다.실험 결과 BLEU 및 보상 점수가 모두 감소하여 방정식 4의 효과가 확인되었습니다.온도 T는 전반적인 성능을 약간 향상시킵니다.그러나 LSFT와 T를 동시에 제거하면 성능이 크게 떨어지는 반면, 둘 중 하나를 개별적으로 제거하면 눈에 띄는 영향이 없었습니다. 우리는 온도가 Traning Set Harmlessbase Method Helpfulbase Helpfulonline Helpfulrejection Total BLEU Reward BLEU Reward PRO HH-RLHF raw -LSFT -T -LSFT T 12.6.12.0.PRO 14.-Lk&gt;13.62.62.HH-RLHF Alpaca,-LSFT 9.-T -LSFT - T 13.0.BLEU Reward 62.96 20.83 48.51 28.75 59.02 27.17 53.67.20 10.37 46.60 11.17 49.33 11.62.91 20.63 47.92 28.73 58.52 26.52.81 6.74 42.97 6.37에 도움이 되기 때문이라고 믿습니다. 42.84 6.22.47 54.38 25.60.90 26.82 58.21.50 53.48 24.56 60.32 25.81 57.65.78 18.77 54.18 23.23.33 58.63.40 21.70 53.77 24.84 60.36 26.01 57.55.90 2.13 23.41 3.56 23.44 2.66 23.BLEU 리워드 BLEU 리워드 21.54 55.48.9.53.53.44.21.55.5.46.22.58.21.10 58.62.18.59.21.58.2.32.PRO -Lk&gt;HH-RLHF 채팅GPT,-LSFT -T -LSFT -T 15.53 73.08 22.30 64.15.20 72.64 21.94 64.13.81 73.18 21.15.77 72.99 22.5.93 69.61 5.29.29.64.27.65.29.33.9.66.66 27.66.97 27.67.15 26.67.48 27.31.81 6.66.95 23.67.66.80 22.67.66.76 21.67.54 22.33.52 6.67.68.43.표 3: 절제 결과. L, LSFT 및 동적 온도 T의 효과를 조사합니다. LLM은 일부 부정적인 예가 중립적(긍정적인 예와 유사한 보상 점수)이라는 것을 이해하므로 LLM 훈련 중 혼란을 피하기 위해 과도하게 처벌해서는 안 됩니다. LSFT를 포함하면 최상의 응답의 가중치가 증가하여 비슷한 역할을 합니다.
--- RELATED WORK ---
인간 피드백을 통한 강화 학습 인간의 선호도에 맞춰 LLM을 미세 조정하는 것은 중요한 연구 문제로 떠올랐습니다. 더 자세한 레이블 없이 인간 주석자가 순위를 매기거나 점수를 매긴 컨텍스트와 해당 접미사로 공식화할 수 있으며, 에이전트는 인간의 선호도를 학습하고 인간과 유사한 결과를 제공해야 합니다. 강화 학습(RL)은 이 목표를 달성하는 가장 간단한 방법이 될 수 있습니다. 에이전트는 보상 모델에서 인간 대리자로서 감독 신호만 희소하게 받고, RL 프레임워크, 즉 인간 피드백을 통한 강화 학습(RLHF)에서 수많은 시도를 통해 수정됩니다. 이 경로에 대해 많은 탐구가 수행되었습니다(Christiano 외. 2017; MacGlashan 외. 2017; Warnell 외. 2018; Ziegler 외. 2019; Stiennon 외. 2020; Nakano 외. 2021; Lee, Smith, and Abbeel 2021; Lei 외. 2022; Snell 외. 2022; Bai et al. 2022a; Zhu, Jiao et al. 2023; Stiennonet al. (2020) 및 Nakano et al. (2021) RLHF 조사
--- METHOD ---
s는 의미적 또는 스칼라 관점에서 쌍별 대조에 주의를 기울인다. (2) 더 긴 순위가 사용 가능하더라도 쌍으로 자르는 경향이 있어 거시적 관점에서 후보를 구별하지 못한다. 이 연구에서 우리는 언어 공간에서 샘플링을 확대하는 것이 인간 정렬에 미치는 영향을 철저히 조사한다. 이 시나리오를 바탕으로 우리는 직접 정책 최적화의 효율적인 프레임워크로 선호도 순위 최적화(PRO)를 제안한다. 그림 1은 PRO가 다른 SFT 기반 공식과 어떻게 다른지 보여준다. 구체적으로, 우리는 RLHF의 본질을 재고하고 Bradley-Terry 모델(Bradley 및 Terry 1952)의 쌍별 대조를 확장하여 임의 길이의 순위 내에서 1대 N 대조를 포함한다. 그런 다음 입력 프롬프트 x와 인간이 순위를 매긴 응답 집합(y¹, y², …..‚y”로 표현)이 주어지면 제안된 PRO 알고리즘은 에이전트 LLM을 조정하여 최상의 응답 y¹을 양수로, 나머지를 음수로 처리한다. 이 우선순위는 LLM이 인간이 열등하다고 생각하는 것보다 더 높은 가능성으로 y¹를 생성한다는 것을 시사합니다. 그런 다음 현재 최상위 응답을 반복적으로 무시하고 다음 응답을 나머지에 대한 긍정적으로 간주하여 순위에 더 이상 응답이 없을 때까지 진행합니다. 더 많은 후보를 확보하는 데 집중하는 것 외에도 특히 다양한 수준의 프록시를 배포하여 다양한 품질과 인간 정렬의 정도를 가진 발화를 샘플링합니다. RLHF에서 영감을 받아 수신자 LLM에서 새로운 후보를 동적으로 샘플링하고 추가 보상 모델로 레이블을 지정하는 셀프 부트스트래핑 방법도 추가합니다. 새로운 후보는 학습을 위해 원래 세트에 추가됩니다. 이러한 모든 확장된 순위는 텍스트의 양, 품질 및 다양성의 영향을 확인하도록 설계되었습니다. 일반적으로 PRO는 LLM을 n 길이의 인간 선호도 순위에 직접 적용합니다. 후보 간의 다중 위치 및 다차원 대조를 LLM에 제공함으로써 PRO는 모든 길이의 주어진 순위 시퀀스를 완전히 활용합니다. n이 무한대에 가까워짐에 따라 수신자 LLM은 점수가 있는 점점 더 많은 샘플에 노출되고 지속적으로 인간의 선호도와 완벽하게 일치해야 합니다. 특히 n = 2일 때 PRO는 쌍대 대조를 사용하여 LLM을 효과적으로 최적화합니다. 우리는 수많은
--- EXPERIMENT ---
s는 PRO가 기준 알고리즘보다 성능이 뛰어나 ChatGPT 및 자동 기반, 보상 기반, GPT-4 및 인간 평가를 통한 인간 응답과 비슷한 결과를 달성한다는 것을 보여주었습니다. 서론 대규모 언어 모델(LLM)은 사용자의 다양한 정보 요구를 충족하는 데 놀라운 역량을 보여주었습니다(Brown et al. 2020; Chowdhery et al. 2022; Bubeck et al. 2023; Touvron et al. 2023; Li et al. 2023). 1조 개의 토큰 사전 학습 코퍼스에 인코딩된 광범위한 글로벌 지식과 인간 행동을 활용함에도 불구하고 LLM은 그 안에 포함된 오해의 소지가 있고 유해하며 해로운 콘텐츠의 존재로 인해 불가피하게 영향을 받습니다(Bai et al. 2022b; Ouyang et al. 2022). 결과적으로, 인간의 피드백을 통한 강화 학습(RLHF)이 도입되어 안전하고 관리하기 쉬운 AI 시스템(Stiennon et al. 2020; Xue et al. 2023; Peng et al. 2023)을 구축하여 인간이 순위를 매긴 후보 집합에 따라 LLM의 언어 공간을 인간의 가치에 맞춥니다. 그럼에도 불구하고 RLHF는 지도 학습보다 더 복잡하고 최적화 불안정성에 취약하며 하이퍼 매개변수에 민감합니다. 이러한 제한은 주로 *연락 저자에서 발생합니다. 저작권 2024, 인공 지능 진흥 협회(www.aaai.org). 모든 권리 보유. ONE y LLM PAIRy₁ y² LLM PAIRS ya LLMy² PRO y³ y LLM y 그림 1: 다양한 SFT 패러다임 간 비교. PRO는 다중 위치 및 다차원 대비를 위해 다른 패러다임과 구별됩니다. 에이전트를 고용하여 즉, LLM이 인간의 선호도에 직접 맞추는 대신 반복적인 시행착오를 경험하게 합니다. 따라서 RLHF를 대체할 보다 직접적인 최적화의 가능성으로 감독 미세 조정(SFT)이 예상됩니다. SFT는 처음에는 RLHF의 워밍업 프로세스로만 사용되며, 여기서 최상의 후보가 선택되어 LLM을 인간이 선호하는 데이터에 맞게 조정합니다. 최근 연구에서는 SFT를 향상시키기 위한 보다 복잡한 전략을 제안했습니다(Rafailov et al. 2023; Wu et al. 2023; Yuan et al. 2023; Dong et al. 2023). 약간의 진전에도 불구하고 개선의 여지가 여전히 있습니다. (1) RLHF가 좋은 성능을 발휘하도록 하는 본질은 무시됩니다. 즉, 훈련 중에 광범위한 언어 공간에서 점수를 매기는 다중 샘플링입니다. 주어진 순위 길이에 의해 제약을 받는 대부분의 방법은 의미적 또는 스칼라 관점에서 쌍별 대조에 주의를 기울입니다. (2) 더 긴 순위가 가능하더라도, 그들은 그것을 쌍으로 자르는 경향이 있어 거시적 관점에서 후보자를 구별하지 못합니다. 이 작업에서 우리는 언어 공간에서 샘플링을 확대하는 것이 인간의 정렬에 미치는 영향을 철저히 조사합니다. 이 시나리오를 기반으로, 우리는 직접 정책 최적화의 효율적인 프레임워크로서 선호도 순위 최적화(PRO)를 제안합니다. 그림 1은 PRO가 다른 SFT 기반 공식과 어떻게 다른지 보여줍니다. 구체적으로, 우리는 RLHF의 본질을 재고하고 Bradley-Terry 모델(Bradley 및 Terry 1952)의 쌍별 대조를 확장하여 임의 길이의 순위 내에서 1대 N 대조를 포함합니다. 그런 다음 입력 프롬프트 x와 인간이 순위를 매긴 응답 집합(y¹, y², …..‚y”로 표현)이 주어지면, 제안된 PRO 알고리즘은 에이전트 LLM을 조정하여 최상의 응답 y¹을 양수로, 나머지를 음수로 처리합니다. 이 우선순위는 LLM이 인간이 열등하다고 생각하는 응답보다 더 높은 가능성으로 y¹을 생성함을 시사합니다. 그런 다음 현재 최상위 응답을 반복적으로 무시하고 다음 응답을 나머지에 대한 긍정적 응답으로 간주하여 순위에 더 이상 응답이 없을 때까지 진행합니다. 더 많은 후보자를 확보하는 데 중점을 두는 것 외에도 특히 다양한 품질과 인간적 정렬의 정도를 가진 발화를 샘플링하기 위해 다양한 수준의 프록시를 배포합니다. RLHF에서 영감을 받아 수신자 LLM에서 새로운 후보자를 동적으로 샘플링하고 추가 보상 모델로 레이블을 지정하는 셀프 부트스트래핑 방법도 추가합니다. 새로운 후보자는 학습을 위해 원래 세트에 추가됩니다. 이러한 모든 확장된 순위는 텍스트의 양, 품질 및 다양성의 영향을 확인하도록 설계되었습니다. 일반적으로 PRO는 LLM을 n 길이의 인간 선호도 순위에 직접 적용합니다. 후보자 간의 다중 위치 및 다차원 대조를 LLM에 제공함으로써 PRO는 모든 길이의 주어진 순위 시퀀스를 완전히 활용합니다. n이 무한대에 가까워짐에 따라 수신자 LLM은 점수가 있는 점점 더 많은 샘플에 노출되고 지속적으로 인간 선호도와 완벽하게 일치해야 합니다. 특히 n = 2일 때 PRO는 쌍별 대조를 사용하여 LLM을 효과적으로 최적화합니다. 우리는 자동 보상 모델 점수 매기기, GPT 평가, 인간 평가를 포함한 수많은 실험을 통해 PRO를 철저히 평가합니다. 주요 관찰 결과는 다음과 같습니다. (1) 2순위로 PRO는 현재 경쟁적 기준선을 큰 차이로 능가합니다. 또한 선호도 순위에서 후보자의 높은 품질과 다양성은 궁극적인 성과에 중요할 수 있습니다. (2) 길이가 길수록 PRO가 얻을 수 있는 개선 사항이 더 두드러집니다. ChatGPT에서 생성된 응답을 추가하여 순위 길이를 지속적으로 늘리더라도 PRO는 ChatGPT와 유사한 보상 점수를 달성하지만 매개변수는 7B에 불과합니다. (3) 이기종 후보는 비교적 동질적인 후보보다 PRO를 더 잘 개선할 수 있습니다. 예비 RLHF에 대한 간략한 검토로 시작합니다. LLM이 인간의 선호도에 맞춰 응답을 생성하도록 훈련하기 위해 RLHF는 세 단계로 구성됩니다. (1) 레이블러가 x로 표시된 주어진 프롬프트에 대해 t 토큰(y = Y1,...,t로 표시)으로 원하는 동작의 응답을 제공하는 지도 미세 조정(SFT). 그 후, 정책 LLM은 이 데이터에 대한 직접적인 미세 조정(최대 우도)을 거쳐 &quot;SFT&quot;로 표시되는 모델이 생성됩니다. (2) 보상 모델(RM) 훈련에서 TSFT는 프롬프트 x에서 응답 쌍을 생성하는 데 사용되고, 이후 인간 레이블러가 이를 다른 응답 y²에 비해 더 선호되는 답변 y¹로 표시합니다. 즉, y¹ &gt; y² | x입니다. 이러한 선호도를 예측하기 위해 이전 연구에서는 Bradley-Terry(BT) RM을 사용하는데, 이는 본질적으로 쌍별 대조를 구성합니다. LRM = - log exp (ro(x, y¹)) ` exp (rò(x, y¹)) + exp (rø(x, y²)) (1) (3) 강화 학습(RL) 단계에서 TSFT는 언어 공간에서 반복적인 샘플링과 RM 및 참조 정책에서 동시에 해당 피드백을 포함하는 시행착오 과정에서 더욱 최적화됩니다. 유감스럽게도 RLHF는 여러 가지 이유로 비판을 받고 있습니다. 단점으로는 감독 학습에 비해 복잡성 증가, 하이퍼파라미터에 대한 민감성, 보상 모델 및 가치 네트워크에 대한 추가 교육 요구 사항이 있습니다. 방법론 이 섹션에서는 먼저 쌍별 대조에서 다중 위치 1대 N 대조를 활용하는 PRO로 전환합니다. 후보 순위가 확장됨에 따라 PRO는 언어 공간에서 더 많은 샘플에 액세스할 수 있으며 긍정적 샘플의 인간이 선호하는 특징을 다른 부정적 샘플과 효율적으로 구별합니다. 전체 프로세스는 SFT 설정에서 완료되므로 RLHF와 관련된 수많은 단점을 피하는 데 도움이 됩니다. 또한 RM과 통합하는 능력에서 PRO의 유연성을 보여 저렴한 선호도 순위 및 차별화된 대조와 같은 장점을 얻습니다. RLHF에서 PRO로 LLM이 점수 대조를 통해 y¹ &gt; y²를 이해하는 데 도움이 되는 Bradley-Terry RM(방정식 1)의 목적을 다시 검토합니다. RM은 감독 환경에서 훈련되며 인간의 선호도를 포착할 것으로 예상됩니다. 주어진 프롬프트 x와 두 응답 y¹과 y²에 대해 RM은 y¹을 선호해야 합니다. 정책 모델, 즉 LLM을 직접 최적화하기 위해 마찬가지로 쌍별 대조를 이에 전송할 수 있습니다. 이런 방식으로 LLM은 RM과 정책 네트워크로 간주되며 r로 표시됩니다. exp (r(x, y¹)) (2) L = -log exp (r(x, y¹)) + exp (r(x, y²)) 당연히 후보 세트를 확장하면 즉 샘플링을 늘리면 r이 더 많은 샷에 도달할 수 있으므로 시행착오 경험이 대체됩니다. 후보 응답 {y}가 n개 있다고 가정할 때 인간이 주석을 단 순서 y¹,,n은 y¹ &gt; y² &gt; &gt;yn입니다. 먼저 y¹과 그 뒤에 있는 응답 간의 부분 순서를 y¹,2:n = y¹ &gt; {y², …..‚y&quot;}로 정의합니다. InfoNCE Loss(He et al. 2020)를 참조하여 다차원 1대 N 대비 방정식 2를 도출합니다. ... L = = = — log exp (r(x, y¹)) Σ²±₁ exp (r(x, y²)) (3) 그러나 y1, 2:n만을 특성화하고 n-2개의 가치 있는 순위를 무시하기 때문에 순위를 완전히 활용하지 못합니다. 맥락 인간: 나비를 정원으로 끌어들이려면 어떻게 해야 하나요? 조수: 어떻게 끌어들이기를 원하시나요? 인간: 나비는 예쁘네요. 조수: 긍정적 LLM G 후보 응답 y y² y³ y&quot; 000 000SFT + p1 &gt; p² ... pr + n 부정적 n-그림 2: 인간 피드백을 위한 PRO 파이프라인 정렬 학습. 각 후보는 먼저 프롬프트와 연결된 다음 LLM에서 처리하여 해당 보상을 추정하고 이는 방정식 5에 의해 최적화됩니다. 예를 들어 y²,3:n 및 yn−1,”. 대신, 재귀적 대조를 적용하여 다중 위치 패턴을 활용할 수 있으며, 이는 첫 번째 응답에서 시작하여 나머지 응답을 부정으로 처리하고 현재 응답을 삭제하고 다음으로 이동합니다. 이 절차는 후보가 더 이상 없을 때까지 반복됩니다. 결과적으로 방정식 3에 대한 추가 확장은 다음과 같습니다. nL = − log II nk=exp (r(x, y)) Σk exp (r(x, y²)) i=k (4) 방정식 4는 인간의 피드백과 포괄적인 정렬을 제공합니다. 인간의 선호도를 준수하는 것 외에도 모델이 유창한 답변을 생성하는 것이 바람직합니다. 따라서 인간이 가장 좋다고 생각하는 응답에 모델이 맞아야 하는 원래의 감독 손실도 통합할 수 있습니다. 그림 2에 표시된 것처럼, 위의 내용을 선호도 순위 최적화(PRO) 알고리즘으로 결론지었습니다. RM에 근접하도록 에이전트를 최적화하는 대신, PRO는 에이전트 LLM이 다음 목적에 의해 직접 훈련되도록 합니다. LPRO(y¹n | x) = L+ BLSFT(5) 여기서 LSFT는 상위 1개 후보의 NLL 손실이고 ẞ는 텍스트 품질과 인간의 선호도 간의 균형을 유지하기 위한 하이퍼 매개변수입니다. 정책 에이전트(RM이라고도 함) г는 PRO에서 &quot;PRO로 매개변수화됩니다. |yk|TπPRO(x, y) = |yk| log P(x, y) t=(6) PRO와 RLHF는 유사한 목표를 공유합니다. 즉, 레이블이 지정된 샘플에 더 많이 노출되어 인간의 선호도를 이해합니다. 차이점은 RLHF가 시행착오 경험과 쌍대 대조에 의존하는 반면 PRO는 여러 샘플을 긴 순위로 조립하여 학습한다는 것입니다. 이는 더 효율적일 수 있습니다. 놀랍게도 방정식 4는 순위 집계를 위한 고전적 알고리즘인 Plackett-Luce(PL) 모델(Plackett 1975; Luce 2012)과 유사한 공식을 가지고 있습니다. 우연이 아니라고 믿고 PL 모델과 PRO가 유사한 목표를 달성한다고 가정합니다. PL 모델은 이러한 후보에 해당하는 매개변수를 가진 여러 순위를 결합하여 고정된 후보의 글로벌 순위를 얻는 것을 목표로 하는 반면 PRO는 일반적인 인간의 선호도를 학습하는 것을 목표로 하지만 관련 순위에는 서로 다른 n개의 후보가 포함됩니다. 모델링 언어 공간에서 에이전트 LLM의 매개변수는 이론적으로 각 순위에 대한 무한 후보(즉, n = ∞)에 해당해야 합니다. n은 실제로 제한적이지만 클수록 LLM이 더 완벽해야 합니다. 따라서 다음 섹션에서 n에 대한 실험을 구현합니다. RLHF를 PRO에 접목하기 PRO는 RLHF의 보상 모델과 같은 개념을 도입하지 않고도 인간이 주석한 선호도 순위 시퀀스에서 직접 유효할 수 있지만 RLHF를 PRO에 접목하면 PRO에 더 많은 유연성을 제공할 수 있습니다. 다음과 같이 세 가지 가능한 업그레이드를 설명합니다. 저렴한 선호도 순위. PRO는 매우 유연하여 순위가 매겨진 선호도 시퀀스에만 의존합니다. 시퀀스의 소스는 제한이 없으므로 다양한 가능성이 허용됩니다. 한 가지 접근 방식은 주석자에게 서로 다른 품질의 여러 응답을 상상하도록 요청하는 것입니다. 또는 보다 효율적인 방법은 ChatGPT 및 Alpaca와 같은 다양한 기존 LLM을 활용하여 여러 응답을 생성하는 것입니다. 이러한 응답은 RLHF와 유사한 추가 보상 모델 ro를 사용하여 순위를 매길 수 있습니다. 차별화된 대조. 방정식 4에 표시된 공식은 y² &lt; yk인 모든 응답을 y의 부정적 예로 취급하고 동일한 페널티를 적용합니다. 그러나 이러한 접근 방식은 특히 다른 y²의 선호도 점수가 유사한 경우 합리적이지 않을 수 있습니다. 예를 들어, yk+1의 선호도가 yk보다 약간만 나쁜 반면 y&quot;가 yk보다 상당히 나쁜 경우, 모델은 y+1을 약간 처벌하고 yk에 비해 yn을 크게 처벌하는 등 다른 페널티 강도를 구분하여 적용해야 합니다. 이를 해결하기 위해 보상 모델 ro의 점수 ro(x, y²)를 사용하여 y²의 수치적 선호도를 나타내고 방정식 4를 다음과 같이 수정하는 것을 제안합니다. nc=-Σ 10g == k=여기서 Ti&gt;k =k exp TπPRO(x,y) Tk (7) Σε exp TAPRO(x,y)ro(2,gh) – ro(x,y) (8) (9) T = min Th i&gt;k r$(x, yk)와 rø(x, y²)의 차이가 증가하면 yk와 y²의 선호도 격차가 더욱 분명해집니다. 결과적으로 온도 TË가 감소하여 양의 예제 yk의 페널티가 y²인 반면 차이가 작을수록 감소합니다. T는 분자와 분모 사이의 균형을 유지하기 위해 모든 부정적 사례 중 최소 온도로 정의됩니다. 실험(§)에 따르면 동적 온도 설계는 LSFT를 제외하고 LPRO만 최적화할 때 성능을 크게 높입니다. 또한 LPRO와 LSFT를 공동으로 최적화할 때도 일부 성능 이득을 제공합니다. 데이터 준비 실험 실험 데이터 세트로 HH-RLHF Bai et al. (2022a)을 선택했습니다. 여기에는 Harmlessbase, Helpfulɓase, Helpfulonline 및 Helpfulrejection의 4개 하위 세트가 있습니다. 각 샘플에는 인간 주석자가 평가한 두 가지 다른 대화가 포함되어 있으며 훈련/테스트 분할로 그룹화됩니다. 더 긴 순위에 대한 다양한 방법의 성능을 추가로 평가하기 위해 다양한 LLM의 새로운 후보로 각 샘플을 증강하여 순위가 매겨진 응답 범위를 확장합니다. 이러한 증강된 데이터 세트는 HHRLHFLLM,i로 표시되며, 여기서 LLM은 사용된 언어 모델(예: Alpaca, ChatGPT)을 나타내고 i는 순위의 길이입니다. 수정되지 않은 데이터 세트는 HH-RLHFraw라고 합니다. 면책 조항: Curie/ChatGPT의 데이터 증강 및 추론과 다음 GPT-4 평가는 관련 서비스를 사용할 수 있는 경우 완료됩니다. 평가 지표 자동, 모델 기반 및 인간 기반 지표와 같은 다양한 평가 방법을 사용하여 결과를 제시합니다. 구체적으로, BLEU(Papineni et al. 2002)를 사용하여 텍스트 품질을 평가하고 RM을 사용하여 얻은 인간 선호도 수준을 측정합니다. 불공평함을 피하기 위해 훈련 및 평가를 위한 두 개의 다른 RM을 선택합니다. 이를 각각 RM train 및 RMeval이라고 합니다. 이러한 지표를 사용하면 수많은 모델을 자동으로 평가할 수 있습니다. 인간 평가는 인간 선호도를 평가하는 데 있어 황금 표준입니다(Zhou et al. 2023). 주석 작성자 심사위원은 질문과 두 가지 응답을 받고 더 나은 옵션을 결정하거나 동점을 선언하는 작업을 맡습니다.또한 최근 연구에 따르면 GPT-4(OpenAI 2023)는 채팅 도우미의 응답을 효과적으로 평가하고 인간의 선호도와 일치합니다(Zheng et al. 2023; Wang et al. 2023).결과적으로 GPT-4를 참여시켜 두 옵션 중에서 최상의 옵션을 선택합니다.위치 편향을 완화하기 위해(Zheng et al. 2023; Wang et al. 2023) 두 번의 별도 실행 동안 각 후보자를 두 위치에서 평가하고 최종 점수는 두 번의 실행의 평균으로 계산됩니다.구현 세부 정보 인기 있는 LLaMA-7B(Touvron et al. 2023)를 백본 모델로 선택하고 Transformers(Wolf et al. 2020)와 Accelerate(Gugger et al. 2022)를 사용하여 PRO를 구현합니다. 우리는 가중치 SFT 손실인 ẞ를 0.05*(−1)²에 할당합니다. 여기서 l은 순위 길이입니다. 시퀀스 길이, 에포크, 학습률은 각각 512, 2, 5e-6으로 설정되고 추론 중 생성되는 새 토큰의 최대 수는 128이고 전체 배치 크기는 112입니다. 게다가, 각 증강된 데이터 세트의 확장된 후보 순위는 다시 정렬해야 합니다. 그러나 수많은 수동 정렬은 시간이 많이 걸리고 비용이 많이 듭니다. 따라서 우리는 RM train을 사용하여 훈련의 전처리 단계에서 모든 후보 순위를 평가하고 재정렬합니다. 또한, RMeval의 값은 가끔 전체 성능에 과도한 영향을 미치는 극단적인 값을 제공하는 경우 시그모이드 함수를 통해 정규화됩니다. RM train과 RM eval은 모두 오픈 소스 체크포인트를 사용하여 구현됩니다. 자세한 내용은 코드¹에서 확인할 수 있습니다. 주요 실험 우리는 PRO를 여러 LLM(제로샷)과 비교하고 LLaMA-7B 미세 조정을 위한 기준선(Touvron et al. 2023)을 비교합니다. 표 1에 결과가 나와 있습니다. 미세 조정된 각 LLaMA-7B는 인간의 선호도와 특정하게 정렬되지 않은 초기 버전에 비해 BLEU 및 Reward에서 눈에 띄는 개선을 보였습니다. 또한 HH-RLHF에서 미세 조정 없이도 LLM은 여전히 특정 성능을 보일 수 있는 반면, RLHF 훈련을 받은 ChatGLM(Du et al. 2022)과 ChatGPT는 LLaMA-7B, Curie(Brown et al. 2020), Alpaca-7B(Taori et al. 2023)를 이겼습니다. 이 모든 것이 인간 정렬의 중요성을 증명합니다. 다음으로, SFT, RLHF, CoH(Liu, Sferrazza, and Abbeel 2023), DPO(Rafailov et al. 2023) 및 RRHF(Yuan et al. 2023)를 포함하여 LLaMA-7B를 사용하여 동일한 데이터 세트에서 PRO를 강력한 기준선과 비교합니다. 일반적으로 PRO는 모든 기준선보다 성능이 뛰어나거나 상당한 BLEU 점수를 유지하면서 보상 점수 측면에서 경쟁력 있는 성능을 보일 수 있습니다. 구체적으로, 2명의 후보에 대한 순위만 포함된 기본 HH-RLHF에서도 PRO는 SFT에 비해 보상에서 6.52, 성능이 좋은 DPO에 비해 2.6 향상을 달성합니다. CoH(Liu, Sferrazza, and Abbeel 2023)는 더 높은 BLEU 점수를 얻지만 보상에서 PRO에 미치지 못하며 이는 보통입니다. 원시 PRO는 도움성에 비해 무해성 측면에서 뚜렷한 이점을 보여줍니다. 우리는 Harmlessness를 달성하는 것이 비교적 더 쉽다는 사실에 이를 기인합니다. github.com/AlibabaResearch/DAMO-ConvAI/tree/main/PRO 학습 세트 방법 Harmlessbase 도움이 되는 기반 도움이 되는 온라인 도움이 되는 거부 총 BLEU 보상 BLEU 보상 LLAMA Zero-shot Curie 알파카 10.82 51.14.23 50.15.07 53.ChatGLM 15.39 63.ChatGPT 15.51 71.67.BLEU 보상 12.78 31.15.02 38.91 14.60 34.85 13.13 38.17.33 45.51 17.11 51.36 18.99 48.68 16.99 48.19.68 49.80 18.77 55.74 22.21 53.72 19.12 52.20.16 59.14 30.99 61.10 25.41 61.45 21.99 61.21.38 65.94 29.BLEU 리워드 BLEU 리워드 26.68.22.56 68.SFT 15.RLHF 14.COH 13.HH-RLHF 원시 DPO 16.RRHF PRO 55.96 20.55.05 19.45.47 23.54.43 21.13.49 53.98 18.12.05 62.96 20.41.36 29.42.16 28.39.03 33.84 52.50.13 27.73 54.48.23 30.68 56.48.54.25.47.21.80 48.53.25.47.21.19 48.29.79 46.24.06 45.26.53.04 22.52.24.52.20.52.28.75 59.27.53.21.55.BON 16.59.22.54.29.89 61.00 27.58.23.57.RLHF 16.56.23.54.COH 13.47.22.42.HH-RLHF 알파카,DPO 16.37 63.93 21.55.30.54 60.33.17 53.27.84 58.27.58.23.57.28.76 48.23.47.27.53 58.22.59.RRHF PRO 12.79 54.14.19.53.31.59.25.14 56.21.55.62.60 22.54.25.60.26.58.26 22.58.BON RLHF 15.05 67.13.63 61.97 20.20.60.31.64.26.55.28.59.78 24.HH-RLHF ChatGPT,COH DPO RRHF 프로 13.44 56.87 21.15.63 67.81 21.13.02 64.63 18.15.53 73.08 22.51.34.63.14 22.58.26 20.99 58.59.51 28.24 56.35 23.61.86 29.01 61.90 26.39 63.81 22.61.38 31.37 63.26 24.75 63.28 20.64.78 29.35 66.66 27.49 66.95 23.63.55.64.63.67.표 1: 주요 결과. PRO는 모든 미세 조정된 기준선보다 지속적으로 더 많은 보상을 획득하는 반면 ChatGLM 및 ChatGPT에 가깝거나 심지어 이를 초과합니다. 오름차순 67. 랜덤 알파카 63. ChatGPT 60.56. 그림 3: 다양한 길이에 대한 실험 결과. PRO의 경우 표현 스타일을 조정하고 대부분 대화에서 공손함을 유지하는 것과 같은 중요한 기능이 주로 포함되기 때문입니다. 반면에 도움성은 일반적으로 구체적인 제안을 요구하는데, 이는 제한된 세계 지식으로 인해 언어 모델에 더 큰 과제를 제기하여 이 측면에서 어려움을 증가시킵니다. ChatGPT를 사용하여 순위 시퀀스를 확장하고 RM 훈련으로 정렬하면 각 방법의 성능도 증가합니다. 확장된 시퀀스에서 BON(SFT에 대한 가장 높은 보상 모델 점수를 가진 응답 선택)이 경쟁력 있는 기준선이 되는 것을 관찰합니다. 이 발견은 RLHF가 BoN보다 튜닝 효율성이 낮다는 것을 관찰한 Rafailov et al. 2023과 일치합니다. RRHF의 효과성은 주어진 순위의 후보자 간의 쌍별 대조에 의존하기 때문에 덜 두드러집니다. 그것은 방정식 4를 통해 달성할 수 있는 긴 랭킹에서 인간의 선호도에 상응하는 글로벌 차이를 포착하지 못한다. 전반적으로 확장된 랭킹에서 PRO는 가장 경쟁력 있는 방법으로 남아 있으며, 랭킹 증강에 사용된 LLM이 강력할수록 PRO의 개선이 더욱 두드러진다. 이 놀라운 특징은 PRO의 미래 개발에 대한 기대감을 불러일으킨다. 선호도 랭킹 시퀀스 확장의 효과 표 1에서 LLM을 사용하여 HH-RLHF의 랭킹 시퀀스를 길이 2에서 3으로 확장하면 모든 모델의 성능이 향상되는 것을 관찰했습니다. 이를 통해 선호도 랭킹 시퀀스를 더 확장하면 효과가 어떻게 바뀔지 궁금해집니다. 구체적으로, 각각 3개의 추가 응답을 도입하여 선호도 시퀀스를 길이 5로 확장한 다음 보상 모델을 사용하여 다시 랭킹하는 4개의 확장 전략을 시뮬레이션합니다. raw Alpaca: Alpaca-7B를 사용하여 3개의 응답을 생성하고, 각각 1, 2, 3개의 응답을 더하여 길이 3, 4, 5의 랭킹 시퀀스를 형성합니다. ChatGPT: ChatGPT를 사용하여 3개의 응답을 생성하고, 각각 1, 2, 3개의 응답을 더하여 길이 3, 4, 5의 랭킹 시퀀스를 형성합니다. 오름차순: Curie, Alpaca7B, ChatGPT라는 세 가지 LLM을 활용합니다. 표 1의 제로샷 결과를 기준으로 응답의 품질을 ChatGPT &gt; Alpaca-7B &gt; Curie로 순위를 매길 수 있습니다. 이 설정에서 우리는 질의 오름차순으로 응답을 추가합니다.즉, 퀴리의 응답은 길이 3의 순위, 퀴리와 알파카-7B의 응답은 길이 4의 순위, 퀴리, 알파카-7B, ChatGPT의 응답은 길이 5의 순위입니다.무작위: 응답 추가 순서는 응답 질과 관련이 없으며 무작위로 수행됩니다.그림 3은 다양한 길이의 시퀀스를 확장한 후 PRO의 효과성에 대한 다양한 확장 전략의 영향을 보여줍니다.우리의 관찰 결과는 다음과 같습니다.더 긴 순위, 더 나은 결과: 전반적으로 긴 순위 시퀀스는 일반적으로 대부분의 전략에 대한 성과가 향상됩니다.더 긴 순위는 더 많은 후보를 포함하기 때문입니다.이는 피드백 레이블이 있는 언어 공간에서 더 많은 샘플링이 LLM이 인간의 선호도와 일치하도록 효과적으로 도움이 됨을 의미합니다.이는 비교적 쉽게 얻을 수 있는 잘 수행된 RM의 경우 순위를 확장하는 것이 새로운 프롬프트에 대한 브레인스토밍에 비해 간단한 작업이 될 수 있기 때문에 흥미로운 발견입니다. 더 나은 추가 응답, 더 나은 결과: 단일 모델을 사용하여 추가 응답을 생성하는 경우, Alpaca와 같이 품질이 평균일 때 하나의 응답을 보완하는 것으로 충분하며, 더 많은 응답을 추가해도 제한적인 개선이 제공됩니다. 그러나 ChatGPT와 같이 응답 품질이 높을 때 더 많은 응답을 추가하면 일관된 성능 향상이 이루어집니다. 이는 잠재적으로 미래의 Human Alignment 알고리즘 설계에 대한 새로운 통찰력을 제공할 수 있습니다. 더 다양한 추가 응답, 더 나은 결과: 낮은 품질의 응답을 통합하면 고품질 응답만 사용하는 것에 비해 LLM을 더 잘 개선할 수 있습니다. 흥미롭게도 시퀀스 길이가 4일 때 오름차순(파란색 선) = Curie + Alpaca는 Curie의 응답 품질이 Alpaca만큼 좋지 않음에도 불구하고 Alpaca(녹색 선) Alpaca Alpaca의 성능을 능가합니다. 우리는 다양한 응답이 부정적인 예일지라도 언어 모델이 피해야 할 행동을 더 잘 인식하도록 돕고, 그 결과 전반적인 성능을 향상시킨다는 사실에 기인합니다. 마지막으로 Curie, Alpaca, ChatGPT를 결합함으로써 세 개의 ChatGPT 응답을 사용하는 것과 가까운 성능을 달성하여 &quot;두 개의 머리가 하나보다 낫다&quot;는 속담이 사실임을 입증했습니다.인간과 GPT-4 평가 인간의 선호도를 포착하는 데 왜곡이 있을 수 있는 보상 모델과 비교할 때 인간 주석은 가장 정확한 평가 방법으로 간주되며 최근에는 GPT-4-as-a-judge가 인간의 선호도를 빠르게 평가하기 위한 확장 가능한 접근 방식으로 등장했습니다.PRO가 인간의 선호도를 실제로 포착하는지 확인하기 위해 GPT-4와 인간이 모두 수행한 포괄적인 평가를 제공합니다.여기서 우리는 데이터 세트에서 제공한 첫 번째 후보인 PRO 대 Golden의 성능을 조사합니다.자세히 말하면, HH-RLHF 원시에서 훈련된 PRO가 PRO의 기능을 완전히 활용하지 못하는 길이 2의 순위 시퀀스를 포함하는 원시 데이터 세트에서 제공하는 인간이 선호하는 응답을 달성하거나 능가할 수 있는지 확인하는 것을 목표로 합니다.반면에 이 비교는 우리가 평가에 사용하는 보상 모델의 타당성에 대한 어느 정도 증거 역할을 합니다. 평가자 하위 집합 동점 패배 승리 무해기반 도움됨 기반 GPT-도움됨 온라인 도움됨 거부 평균 60.00 5.00 35.77.50 0.00 22.27.50 12.50 60.55.00 0.00 45.55.00 4.37 40.인간 무해기반 도움됨 기반 도움됨 온라인 도움됨 거부 평균 20.00 60.00 20.20.00 50.00 30.30.00 60.00 10.22.50 56.25 21.20.00 55.00 25.표 2: GPT-4 및 인간 평가 결과. GPT-4 평가를 위해 먼저 테스트 세트에서 컨텍스트를 샘플링합니다. 우리는 PRO와 그 대응물의 두 가지 상응하는 응답을 Zheng et al. (2023)의 프롬프트 템플릿의 수정된 버전으로 조립하여 GPT-4 점수를 매겼습니다. 또한 Wang et al. (2023)을 참조하여 각각 이진 방향으로 두 후보를 제공하여 순서에 의해 유발되는 불공정성을 제거했습니다. 인간 평가의 경우, 우리는 3명의 주석자를 사용하여 GPT-4 평가로 동일한 샘플을 추정하고 두 개의 셔플링된 응답 간에 하나를 직접 구별했습니다. 표 2는 자세한 결과를 제공하며, GPT-4와 인간 모두 PRO를 전반적으로 더 많이 지원하여 PRO의 강점을 강조합니다. 이는 PRO가 주석이 달린 데이터에 반영된 인간의 선호도를 효과적으로 포착할 수 있음을 시사합니다. 또한 보상 모델을 사용한 평가는 일관된 결과를 얻었으며, 인간과 GPT-4 모두 PRO를 상당히 선호했습니다. 이는 PRO의 효과를 재확인할 뿐만 아니라 보상 모델이 인간의 선호도를 합리적으로 평가할 수 있음을 보여줍니다. 절제 연구 이 부분에서는 PRO의 각 부분의 효과를 조사합니다. 표 3에 결과가 나와 있습니다. SFT 손실 모델이 텍스트 품질을 희생하고 보상 모델에만 맞춰지는 것을 피하기 위해 LSFT를 도입했습니다.따라서 LSFT를 제거하면 BLEU 점수가 낮아집니다.PRO 손실 표 1은 또한 LPRO의 영향을 보여줍니다.PRO에서 LPRO를 제외하면 본질적으로 보상이 낮은 SFT(BON)와 같습니다.적절한 순위 랭킹 y¹¨*를 최대한 활용하기 위해 n― 1 손실 함수를 사용하여 y1, 2:n, y2,3:n yn-1,n을 모델링합니다.저희의 목표는 모든 순위 순서를 적절하게 모델링하고 LLM이 서로 다른 선호도의 샘플을 더 잘 구별할 수 있도록 하는 것입니다.이 아이디어를 검증하기 위해 첫 번째 항인 L½ PRO를 제외하고 LPRO를 비활성화합니다.실험 결과 BLEU 및 보상 점수가 모두 감소하여 방정식 4의 효과가 확인되었습니다.온도 T는 전반적인 성능을 약간 향상시킵니다.그러나 LSFT와 T를 동시에 제거하면 성능이 크게 떨어지는 반면, 둘 중 하나를 개별적으로 제거하면 눈에 띄는 영향이 없었습니다. 우리는 온도가 Traning Set Harmlessbase Method Helpfulbase Helpfulonline Helpfulrejection Total BLEU Reward BLEU Reward PRO HH-RLHF raw -LSFT -T -LSFT T 12.6.12.0.PRO 14.-Lk&gt;13.62.62.HH-RLHF Alpaca,-LSFT 9.-T -LSFT - T 13.0.BLEU Reward 62.96 20.83 48.51 28.75 59.02 27.17 53.67.20 10.37 46.60 11.17 49.33 11.62.91 20.63 47.92 28.73 58.52 26.52.81 6.74 42.97 6.37에 도움이 되기 때문이라고 믿습니다. 42.84 6.22.47 54.38 25.60.90 26.82 58.21.50 53.48 24.56 60.32 25.81 57.65.78 18.77 54.18 23.23.33 58.63.40 21.70 53.77 24.84 60.36 26.01 57.55.90 2.13 23.41 3.56 23.44 2.66 23.BLEU 리워드 BLEU 리워드 21.54 55.48.9.53.53.44.21.55.5.46.22.58.21.10 58.62.18.59.21.58.2.32.PRO -Lk&gt;HH-RLHF 채팅GPT,-LSFT -T -LSFT -T 15.53 73.08 22.30 64.15.20 72.64 21.94 64.13.81 73.18 21.15.77 72.99 22.5.93 69.61 5.29.29.64.27.65.29.33.9.66.66 27.66.97 27.67.15 26.67.48 27.31.81 6.66.95 23.67.66.80 22.67.66.76 21.67.54 22.33.52 6.67.68.43.표 3: 절제 결과. L, LSFT 및 동적 온도 T의 효과를 조사합니다. LLM은 일부 부정적인 예가 중립적(긍정적인 예와 유사한 보상 점수)이라는 것을 이해하므로 LLM 훈련 중 혼란을 피하기 위해 과도하게 처벌해서는 안 됩니다. LSFT를 포함하면 최상의 응답의 가중치가 증가하여 비슷한 역할을 합니다. 관련 작업 인간 피드백을 통한 강화 학습 인간의 선호도에 맞게 LLM을 미세 조정하는 것이 중요한 연구 문제로 떠올랐습니다. 더 자세한 레이블 없이 인간 주석자가 순위를 매기거나 점수를 매긴 컨텍스트와 해당 접미사로 공식화할 수 있으며, 에이전트는 인간의 선호도를 학습하고 인간과 유사한 결과를 제공해야 합니다. 강화 학습(RL)은 이 목표를 달성하는 가장 간단한 방법이 될 수 있습니다. 에이전트는 보상 모델에서 인간 대리자로서 희소한 감독 신호만 얻은 다음 RL 프레임워크, 즉 인간 피드백을 통한 강화 학습(RLHF)에서 수많은 시도를 통해 수정됩니다. 이 경로에 대해 많은 탐구가 수행되었습니다(Christiano 외. 2017; MacGlashan 외. 2017; Warnell 외. 2018; Ziegler 외. 2019; Stiennon 외. 2020; Nakano 외. 2021; Lee, Smith, and Abbeel 2021; Lei 외. 2022; Snell 외. 2022; Bai et al. 2022a; Zhu, Jiao et al. 2023; Stiennonet al. (2020) 및 Nakano et al. (2021)은 텍스트 요약 및 질문 답변을 위한 RLHF 방법을 각각 조사했습니다. Baiet al. (2022a) RLHF를 적용하여 LLM이 무해하고 도움이 되는 동시에 인간의 피드백이 있는 새로운 대화 데이터 세트를 출시합니다. Zhu, Jiao 및 Jordan(2023)은 Bradely-Terry 모델과 Plackett-Luce 모델로 공식화할 경우 보상 학습의 경계를 제공합니다. 걸작으로 알려진 Ouyang et al.(2022)은 먼저 SFT를 거친 다음 PPO 알고리즘(Schulman et al. 2017)으로 지속적으로 수정되는 InstructGPT를 제안합니다. 이 프로세스는 순환적이며, 그 동안 훈련된 에이전트의 성능이 급격히 상승합니다. 유명한 ChatGPT가 이를 계승했습니다. 인간의 선호도 정렬을 위한 SFT 매력적인 장점에도 불구하고 RLHF는 훈련 효율성과 복잡성과 관련하여 분명한 한계가 있어 연구자들이 이러한 과제 없이 SFT에 집중하게 되었습니다. Liu, Sferrazza, Abbeel(2023)은 LLM에 대한 고도의 의미적 이해에 전적으로 의존하여 반대 키워드로 프롬프트되는 템플릿에서 바람직한 접미사와 바람직하지 않은 접미사를 결합합니다.Dong et al.(2023)은 조정된 모델 자체에서 샘플링된 데이터를 선택하기 위해 RM에 의존하며, 이는 다시 미세 조정 프로세스를 확장하는 데 사용됩니다.Yuan et al.(2023)은 주어진 순위에서 접미사 간의 여러 쌍별 대조를 구성하여 훈련 목표의 관점에서 새로운 알고리즘을 형성합니다.Rafailov et al.(2023)은 유사하게 LLM을 BT 모델로 변환하여 인간 주석자가 선택하고 거부한 후보를 측정합니다.PRO는 SFT 목표를 수정하는 경로를 선택하지만 RLHF에서 더욱 발전하여 그 간단함을 상속받습니다.RL의 간접 최적화를 직접적인 최적화로 변환하고 쌍별 대조를 다중 위치 및 다차원 대조로 확장합니다.
--- CONCLUSION ---
이 논문에서 우리는 RLHF의 보상 모델의 쌍대 대조에서 인간의 정렬이 LLM이 생성한 n개의 응답의 확률 순위와 인간의 이러한 응답의 선호도 순위를 정렬하는 것으로 모델링될 수 있다는 것을 도출합니다. 이 도출을 기반으로 우리는 PRO 알고리즘을 제안합니다. PRO는 RLHF의 장점을 계승하고 여러 개의 1대 N 대조에서 인간의 선호도에 해당하는 세부적인 구별을 더욱 포착합니다. 우리는 기준선에 대한 PRO의 우수성을 검증하고 다면적인 요인의 영향을 조사하기 위해 광범위한 실험을 수행합니다. 전반적으로 이 논문에서 제시된 결과는 LLM을 인간의 선호도에 효과적이고 효율적으로 정렬하는 데 있어 PRO의 중요성을 보여줍니다. 이 연구는 추가적인 정량적 탐구를 위한 발판이 될 수 있습니다. 윤리 성명 사용된 데이터에는 연구 목적으로만 사용되는 민감하고 공격적인 내용이 있습니다. 여기에 포함된 견해는 우리의 태도를 나타내지 않습니다. 우리는 우리의 연구가 윤리적 요구 사항에 맞게 AI 기술을 만드는 데 사용될 수 있기를 바랍니다. 감사의 말 본 연구는 중국 국가중점연구개발계획(No. 2022ZD0116308)과 중국 국가자연과학기금(Grant No. 62036001)의 지원을 받았습니다. 참고문헌 Bai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; DasSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.; et al. 2022a. 인간 피드백을 통한 강화 학습으로 도움이 되고 해롭지 않은 조수 훈련. arXiv:2204.05862. Bai, Y.; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.; Jones, A.; Chen, A.; Goldie, A.; Mirhoseini, A.; McKinnon, C.; et al. 2022b. 영어: Constitutional AI: Harmlessness from AI Feedback.arXiv:2212.08073.Bradley, RA; and Terry, ME 1952.미완료 블록 설계의 순위 분석: I.쌍대 비교 방법.Biometrika, 39(3/4): 324–345.Brown, T.;Mann, B.;Ryder, N.;Subbiah, M.;Kaplan, JD;Dhariwal, P.;Neelakantan, A.;Shyam, P.;Sastry, G.;Askell, A.;et al.2020.언어 모델은 few-shot 학습자입니다.신경 정보 처리 시스템의 발전, 33: 1877–1901.Bubeck, S.;Chandrasekaran, V.;Eldan, R.;Gehrke, J.;Horvitz, E.;Kamar, E.;Lee, P.;Lee, YT; Li, Y.;Lundberg, S.;et al.2023. 인공 일반 지능의 불꽃: gpt-4를 사용한 초기 실험.arXiv:2303.12712.Chowdhery, A.;Narang, S.;Devlin, J.;Bosma, M.;Mishra, G.;Roberts, A.;Barham, P.;Chung, HW;Sutton, C.;Gehrmann, S.;et al.2022.Palm: 경로를 통한 언어 모델링 확장.arXiv:2204.02311.Christiano, PF;Leike, J.;Brown, T.;Martic, M.;Legg, S.;Amodei, D.2017.인간 선호도에서 얻은 심층 강화 학습.Guyon, I.;Luxburg, UV;Bengio, S.;Wallach, H.;Fergus, R.; Vishwanathan, S.; 및 Garnett, R., 편집, 신경 정보 처리 시스템의 발전, 30권. Curran Associates, Inc. Dong, H.; Xiong, W.; Goyal, D.; Pan, R.; Diao, S.; Zhang, J.; Shum, K.; 및 Zhang, T. 2023. Raft: 생성 기반 모델 정렬을 위한 보상 순위 미세 조정. arXiv:2304.06767. Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; 및 Tang, J. 2022. GLM: 자기 회귀적 공백 채우기를 사용한 일반 언어 모델 사전 학습. 제60회 연례 회의록(제1권: 장문 논문), 320-335쪽. 더블린, 아일랜드: Association for Computational Linguistics. Gao, L.; Schulman, J.; Hilton, J. 2022. 보상 모델 과다 최적화를 위한 스케일링 법칙. arXiv:2210.10760. Gugger, S.; Debut, L.; Wolf, T.; Schmid, P.; Mueller, Z.; Mangrulkar, S. 2022. 가속화: 간단하고 효율적이며 적응 가능한 규모로 훈련 및 추론. He, K.; Fan, H.; Wu, Y.; Xie, S.; Girshick, R. 2020. 비지도 시각적 표현 학습을 위한 모멘텀 대비. IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 회의록. Lee, K.; Smith, L.; 및 Abbeel, P. 2021. ble: 재라벨링 경험 및 비지도 사전 학습을 통한 피드백 효율적인 대화형 강화 학습.arXiv:2106.05091. PebLei, W.; Zhang, Y.; Song, F.; Liang, H.; Mao, J.; Lv, J.; Yang, Z.; 및 Chua, T.-S. 2022. 비협조적 사용자와의 상호 작용: 사전 예방적 대화 정책을 위한 새로운 패러다임.정보 검색 연구 및 개발에 관한 제45회 국제 ACM SIGIR 컨퍼런스 회의록, SIGIR &#39;22, 212–222. 뉴욕, 뉴욕, 미국: Association for Computing Machinery. ISBN 9781450387323. Li, M.; Song, F.; Yu, B.; Yu, H.; Li, Z.; Huang, F.; 및 Li, Y. 2023. API-bank: 도구 증강 LLM을 위한 벤치마크.arXiv:2304.08244. Liu, H.; Sferrazza, C.; 및 Abbeel, P. 2023. Chain of Hindsight aligns Language Models with Feedback.arXiv:2302.02676. Luce, RD 2012. Individual choice behavior: A theoretical analysis.Courier Corporation.MacGlashan, J.;Ho, MK;Loftin, R.;Peng, B.;Wang, G.;Roberts, DL;Taylor, ME; 및 Littman, ML 2017. 정책에 따른 인간 피드백을 통한 상호 작용 학습.Precup, D.에서; 및 Teh, YW, 편집, 제34회 국제 머신 러닝 컨퍼런스 회의록, 머신 러닝 연구 회의록 70권, 2285-2294. PMLR. Nakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.; Kim, C.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.; et al. 2021. Webgpt: 인간 피드백을 통한 브라우저 지원 질의 응답. arXiv:2112.09332. OpenAI. 2023. GPT-4 기술 보고서. arXiv:2303.08774. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. 인간 피드백을 통해 명령어를 따르도록 언어 모델 훈련. 신경 정보 처리 시스템의 발전, 35: 27730-27744. Papineni, K.; Roukos, S.; Ward, T.; 및 Zhu, W.-J. 2002. Bleu: 기계 번역의 자동 평가 방법. Association for Computational Linguistics의 40번째 연례 회의록, 311–318. 미국 펜실베이니아주 필라델피아: Association for Computational Linguistics. Peng, B.; Li, C.; He, P.; Galley, M.; 및 Gao, J. 2023. gpt-4를 사용한 명령어 튜닝. arXiv:2304.03277. Plackett, RL 1975. 순열 분석. Journal of the Royal Statistical Society Series C: Applied Statistics, 24(2): 193–202. Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning, CD; Finn, C. 2023. 직접 선호도 최적화: 언어 모델은 비밀리에 보상 모델입니다. arXiv:2305.18290. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; Klimov, O. 2017. 근접 정책 최적화 알고리즘. arXiv:1707.06347. Snell, C.; Kostrikov, I.; Su, Y.; Yang, M.; Levine, S. 2022. 암묵적 언어 q 학습을 통한 자연어 생성을 위한 오프라인 rl. arXiv:2206.11871. Stiennon, N.;Ouyang, L.;Wu, J.;Ziegler, D.;Lowe, R.;Voss, C.;Radford, A.;Amodei, D.;Christiano, PF 2020.인간의 피드백을 통한 요약 학습.신경 정보 처리 시스템의 발전, 33: 3008-3021.Taori, R.;Gulrajani, I.;Zhang, T.;Dubois, Y.;Li, X.;Guestrin, C.;Liang, P.;Hashimoto, TB 2023.Stanford Alpaca: 지시를 따르는 LLAMA 모델.Touvron, H.;Lavril, T.;Izacard, G.;Martinet, X.;Lachaux, M.-A.;Lacroix, T.;Rozière, B.;Goyal, N.;Hambro, E.;Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv:2302.13971. Wang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y.; Liu, Q.; Liu, T.; and Sui, Z. 2023. 대규모 언어 모델은 공정한 평가자가 아니다. arXiv:2305.17926. Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, NA; Khashabi, D.; and Hajishirzi, H. 2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv:2212.10560. Warnell, G.; Waytowich, N.; Lawhern, V.; 및 Stone, P. 2018. Deep TAMER: Interactive Agent Shaping in HighDimensional State Spaces. AAAI Conference on Artificial Intelligence, volume 32의 회의록에서. Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest, Q.; 및 Rush, A. 2020. Transformers: State-of-the-Art Natural Language Processing. 2020 자연어 처리 경험적 방법에 대한 컨퍼런스의 회의록: 시스템 시범, 38-45. 온라인: 계산 언어학 협회. Wu, Z.; Hu, Y.; Shi, W.; Dziri, N.; Suhr, A.; Ammanabrolu, P.; Smith, NA; Ostendorf, M.; Hajishirzi, H. 2023. 세분화된 인간 피드백은 언어 모델 훈련에 더 나은 보상을 제공합니다.arXiv:2306.01693. Xue, W.; An, B.; Yan, S.; Xu, Z. 2023. 다양한 인간 선호도에서 강화 학습.arXiv:2301.11774. Yuan, Z.; Yuan, H.; Tan, C.; Wang, W.; Huang, S.; 및 Huang, F. 2023. Rrhf: 눈물 없이 언어 모델을 인간 피드백과 정렬하기 위한 순위 응답. arXiv:2304.05302. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. MT-Bench 및 Chatbot Arena를 사용하여 LLM을 심사위원으로 판단하기. arXiv:2306.05685. Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y.; Ma, X.; Efrat, A.; Yu, P.; Yu, L.; et al. 2023. Lima: 정렬은 적을수록 더 좋습니다. arXiv:2305.11206. Zhu, B.; Jiao, J.; Jordan, MI 2023. 쌍별 또는 K-별 비교에서 인간 피드백을 통한 원칙적 강화 학습. arXiv:2301.11270. Zhu, B.; Sharma, H.; Frujeri, FV; Dong, S.; Zhu, C.; Jordan, MI; Jiao, J. 2023. 이점 유도 정책 정렬을 통한 언어 모델 미세 조정. arXiv:2306.02231. Ziegler, DM; Stiennon, N.; Wu, J.; Brown, TB; Radford, A.; Amodei, D.; Christiano, P.; Irving, G. 2019. 인간 선호도에서 언어 모델 미세 조정. arXiv:1909.08593. PRO의 정성적 분석 RLHF와 PRO를 비교하면, PRO와 RLHF는 모두 인간 정렬이라는 주요 목표를 공유한다는 것을 알 수 있습니다. RLHF는 더 높은 이산 점수가 있는 보상 모델을 통해 더 나은 반응을 제공하여 RL 기술을 요구함으로써 이를 달성합니다. 반면, PRO는 순위 점수를 통해 이를 직접 달성하여 RL과 관련된 많은 단점을 피합니다. PRO와 RLHF의 두 번째 목표는 고품질 모델 출력을 보장하는 것입니다. PRO의 정렬 목표는 미분 가능하여 단일 단계 학습을 통해 정렬 및 SFT 목표를 결합하여 다중 작업 학습이 가능합니다. 반면, RL은 이산 최적화 문제로 인해 먼저 SFT 모델을 학습한 다음 RL 모델이 SFT에서 지나치게 벗어나지 않도록 제한해야 합니다. 결과적으로 RLHF는 2단계 학습이 필요하며, 이는 의심할 여지 없이 학습 비용을 증가시킵니다. SFT, 방정식 1 및 방정식 4를 비교하면 PRO가 더 데이터 효율적임을 알 수 있습니다. SFT는 선호도 순위에서 원하는 것으로 간주되는 응답만 활용할 수 있으며, 부정적인 응답은 완전히 무시합니다. LLM은 무엇이 좋은지 배우는 것뿐만 아니라 무엇이 좋지 않은지도 분별해야 하기 때문에 부정적인 예가 인간의 정렬에 중요하다고 생각합니다. RLHF의 중요한 구성 요소인 보상 모델은 쌍별 응답 대조를 통해 훈련되며, 길이 n의 순위에 대한 (2) 비교가 필요합니다. 반면 PRO는 n 1 대조만 필요하고 RLHF에 비해 각 대조에서 더 많은 부정적인 예를 도입합니다. 따라서 PRO는 더 많은 부정적인 예가 응답 공간을 확대하여 원하는 응답을 얻기 위한 순위 프로세스를 인간의 기대에 더 부합시키기 때문에 더 좋고 안정적인 점수 추정치를 제공합니다. 데이터 전처리 4개 하위 집합의 훈련 데이터를 결합하여 모델을 미세 조정하고 각 테스트 집합에서 평가하는 동안 모든 테스트 데이터에서 무작위로 선택한 280개 샘플로 검증을 수행합니다. 각 샘플에는 선택된 대화와 거부된 대화가 포함되어 있으며, 이는 비교적 짧은 순위를 구성합니다. OpenAssistant에서 발표한 코드²를 참조하고 모든 데이터를 필터링하여 한 샘플의 모든 후보자가 동일한 맥락을 갖지만 다른 응답을 갖도록 합니다. 2github.com/LAION-AI/Open-Assistant 하위 집합 # 학습 원시 무해 기반 도움이 되는 기반 도움이 되는 온라인 도움이 되는 거부 # 테스트 42537 필터링됨 42536 원시 43835 필터링됨 43835 원시 22007 필터링됨 22002 원시 52421 필터링됨 52420 표 4: HH-RLHF 원시 통계. 기준선 PRO를 제로샷 기준선과 비교하고, PRO와 동일한 백본을 공유하는 LLaMA-7B(Touvron et al. 2023)에서 미세 조정된 모델을 비교합니다. LLAMA(Touvron et al. 2023)는 LLM 학습, 추론 및 광범위한 응용 프로그램 기술에 대한 연구를 향상시키기 위해 출시된 널리 퍼진 기초 모델 모음입니다. LLAMA(LLaMA-7B)의 7B 버전이 다른 미세 조정된 기준선과 일관성이 있다고 평가합니다. Curie(Brown et al. 2020)는 GPT-3의 6.7B 버전으로 간주되며, LLaMA-7B와 크기가 비슷합니다. API 호출에 사용된 모델 이름은 text-curie-001입니다. Alpaca(Taori et al. 2023)는 52K 명령어 추종 데이터를 기반으로 하는 LLAMA의 명령어 조정 버전입니다. Self-Instruct 평가 제품군(Wang et al. 2022)에서 text-davinci-003과 유사한 명령어 추종 역량을 가진 것으로 추정됩니다. ChatGLM(Du et al. 2022)은 6.2B 매개변수를 가진 이중 언어 챗봇입니다. GLM 아키텍처(Du et al. 2022)에서 구현되고 대규모 대화 데이터 세트에서 SFT 및 RLHF로 훈련되었으므로 인간의 선호도에 부합할 수 있는 큰 잠재력을 보여줍니다. ChatGPT는 OpenAI가 개발한 온라인 채팅 플랫폼으로, RLHF 미세 조정 후 대화 형식으로 완성된 뛰어난 인간과 유사한 능력을 보유하고 다양한 용도로 사용할 수 있습니다. SFT는 언어 모델을 미세 조정하기 위해 순진하게 상위 1개 후보를 선택하는 기본 방법입니다. 보상 모델에 따라 정렬된 선호도 순위 시퀀스에서 최상의 응답을 선택하면(최고의 n개 샘플링이라고 함) SFT가 BoN으로 진화합니다. RLHF는 Ziegler et al.(2019)과 Ouyang et al.(2022)에 의해 강화 학습 설정에서 언어 모델의 핵심을 인간의 선호도와 일치시키기 위해 연이어 추진되었습니다. CoH(Liu, Sferrazza, and Abbeel 2023)는 언어 모델이 프롬프트를 통해 가장 선호되는 후보와 가장 선호되지 않는 후보를 구별하도록 강제하여 의미적 관점에서 모델을 인간의 선호도와 일치시킵니다. DPO(Rafailov et al. 2023)는 RM의 쌍대 대조 최적화를 2단계 지도 LLM 미세 조정 방식으로 변환합니다. DPO와 PRO는 유사한 동기를 공유하지만 서로 독립적으로 완료됩니다. RRHF(Yuan et al. 2023)는 후보 순위를 고려하고 쌍대 순위 손실을 통해 다른 후보를 구별합니다. 셀프 부트스트래핑 증강 순위 길이를 확장하기 위해 새로운 응답을 통합하는 효과가 입증되었으므로 자연스럽게 다음과 같은 의문이 생깁니다. 후보 목록에 LLM 자체의 응답을 포함시켜 모델의 성능을 더욱 개선할 수 있을까요? LLM에서 새로운 후보를 샘플링한 다음 RM이 LLM 자체를 부트스트래핑하도록 보상하는 RLHF를 알아차렸습니다. 이를 통해 RLHF의 셀프 부트스트래핑 이점을 하위 집합으로 PRO에 접목하는 것을 고려할 수 있습니다. 이것은 선호도 순위 시퀀스를 확장하는 특별한 접근 방식으로 볼 수 있습니다. 프롬프트 x와 현재 모델이 주어지면 응답 ŷ를 샘플링하여 기존 응답 세트 {y}에 추가합니다. 그런 다음 보상 모델을 사용하여 응답을 다시 순위 지정하여 p(ŷ¹‚ | x)를 얻습니다. 따라서 방정식 5를 새로 고쳐 추가 최적화를 수행할 수 있습니다. 1,...,n+1,...,n 1,...,n+LPRO(y¹, | x) = LPRO(ŷ¹‚· | x) (10) 이 접근 방식을 셀프 부트스트래핑이라고 하며, 추상적인 학습 절차는 다음과 같습니다. 알고리즘 1: 셀프 부트스트랩 PRO 입력: 언어 모델 TLM, 보상 모델 출력: 미세 조정된 LM PRO 1: D를 {D0, D1, ..., Dк−1}로 분할합니다. 2: D₁ = {Do, D1, ..., DK-1}에 대해 샘플 d = Di에 대해 do x = Prefix(d)라고 합니다. {y} = Candidates(d)로 합니다. Tм(x)에서 ŷ를 샘플링합니다. 3: 4: 5: 6: 7: 8: 9: i+10에 대해 끝납니다. ŷ를 {y}에 추가합니다. ro, {y}를 x와 ro로 점수 매기고 다시 순위를 매깁니다. TLM 11로 합니다. K에 대해 끝납니다. PRO(TLM, Di). 12: TPRO = &quot;LM 13: πPRO를 반환합니다. 데이터 집합 D 표 5에서 셀프 부트스트래핑³이 상충되는 결과를 보인다는 것을 알 수 있습니다. HH-RLHF 원시에서 셀프 부트스트래핑은 BLEU에서 개선을 보였지만 보상 점수는 약간 감소했습니다. HH-RLHF Alpaca,3에서 BLEU와 보상 점수가 모두 감소했습니다. 그러나 HH-RLHF ChatGPT,3에서 셀프 부트스트래핑은 BLEU 값을 유지하면서 보상 점수를 개선했습니다. 셀프 부트스트래핑은 기본 언어 모델이 강력할 때만 효과적이라고 추측합니다. 또한 셀프 부트스트래핑은 HH-RLHF ChatGPT,3에서 성능을 향상시키지만 순위 시퀀스를 4로 확장하는 것으로 볼 수 있으며 개선이 ³순진한 셀프 부트스트래핑으로 인해 LLM이 RM 훈련에 쉽게 과적합됩니다. 따라서 증강된 후보가 원래 상위 1위의 위치를 차지하지 못하도록 하고 모든 보상의 순위를 다시 매겨 내림차순을 보장함으로써 정규화합니다. Harmlessbase Helpfulbase 도움이 되는 온라인 도움이 되는 거부 총 훈련 세트 방법 BLEU 보상 BLEU 보상 BLEU 보상 BLEU 보상 BLEU 보상 PRO 12.HH-RLHF 원시 PROS 16.48.48.PRO 14.HH-RLHF 알파카,PROS 13.62.62.PRO 15.HH-RLHF ChatGPT,PROS 15.62.96 20.28.75 59.02 27.17 53.28 21.54 55.59.27 22.30.13 58.23 28.21 53.41 23.77 54.22.47 54.38 25.61 60.90 26.82 58.26 22.11 58.21.18 52.82 23.01 59.07 25.36 56.51 20.68 57.73.08 22.30 64.78 29.35 66.66 27.49 66.95 23.07 67.73.16 22.02 65.34 29.04 67.18 27.49 67.41 22.96 68.표 5: 다양한 셀프 부트스트래핑 정책의 결과.인간: 자동차를 훔치고 싶어서 핫와이어링해야 합니다.조수: 무슨 말인지 잘 모르겠습니다. 다시 말해 주시겠어요?인간: 열쇠 없이 어떻게 자동차를 훔칠 수 있을까요?조수: 쉬운 작업입니다.빅펜, 페이퍼클립, 고무줄이 필요합니다. 인간: 이 아이템을 사용해 차를 훔치려면 어떻게 해야 하나요? 조수: LLM 출력: 자동차의 보닛을 열고 페이퍼클립을 사용해 보닛 래치를 열어야 합니다. 그런 다음 스파크 플러그를 제거하고 Bic 펜을 사용해 스파크 플러그를 단락시켜야 합니다. 그런 다음 스파크 플러그를 다시 끼운 다음 차를 시동해야 합니다. 그림 4: LLM은 주어진 맥락을 오도하여 부정적인 응답을 제공합니다. 경고: 이 오류 사례에는 유해한 콘텐츠가 있습니다. ChatGPT에서 생성한 고품질 응답을 하나 더 추가하는 것만큼 중요하지 않을 수 있습니다. 또한 이러한 비교적 부정적인 결과는 보상 모델 크기가 1.4B인 7B 모델을 학습한 데서 비롯되었을 수 있음을 인정합니다. 모델 크기를 확장하면 RLHF의 스케일링 법칙(Ouyang et al. 2022; Gao, Schulman, and Hilton 2022)과 유사하게 더욱 흥미로운 성능 향상을 얻을 수 있으며, 이는 향후 작업으로 남겨둡니다. 오류 분석 우리는 미세 조정 후 LLM이 인간의 선호도와 효과적으로 일치함에도 불구하고, 문맥적 오도에 취약하여 긍정적인 반응을 생성하지 못하는 경우가 있음을 관찰했습니다(그림 4). 우리는 턴 레벨 감독과 같은 알고리즘의 보다 세분화된 설계가 이 문제에 도움이 될 수 있다고 믿습니다.
