--- ABSTRACT ---
우리는 2D 이미지, 언어, 오디오 및 비디오와 포인트 클라우드를 정렬하는 3D 멀티 모달리티 모델인 Point-Bind를 소개합니다. ImageBind의 안내에 따라 3D와 멀티 모달리티 사이에 공동 임베딩 공간을 구축하여 모든 모달리티에서 3D로의 생성, 3D 임베딩 산술 및 3D 오픈 월드 이해와 같은 많은 유망한 애플리케이션을 가능하게 합니다. 여기에 더하여 3D 멀티 모달 명령어를 따르는 최초의 3D 대규모 언어 모델(LLM)인 PointLLM을 추가로 제시합니다. Point-LLM은 매개변수 효율적인 미세 조정 기술을 통해 Point-Bind의 의미를 사전 학습된 LLM(예: LLaMA)에 주입합니다. LLaMA는 3D 명령어 데이터가 필요하지 않지만 뛰어난 3D 및 멀티 모달 질의 응답 용량을 보여줍니다. 우리의 작업이 3D 포인트 클라우드를 멀티 모달리티 애플리케이션으로 확장하기 위한 커뮤니티에 빛을 비추기를 바랍니다. 코드는 https://github.com/ZiyuGuo99/ Point-Bind_Point-LLM에서 확인할 수 있습니다. 1.
--- INTRODUCTION ---
최근 몇 년 동안 3D 비전은 자율 주행[11, 69, 71], 내비게이션[72, 76, 98], 3D 장면 이해[2, 43, 46, 74], 로봇공학[31, 67]의 인기 상승에 힘입어 상당한 주목과 개발을 받았습니다. 응용 프로그램 시나리오를 확장하기 위해 다른 모달리티의 데이터와 3D 포인트 클라우드를 통합하여 3D 이해[1, 23], 텍스트-3D 생성[35, 49, 52], 3D 질의 응답[3, 28]을 개선하기 위한 수많은 노력[1, 23, 92, 95]이 이루어졌습니다. 3D 기하 이해를 위해 이전 작업에서는 2D 언어 임베딩을 활용하여 3D 오픈 월드 인식[90, 100]을 안내하거나 시각적 및 텍스트 의미론적 의미론을 활용했습니다.[3D] 포인트 클라우드 이미지 &quot;비행기의 3D 모델.&quot; 언어 POINT-BIND 3D Q&amp;A를 위한 Point-LLM 3D 임베딩 산술 [비행기 엔진] l 오디오 Any-to-3D 생성 3D 제로샷 학습 그림 1. Point-Bind의 특성. 우리는 3D를 다중 모달리티와 맞추고 다양한 3D 다중 모달 애플리케이션을 확장하는 통합 프레임워크인 PointBind를 개발할 것을 제안합니다. Point-Bind를 기반으로 이중 언어 3D 지시 수행 용량을 갖춘 3D 대규모 언어 모델인 Point-LLM을 추가로 소개합니다. 3D 표현 학습을 돕기 위해 다양한 언어 틱[39,56,84]을 사용합니다. 그러나 그들의 지각 능력은 대부분 훈련 단계에서 제공되는 제한된 모달리티에 의해 제한됩니다. 2D 생성 모델[60, 63, 64]에서 영감을 얻은 방법 모음[35,49,52]은 고품질과 효율성으로 텍스트-3D 합성을 달성했습니다. 그럼에도 불구하고, 그들은 다중 모달 입력, 즉 모든-3D 생성에 따라 3D 모양을 생성하는 능력이 부족합니다.또 다른 일련의 작업은 설명적 자연어를 3D 데이터와 연결하여 3D 캡션[12, 87], 질의 응답[3, 78] 및 시각적 접지[24, 79]에 적용합니다.그러나 그들은 3D Q&amp;A를 위해 prePoint-LLM을 활용하지 못합니다.모든-3D 생성 3D 임베딩 산술 3D 제로샷 이해 ➤ 3D 지침 데이터 필요 없음 3D 및 다중 모달 추론 텍스트-3D: 3D 객체를 자세히 설명합니다.&quot;자동차&quot; 3D 포인트 클라우드[바다 파도 오디오] 3D 객체는 작은 비행기, 구체적으로는 개인용 제트기입니다. 검은색 색상 구성표를 사용하고 소형 항공기처럼 보입니다.그것은 ........ 무슨 일이 일어나고 있는 걸까요? [물을 붓는 오디오] 오디오-3D: [ 자동차 경적 ] 이미지-3D: a 포인트-3D: 이 장면은 사람이 맑은 액체(아마도 물)를 유리잔에 붓는 모습을 보여줍니다. 유리잔은 테이블 위에 놓여 있고, 넘치도록 채워져 있어 상당한 양의 액체가 부어졌음을 나타냅니다. 크로스 모달 검색 텍스트 참조: 비행기 피아노 기타 비행기, 피아노, 기타 CLASS 포인트-바인드 모델 오디오 참조: ↓ [비행기 엔진] [피아노 음악] [기타 사운드] 포인트-바인드 ↓ 비행기 피아노 기타 그림 2. 포인트-바인드의 3D 멀티 모달 응용 프로그램. 영어: 조인트 3D 멀티 모달 임베딩 공간을 통해 Point-Bind는 많은 유망한 응용 프로그램 시나리오를 가능하게 합니다.예: 3D 지침 따르기를 위한 Point-LLM, 모든 모달리티에 조건화된 3D 생성, 3D를 사용한 임베딩 공간 산술, 멀티 모달 3D 제로샷 이해, 대규모 언어 모델(LLM) 내에서 학습된 언어 지식을 통해 3D 기하학을 더 잘 포착합니다.따라서 일반적인 3D 학습을 위해 멀티 모달리티에 맞춰 통합된 3D 프레임워크를 개발하는 방법은 여전히 미해결 문제로 남아 있습니다.아주 최근에 ImageBind[22]가 이미지, 텍스트, 오디오, 깊이, 열 및 IMU 데이터 등 6가지 서로 다른 모달리티에서 공유된 표현 공간을 학습하도록 제안되었습니다.이에 동기를 부여하여 다음과 같은 질문을 던집니다.통합된 3D 이해, 생성 및 지침 따르기를 위해 3D와 멀티 모달리티 사이에 조인트 임베딩 공간을 구성할 수 있을까요? 이를 위해, 그림 1에 표시된 것처럼 일반적인 3D 분석을 위해 포인트 클라우드를 여러 모달리티에 맞춰 정렬하는 3D 멀티모달리티 프레임워크인 Point-Bind를 소개합니다. 구체적으로, 우리는 3D-이미지-텍스트-오디오 쌍을 훈련 데이터로 수집하고, ImageBind에 의해 안내되는 공동 임베딩 공간을 구성합니다. 우리는 훈련 가능한 3D 인코더(예: I2PMAE [92])에서 추출된 특징과 ImageBind의 동결된 멀티모달 인코더 간의 대조 손실을 채택합니다. 이처럼 간단한 전략은 여러 모달리티를 통합된 표현 공간으로 효율적으로 통합할 수 있으며, 그림 2에서 다양한 3D 중심 멀티모달 작업을 허용합니다. • Point-Bind의 주요 기여는 다음과 같습니다. • ImageBind에 3D 맞춤. 공동 임베딩 공간 내에서 Point-Bind는 먼저 2D 이미지, 비디오, 언어, 오디오 등을 포함하여 ImageBind에 의해 안내되는 멀티모달리티에 3D 포인트 클라우드를 맞춥니다. • Any-to-3D 생성. 기존 텍스트-3D 생성 모델을 기반으로 Point-Bind는 텍스트/이미지/오디오/포인트-메시 생성 등 모든 모달리티에 따라 3D 모양 합성을 가능하게 합니다.• 3D 임베딩 공간 산술. Point-Bind의 3D 피처를 다른 모달리티에 추가하여 의미를 통합하고 구성된 크로스 모달 검색을 달성할 수 있음을 관찰했습니다.• 3D 제로샷 이해. Point-Bind는 3D 제로샷 분류를 위해 최첨단 성능을 달성합니다.또한, 우리의 접근 방식은 텍스트 참조 외에도 오디오 참조 3D 오픈 월드 이해를 지원합니다.더 나아가, 우리의 공동 임베딩 공간 위에 Point-Bind를 LLAMA[73]와 통합하여 நிென்라는 최초의 3D 대규모 언어 모델(LLM)을 개발할 것을 제안합니다.영어로 3D 객체를 자세히 설명합니다. ... 3D 객체는 건반과 현악기가 있는 악기인 그랜드 피아노입니다. 피아노에는 흰색과 검은색 건반이 격자 모양으로 배열된 피아노 건반이 있습니다...... 중국어로 ос + 请描述这个三维物体的细节。장면을 자세히 설명하세요.这个 3维物体是一架飞机,有 着明显的机翼呈圆机. 이 장면에는 비행기의 창가 좌석에 앉아 비행기가 이륙하는 동안 창밖을 내다보는 여성의 모습이 담겨 있습니다. 그녀는 바깥 풍경을 즐기고 있는 것 같습니다. 비행기의 날개가 배경에 보이고, 이 장면은 시작하려는 여행에 대한 설렘과 기대감을 포착합니다.C 이 장면의 세부 사항을 자세히 설명해 주세요.[물이 떨어지는 오디오] 이 장면은 수도꼭지가 켜진 욕조에 사람의 손이 물에 잠겨 있습니다.물이 튀며 비눗방울이 생기는데, 욕조에 보입니다.사람이 손을 씻거나 목욕을 하는 것처럼 보입니다.욕조는 왼쪽에 위치하고 수도꼭지는 오른쪽에 위치하여 물이 떨어지고 있습니다.Ogle.제공된 자료로 이 장면에 대해 무엇을 알고 있습니까?[Wood Door Creak 오디오] 이미지에는 사람이 손을 뻗어 열쇠로 문, 아마도 나무 문을 여는 모습이 나와 있습니다.사람은 문의 왼쪽에 위치하고 손은 손잡이에 가까이 있습니다. 이 장면은 사람의 손과 열쇠를 클로즈업으로 보여주는 것으로, 문을 여는 동작을 강조하고 있습니다.장면에 대한 자세한 해석을 제공해 주십시오.[키보드 타이핑 오디오] 이미지는 노트북 키보드 위에 마우스를 올려놓은 클로즈업을 특징으로 합니다.노트북 화면이 켜져 있고 화면에 &quot;노트북&quot;이라는 단어가 표시됩니다.키보드는 완전히 보이고 각 키가 깔끔하게 정리되어 있습니다.마우스는 키보드의 오른쪽에 위치하며 가운데 키 줄보다 약간 위에 있습니다.이 장면은 사용자가 노트북에서 작업하고 키보드로 타이핑하는 듯한 인상을 줍니다.이 장면을 자세히 설명해 주시겠습니까?이 장면에 대한 포괄적인 설명을 부탁드립니다.이 장면에 대한 자세한 설명을 제공해 주십시오.도서관에 서 있는 어린 소년이 등장하며, 주변에는 수많은 책이 있습니다.그는 손에 책을 들고 있고, 아마도 책을 읽거나 보고 있을 것입니다.책은 다양한 위치에 배열되어 있으며, 일부는 선반에, 다른 일부는 바닥에 놓여 있습니다. 책꽂이 설정은 소년이 독서에 열중하거나 방대한 양의 책을 탐색하는 도서관과 같은 학습 환경을 암시합니다.Q.. 이미지는 다양한 옷걸이에 둘러싸인 의류 매장에 서 있는 여성을 묘사합니다.그녀는 검은색 셔츠를 입고 있으며, 걸이에 진열된 품목을 보고 있는 것처럼 보입니다.매장에는 수많은 셔츠가 잘 비치되어 있으며, 일부는 걸이에 걸려 있고 다른 일부는 바닥에 놓여 있습니다.여성은 매장에서 판매하는 의류를 둘러보고 있는 것처럼 보입니다.... 이미지는 벽난로와 벽에 기대어 있는 사다리가 있는 방을 묘사합니다.사다리는 굴뚝이 있는 벽난로 근처에 있습니다.방은 깨진 창문과 무너진 천장으로 황폐해진 것처럼 보입니다.벽난로는 바닥에 흩어져 있는 책과 종이를 포함한 잔해로 둘러싸여 있습니다.방의 전반적인 분위기는 오랫동안 버려지거나 방치된 것처럼 보입니다.그림 3. Point-LLM의 3D 질의응답 예. 3D 및 다중 모달 지침이 주어지면, 우리의 Point-LLM은 세부적인 응답을 효과적으로 생성하고 우수한 교차 모달 추론을 수행할 수 있습니다. 특히, 우리는 Point-LLM으로 훈련을 위해 3D 지침 데이터가 필요하지 않습니다. 그림 2에서 볼 수 있듯이, 우리의 Point-LLM은 3D 포인트 클라우드 조건으로 언어 지침에 응답하고 공간적 기하학적 특성을 효과적으로 포착할 수 있습니다. ImageBind-LLM [19]을 참조하여, 우리는 PointBind와 LLAMA를 연결하기 위해 시각적 캐시 모델과 함께 바인드 네트워크를 활용하고, 매개변수 효율적인 미세 조정을 위해 초기화되지 않은 게이팅 메커니즘 [20, 91]을 채택합니다. 우수한 데이터 효율성으로, PointLLM의 전체 훈련 단계에는 3D 지침 데이터 세트가 필요하지 않으며, 비전 언어 조정을 위해 공개 비전 언어 데이터 [9,13,68,70]만 활용합니다. 이런 식으로, 우리는 LLM이 3D 및 다중 모달 데이터에 대한 교차 모달 추론을 이해하고 수행할 수 있도록 하여 영어와 중국어 모두에서 우수한 3D 질의 응답 용량을 달성합니다. • Point-LLM의 주요 기여는 다음과 같습니다. 3D 질의응답을 위한 Point-LLM. PointBind를 사용하여 영어와 중국어를 모두 지원하는 3D 포인트 클라우드 조건으로 명령에 응답하는 최초의 3D LLM인 Point-LLM을 소개합니다. • 데이터 및 매개변수 효율성. 3D 명령 데이터 없이 튜닝을 위해 공개 비전 언어 데이터만 활용하고 매개변수 효율적인 미세 조정 기술을 채택하여 광범위한 리소스를 절약합니다. • 3D 및 다중 모달 추론. 조인트 임베딩 공간을 통해 Point-LLM은 3D 및 다중 모달 입력의 조합(예: 이미지/오디오가 있는 포인트 클라우드)을 추론하여 설명적 응답을 생성할 수 있습니다. 2.
--- RELATED WORK ---
다중 모달 학습. 단일 모달 접근 방식에 비해 다중 모달 학습은 여러 모달리티에서 동시에 학습하여 더욱 견고하고 다양한 표현 학습을 달성하는 것을 목표로 합니다. 2D 이미지, 비디오, 텍스트 및 오디오[15, 17, 48]를 포함하여 수많은 연구에서 효능이 입증되었으며 다운스트림 작업의 교차 모달 성능[5, 25, 37, 61]과 텍스트 생성을 위한 비디오-텍스트-오디오 통합[36]을 향상시킵니다. 대표적인 시각-언어 사전 학습인 CLIP[59]은 2D 이미지와 텍스트 간의 격차를 효과적으로 메워 교차 모달 학습에 대한 추가 탐색을 장려합니다. 최근 ImageBind[22]는 공동 임베딩 공간에서 6개 모달리티를 성공적으로 정렬하여 새로운 제로샷 교차 모달 기능에 대한 힘을 발휘합니다. 그러나 ImageBind는 3D 포인트 클라우드에서의 효능을 조사하지 못했습니다. 3D 도메인에서 대부분의 기존 크로스 모달 작업은 3D 포인트 클라우드에 비전-언어 정렬[1, 10, 23, 84, 90]을 도입하고, 주로 오픈 월드 인식 작업에 초점을 맞추는데, 이는 더 광범위한 3D 애플리케이션을 위한 멀티 모달 의미론의 잠재력을 무시합니다. 이 논문에서, 우리의 Point-Bind는 ImageBind의 안내에 따라 3D 포인트 클라우드를 다른 여섯 가지 모달리티와 정렬하는 일반적인 3D 멀티 모달리티 모델을 개발하여 더 다양한 3D 크로스 모달 이해를 가능하게 합니다. 3D의 대형 모델. 대규모 사전 학습된 모델은 언어 및 2D 이미지 처리에서 놀라운 다운스트림 성능을 달성했습니다. 여기에서 영감을 얻어 많은 노력에서 3D 학습을 지원하기 위해 2D 및 언어 대형 모델을 도입했습니다. 이전 PointCLIP 시리즈[30, 90, 100]는 3D 포인트 클라우드를 깊이 맵에 투사하고, 제로 샷 인식에 CLIP[59]을 활용합니다. Image2Point[82]는 대신 2D 사전 학습된 모델을 우수한 네트워크 초기화로 3D 공간으로 변환합니다. 대조 학습을 통해 ULIP 시리즈[84, 85] 및 기타 작업[27, 39]은 CLIP의 시각 언어 임베딩 공간에 의해 안내되는 3D 네트워크를 사전 학습합니다. 또 다른 작업 분야는 설명적 텍스트 입력을 인코딩하여 텍스트 조건의 3D 객체[32, 41, 65, 83] 또는 양식화된 메시[45,47] 생성을 안내하기 위해 CLIP을 사용합니다. 일부 작업은 PointCLIP V2[100] 및 ViewRefer[24]와 같이 3D 공간 기하학에 대한 언어 기반 이해를 향상시키기 위해 GPT-3[6]도 채택합니다. 이들과 달리 우리는 ImageBind[22]를 활용하여 3D 포인트 클라우드와 여러 모달리티 간의 공동 임베딩 공간을 구성합니다. 파생된 Point-Bind는 일반적인 3D 교차 모달 이해, 생성 및 질의 응답을 위해 다중 모달 의미론을 잘 활용할 수 있습니다. 3D에서의 사전 학습. 최근 몇 년 동안 3D 비전 작업을 위한 지도 학습에서 상당한 진전이 이루어졌습니다[54, 55, 57, 93, 99]. 그러나 이러한 접근 방식은 도메인 외부 데이터에 대한 만족스러운 일반화 기능이 부족합니다. 이를 해결하기 위해 자기 지도 학습이 3D 전이 학습을 향상시키는 유망한 솔루션으로 등장했습니다[10, 34, 53, 86]. 대부분의 자기 지도 사전 학습
--- METHOD ---
s [35,49,52]는 높은 품질과 효율성으로 텍스트-3D 합성을 달성했습니다.그럼에도 불구하고 다중 모달 입력, 즉 모든-3D 생성에 따라 3D 모양을 생성하는 기능이 부족합니다.또 다른 일련의 작업은 설명적 자연어를 3D 데이터와 연결하여 3D 캡션[12, 87], 질문 답변[3, 78] 및 시각적 접지[24, 79]에 적용합니다.그러나 3D Q&amp;A를 위해 prePoint-LLM을 활용하지 못했습니다.모든-3D 생성 3D 임베딩 산술 3D 제로샷 이해 ➤ 3D 지침 데이터 필요 없음 3D 및 다중 모달 추론 텍스트-3D: 3D 객체를 자세히 설명합니다.&quot;자동차&quot; 3D 포인트 클라우드[바다 파도 오디오] 3D 객체는 작은 비행기, 구체적으로는 개인 제트기입니다. 그리고 검은색 색상 구성표이며 소형 항공기처럼 보입니다.그것은 ........ 무슨 일이 일어나고 있을까요?[물을 붓는 오디오]오디오-3D: [자동차 경적] 이미지-3D: a 포인트-3D: 이 장면은 사람이 맑은 액체(아마도 물)를 유리잔에 붓는 모습을 보여줍니다.유리잔은 테이블 위에 놓여 있고, 넘치도록 채워져 있어 상당한 양의 액체가 부어졌음을 나타냅니다.크로스 모달 검색 텍스트 참조: 비행기 피아노 기타 비행기, 피아노, 기타 CLASS 포인트-바인드 모델 오디오 참조: ↓ [비행기 엔진] [피아노 음악] [기타 사운드] 포인트-바인드 ↓ 비행기 피아노 기타 그림 2. 포인트-바인드의 3D 멀티 모달 응용 프로그램. 영어: 조인트 3D 멀티 모달 임베딩 공간을 통해 Point-Bind는 많은 유망한 응용 프로그램 시나리오를 가능하게 합니다.예: 3D 지침 따르기를 위한 Point-LLM, 모든 모달리티에 조건화된 3D 생성, 3D를 사용한 임베딩 공간 산술, 멀티 모달 3D 제로샷 이해, 대규모 언어 모델(LLM) 내에서 학습된 언어 지식을 통해 3D 기하학을 더 잘 포착합니다.따라서 일반적인 3D 학습을 위해 멀티 모달리티에 맞춰 통합된 3D 프레임워크를 개발하는 방법은 여전히 미해결 문제로 남아 있습니다.아주 최근에 ImageBind[22]가 이미지, 텍스트, 오디오, 깊이, 열 및 IMU 데이터 등 6가지 서로 다른 모달리티에서 공유된 표현 공간을 학습하도록 제안되었습니다.이에 동기를 부여하여 다음과 같은 질문을 던집니다.통합된 3D 이해, 생성 및 지침 따르기를 위해 3D와 멀티 모달리티 사이에 조인트 임베딩 공간을 구성할 수 있을까요? 이를 위해, 그림 1에 표시된 것처럼 일반적인 3D 분석을 위해 포인트 클라우드를 여러 모달리티에 맞춰 정렬하는 3D 멀티모달리티 프레임워크인 Point-Bind를 소개합니다. 구체적으로, 우리는 3D-이미지-텍스트-오디오 쌍을 훈련 데이터로 수집하고, ImageBind에 의해 안내되는 공동 임베딩 공간을 구성합니다. 우리는 훈련 가능한 3D 인코더(예: I2PMAE [92])에서 추출된 특징과 ImageBind의 동결된 멀티모달 인코더 간의 대조 손실을 채택합니다. 이처럼 간단한 전략은 여러 모달리티를 통합된 표현 공간으로 효율적으로 통합할 수 있으며, 그림 2에서 다양한 3D 중심 멀티모달 작업을 허용합니다. • Point-Bind의 주요 기여는 다음과 같습니다. • ImageBind에 3D 맞춤. 공동 임베딩 공간 내에서 Point-Bind는 먼저 2D 이미지, 비디오, 언어, 오디오 등을 포함하여 ImageBind에 의해 안내되는 멀티모달리티에 3D 포인트 클라우드를 맞춥니다. • Any-to-3D 생성. 기존 텍스트-3D 생성 모델을 기반으로 Point-Bind는 텍스트/이미지/오디오/포인트-메시 생성 등 모든 모달리티에 따라 3D 모양 합성을 가능하게 합니다.• 3D 임베딩 공간 산술. Point-Bind의 3D 피처를 다른 모달리티에 추가하여 의미를 통합하고 구성된 크로스 모달 검색을 달성할 수 있음을 관찰했습니다.• 3D 제로샷 이해. Point-Bind는 3D 제로샷 분류를 위해 최첨단 성능을 달성합니다.또한, 우리의 접근 방식은 텍스트 참조 외에도 오디오 참조 3D 오픈 월드 이해를 지원합니다.더 나아가, 우리의 공동 임베딩 공간 위에 Point-Bind를 LLAMA[73]와 통합하여 நிென்라는 최초의 3D 대규모 언어 모델(LLM)을 개발할 것을 제안합니다.영어로 3D 객체를 자세히 설명합니다. ... 3D 객체는 건반과 현악기가 있는 악기인 그랜드 피아노입니다. 피아노에는 흰색과 검은색 건반이 격자 모양으로 배열된 피아노 건반이 있습니다...... 중국어로 ос + 请描述这个三维物体的细节。장면을 자세히 설명하세요.这个 3维物体是一架飞机,有 着明显的机翼呈圆机. 이 장면에는 비행기의 창가 좌석에 앉아 비행기가 이륙하는 동안 창밖을 내다보는 여성의 모습이 담겨 있습니다. 그녀는 바깥 풍경을 즐기고 있는 것 같습니다. 비행기의 날개가 배경에 보이고, 이 장면은 시작하려는 여행에 대한 설렘과 기대감을 포착합니다.C 이 장면의 세부 사항을 자세히 설명해 주세요.[물이 떨어지는 오디오] 이 장면은 수도꼭지가 켜진 욕조에 사람의 손이 물에 잠겨 있습니다.물이 튀며 비눗방울이 생기는데, 욕조에 보입니다.사람이 손을 씻거나 목욕을 하는 것처럼 보입니다.욕조는 왼쪽에 위치하고 수도꼭지는 오른쪽에 위치하여 물이 떨어지고 있습니다.Ogle.제공된 자료로 이 장면에 대해 무엇을 알고 있습니까?[Wood Door Creak 오디오] 이미지에는 사람이 손을 뻗어 열쇠로 문, 아마도 나무 문을 여는 모습이 나와 있습니다.사람은 문의 왼쪽에 위치하고 손은 손잡이에 가까이 있습니다. 이 장면은 사람의 손과 열쇠를 클로즈업으로 보여주는 것으로, 문을 여는 동작을 강조하고 있습니다.장면에 대한 자세한 해석을 제공해 주십시오.[키보드 타이핑 오디오] 이미지는 노트북 키보드 위에 마우스를 올려놓은 클로즈업을 특징으로 합니다.노트북 화면이 켜져 있고 화면에 &quot;노트북&quot;이라는 단어가 표시됩니다.키보드는 완전히 보이고 각 키가 깔끔하게 정리되어 있습니다.마우스는 키보드의 오른쪽에 위치하며 가운데 키 줄보다 약간 위에 있습니다.이 장면은 사용자가 노트북에서 작업하고 키보드로 타이핑하는 듯한 인상을 줍니다.이 장면을 자세히 설명해 주시겠습니까?이 장면에 대한 포괄적인 설명을 부탁드립니다.이 장면에 대한 자세한 설명을 제공해 주십시오.도서관에 서 있는 어린 소년이 등장하며, 주변에는 수많은 책이 있습니다.그는 손에 책을 들고 있고, 아마도 책을 읽거나 보고 있을 것입니다.책은 다양한 위치에 배열되어 있으며, 일부는 선반에, 다른 일부는 바닥에 놓여 있습니다. 책꽂이 설정은 소년이 독서에 열중하거나 방대한 양의 책을 탐색하는 도서관과 같은 학습 환경을 암시합니다.Q.. 이미지는 다양한 옷걸이에 둘러싸인 의류 매장에 서 있는 여성을 묘사합니다.그녀는 검은색 셔츠를 입고 있으며, 걸이에 진열된 품목을 보고 있는 것처럼 보입니다.매장에는 수많은 셔츠가 잘 비치되어 있으며, 일부는 걸이에 걸려 있고 다른 일부는 바닥에 놓여 있습니다.여성은 매장에서 판매하는 의류를 둘러보고 있는 것처럼 보입니다.... 이미지는 벽난로와 벽에 기대어 있는 사다리가 있는 방을 묘사합니다.사다리는 굴뚝이 있는 벽난로 근처에 있습니다.방은 깨진 창문과 무너진 천장으로 황폐해진 것처럼 보입니다.벽난로는 바닥에 흩어져 있는 책과 종이를 포함한 잔해로 둘러싸여 있습니다.방의 전반적인 분위기는 오랫동안 버려지거나 방치된 것처럼 보입니다.그림 3. Point-LLM의 3D 질의응답 예. 3D 및 다중 모달 지침이 주어지면, 우리의 Point-LLM은 세부적인 응답을 효과적으로 생성하고 우수한 교차 모달 추론을 수행할 수 있습니다. 특히, 우리는 Point-LLM으로 훈련을 위해 3D 지침 데이터가 필요하지 않습니다. 그림 2에서 볼 수 있듯이, 우리의 Point-LLM은 3D 포인트 클라우드 조건으로 언어 지침에 응답하고 공간적 기하학적 특성을 효과적으로 포착할 수 있습니다. ImageBind-LLM [19]을 참조하여, 우리는 PointBind와 LLAMA를 연결하기 위해 시각적 캐시 모델과 함께 바인드 네트워크를 활용하고, 매개변수 효율적인 미세 조정을 위해 초기화되지 않은 게이팅 메커니즘 [20, 91]을 채택합니다. 우수한 데이터 효율성으로, PointLLM의 전체 훈련 단계에는 3D 지침 데이터 세트가 필요하지 않으며, 비전 언어 조정을 위해 공개 비전 언어 데이터 [9,13,68,70]만 활용합니다. 이런 식으로, 우리는 LLM이 3D 및 다중 모달 데이터에 대한 교차 모달 추론을 이해하고 수행할 수 있도록 하여 영어와 중국어 모두에서 우수한 3D 질의 응답 용량을 달성합니다. • Point-LLM의 주요 기여는 다음과 같습니다.3D 질의응답을 위한 Point-LLM.PointBind를 사용하여 영어와 중국어를 모두 지원하는 3D 포인트 클라우드 조건으로 명령에 응답하는 최초의 3D LLM인 Point-LLM을 소개합니다.• 데이터 및 매개변수 효율성.3D 명령 데이터 없이 튜닝을 위해 공개 비전 언어 데이터만 활용하고 매개변수 효율적인 미세 조정 기술을 채택하여 광범위한 리소스를 절약합니다.• 3D 및 다중 모달 추론.공동 임베딩 공간을 통해 Point-LLM은 이미지/오디오가 있는 포인트 클라우드와 같이 3D 및 다중 모달 입력의 조합을 추론하여 설명적 응답을 생성할 수 있습니다.2. 관련 연구 다중 모달 학습.단일 모달 접근 방식에 비해 다중 모달 학습은 여러 모달리티에서 동시에 학습하여 더욱 견고하고 다양한 표현 학습을 달성하는 것을 목표로 합니다. 수많은 연구에서 2D 이미지, 비디오, 텍스트 및 오디오[15, 17, 48]를 포함하여 그 효능이 입증되었고 다운스트림 작업의 교차 모달 성능[5, 25, 37, 61]과 텍스트 생성을 위한 비디오-텍스트-오디오 통합[36]이 향상되었습니다. 대표적인 시각-언어 사전 학습인 CLIP[59]은 2D 이미지와 텍스트 간의 격차를 효과적으로 메워 교차 모달 학습에 대한 추가 탐색을 장려합니다. 최근 ImageBind[22]는 공동 임베딩 공간에서 6개 모달리티를 성공적으로 정렬하여 떠오르는 제로샷 교차 모달 기능에 대한 힘을 발휘합니다. 그러나 ImageBind는 3D 포인트 클라우드에서의 효능을 조사하지 못했습니다. 3D 도메인에서 대부분의 기존 크로스 모달 작업은 3D 포인트 클라우드에 비전-언어 정렬[1, 10, 23, 84, 90]을 도입하고, 주로 오픈 월드 인식 작업에 초점을 맞추는데, 이는 더 광범위한 3D 애플리케이션을 위한 멀티 모달 의미론의 잠재력을 무시합니다. 이 논문에서, 우리의 Point-Bind는 ImageBind의 안내에 따라 3D 포인트 클라우드를 다른 여섯 가지 모달리티와 정렬하는 일반적인 3D 멀티 모달리티 모델을 개발하여 더 다양한 3D 크로스 모달 이해를 가능하게 합니다. 3D의 대형 모델. 대규모 사전 학습된 모델은 언어 및 2D 이미지 처리에서 놀라운 다운스트림 성능을 달성했습니다. 여기에서 영감을 얻어 많은 노력에서 3D 학습을 지원하기 위해 2D 및 언어 대형 모델을 도입했습니다. 이전 PointCLIP 시리즈[30, 90, 100]는 3D 포인트 클라우드를 깊이 맵에 투사하고, 제로 샷 인식에 CLIP[59]을 활용합니다. Image2Point[82]는 대신 2D 사전 학습된 모델을 우수한 네트워크 초기화로 3D 공간으로 변환합니다. 대조 학습을 통해 ULIP 시리즈[84, 85] 및 기타 작업[27, 39]은 CLIP의 시각 언어 임베딩 공간에 의해 안내되는 3D 네트워크를 사전 학습합니다. 또 다른 작업 분야는 설명적 텍스트 입력을 인코딩하여 텍스트 조건의 3D 객체[32, 41, 65, 83] 또는 양식화된 메시[45,47] 생성을 안내하기 위해 CLIP을 사용합니다. 일부 작업은 PointCLIP V2[100] 및 ViewRefer[24]와 같이 3D 공간 기하학에 대한 언어 기반 이해를 향상시키기 위해 GPT-3[6]도 채택합니다. 이들과 달리 우리는 ImageBind[22]를 활용하여 3D 포인트 클라우드와 여러 모달리티 간의 공동 임베딩 공간을 구성합니다. 파생된 Point-Bind는 일반적인 3D 교차 모달 이해, 생성 및 질의 응답을 위해 멀티모달 의미론을 잘 활용할 수 있습니다.3D 사전 학습.최근 몇 년 동안 3D 비전 작업을 위한 지도 학습에서 상당한 진전이 이루어졌습니다[54, 55, 57, 93, 99].그러나 이러한 접근 방식은 도메인 외부 데이터에 대한 만족스러운 일반화 기능이 부족합니다.이를 해결하기 위해 자기 지도 학습이 3D 전이 학습을 향상시키는 유망한 솔루션으로 등장했습니다[10, 34, 53, 86].대부분의 자기 지도 사전 학습 방법은 인코더-디코더 프레임워크를 사용하여 포인트 클라우드를 잠재 표현으로 인코딩한 다음 원래 데이터 형태를 재구성합니다[62, 66, 75].여기에서 Point-MAE[50]와 PointM2AE[89]는 3D 포인트 클라우드 사전 학습에 마스크된 자동 인코더[26]를 도입하여 다양한 3D 작업에서 경쟁력 있는 결과를 달성합니다. 또는, 3D 일반화 능력을 향상시키기 위해 교차 모달 사전 학습 접근 방식도 활용됩니다[40, 56, 58, 77]. 예를 들어, ACT[16] 및 I2P-MAE[92]는 사전 학습된 2D 변환기를 교사로 활용하여 3D 표현 학습을 안내합니다. 이전 작업에서 영감을 얻어 자체 감독 사전 학습을 위해 수집된 3D-이미지-텍스트-오디오 쌍을 채택하고 ImageBind의 인코더를 대조 학습을 위한 안내로 간주합니다. 이런 식으로 Point-Bind는 3D와 다중 모달 간의 공동 임베딩 공간을 얻도록 사전 학습되어 다양한 3D 다운스트림 작업에서 뛰어난 성능을 발휘합니다. 3. Point-Bind Point-Bind의 전체 파이프라인은 그림 4에 나와 있습니다. 섹션 3.1에서 먼저 ImageBind[22]의 예비 정보를 제공합니다. 그런 다음, 섹션 3.2 및 3.3에서 각각 Point-Bind에 대한 훈련 데이터 및 다중 모달 정렬에 대해 설명합니다.마지막으로, 섹션 3.4에서 Point-Bind에서 파생된 여러 3D 중심 애플리케이션을 소개합니다.3.1. ImageBind의 서론 ImageBind[22]는 여러 모달리티를 결합하는 접근 방식을 제안하며, 이는 이미지 쌍으로 구성된 3D-이미지-텍스트-오디오 쌍만을 활용합니다.3D 포인트 클라우드: 3D 인코더 ↑ 3D 임베딩 크로스 모달 검색 헤드 o 오디오: [피아노 음악] →o 텍스트: &quot;A piano in the ...&quot; o 이미지: 피아노 이미지 Point-LLM 대조 손실 오디오: [피아노 음악] 텍스트: &quot;피아노 모델.&quot; ImageBind 이미지 및 텍스트 및 오디오 임베딩 3D 생성 디코더 →&gt; 이미지: 제로 샷 인식 헤드 3D 객체를 설명합니다. 3D 객체는 악기인 그랜드 피아노입니다 ↑ 악보 비행기 피아노 기타 키보드 그림 4. Point-Bind의 전체 파이프라인. 대조 학습을 위해 3D-이미지-오디오-텍스트 데이터 쌍을 수집하여 3D 모달리티를 다른 ImageBind[22]가 안내하는 것과 맞춥니다. 조인트 임베딩 공간을 갖춘 Point-Bind는 3D 크로스 모달 검색, 모든-3D 생성, 3D 제로샷 이해 및 3D 대규모 언어 모델인 Point-LLM을 개발하는 데 활용할 수 있습니다. 데이터를 사용하여 이미지, 텍스트, 오디오, 깊이, 열 및 IMU 데이터인 6가지 모달리티의 조인트 임베딩 공간을 학습합니다. 6가지 모달리티를 모두 페어링하는 학습 데이터 세트가 필요하지 않지만 2D 이미지의 바인딩 속성을 활용하여 각 모달리티를 이미지에 독립적으로 정렬합니다. 구체적으로 ImageBind는 다중 모달 입력을 해당 인코더에 공급하고 크로스 모달 대조 학습을 위해 채택합니다. ImageBind는 대규모 이미지 쌍 데이터에 대한 학습 후 6가지 모달리티를 단일 공유 표현 공간으로 효과적으로 정렬하여 새로운 모달 간 제로 샷 기능을 구현합니다.기존 시각-언어 모델을 기반으로 ImageBind는 텍스트-오디오/비디오 검색, 오디오-이미지 생성, 오디오 참조 객체 감지와 같은 여러 멀티 모달 작업에도 활용할 수 있습니다.이에 영감을 받아 일반적인 3D 이해, 생성 및 지침 따르기를 위해 3D 포인트 클라우드를 다른 모달리티와 통합하는 3D 멀티 모달 프레임워크를 개발할 것을 제안합니다.3.2. 학습 데이터 3D를 멀티 모달리티와 정렬하기 위해 ImageBind[22]의 사전 학습된 조인트 임베딩 공간을 활용하고 대조 손실[59,96]을 사용하여 3D 포인트 클라우드를 이미지, 텍스트 및 오디오의 다른 세 가지 모달리티와 동시에 정렬합니다. 대조적 훈련 데이터를 얻기 위해 3D-이미지-오디오-텍스트 쌍의 교차 모달 데이터 세트를 수집합니다. 다음과 같이 데이터 세트 수집을 위한 세 단계가 있습니다. 3D-이미지-텍스트 쌍. 우리는 ULIP [84]의 3D, 이미지 및 텍스트 데이터 쌍을 채택합니다. 여기에는 풍부한 3D CAD 모델을 포함하는 일반적으로 사용되는 데이터 세트인 ShapeNet [8]에서 구축된 3D이미지-텍스트 트리플릿이 포함됩니다. 각 3D 포인트 클라우드는 공간 모양의 의미 정보를 설명하는 해당 텍스트와 다중 뷰 이미지 렌더링으로 생성된 2D 대응물과 쌍을 이룹니다. 텍스트 설명은 범주 이름과 64개의 사전 정의된 템플릿의 synset으로 구성됩니다. 3D-오디오 쌍. 더 많은 대조 신호를 제공하기 위해 ESC-50 [51] 및 ShapeNet 데이터 세트에서 3D 및 오디오의 데이터 쌍도 수집합니다. 구체적으로, 먼저 &#39;비행기&#39;, &#39;시계&#39;, &#39;세탁기&#39;, &#39;키보드&#39;와 같이 ShapeNet의 55개 카테고리에서 실제 세계에서 소리를 낼 수 있는 객체의 카테고리를 선택합니다. 그런 다음 ESC-50 내에 있는 카테고리만 보존합니다. 이 표준에 따라 광범위한 오디오 클립이 있는 3D 오디오 쌍 데이터의 9개 카테고리를 얻습니다. 3D 이미지 오디오 텍스트 쌍 구축. 마지막으로 각 3D 오디오 쌍을 해당 3D 이미지 텍스트 데이터와 매칭하여 광범위한 크로스 모달 쌍이 있는 통합된 3D 이미지 오디오 텍스트 데이터 세트를 만듭니다. 학습하는 동안 대조 학습을 위해 포인트 클라우드와 세 가지 모달리티의 쌍 데이터를 동시에 공급합니다. 3.3. 다중 모달리티에 3D 정렬 3D 쌍 데이터를 수집한 후 대조 학습을 수행하여 3D 및 다중 모달리티를 정렬하는 공동 임베딩 공간을 학습합니다. 각 데이터 샘플에는 포인트 클라우드 P와 페어링된 2D 이미지 I, 텍스트 설명 Ts, 오디오 A가 포함되어 있으며, 여기서 Ts는 64개의 사전 정의된 템플릿 세트를 나타냅니다. 포인트 클라우드의 경우, Encoder3D()로 표시되는 학습 가능한 3D 인코더로 I2P-MAE[92]를 채택하고, 인코딩된 3D 피처를 ImageBind의 다중 모달 임베딩 공간으로 변환하는 두 개의 선형 레이어의 투영 네트워크 Proj(·)를 추가합니다. 이를 F3D=Proj(Encoder3D(P)), (1)로 공식화합니다. 여기서 F3D ER1×C는 투영된 3D 임베딩을 나타내고 C는 ImageBind의 피처 차원과 같습니다. 이미지-텍스트-오디오 쌍 데이터의 경우, 우리는 ImageBind의 해당 인코더를 활용하여 특징 추출을 수행하는데, 이는 학습하는 동안 동결되며 F2D, F, FA = ImageBind(I, T³, A), (2)로 공식화됩니다. 여기서 F2D, FA E R¹×C는 이미지 및 오디오 임베딩을 나타내고 F = R64×C는 64개 설명 세트에 대한 텍스트 임베딩을 나타냅니다. 그런 다음, FT = Average (Fƒ) Є R¹×C로 평균 풀링을 수행합니다. (3) 이는 보다 견고한 집계된 텍스트 임베딩을 나타냅니다. 그 후, 3D와 다른 모달리티 간에 대조 손실[96]을 채택하여 3D 임베딩이 공동 표현 공간과 일치하도록 효과적으로 적용하며 Ltotal = L(F3D, F2D) + L(F3D, FT) + L(F3D, FA)로 공식화됩니다. 일부 훈련 데이터 범주에는 본질적으로 소리를 낼 수 없기 때문에 페어링된 오디오 A가 포함되지 않는다는 점에 유의하세요(예: 병, 화분, 소파). 이러한 오디오 특징과 손실은 무시합니다. 3.4. 다중 모달 애플리케이션 PointBind의 조인트 임베딩 공간의 이점을 활용하여 3D 및 다중 모달리티와 관련된 여러 가지 새로운 애플리케이션 시나리오를 각각 도입합니다. 모든 모달에서 3D로의 생성. 2D 생성 모델에서 상속받은 기존 3D 생성 방법은 텍스트에서 3D로의 합성만 달성할 수 있습니다. 반면 Point-Bind의 조인트 임베딩 공간을 사용하면 텍스트/이미지/오디오/포인트 토메시와 같은 모든 모달리티에 따라 3D 모양을 생성할 수 있습니다. 자세히 말하면 Point-Bind의 다중 모달 인코더를 현재 CLIP 기반 텍스트에서 3D로의 모델(예: CLIP-Forge[65])의 사전 훈련된 디코더와 직접 연결합니다. 추가 학습 없이 입력 자동차 경적을 기반으로 3D 자동차 메시를 합성할 수 있습니다.3D 임베딩 공간 산술.Point-Bind로 인코딩된 3D 피처는 다른 모달리티와 직접 추가하여 의미를 통합하고 구성된 크로스 모달 검색을 더욱 달성할 수 있음을 관찰했습니다.예를 들어, 3D 자동차와 파도 오디오의 결합된 임베딩은 해변 옆에 주차된 자동차를 보여주는 이미지를 검색할 수 있는 반면, 3D 노트북과 키보드 타이핑 오디오의 구성은 노트북으로 작업하는 사람의 이미지를 검색할 수 있습니다.3D 제로샷 이해.기존 텍스트 추론 3D 제로샷 분류의 경우 Point-Bind는 추가 멀티모달 감독의 안내를 받아 최첨단 성능을 달성합니다.게다가 Point-Bind는 오디오 참조 3D 오픈 월드 이해, 즉 해당 오디오 데이터에서 표시된 새로운 범주의 3D 모양을 인식할 수도 있습니다[51]. 4. Point-LLM 이 섹션에서는 Point-Bind를 활용하여 Point-LLM이라고 하는 3D 대규모 언어 모델(LLM)을 개발하는 방법을 설명합니다.이 모델은 LLAMA[73]를 미세 조정하여 3D 질의 응답 및 다중 모달 추론을 달성합니다.Point-LLM의 전체 파이프라인은 그림 5에 나와 있습니다.4.1. 3D 명령어 수행 용량 Point-LLM은 ImageBind의 의미를 LLAMA에 주입하여 다중 모달 명령어 조정을 수행하는 ImageBindLLM[19] 위에 개발되었습니다.우리의 접근 방식은 데이터와 매개변수 효율성을 모두 보여줍니다.3D 명령어 데이터 필요 없음.학습 중에는 LLAMA를 미세 조정하여 3D 조건 응답 용량을 학습하는 데 공개 시각 언어 데이터[9,13,68,70]만 필요합니다. Point-Bind는 3D와 다중 모달리티 사이에 조인트 임베딩 공간을 구축했으므로 모달리티 중 하나가 LLAMA와 연결되도록 훈련되면 다른 모달리티도 동시에 정렬됩니다.이를 고려하여 가장 많은 공개 데이터가 있는 2D 이미지 모달리티를 선택합니다.ImageBind의 이미지 인코더만 LLAMA와 정렬함으로써 대규모 3D 명령어 데이터를 수집하고 주석을 달기 위한 값비싼 비용을 피할 수 있어 광범위한 리소스를 절약할 수 있습니다.매개변수 효율적 훈련.전체 LLMS[38, 97]를 조정하는 대신 효율적인 시각 언어 명령어 조정을 위해 LLAMA 내에서 일부 매개변수만 동결 해제합니다.특히 학습 가능한 바인드 네트워크를 채택하여 ImageBind의 이미지 인코더와 LLAMA의 언어 공간을 연결합니다. 그런 다음, 0으로 초기화된 게이팅 메커니즘을 제안하여 바인드 2D 이미지 뒤에 이미지 피처를 추가합니다.오디오: [물 붓기] 3D 포인트 클라우드: 선택적 다중 모달 입력 변환된 피처 주입 ImageBind ↑ 바인드 네트워크 ↑ 캐시 모델 포인트 바인드 추가 검색으로 향상됨 무슨 일이 일어나고 있나요?↓ LLAMA ↓ 이 장면은 사람이 테이블 위에 놓인 유리잔에 맑은 액체를 붓는 모습을 보여줍니다...그림 5. Point-LLM의 추론 패러다임.ImageBind-LLM[19]을 참조하여 바인드 네트워크, 시각적 캐시 모델 및 0으로 초기화된 게이팅 메커니즘을 채택하여 LLAMA[73]가 3D 지침을 따르도록 미세 조정합니다.선택적으로 Point-LLM은 다중 모달 데이터를 입력으로 사용하고 언어 응답에 대한 교차 모달 추론을 수행할 수도 있습니다.네트워크를 LLAMA 내의 단어 토큰으로 변환합니다. 이 메커니즘은 LLAMA-Adapter[91]에서 영감을 얻어 초기 단계에서 안정적인 학습을 위해 LLAMA에 시각적 지시 신호를 점진적으로 주입할 수 있습니다.이러한 매개변수 효율적인 미세 조정 전략에 따라 LLAMA의 대부분 매개변수는 고정되고, 0으로 초기화된 게이팅 요인과 바이어스 규범 가중치[91]만 미세 조정을 위해 학습할 수 있습니다.추가 학습 세부 정보는 ImageBind-LLM[19]을 참조하십시오.시각-언어 학습 후, 조인트 임베딩 공간을 통해 LLAMA는 ImageBind 내의 오디오 및 Point-Bind의 3D 포인트 클라우드와 같은 다른 모달리티와 자연스럽게 정렬할 수 있습니다.따라서 Point-LLM은 3D 지시 데이터 없이도 LLAMA에 3D 지시 따르기 용량을 효과적으로 제공하여 뛰어난 데이터 효율성을 나타냅니다.4.2. 3D 질의 응답 입력 언어 지시와 3D 포인트 클라우드의 경우 각각 미세 조정된 LLaMA와 PointBind에 공급합니다. 그런 다음, 인코딩된 3D 특징은 바인드 네트워크에 공급되기 전에 ImageBindLLM에서 제안된 시각적 캐시 모델에 의해 향상됩니다. 캐시 모델은 추론 중에만 채택되고, 학습 없는 방식으로 구성됩니다[94]. 시각적 캐시에 의한 향상. 학습을 위해 ImageBind의 이미지 인코더를 채택하지만 추론을 위해 PointBind의 3D 인코더로 전환함에 따라 캐시 모델은 더 나은 3D 기하 이해를 위해 이러한 모달리티 불일치를 완화하도록 설계되었습니다. ImageBind-LLM을 참조하면, 캐시 모델은 지식 검색을 위한 키와 값으로 간주되는 학습 데이터에서 세 개의 ImageBind로 인코딩된 이미지 특징을 저장합니다. 입력 3D 특징을 쿼리로 간주하고 캐시 모델에서 상위 k개의 유사한 시각적 키를 검색합니다. 그런 다음 코사인 유사도에 따라 해당 캐시된 값(상위 k개의 유사한 이미지 특징)을 집계하고 잔여 연결을 통해 결과를 원래 3D 특징에 추가합니다. 이런 식으로, 향상된 3D 기능은 캐시 모델에서 유사한 시각적 의미론을 적응적으로 통합할 수 있습니다. 이를 통해 3D 모양의 표현 품질이 향상되고 Point-LLM 내의 2D-3D 인코더의 의미적 갭이 완화됩니다. 이후, 향상된 기능은 기능 변환을 위해 바인드 네트워크에 공급되고 응답 생성을 위해 LLAMA에 공급됩니다. 3D 및 다중 모달 추론. Point-LLM은 포인트 클라우드 외에도 교차 모달 추론을 수행하고 다중 모달리티에 따라 응답을 생성할 수 있습니다. 추가 입력 이미지 또는 오디오의 경우 ImageBind의 이미지 또는 오디오 인코더를 사용하여 기능을 추출하고 Point-Bind에서 인코딩한 3D 기능과 직접 추가합니다. 이러한 통합 기능을 LLAMA에 주입함으로써 Point-LLM은 교차 모달 의미론을 추론하고 모든 입력 모달리티의 정보로 응답할 수 있습니다. 이는 다중 모달리티를 3D LLM과 정렬하는 것의 유망한 중요성을 보여줍니다. 5.
--- EXPERIMENT ---
s 그런 다음, 이 섹션에서는 먼저 Point-Bind에 대한 멀티모달 훈련의 구현 세부 사항을 제시합니다. 3D 지침 따르기를 위한 Point-LLM, 3D 교차 모달 검색, 3D 임베딩 공간 산술, 모든 3D 생성+ → 바다 파도 + 키보드 타이핑 +… 물 그림 6. 3D 및 오디오의 임베딩 공간 산술. 3D 포인트 클라우드와 오디오 임베딩을 결합하여 2D 이미지를 검색하여 멀티모달 의미 구성 및 3D 제로 샷 이해를 위한 Point-Bind의 기능을 보여줍니다. 마지막으로 설계의 효과를 확인하기 위해 절제 연구를 수행합니다. 5.1. 구현 세부 정보 우리는 Point-Bind의 3D 인코더로 사전 훈련된 I2P-MAE[92]를 채택하고, 사전 훈련을 위해 수집된 3D-이미지-텍스트-오디오 쌍을 활용합니다. 우리는 새로 추가된 투영 네트워크로 3D 인코더만 업데이트하고, ImageBind[22]에서 다른 모달리티의 인코더를 동결합니다. 투영 네트워크는 중간 LayerNorm[4]이 있는 두 개의 선형 레이어로 구성됩니다. 우리는 64의 배치 크기로 300에포크 동안 Point-Bind를 훈련하고, 학습률이 0.003인 최적화 도구로 AdamW[44]를 채택합니다. 5.2. 3D Q&amp;A 설정을 위한 Point-LLM. 우리는 ImageBind-LLM[19]을 참조하여 사전 훈련된 LLAMA 7B 모델[73]에 3D 명령어를 주입하기 위한 매개변수 및 데이터 효율적인 미세 조정을 수행합니다. 자세히 말하면, 미세 조정 기술에는 0으로 초기화된 게이팅[20,91], LORA[29], 바이어스-노름 조정[18,21, 81,88]이 포함됩니다. 우리는 시각-언어 훈련을 위해 여러 데이터 세트[9,68,70]를 활용하며, 학습된 조인트 임베딩 공간으로 인해 3D 지시 따르기 데이터 세트가 필요하지 않습니다. 분석. 그림 3에서 우리는 유리한 3D 지시 따르기 및 다중 모달 추론 용량을 보여주는 Point-LLM의 질의 응답 예를 제공합니다. 표시된 대로 영어 또는 중국어 지시에 대해 PointLLM은 입력 포인트 클라우드의 공간적 기하학을 효과적으로 통합하고 자세한 언어 응답을 생성할 수 있습니다. 이는 전역 및 지역적 특성에 대한 포괄적인 3D 이해를 얻습니다.예를 들어, 3D-to-3D, 2D-to-3D, 3D-to-2D 및 텍스트-to-3D 검색을 포함한 3D 크로스 모달 검색 성능입니다.ModelNet40 [80] 데이터 세트에 대한 mAP 점수(%)를 보고합니다.방법 3D 3D 2D → 3D 3D → 2D 텍스트 → 3D PointCLIP [90] PointCLIP-V2 [100] ULIP [84] Point-Bind Gain 37.13.5.10.47.20.9.52.60.20.29.50.63.34.42.64.+2.+14.+13.+13.피아노 건반의 모양과 비행기 날개와 꼬리 모양을 인식합니다. 그런 다음 Point-LLM도 교차 모달 이해로 응답할 수 있습니다. 2D 이미지나 오디오가 있는 입력 3D 모델의 경우 Point-LLM은 LLAMA가 두 조건을 모두 이해하고 추론할 수 있도록 하여 출력 언어 응답에 다중 모달 의미론을 통합할 수 있습니다. 뛰어난 데이터 및 매개변수 효율성으로 이 예는 Point-LLM의 3D 다중 모달 지시 수행 기능을 나타냅니다. 5.3. 3D 교차 모달 검색 Point-Bind의 다중 모달 정렬을 평가하기 위해 여러 교차 모달 검색 작업, 즉 3D-3D, 2D-3D, 3D-2D 및 텍스트-3D 검색을 실험합니다. 설정. 40개 범주의 학습을 위한 9,CAD 모델과 테스트를 위한 2,468개가 포함된 다중 모달 ModelNet40[80] 데이터 세트에서 3D 제로 샷 검색을 수행합니다. ModelNet40은 이미지, 포인트 클라우드, 메시의 세 가지 검색 모달리티의 데이터를 제공합니다. embedText-to-3D: &quot;비행기&quot; &quot;의자&quot; Audio-to-3D: Image-to-3D: Point-to-3D: [자동차 경적] [트럭 소리] … ↑ 그림 7. Any-to-3D 생성. CLIP-Forge[65]에 따르면, 우리가 구축한 조인트 임베딩 공간은 텍스트, 오디오, 이미지, 포인트 클라우드 입력을 조건으로 하는 3D 메시 모델을 효과적으로 생성할 수 있습니다. 표 2. 3D 제로샷 분류의 성능. ModelNet40[80]에서 분류 정확도(%)를 보고합니다. 모델 인코더 성능 PointCLIP [90] ULIP [84] CLIP 20.Point-BERT 60.PointCLIP V2 [100] CLIP 64.ULIP 2 [85] Point-BERT 66.Point-Bind Point-BERT I2P-MAE 76.78.+11.Point-Bind 포인트 클라우드 및 기타 모달리티의 이득. 이전 연구[33,42]에 따라 검색 작업에 일반적으로 사용되는 평가 기준인 평균 평균 정밀도(mAP) 점수를 통해 네트워크를 측정합니다. 분석. 표 1에서는 3D 제로 샷 검색에 대한 정량적 결과를 보고합니다. 여기서 Point-Bind는 이전 연구와 비교하여 모든 벤치마크에서 최첨단 성능을 달성했습니다. 특히, 2D-3D 및 텍스트-3D 검색의 경우 Point-Bind는 2위 ULIP[84]를 각각 +14.29% 및 +13.99% 개선으로 상당히 능가합니다. 이는 우리 접근 방식의 우수한 크로스 모달 이해 용량을 나타냅니다. 5.4. 3D를 사용한 임베딩 공간 산술 다중 모달 정렬을 통해 임베딩 구성, 즉 3D 및 오디오와 같은 다른 모달리티의 임베딩 공간 산술 기능을 추가로 탐색합니다. 설정. 산술을 위한 다중 모달 입력을 얻기 위해 ShapeNet[8] 및 TextANIMAR2023[7]의 3D 객체와 ESC-50[51]의 오디오 클립을 활용합니다. 우리는 간단히 Point-Bind와 ImageBind에서 각각 3D와 오디오 임베딩을 추가한 다음 1,000개의 이미지 카테고리가 있는 ImageNet[14]에서 2D 이미지를 검색합니다.분석.그림 6에서 3D와 오디오 사이에 합성된 임베딩을 사용하여 2D 이미지 검색의 결과를 보여줍니다.첫 번째 행에 표시된 것처럼 3D 개와 바다 파도 오디오의 결합된 임베딩을 사용하여 바다에 있는 개의 2D 이미지를 효과적으로 검색합니다.마찬가지로 3D 노트북과 키보드 타이핑 오디오의 조합으로 얻은 이미지는 누군가가 노트북으로 작업하거나 고양이가 실수로 키보드를 눌렀음을 보여줍니다.마찬가지로 마지막 행은 3D 곰의 임베딩과 흐르는 물의 오디오를 사용하여 물가에서 사냥하는 곰의 이미지를 검색합니다. 영어: 이 예제는 Point-Bind로 인코딩된 3D 피처가 다른 모달리티에 직접 추가될 수 있고, 그 의미론을 잘 통합하여 유리한 구성된 크로스 모달 검색 용량을 달성할 수 있음을 보여줍니다.5. 모든 것에서 3D로의 생성 설정.기존 텍스트에서 3D로의 생성 방법은 일반적으로 입력 언어 프롬프트를 처리하기 위해 CLIP의 텍스트 인코더를 채택합니다.이를 고려하여 추가 학습 없이 Point-Bind 및 ImageBind의 다중 모달리티 인코더로 간단히 대체하여 3D 모양 합성을 위한 원래 생성 디코더를 따릅니다.기본적으로 CLIP-Forge[65]의 디코더를 채택합니다.분석.그림 7에서 Point-Bind로 구동되는 모든 것에서 3D로의 생성 예를 보여줍니다.텍스트, 오디오 및 포인트 클라우드 프롬프트의 경우, 우리의 접근 방식은 모두 만족스러운 3D 메시를 생성할 수 있습니다.이는 3D 및 다중 모달리티의 잘 정렬된 임베딩 공간을 보여줍니다.5.6. 3D 제로샷 이해 이 섹션에서는 ModelNet40 [80] 데이터 세트에서 3D 제로샷 분류를 통해 Point-Bind의 오픈 워드 이해 능력, 즉 새로운 클래스를 인식하는 능력을 테스트합니다.설정.이전 연구에 따라 CLIP [59] 또는 ImageBind [22]의 텍스트 인코더에서 텍스트 임베딩을 활용하여 제로샷 분류 헤드를 구성합니다.SpecifiTable 3. 투영 네트워크와 3D 인코더의 다양한 디자인을 탐색하는 절제 연구.ModelNet40 [80]에서 제로샷 분류에 대한 결과(%)를 보고합니다.투영 1 선형 Acc.3D 인코더 Acc. 76.PointNext [57] 67.Two Linear 78.Three Linear 76.Point-BERT [86] I2P-MAE [92] 76.78.cally, 우리는 ModelNet40의 40개 카테고리에 대해 &#39;a/an [CLASS]&#39;의 간단한 템플릿을 적용하고 3D와 모든 텍스트 임베딩 간의 코사인 유사도를 계산하여 가장 유사한 것을 최종 예측으로 선택합니다.분석. 우리는 표 2에서 3D 제로샷 분류 정확도를 보고합니다.여기서 우리의 Point-Bind는 최첨단 성능으로 기존 방법을 능가합니다.이는 Point-Bind의 통합 표현 공간이 강력한 새로운 3D 오픈 월드 인식으로 이어진다는 것을 나타냅니다.5.7. Ablation Study PointBind에서 우리 디자인의 효과를 조사하기 위해 표 3의 투영 네트워크와 3D 인코더에 대한 Ablation Study를 수행합니다.우리는 ModelNet40 [80] 데이터 세트에서 제로샷 분류의 성능을 보고합니다. 처음 두 열에서는 3D 인코더 이후의 임베딩을 위한 다양한 투영 방식을 실험합니다. 표시된 대로 임베딩 투영에 두 개의 선형 레이어를 사용하면 가장 좋은 성능을 보입니다. 마지막 두 열에서는 Point-Bind의 다양한 3D 인코더, 즉 Point-BERT[86], PointNext[57], I2P-MAE[92]를 활용합니다. 보고된 대로 자체 감독 Point-BERT와 I2P-MAE는 훨씬 더 나은 성능을 달성하여 멀티모달 정렬을 향상시키기 위한 3D 사전 학습의 중요성을 나타냅니다.
--- CONCLUSION ---
이 논문에서 우리는 ImageBind의 안내에 따라 3D 포인트 클라우드를 멀티모달리티에 정렬하는 3D 멀티모달리티 모델인 Point-Bind를 제안합니다. Point-Bind는 3D 객체를 해당 이미지-오디오-텍스트 쌍에 정렬하여 공동 임베딩 공간을 얻고, 모든 것에서 3D로의 생성, 3D 임베딩 산술, 3D 오픈 월드 이해와 같은 유망한 3D 멀티모달 작업을 보여줍니다. 그 위에 우리는 영어와 중국어 모두에서 명령어 따르기 기능을 갖춘 최초의 3D 대규모 언어 모델(LLM)인 Point-LLM을 추가로 소개합니다. 우리의 향후 작업은 실내 및 실외 장면과 같은 더 다양한 3D 데이터에 멀티모달리티를 정렬하는 데 중점을 둘 것이며, 이를 통해 더 광범위한 적용 시나리오가 가능합니다. 참고문헌 [1] Mohamed Afham, Isuru Dissanayake, Dinithi Dissanayake, Amaya Dharmasiri, Kanchana Thilakarathna, Ranga Rodrigo. CrossPoint: 3D 포인트 클라우드 이해를 위한 자기 지도형 교차 모달 대조 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 9902-9912페이지, 2022. 1,[2] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, Silvio Savarese. 대규모 실내 공간의 3D 의미 구문 분석. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 1534-1543페이지, 2016.[3] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe. ScanQA: 공간 장면 이해를 위한 3D 질문 답변. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 19129-19139페이지, 2022.[4] Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton. 레이어 정규화. arXiv 사전 인쇄본 arXiv:1607.06450, 2016.[5] Adam Botach, Evgenii Zheltonozhskii, Chaim Baskin. 멀티모달 트랜스포머를 사용한 엔드투엔드 참조 비디오 객체 분할. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 4985-4995페이지, 2022.[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei. 언어 모델은 몇 번의 학습입니다. 신경 정보 처리 시스템의 발전, 33권, 1877~1901페이지, 2020. [7] 텍스트ANIMAR 챌린지. 텍스트ANIMAR. https://aichallenge.hcmus.edu.vn/textanimar, 2023.[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. ShapeNet: An Information-rich 3D Model Repository. arXiv 사전 인쇄본 arXiv:1512.03012, 2015. 5,[9] Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut. 개념적 12m: 긴 꼬리 시각적 개념을 인식하기 위한 웹 규모 이미지-텍스트 사전 학습 푸시. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 3558-3568페이지, 2021. 3, 6,[10] Anthony Chen, Kevin Zhang, Renrui Zhang, Zihan Wang, Yuheng Lu, Yandong Guo, Shanghang Zhang. PiMAE: 3D 객체 감지를 위한 포인트 클라우드 및 이미지 대화형 마스크 자동 인코더. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 5291-5301페이지, 2023.[11] Siheng Chen, Baoan Liu, Chen Feng, Carlos VallespiGonzalez, Carl Wellington. 자율 주행을 위한 3D 포인트 클라우드 처리 및 학습: 맵 생성, 현지화 및 인식에 미치는 영향. IEEE Signal Processing Magazine, 38(1):68-86, 2020.[12] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, Tao Chen. vote2cap-detr를 사용한 종단 간 3D 고밀도 캡션. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 11124~11133페이지, 2023.[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, C Lawrence Zitnick. Microsoft COCO 캡션: 데이터 수집 및 평가 서버. arXiv 사전 인쇄본 arXiv:1504.00325, 2015. 3,[14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei. ImageNet: 대규모 계층적 이미지 데이터베이스. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 248~255페이지, 2009.[15] Karan Desai 및 Justin Johnson. VirTex: 텍스트 주석에서 시각적 표현 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 11162-11173페이지, 2021.[16] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, Kaisheng Ma. 교차 모달 교사로서의 자동 인코더: 사전 학습된 2D 이미지 변환기가 3D 표현 학습에 도움이 될 수 있을까? arXiv 사전 인쇄본 arXiv:2212.08320, 2022.[17] Han Fang, Pengfei Xiong, Luhui Xu, Yu Chen. CLIP2Video: 이미지 CLIP을 통한 비디오-텍스트 검색 마스터링. arXiv 사전 인쇄본 arXiv:2106.11097, 2021.[18] Jonathan Frankle, David J Schwab, Ari S Morcos. 훈련 배치표준 및 유일한 배치표준: CNN의 무작위 특성의 표현력에 관한 것입니다. arXiv 사전 인쇄 arXiv:2003.00152, 2020.[19] Peng Gao, Jiaming Han, Chris Liu, Ziyi Lin, Renrui Zhang 및 Ziyu Guo. ImageBind-LLM. https://github. com/OpenGVLab/LLAMA-Adapter/tree/main/ imagebind_LLM, 2023. 3, 6, 7,[20] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. LLAMA-Adapter V2: 매개변수 효율적인 시각적 교육 모델. arXiv 사전 인쇄본 arXiv:2304.15010, 2023. 3,[21] Angeliki Giannou, Shashank Rajput, Dimitris Papailiopoulos. 표준 레이어만 조정하는 표현력. arXiv 사전 인쇄본 arXiv:2302.07937, 2023.[22] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra. ImageBind: 모든 것을 묶는 하나의 임베딩 공간. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 15180-15190페이지, 2023. 2, 4, 5, 8,[23] Ziyu Guo, Xianzhi Li, Pheng Ann Heng. Joint-mae: 3D 포인트 클라우드 사전 학습을 위한 2D-3D 조인트 마스크 자동 인코더. arXiv 사전 인쇄본 arXiv:2302.14007, 2023. 1,[24] Ziyu Guo, Yiwen Tang, Renrui Zhang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li. Viewrefer: gpt 및 프로토타입 가이드를 사용하여 3D 시각적 접지를 위한 다중 뷰 지식 파악. arXiv 사전 인쇄본 arXiv:2303.16894, 2023. 1,[25] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, Bin Cui. Calip: 매개변수 없는 주의를 사용한 클립의 제로 샷 향상. AAAI 인공지능 컨퍼런스 회의록, 제37권, 746-754페이지, 2023년.[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick. Masked Autoencoders Are Scalable Vision Learners. arXiv:2111.06377, 2021년.[27] Deepti Hegde, Jeya Maria Jose Valanarasu, Vishal M Patel. CLIP goes 3D: 언어 기반 3D 인식을 위한 프롬프트 튜닝 활용. arXiv 사전 인쇄본 arXiv:2303.11313, 2023년.[28] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen, Joshua B. Tenenbaum, Chuang Gan. 다중 뷰 이미지에서 3D 개념 학습 및 추론. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 컨퍼런스, 페이지 9202-9212, 2023.[29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang 및 Weizhu Chen. Lora: 대규모 언어 모델의 낮은 순위 적응. arXiv 사전 인쇄 arXiv:2106.09685, 2021.[30] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang 및 Wangmeng Zuo. CLIP2Point: 이미지 깊이 사전 학습을 통해 CLIP을 포인트 클라우드 분류로 전송합니다. arXiv 사전 인쇄 arXiv:2210.01055, 2022.[31] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu 및 Li Fei-Fei. Voxposer: 언어 모델을 사용한 로봇 조작을 위한 구성 가능한 3D 값 맵. arXiv 사전 인쇄본 arXiv:2307.05973, 2023.[32] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole. Dream Fields를 사용한 Zero-Shot 텍스트 가이드 객체 생성. 2022.[33] Longlong Jing, Elahe Vahdani, Jiaxing Tan, Yingli Tian. 3D 크로스 모달 검색을 위한 크로스 모달 중심 손실. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 3142-3151페이지, 2021.[34] Ruihui Li, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng. PU-GAN: 포인트 클라우드 업샘플링 적대적 네트워크. International Conference on Computer Vision, 2019.[35] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin. Magic3d: 고해상도 텍스트-3D 콘텐츠 생성. IEEE Conference on Computer Vision and Pattern Recognition, 300309페이지, 2023.[36] Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, Lorenzo Torresani. VX2TEXT: 다중 모달 입력에서 비디오 기반 텍스트 생성의 종단 간 학습. IEEE Conference on Computer Vision and Pattern Recognition, 7005-7015페이지, 2021.[37] 린 얀보, 청홍유, 이신잉, 린옌유, 양밍수안. 약하게 감독되는 AudioVisual 비디오 구문 분석을 위한 교차 비디오 및 교차 양식 신호 탐색. 신경 정보 처리 시스템의 발전, 34, 2021.[38] Haotian Liu, Chunyuan Li, Qingyang Wu, 그리고 이용재. 시각적 지시 조정, 2023.[39] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli 및 Hao Su. OpenShape: 오픈 월드 이해를 위한 3D 모양 표현 확장. arXiv 사전 인쇄본 arXiv:2305.10764, 2023. 1,[40] Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu Tseng, Winston H Hsu. 2D에서 학습: 3D 사전 학습을 위한 대조적 픽셀-포인트 지식 전달. arXiv 사전 인쇄본 arXiv:2104.04687, 2021.[41] Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, ChiWing Fu. ISS++: 텍스트 가이드 3D 모양 생성을 위한 발판으로서의 이미지. arXiv 사전 인쇄본 arXiv:2303.15181, 2023.[42] Zhitao Liu, Zengyu Liu, Jiwei Wei, Guan Wang, Zhenjiang Du, Ning Xie, Heng Tao Shen. 3D 교차 모달 검색을 위한 가우시안 RBF 커널을 사용한 인스턴스-변형 손실. arXiv 사전 인쇄본 arXiv:2305.04239, 2023.[43] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, Xin Tong. 변압기를 통한 그룹 없는 3D 객체 감지. 국제 컴퓨터 비전 컨퍼런스, 29492958페이지, 2021.[44] Ilya Loshchilov 및 Frank Hutter. 분리된 가중치 감소 정규화. arXiv 사전 인쇄본 arXiv:1711.05101, 2017.[45] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, Rana Hanocka. Text2mesh: 메시를 위한 텍스트 기반 신경 스타일화. arXiv 사전 인쇄본 arXiv:2112.03221, 2021.[46] Ishan Misra, Rohit Girdhar, Armand Joulin. 3D 객체 감지를 위한 엔드투엔드 트랜스포머 모델. International Conference on Computer Vision, 29062917페이지, 2021.[47] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, Tiberiu Popa. CLIP-Mesh: 사전 학습된 이미지-텍스트 모델을 사용하여 텍스트에서 텍스처 메시 생성. ACM SIGGRAPH Asia Conference, 1-8페이지, 2022.[48] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, Cordelia Schmid. 이미지 캡션에서 오디오-비디오 모달리티 학습. European Conference on Computer Vision, 407-426페이지, 2022.[49] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Mark Chen. Point-e: 복합 프롬프트에서 3D 포인트 클라우드를 생성하는 시스템. arXiv 사전 인쇄본 arXiv:2212.08751, 2022.[50] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, Li Yuan. 포인트 클라우드 자기 감독 학습을 위한 마스크 자동 인코더. European Conference on Computer Vision, 604-621페이지, 2022.[51] Karol J Piczak. ESC: 환경 소음 분류를 위한 데이터 세트. ACM International Conference on Multimedia, 1015-1018페이지, 2015. 5, 6,[52] Ben Poole, Ajay Jain, Jonathan T Barron, Ben Mildenhall. Dreamfusion: 2D 확산을 사용한 텍스트-3D. arXiv 사전 인쇄본 arXiv:2209.14988, 2022.[53] Omid Poursaeed, Tianxing Jiang, Quintessa Qiao, Nayun Xu, Vladimir G. Kim. 방향 추정을 통한 포인트 클라우드의 자체 감독 학습. arXiv 사전 인쇄본 arXiv:2008.00305, 2020.[54] Charles R Qi, Hao Su, Kaichun Mo, Leonidas J Guibas. PointNet: 3D 분류 및 분할을 위한 포인트 세트에 대한 딥 러닝. arXiv 사전 인쇄본 arXiv:1612.00593, 2016.[55] Charles R Qi, Li Yi, Hao Su, Leonidas J Guibas. PointNet++: 메트릭 공간의 점 집합에 대한 심층 계층적 특징 학습. arXiv 사전 인쇄본 arXiv:1706.02413, 2017.[56] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, Li Yi. 재구성과 대조: 생성적 사전 학습을 통해 안내되는 대조적 3D 표현 학습. 기계 학습 국제 컨퍼런스, 2023. 1,[57] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, Bernard Ghanem. PointNext: 개선된 학습 및 확장 전략을 사용한 PointNet++ 재검토. 신경 정보 처리 시스템의 발전, 2022. 4,[58] Guocheng Qian, Xingdi Zhang, Abdullah Hamdi, 및 Bernard Ghanem. Pix4Point: 3D 포인트 클라우드 이해를 위한 이미지 사전 학습된 변압기. arXiv, 2022.[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. 기계 학습 국제 컨퍼런스, 8748-8763페이지, 2021. 4, 5,[60] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, 및 Mark Chen. 클립 잠재 데이터를 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125, 1(2):3, 2022.[61] Karthik Ramesh, Chao Xing, Wupeng Wang, Dong Wang, Xiao Chen. Vset: 시각적 음성 향상을 위한 다중 모드 변환기. 음향, 음성 및 신호 처리 국제 컨퍼런스, 66586662페이지, 2021.[62] Yongming Rao, Jiwen Lu, Jie Zhou. 3D 포인트 클라우드의 비지도 표현 학습을 위한 글로벌-로컬 양방향 추론. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 2020.[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 10684-10695페이지, 2022.[64] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전, 35:36479-36494, 2022.[65] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, and Marco Fumero. Clip-Forge: 제로샷 텍스트-모양 생성을 향해. arXiv 사전 인쇄본 arXiv:2110.02624, 2021. 4,6,[66] Jonathan Sauder 및 Bjarne Sievers. 공간 재구성을 통한 포인트 클라우드의 자체 감독 딥 러닝. 신경 정보 처리 시스템의 발전, 32, 2019.[67] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: 구체화된 AI 연구를 위한 플랫폼. International Conference on Computer Vision, 9339–9347페이지, 2019.[68] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: 차세대 이미지-텍스트 모델을 학습하기 위한 대규모 오픈 데이터 세트. 신경 정보 처리 시스템의 발전, 35:25278-25294, 2022. 3,6,[69] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, Yu Liu. 해석 가능한 센서 융합 변압기를 사용한 안전성이 강화된 자율 주행. 로봇 학습 컨퍼런스, 726-737페이지. PMLR, 2023.[70] Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut. 개념적 캡션: 자동 이미지 캡션을 위한 정리되고 하이퍼니밍된 이미지 대체 텍스트 데이터 세트. 2018년 계산언어학 협회 연례 회의, 2556-2565페이지. 3, 6,[71] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, Hongsheng Li. Pv-rcnn: 3D 객체 감지를 위한 Pointvoxel 기능 세트 추상화. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10529-10538페이지, 2020.[72] Desney S Tan, George G Robertson, Mary Czerwinski. 3D 탐색: 속도 결합 비행과 궤도 이동 결합. SIGCHI 컴퓨팅 시스템의 인적 요인 컨퍼런스 회의록, 418-425페이지, 2001.[73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: 개방적이고 효율적인 기반 언어 모델. arXiv 사전 인쇄본 arXiv:2302.13971, 2023. 2, 6, 7,[74] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, Chang D Yoo. 포인트 클라우드의 3D 인스턴스 분할을 위한 Softgroup. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 2708-2717페이지, 2022.[75] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, Matthew J. Kusner. Occlusion Completion을 통한 비지도 포인트 클라우드 사전 학습. International Conference on Computer Vision, 9782-9792페이지, 2021.[76] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang. 시각 언어 탐색을 위한 강화된 교차 모달 매칭 및 자체 지도 모방 학습. IEEE Conference on Computer Vision and Pattern Recognition, 6629-6638페이지, 2019.[77] Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, Jiwen Lu. P2P: 포인트-픽셀 프롬프팅을 사용한 포인트 클라우드 분석을 위한 사전 학습된 이미지 모델 조정. 신경 정보 처리 시스템의 발전, 35:14388–14402, 2022.[78] Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, Dhruv Batra. 포인트 클라우드 인식을 통한 사실적인 환경에서의 체화된 질문 답변. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 6659–6668페이지, 2019.[79] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, Jian Zhang. Eda: 3D 시각적 접지를 위한 명시적 텍스트 분리 및 고밀도 정렬. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 1923119242페이지, 2023.[80] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, Jianxiong Xiao. 3d shapenets: 체적 모양을 위한 심층 표현. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 1912-1920페이지, 2015. 8, 9,[81] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Difffit: Zhaoqiang Liu, Jiawei Li, Zhenguo Li. 간단한 매개변수 효율적 미세 조정을 통해 대규모 확산 모델의 이전성 잠금 해제. arXiv 사전 인쇄본 arXiv:2304.06648, 2023.[82] Chenfeng Xu, Shijia Yang, Tomer Galanti, Bichen Wu, Xiangyu Yue, Bohan Zhai, Wei Zhan, Peter Vajda, Kurt Keutzer 및 Masayoshi Tomizuka. Image2Point: 2D 이미지 사전 학습 모델을 사용한 3D 포인트 클라우드 이해. 컴퓨터 비전에 관한 유럽 회의, 페이지 638-656, 2022.[83] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie 및 Shenghua Gao. Dream3D: 3D 형상 사전 및 텍스트-이미지 확산 모델을 사용한 제로샷 텍스트-3D 합성. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 컨퍼런스, 페이지 20908-20918, 2023.[84] Le Xue, Mingfei Gao, Chen Xing, Roberto Martín-Martín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese. ULIP: 3D 이해를 위한 언어, 이미지 및 포인트 클라우드의 통합 표현 학습. arXiv 사전 인쇄본 arXiv:2212.05171, 2022. 1, 4, 5, 8,[85] Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto MartínMartín, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese. ULIP-2: 3D 이해를 위한 확장 가능한 멀티모달 사전 학습을 향해. arXiv 사전 인쇄본 arXiv:2305.08275, 2023. 4,[86] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, Jiwen Lu. Point-BERT: 마스크된 포인트 모델링을 사용한 3D 포인트 클라우드 변환기 사전 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 2022. 4,[87] Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin Li, Shuguang Cui, Zhen Li. X-trans2cap: 3D 고밀도 캡션을 위한 변환기를 사용한 크로스 모달 지식 전송. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 8563-8573페이지, 2022.[88] Elad Ben Zaken, Shauli Ravfogel, Yoav Goldberg. Bitfit: 변압기 기반 마스크 언어 모델을 위한 간단한 매개변수 효율적 미세 조정. 사전 인쇄본 arXiv:2106.10199, 2021.arXiv [89] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao 및 Hongsheng Li. PointM2AE: 계층적 포인트 클라우드 사전 학습을 위한 다중 스케일 마스크 자동 인코더. arXiv 사전 인쇄본 arXiv:2205.14401, 2022.[90] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao 및 Hongsheng Li. PointCLIP: CLIP을 통한 포인트 클라우드 이해. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 8552-8562페이지, 2022. 1, 4, 8,[91] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao 및 Yu Qiao. LLAMA-Adapter: Zero-init Attention을 통한 언어 모델의 효율적인 미세 조정. arXiv 사전 인쇄본 arXiv:2303.16199, 2023. 3, 7,[92] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao 및 Hongsheng Li. 이미지-포인트 마스크 자동 인코더를 통해 2D 사전 학습된 모델에서 3D 표현 학습. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스, 21769-21780페이지, 2023. 1, 2, 4, 6, 8,[93] Renrui Zhang, Liuhui Wang, Yali Wang, Peng Gao, Hongsheng Li, Jianbo Shi. 매개변수가 필요한 전부는 아닙니다. 3D 포인트 클라우드 분석을 위한 비모수적 네트워크부터 시작. CVPR 2023, 2023.[94] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, Hongsheng Li. Tipadapter: few-shot 분류를 위한 클립의 학습 없는 적응. 유럽 컴퓨터 비전 컨퍼런스, 493-510페이지. Springer, 2022.[95] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, Xiangyu Yue. Metatransformer: 다중 모드 학습을 위한 통합 프레임워크. arXiv 사전 인쇄본 arXiv:2307.10802, 2023.[96] Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, Curtis P Langlotz. 쌍을 이룬 이미지와 텍스트에서 의료 시각적 표현의 대조 학습. Machine Learning for Healthcare Conference, 2-25페이지, 2022. 5,[97] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny. Minigpt-4: 고급 대규모 언어 모델을 통한 시각 언어 이해 향상. arXiv 사전 인쇄본 arXiv:2304.10592, 2023.[98] Fengda Zhu, Yi Zhu, Xiaojun Chang 및 Xiaodan Liang. 자기 감독 보조 추론 작업을 통한 비전 언어 탐색. 컴퓨터 비전 및 패턴 인식에 관한 IEEE 컨퍼런스, 페이지 10012-10022, 2020.[99] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Jiaming Liu, Hao Dong 및 Peng Gao. 더 적을수록 좋습니다: 훈련 없는 네트워크를 통해 효율적인 퓨샷 3D 의미론적 분할을 지향합니다. arXiv 사전 인쇄 arXiv:2308.12961, 2023.[100] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang 및 Peng Gao. PointCLIP V2: 강력한 3D 오픈 월드 학습을 위해 CLIP 및 GPT를 유도합니다. arXiv 사전 인쇄 arXiv:2211.11682, 2022. 1, 4, 8,
