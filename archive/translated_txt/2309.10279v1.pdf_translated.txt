--- ABSTRACT ---
단일 이미지에서 전체 360° 뷰 3D 모델을 만드는 새로운 프레임워크인 POP3D를 소개합니다. POP3D는 단일 뷰 재구성을 제한하는 두 가지 중요한 문제를 해결합니다. 첫째, POP3D는 임의의 범주에 대한 상당한 일반화 가능성을 제공하는데, 이는 이전 방법에서는 달성하기 어려운 특성입니다. 둘째, POP3D는 동시 작업에서 부족한 중요한 측면인 재구성 충실도와 자연스러움을 더욱 개선합니다. 저희의 접근 방식은 네 가지 주요 구성 요소의 장점을 결합합니다. (1) 중요한 기하학적 단서를 예측하는 데 사용되는 단안 깊이 및 일반 예측기, (2) 대상 객체의 잠재적으로 보이지 않는 부분을 구분할 수 있는 공간 조각 방법, (3) 보이지 않는 부분을 완료할 수 있는 대규모 이미지 데이터 세트에서 사전 학습된 생성 모델 이 작업의 일부 또는 전부를 개인 또는 교실에서 사용하기 위해 디지털 또는 인쇄본으로 만드는 허가는 사본이 이익 또는 상업적 이익을 위해 만들어지거나 배포되지 않고 사본에 이 고지 사항과 첫 페이지에 있는 전체 인용문이 있는 경우 무료로 부여됩니다. 저자가 아닌 다른 사람이 소유한 이 작품의 구성 요소에 대한 저작권은 존중해야 합니다. 출처를 명시한 초록은 허용됩니다. 그렇지 않은 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org로 허가를 요청하세요. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW주 시드니 © 2023 저작권은 소유자/저자에게 있습니다. 출판권은 ACM에 라이선스되었습니다. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.대상 영역, (4) 단안 기하학적 단서와 함께 RGB 이미지를 사용하여 객체를 재구성하는 데 맞춤화된 신경 암묵적 표면 재구성 방법. 이러한 구성 요소를 결합하면 POP3D는 다양한 야생 이미지에서 쉽게 일반화하고 최첨단 재구성을 생성하여 유사한 작업보다 상당한 마진으로 성능이 향상됩니다. 프로젝트 페이지: http://cg.postech.ac.kr/research/POP3D. CCS 개념 • 컴퓨팅 방법론 → 재구성; 컴퓨터 그래픽; 인공 지능. 키워드 단일 뷰 3D 재구성, 모양 및 외관 재구성, 새로운 뷰 합성, 공간 조각, 아웃페인팅 ACM 참조 형식: 류 누리, 공민수, 김건웅, 이주행, 조성현. 2023. 공간 조각 아웃페인팅을 사용한 단일 이미지에서 360° 재구성. SIGGRAPH Asia 2023 Conference Papers(SA Conference Papers &#39;23), 2023년 12월 12-15일, 호주 NSW 시드니. ACM, New York, NY, USA, 11페이지. https://doi.org/10.1145/3610548.SA Conference Papers &#39;23, 2023년 12월 12-15일, Sydney, NSW, Australia
--- INTRODUCTION ---
최소한의 입력으로 고품질의 사실적인 3D 모델을 생성하는 능력은 컴퓨터 그래픽, 비전, 가상 현실 및 증강 현실의 다양한 응용 분야에서 지속적인 과제입니다.신경 표현의 미분 가능한 렌더링을 통한 다중 뷰 재구성 분야의 최근 진전에도 불구하고 [Mildenhall et al. 2020; Niemeyer et al. 2020; Sitzmann et al. 2019], 이러한 방법은 카메라 매개변수와 페어링된 방대한 양의 이미지에 크게 의존합니다.이러한 의존성은 인상적인 결과를 가져올 수 있지만 실용성과 접근성을 저해하며, 특히 객체의 여러 뷰를 얻는 것이 비실용적이거나 불가능한 시나리오에서 그렇습니다.실제 시나리오에서 사용자는 객체에 대한 단일 뷰만 가질 수 있습니다.예를 들어, 이미지는 쉽게 접근할 수 없는 객체의 사진이거나 2D 생성 모델의 출력일 수 있습니다.결과적으로 단일 이미지에서 3D 모델을 생성하는 것은 엄청난 실질적 의미를 가지며, 더 광범위한 응용 프로그램을 가능하게 하고 더 광범위한 사용자 기반이 3D 모델링에 더 쉽게 접근할 수 있게 합니다. 단일 이미지로부터의 3D 재구성은 실용적인 중요성 때문에 활발한 연구 분야였습니다. 그러나 기존 방법은 여전히 일반화 가능성과 재구성 충실도라는 두 가지 주요 문제에 시달리고 있습니다. 단일 뷰 재구성을 위해 3D 데이터 또는 객체 중심 비디오에서 학습하는 다양한 방법이 제안되었습니다[Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Nichol et al. 2022; Saito et al. 2019; Wang et al. 2018; Wu et al. 2023a; Xie et al. 2019]. 그러나 이러한 데이터의 수집은 구조화되지 않은 2D 데이터를 수집하는 것에 비해 종종 더 어렵기 때문에 이러한 방법의 확장성과 일반화 가능성이 훼손됩니다. 2D 이미지 데이터에 의존하여 3D 데이터의 필요성을 우회하는 다른 기술도 제안되었지만 이러한 방법은 종종 특정 범주에 국한됩니다[Henzler et al. 2019; Jang 및 Agapito 2021; Pavllo 등 2023; Wu 등 2023b; Ye 등 2021], 따라서 일반화 가능성이 제한됩니다. 증류 손실[Poole 등 2023]을 통해 대규모 이미지 사전[Rombach 등 2022]을 활용하는 동시 방법[Deng 등 2023; Melas-Kyriazi 등 2023; Xu 등 2023]은 종종 입력 뷰를 충실하게 재구성하지 못합니다. 이러한 불일치는 증류 손실이 입력 뷰의 RGB 재구성 손실을 방해하고 재구성의 제한된 대상 해상도가 이 문제를 더욱 악화시키기 때문에 발생합니다. 더욱이 순진한 신경 밀도 필드를 사용하면 종종 충실도가 낮은 표면 재구성으로 이어집니다. 이 논문에서는 앞서 언급한 일반화 가능성 및 재구성 충실도 문제를 해결하도록 설계된 새로운 프레임워크인 POP3D(Progressive OutPainting 3D)를 제시합니다. 일반화의 과제를 해결하기 위해, 저희 프레임워크는 대규모 데이터 세트에서 사전 학습된 다양한 사전 확률의 힘을 활용합니다. 이 접근 방식은 임의의 범주에서 단일 RGB 이미지로부터 3D 재구성의 본질적인 부적절함을 효과적으로 완화합니다. 객체의 전체 360° 뷰를 포괄하는 고충실도 재구성을 위해, 저희는 대규모 생성 모델을 통해 주어진 뷰의 품질과 일치하는 새로운 뷰를 생성합니다. 이러한 새로운 뷰는 단안 기하학 예측과 함께 의사-지상 진실 데이터 세트를 형성합니다. 단안 기하학 신호를 통합하도록 맞춤화된 학습 전략에 따라 이 데이터 세트에서 학습함으로써, 저희는 신경 암묵적 표면과 Ryu et al.의 해당 모양을 재구성합니다. 그림 1에서 설명한 것과 같이 동시 작업과 비교하여 높은 충실도를 가진 단일 이미지가 제공됩니다.자세히 설명하자면, 프레임워크는 최첨단 단안 깊이 및 일반 예측자[Bhat et al. 2023; Eftekhar et al. 2021]를 사용하여 단일 RGB 입력을 처리하여 기하학적 신호를 추론하는 것으로 시작합니다.입력 RGB와 단안 기하학적 예측은 초기 데이터 세트를 구성하며 MonoSDF[Yu et al. 2022]의 학습 전략에 따라 초기 3D 모델을 학습하는 데 사용됩니다.3D 모델을 초기화한 후 대상 객체의 전체 360° 보기를 포함하는 카메라 일정에 따라 카메라 위치를 업데이트합니다.그런 다음 프레임워크는 객체의 시각적 껍질[Laurentini 1994]을 찾아 대상 객체의 보이는 영역과 잠재적으로 보이지 않는 영역을 계산합니다.보이는 영역을 제거하여 생성 모델이 객체의 자연스럽고 새로운 보기를 생성하는 가이드로 사용되는 아웃페인팅 마스크를 얻습니다. 구체적으로, 우리는 마스크와 텍스트 조건이 주어진 이미지를 아웃페인팅할 수 있는 대규모 데이터 세트[Schuhmann et al. 2022]에서 학습된 조건부 확산 모델[Rombach et al. 2022]을 사용합니다. 아웃페인팅된 결과의 단안 기하학 정보를 추출하는 과정을 거친 후, 처리된 데이터로 의사 지상 진실 데이터 세트를 확장합니다. 이 업데이트된 데이터 세트를 사용하여 3D 모델을 다시 학습하고, 객체의 전체 360° 보기를 포함하는 데이터 세트를 생성할 때까지 이 점진적인 아웃페인팅 과정을 반복하여 궁극적으로 고충실도 360° 3D 재구성을 이룹니다. 우리 프레임워크는 몇 가지 독특한 이점을 제공합니다. 첫째, 대규모 데이터 세트에서 학습한 사전 지식 덕분에 우리 프레임워크는 특정 범주의 객체에 국한되지 않고 임의의 범주의 광범위한 객체를 처리할 수 있습니다. 둘째, 우리 프레임워크는 기성 모델에서 이미 학습한 사전 지식을 채택하므로 다중 뷰 이미지나 3D 기하학과 같은 추가 외부 학습 데이터가 필요하지 않습니다. 셋째, 대상 객체의 360도 뷰 데이터 세트를 구축하는 점진적 아웃페인팅 방식은 고품질의 새로운 뷰 이미지 생성과 입력 이미지의 충실한 재구성을 보장합니다. 마지막으로, 가상 지상 진실 데이터 세트를 사용하여 신경 암묵적 표면 표현을 학습함으로써 잘 정의된 고품질 표면을 추출할 수 있습니다.2. 요약하자면, 우리의 주요 기여는 다음과 같습니다. • 단일 이미지에서 전체 360도 모델을 재구성하는 새로운 프레임워크를 소개합니다. 우리의 프레임워크는 기성품 사전 학습을 활용하여 범주별 사전 학습 없이도 실제 RGB 이미지로 일반화됩니다. • 3D 모델 재구성을 위한 가상 지상 진실 이미지를 생성하는 점진적 아웃페인팅 방식을 개발합니다. 우리의 방법은 입력 이미지와 자연스럽게 조화를 이루는 새로운 뷰 이미지로 충실한 재구성을 보장합니다. 우리의 모델 설계는 기하학적 일관성과 광도 일관성을 모두 고려하여 충실도 높은 모양과 외관 재구성을 실현합니다. • 우리는 우리 프레임워크가 새로운 관점 합성 및 기하 재구성의 관점에서 단일 RGB 이미지로부터 최첨단 360° 재구성 결과를 생성할 수 있음을 보여줍니다.
--- RELATED WORK ---
S Few-View-to-3D Reconstruction NERF[Mildenhall et al. 2020]와 그 변형[Barron et al. 2021, 2022; Verbin et al. 2022; Zhang et al. 2020]은 카메라 포즈와 페어링된 RGB 이미지만 주어진 장면과 객체의 Space Carved Outpainting 재구성 성능을 사용하여 단일 이미지에서 놀라운 360° 재구성을 보여주었습니다. 그러나 밀도가 높은 카메라 뷰가 없으면 신경 광도장을 학습하는 것이 심각하게 제약이 부족한 문제가 됩니다. 뷰가 몇 개만 주어지면 이러한 모델은 주어진 각 뷰에 과적합되어 새로운 뷰를 렌더링할 때 깨진 지오메트리와 흐릿한 노이즈가 발생할 수 있습니다[Jain et al. 2021]. 최근에는 고충실도 재구성에 필요한 뷰 수를 줄이기 위한 작업 라인이 도입되었습니다[Jain et al. 2021; Roessle et al. 2022; Sajjadi et al. 2022; Verbin et al. 2022; Yu et al. 2021; Zhou and Tulsiani 2023]. 그럼에도 불구하고 적절한 재구성을 위해서는 여전히 단일 뷰 이상이 필요합니다. 2.2 단일 뷰에서 3D로의 재구성 단일 이미지에서 3D 모델을 재구성하는 초기 작업의 대부분은 음영 [Zhang et al. 1999], 텍스처 [Loh 2006] 또는 초점 흐림 [Favaro and Soatto 2005]과 같은 이미지에 제공된 가시적 정보에 의존합니다. 최근 작업에서는 입력 이미지의 보이지 않는 부분을 생성하기 위해 보다 일반적인 사전을 사용합니다. 예를 들어, 일부
--- METHOD ---
. (b) 및 (c)의 결과는 증류 손실 및 신경 밀도 필드의 순진한 사용으로 인해 최적이 아닌 새로운 뷰와 저충실도 표면이 생성됨을 보여줍니다 [Melas-Kyriazi et al. 2023; Xu et al. 2023]. 반면에, 저희 프레임워크는 원래 입력 이미지와 유사한 새로운 뷰를 성공적으로 생성하고 (d) 및 (e)에서 볼 수 있듯이 높은 충실도로 3D 객체의 표면을 재구성합니다. (a)의 이미지: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [Horton, CC BY]. 초록 단일 이미지에서 전체 360° 뷰 3D 모델을 만드는 새로운 프레임워크인 POP3D를 소개합니다. POP3D는 단일 뷰 재구성을 제한하는 두 가지 중요한 문제를 해결합니다. 첫째, POP3D는 임의의 범주에 대한 상당한 일반화 가능성을 제공하는데, 이는 이전 방법에서는 달성하기 어려운 특성입니다. 둘째, POP3D는 동시 작업에서 부족한 중요한 측면인 재구성 충실도와 자연스러움을 더욱 개선합니다. 저희의 접근 방식은 네 가지 주요 구성 요소의 강점을 결합합니다. (1) 중요한 기하학적 단서를 예측하는 데 사용되는 단안 깊이 및 정상 예측기, (2) 대상 객체의 잠재적으로 보이지 않는 부분을 구분할 수 있는 공간 조각 방법, (3) 보이지 않는 를 완성할 수 있는 대규모 이미지 데이터 세트에서 사전 학습된 생성 모델 이 작업의 일부 또는 전부를 개인 또는 교실에서 사용하기 위해 디지털 또는 하드 카피로 만드는 허가는 이윤 또는 상업적 이점을 위해 사본을 만들거나 배포하지 않고 사본에 이 고지 사항과 첫 페이지에 전체 인용문을 표시하는 경우 무료로 부여됩니다. 저자가 아닌 다른 사람이 소유한 이 작업의 구성 요소에 대한 저작권은 존중해야 합니다. 출처를 명시한 초록은 허용됩니다. 다른 방식으로 복사하거나, 재출판하거나, 서버에 게시하거나, 목록에 재배포하려면 사전에 구체적인 허가 및/또는 수수료가 필요합니다. permissions@acm.org에서 허가를 요청하세요. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 © 2023 저작권은 소유자/저자가 소유합니다. 출판권은 ACM에 라이선스되었습니다. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.대상 영역, 및 (4) 단안 기하학적 단서와 함께 RGB 이미지를 사용하여 객체를 재구성하는 데 맞춤화된 신경 암묵적 표면 재구성 방법. 이러한 구성 요소를 결합하면 POP3D는 다양한 야생 이미지에서 쉽게 일반화하고 최첨단 재구성을 생성하여 유사한 작업보다 상당한 마진으로 성능이 향상됩니다. 프로젝트 페이지: http://cg.postech.ac.kr/research/POP3D. CCS 개념 • 컴퓨팅 방법론 → 재구성; 컴퓨터 그래픽; 인공 지능. 키워드 단일 뷰 3D 재구성, 모양 및 외관 재구성, 새로운 뷰 합성, 공간 조각, 아웃페인팅 ACM 참조 형식: 류 누리, 공민수, 김건웅, 이주행, 조성현. 2023. 공간 조각 아웃페인팅을 사용한 단일 이미지에서 360° 재구성. SIGGRAPH Asia 2023 Conference Papers(SA Conference Papers &#39;23), 2023년 12월 12-15일, 호주 NSW 시드니. ACM, 뉴욕, 뉴욕, 미국, 11페이지. 한국어: https://doi.org/10.1145/3610548.SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 서론 최소한의 입력으로 고품질의 사실적인 3D 모델을 생성하는 능력은 컴퓨터 그래픽, 비전, 가상 현실 및 증강 현실의 다양한 응용 분야에서 지속적인 과제입니다. 신경 표현의 미분 가능한 렌더링을 통한 다중 뷰 재구성 분야의 최근 진전에도 불구하고 [Mildenhall et al. 2020; Niemeyer et al. 2020; Sitzmann et al. 2019], 이러한 방법은 카메라 매개변수와 페어링된 방대한 양의 이미지에 크게 의존합니다. 이러한 의존성은 인상적인 결과를 가져올 수 있지만, 특히 객체의 여러 뷰를 얻는 것이 비실용적이거나 불가능한 시나리오에서 실용성과 접근성을 저해합니다. 실제 시나리오에서 사용자는 객체의 단일 뷰만 가질 수 있습니다. 예를 들어, 이미지는 쉽게 접근할 수 없는 객체의 사진이거나 2D 생성 모델의 출력일 수 있습니다. 결과적으로 단일 이미지에서 3D 모델을 생성하는 것은 엄청난 실용적 의미를 가지며, 더 광범위한 응용 프로그램을 가능하게 하고 3D 모델링을 더 광범위한 사용자 기반이 더 쉽게 접근할 수 있게 합니다. 그 실용적 중요성으로 인해 단일 이미지에서 3D 재구성은 활발한 연구 분야였습니다. 그러나 기존 방법은 여전히 일반화 가능성과 재구성 충실도라는 두 가지 주요 문제로 어려움을 겪고 있습니다. 단일 뷰 재구성을 위해 3D 데이터 또는 객체 중심 비디오에서 학습하는 다양한 방법이 제안되었습니다[Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Nichol et al. 2022; Saito et al. 2019; Wang et al. 2018; Wu et al. 2023a; Xie et al. 2019]. 그러나 이러한 데이터의 수집은 구조화되지 않은 2D 데이터를 수집하는 것에 비해 종종 더 어려워서 이러한 방법의 확장성과 일반화가 훼손됩니다. 2D 이미지 데이터에 의존하여 3D 데이터의 필요성을 우회하는 다른 기술도 제안되었지만 이러한 방법은 종종 특정 범주에 묶여 있기 때문에 [Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021] 일반화가 제한됩니다. 증류 손실 [Poole et al. 2023]을 통해 대규모 이미지 사전 [Rombach et al. 2022]을 활용하는 동시 방법 [Deng et al. 2023; Melas-Kyriazi et al. 2023; Xu et al. 2023]은 입력 뷰를 충실하게 재구성하지 못하는 경우가 많습니다. 이러한 불일치는 증류 손실이 입력 뷰의 RGB 재구성 손실을 방해하고 재구성의 제한된 대상 해상도가 이 문제를 더욱 악화시키기 때문에 발생합니다. 나아가, 순진한 신경 밀도 필드를 사용하면 종종 충실도가 낮은 표면 재구성으로 이어집니다. 이 논문에서는 앞서 언급한 일반화 가능성 및 재구성 충실도 문제를 해결하도록 설계된 새로운 프레임워크인 POP3D(Progressive OutPainting 3D)를 제시합니다. 일반화 가능성의 과제를 해결하기 위해, 저희 프레임워크는 대규모 데이터 세트에서 사전 학습된 다양한 사전 확률의 힘을 활용합니다. 이 접근 방식은 임의의 범주에서 단일 RGB 이미지에서 3D 재구성의 고유한 부적절함을 효과적으로 완화합니다. 객체의 전체 360° 뷰를 포괄하는 충실도 높은 재구성의 경우, 저희는 대규모 생성 모델을 통해 주어진 뷰의 품질과 일치하는 새로운 뷰를 생성합니다. 이러한 새로운 뷰는 단안 기하학 예측과 함께 의사-지상-진실 데이터 세트를 형성합니다. 단안 기하학적 단서를 통합하도록 맞춤화된 훈련 전략에 따라 이 데이터 세트에서 훈련함으로써, 그림 1에서 설명한 바와 같이 동시 작업과 비교하여 높은 충실도로 Ryu et al.이 제공한 단일 이미지의 신경 암묵적 표면과 해당 모양을 재구성합니다.자세히 설명하자면, 프레임워크는 최첨단 단안 깊이 및 일반 예측자[Bhat et al. 2023; Eftekhar et al. 2021]를 사용하여 단일 RGB 입력을 처리하여 기하학적 단서를 추론하는 것으로 시작합니다.입력 RGB와 단안 기하학적 예측은 초기 데이터 세트를 구성하며 MonoSDF[Yu et al. 2022]의 훈련 전략에 따라 초기 3D 모델을 훈련하는 데 사용됩니다.3D 모델을 초기화한 후 대상 객체의 전체 360° 보기를 포함하는 카메라 일정에 따라 카메라 위치를 업데이트합니다.그런 다음 프레임워크는 객체의 시각적 껍질[Laurentini 1994]을 찾아 대상 객체의 보이는 영역과 잠재적으로 보이지 않는 영역을 계산합니다. 보이는 영역을 제거함으로써, 우리는 객체의 자연스러운 새로운 뷰를 생성하는 생성 모델의 가이드로 사용되는 아웃페인팅 마스크를 얻습니다. 구체적으로, 우리는 마스크와 텍스트 조건이 주어진 이미지를 아웃페인팅할 수 있는 대규모 데이터 세트[Schuhmann et al. 2022]에서 훈련된 조건부 확산 모델[Rombach et al. 2022]을 사용합니다. 아웃페인팅된 결과의 단안 기하학 정보를 추출하는 프로세스 후, 우리는 처리된 데이터로 우리의 의사-지상-진실 데이터 세트를 확장합니다. 그런 다음 이 업데이트된 데이터 세트를 사용하여 3D 모델을 다시 훈련하고, 우리는 궁극적으로 충실도 높은 360° 3D 재구성으로 이어지는 객체의 전체 360° 뷰를 포함하는 데이터 세트를 생성할 때까지 이 점진적인 아웃페인팅 프로세스를 반복합니다. 우리의 프레임워크는 몇 가지 독특한 이점을 제공합니다. 첫째, 대규모 데이터 세트에서 학습한 사전 지식 덕분에 우리의 프레임워크는 특정 범주의 객체에 국한되지 않고 임의의 범주의 광범위한 객체를 처리할 수 있습니다. 둘째, 우리의 프레임워크는 기성품 모델에서 이미 학습한 사전 확률을 채택하므로 다중 뷰 이미지나 3D 기하 구조와 같은 추가적인 외부 학습 데이터가 필요하지 않습니다.셋째, 대상 객체의 360도 뷰 데이터 세트를 구축하는 우리의 점진적 아웃페인팅 방식은 고품질의 새로운 뷰 이미지 생성과 입력 이미지의 충실한 재구성을 보장합니다.마지막으로, 가상 지상 진실 데이터 세트를 사용하여 신경 암묵적 표면 표현을 학습함으로써 잘 정의된 고품질 표면을 추출할 수 있습니다.2.요약하자면, 우리의 주요 기여는 다음과 같습니다.• 단일 이미지에서 전체 360도 모델을 재구성하는 새로운 프레임워크를 소개합니다.우리의 프레임워크는 기성품 사전 확률을 활용하여 범주별 사전 학습 없이도 실제 RGB 이미지로 일반화됩니다.• 3D 모델 재구성을 위한 가상 지상 진실 이미지를 생성하는 점진적 아웃페인팅 방식을 개발합니다.우리의 방법은 입력 이미지와 자연스럽게 조화를 이루는 새로운 뷰 이미지로 충실한 재구성을 보장합니다. 우리의 모델 설계는 기하학적 일관성과 광도적 일관성을 모두 고려하여 고충실도의 모양과 외관 재구성을 이룹니다.• 우리는 우리의 프레임워크가 새로운 뷰 합성 및 기하 재구성 측면에서 단일 RGB 이미지로부터 최첨단 360° 재구성 결과를 생성할 수 있음을 보여줍니다.관련 연구 Few-View-to-3D Reconstruction NERF [Mildenhall et al. 2020]와 그 변형 [Barron et al. 2021, 2022; Verbin et al. 2022; Zhang et al. 2020]은 공간 조각된 아웃페인팅을 사용하여 단일 이미지에서 놀라운 360° 재구성을 보였습니다.카메라 포즈와 페어링된 RGB 이미지만 주어진 장면과 객체의 재구성 성능입니다.그러나 고밀도 카메라 뷰가 없으면 신경 광도장을 학습하는 것이 심각하게 제약이 부족한 문제가 됩니다.몇 개의 뷰만 주어지면 이러한 모델은 주어진 각 뷰에 과적합되어 새로운 뷰를 렌더링할 때 깨진 기하 구조와 흐릿한 노이즈가 발생할 수 있습니다 [Jain et al. 2021]. 최근, 고충실도 재구성에 필요한 뷰 수를 줄이기 위한 작업 라인이 도입되었습니다 [Jain et al. 2021; Roessle et al. 2022; Sajjadi et al. 2022; Verbin et al. 2022; Yu et al. 2021; Zhou and Tulsiani 2023]. 그럼에도 불구하고 적절한 재구성을 위해서는 여전히 두 개 이상의 뷰가 필요합니다. 2.2 단일 뷰에서 3D로 재구성 단일 이미지에서 3D 모델을 재구성하는 초기 작업의 대부분은 음영 [Zhang et al. 1999], 텍스처 [Loh 2006] 또는 디포커스 [Favaro and Soatto 2005]와 같은 이미지에 제공된 가시적 정보에 의존합니다. 최근 작업에서는 입력 이미지의 보이지 않는 부분을 생성하기 위해 보다 일반적인 사전을 사용합니다. 예를 들어, 일부 방법은 재구성에 사용할 수 있는 3D 사전 확률을 학습하기 위해 3D 데이터 세트를 사용합니다[Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Saito et al. 2019; Wang et al. 2018; Xie et al. 2019]. 그러나 이러한 모델이 실제 이미지로 일반화하려면 대규모 3D 데이터 세트가 필요합니다. 58억 5천만 개의 이미지-텍스트 쌍을 제공하는 LAION-5B와 같은 대규모 2D 이미지 데이터 세트와 비교할 때[Schuhmann et al. 2022], 3D 데이터 세트는 종종 다양성과 규모 면에서 제한적입니다. 반면에, 우리 모델은 3D 학습 데이터가 전혀 필요하지 않지만 기하학과 이미지 사전 확률을 활용하여 실제 이미지로 일반화할 수 있습니다[Bhat et al. 2023; Eftekhar et al. 2021; Rombach et al. 2022] 대규모 데이터 세트에서 학습되었습니다. 3D 학습 데이터 세트가 필요하다는 것에서 발생하는 문제를 극복하기 위해 이미지 컬렉션에서 3D 구조를 학습하는 방법이 도입되었습니다[Chan et al. 2023; Gu et al. 2023; Guo et al. 2022; Henzler et al. 2019; Jang and Agapito 2021; Kanazawa et al. 2018; Karnewar et al. 2023; Lin et al. 2023; Pavllo et al. 2023; Vasudev et al. 2022; Wu et al. 2023b; Ye et al. 2021]. 그러나 의미적 키 포인트 및 분할 마스크와 같은 추가 주석[Kanazawa et al. 2018]이나 정확한 카메라 매개변수가 있는 동일한 장면의 다중 뷰 이미지[Chan et al. 2023; Gu et al. 2023; Guo et al. 2022; Karnewar et al. 2023; Lin et al. 2023; Vasudev et al. 2022]. 장면당 단일 뷰로 학습하는 다른 방법은 범주별입니다[Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021]. 이와 대조적으로, 우리 모델은 우리가 통합하는 기성 모델 덕분에 단일 RGB 이미지 외에는 추가 정보가 필요하지 않습니다. 또한, 우리 모델은 주어진 뷰의 범주에 관계없이 실제 이미지로 일반화할 수 있다고 강조합니다. 3D 확산 모델[Shue et al. 2023; Wang et al. 2023b]도 주목을 받고 있지만, 동시 작업[Deng et al. 2023; Melas-Kyriazi et al. 2023; Tang et al. 2023; Xu et al. 2023]은 대규모 이미지-텍스트 데이터 세트[Schuhmann et al. 2022]에서 학습된 2D 확산 모델[Rombach et al. 2022]을 단일 뷰 재구성을 위한 사전 확률로 직접 사용하려고 시도했습니다.참조 뷰에서 보이지 않는 영역을 생성하기 위해 Poole et al. [2023]에서 도입한 점수 증류 샘플링 손실과 유사한 증류 손실에 크게 의존합니다.문제는 증류 손실이 주어진 단일 뷰에서 겹치는 영역이 있는 뷰에 동시에 적용된다는 것입니다.SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니.이것은 종종 RGB 손실을 방해하고 결과적으로 종종 입력 뷰의 재구성이 불량하게 됩니다.아주 최근의 작업[Tang et al. 2023]은 참조 이미지를 훈련된 3D 표현에 투영하여 이 문제를 우회하려고 시도하지만 참조 뷰에서 멀리 떨어진 새로운 뷰는 품질이 부족한 경향이 있습니다.반대로, 우리의 프레임워크는 다중 뷰 이미지로 구성된 의사 지상 진실 데이터 세트를 구축하여 다중 뷰 재구성 전략을 사용하여 고충실도 재구성을 가능하게 합니다.또한 이러한 모델[Deng et al. 2023; Melas-Kyriazi et al. 2023; Tang et al. 2023; Xu et al. 2023]에는 다른 한계도 있습니다.첫째, 96×96 또는 128×128과 같이 대상 해상도가 낮은 반면, 우리의 방법은 384×의 해상도를 목표로 하여 더 높은 품질과 전반적인 세부 정보를 제공하는 결과를 얻습니다.Tang et al. [2023]은 2단계 학습 방식을 통해 이 문제를 극복하려고 하지만 아래에 설명된 다른 문제도 공유합니다. 둘째, 이러한 작업은 기하 표현으로 순진한 신경 밀도 함수를 사용하는데, 이는 잘 정의된 표면 임계값이 없기 때문에 노이즈가 많은 아티팩트를 생성할 수 있습니다.반대로, 우리의 방법은 단순히 학습된 신경 암묵적 표면의 제로 레벨 세트에서 고충실도 기하 추출을 허용합니다.마지막으로, 이러한 모델은 주어진 단일 이미지와 그 증강에만 의존하여 텍스트 역전[Gal et al. 2023]과 유사한 방법을 사용하여 입력 이미지와 일치하는 보이지 않는 영역을 생성하려고 시도하여 확산 모델을 개인화합니다.이러한 방법과 달리, 우리의 데이터 생성 프레임워크는 다중 뷰 의사 지상 진실 이미지를 사용하여 동일한 객체의 여러 뷰가 필요한 최첨단 확산 모델 개인화 방법인 DreamBooth[Ruiz et al. 2023]를 사용할 수 있으므로 더 나은 개인화 품질이 가능합니다.Raj et al.[2023]도 DreamBooth를 사용하여 고품질의 개인화된 텍스트-3D를 달성할 수 있음을 보여주었습니다. 그러나 그들의 방법은 대상 객체의 여러 뷰를 필요로 하는 반면, 우리의 방법은 우리의 의사-지상-진실 다중 뷰 생성 방식 덕분에 객체의 단일 뷰만 필요합니다.단일 뷰에서 포인트 클라우드로.다른 최근 연구는 참조 뷰를 기반으로 색상화된 포인트 클라우드를 재구성하는 것을 목표로 합니다.예를 들어, MCC [Wu et al. 2023a]는 RGB-D 이미지를 입력으로 사용하여 리프팅된 색상 포인트를 대상 객체의 완전한 포인트 클라우드로 재구성합니다.마찬가지로, Point-E [Nichol et al. 2022]는 참조 RGB 이미지를 사용하여 입력 이미지와 유사한 색상화된 포인트 클라우드를 생성하는 포인트 클라우드 확산 모델을 도입합니다.이러한 모델과 달리, 우리의 프레임워크는 고충실도 신경 암묵적 표면과 뛰어난 품질의 모양을 재구성합니다.3D 사진.또 다른 작업 라인은 단안 깊이 예측과 색상 인페인팅을 사용하여 단일 이미지에서 3D 사진이나 장면을 생성합니다 [Han et al. 2022; Höllein et al. 2023; Shih et al. 2020; Zhang et al. 2023]. 그러나 이러한 방법은 전경과 배경을 동시에 인페인팅하도록 설계되었으며 객체의 뒷면을 고려하지 않습니다. 따라서 객체의 360° 재구성에 직접 적용할 수 없습니다. 단일 뷰에서의 새로운 뷰 합성. 일부 작업[Liu et al. 2023; Watson et al. 2023]은 입력 이미지와 상대 포즈가 주어졌을 때 3D 새로운 뷰를 생성하는 데 중점을 둡니다. 그러나 이러한 모델의 출력은 대략 3D 일관성만 있으므로 충실도 높은 모양 재구성을 보장하지 않습니다. SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니 초기화 카메라 위치 업데이트 초기화 아웃페인팅 마스크 수집 카메라 위치 업데이트 아웃페인팅 아웃페인팅 마스크 수집 프로세스 RGB 훈련 시각적 헐 추출 단일 RGB 아웃페인팅 볼륨 렌더링 초기 데이터 세트 초기화된 3D 모델 업데이트된 카메라 이전 카메라 &quot;[Dir]에서 본 흰색 배경의 [V] [Class] 사진&quot; 프로세스 RGB LDM 업데이트된 카메라에서 실루엣 획득 ה Ryu et al. 3D 모델 업데이트 보이는 영역 빼기 훈련 아웃페인팅 마스크 3D 모델 업데이트 훈련된 3D 모델 초기 새로운 뷰 업데이트된 가상 지상 진실 데이터 세트 훈련된 3D 모델 그림 2: 프레임워크 개요.POP3D는 상호 연결된 5단계로 작동합니다.처음에는 단일 RGB 입력을 처리하여 예비 가상 지상 진실 데이터 세트를 만들고 이 데이터를 사용하여 3D 모델을 초기화합니다. 그런 다음 대상 객체의 360° 뷰 전체를 포괄하는 것을 목표로 하는 단계 루프를 진행합니다. 이 루프에는 다음이 포함됩니다. 미리 정해진 일정에 따라 카메라 위치 업데이트, 가상 지상 진실 데이터 세트에서 시각적 헐을 추출하고 보이는 영역을 빼서 아웃페인팅 마스크 획득, 훈련된 3D 모델의 초기 신규 뷰, 아웃페인팅 마스크 및 적절한 텍스트 프롬프트를 사용하여 가상 지상 진실 신규 뷰 생성, 업데이트된 가상 지상 진실 데이터 세트를 사용하여 3D 모델 훈련. 이 프로세스는 객체의 360° 뷰를 포함할 때까지 계속됩니다. 입력 이미지: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [©laboratorija, CC BY-NC-SA]. 그림 3: 시각적 헐 추출. 두 카메라 뷰에서 시각적 헐을 획득하는 방법을 설명합니다. 대상 객체의 보이는 영역과 보이지 않는 영역을 모두 구성하는 오른쪽에 보이는 음영 처리된 영역을 보존합니다. 영어: 자동차 이미지: freesvg에서 가져옴 [퍼블릭 도메인].방법 그림 2에서 POP3D에 대한 개요를 제시합니다.객체의 단일 이미지가 주어지면 프레임워크는 신경 암묵적 표면 표현을 사용하여 360° 모양과 모양을 재구성합니다.핵심 아이디어는 색상과 기하학적 정보를 합성하여 객체의 보이지 않는 영역을 점진적으로 아웃페인팅하는 것입니다.이를 위해 프레임워크는 초기화, 카메라 위치 업데이트, 아웃페인팅 마스크 획득, 아웃페인팅, 3D 모델 업데이트의 5단계로 구성됩니다.초기화 단계에서는 입력 이미지의 깊이와 법선 s를 추정하여 3D 뷰로 끌어올립니다.그런 다음 카메라 위치를 이전에 본 적이 없는 근처 관점으로 맵을 업데이트하고 공간 조각을 사용하여 아웃페인팅할 영역을 나타내는 아웃페인팅 마스크를 얻습니다[Kutulakos and Seitz 1999].다음으로 잠재 확산 모델(LDM)을 사용하여 색상과 기하학적 정보를 생성하여 마스크된 영역을 아웃페인팅합니다[Rombach et al. 2022]. 마지막으로, 우리는 아웃페인팅된 정보를 사용하여 객체의 3D 모델을 업데이트합니다. 우리는 객체의 360° 전체를 커버할 때까지 이 단계를 반복합니다. 3D 객체의 모양과 외관을 표현하기 위해 우리는 신경망 쌍을 사용하여 3D 객체를 표현하는 VolSDF [Yariv et al. 2021]를 채택합니다. 구체적으로, 객체의 기하 구조를 표현하기 위해 우리는 3D 지점 x € R³를 표면과의 부호 거리 s = R에 매핑하는 부호 거리 함수(SDF) fo : x → s를 모델링하는 신경망을 사용합니다. 외관을 설명하기 위해 우리는 또 다른 신경망을 사용합니다. 여기서 ŵn은 지점 x에서 SDF의 공간적 기울기입니다. z는 Yariv et al. [2020]에서와 동일한 전역 기하 구조 특징 벡터입니다. VolSDF와 달리, 우리는 Lê에 대한 입력으로 시야 방향을 제공하지 않고, 단일 이미지가 시야에 따른 조명 정보를 제공하지 않고 기존의 아웃페인팅 방법이 시야 종속성을 고려하지 않기 때문에 시야에 따른 색상 변화를 무시합니다. 단일 이미지에서 3D 모델을 생성하는 것은 매우 잘못된 작업이므로, 우리는 재구성 결과의 가능한 결과를 제한하기 위해 몇 가지 가정을 부과합니다. 첫째, 우리는 대상 객체가 중심이 원점에 있고 길이가 2인 모서리가 좌표 축과 정렬된 큐브 내에 있다고 가정하고, Atzmon과 Lipman [2020]에 따라 객체를 단위 구로 초기화합니다. 또한 3D 재구성 프로세스 동안 대상 3D 객체를 바라보는 가상 카메라를 가정합니다. 구체적으로, 우리는 원점을 가리키도록 반경 3의 구에 카메라를 배치하고 구면 좌표 각도를 사용하여 위치를 매개변수화합니다. 입력 이미지의 카메라 매개변수가 주어지지 않았다고 가정할 때, 카메라의 시야(FOV)는 60°로 설정됩니다. 다음에서 프레임워크의 각 단계를 자세히 설명합니다.공간 조각 아웃페인팅을 사용한 단일 이미지에서 360° 재구성 3. 초기화 초기화 단계는 대상 객체의 입력 이미지 Lo에서 초기 3D 모델을 구성합니다.특히 Lo가 주어지면 먼저 기성품 바이너리 분할 방법[Lee et al. 2022]을 사용하여 바이너리 마스크 Mo를 추정하여 전경 객체를 추출합니다.그런 다음 기성품 단안 깊이 및 노멀 추정기를 사용하여 전경 객체의 깊이 맵 Do와 노멀 맵 No를 추정합니다[Bhat et al. 2023; Eftekhar et al. 2021].추정된 깊이 및 노멀 맵과 바이너리 마스크를 사용하여 초기 3D 모델을 추정합니다. 구체적으로, 먼저 의사-지상 진실 데이터 집합 P를 P = {(Lo, Do, No, Mo, 0)}로 초기화합니다. 여기서 0은 초기 카메라 위치이고, P를 사용하여 초기 3D 모델(fe, Lė)의 암묵적 표현을 학습합니다. 초기 카메라 위치는 0 = (90°, 0°)로 설정됩니다. 여기서 첫 번째와 두 번째 각도는 각각 극각과 방위각이며, 초기 이미지에 대상 객체의 정면이 포함되어 있다고 가정합니다. 의사-지상 진실 데이터 집합은 다음 단계에서 반복적으로 업데이트되어 대상 객체의 3D 모델을 점진적으로 재구성합니다. 암묵적 표현을 학습하기 위해 MonoSDF [Yu et al. 2022]의 접근 방식을 채택하여 마스크 Mo를 고려하도록 약간 수정했습니다. 학습에 대한 자세한 내용은 보충 자료를 참조하세요. 3. 카메라 위치 업데이트 모델이 초기화된 후 카메라 위치를 변경하여 대상 객체의 보이지 않는 영역을 탐색하면서 3D 모델을 반복적으로 업데이트합니다. 이를 위해 우리는 대상 객체의 360° 뷰를 포함하도록 설계된 카메라 스케줄 S = [40, 1, ..., s]를 정의하고 S에 따라 각 반복에서 카메라 위치를 업데이트합니다. 이론적으로 카메라 스케줄은 전체 360° 뷰를 포함하는 경우 임의의 집합이 될 수 있습니다. 그러나 우리는 지나치게 작거나 큰 간격이 출력에 부정적인 영향을 미칠 수 있음을 발견했습니다. 따라서 우리는 우리의
--- EXPERIMENT ---
s, 그리고 4.2.1절에서 지나치게 세분화되거나 거친 카메라 일정의 부정적인 영향에 대해 논의합니다. 이 섹션의 나머지 부분에서는 S에서 i번째 카메라 위치까지 탐색된 카메라 위치를 So:i로 표시하여 0 ≤ i ≤ s가 되도록 합니다. 3.3 아웃페인팅 마스크 획득 보이지 않는 영역의 모양과 모양을 원활하게 생성하려면 아웃페인팅을 위해 지정된 영역을 적절히 선택해야 합니다. 이를 해결하기 위해 시각적 껍질[Laurentini 1994]의 개념을 활용합니다. 시각적 껍질은 다양한 관점에서 본 객체의 실루엣에서 파생된 객체 모양의 대략적인 근사치를 제공합니다. 현재 데이터 세트 P를 사용하여 시각적 껍질을 계산하여 그림 3에서 설명한 대로 객체가 차지할 수 있는 최대 가능 영역을 결정할 수 있습니다. 업데이트된 카메라 뷰에서 보이는 시각적 껍질의 실루엣을 계산함으로써 이전에 관찰된 영역과 잠재적으로 보이지 않는 영역을 모두 포함하는 초기 마스크를 얻습니다. 아웃페인팅 마스크를 생성하기 위해 이 초기 마스크에서 관찰된 영역을 빼서 잠재적으로 새로운 가시 영역만 남겨둡니다.공간 조각을 통한 시각적 헐 계산.시각적 헐을 계산하기 위해 [Laurentini 1994], 투표 방식 [Kutulakos and Seitz 1999]에 의해 구동되는 깊이 기반 폭셀 조각 방법을 사용합니다.이 프로세스는 먼저 객체 경계 큐브를 폭셀화합니다.현재 SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW 시드니에서 So:i–1의 카메라 위치를 탐색했다고 가정합니다.그런 다음 폭셀화된 경계 큐브의 폭셀 p에 대해 j번째 뷰에 대한 투영이 j = {0,..., i - 1}인 전경 영역 내부에 있고 j번째 뷰의 카메라 중심까지의 거리가 전경 영역과 j번째 뷰의 카메라 중심 사이의 거리보다 길면 투표를 합니다. 수학적으로, KPjp € Mj, ||p − 0;||2 ≥ ||p* − 0j||2, (1)일 때 p에 대한 투표를 제기합니다. 여기서 K는 카메라 고유 행렬입니다. P; 및 o;는 각각 P의 j번째 뷰의 카메라 공간과 카메라 중심으로의 투영을 나타냅니다. p*는 fo의 제로 레벨 세트와 o;에서 p로 향하는 레이 캐스트의 교차점입니다. 여기서 우리는 레이가 외부에서 처음으로 객체를 관통하는 첫 번째 교차점만 고려합니다. 총 투표 수가 P의 크기 또는 뷰 수와 같으면 폭셀이 보존됩니다. 그렇지 않으면 폭셀이 조각됩니다. 이 절차를 통해 P의 시각적 껍질을 구성하는 폭셀을 수집합니다. 추가적으로, 단일 이미지만 있는 경우 이 프로세스는 훈련된 3D 표면의 돌출로 생각할 수 있습니다. 시각적 헐을 i번째 관점에 투영함으로써 워핑 연산을 통한 실루엣 MVH 전경 마스크 계산을 얻습니다. MVH에는 보이는 영역과 보이지 않는 영역이 모두 포함되어 있으므로 보이는 영역을 빼서 아웃페인팅 마스크 M¿를 얻어야 합니다. 이는 워핑 연산을 사용하여 대상 뷰에서 전경 마스크 MEG를 계산함으로써 달성됩니다. 이 프로세스에는 이전 관점 So:i-1에서 깊이를 렌더링하고, 이미지 포인트를 3D 공간으로 들어올린 다음, 들어올린 포인트를 대상 뷰 i에 투영하는 것이 포함됩니다. 워핑 프로세스 중에 앨리어싱을 완화하기 위해 이미지를 배율 인수 8로 확대합니다. 가시성을 고려하고 백포인트 컬링을 통해 대상 관점에서 보이지 않는 픽셀을 워핑하지 않습니다. 구체적으로 백포인트 컬링을 포함한 워핑 프로세스는 다음과 같이 수행됩니다. j번째 관점에서 i번째 관점으로 이미지를 워핑하는 프로세스에서 P의 j번째 이미지에서 들어올린 픽셀을 p³로 표시합니다. 그런 다음, ƒÃ에서 정상 Ñ³를 렌더링합니다. 픽셀은 다음의 경우에만 대상 카메라 중심 o에 대해 워핑됩니다. (p³ – oi) · Ñ³
--- CONCLUSION ---
및 향후 작업 이 연구에서는 단일 RGB 이미지에서 360° 재구성 분야의 두 가지 오랜 과제인 일반화와 충실도를 해결하는 새로운 프레임워크인 POP3D를 제시합니다.POP3D는 대규모 데이터 세트에서 학습된 최신 사전 확률을 최대한 활용하고 일반화 문제를 성공적으로 극복합니다.SA Conference Papers &#39;23, 2023년 12월 12-15일, 호주 NSW주 시드니 Ryu et al. (a) 입력 RGB 이미지 (b) 새로운 뷰 (c) 전체 뒷면 그림 7: 제한 사항.왼쪽의 입력 RGB(a)가 주어지면, 우리 모델은 카메라 일정에 따라 그럴듯한 새로운 뷰(b)를 생성합니다.그러나 우리 모델은 때때로 (c)에서 볼 수 있듯이 입력 단일 RGB와 비교했을 때 열악한 전체 뒷면 이미지를 생성합니다.(a)의 이미지: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [ShekhirevaVictoria, CC BY]. 임의의 이미지에. 실험적으로, 우리의 프레임워크가 제공된 단일 RGB 입력을 충실하게 재구성할 뿐만 아니라 현실적인 새로운 뷰도 생성한다는 것을 보여줍니다. 이러한 뷰는 집합적으로 의사-지상-진실 다중 뷰 데이터 세트를 형성하여 다중 뷰 재구성 전략의 직접적인 적용을 용이하게 합니다. 기존 방법론과 비교할 때, 우리의 접근 방식은 더 뛰어난 성능을 보여 3D 재구성 작업을 위한 견고한 솔루션으로서의 잠재력을 재확인합니다. 5. 제한 사항 우리의 접근 방식에는 특정한 제한 사항이 있습니다. 우리의 프레임워크는 파이프라인에서 각각 중요한 역할을 하는 기성품 사전 확률의 구성이기 때문에 한 모델이 실패하면 최종 재구성 결과에 영향을 미칠 수 있습니다. 예를 들어, 단안 깊이 또는 일반 예측 변수가 얇은 구조와 같은 어려운 경우에 실패하면 재구성된 모양에 아티팩트가 발생할 수 있습니다. 게다가, 우리의 프레임워크는 때때로 입력 뷰와 비교했을 때 객체의 완전한 뒷면을 열악한 품질로 생성합니다. 그림 7에서 이 문제를 설명합니다. 이러한 결함은 기성품 사전 모델의 성능을 손상시키고 장기적으로 전체 이미지 품질을 저하시킬 수 있는 아웃페인팅 아티팩트의 축적에 기인할 수 있습니다. 저희의 접근 방식은 가상 지상 진실 이미지를 생성하여 뷰 수를 점진적으로 늘리기 때문에 3D 모델 학습과 관련된 계산 시간도 카메라 일정에 따라 증가합니다. 단일 객체를 재구성하는 데 현재 실행 시간은 단일 3090 RTX GPU를 사용하여 약 7시간이 걸립니다. 그럼에도 불구하고 저희 프레임워크는 모듈식 구조를 가지고 있으며 각 단계에서 사용된 모델을 쉽게 교체할 수 있습니다. 특히 VolSDF [Yariv et al. 2021]를 보다 고급 방법 [Rosu and Behnke 2023; Wang et al. 2022]으로 교체하여 재구성 프로세스를 가속화할 수 있습니다. 나아가 DreamBooth [Ruiz et al. 2023]를 가속화하기 위해 LORA [Hu et al. 2022]를 채택할 수도 있습니다. 이러한 문제를 해결하기 위해 향후 연구에서는 아티팩트를 최소화하고 재구성 시간을 개선하는 동시에 재구성 프로세스를 더욱 개선하는 방법을 탐색하는 데 중점을 둘 것입니다.감사의 말 이 연구는 한국 정부(MSIT)가 지원한 IITP 보조금(IITP-2021-0-02068, IITP-2019-0-01906)과 중소기업창업부(MSS, 한국)가 2021년에 지원한 창업성장기술연구개발사업(TIPS 프로그램, (No. S3200141))의 지원을 받았습니다.참고문헌 Matan Atzmon 및 Yaron Lipman. 2020. SAL: 원시 데이터에서 모양에 대한 부호 독립적 학습.CVPR 논문집.Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla 및 Pratul P. Srinivasan. 2021. Mip-NeRF: 앤티 앨리어싱 신경 복사장에 대한 다중 스케일 표현. ICCV의 Proc.에서. Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, Peter Hedman. 2022. Mip-NeRF 360: 무제한 앤티 앨리어싱 신경 복사장. CVPR의 Proc.에서. Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, Matthias Müller. 2023. ZoeDepth: 상대적 깊이와 메트릭 깊이를 결합하여 제로 샷 전송. arXiv:2302.12288 [cs.CV] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. 2023. 3D 인식 확산 모델을 사용한 생성적 새로운 뷰 합성. arXiv:2304.02602 [cs.CV] Christopher B Choy, Danfei Xu, Jun Young Gwak, Kevin Chen, and Silvio Savarese. 2016. 3D-R2N2: 단일 및 다중 뷰 3D 객체 재구성을 위한 통합 접근 방식. ECCV 논문에서. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi 및 Ali Farhadi. 2022. Objaverse: 주석이 달린 3D 객체의 세계. arXiv:2212.08051 [cs.CV] Congyue Deng, Chiyu &quot;Max&quot; Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas 및 Dragomir Anguelov. 2023. NeRDi: 일반 이미지 우선 순위로서 언어 기반 확산을 사용한 단일 뷰 NeRF 합성. Proc에서 CVPR의. 2063720647. Ainaz Eftekhar, Alexander Sax, Jitendra Malik 및 Amir Zamir. 2021. Omnidata: 3D 스캔에서 다중 작업 중간 수준 비전 데이터 세트를 만드는 확장 가능한 파이프라인. ICCV의 Proc.에서. 10786-10796. P. Favaro 및 S. Soatto. 2005. 디포커스에서 모양을 만드는 기하학적 접근 방식. IEEE 패턴 분석 및 머신 인텔리전스(PAMI) 27, 3(2005), 406417. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik 및 Daniel Cohen-or. 2023. 이미지는 한 단어의 가치가 있습니다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. ICLR의 Proc.에서. R. Girdhar, DF Fouhey, M. Rodriguez 및 A. Gupta. 2016. 객체에 대한 예측 가능하고 생성적인 벡터 표현 학습. ECCV 논문에서. Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan Russell, Mathieu Aubry. 2018. AtlasNet: 3D 표면 생성 학습을 위한 파피에마셰 접근법. CVPR 논문에서. Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, Ravi Ramamoorthi. 2023. NerfDiff: 3D 인식 확산에서 NeRF 유도 증류를 사용한 단일 이미지 뷰 합성. ICML 논문에서. Pengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel Ulbricht, Joshua M. Susskind, Qi Shan. 2022. 빠르고 명시적인 신경 뷰 합성. WACV 논문집에서. 3791-3800. Yuxuan Han, Ruicheng Wang, Jiaolong Yang. 2022. 학습된 적응형 다중 평면 이미지를 사용한 야생에서의 단일 뷰 뷰 합성. ACM SIGGRAPH 논문집에서. Philipp Henzler, Niloy J Mitra,, Tobias Ritschel. 2019. 플라톤의 동굴 탈출: 적대적 렌더링에서 얻은 3D 모양. ICCV 논문집에서. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2022. LORA: 대규모 언어 모델의 저순위 적응. ICLR 논문집에서. Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, Matthias Nieẞner. 2023. Text2Room: 2D 텍스트-이미지 모델에서 텍스처가 있는 3D 메시 추출. arXiv:2303.11989 [cs.CV] Ajay Jain, Matthew Tancik, Pieter Abbeel. 2021. NeRF 다이어트: 의미적으로 일관된 Few-Shot View 합성. ICCV 논문집에서. 5885-5894. Wonbong Jang, Lourdes Agapito. 2021. CodeNeRF: 객체 범주에 대한 얽힌 신경 광도장. ICCV 논문집에서. 12949-12958. Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, Jitendra Malik. 2018. 이미지 컬렉션에서 범주별 메시 재구성 학습. ECCV 논문집에서. Animesh Karnewar, Andrea Vedaldi, David Novotny, Niloy J. Mitra. 2023. HOLODIFFUSION: 2D 이미지를 사용한 3D 확산 모델 훈련. CVPR 논문집에서. 18423-18433. KN Kutulakos와 SM Seitz. 1999. 공간 조각에 의한 모양 이론. ICCV 논문집에서. 307-314 vol.1. A. Laurentini. 1994. 실루엣 기반 이미지 이해를 위한 시각적 껍질 개념. IEEE 패턴 분석 및 머신 인텔리전스(PAMI) 16,(1994), 150-162. Min Seok Lee, WooSeok Shin, Sung Won Han. 2022. TRACER: 극도의 주의 유도 돌출 객체 추적 네트워크. CVPR 논문집에서. AAAI Conference on Artificial 360° Reconstruction From a Single Image Using Space Carved Outpainting Intelligence, Vol. 36. 12993-12994. Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. 2023. Vision Transformer for NeRF-Based View Synthesis From a Single Input Image. In Proc. of WACV. 806-815. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot One Image to 3D Object. arXiv:2303.11328 [cs.CV] Angeline Loh. 2006. The recovery of 3-D structure using visual texture patterns. 박사 학위 논문. Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. 2023. RealFusion: 360도 Reconstruction of Any Object From a Single Image. CVPR 논문집에서. 8446-8455. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. ECCV 논문집에서. Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. 2013. &quot;완전히 블라인드&quot; 이미지 품질 분석기 만들기. IEEE Signal Processing Letters 20, 3(2013), 209-212. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022. Point-E: 복합 프롬프트에서 3D 포인트 클라우드를 생성하는 시스템. arXiv:2212.08751 [cs.CV] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Differentiable Volumetric Rendering: 3D Supervision 없이 암묵적 3D 표현 학습. CVPR의 Proc.에서. Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, and Federico Tombari. 2023. Bootstrapped를 통한 단일 이미지의 모양, 포즈 및 외관 Radiance Field Inversion. CVPR의 논문에서. Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall. 2023. DreamFusion: 2D 확산을 사용한 텍스트-3D. ICLR의 논문에서. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 2021. 자연어 감독에서 전이 가능한 시각적 모델 학습. ICML의 논문에서, Vol. 139. 8748-8763. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li 및 Varun Jampani. 2023. DreamBooth3D: 주제 기반 텍스트를 3D로 생성. arXiv:2303.13508 [cs.CV] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan 및 Matthias Nieẞner. 2022. 희소 입력 뷰의 신경 복사 필드에 대한 조밀한 깊이 사전. Proc에서 CVPR의. 로빈 롬바흐(Robin Rombach), 안드레아스 블라트만(Andreas Blattmann), 도미니크 로렌츠(Dominik Lorenz), 패트릭 에세르(Patrick Esser), 비요른 오머(Björn Ommer). 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. CVPR 논문집에서. 10684-10695. Radu Alexandru Rosu와 Sven Behnke. 2023. PermutoSDF: Permutohedral 격자를 사용한 암묵적 표면을 사용한 빠른 다중 뷰 재구성. CVPR 논문집에서. 8466-8475. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. 2023. DreamBooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. CVPR 논문집에서. 22500-22510. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li. 2019. PIFu: 고해상도 옷을 입은 인간 디지털화를 위한 픽셀 정렬 암시적 함수. ICCV의 논문. Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lučić, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas Funkhouser, Andrea Tagliasacchi. 2022. 장면 표현 변환기: 세트-잠재적 장면 표현을 통한 기하학 없는 새로운 뷰 합성. CVPR의 논문. 6229-6238. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev. 2022. LAION-5B: 차세대 이미지-텍스트 모델을 학습하기 위한 대규모 오픈 데이터 세트. NeurIPS 논문. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, Jia-Bin Huang. 2020. 컨텍스트 인식 계층적 깊이 인페인팅을 사용한 3D 사진. CVPR 논문. J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, Gordon Wetzstein. 2023. Triplane Diffusion을 사용한 3D 신경장 생성. CVPR 논문집에서. 20875-20886. Vincent Sitzmann, Michael Zollhöfer, Gordon Wetzstein. 2019. 장면 표현 네트워크: 연속적인 3D 구조 인식 신경 장면 표현. NeurIPS 논문집에서. Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, Dong Chen. 2023. Make-It-3D: 확산 사전을 사용한 단일 이미지에서 고충실도 3D 생성. arXiv:2303.14184 [cs.CV] Kalyan Alwala Vasudev, Abhinav Gupta, Shubham Tulsiani. 2022. 사전 학습, 자체 학습, 증류: 3D 재구성을 위한 간단한 레시피. CVPR의 Proc.에서. Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, Pratul P. Srinivasan. 2022. Ref-NeRF: SA 컨퍼런스 논문 &#39;23을 위한 구조화된 뷰 종속적 모양, 2023년 12월 12-15일, 호주 NSW 시드니 신경 복사장. CVPR의 Proc.에서. Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, Chen Change Loy. 2023a. 실제 세계 이미지 초고해상도를 위한 확산 사전 활용. arXiv:2305.07015 [cs.CV] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, Yu-Gang Jiang. 2018. Pixel2Mesh: 단일 RGB 이미지에서 3D 메시 모델 생성. ECCV의 Proc.에서 Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, Baining Guo. 2023b. RODIN: 확산을 사용하여 3D 디지털 아바타를 조각하기 위한 생성 모델. CVPR의 Proc.에서. 4563-4573. Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, Lingjie Liu. 2022. NeuS2: 다중 뷰 재구성을 위한 신경 암묵적 표면의 빠른 학습. arXiv:2212.05231 [cs.CV] Zhou Wang, AC Bovik, HR Sheikh, EP Simoncelli. 2004. 이미지 품질 평가: 오류 가시성에서 구조적 유사성까지. IEEE Transactions on Image Processing 13, 4 (2004), 600-612. Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. 2023. 확산 모델을 사용한 새로운 뷰 합성. ICLR의 Proc.에서. Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. 2023a. 3D 재구성을 위한 다중 뷰 압축 코딩. CVPR의 Proc.에서. 9065-9075. Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. 2023b. MagicPony: 야생에서 관절이 있는 3D 동물 학습. CVPR의 Proc. Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou 및 Shengping Zhang. 2019. Pix2Vox: 단일 및 다중 뷰 이미지의 상황 인식 3D 재구성. Proc에서 ICCV의. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang 및 Zhangyang Wang. 2023. NeuralLift-360: 야생 2D 사진을 360도 뷰의 3D 개체로 들어 올리기. Proc에서 CVPR의. 4479-4489. Lior Yariv, Jiatao Gu, Yoni Kasten 및 Yaron Lipman. 2021. 신경 암시적 표면의 볼륨 렌더링. Proc에서 NeurIPS의 Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. 기하학과 외관을 분리하여 다중 시점 신경 표면 재구성. NeurIPS 논문집에서. Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. 2021. 야생에서의 선반 감독 메시 예측. CVPR 논문집에서. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelNeRF: 하나 또는 몇 개의 이미지에서 추출한 신경 복사장. CVPR 논문집에서. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. 2022. MonoSDF: 신경 암묵적 표면 재구성을 위한 단안적 기하학적 단서 탐색. 논문집에서. NeurIPS의 Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao. 2023. Text2NeRF: 신경 복사장을 사용한 텍스트 기반 3D 장면 생성. arXiv:2305.11588 [cs.CV] Kai Zhang, Gernot Riegler, Noah Snavely, Vladlen Koltun. 2020. NeRF++: 신경 복사장 분석 및 개선. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang. 2018. 지각적 지표로서의 딥 피처의 비합리적 효과. CVPR의 Proc.에서. Ruo Zhang, Ping-Sing Tsai, JE Cryer, M. Shah. 1999. 셰이딩에서 모양: 조사. IEEE 패턴 분석 및 머신 인텔리전스(PAMI) 21,(1999), 690-706. Zhizhuo Zhou 및 Shubham Tulsiani. 2023. SparseFusion: 3D 재구성을 위한 뷰 조건 확산 증류. CVPR 논문집. 12588-12597. SS3D Pe MCC NL RF H Ours SA 학술 대회 논문 &#39;23, 2023년 12월 12-15일, 호주 NSW주 시드니 입력 그림 8: 정성적 비교. 다양한 모델을 사용하여 위에 주어진 단일 RGB 이미지의 360° 모양과 외관을 재구성하고 이를 결과와 비교합니다. NL, RF, MCC, Pe 및 SS3D는 NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], Point-E [Nichol et al. 2022], SS3D [Vasudev et al. 2022]를 각각 사용합니다. SS3D [Vasudev et al. 2022]는 객체의 모양을 재구성하지 않으므로 모양 출력만 표시합니다. 더 나은 시각화를 위해 MCC [Wu et al. 2023a]에 마칭 큐브를 사용하여 포인트 클라우드를 샘플링하는 데 사용된 것과 동일한 점유 임계값을 가진 표면을 추출합니다. Point-E [Nichol et al. 2022]의 경우 저자가 제공한 포인트 클라우드-메시 변환을 사용합니다. 입력: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [©Horton, CC BY]. Ryu et al. SS3D Pe MCC NL RF Ours 360° Reconstruction From a Single Image Using Space Carved Outpainting H Input SA Conference Papers &#39;23, December 12-15, 2023, Sydney, NSW, Australia ] 그림 9: 또 다른 정성적 비교. 다양한 모델을 사용하여 위에 주어진 단일 RGB 이미지의 360° 모양과 외관을 재구성하고 이를 결과와 비교합니다. NL, RF, MCC, Pe 및 SS3D는 각각 NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], Point-E [Nichol et al. 2022] 및 SS3D [Vasudev et al. 2022]를 의미합니다. SS3D [Vasudev et al. 2022]는 객체의 모양을 재구성하지 않으므로 모양 출력만 보여줍니다. 더 나은 시각화를 위해 MCC [Wu et al. 2023a]에 마칭 큐브를 사용하여 포인트 클라우드를 샘플링하는 데 사용된 것과 동일한 점유 임계값을 가진 표면을 추출합니다. Point-E [Nichol et al. 2022]의 경우 저자가 제공한 포인트 클라우드-메시 변환을 사용합니다. 입력: Objaverse 데이터 세트의 데이터에서 렌더링됨 [Deitke et al. 2022] [©shirava, CC BY].
