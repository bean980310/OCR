--- INTRODUCTION ---
매력적인 스토리텔링 비디오를 만드는 것은 일반적으로 실사 촬영이나 CG 애니메이션 제작을 포함하는 복잡하고 힘든 과정입니다. 이러한 기술적 특성은 전문 콘텐츠 제작자에게 상당한 리소스를 요구할 뿐만 아니라 일반 대중이 이 강력한 매체를 효과적으로 활용하는 데 장벽을 만듭니다. 최근 텍스트-비디오(T2V) 생성에서 상당한 진전이 이루어져 텍스트 설명을 기반으로 비디오를 자동으로 생성할 수 있게 되었습니다[He et al. 2022; Ho et al. 2022a; Singer et al. 2022; Zhou et al. 2022]. 그러나 이러한 비디오 생성 기술의 효과는 여전히 제한되어 기대에 미치지 못하는 결과를 낳고 실제 적용을 방해합니다. 또한 생성된 비디오의 레이아웃과 구성을 텍스트를 통해 제어할 수 없으며, 이는 매력적인 스토리를 시각화하고 영화를 촬영하는 데 중요합니다. 예를 들어 클로즈업, 롱샷 및 구성은 감독이 관객에게 암묵적인 정보를 전달하는 데 도움이 될 수 있습니다. 현재의 텍스트-비디오 생성 모델은 필름의 요구 사항을 충족하는 적절한 동작과 레이아웃을 생성하기 어렵습니다. 이러한 과제를 극복하기 위해, 우리는 풍부한 기존 비디오 콘텐츠를 T2V 생성 프로세스에 통합하는 새로운 비디오 생성 방식을 제안합니다. 이를 검색 증강 비디오 생성이라고 합니다. 구체적으로, 우리의 방식은 텍스트 프롬프트를 기반으로 외부 데이터베이스에서 비디오를 검색하여 T2V 생성을 위한 안내 신호로 활용합니다. 이 아이디어를 바탕으로, 우리의 방식은 입력 검색 비디오를 구조 참조로 활용하여 스토리를 애니메이션화할 때 생성된 비디오의 레이아웃과 구성을 사용자가 더 잘 제어할 수 있도록 합니다. 또한, 검색된 비디오에 포함된 풍부한 지식과 정보를 활용하여 생성된 비디오의 품질을 향상시켜 생성된 장면의 사실성과 일관성을 개선할 수 있습니다. 텍스트 프롬프트는 이제 새로운 비디오를 생성하기 위해 장면과 객체의 모양을 렌더링하는 역할을 합니다. 그러나 이러한 검색 유도 비디오 생성 프로세스는 여전히 서로 다른 비디오 클립에서 캐릭터의 불일치 문제로 어려움을 겪습니다. 게다가 캐릭터의 모습은 텍스트 프롬프트에 의해 제어되고 사용자 제어가 부족한 확률적 방식으로 생성됩니다. 이 문제를 더욱 해결하기 위해 개인화에 대한 기존 문헌[Ruiz et al. 2023]을 연구하여 생성 모델을 미세 조정하여 캐릭터의 모습을 다시 렌더링하고 개인화된 개념을 더 잘 표현하고 성능을 개선하는 새로운 방식(TimeInv)을 제안합니다. 검색 강화 T2V 생성과 비디오 캐릭터 재렌더링이라는 두 가지 핵심 모듈을 통합함으로써, 우리의 접근 방식은 콘텐츠 제작자가 고품질 애니메이션 비디오를 제작할 수 있는 보다 효율적이고 접근 가능한 방식을 제공합니다. 그 효과를 정당화하기 위해, 우리는 다음과 같은 관점에서 우리의 방법을 평가합니다. 첫째, 우리의 검색 강화 T2V 생성 모델을 기존 기준선과 비교하여 비디오 생성 성능에서 눈에 띄는 우수성을 보여줍니다. 둘째, 우리는 기존 경쟁자와 관련하여 제안된 개인화 방법의 이점을 정당화합니다. 또한, 제안된 스토리텔링 비디오 합성 프레임워크의 전반적인 효과성에 대한 포괄적인 실험을 수행하여 실제 응용 분야에 대한 잠재력을 제시합니다.2. 우리의 기여는 다음과 같이 요약됩니다. • 스토리텔링 비디오 합성을 위한 새로운 검색 증강 패러다임을 제시하여 처음으로 스토리텔링 목적으로 기존의 다양한 비디오를 사용할 수 있게 합니다. 실험 결과는 프레임워크의 효과를 입증하여 놀라운 편의성을 갖춘 새로운 비디오 제작 도구로 자리 매김합니다. • 구조 안내와 문자 생성 간의 갈등을 효과적으로 해결하는 조정 가능한 구조 안내 텍스트-비디오 모델을 제안합니다. • 기존 경쟁자보다 눈에 띄게 우수한 새로운 개념의 개인화 접근 방식인 TimeInv를 제안합니다.
--- RELATED WORK ---
비디오 세대 수많은 초기 작품 [Saito et al. 2017; Skorokhodov et al. 2022년; Tulyakov et al. 2018; Vondrick et al. 2016; Wuet al. 2021] 무조건 영상 생성에 집중
--- METHOD ---
다양한 플롯 시나리오에서 일관된 캐릭터 렌더링을 보장합니다. 각 클립은 세 개의 키 프레임으로 시각화됩니다. 시각적 스토리텔링을 위한 비디오를 생성하는 것은 일반적으로 라이브 액션 촬영이나 그래픽 애니메이션 렌더링이 필요한 지루하고 복잡한 프로세스가 될 수 있습니다. 이러한 과제를 우회하기 위해 우리의 핵심 아이디어는 기존 비디오 클립의 풍부함을 활용하고 모양을 사용자 지정하여 일관된 스토리텔링 비디오를 합성하는 것입니다. 우리는 두 가지 기능 모듈로 구성된 프레임워크를 개발하여 이를 달성합니다. (i) 쿼리 텍스트로 설명된 원하는 장면 또는 모션 컨텍스트를 비디오 후보에 제공하는 모션 구조 검색, (ii) 모션 구조 및 *첫 번째 저자 *연락 저자 프로젝트 페이지: https://videocrafter.github.io/Animate-A-Story 텍스트 프롬프트의 안내에 따라 플롯에 맞춰진 비디오를 생성합니다. 첫 번째 모듈의 경우 기성품 비디오 검색 시스템을 활용하고 비디오 깊이를 모션 구조로 추출합니다. 두 번째 모듈에서는 구조와 캐릭터에 대한 유연한 제어를 제공하는 제어 가능한 비디오 생성 모델을 제안합니다. 비디오는 구조적 지침과 모양 지침을 따라 합성됩니다. 클립 전체에서 시각적 일관성을 보장하기 위해 텍스트 프롬프트를 통해 원하는 캐릭터 정체성을 지정할 수 있는 효과적인 개념 개인화 접근 방식을 제안합니다. 광범위한
--- EXPERIMENT ---
s는 우리의 접근 방식이 다양한 기존 기준선에 비해 상당한 이점을 보인다는 것을 보여줍니다.추가 키워드 및 구문: 스토리 시각화, 비디오 확산 모델, 검색 증강 생성, 개인화된 생성.Yingqing HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, Qifeng Chen 서론 매력적인 스토리텔링 비디오를 만드는 것은 일반적으로 실사 촬영이나 CG 애니메이션 제작을 포함하는 복잡하고 힘든 과정입니다.이러한 기술적 특성은 전문 콘텐츠 제작자에게 상당한 리소스를 요구할 뿐만 아니라 일반 대중이 이 강력한 매체를 효과적으로 활용하는 데 장벽을 만듭니다.최근에는 텍스트-비디오(T2V) 생성에서 상당한 진전이 이루어져 텍스트 설명을 기반으로 비디오를 자동으로 생성할 수 있게 되었습니다[He et al. 2022; Ho et al. 2022a; Singer et al. 2022; Zhou et al. 2022]. 그러나 이러한 비디오 생성 기술의 효과는 여전히 제한적이어서 기대에 미치지 못하는 결과를 낳고 실제 적용을 방해합니다. 또한 생성된 비디오의 레이아웃과 구성은 텍스트를 통해 제어할 수 없으며, 이는 매력적인 스토리를 시각화하고 영화를 촬영하는 데 중요합니다. 예를 들어 클로즈업, 롱샷 및 구성은 감독이 관객에게 암묵적인 정보를 전달하는 데 도움이 될 수 있습니다. 현재의 텍스트-비디오 생성 모델은 필름의 요구 사항을 충족하는 적절한 동작과 레이아웃을 생성하기 어렵습니다. 이러한 과제를 극복하기 위해 기존 비디오 콘텐츠의 풍부함을 T2V 생성 프로세스에 통합하는 새로운 비디오 생성 방식을 제안합니다. 이를 검색 증강 비디오 생성이라고 합니다. 구체적으로, 우리의 방식은 텍스트 프롬프트를 기반으로 외부 데이터베이스에서 비디오를 검색하여 T2V 생성을 위한 안내 신호로 활용합니다. 이 아이디어를 바탕으로, 우리의 방식은 사용자가 입력 검색 비디오를 구조 참조로 활용하여 스토리를 애니메이션화할 때 생성된 비디오의 레이아웃과 구성을 더 잘 제어할 수 있도록 합니다. 또한, 검색된 비디오에 포함된 풍부한 지식과 정보를 활용하여 생성된 비디오의 품질을 향상시켜 생성된 장면의 사실성과 일관성을 개선할 수 있습니다. 텍스트 프롬프트는 이제 장면과 객체의 모양을 렌더링하여 새로운 비디오를 생성하는 역할을 합니다. 그러나 이러한 검색 기반 비디오 생성 프로세스는 여전히 서로 다른 비디오 클립에서 캐릭터의 불일치 문제로 어려움을 겪습니다. 게다가 캐릭터의 모양은 텍스트 프롬프트에 의해 제어되고 사용자 제어가 부족한 확률적 방식으로 생성됩니다. 이 문제를 더욱 해결하기 위해 개인화에 대한 기존 문헌[Ruiz et al. 2023]을 연구하여 생성 모델을 미세 조정하여 캐릭터의 모양을 다시 렌더링하고 개인화된 개념을 더 잘 표현하고 성능을 개선하는 새로운 접근 방식(TimeInv)을 제안합니다. 검색 강화 T2V 생성 및 비디오 캐릭터 재렌더링이라는 두 가지 핵심 모듈을 통합하여 콘텐츠 제작자가 고품질 애니메이션 비디오를 제작할 수 있는 보다 효율적이고 접근 가능한 방법을 제공합니다. 효과를 입증하기 위해 다음과 같은 관점에서 방법을 평가합니다. 첫째, 검색 강화 T2V 생성 모델을 기존 기준선과 비교하여 비디오 생성 성능에서 눈에 띄는 우수성을 입증합니다. 둘째, 기존 경쟁자와 관련하여 제안한 개인화 방법의 이점을 입증합니다. 나아가 제안한 스토리텔링 비디오 합성 프레임워크의 전반적인 효과에 대한 포괄적인 실험을 수행하여 실용적인 응용 분야에 대한 잠재력을 제시합니다. 2. 기여는 다음과 같이 요약됩니다. • 스토리텔링 비디오 합성을 위한 새로운 검색 강화 패러다임을 제시하여 처음으로 스토리텔링 목적으로 기존의 다양한 비디오를 사용할 수 있게 합니다. 실험 결과는 프레임워크의 효과를 입증하여 놀라운 편의성을 갖춘 새로운 비디오 제작 도구로 자리 매김합니다. • 구조 안내와 문자 생성 간의 갈등을 효과적으로 해결하는 조정 가능한 구조 안내 텍스트-비디오 모델을 제안합니다. • 기존 경쟁자보다 눈에 띄게 우수한 성과를 보이는 새로운 개념의 개인화 접근 방식인 TimeInv를 제안합니다. 관련 연구 비디오 생성 이전의 여러 연구[Saito et al. 2017; Skorokhodov et al. 2022; Tulyakov et al. 2018; Vondrick et al. 2016; Wu et al. 2021]는 무조건 비디오 생성 방법에 초점을 맞추고 생성적 적대 네트워크(GAN) 또는 변형 자동 인코더(VAE)를 사용하여 비디오 분포를 모델링합니다. 예를 들어, VGAN[Vondrick et al. 2016]은 랜덤 노이즈를 시공간 입방체 피처 맵에 매핑하여 전경과 배경을 별도로 모델링합니다. 융합된 피처 맵은 생성된 비디오를 나타내며 판별기에 입력됩니다. TGAN[Saito et al. 2017]은 시간 생성기로 노이즈 시퀀스를 생성한 다음 이미지 생성기를 사용하여 이미지로 변환합니다. 이러한 이미지는 연결되어 비디오 판별기에 입력됩니다. StyleGAN-V[Skorokhodov et al. 2022]는 StyleGAN의 기능을 활용합니다. 여러 개의 랜덤 노이즈를 사용하여 동작을 제어하고 추가 노이즈를 사용하여 모양을 관리합니다. 여러 방법[Ge et al. 2022; Yan et al. 2021; Yu et al. 2023a]은 잠재 공간의 변환기를 사용하여 시공간적 종속성을 포착하는 것을 목표로 합니다. 처음에는 VAE 또는 VQGAN[Esser et al. 2021]을 학습하여 비디오를 잠재 공간에 투사한 다음 변환기를 학습하여 잠재 분포를 모델링합니다. 예를 들어, TATS[Ge et al. 2022]는 시간에 독립적인 VQGAN을 학습한 다음 잠재 특징을 기반으로 시간에 민감한 변환기를 학습합니다. VideoGPT[Yan et al. 2021]는 비슷한 파이프라인을 따릅니다. 생성된 콘텐츠에 대한 텍스트 제어를 가능하게 하기 위해 일부 작업[Hong et al. 2022; Villegas et al. 2022] 시각적 및 텍스트 토큰을 추출하여 동일한 잠재 공간에 투사합니다. 변환기는 지속적으로 그들 간의 상호 의존성을 모델링하는 데 사용됩니다. 최근 텍스트-이미지(T2I) 생성[Ramesh et al. 2022; Rombach et al. 2022; Saharia et al. 2022]은 주로 확산 기반 모델 덕분에 고품질 이미지 생성에서 상당한 진전을 이루었습니다. T2I 생성의 진전을 활용하여 텍스트-비디오 생성 분야도 획기적인 발전을 이루었습니다. VDM[Ho et al. 2022b]은 비디오 생성에 확산 모델을 사용한 최초의 작업입니다. Make-a-video[Singer et al. 2022]와 Imagen Video[Ho et al. 2022a]는 처음에 저해상도에서 비디오 분포를 모델링한 다음 시공간 보간을 적용하여 해상도와 시간 지속 시간을 늘리는 계단식 모델입니다. LDM[Rombach et al. 2022], 여러 연구[Blattmann et al. 2023; He et al. 2022; Luo et al. 2023; Mei and Patel 2022; Yu et al. 2023b; Zhou et al. 2022]는 비디오 생성을 위해 LDM을 확장합니다. 예를 들어, LVDM[He et al. 2022]은 시간적 주의 계층을 도입하여 LDM을 비디오 버전으로 팽창시킵니다. 사전 학습된 LDM을 초기화로 사용하고 비디오 데이터로 학습 가능한 매개변수를 학습합니다. LDM과 유사하게 텍스트 임베딩은 교차 주의 메커니즘을 사용하여 UNet에 주입됩니다. 비디오 LDM[Blattmann et al. 2023]은 LVDM과 동일한 개념을 공유하지만 LDM의 공간적 가중치를 고정한다는 차이점이 있습니다. PlotAnimate-A-Story: 검색 증강 비디오 생성을 통한 스토리텔링 스토리 스크립트 스토리보드 설명 2.2 구조 기반 비디오 생성 T2I의 진화를 반영하여 수많은 연구[Wang et al. 2023a, b; Xing et al. 2023; Yang et al. 2023; Zhang et al. 2023]에서 사전 학습된 텍스트-이미지 또는 텍스트-비디오 모델을 기반으로 하는 조건부 비디오 생성의 기능을 조사했습니다. 예를 들어, Make-Your-Video[Xing et al. 2023]는 텍스트 외에 깊이를 추가 조건으로 사용합니다. Stable Diffusion의 공간적 가중치는 고정되어 있는 반면, 새로 추가된 시간적 가중치는 비디오 데이터에서 학습됩니다. 깊이는 비디오에서 추출되므로 소스 비디오의 모양을 다시 렌더링할 수 있습니다. Follow-Your-Pose[Ma et al. 2023]는 포즈를 조건으로 활용하여 인간과 유사한 캐릭터 비디오 합성 프로세스를 안내합니다. VideoComposer[Wang et al. 2023b]는 Composer [Huang et al. 2023]의 확장으로 RGB 이미지, 스케치, 깊이 등과 같은 여러 유형의 이미지를 조건으로 사용합니다. 이러한 조건은 잠재 공간에서 융합되어 교차 주의를 통해 UNet과 상호 작용합니다. 텍스트 쿼리 동작 구조 비디오 데이터베이스 검색 플롯 i 플롯 n 텍스트 프롬프트 구조 안내 텍스트-비디오 합성2.3 개념 사용자 지정 지정된 개체로 이미지를 생성하는 것을 사용자 지정 또는 개인화라고 합니다. 수많은 작업 [Alaluf et al. 2023; Gal et al. 2022; Kumari et al. 2023; Ruiz et al. 2023; Wei et al. 2023]은 다양한 관점에서 이 작업을 탐구합니다. 안정적 확산에 대한 첫 번째 역전 작업인 텍스트 역전 [Gal et al. 2022]은 주어진 개체의 이미지에 대한 모델을 조정하지 않고 토큰을 최적화합니다. 이와 대조적으로 Dreambooth[Gal et al. 2022]는 토큰을 학습하고 전체 모델 매개변수를 미세 조정합니다.다중 개념[Kumari et al. 2023]은 여러 개념을 반전하는 방법을 제안한 최초의 사례입니다.ELITE[Wei et al. 2023]는 최적화보다는 사용자 지정 텍스트-이미지 생성을 위해 시각적 이미지를 텍스트 임베딩에 매핑하도록 인코더를 학습합니다.NeTI[Alaluf et al. 2023]는 타임스텝과 UNet 계층에 모두 의존하는 새로운 텍스트 컨디셔닝 잠재 공간을 도입합니다.타임스텝과 계층 인덱스를 임베딩 공간에 투영하는 매핑을 학습합니다.방법 우리의 목표는 스토리라인 스크립트를 기반으로 하거나 최소한의 상호 작용 노력으로 고품질 스토리텔링 비디오를 자동으로 생성할 수 있는 프레임워크를 개발하는 것입니다.이를 달성하기 위해 기존 비디오 자산을 검색하여 T2V 생성의 성능을 향상시키는 것을 제안합니다(3.1절 참조). 구체적으로, 우리는 검색된 비디오에서 구조를 추출하는데, 이는 T2V 프로세스에 제공되는 안내 신호 역할을 할 것입니다(3.2절 참조). 또한, 우리는 제안된 TimeInv 접근 방식을 기반으로 비디오 캐릭터 재렌더링을 수행하여 서로 다른 비디오 클립에서 일관된 캐릭터를 합성합니다(3.3절 참조). 다음 섹션에서는 이 기능을 가능하게 하는 주요 기술 설계를 자세히 살펴보겠습니다.그림 2. 검색 증강 비디오 합성 프레임워크의 흐름도.텍스트 스토리 스크립트가 주어지면, 우리는 먼저 주요 플롯을 추출하고 그 설명을 텍스트 쿼리와 프롬프트로 변조합니다.각 플롯은 비디오 검색 시스템과 구조 안내 텍스트-비디오 모델의 두 모듈을 통해 생성된 비디오 클립으로 변환됩니다.3. 검색 증강 텍스트-비디오 생성 그림 2에서 볼 수 있듯이, 우리의 비디오 생성 프레임워크는 텍스트 처리, 비디오 검색, 비디오 합성의 세 가지 절차를 포함합니다.텍스트 처리 단계에서는 스토리보드 분석을 통해 스토리 스크립트에서 주요 플롯을 추출합니다. 문제를 단순화하기 위해, 우리는 샷 전환 없이 개별 플롯을 단일 이벤트로 규제합니다. 예를 들어, &quot;소년이 숲에서 늑대를 만났다&quot;는 단일 플롯인 반면, &quot;소년이 숲에서 늑대를 만났고 총으로 늑대를 죽였다&quot;는 두 개의 플롯으로 분리해야 합니다. 각 플롯에 대해 설명을 추가로 조정하고 장식하여 각각 효과적인 텍스트 쿼리와 텍스트 프롬프트 역할을 할 수 있도록 합니다. 이 단계는 수동으로 완료되거나 GPT-4와 같은 대규모 언어 모델(LLM)의 도움을 받아 완료됩니다[OpenAI 2023]. 그 후, 순차적으로 수행되는 두 개의 모듈을 사용하여 각 플롯을 개별적으로 처리합니다. 텍스트 쿼리가 주어지면, 인터넷에서 수집된 약 10M 개의 오픈 월드 비디오가 있는 데이터베이스와 연결된 기성형 텍스트 기반 비디오 검색 엔진[Bain et al. 2021]을 통해 원하는 시나리오를 보여주는 비디오 후보를 얻을 수 있습니다. 비디오 모양이 플롯과 정확히 일치하지 않을 수 있으므로 깊이 추정 알고리즘을 적용하여 모션 구조만 가져옵니다. 이를 통해 기존 비디오의 사용성이 확장됩니다. 그림 3에 나와 있는 예와 같이 &quot;숲에서 늑대와 놀고 있는 산타클로스&quot; 비디오를 합성하기 위해 우리는• Yingqing HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, Qifeng Chen 텍스트 프롬프트: 강한 모래 폭풍이 불고 있는 화성을 걷는 우주인 CLIP 텍스트 인코더 × (T-1) ZT-Zo ZT 구조 인코더 D 깊이 안내 생성된 비디오 그림 3. 조정 가능한 구조 안내 텍스트-비디오 모델 개요. 우리는 소스 비디오의 깊이 정보를 사용하여 비디오 합성 프로세스를 안내합니다. 이 모델은 두 개의 분기로 구성됩니다. 잠재 공간의 비디오 확산 모델인 일반 텍스트-비디오 합성 분기와 구조 제어를 인코딩하고 적용하기 위한 측면 분기입니다. 제어 메커니즘은 요소별 피처 추가입니다. 특히 깊이 제어는 조정 가능하며, 이 속성은 3.3절에서 설명할 추가적인 캐릭터 렌더링에 필수적입니다.비디오 데이터베이스에서 꽤 흔한 &quot;공원에서 개와 놀고 있는 남자&quot; 비디오의 동작 구조.동작 구조를 가이드로 활용하여 텍스트 프롬프트를 통해 플롯에 맞춰진 비디오를 합성할 수 있습니다.다음으로 구조 기반 T2V 모델을 자세히 설명합니다.3.2 구조 기반 텍스트-비디오 합성 예비.잡음 제거 확산 확률 모델(DDPM)[Ho et al. 2020]은 약칭 확산 모델(DM)이라고도 하며, 표준 가우시안 분포에서 대상 분포로의 매핑을 구축하여 경험적 데이터 분포 Pdata(x)를 모델링하는 방법을 학습합니다. 특히, 순방향 프로세스는 다음과 같이 표기되는 고정 확산 프로세스로 공식화됩니다.q(xt|xt−1) = N(xt, √1 − ßtxt−1, ßtI). (1) 이것은 분산 일정 ẞt = (0, 1)을 사용하여 각 샘플 xo에 점차적으로 노이즈를 추가하는 마르코프 체인입니다.여기서 t = {1, ..., T}이고 T는 확산 체인의 전체 단계입니다.역방향 프로세스는 현재 시간 단계 t와 함께 확산된 xt를 입력으로 사용하는 노이즈 제거 모델 fo를 통해 실현되고 노이즈 제거 점수 매칭 목표를 최소화하여 최적화됩니다.L(0) = Exo Pdatast|| Et - fp (xt; c, t) || 2, (2) 여기서 c는 선택적 조건 정보(예: 텍스트 임베딩)이고 감독 &amp;는 순방향 확산 프로세스에 사용되는 무작위 샘플링 노이즈입니다.X0 → xt. DM에서 확장된 LDM은 잠재 공간에서 확산 및 잡음 제거 프로세스를 공식화합니다. 이는 일반적으로 인코더 &amp; 와 디코더 D로 구성된 변형 자동 인코더(VAE) [Kingma 및 Welling 2014]를 통해 분리 및 학습된 압축 표현 공간입니다. 그림 3에 나와 있는 개요와 같이 제어 가능한 비디오 합성을 학습하기 위해 조건부 LDM을 사용합니다. 저희의 접근 방식에서 비디오는 프레임 단위로 잠재 공간 =으로(또는 잠재 공간에서) 변환(또는 재구성)됩니다. 구체적으로, 저희 인코더 &amp; 는 다운샘플링 인수 8로 입력 RGB 이미지 x = R³×HXW를 다운샘플링하고 잠재 표현 z = R 4×&#39;×W&#39;를 출력합니다. 여기서 H&#39; = H/8, W&#39; W/8을 사용하면 잡음 제거 모델이 훨씬 더 낮은 차원의 데이터에서 작동할 수 있으므로 실행 시간과 메모리 효율성이 향상됩니다. 잠재 확산 모델은 동작 구조와 텍스트 프롬프트에 따라 조건화됩니다. L 프레임의 비디오 클립 XЄ RLX3XHXW가 주어지면, z = 8(x)를 통한 잠재 표현 z를 얻습니다.여기서 z = RL×4×H&#39;×W&#39;입니다.고정 순방향 프로세스에서 z = zo는 T 단계에 의해 순수 노이즈 텐서 zÃ로 확산됩니다.역 프로세스(즉, 노이즈 제거 프로세스)에서 노이즈 제거 모델은 임베디드 텍스트 프롬프트와 프레임별 깊이 맵을 조건으로 사용하여 현재 노이즈 데이터 Zt에서 이전 단계 데이터 Zt-1을 예측하고, 깨끗한 데이터 z는 무작위 노이즈 ZT에서 반복적으로 샘플링될 수 있습니다.특히, 노이즈 제거 모델은 [He et al. 2022]의 아키텍처를 채택한 3D U-Net입니다.입력 텍스트 프롬프트에서 시각적으로 정렬된 토큰을 추출하기 위해 텍스트 인코더로 CLIP [Radford et al. 2021]을 채택합니다.깊이 추정기의 경우 Midas 깊이 추정 모델 [Ranft] et al.을 선택합니다. 2022] 다양한 비디오에서의 견고성으로 인해. 깊이 맵은 CNN 기반 구조 인코더를 통해 인코딩되고 다중 스케일 피처는 구조적 변조를 위해 노이즈 제거기 U-Net의 피처 맵에 추가됩니다. 구조 제어와 달리 텍스트 프롬프트를 통한 의미 제어는 교차 주의 모듈을 통해 백본 피처에 영향을 미칩니다[Rombach et al. 2022]. 3.3 비디오 캐릭터 렌더링 위에서 언급한 비디오 합성 프레임워크는 비디오에 고품질의 다양한 동작을 제공합니다. 그러나 텍스트 프롬프트에 의해 제어되는 생성된 캐릭터 모양은 비디오 클립마다 다릅니다. 이러한 과제를 극복하기 위해 다음과 같은 목표로 이 문제를 공식화합니다. 사전 학습된 비디오 생성 모델과 사용자가 지정한 캐릭터가 주어지면, 우리의 목표는 서로 다른 비디오 클립에서 일관된 캐릭터를 생성하는 것입니다. 이 작업을 비디오 캐릭터 렌더링이라고 합니다. 이를 위해 이미지 확산 모델의 개인화 접근 방식에 대한 기존 문헌을 조사했습니다. 그러나 이러한 방법을 비디오 개인화에 직접 적용하는 데에는 몇 가지 과제가 있습니다. 1) 비디오 모델을 개인화하기 위해 이미지 데이터를 활용하는 방법은 무엇입니까? 비디오 개인화를 위한 간단한 접근 방식 중 하나는 특정 캐릭터를 묘사하는 비디오 데이터를 활용하는 것입니다. 그러나 일관된 캐릭터의 비디오 데이터는 이미지보다 수집하기가 훨씬 어렵습니다. 2) 개념 구성성과 캐릭터 충실도 간의 균형을 어떻게 조정할 것인가? 이러한 과제는 이미지 개인화 문헌에서도 나타납니다. 이 섹션에서는 예비적 접근 방식과 방법을 자세히 설명합니다. 예비: 텍스트 역전. 텍스트 역전은 새 토큰 S*에 새 개념을 표현하고 CLIP 텍스트 인코더 co에서 해당 새 토큰 임베딩 벡터 U*를 학습하는 것을 목표로 하는 이미지 개인화 접근 방식입니다. v*는 특정 개념을 묘사하는 3~10개의 이미지로 직접 최적화됩니다. 학습 목표는 원래의 확산 손실 모델과 동일하며 다음과 같이 정의할 수 있습니다. v₁ = arg min B2-8(x),y,eN(0,1),t ||| e- fp (Z₁, t, c)|||], (3) v* ย E~ &quot;V* 고양이 사진&quot; xT CLIP 텍스트 V* 고양이 사진 V* 고양이 사진 V* 고양이 사진 인코더 Animate-A-Story: 검색 증강 비디오 생성을 통한 스토리텔링 검색된 비디오 깊이 T=T = 0.T = 0.T = 0.U-Net Wa Wk W₁ W₁e Wk W₁₂ WW√ D Wy Ours Trainable Frozen Tokenizer DreamBooth Textual Inversion Custom Diffusion 학습을 위한 문자 이미지 텍스트 변환기 ☐ 토큰 임베딩 그림 4. 개인화를 위한 다양한 접근 방식의 개념도. 생성된 문자의 불일치 문제를 극복하기 위해 기존 개인화 접근 방식을 연구하고 대상 문자의 모양을 다시 렌더링하는 새로운 방법을 제안합니다. 우리는 CLIP 텍스트 인코더와 노이즈 제거기 U-Net의 모든 매개변수를 동결 상태로 유지하고 대상 문자의 의미적 특징을 나타내기 위해 타임스텝 종속 토큰 임베딩을 학습합니다. 또한, 어텐션 모듈에서 q, k, v의 투영 계층에 새로운 분기를 삽입하고 사전 훈련된 가중치를 변조하여 문자를 더 잘 나타냅니다. 훈련 후, 새로운 토큰을 다른 단어 토큰과 결합하여 문장을 형성할 수 있습니다. 그런 다음 이 토큰 시퀀스를 텍스트 인코더를 통해 전달하여 원하는 개념을 생성하기 위한 이미지 생성 제어를 용이하게 하는 조건부 텍스트 토큰 임베딩을 얻을 수 있습니다. 타임스텝 변수 텍스트 역전(TimeInv). 그러나 단일 토큰 임베딩 벡터를 최적화하는 것은 제한된 최적화된 매개변수 크기 때문에 표현 용량이 제한적입니다. 또한 풍부한 시각적 특징과 세부 정보가 있는 개념을 설명하는 데 하나의 단어를 사용하는 것은 매우 어렵고 불충분합니다. 따라서 개념의 충실도와 관련하여 만족스럽지 못한 결과가 발생하는 경향이 있습니다. 이 문제를 해결하기 위해 타임스텝 변수 텍스트 역전(TimeInv)을 제안합니다. TimeInv는 추론 단계에서 서로 다른 타임스텝이 서로 다른 이미지 속성의 렌더링을 제어한다는 관찰에 기반합니다.예를 들어, 노이즈 제거 프로세스의 이전 타임스텝은 글로벌 레이아웃과 객체 모양을 제어하고, 노이즈 제거 프로세스의 이후 타임스텝은 텍스처와 색상과 같은 저수준 세부 정보를 제어합니다[Voynov et al. 2022].대상 개념을 묘사하는 토큰을 더 잘 학습하기 위해 모든 타임스텝에서 제어 토큰 임베딩을 저장하기 위해 타임스텝 종속 토큰 임베딩 테이블을 설계합니다.학습하는 동안 모든 ddpm 타임스텝 중에서 임의의 타임스텝을 샘플링하여 타임스텝 임베딩 매핑 테이블 V = RTxd를 직접 최적화합니다.여기서 T는 확산 프로세스의 총 타임스텝이고 d는 토큰 임베딩의 차원입니다.학습 목표는 다음과 같이 정의할 수 있습니다.V := arg min Ez~ɛ(x),y,€ ~ N(0,1),t |||€ — fø (Zt, t, co (y, t))|||]. (4) 01:T 추론 중에 토큰 임베딩은 현재 노이즈 제거 타임스텝을 기반으로 검색된 다음 v+ = Vt로 정의된 토큰 임베딩 시퀀스에 합성됩니다. 이미지 데이터를 사용한 비디오 사용자 지정. 비디오 개인화에 대한 또 다른 과제는 이미지 데이터를 활용하여 최적화하는 방법입니다. 그림 5. 7을 조정하는 효과. 작은 7은 깊이 제어를 완화하여 모양을 캐릭터 모양으로 렌더링하는 동시에 깊이에서 거친 레이아웃과 동작 제어를 유지할 수 있습니다. 이 기술은 다양한 동작(예: 요가 수행)을 하는 실제 테디 베어 비디오가 부족하기 때문에 수집하기 매우 어려운 테디 베어의 동작 비디오를 검색할 필요 없이 테디 베어 비디오를 생성할 수 있습니다. 비디오 생성 모델. 이미지를 비디오에 직접 반복한 다음 토큰을 최적화하면 동작 누락 문제가 발생합니다. 정적 동작은 대상 개념과 결합되는 경향이 있고 다양한 동작으로 개념을 생성하기 어렵기 때문입니다. 이전에 도입한 구조 안내 모듈 덕분에 이제 정적 구조 안내를 사용하여 개념을 학습할 수 있습니다. 구체적으로, 우리는 L 프레임의 가상 비디오에 개념 이미지를 반복하고 프레임별 깊이 신호를 추출하여 정적 개념 비디오를 합성하기 위한 비디오 생성 모델을 제어합니다. 추론하는 동안, 그것은 다양한 동작으로 개념을 생성하기 위해 다른 동작 안내와 대상 개념을 쉽게 결합할 수 있습니다. 낮은 순위 가중치 변조. 텍스트 반전만을 사용하는 것은 여전히 주어진 캐릭터의 외관 세부 사항을 포착하기 어렵습니다. 모델 매개변수를 직접 최적화하는 이전 접근 방식 대신, 우리는 사전 훈련된 모델에서 개념 생성 및 구성 능력을 손상시키지 않고 주의 모듈의 사전 훈련된 선형 계층에 추가적인 낮은 순위 [Hu et al. 2021] 행렬을 추가합니다. 낮은 순위 행렬은 두 개의 훈련 가능한 선형 계층으로 구성됩니다. 우리는 이러한 행렬을 우리 모델의 교차 및 공간 자기 주의 모듈에 삽입합니다. 구조 안내와 개념 생성 간의 충돌. 우리의 맞춤형 디자인으로 개념을 비디오 생성에 성공적으로 주입할 수 있지만, 여전히 심각한 개념 안내 충돌 문제가 있습니다. 구체적으로, 개인화된 테디베어를 학습한 다음 소스 비디오를 사용하여 동작 안내를 제공하려는 경우 테디베어의 움직이는 비디오를 수집하는 것은 어렵고 시간이 많이 걸립니다. 게다가 깊이가 제공하는 모양은 생성된 모양이 id 모양을 따라야 하기 때문에 id 유사성에 심각한 영향을 미칩니다. 따라서 깊이 안내 모델이 깊이 제어를 완화할 수 있는 기능이 있어야 합니다. 이를 해결하기 위해 샘플링 중에 타임스텝 클램핑을 통해 깊이 안내 모듈을 조정할 수 있도록 만들었습니다. 구체적으로, 우리는 t = T, ..., 7의 시간 단계에 대해서만 피처에 깊이 가이드를 적용하고 시간 단계 7 이후에 깊이 피처를 삭제합니다. 또한 초기 시도에서 추론 중 피처 재조정을 실험했는데, 이는 시간 단계 클램핑보다 더 나쁜 깊이 조정을 보여줍니다. 실험 스토리보드 분할은 수동으로 수행되거나 LLM의 도움을 받으므로 주로 검색 증강 비디오 생성을 평가합니다. 가이드가 없는 텍스트 및 문자 렌더링이 없는 TimeInv 문자 렌더링이 없는 프롬프트 검색된 비디오 6 • Yingqing HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, Qifeng Chen &quot;A<new1> 테디베어가 큰 눈보라가 내리는 겨울에 아름다운 정원을 걷고 있습니다.&quot;<new1> 테디베어가 자전거를 타고 눈 덮인 숲을 지나갑니다.&quot; &quot;A<new1> 테디베어가 꽃과 창문이 있는 방에서 와인 한 잔을 마시고 있습니다.&quot; 티터스톡셔터 엑소터스톡 스톡 스톡 그림 6. 구조 안내, 캐릭터 렌더링, 타임라인을 포함한 파이프라인의 핵심 구성 요소의 분산 결과. 이 논문의 주요 기술 혁신입니다. 구체적으로, 먼저 4.2절에서 스토리텔링 비디오 합성을 위한 전체 파이프라인의 효과를 검증한 다음, 4.3절과 4.4절에서 각각 비디오 합성 품질과 콘셉트 사용자 정의 성능을 평가합니다. 4. 구현 세부 정보 비디오 생성 모델은 세 단계로 학습됩니다. 먼저 WebVid-10M [Bain et al. 2021] 데이터 세트에서 기본 텍스트-비디오 모델을 학습하고, 공개적으로 사용 가능한 사전 학습된 안정 확산 이미지 LDM [Rombach et al. 2022]으로 공간 매개변수를 초기화합니다. WebVid-10M은 10.7M 비디오 캡션 쌍은 총 52,000시간의 비디오 시간입니다. 학습을 위해 비디오 크기를 256×256 해상도로 조정하고 프레임 스트라이드 8로 16개 프레임을 샘플링합니다. 두 번째로, 모델에 깊이 안내를 제공하기 위해 사전 학습된 기본 모델을 동결한 동일한 학습 세트에서 구조 인코더를 학습합니다. 마지막으로, 개념 사용자 지정을 위해 깊이 안내 텍스트-비디오 모델을 특정 텍스처 토큰 최적화와 함께 조정합니다. 이는 작업에 따라 달라지는 작은 데이터 세트에서 수행됩니다. 모델은 고정된 해상도로 학습되었지만 추론 단계에서 다른 해상도도 잘 지원한다는 것을 알게 되었습니다. Animate-A-Story: 검색 증강 비디오 생성을 통한 스토리텔링 검색된 캐릭터 이미지 비디오 DreamBooth-비디오 텍스트 반전-비디오 사용자 지정 확산-비디오 및 프롬프트 A<new1> 테디베어는 꽃과 창문이 있는 방에서 와인 한 잔을 마시고 있습니다. 슬픈<new 1> 테디베어가 침실에서 와인 한 잔을 마시고 있습니다<new1> 고양이는 궁전에 있는 큰 테르스토 의자에 누워 있습니다.<new 1> 고양이가 뉴욕 거리를 걷는 모습 우리의 그림 7. 이전 개인화 접근 방식과의 양적 비교. 네 가지 다른 접근 방식을 사용하여 두 캐릭터의 결과를 보여줍니다. 각 접근 방식에서 하나의 비디오 클립을 보여주고 동일한 랜덤 시드를 고정합니다. 각 비디오 클립은 프레임 샘플링 스트라이드가 8인 두 프레임을 보여줍니다. 독자는 더 잘 보려면 확대할 수 있습니다. 4. 스토리텔링 비디오 합성에 대한 평가 스토리텔링 비디오 합성 작업을 다루는 최초의 작업이기 때문에 비교할 기존 기준이 없습니다. 따라서 그림 6과 같이 디자인 선택의 효과를 정당화하기 위해 절제 실험을 수행합니다. TimeInv를 사용하지 않으면 대상 캐릭터의 생성 품질이 저하되는 것을 볼 수 있습니다. 또한 TimeInv 없이는 개념 구성 능력이 손상됩니다. 예를 들어, 모델은 세 번째 클립에서 꽃과 창문을 생성하지 못합니다. 개인화 프로세스가 없으면 다양한 클립에서 캐릭터의 일관성을 유지하기 어렵습니다. 비디오 검색을 포함하지 않으면 캐릭터 위치와 전체 레이아웃을 제어하기가 어려워집니다. 또한, 표 1에 제시된 정량적 결과에서 알 수 있듯이 생성 성능은 검색 증강 생성에 비해 떨어집니다. 비디오 결과는 보충 결과에서 확인할 수 있습니다. 4.3 텍스트-비디오 합성 기준선 및 평가 지표에 대한 평가. 우리는 깊이 제어를 주입하기 위해 ControlNet과 결합된 T2V-Zero [Khachatryan et al. 2023] 및 두 가지 오픈 소싱 text2video 생성 모델인 ModelScope [damo vilab 2023] 및 LVDM [He et al. 2022]을 포함하여 기존의 사용 가능한 (깊이 안내) 텍스트-비디오 모델과 우리의 접근 방식을 비교합니다. 우리는 FVD 및 KVD를 통해 비디오 생성 성능을 측정합니다. 사용된 실제 비디오 데이터 세트는 UCF-101 [Soomro et al. 2012]이며 2048개의 실제 비디오와 해당 클래스 이름을 샘플링합니다. 우리는 클래스 이름을 텍스트 프롬프트로 사용한 다음 각 프롬프트에 대해 하나의 가짜 비디오를 샘플링합니다. 세 가지 모델 모두 DDIM 50단계 방법 조건 FVD↓ KVD↓ T2V-Zero + ControlNet [Khachatryan et al. 2023] 텍스트 + 깊이 4685.27 168.ModelScope [damo vilab 2023] LVDM [He et al. 2022] 텍스트 텍스트 2616.06 2052.917.63 116.우리의 텍스트 깊이 516.15 47.표 1. 제로 샷 설정에서 UCF-101에서 오픈 소싱 비디오 생성 모델과의 정량적 비교.및 기본 분류기 없는 안내 척도(ModelScope 및 T2V-Zero의 경우 9, LVDM의 경우 10)를 사용합니다.결과.표 1에서는 비디오 합성 성능의 정량적 결과를 보여줍니다.볼 수 있듯이 깊이 구조 안내가 장착된 텍스트-비디오 합성은 순수 텍스트의 비디오 합성보다 훨씬 더 나은 성능을 달성합니다. 또한, 우리의 접근 방식은 기존의 깊이 기반 텍스트-비디오 생성 방법인 ControlNet과 결합된 T2V-Zero보다 뛰어나서 우리 모델의 우수성을 입증합니다.4. 개인화 기준선 및 평가 지표에 대한 평가.개인화 모듈의 효과를 평가하기 위해, 우리는 우리의 접근 방식을 이전의 세 가지 기준선 접근 방식인 Dreambooth[Ruiz et al. 2023], Textual inversion[Gal et al. 2022], Custom Diffusion[Kumari et al. 2023]과 비교합니다.양적 평가를 위해, 우리는 생성된 비디오와 텍스트 간의 의미적 정렬과 생성된 비디오와 사용자가 제공한 개념 이미지 간의 개념 충실도를 측정합니다.각 접근 방식에 대해, 우리는 프롬프트-비디오 쌍을 사용하여 10개의 무작위 비디오를 샘플링하여 총 200개의 생성된 비디오를 구성합니다. • Yingqing HE, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, Qifeng Chen 프롬프트는 다른 객체와 함께 다양한 배경과 다양한 구성을 갖습니다. 그런 다음 의미적 정렬은 각 프레임의 CLIP 텍스트 임베딩과 CLIP 이미지 임베딩 간의 코사인 유사도에 의해 계산된 다음 모든 프레임의 점수를 평균하여 텍스트와 생성된 비디오 클립 간의 정렬 점수를 얻습니다. 개념 충실도는 생성된 비디오 클립과 해당 개념 이미지 간의 평균 정렬에 의해 측정됩니다. 각 쌍에 대해 클립의 각 프레임과 대상 개념 이미지 간의 코사인 유사도를 계산한 다음 모든 프레임의 점수를 평균합니다. 서로 다른 접근 방식은 공정한 비교를 위해 동일한 난수 시드 세트를 공유합니다. 구현 세부 정보. Custom Diffusion의 캐릭터 이미지를 채택하고 각각 7개와 5개의 이미지가 포함된 테디베어와 고양이로 실험합니다. 우리는 다음을 사용합니다.<new1> 입력 텍스트 프롬프트에서 주어진 문자를 나타내는 의사 단어로 사용합니다. 테디 베어의 캐릭터의 경우, 접근 방식과 기준 접근 방식을 1,000단계로 훈련합니다. 접근 방식과 텍스트 역전에 사용한 학습률은 1.0e-04인 반면, dreambooth와 custom diffusion의 경우 사전 훈련된 모델 매개변수를 직접 최적화하기 때문에 1.0e-5의 학습률을 사용합니다. 이러한 모든 접근 방식에서 과적합 문제를 방지하기 위해 정규화 데이터 세트와 동일한 범주의 실제 비디오를 사용합니다. 정규화 데이터는 WebVid 데이터 세트에서 검색합니다. 각 캐릭터에 대해 200개의 실제 비디오를 검색했습니다. 또한 모든 접근 방식에서 대상 캐릭터 이미지의 데이터 증가를 사용하여 Custom Diffusion에 따른 훈련 데이터 세트의 다양성을 풍부하게 합니다. 추론하는 동안 모든 접근 방식에 대해 50개의 샘플링 타임스텝과 15개의 분류자 없는 안내를 사용하여 DDIM 샘플링을 사용합니다. 결과. 그림 7에서 비디오 생성 설정에서 기준 개인화 접근 방식과 비교한 정성적 결과를 제시합니다. 보시다시피 DreamBooth는 사전 훈련된 매개변수 전체를 업데이트하므로 과적합 문제가 발생하여 생성 다양성이 방해를 받습니다(예: 세 번째와 네 번째 행의 배경은 훈련 문자 이미지와 매우 유사함). 텍스트 역전은 대상 문자의 모양을 나타내기 위해 단일 토큰 임베딩만 최적화하므로 문자 세부 정보를 캡처하기 어렵고 문자 충실도가 낮습니다. 사용자 정의 확산은 텍스트 역전과 함께 사전 훈련된 네트워크 내의 어텐션 모듈에서 k와 v를 계산하기 위한 선형 계층을 업데이트합니다. 문자 충실도는 향상되지만 생성된 문자 모양에 종종 아티팩트가 나타납니다. 표 2에서 비교의 정량적 결과를 제공합니다. 제안된 TimeInv는 텍스트 역전을 대체할 수 있으며 사용자 정의 확산과 결합하여 더 나은 의미적 정렬을 달성할 수 있습니다. 비디오 생성 설정 외에도 일반적인 이미지 개인화 설정에서 제안된 타임스텝 변수 텍스트 역전을 평가합니다. 사용자 정의 확산 코드베이스를 사용하고 TimeInv와 텍스트 역전의 성능을 비교합니다. 결과는 그림 8에 나와 있습니다.TimeInv를 Custom Diffusion과 결합하면 배경 다양성이 더 좋고, 개념 구성도 더 좋습니다(예: 공이 Custom Diffusion + Textual Inversion보다 더 자주 나타남).모델 매개변수를 업데이트하지 않고 TimeInv와 Textual Inversion을 직접 비교하면 TimeInv의 문자 유사도(즉, 고양이의 고유한 질감)가 더 좋습니다.방법 DreamBooth [Ruiz et al. 2023]-비디오 Textual Inversion [Gal et al. 2022]-비디오 Custom diffusion [Kumari et al. 2023]-비디오 Ours Teddy Bear Cat Sem.ID Sem. ID 0.272 0.778 0.255 0.0.263 0.666 0.252 0.0.275 0.849 0.256 0.0.295 0.853 0.257 0.표 2. 이전 개인화 접근 방식과의 양적 비교. 캐릭터 이미지 사용자 정의 확산 + 텍스트 반전 텍스트 반전 프롬프트 &quot;<new1> 공 가지고 노는 고양이&quot; 사용자 정의 확산 + TimeInv(우리) TimeInv(우리) 그림 8. 사전 훈련된 안정적 확산을 사용하여 이미지 개인화에 대한 제안된 Timestep-variable Textual Inversion(Timelny)의 효과. 동일한 열의 결과는 동일한 훈련 단계와 랜덤 시드로 비교됩니다. 이는 우리의 접근 방식이 이미지 및 비디오 생성 작업 모두에서 개인화를 위한 일반적인 접근 방식으로 사용될 수 있음을 보여줍니다. 5
--- CONCLUSION ---
스토리텔링 비디오 합성을 위한 새로운 검색 기반 파이프라인을 소개합니다. 이 시스템은 사용자 지정되고 캐릭터와 일관된 스토리텔링 비디오를 제작하기 위해 더 나은 비디오 합성 품질, 레이아웃, 동작 제어 및 캐릭터 개인화를 가능하게 합니다. 우리는 기본 텍스트-비디오 모델에 구조 가이드 비디오 생성 모듈을 통합하고 캐릭터 제어 성능을 높이기 위한 새로운 개인화 방법을 고안합니다. 또한 조정 가능한 깊이 제어 모듈로 캐릭터 깊이 충돌 문제를 해결합니다. 우리는 어려운 스토리라인 합성 작업에 대한 시스템의 능력을 보여주었지만 여러 측면에서 향후 개선의 여지가 많습니다. 예를 들어 미세 조정이 없는 일반적인 캐릭터 제어 메커니즘과 캐릭터 제어와 구조 제어 간의 더 나은 협력 전략이 잠재적인 방향이 될 수 있습니다. 참고문헌 Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. 2023. 텍스트-이미지 개인화를 위한 신경 공간-시간 표현. arXiv 사전 인쇄본 arXiv:2305.15391 (2023). Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. 2021. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. IEEE International Conference on Computer Vision(ICCV)에서. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. 2023. Align your latents: High-resolution video synthesis with latent diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록에서. 22563-22575. damo vilab. 2023. modelscope-text-to-video-synthesis. https://huggingface.co/damovilab/modelscope-damo-text-to-video-synthesis Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. 고해상도 이미지 합성을 위한 트랜스포머 길들이기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 12873-12883. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, Daniel Cohen-Or. 2022. 이미지는 단어 하나만큼 가치가 있다: 텍스트 반전을 사용하여 텍스트-이미지 생성 개인화. arXiv 사전 인쇄본 arXiv:2208.01618(2022). Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, Devi Parikh. 2022. 시간에 독립적인 vqgan을 사용한 긴 비디오 생성 Animate-A-Story: 검색 증강 비디오 생성을 사용한 스토리텔링 및 시간에 민감한 트랜스포머. Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII. Springer, 102-118. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. 2022. Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths. arXiv 사전 인쇄본 arXiv:2211.13221 (2022). Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. 2022a. Imagen video: 확산 모델을 사용한 고화질 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02303(2022). Jonathan Ho, Ajay Jain, Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33(2020), 6840-6851. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J Fleet. 2022b. Video diffusion models. arXiv 사전 인쇄본 arXiv:2204.(2022). Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang. 2022. Cogvideo: Transformers를 통한 텍스트-비디오 생성을 위한 대규모 사전 학습. arXiv 사전 인쇄본 arXiv:2205.15868(2022). Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen. 2021. Lora: 대규모 언어 모델의 저순위 적응. arXiv 사전 인쇄본 arXiv:2106.09685(2021). Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, Jingren Zhou. 2023. Composer: 구성 가능한 조건을 갖춘 창의적이고 제어 가능한 이미지 합성. arXiv 사전 인쇄본 arXiv:2302.09778(2023). Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi. 2023. Text2video-zero: 텍스트-이미지 확산 모델은 제로샷 비디오 생성기입니다. arXiv 사전 인쇄본 arXiv:2303.13439(2023). Diederik P. Kingma와 Max Welling. 2014. 자동 인코딩 변분 베이즈. 국제 학습 표현 컨퍼런스(ICLR)에서. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu. 2023. 텍스트-이미지 확산의 다중 개념 사용자 지정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록에서. 1931–1941. Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jinren Zhou, Tieniu Tan. 2023. 고품질 비디오 생성을 위한 분해 확산 모델. arXiv 사전 인쇄본 arXiv:2303.08320(2023). Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. 2023. Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos. arXiv 사전 인쇄본 arXiv:2304.01186 (2023). Kangfu Mei and Vishal M Patel. 2022. VIDM: Video Implicit Diffusion Models. arXiv 사전 인쇄본 arXiv:2212.00235 (2022). OpenAI. 2023. GPT-4 기술 보고서. arXiv (2023). arXiv:2303.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. 2021. 자연어 감독을 통한 전이 가능한 시각적 모델 학습. 국제 기계 학습 컨퍼런스(ICML)의 회의록에서. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. 2022. 클립 잠재성을 사용한 계층적 텍스트 조건부 이미지 생성. arXiv 사전 인쇄본 arXiv:2204.06125(2022). René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun. 2022. 견고한 단안경 깊이 추정을 향하여: ZeroShot 교차 데이터세트 전송을 위한 데이터세트 혼합. IEEE Trans. Pattern Anal. Mach. Intell. 44, 3 (2022), 1623-1637. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer. 2022. 잠재 확산 모델을 사용한 고해상도 이미지 합성. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 10684-10695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman. 2023. Dreambooth: 주제 중심 생성을 위한 텍스트-이미지 확산 모델 미세 조정. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 22500-22510. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. 심층적 언어 이해를 갖춘 사실적인 텍스트-이미지 확산 모델. 신경 정보 처리 시스템의 발전 35(2022), 36479-36494. Masaki Saito, Eiichi Matsumoto, Shunta Saito. 2017. 특이값 클리핑을 사용한 시간적 생성적 적대적 네트워크. IEEE 국제 컴퓨터 비전 컨퍼런스 회의록. 2830-2839. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video: 텍스트-비디오 데이터 없이 텍스트-비디오 생성. arXiv 사전 인쇄본 arXiv:2209.14792(2022). Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. 2022. Stylegan-v: stylegan2의 가격, 이미지 품질 및 특전을 갖춘 연속 비디오 생성기. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집. 3626-3636. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. 2012. UCF101: 야생 비디오에서 추출한 101개의 인간 행동 클래스 데이터 세트. arXiv 사전 인쇄본 arXiv:1212.(2012). Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz. 2018. Mocogan: 비디오 생성을 위한 동작 및 콘텐츠 분해. IEEE 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록. 1526-1535. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan. 2022. Phenaki: 오픈 도메인 텍스트 설명에서 가변 길이 비디오 생성. arXiv 사전 인쇄본 arXiv:2210.02399(2022). Carl Vondrick, Hamed Pirsiavash, Antonio Torralba. 2016. 장면 역학을 사용하여 비디오 생성. 신경 정보 처리 시스템의 발전 29(2016). 안드레이 보이노프(Andrey Voynov), 크피르 애버만(Kfir Aberman), 다니엘 코헨-오르(Daniel Cohen-Or). 2022. 스케치 기반 텍스트-이미지 확산 모델. arXiv 사전 인쇄 arXiv:2211.13752 (2022). Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu 및 Hongsheng Li. 2023a. Gen-L-Video: 시간적 CoDenoising을 통한 다중 텍스트에서 긴 비디오 생성. arXiv:2305.18264 [cs.CV] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao 및 Jingren Zhou. 2023b. VideoComposer: 모션 제어 기능을 갖춘 컴포지션 비디오 합성. arXiv 사전 인쇄 arXiv:2306.02018(2023). Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang 및 Wangmeng Zuo. 2023. 엘리트: 맞춤형 텍스트-이미지 생성을 위해 시각적 개념을 텍스트 임베딩으로 인코딩합니다. arXiv 사전 인쇄 arXiv:2302.13848 (2023). Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro 및 Nan Duan. 2021. Godiva: 자연 설명을 바탕으로 오픈 도메인 비디오 생성. arXiv 사전 인쇄 arXiv:2104.14806(2021). Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang 등 2023. Make-YourVideo: 텍스트 및 구조적 지침을 사용한 맞춤형 비디오 생성. arXiv 사전 인쇄 arXiv:2306.00943 (2023). Wilson Yan, Yunzhi Zhang, Pieter Abbeel 및 Aravind Srinivas. 2021. Videogpt: vq-vae 및 변환기를 사용한 비디오 생성. arXiv 사전 인쇄 arXiv:2104.10157 (2021). Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy. 2023. 비디오 재렌더링: 제로샷 텍스트 안내 비디오-비디오 번역. arXiv 사전 인쇄본 arXiv:2306.(2023). Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. 2023a. Magvit: Masked generative video transformer. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록에서. 10459-10469. Sihyun Yu, Kihyuk Sohn, Subin Kim, Jinwoo Shin. 2023b. 투사된 잠재 공간에서의 비디오 확률적 확산 모델. IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록에서. 18456-18466. Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo 및 Qi Tian. 2023. ControlVideo: 훈련이 필요 없는 제어 가능한 텍스트-비디오 생성. arXiv 사전 인쇄 arXiv:2305.13077(2023). Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu 및 Jiashi Feng. 2022. Magicvideo: 잠재 확산 모델을 사용한 효율적인 비디오 생성. arXiv 사전 인쇄 arXiv:2211.11018 (2022).
