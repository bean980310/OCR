--- ABSTRACT ---
3D 시각적 접지는 가정용 로봇에 중요한 기술로, 로봇이 주변 환경에 따라 탐색하고, 물체를 조작하고, 질문에 답할 수 있도록 해줍니다. 기존 접근 방식은 종종 광범위한 레이블이 지정된 데이터에 의존하거나 복잡한 언어 쿼리를 처리하는 데 한계가 있는 반면, 저희는 새로운 제로샷, 오픈 어휘, 대규모 언어 모델(LLM) 기반 3D 시각적 접지 파이프라인인 LLM-Grounder를 제안합니다. LLM-Grounder는 LLM을 활용하여 복잡한 자연어 쿼리를 의미적 구성 요소로 분해하고 OpenScene 또는 LERF와 같은 시각적 접지 도구를 사용하여 3D 장면에서 객체를 식별합니다. 그런 다음 LLM은 제안된 객체 간의 공간적 및 상식적 관계를 평가하여 최종 접지 결정을 내립니다. 저희의 방법은 레이블이 지정된 학습 데이터가 필요하지 않으며 새로운 3D 장면과 임의의 텍스트 쿼리로 일반화할 수 있습니다. 저희는 ScanRefer 벤치마크에서 LLM-Grounder를 평가하고 최첨단 zero*Equal 기여를 보여줍니다. ¹컴퓨터 과학 및 공학, 미시간 대학교, 미시간주 앤아버, 미국, 48109. 연락처: Jianing Yang jianingy@umich.edu. 2Nikhil Madaan은 독립 연구원입니다. 3뉴욕 대학교. 이 연구는 NSF IIS-1949634, NSF SES2128623 및 Microsoft Academic Program Computing Credit의 후원을 받았습니다. 프로젝트 웹사이트: https://chat-with-nerf.github.io/ 샷 그라운딩 정확도. 저희의 연구 결과에 따르면 LLM은 그라운딩 기능을 크게 개선하며, 특히 복잡한 언어 쿼리의 경우 LLM-Grounder가 로봇 공학의 3D 비전 언어 작업에 효과적인 접근 방식입니다. I.
--- INTRODUCTION ---
3D 장면에 배치되어 &quot;식탁과 창문 사이의 의자&quot;(그림 1)를 찾으라는 요청을 받았다고 상상해 보세요. 사람이 답을 알아내는 것은 쉽습니다. 이러한 기술을 3D 시각적 접지라고 하며, 일반적으로 물건을 찾는 것에서 도구를 조작하는 것까지 일상적인 작업에 이 기술을 사용합니다. 이러한 능력을 익히는 것은 복잡한 탐색(어디로 가야 할지 아는 것), 조작(무엇/어디를 잡을지) 및 질의 응답에 필요한 기본 기술이므로 사람을 돕는 가정용 로봇을 만드는 데 중요합니다. 로봇에 이러한 능력을 부여하기 위해 연구자들은 여러 가지 접근 방식을 개발했습니다. 한 가지 방향은 3D 및 텍스트 엔드투엔드 신경 구조를 훈련하여 물체 주위에 경계 상자를 제안하고 텍스트-경계 상자 매칭을 공동으로 모델링하는 것입니다[2–11]. 그러나 이러한 모델은 일반적으로 훈련 데이터를 위해 많은 양의 3D-텍스트 쌍이 필요하며, 이는 이 작업은 가능한 출판을 위해 IEEE에 제출되었습니다. 저작권은 통지 없이 양도될 수 있으며, 그 후에는 이 버전에 더 이상 액세스할 수 없을 수 있습니다. 영어: 얻기 어려움[12,13]. 결과적으로, 이러한 훈련된 방법은 종종 새로운 장면에서 좋은 성능을 얻지 못합니다. 최근에는 오픈 어휘 3D 시각적 접지를 해결하려는 시도가 이루어졌으며[1,14–23], 종종 CLIP의 강점을 기반으로 구축되었습니다[24]. 그러나 CLIP에 대한 의존성으로 인해 &quot;단어 가방&quot; 동작을 나타내어 순서 없는 콘텐츠가 잘 모델링되지만 텍스트와 시각적 정보를 처리할 때 속성, 관계 및 순서가 무시됩니다[25]. 예를 들어, 그림 1에서 볼 수 있듯이 텍스트 쿼리 &quot;식탁과 창문 사이의 의자&quot;가 OpenScene[1]에 제공되면 모델은 창문과 식탁이 대상 의자와의 공간적 관계를 제공하는 데 사용되는 랜드마크일 뿐이라는 사실을 무시하고 방에 있는 모든 의자, 창문 및 식탁을 접지합니다. 동시에 ChatGPT 및 GPT-4[26]와 같은 대규모 언어 모델(LLM)은 계획 및 도구 사용을 포함하여 인상적인 언어 이해 기능을 보여주었습니다. 이러한 능력을 통해 LLM은 작업을 더 작은 조각으로 나누고 하위 작업을 완료하기 위해 언제, 무엇, 어떻게 도구를 사용해야 하는지 아는 방식으로 복잡한 작업을 해결하는 에이전트로 사용될 수 있습니다[27–34]. 이는 복잡한 자연어 쿼리를 사용한 3D 시각적 접지에 정확히 필요한 것입니다. 구성 언어를 더 작은 의미적 구성 요소로 구문 분석하고, 도구 및 환경과 상호 작용하여 피드백을 수집하고, 공간적 지식과 상식적 지식으로 추론하여 언어를 대상 객체에 반복적으로 접지합니다. 이러한 관찰 결과를 바탕으로 LLM 기반 에이전트를 사용하여 제로샷 오픈 어휘 3D 시각적 접지를 개선할 수 있는지라는 질문을 던집니다. 이 연구에서는 새로운 오픈 어휘, 제로샷, LLM 에이전트 기반 3D 시각적 접지 파이프라인인 LLM-Grounder를 제안합니다. 우리의 직감은 LLM이 CLIP 기반 시각적 그라운더의 &quot;단어 가방&quot; 약점을 완화하여 LLM 자체에 어려운 언어 분해, 공간 및 상식적 추론 작업을 맡기고 시각적 그라운더의 강점을 활용하여 간단한 명사 구를 그라운딩할 수 있다는 것입니다. 섹션 III에서 설명한 LLM-Grounder는 그라운딩 프로세스를 조율하기 위해 핵심에 LLM을 사용합니다. LLM은 먼저 구성적 자연어 쿼리를 객체 범주, 객체 속성(색상, 모양 및 재료), 랜드마크 및 공간 관계와 같은 의미적 개념으로 구문 분석합니다. 이러한 하위 쿼리는 CLIP 기반[24] 오픈 어휘 3D 시각적 그라운딩 방법인 OpenScene[1] 또는 LERF[35]가 지원하는 시각적 그라운더 도구로 전달되어 장면에서 각 개념을 그라운딩합니다. 시각적 그라운더는 개념에 대한 장면에서 가장 관련성 있는 후보 영역 주변에 몇 개의 경계 상자를 제안합니다. 이러한 후보들 각각에 대해 시각적 그라운더 도구는 객체 볼륨 및 랜드마크까지의 거리와 같은 공간 정보를 계산하여 LLM 에이전트에게 제공하여 에이전트가 공간 관계 및 상식의 관점에서 상황을 전체적으로 평가하고 원래 쿼리의 모든 기준과 가장 잘 일치하는 후보를 선택할 수 있도록 합니다. 이 프로세스는 LLM 에이전트가 결론에 도달했다고 결정할 때까지 반복됩니다. 주목할 점은, 우리의 접근 방식은 에이전트에게 환경 피드백을 제공하고 에이전트의 추론 프로세스를 폐쇄 루프로 만들어 이전의 신경 기호 접근 방식[36]을 확장합니다. 우리의 접근 방식은 레이블이 지정된 데이터에 대한 어떠한 훈련도 필요하지 않다는 점에 유의하는 것이 중요합니다. 그것은 개방형 어휘이며 새로운 3D 장면과 임의의 텍스트 쿼리로 제로샷 일반화할 수 있으며, 3D 장면의 의미적 다양성과 3D 텍스트 레이블이 지정된 데이터의 제한된 가용성을 감안할 때 바람직한 속성입니다. 우리의 실험(섹션 IV)에서 우리는 ScanRefer 벤치마크[12]에서 LLM-Grounder를 평가합니다. 이 벤치마크는 주로 구성적 시각적 참조 표현에 대한 이해가 필요한 3D 비전 언어 접지 기능을 평가합니다. 저희의 접근 방식은 OpenScene 및 LERF와 같은 제로샷 오픈 어휘 방법의 접지 기능을 개선하고 레이블이 지정된 데이터를 사용하지 않고 ScanRefer에서 최첨단 제로샷 접지 정확도를 보여줍니다. 저희의 절제 연구는 언어 쿼리가 더 복잡해질수록 LLM이 접지 기능을 더 많이 증가시킨다는 것을 보여줍니다. 이러한 결과는 LLM-Grounder가 3D 비전 언어 작업에 효과적인 접근 방식으로서 잠재력을 강조하며, 복잡한 환경을 이해하고 동적 쿼리에 응답하는 것이 필수적인 로봇 응용 프로그램에 특히 적합합니다. 요약하면, 이 논문의 기여는 다음과 같습니다. • LLM을 에이전트로 사용하면 3D 시각적 접지 작업에서 제로샷, 오픈 어휘 방법에 대한 접지 기능을 개선할 수 있음을 발견했습니다. • 레이블이 지정된 데이터를 사용하지 않고 제로샷 설정에서 ScanRefer에 대한 SOTA를 달성합니다. • 접지 쿼리 텍스트가 더 복잡할 때 LLM이 더 효과적임을 발견했습니다. II.
--- RELATED WORK ---
자연어를 사용한 3D 시각적 접지. 구조화되지 않은 3D 장면에서 자연어 쿼리를 접지하는 것은 다양한 로봇 작업에 필수적입니다. ScanRefer [12] 및 ReferIt3D [13]와 같은 선구적인 벤치마크는 이 분야를 발전시켰습니다. 이러한 벤치마크에서 제안한 대로 3D 및 텍스트의 참조 작업은 언어의 구성적 의미론과 3D 장면의 구조, 기하학 및 의미론에 대한 심층적인 이해가 필요합니다. 수많은
--- METHOD ---
, 시각적 그라운더로서. 공간 정보 텍스트 쿼리 &quot;식탁과 창문 사이의 의자&quot;를 그라운딩하라는 요청을 받았을 때, 대상이 아니라 참조 랜드마크(빨간색 경계 상자)인 식탁과 창문을 잘못 강조합니다. 우리는 대규모 언어 모델(LLM)을 활용하여 이 문제를 해결하고자 합니다. 1. 복잡한 시각적 그라운딩 쿼리를 하위 작업으로 분해하는 계획을 의도적으로 생성합니다. 2. 대상 찾기 및 랜드마크 찾기와 같은 도구를 조정하고 상호 작용하여 정보를 수집합니다. 3. 공간적 지식과 상식적 지식을 활용하여 도구에서 수집한 피드백을 반영합니다. 초록 3D 시각적 그라운딩은 가정용 로봇에 중요한 기술로, 로봇이 주변 환경에 따라 탐색하고, 물체를 조작하고, 질문에 답할 수 있도록 합니다. 기존 접근 방식은 종종 광범위한 레이블이 지정된 데이터에 의존하거나 복잡한 언어 쿼리를 처리하는 데 한계가 있지만, 우리는 새로운 제로샷, 오픈 어휘, 대규모 언어 모델(LLM) 기반 3D 시각적 그라운딩 파이프라인인 LLM-Grounder를 제안합니다. LLM-Grounder는 LLM을 사용하여 복잡한 자연어 쿼리를 의미적 구성 요소로 분해하고 OpenScene 또는 LERF와 같은 시각적 접지 도구를 사용하여 3D 장면에서 객체를 식별합니다. 그런 다음 LLM은 제안된 객체 간의 공간적 및 상식적 관계를 평가하여 최종 접지 결정을 내립니다. 저희 방법은 레이블이 지정된 학습 데이터가 필요하지 않으며 새로운 3D 장면과 임의의 텍스트 쿼리로 일반화할 수 있습니다. 저희는 ScanRefer 벤치마크에서 LLM-Grounder를 평가하고 최첨단 zero*Equal 기여를 보여줍니다. ¹컴퓨터 과학 및 공학, 미시간 대학교, 앤아버, 미시간, 미국, 48109. 연락처: Jianing Yang jianingy@umich.edu. 2Nikhil Madaan은 독립 연구원입니다. 3뉴욕 대학교. 이 연구는 NSF IIS-1949634, NSF SES2128623 및 Microsoft Academic Program Computing Credit의 후원을 받았습니다. 프로젝트 웹사이트: https://chat-with-nerf.github.io/ 샷 그라운딩 정확도. 저희의 연구 결과에 따르면 LLM은 그라운딩 기능을 크게 개선하는데, 특히 복잡한 언어 쿼리의 경우 LLM-Grounder가 로봇공학에서 3D 비전 언어 작업에 효과적인 접근 방식입니다. I. 서론 3D 장면에 들어가서 &quot;식탁과 창문 사이의 의자&quot;(그림 1)를 찾으라는 요청을 받았다고 상상해 보세요. 사람이 답을 알아내는 것은 쉽습니다. 이러한 기술을 3D 시각적 그라운딩이라고 하며, 일반적으로 물건을 찾는 것에서 도구를 조작하는 것에 이르기까지 일상적인 작업에 의존합니다. 이러한 능력을 습득하는 것은 복잡한 탐색(어디로 가야 하는지 아는 것), 조작(무엇/어디를 잡을지), 질의 응답에 필요한 기본 기술 역할을 하기 때문에 사람을 돕는 가정용 로봇을 만드는 데 중요합니다. 로봇에 이러한 능력을 부여하기 위해 연구자들은 여러 가지 접근 방식을 개발했습니다. 한 가지 방향은 객체 주변의 경계 상자를 제안하고 텍스트-경계 상자 매칭을 공동으로 모델링하기 위해 3D 및 텍스트 엔드투엔드 신경 구조를 훈련하는 것입니다[2–11]. 그러나 이러한 모델은 일반적으로 훈련 데이터를 위해 대량의 3D-텍스트 쌍이 필요하며, 이는 얻기 어렵습니다[12,13]. 결과적으로 이러한 훈련된 방법은 종종 새로운 장면에서 좋은 성능을 얻지 못합니다. 최근에는 오픈 어휘 3D 시각적 접지를 해결하려는 시도가 이루어졌으며[1,14–23], 종종 CLIP의 강점을 기반으로 합니다[24]. 그러나 CLIP에 대한 의존성으로 인해 순서 없는 내용은 잘 모델링되지만 텍스트 및 시각적 정보를 처리할 때 속성, 관계 및 순서는 무시되는 &quot;단어 가방&quot; 동작을 보입니다[25]. 예를 들어, 그림 1에서 설명한 대로, 텍스트 쿼리 &quot;식탁과 창문 사이의 의자&quot;가 OpenScene [1]에 주어지면 모델은 방에 있는 모든 의자, 창문, 식탁을 근거로 삼고, 창문과 식탁이 대상 의자와의 공간적 관계를 제공하는 데 사용되는 랜드마크일 뿐이라는 사실을 무시합니다. 동시에 ChatGPT 및 GPT-4 [26]와 같은 대규모 언어 모델(LLM)은 계획 및 도구 사용을 포함한 인상적인 언어 이해 기능을 보여주었습니다. 이러한 기능을 통해 LLM은 작업을 더 작은 조각으로 나누고 하위 작업을 완료하기 위해 도구를 언제, 무엇을, 어떻게 사용해야 하는지 아는 방식으로 복잡한 작업을 해결하는 에이전트로 사용될 수 있습니다 [27–34]. 이는 복잡한 자연어 쿼리를 사용한 3D 시각적 근거에 정확히 필요한 것입니다. 구성 언어를 더 작은 의미 구성 요소로 구문 분석하고, 도구 및 환경과 상호 작용하여 피드백을 수집하고, 공간적 지식과 상식적 지식으로 추론하여 언어를 대상 객체에 반복적으로 근거로 삼는 것입니다. 이러한 관찰을 바탕으로, 우리는 LLM 기반 에이전트를 사용하여 제로샷 오픈 어휘 3D 시각적 접지를 개선할 수 있을까라는 질문을 던집니다. 이 연구에서 우리는 새로운 오픈 어휘, 제로샷, LLM 에이전트 기반 3D 시각적 접지 파이프라인인 LLM-Grounder를 제안합니다. 우리의 직감은 LLM이 CLIP 기반 시각적 접지의 &quot;단어 가방&quot; 약점을 완화하여 LLM 자체에 어려운 언어 분해, 공간 및 상식적 추론 작업을 수행하는 동시에 시각적 접지의 강점을 활용하여 간단한 명사 구를 접지할 수 있다는 것입니다. 섹션 III에서 설명한 LLM-Grounder는 LLM을 핵심으로 사용하여 접지 프로세스를 조율합니다. LLM은 먼저 구성적 자연어 쿼리를 객체 범주, 객체 속성(색상, 모양 및 재료), 랜드마크 및 공간 관계와 같은 의미적 개념으로 구문 분석합니다. 이러한 하위 쿼리는 장면에서 각 개념을 접지하기 위해 CLIP 기반[24] 오픈 어휘 3D 시각적 접지 방법인 OpenScene[1] 또는 LERF[35]에서 지원하는 시각적 접지 도구로 전달됩니다. 시각적 접지 도구는 개념에 대한 장면에서 가장 관련성 있는 후보 영역 주변에 몇 개의 경계 상자를 제안합니다. 이러한 각 후보에 대해 시각적 접지 도구는 객체 볼륨 및 랜드마크까지의 거리와 같은 공간 정보를 계산하여 LLM 에이전트로 다시 제공하여 에이전트가 공간 관계 및 상식의 관점에서 상황을 전체적으로 평가하고 원래 쿼리의 모든 기준과 가장 잘 일치하는 후보를 선택할 수 있도록 합니다. 이 프로세스는 LLM 에이전트가 결론에 도달했다고 결정할 때까지 반복됩니다. 특히, 우리의 접근 방식은 에이전트에게 환경 피드백을 제공하고 에이전트의 추론 프로세스를 폐쇄 루프로 만들어 이전의 신경 기호 접근 방식[36]을 확장합니다. 우리의 접근 방식은 레이블이 지정된 데이터에 대한 훈련이 필요하지 않다는 점에 유의하는 것이 중요합니다. 그것은 개방형 어휘이며 새로운 3D 장면과 임의의 텍스트 쿼리에 대해 제로 샷 일반화할 수 있으며, 3D 장면의 의미적 다양성과 3D 텍스트 레이블이 지정된 데이터의 제한된 가용성을 감안할 때 바람직한 속성입니다. 우리의
--- EXPERIMENT ---
s(섹션 IV)에서 ScanRefer 벤치마크[12]에서 LLM-Grounder를 평가합니다. 이 벤치마크는 주로 구성적 시각적 참조 표현에 대한 이해가 필요한 3D 비전 언어 접지 기능을 평가합니다. 저희의 접근 방식은 OpenScene 및 LERF와 같은 제로샷 오픈 어휘 방법의 접지 기능을 개선하고 레이블이 지정된 데이터를 사용하지 않고 ScanRefer에서 최첨단 제로샷 접지 정확도를 보여줍니다. 저희의 절제 연구는 언어 쿼리가 더 복잡해질수록 LLM이 접지 기능을 더 많이 증가시킨다는 것을 보여줍니다. 이러한 결과는 LLM-Grounder가 3D 비전 언어 작업을 위한 효과적인 접근 방식이라는 잠재력을 강조하며, 복잡한 환경을 이해하고 동적 쿼리에 응답하는 것이 필수적인 로봇 응용 프로그램에 특히 적합합니다. 요약하면, 이 논문의 기여는 다음과 같습니다. • 저희는 LLM을 에이전트로 사용하면 3D 시각적 접지 작업에서 제로샷, 오픈 어휘 방법에 대한 접지 기능을 개선할 수 있다는 것을 발견했습니다. • 레이블이 지정되지 않은 데이터를 사용하여 제로샷 설정에서 ScanRefer에서 SOTA를 달성합니다.• LLM은 접지 쿼리 텍스트가 더 복잡할 때 더 효과적입니다.II. 관련 연구 자연어를 사용한 3D 시각적 접지.구조화되지 않은 3D 장면에서 자연어 쿼리를 접지하는 것은 다양한 로봇 작업에 필수적입니다.ScanRefer[12] 및 ReferIt3D[13]와 같은 선구적인 벤치마크는 이 분야를 발전시켰습니다.이러한 벤치마크에서 제안한 것처럼 3D 및 텍스트의 참조 작업에는 언어의 구성적 의미론과 3D 장면의 구조, 기하학 및 의미론에 대한 심층적인 이해가 필요합니다.성능을 향상시키기 위해 3D 및 언어에 대해 공동으로 훈련된 수많은 방법이 제안되었습니다[2-11].그러나 이러한 방법은 이러한 벤치마크가 구축된 원래 ScanNet[37]에 제시된 특정 개체 클래스로 인해 폐쇄형 어휘 설정으로 제한됩니다. 영어: 2D 오픈 어휘 분할의 발전[38–40]에 의해 동기를 부여받은 연구자들은 3D 오픈 어휘 접지[1,14–21,35,41]를 탐구했습니다.그러나 이러한 방법은 대부분 기본 시각-언어 브리지로 CLIP[24]에 의존합니다.이것은 접지 텍스트 쿼리가 간단한 명사 구(예: &quot;빨간 사과&quot;)인 경우에 효과적입니다.그러나 연구에 따르면 CLIP은 &quot;단어 가방&quot; 동작을 보이며 텍스트나 시각적 객체의 관계, 귀속 및 순서와 같은 구성적 이해가 부족합니다[25].이는 ScanRefer 및 ReferIt3D에서 제시된 과제의 중요한 측면입니다.이러한 측면을 인식하여 의미 추상화[22] 및 3D-CLR[23]은 공간 정보 텍스트 및 3D 데이터를 사용하여 전담 신경 학습을 훈련합니다.테이블 근처에서 나무 의자 찾기.시간 LLM 에이전트테이블 근처에서 나무 의자를 확인하겠습니다.LLM 에이전트대상 #이 원하는 것 같습니다. LLM 에이전트 타겟 파인더 후보자 이름: 의자 속성: 나무 관찰: 사용자는 테이블 근처에 있는 나무 의자를 찾고 있습니다. 추론: &quot;테이블 근처에 있는 나무 의자를 찾아야 합니다. 계획: 1. 나무 의자를 가지고 Target Finder를 호출하고 테이블을 가지고 Landmark Finder를 호출합니다. 2. 랜드마크 테이블과의 근접성을 기준으로 후보자를 비교합니다. 3. 필요한 경우 사용자에게 추가 설명을 요청합니다. 4. 올바른 OpenScene/LERF CandidateLandmark Finder 이름: 테이블 관계: OpenScene/LERF 관찰 근처: Target Finder와 Landmark Finder가 좌표를 반환할 때까지 반복합니다. 또한 각 후보자의 랜드마크와의 거리와 볼륨을 객체화합니다. 추론: 정보를 집계하여 결정해야 합니다. 계획: 1. 랜드마크 테이블과의 근접성을 기준으로 의자 후보자를 비교합니다. 2. 객체 볼륨을 검사하고 너무 작은 객체를 필터링합니다. 3. 각 후보자를 전체적으로 점수 매기고 순위를 매기고 필터링합니다. 4. 접지 결정을 내립니다. 자기 비판: 모든 요소를 고려하여 전체적인 상황에 따라 결정을 내려야 합니다. 객체를 찾을 수 있습니다. 그림 2: LLM-Grounder 개요. 주어진 객체를 접지하기 위한 쿼리, LLM 에이전트의 지원을 받는 우리의 접근 방식은 사용자의 요청에 대해 추론하고 도구를 사용하여 객체를 접지하기 위한 계획을 생성합니다. 에이전트는 대상 찾기 및 랜드마크 찾기와 같은 도구와 상호 작용하여 도구에서 객체 경계 상자, 객체 볼륨 및 랜드마크까지의 거리와 같은 정보를 수집합니다. 그런 다음 이 정보는 에이전트로 반환되어 추가 공간 및 상식적 추론을 수행하여 가장 잘 일치하는 후보를 순위를 매기고 필터링하고 선택합니다. 네트워크를 사용하여 접지하기 전에 텍스트 쿼리의 구성적 의미를 구문 분석하고 접지합니다. 이와 대조적으로 우리의 방법은 훈련 없이(제로샷) 동일한 것을 달성하기 위해 LLM 에이전트를 사용할 가능성을 탐구합니다. NS3D[36]는 LLM 기반 코드 생성을 사용하여 이 문제를 해결하는 프로그램을 생성하는데, 이는 우리의 접근 방식과 더 유사하지만, 또한 기준 진실 객체 분할 및 범주를 사용하여 시각적 접지를 단순화하므로 오픈 어휘 및 제로샷 기능이 부족합니다. LLM 에이전트 대규모 언어 모델(LLM)[26,42-46]의 최근 발전은 놀라운 새로운 능력을 보여주었습니다. 여기서는 LLM을 에이전트로 사용할 수 있도록 하는 몇 가지 능력을 나열합니다. a) 계획: 계획은 복잡한 목표를 하위 목표로 나누고 발행된 조치와 환경 피드백을 기반으로 자기 반성을 하는 것을 포함합니다. Chain-of-thought[27]은 LLM이 복잡한 작업을 더 작은 작업으로 분해하여 &quot;단계적으로 생각&quot;하도록 지시받았을 때 더 나은 계획 능력을 보였다는 것을 보여줍니다. Tree-of-thoughts[28]는 단계당 여러 생각을 탐구하여 사슬을 트리로 바꾸어 이 접근 방식을 확장합니다. [47-50]은 LLM이 출력과 환경 피드백에 대해 자기 반성하도록 지시받았을 때 더 나은 출력을 생성할 수 있음을 보여줍니다. b) 도구 사용: 도구를 사용하는 능력은 인간 지능의 고유한 특징입니다. 현재 LLM이 모든 작업(예: 수학과 사실적 질의응답 문제)에 능숙하지 않다는 것을 인식한 연구자들은 LLM이 작업을 완료하기 위해 도구 사용을 조율하도록 하는 가능성을 탐구했습니다. 도구 사용 문제의 핵심은 어떤 도구를 사용할지, 언제 사용할지 결정하는 것입니다. Socratic Models[29]는 자연어를 매체로 사용하여 LLM 에이전트가 시각 언어 모델 및 오디오 언어 모델과 같은 다른 다중 모드 언어 모델과 함께 안내된 토론에 참여하여 작업을 집단적으로 완료하도록 합니다. MRKL[51] 및 TAML[52]은 LLM에 계산기를 장착하고 수학 문제를 푸는 능력이 향상되었음을 보여줍니다. 이러한 결과를 바탕으로 LangChain[53]과 같은 소프트웨어 라이브러리가 개발되어 개발자를 위해 LLM 도구 사용을 간소화했습니다. ToolFormer[30], HuggingGPT[54] 및 API-Bank[55]는 더 많은 API와 머신 러닝 모델을 LLM이 사용할 도구로 개방하여 도구 사용을 더욱 확대합니다. 로봇공학에서 SayCan[31], InnerMonologue[32], Code as Policies[33] 및 LM-Nav[34]는 LLM의 계획 및 도구 사용 기능을 사용하여 장기적이고 복잡한 작업을 위한 실제 로봇의 고급 컨트롤러 역할을 하도록 합니다. 이러한 작업에서 얻은 성공은 LLM을 에이전트로 사용하여 3D 시각적 접지에서 제시된 구성적 언어-시각 이해 과제를 해결하는 데 동기를 부여합니다.III. 방법 최근 Auto-GPT[56], GPTEngineer[57] 및 ToolFormer[30]의 성공 사례는 LLM을 에이전트로 사용하는 데 있어 초기 성공 징후를 보여줍니다.에이전트는 목표에 의해 주도되고 목표에 대한 추론을 하고 계획을 세우고 도구를 조사하고 사용하며 환경과 상호 작용하고 환경으로부터 피드백을 수집한다는 점에서 기계 학습의 기존 모델과 다릅니다.3D 시각적 접지 설정에서 에이전트는 기존 모델이 보이는 &quot;단어 가방&quot; 동작에 대한 유망한 솔루션이 될 수 있습니다. LLM-Grounder에서는 GPT-4를 에이전트로 사용하여 세 가지 작업을 완료하도록 합니다. 1. 복잡한 텍스트 쿼리를 OpenScene 및 LERF와 같은 CLIP 기반 3D 시각적 그라운더와 같은 다운스트림 도구가 더 잘 처리할 수 있는 하위 작업으로 분해합니다. 2. 이러한 도구를 조율하고 사용하여 제안된 하위 작업을 해결합니다. 3. 환경으로부터 피드백에 대한 추론은 Training Size Open-Vocab Method Visual Grounder + LLM Agent Acc@0.25 ↑ Acc@0.5 ↑ 36k 레이블이 지정된 3D 텍스트 데이터 closed-vocab ScanRefer[12] 3DVG-Trans[2] 34.20.41.28.zero-shot open-vocab LERF[35] Ours LERF LERF 4.0.GPT-6.9 (+2.5) 1.6 (+1.3) zero-shot open-vocab OpenScene[1] Ours Ours OpenScene OpenScene OpenScene 13.5.✓ GPT-3.✓ GPT-14.3 (+1.3) 17.1 (+4.1) 4.7 (-0.4) 5.3 (+0.2) 표 I: ScanRefer에 대한 실험 결과. LLM(GPT-4) 에이전트는 LERF 및 OpenScene과 같은 제로샷 오픈 어휘 3D 그라운더의 3D 그라운딩 기능을 크게 증가시킵니다. 우리는 그라운딩 기능을 Accuracy@0.and @0.5로 측정하는데, 이는 교차-연합(IoU) wrt 그라운드-트루스 박스가 각각 0.25와 0.5를 초과하는 바운딩 박스 예측의 정확도입니다. 괄호 안의 숫자는 LLM 에이전트를 추가한 후의 성능 이득 또는 손실을 나타냅니다. 또한 GPT-3.5와 같은 덜 강력한 LLM은 강력한 그라운딩 기능 이득을 달성할 수 없음을 결과에 보여줍니다. 마지막으로, 제로샷 오픈 어휘인 우리의 방법과 직접 비교할 수는 없지만, 완전성을 위해 ScanRefer와 폐쇄 어휘에서 훈련된 방법에 대한 성능이 나열되어 있습니다. LERF 낮은 시각적 난이도 LLM 없음 LLM 있음 10.15.1 (+4.3) LLM 없음 LLM 있음 2.4.4 (1.9) 높은 시각적 난이도 OpenScene 27.33.6 (+6.0) 8.12.1 (+3.5) 표 II: 시각적 복잡성에 대한 소거 연구. LLM 에이전트는 낮은 시각적 난이도 설정에서 3D 접지에 더 효과적입니다. 표시된 숫자는 Acc@.25입니다. 접지 결정을 내리기 위한 공간 이해와 상식. 계획. LLM의 첫 번째 장점은 계획하는 능력입니다. 연구에 따르면 사고의 사슬 추론[27], 즉 LLM이 복잡한 목표를 더 작은 하위 작업으로 분해하도록 명시적으로 촉구하는 것(&quot;단계적으로 생각하기&quot;)은 산술, 상식 및 기호 추론 작업에 도움이 될 수 있습니다. 이러한 발견에서 영감을 얻어 그림 2에 나와 있는 것과 같이 에이전트를 설계합니다. 구체적으로, 먼저 에이전트에게 관찰 내용을 설명하도록 요청하여 에이전트가 현재 상황을 요약할 수 있는 기회를 제공합니다. 맥락에는 인간 텍스트 쿼리와 도구에서 반환된 정보(아래 설명)가 포함될 수 있습니다. 그런 다음 에이전트는 추론이라는 섹션을 시작하는데, 이는 에이전트가 고수준 계획을 수행하는 데 필요한 정신적 스크래치패드 역할을 합니다. 그런 다음 계획 섹션에서 에이전트는 도구 사용, 비교 또는 계산을 포함하여 고수준 계획을 이행하기 위한 보다 구체적인 단계를 나열해야 합니다. 에이전트는 자기 비판 섹션에서 생성된 계획을 반영하고 최종 수정을 할 수 있습니다[50]. 도구 사용. LLM의 두 번째 장점은 도구를 사용할 수 있는 능력에서 비롯됩니다. LLM 에이전트에게 도구를 사용하여 &quot;단어 가방&quot; 동작을 해결하도록 지시합니다(제2절). 그림 2에서 보듯이, 시각적 접지 및 피드백을 위해 설계한 두 도구의 예상 입력 및 출력 형식, 즉 API를 LLM에 알리고 LLM 에이전트에게 주어진 형식에 따라 상호 작용하도록 요청합니다. 도구에는 대상 파인더와 랜드마크 파인더가 포함됩니다. 대상 파인더와 랜드마크 파인더. 대상 파인더와 랜드마크 파인더는 텍스트 쿼리 입력을 받고 쿼리에 대한 가능한 위치 클러스터의 경계 상자를 찾고 중심과 크기(Cx, Cy, Cz, AX, AY, AZ)의 형태로 후보 경계 상자 목록을 반환합니다. 대상은 사용자가 쿼리에서 참조하는 주요 객체(식탁과 창문 사이의 의자에서 &quot;의자&quot;)입니다. 랜드마크는 대상을 공간적으로 참조하는 데 사용되는 객체(&quot;식탁&quot;과 &quot;창문&quot;)입니다. 대상 파인더는 또한 각 후보의 볼륨을 계산하고 랜드마크 파인더는 또한 각 대상 후보의 중심에서 랜드마크의 중심까지의 유클리드 거리를 계산합니다. 볼륨, 거리 및 경계 상자는 LLM 에이전트가 공간 및 상식적 추론을 수행하기 위한 피드백을 함께 제공합니다. 예를 들어, 볼륨이 0.01m³만큼 작은 후보 &quot;의자&quot;는 아마도 거짓 양성일 것이고 필터링되어야 합니다. 랜드마크와의 거리가 쿼리에서 언급한 공간 관계를 준수하지 않는 후보는 거부되어야 합니다. 타겟 파인더와 랜드마크 파인더는 오픈 어휘 CLIP 기반 3D 시각적 그라운더 LERF [35] 및 OpenScene [1]에 의해 구현됩니다. 이러한 도구만으로는 복잡한 텍스트 쿼리가 주어졌을 때 &quot;단어 가방&quot; 동작(Sec. I)을 보입니다. 그러나 간단한 명사구(&quot;의자&quot;)와 같은 더 간단한 텍스트 쿼리가 주어졌을 때 이러한 도구는 일반적으로 잘 작동할 수 있습니다. LLM 에이전트는 이러한 3D 시각적 그라운더의 명사구 그라운딩 기능을 활용하면서 복잡한 그라운딩 쿼리를 분해하고 한 번에 하나의 객체를 그라운딩하고 나중에 공간 관계에 대해 추론함으로써 언어 이해와 공간 추론의 약점을 보완합니다. 타겟 파인더를 사용하기 위해 LLM 에이전트에게 원래 자연어 쿼리에서 명사 구(예: &quot;나무 의자&quot;)를 구문 분석하도록 지시합니다. 랜드마크 파인더를 사용하기 위해 LLM 에이전트에게 원래 쿼리에서 언급된 랜드마크 객체와 타겟 객체와의 공간 관계를 구문 분석하도록 지시합니다. IV. 실험 실험에서 먼저 LLM 기반 에이전트가 CLIP 기반 3D 시각적 접지 방법과 비교하여 제로샷 오픈 어휘 3D 시각적 접지를 얼마나 잘 개선하는지 평가하고 싶습니다. 그런 다음 폐쇄 어휘 설정에서 방법을 평가하고 폐쇄 어휘 및 훈련된 접근 방식과 비교합니다. 마지막으로 접근 방식의 일반화를 보여주기 위해 야생 장면에서 몇 가지 정성적 예를 보여줍니다. A. 데이터 집합 ScanRefer. ScanRefer[12]는 자연어를 사용하여 실내 3D 장면에서 3D 객체 현지화에 대한 벤치마크입니다. 18개의 11,개 객체에 대한 51,583개의 인간이 쓴 설명으로 구성되어 있습니다. 영어: 800개의 ScanNet[37] 3D 장면의 의미 범주, 여기서 train/val/test 분할에는 각각 36,665개, 9,508개 및 5,410개의 설명이 포함됩니다. 표 I에 제시된 실험에 대한 검증 분할의 첫 번째 장면을 사용하며, 이는 998개의 텍스트-3D-객체 쌍으로 구성됩니다. 또한 ScanRefer의 두 가지 표준 메트릭인 Accuracy@0.25 및 Accuracy@0.5를 보고합니다. 0.25와 0.5는 3D 경계 상자의 IoU에 대한 다른 임계값입니다. B. 기준 방법 ScanRefer. ScanRefer[12]는 종단 간 3D 텍스트 신경망 아키텍처를 사용하여 자연어 입력이 주어졌을 때 객체를 지역화합니다. 구체적으로 3D 포인트 클라우드를 PointNet++[58] 기능으로 처리한 다음 포인트를 클러스터링하고 객체의 경계 상자를 제안합니다. 그런 다음 언어 기능을 클러스터 및 상자와 융합하여 언어에서 참조하는 상자를 결정합니다.파이프라인은 텍스트 및 b-상자 쌍과 장면의 모든 객체에 대한 기준 진실 b-상자 및 의미 클래스를 감독합니다.이 기준선을 현재 훈련된 파이프라인의 성능을 보여주는 것으로 포함시켜 감독이 사용되지 않는 제로샷 설정과 비교한 한계로 사용합니다.3DVG-Transformer.3DVG-Transformer[2]는 ScanRefer의 종단 간 신경 구조를 기반으로 하며 경계 상자를 제안하기 전에 근처 클러스터를 집계하는 새로운 신경 모듈을 제안합니다.ScanRefer와 유사하게 3DVGTransformer도 기준 진실 객체 b상자, 의미 클래스 및 인간이 주석을 단 설명의 감독을 사용합니다.OpenScene 및 LERF.OpenScene[1] 및 LERF[35]는 제로샷 오픈 어휘 3D 장면 이해 접근 방식입니다. OpenScene은 2D CLIP 기능을 3D 포인트 클라우드로 추출하고, 쿼리의 CLIP 텍스트 임베딩과 3D 포인트 클라우드의 모든 포인트 간의 코사인 유사도를 계산하여 텍스트 쿼리로 접지를 허용합니다. LERF는 신경 복사장에 CLIP 임베딩을 인코딩하여 동일한 것을 달성합니다. 이러한 방법은 단독으로 사용될 때 1에서 설명한 대로 &quot;단어 가방&quot; 동작을 보이는데, 이는 LLM 에이전트 심의 추론으로 해결하고자 하는 문제입니다. 3D 시각적 접지 벤치마크 ScanRefer에 OpenScene과 LERF를 사용하여 경계 상자를 생성하기 위해 코사인 유사도가 높은 지점에 DBSCAN 클러스터링[59]을 적용하고 그 주위에 경계 상자를 그립니다. C. 결과 먼저 그림 3에서 LLM-Grounder의 정성적 결과를 보여줍니다. 더 많은 결과와 데모는 프로젝트 웹사이트¹에서 찾을 수 있으며, 여기에는 실제 장면도 포함됩니다. 기준선과 비교했을 때 LLM 에이전트가 제로 샷, 오픈 어휘 접지를 개선할 수 있다는 것을 발견했습니다. 표 I에서 볼 수 있듯이 LLM 에이전트를 추가하면 각각 정확도@0.25에서 5.0%와 17.1%를 달성하여 LERF와 OpenScene의 접지 성능을 크게 높일 수 있습니다. 우리는 LERF의 성능 증가가 낮은 것은 LERF의 전반적인 접지 기능이 약하기 때문입니다. 낮은 증가는 도구가 LLM 에이전트에 너무 노이즈가 많은 피드백을 제공할 때 LLM 에이전트가 노이즈가 많은 입력으로 추론하고 성능을 개선하기 어렵다는 것을 시사합니다. 또한 예측된 b-박스가 접지 진실 박스와 50% 이상 겹치도록 요구하는 Accuracy@0.5에서 성능이 낮게 증가한 것을 알 수 있습니다. 이는 기본 접지기의 인스턴스 분할 기능이 부족하기 때문입니다. 접지기는 종종 올바르게 접지된 객체에 대해 너무 크거나 너무 작은 경계를 예측하는 것을 관찰합니다. 이러한 예측은 LLM에서 수정할 수 없으므로 정확한 시각적 접지가 어렵고 성능이 낮습니다. 또한 OpenScene의 에이전트로 GPT-3.5를 사용할 때 성능이 GPT 없이 사용할 때보다 떨어집니다. 이는 GPT-3.5의 도구 사용 및 공간 및 상식적 추론 기능이 약하기 때문입니다. D. 절제 연구 그런 다음 LLM 에이전트가 주로 무엇을 개선하는지 평가합니다. 두 가지 다른 설정을 테스트합니다. (1) LLM 에이전트가 더 어려운 시각적 맥락에서 더 많은 도움이 됩니까? (2) LLM 에이전트가 더 어려운 텍스트 쿼리에서 더 많은 도움이 됩니까? 시각적 맥락의 난이도. 표 II에서 결과를 시력 난이도에 따라 분류하고 LLM 에이전트가 낮은 시력 난이도 쿼리에 더 효과적이라는 것을 발견했으며, 이는 더 높은 접지 성능 증가로 입증됩니다. 구체적으로 접지 쿼리를 낮은 시각적 난이도와 높은 시각적 난이도 범주로 구분합니다. 텍스트 쿼리에서 언급된 객체가 장면에서 해당 클래스의 유일한 객체(0개의 방해 객체)인 경우 쿼리는 낮은 시각적 난이도입니다. 장면에서 같은 클래스의 방해 객체가 두 개 이상 있는 경우 쿼리는 높은 시각적 난이도입니다. 평가한 998개의 쿼리 중 232개의 쿼리는 낮은 시각적 난이도였고 766개의 쿼리는 높은 시각적 난이도였습니다. 표 II의 결과는 LLM이 낮은 시각적 난이도 쿼리에 대해 더 많은 성능 증가를 가져온다는 것을 보여줍니다. 이러한 동작은 낮음 및 높음 시각적 난이도 설정에서 제시되는 다양한 과제로 설명할 수 있습니다. 낮은 시각적 난이도 설정에서 오픈 어휘 3D 그라운더가 직면하는 주요 과제는 &quot;단어 가방&quot; 동작입니다. 예를 들어, 텍스트 쿼리가 &quot;주방의 싱크대&quot;이고 장면에 싱크대가 하나만 있는 경우 단어 가방 그라운더는 전체 주방을 강조하여 그라운딩 정확도가 낮아집니다. LLM 에이전트는 https://chat-with-nerf.github.io/에서 이 문제를 해결하는 데 특히 능숙합니다. &quot;책상 왼쪽에 밝은 회색 모니터가 있습니다. 책상은 더 작고 휘어져 있습니다.&quot; LLM 에이전트 대상 찾기 대상 찾기를 사용하여 밝은 회색 모니터라는 문구로 대상을 그라운딩합니다. 랜드마크 찾기를 사용하여 창문 옆의 더 작은 곡선 책상이라는 문구로 랜드마크를 그라운딩합니다. 대상과의 관계는 아래에 있습니다. 랜드마크 찾기 LLM 에이전트 모니터는 책상의 왼쪽에 있어야 하므로 중심의 x 값은 랜드마크의 x 값보다 작아야 합니다. 그리고 각 후보에 대한 바운딩 박스와 볼륨을 받았습니다. 지금까지 가장 잘 맞는 후보를 선택했습니다.그림 3: 정성적 예.LLM 에이전트는 공간 추론을 사용하여 올바른 객체 인스턴스를 성공적으로 모호하지 않게 합니다.LLM 사용 - LLM 미사용 10%R^2 OpenScene = 0.8%- R^2 LERF = 0.OpenScene LERF 5% 2% 0% -2% -5%-8%문장의 명사 수그림 4: 성능 델타(LLM 사용 - LLM 미사용) 대 쿼리 텍스트 복잡성.텍스트 쿼리가 더 복잡할수록 LLM이 더 도움이 되지만 복잡도가 더 높으면 크게 도움이 되지 않습니다. Avg Acc@0.25loU R (OpenScene LLM) = -0.35% 30%R (OpenScene noLLM) = -0.25% R (LERF LLM) = -0.*R (LERF noLLM) = -0.20% 15% 10% 5% 0%OpenScene w/ LLM OpenScene w/o LLM LERF w/ LLM LERF w/o LLM문장의 명사 수그림 5: 다양한 모델의 성능 대 쿼리 텍스트 복잡성. 모든 모델은 더 복잡한 문장에서 어려움을 겪지만 LLM 에이전트가 있는 모델은 특히 이러한 더 높은 복잡성에서 더 나은 성과를 보입니다. 대상 객체 &quot;sink&quot;를 구문 분석하고 이 단일 명사만 grounder에 발행하여 bag-ofwords 동작을 우회합니다. 그러나 높은 시각적 난이도 설정의 경우 인스턴스 모호성 해소라는 추가 과제가 있습니다. 장면에 동일한 클래스의 인스턴스가 여러 개 있기 때문에 시각적 grounder는 많은 후보를 LLM 에이전트에 반환합니다. LLM 에이전트는 공간 및 상식적 추론 기능을 사용하여 볼륨과 랜드마크 정보까지의 거리가 있는 일부 인스턴스를 필터링할 수 있지만 보다 복잡한 인스턴스 모호성 해소에는 일반적으로 보다 미묘한 시각적 단서가 필요한데, LLM 에이전트는 시각 장애가 있기 때문에 이러한 특권이 없습니다. 텍스트 쿼리의 난이도. 쿼리가 더 복잡해질수록 LLM 에이전트는 성능을 향상시키지만 어느 정도까지만 가능합니다. 문장에 있는 명사의 수를 세어 쿼리 복잡성을 측정할 수 있습니다. 설명에 명사가 많을수록 특정 객체를 접지하기가 더 어려워집니다. 그림 5에서 LLM 에이전트의 도움이 있든 없든 문장 복잡성이 증가함에 따라 성능이 감소하는 것을 볼 수 있습니다. 그러나 LLM 에이전트를 사용하고 사용하지 않는 것의 성능 차이를 분석한 결과, 쿼리 복잡성에 대한 2차 종속성이 있음을 알 수 있습니다(그림 4). 이는 LLM이 더 높은 복잡성 쿼리가 제공될 때 접지에 대한 이점을 제공하지만, 어떤 임계값에 도달한 후에는 성능 이점이 감소함을 시사합니다. 쿼리 복잡성이 낮을 때 LLM이 없는 모델은 객체를 효과적으로 접지할 수 있으므로 LLM은 최소한의 이점을 제공합니다. 복잡성이 증가함에 따라 기준 모델의 성능이 떨어지고 LLM은 더 큰 이점을 제공합니다. 그러나 참조 표현의 복잡성이 증가함에 따라 LLM의 공간 추론 기능은 noLLM 기준의 성능을 능가하지 못할 수 있습니다. 이러한 더 높은 복잡성 범위에서 이점을 얻으려면 더 강력한 LLM이 필요할 수 있습니다. V.
--- CONCLUSION ---
. 특히, 우리의 접근 방식은 에이전트에게 환경 피드백을 제공하고 에이전트의 추론 프로세스를 폐쇄 루프로 만들어 이전의 신경 기호 접근 방식[36]을 확장합니다. 우리의 접근 방식은 레이블이 지정된 데이터에 대한 어떠한 훈련도 필요하지 않다는 점에 유의하는 것이 중요합니다. 그것은 개방형 어휘이며 새로운 3D 장면과 임의의 텍스트 쿼리로 제로샷 일반화할 수 있습니다. 이는 3D 장면의 의미적 다양성과 3D 텍스트 레이블이 지정된 데이터의 제한된 가용성을 감안할 때 바람직한 속성입니다. 우리의 실험(섹션 IV)에서 우리는 ScanRefer 벤치마크[12]에서 LLM-Grounder를 평가합니다. 이 벤치마크는 주로 구성적 시각적 참조 표현에 대한 이해가 필요한 3D 시각 언어 접지 기능을 평가합니다. 우리의 접근 방식은 OpenScene 및 LERF와 같은 제로샷 개방형 어휘 방법의 접지 기능을 개선하고 레이블이 지정된 데이터를 사용하지 않고 ScanRefer에서 최첨단 제로샷 접지 정확도를 보여줍니다. 우리의 절제 연구는 언어 쿼리가 더 복잡해질수록 LLM이 접지 기능을 더 많이 증가시킨다는 것을 보여줍니다. 이러한 연구 결과는 LLM-Grounder가 3D 시각 언어 작업을 위한 효과적인 접근 방식으로서 잠재력을 강조하며, 복잡한 환경을 이해하고 동적 쿼리에 응답하는 것이 필수적인 로봇 응용 프로그램에 특히 적합합니다.요약하면, 이 논문의 기여는 다음과 같습니다.• LLM을 에이전트로 사용하면 3D 시각 접지 작업에서 제로샷, 오픈 어휘 방법에 대한 접지 기능을 개선할 수 있음을 발견했습니다.• 레이블이 지정되지 않은 데이터를 사용하여 제로샷 설정에서 ScanRefer에 대한 SOTA를 달성했습니다.• 접지 쿼리 텍스트가 더 복잡할 때 LLM이 더 효과적임을 발견했습니다.II. 관련 연구 자연어를 사용한 3D 시각 접지. 구조화되지 않은 3D 장면에서 자연어 쿼리를 접지하는 것은 다양한 로봇 작업에 필수적입니다.ScanRefer[12] 및 ReferIt3D[13]와 같은 선구적인 벤치마크가 이 분야를 발전시켰습니다. 이러한 벤치마크에서 제안된 대로 3D 및 텍스트의 참조 작업은 언어의 구성적 의미론과 3D 장면의 구조, 기하학 및 의미론에 대한 심층적인 이해가 필요합니다. 성능을 향상시키기 위해 3D 및 언어에 대해 공동으로 훈련된 수많은 방법이 제안되었습니다[2-11]. 그러나 이러한 방법은 이러한 벤치마크가 구축된 원래 ScanNet[37]에 제시된 특정 개체 클래스로 인해 폐쇄형 어휘 설정으로 제한됩니다. 2D 개방형 어휘 분할[38–40]의 발전에 의해 동기를 부여받은 연구자들은 3D 개방형 어휘 접지[1,14–21,35,41]를 탐구했습니다. 그러나 이러한 방법은 대부분 기본 비전-언어 브리지로 CLIP[24]에 의존합니다. 이것은 접지 텍스트 쿼리가 간단한 명사 구(예: &quot;빨간 사과&quot;)일 때 잘 작동합니다. 그러나 연구에 따르면 CLIP은 &quot;단어 가방&quot; 행동을 보이며 텍스트나 시각적 객체의 관계, 귀속, 순서와 같은 구성적 이해가 부족합니다[25]. 이는 ScanRefer와 ReferIt3D에서 제시된 과제의 중요한 측면입니다. 이 측면을 인식하여 의미 추상화[22]와 3D-CLR[23]은 공간 정보 텍스트 및 3D 데이터를 사용하여 전담 신경 학습을 수행합니다. 테이블 근처에서 나무 의자 찾기. 시간 LLM 에이전트 테이블 근처에서 나무 의자를 확인하겠습니다. LLM 에이전트 타겟 #이 원하는 내용이라고 생각합니다. LLM 에이전트 타겟 찾기 후보 이름: 의자 속성: 나무 관찰: 사용자가 테이블 근처에서 나무 의자를 찾고 있습니다. 추론: &quot;테이블 근처에 있는 나무 의자를 찾아야 합니다. 계획: 1. 나무 의자를 가지고 Target Finder를 호출하고 테이블을 가지고 Landmark Finder를 호출합니다. 2. 랜드마크 테이블과의 근접성을 기준으로 후보자를 비교합니다. 3. 필요한 경우 사용자에게 추가 설명을 요청합니다. 4. 올바른 OpenScene/LERF CandidateLandmark Finder 이름: 테이블 관계: OpenScene/LERF 관찰 근처: Target Finder와 Landmark Finder가 좌표를 반환할 때까지 반복합니다. 또한 각 후보자의 랜드마크와의 거리와 볼륨을 객체화합니다. 추론: 정보를 집계하여 결정해야 합니다. 계획: 1. 랜드마크 테이블과의 근접성을 기준으로 의자 후보자를 비교합니다. 2. 객체 볼륨을 검사하고 너무 작은 객체를 필터링합니다. 3. 각 후보자를 전체적으로 점수 매기고 순위를 매기고 필터링합니다. 4. 접지 결정을 내립니다. 자기 비판: 모든 요소를 고려하여 전체적인 상황에 따라 결정을 내려야 합니다. 객체를 찾을 수 있습니다. 그림 2: LLM-Grounder 개요. 주어진 객체를 접지하기 위한 쿼리, LLM 에이전트의 지원을 받는 우리의 접근 방식은 사용자의 요청에 대해 추론하고 도구를 사용하여 객체를 접지하기 위한 계획을 생성합니다. 에이전트는 대상 찾기 및 랜드마크 찾기와 같은 도구와 상호 작용하여 도구에서 객체 경계 상자, 객체 볼륨 및 랜드마크까지의 거리와 같은 정보를 수집합니다. 그런 다음 이 정보는 에이전트로 반환되어 추가 공간 및 상식적 추론을 수행하여 가장 잘 일치하는 후보를 순위를 매기고 필터링하고 선택합니다. 네트워크를 사용하여 접지하기 전에 텍스트 쿼리의 구성적 의미를 구문 분석하고 접지합니다. 이와 대조적으로 우리의 방법은 훈련 없이(제로샷) 동일한 것을 달성하기 위해 LLM 에이전트를 사용할 가능성을 탐구합니다. NS3D[36]는 LLM 기반 코드 생성을 사용하여 이 문제를 해결하는 프로그램을 생성하는데, 이는 우리의 접근 방식과 더 유사하지만, 또한 기준 진실 객체 분할 및 범주를 사용하여 시각적 접지를 단순화하므로 오픈 어휘 및 제로샷 기능이 부족합니다. LLM 에이전트 대규모 언어 모델(LLM)[26,42-46]의 최근 발전은 놀라운 새로운 능력을 보여주었습니다. 여기서는 LLM을 에이전트로 사용할 수 있도록 하는 몇 가지 능력을 나열합니다. a) 계획: 계획은 복잡한 목표를 하위 목표로 나누고 발행된 조치와 환경 피드백을 기반으로 자기 반성을 하는 것을 포함합니다. Chain-of-thought[27]은 LLM이 복잡한 작업을 더 작은 작업으로 분해하여 &quot;단계적으로 생각&quot;하도록 지시받았을 때 더 나은 계획 능력을 보였다는 것을 보여줍니다. Tree-of-thoughts[28]는 단계당 여러 생각을 탐구하여 사슬을 트리로 바꾸어 이 접근 방식을 확장합니다. [47-50]은 LLM이 출력과 환경 피드백에 대해 자기 반성하도록 지시받았을 때 더 나은 출력을 생성할 수 있음을 보여줍니다. b) 도구 사용: 도구를 사용하는 능력은 인간 지능의 고유한 특징입니다. 현재 LLM이 모든 작업(예: 수학과 사실적 질의응답 문제)에 능숙하지 않다는 것을 인식한 연구자들은 LLM이 작업을 완료하기 위해 도구 사용을 조율하도록 하는 가능성을 탐구했습니다. 도구 사용 문제의 핵심은 어떤 도구를 사용할지, 언제 사용할지 결정하는 것입니다. Socratic Models[29]는 자연어를 매체로 사용하여 LLM 에이전트가 시각 언어 모델 및 오디오 언어 모델과 같은 다른 다중 모드 언어 모델과 함께 안내된 토론에 참여하여 작업을 집단적으로 완료하도록 합니다. MRKL[51] 및 TAML[52]은 LLM에 계산기를 장착하고 수학 문제를 푸는 능력이 향상되었음을 보여줍니다. 이러한 결과를 바탕으로 LangChain[53]과 같은 소프트웨어 라이브러리가 개발되어 개발자를 위해 LLM 도구 사용을 간소화했습니다. ToolFormer[30], HuggingGPT[54] 및 API-Bank[55]는 더 많은 API와 머신 러닝 모델을 LLM이 사용할 도구로 개방하여 도구 사용을 더욱 확대합니다. 로봇공학에서 SayCan[31], InnerMonologue[32], Code as Policies[33] 및 LM-Nav[34]는 LLM의 계획 및 도구 사용 기능을 사용하여 장기적이고 복잡한 작업을 위한 실제 로봇의 고급 컨트롤러 역할을 하도록 합니다. 이러한 작업에서 얻은 성공은 LLM을 에이전트로 사용하여 3D 시각적 접지에서 제시된 구성적 언어-시각 이해 과제를 해결하는 데 동기를 부여합니다.III. 방법 최근 Auto-GPT[56], GPTEngineer[57] 및 ToolFormer[30]의 성공 사례는 LLM을 에이전트로 사용하는 데 있어 초기 성공 징후를 보여줍니다.에이전트는 목표에 의해 주도되고 목표에 대한 추론을 하고 계획을 세우고 도구를 조사하고 사용하며 환경과 상호 작용하고 환경으로부터 피드백을 수집한다는 점에서 기계 학습의 기존 모델과 다릅니다.3D 시각적 접지 설정에서 에이전트는 기존 모델이 보이는 &quot;단어 가방&quot; 동작에 대한 유망한 솔루션이 될 수 있습니다. LLM-Grounder에서는 GPT-4를 에이전트로 사용하여 세 가지 작업을 완료하도록 합니다. 1. 복잡한 텍스트 쿼리를 OpenScene 및 LERF와 같은 CLIP 기반 3D 시각적 그라운더와 같은 다운스트림 도구가 더 잘 처리할 수 있는 하위 작업으로 분해합니다. 2. 이러한 도구를 조율하고 사용하여 제안된 하위 작업을 해결합니다. 3. 환경으로부터 피드백에 대한 추론은 Training Size Open-Vocab Method Visual Grounder + LLM Agent Acc@0.25 ↑ Acc@0.5 ↑ 36k 레이블이 지정된 3D 텍스트 데이터 closed-vocab ScanRefer[12] 3DVG-Trans[2] 34.20.41.28.zero-shot open-vocab LERF[35] Ours LERF LERF 4.0.GPT-6.9 (+2.5) 1.6 (+1.3) zero-shot open-vocab OpenScene[1] Ours Ours OpenScene OpenScene OpenScene 13.5.✓ GPT-3.✓ GPT-14.3 (+1.3) 17.1 (+4.1) 4.7 (-0.4) 5.3 (+0.2) 표 I: ScanRefer에 대한 실험 결과. LLM(GPT-4) 에이전트는 LERF 및 OpenScene과 같은 제로샷 오픈 어휘 3D 그라운더의 3D 그라운딩 기능을 크게 증가시킵니다. 우리는 그라운딩 기능을 Accuracy@0.and @0.5로 측정하는데, 이는 교차-연합(IoU) wrt 그라운드-트루스 박스가 각각 0.25와 0.5를 초과하는 바운딩 박스 예측의 정확도입니다. 괄호 안의 숫자는 LLM 에이전트를 추가한 후의 성능 이득 또는 손실을 나타냅니다. 또한 GPT-3.5와 같은 덜 강력한 LLM은 강력한 그라운딩 기능 이득을 달성할 수 없음을 결과에 보여줍니다. 마지막으로, 제로샷 오픈 어휘인 우리의 방법과 직접 비교할 수는 없지만, 완전성을 위해 ScanRefer와 폐쇄 어휘에서 훈련된 방법에 대한 성능이 나열되어 있습니다. LERF 낮은 시각적 난이도 LLM 없음 LLM 있음 10.15.1 (+4.3) LLM 없음 LLM 있음 2.4.4 (1.9) 높은 시각적 난이도 OpenScene 27.33.6 (+6.0) 8.12.1 (+3.5) 표 II: 시각적 복잡성에 대한 소거 연구. LLM 에이전트는 낮은 시각적 난이도 설정에서 3D 접지에 더 효과적입니다. 표시된 숫자는 Acc@.25입니다. 접지 결정을 내리기 위한 공간 이해와 상식. 계획. LLM의 첫 번째 장점은 계획하는 능력입니다. 연구에 따르면 사고의 사슬 추론[27], 즉 LLM이 복잡한 목표를 더 작은 하위 작업으로 분해하도록 명시적으로 촉구하는 것(&quot;단계적으로 생각하기&quot;)은 산술, 상식 및 기호 추론 작업에 도움이 될 수 있습니다. 이러한 발견에서 영감을 얻어 그림 2에 나와 있는 것과 같이 에이전트를 설계합니다. 구체적으로, 먼저 에이전트에게 관찰 내용을 설명하도록 요청하여 에이전트가 현재 상황을 요약할 수 있는 기회를 제공합니다. 맥락에는 인간 텍스트 쿼리와 도구에서 반환된 정보(아래 설명)가 포함될 수 있습니다. 그런 다음 에이전트는 추론이라는 섹션을 시작하는데, 이는 에이전트가 고수준 계획을 수행하는 데 필요한 정신적 스크래치패드 역할을 합니다. 그런 다음 계획 섹션에서 에이전트는 도구 사용, 비교 또는 계산을 포함하여 고수준 계획을 이행하기 위한 보다 구체적인 단계를 나열해야 합니다. 에이전트는 자기 비판 섹션에서 생성된 계획을 반영하고 최종 수정을 할 수 있습니다[50]. 도구 사용. LLM의 두 번째 장점은 도구를 사용할 수 있는 능력에서 비롯됩니다. LLM 에이전트에게 도구를 사용하여 &quot;단어 가방&quot; 동작을 해결하도록 지시합니다(제2절). 그림 2에서 보듯이, 시각적 접지 및 피드백을 위해 설계한 두 도구의 예상 입력 및 출력 형식, 즉 API를 LLM에 알리고 LLM 에이전트에게 주어진 형식에 따라 상호 작용하도록 요청합니다. 도구에는 대상 파인더와 랜드마크 파인더가 포함됩니다. 대상 파인더와 랜드마크 파인더. 대상 파인더와 랜드마크 파인더는 텍스트 쿼리 입력을 받고 쿼리에 대한 가능한 위치 클러스터의 경계 상자를 찾고 중심과 크기(Cx, Cy, Cz, AX, AY, AZ)의 형태로 후보 경계 상자 목록을 반환합니다. 대상은 사용자가 쿼리에서 참조하는 주요 객체(식탁과 창문 사이의 의자에서 &quot;의자&quot;)입니다. 랜드마크는 대상을 공간적으로 참조하는 데 사용되는 객체(&quot;식탁&quot;과 &quot;창문&quot;)입니다. 대상 파인더는 또한 각 후보의 볼륨을 계산하고 랜드마크 파인더는 또한 각 대상 후보의 중심에서 랜드마크의 중심까지의 유클리드 거리를 계산합니다. 볼륨, 거리 및 경계 상자는 LLM 에이전트가 공간 및 상식적 추론을 수행하기 위한 피드백을 함께 제공합니다. 예를 들어, 볼륨이 0.01m³만큼 작은 후보 &quot;의자&quot;는 아마도 거짓 양성일 것이고 필터링되어야 합니다. 랜드마크와의 거리가 쿼리에서 언급한 공간 관계를 준수하지 않는 후보는 거부되어야 합니다. 타겟 파인더와 랜드마크 파인더는 오픈 어휘 CLIP 기반 3D 시각적 그라운더 LERF [35] 및 OpenScene [1]에 의해 구현됩니다. 이러한 도구만으로는 복잡한 텍스트 쿼리가 주어졌을 때 &quot;단어 가방&quot; 동작(Sec. I)을 보입니다. 그러나 간단한 명사구(&quot;의자&quot;)와 같은 더 간단한 텍스트 쿼리가 주어졌을 때 이러한 도구는 일반적으로 잘 작동할 수 있습니다. LLM 에이전트는 이러한 3D 시각적 그라운더의 명사구 그라운딩 기능을 활용하면서 복잡한 그라운딩 쿼리를 분해하고 한 번에 하나의 객체를 그라운딩하고 나중에 공간 관계에 대해 추론함으로써 언어 이해와 공간 추론의 약점을 보완합니다. 타겟 파인더를 사용하기 위해 LLM 에이전트에게 원래 자연어 쿼리에서 명사 구(예: &quot;나무 의자&quot;)를 구문 분석하도록 지시합니다. 랜드마크 파인더를 사용하기 위해 LLM 에이전트에게 원래 쿼리에서 언급된 랜드마크 객체와 타겟 객체와의 공간 관계를 구문 분석하도록 지시합니다. IV. 실험 실험에서 먼저 LLM 기반 에이전트가 CLIP 기반 3D 시각적 접지 방법과 비교하여 제로샷 오픈 어휘 3D 시각적 접지를 얼마나 잘 개선하는지 평가하고 싶습니다. 그런 다음 폐쇄 어휘 설정에서 방법을 평가하고 폐쇄 어휘 및 훈련된 접근 방식과 비교합니다. 마지막으로 접근 방식의 일반화를 보여주기 위해 야생 장면에서 몇 가지 정성적 예를 보여줍니다. A. 데이터 집합 ScanRefer. ScanRefer[12]는 자연어를 사용하여 실내 3D 장면에서 3D 객체 현지화에 대한 벤치마크입니다. 18개의 11,개 객체에 대한 51,583개의 인간이 쓴 설명으로 구성되어 있습니다. 영어: 800개의 ScanNet[37] 3D 장면의 의미 범주, 여기서 train/val/test 분할에는 각각 36,665개, 9,508개 및 5,410개의 설명이 포함됩니다. 표 I에 제시된 실험에 대한 검증 분할의 첫 번째 장면을 사용하며, 이는 998개의 텍스트-3D-객체 쌍으로 구성됩니다. 또한 ScanRefer의 두 가지 표준 메트릭인 Accuracy@0.25 및 Accuracy@0.5를 보고합니다. 0.25와 0.5는 3D 경계 상자의 IoU에 대한 다른 임계값입니다. B. 기준 방법 ScanRefer. ScanRefer[12]는 종단 간 3D 텍스트 신경망 아키텍처를 사용하여 자연어 입력이 주어졌을 때 객체를 지역화합니다. 구체적으로 3D 포인트 클라우드를 PointNet++[58] 기능으로 처리한 다음 포인트를 클러스터링하고 객체의 경계 상자를 제안합니다. 그런 다음 언어 기능을 클러스터 및 상자와 융합하여 언어에서 참조하는 상자를 결정합니다.파이프라인은 텍스트 및 b-상자 쌍과 장면의 모든 객체에 대한 기준 진실 b-상자 및 의미 클래스를 감독합니다.이 기준선을 현재 훈련된 파이프라인의 성능을 보여주는 것으로 포함시켜 감독이 사용되지 않는 제로샷 설정과 비교한 한계로 사용합니다.3DVG-Transformer.3DVG-Transformer[2]는 ScanRefer의 종단 간 신경 구조를 기반으로 하며 경계 상자를 제안하기 전에 근처 클러스터를 집계하는 새로운 신경 모듈을 제안합니다.ScanRefer와 유사하게 3DVGTransformer도 기준 진실 객체 b상자, 의미 클래스 및 인간이 주석을 단 설명의 감독을 사용합니다.OpenScene 및 LERF.OpenScene[1] 및 LERF[35]는 제로샷 오픈 어휘 3D 장면 이해 접근 방식입니다. OpenScene은 2D CLIP 기능을 3D 포인트 클라우드로 추출하고, 쿼리의 CLIP 텍스트 임베딩과 3D 포인트 클라우드의 모든 포인트 간의 코사인 유사도를 계산하여 텍스트 쿼리로 접지를 허용합니다. LERF는 신경 복사장에 CLIP 임베딩을 인코딩하여 동일한 것을 달성합니다. 이러한 방법은 단독으로 사용될 때 1에서 설명한 대로 &quot;단어 가방&quot; 동작을 보이는데, 이는 LLM 에이전트 심의 추론으로 해결하고자 하는 문제입니다. 3D 시각적 접지 벤치마크 ScanRefer에 OpenScene과 LERF를 사용하여 경계 상자를 생성하기 위해 코사인 유사도가 높은 지점에 DBSCAN 클러스터링[59]을 적용하고 그 주위에 경계 상자를 그립니다. C. 결과 먼저 그림 3에서 LLM-Grounder의 정성적 결과를 보여줍니다. 더 많은 결과와 데모는 프로젝트 웹사이트¹에서 찾을 수 있으며, 여기에는 실제 장면도 포함됩니다. 기준선과 비교했을 때 LLM 에이전트가 제로 샷, 오픈 어휘 접지를 개선할 수 있다는 것을 발견했습니다. 표 I에서 볼 수 있듯이 LLM 에이전트를 추가하면 각각 정확도@0.25에서 5.0%와 17.1%를 달성하여 LERF와 OpenScene의 접지 성능을 크게 높일 수 있습니다. 우리는 LERF의 성능 증가가 낮은 것은 LERF의 전반적인 접지 기능이 약하기 때문입니다. 낮은 증가는 도구가 LLM 에이전트에 너무 노이즈가 많은 피드백을 제공할 때 LLM 에이전트가 노이즈가 많은 입력으로 추론하고 성능을 개선하기 어렵다는 것을 시사합니다. 또한 예측된 b-박스가 접지 진실 박스와 50% 이상 겹치도록 요구하는 Accuracy@0.5에서 성능이 낮게 증가한 것을 알 수 있습니다. 이는 기본 접지기의 인스턴스 분할 기능이 부족하기 때문입니다. 접지기는 종종 올바르게 접지된 객체에 대해 너무 크거나 너무 작은 경계를 예측하는 것을 관찰합니다. 이러한 예측은 LLM에서 수정할 수 없으므로 정확한 시각적 접지가 어렵고 성능이 낮습니다. 또한 OpenScene의 에이전트로 GPT-3.5를 사용할 때 성능이 GPT 없이 사용할 때보다 떨어집니다. 이는 GPT-3.5의 도구 사용 및 공간 및 상식적 추론 기능이 약하기 때문입니다. D. 절제 연구 그런 다음 LLM 에이전트가 주로 무엇을 개선하는지 평가합니다. 두 가지 다른 설정을 테스트합니다. (1) LLM 에이전트가 더 어려운 시각적 맥락에서 더 많은 도움이 됩니까? (2) LLM 에이전트가 더 어려운 텍스트 쿼리에서 더 많은 도움이 됩니까? 시각적 맥락의 난이도. 표 II에서 결과를 시력 난이도에 따라 분류하고 LLM 에이전트가 낮은 시력 난이도 쿼리에 더 효과적이라는 것을 발견했으며, 이는 더 높은 접지 성능 증가로 입증됩니다. 구체적으로 접지 쿼리를 낮은 시각적 난이도와 높은 시각적 난이도 범주로 구분합니다. 텍스트 쿼리에서 언급된 객체가 장면에서 해당 클래스의 유일한 객체(0개의 방해 객체)인 경우 쿼리는 낮은 시각적 난이도입니다. 장면에서 같은 클래스의 방해 객체가 두 개 이상 있는 경우 쿼리는 높은 시각적 난이도입니다. 평가한 998개의 쿼리 중 232개의 쿼리는 낮은 시각적 난이도였고 766개의 쿼리는 높은 시각적 난이도였습니다. 표 II의 결과는 LLM이 낮은 시각적 난이도 쿼리에 대해 더 많은 성능 증가를 가져온다는 것을 보여줍니다. 이러한 동작은 낮음 및 높음 시각적 난이도 설정에서 제시되는 다양한 과제로 설명할 수 있습니다. 낮은 시각적 난이도 설정에서 오픈 어휘 3D 그라운더가 직면하는 주요 과제는 &quot;단어 가방&quot; 동작입니다. 예를 들어, 텍스트 쿼리가 &quot;주방의 싱크대&quot;이고 장면에 싱크대가 하나만 있는 경우 단어 가방 그라운더는 전체 주방을 강조하여 그라운딩 정확도가 낮아집니다. LLM 에이전트는 https://chat-with-nerf.github.io/에서 이 문제를 해결하는 데 특히 능숙합니다. &quot;책상 왼쪽에 밝은 회색 모니터가 있습니다. 책상은 더 작고 휘어져 있습니다.&quot; LLM 에이전트 대상 찾기 대상 찾기를 사용하여 밝은 회색 모니터라는 문구로 대상을 그라운딩합니다. 랜드마크 찾기를 사용하여 창문 옆의 더 작은 곡선 책상이라는 문구로 랜드마크를 그라운딩합니다. 대상과의 관계는 아래에 있습니다. 랜드마크 찾기 LLM 에이전트 모니터는 책상의 왼쪽에 있어야 하므로 중심의 x 값은 랜드마크의 x 값보다 작아야 합니다. 그리고 각 후보에 대한 바운딩 박스와 볼륨을 받았습니다. 지금까지 가장 잘 맞는 후보를 선택했습니다.그림 3: 정성적 예.LLM 에이전트는 공간 추론을 사용하여 올바른 객체 인스턴스를 성공적으로 모호하지 않게 합니다.LLM 사용 - LLM 미사용 10%R^2 OpenScene = 0.8%- R^2 LERF = 0.OpenScene LERF 5% 2% 0% -2% -5%-8%문장의 명사 수그림 4: 성능 델타(LLM 사용 - LLM 미사용) 대 쿼리 텍스트 복잡성.텍스트 쿼리가 더 복잡할수록 LLM이 더 도움이 되지만 복잡도가 더 높으면 크게 도움이 되지 않습니다. Avg Acc@0.25loU R (OpenScene LLM) = -0.35% 30%R (OpenScene noLLM) = -0.25% R (LERF LLM) = -0.*R (LERF noLLM) = -0.20% 15% 10% 5% 0%OpenScene w/ LLM OpenScene w/o LLM LERF w/ LLM LERF w/o LLM문장의 명사 수그림 5: 다양한 모델의 성능 대 쿼리 텍스트 복잡성. 모든 모델은 더 복잡한 문장에서 어려움을 겪지만 LLM 에이전트가 있는 모델은 특히 이러한 더 높은 복잡성에서 더 나은 성과를 보입니다. 대상 객체 &quot;sink&quot;를 구문 분석하고 이 단일 명사만 grounder에 발행하여 bag-ofwords 동작을 우회합니다. 그러나 높은 시각적 난이도 설정의 경우 인스턴스 모호성 해소라는 추가 과제가 하나 있습니다. 장면에 동일한 클래스의 인스턴스가 여러 개 있기 때문에 시각적 grounder는 많은 후보를 LLM 에이전트에 반환합니다. LLM 에이전트는 공간 및 상식적 추론 기능을 사용하여 볼륨과 랜드마크 정보까지의 거리가 있는 일부 인스턴스를 필터링할 수 있지만 보다 복잡한 인스턴스 모호성 해소에는 일반적으로 보다 미묘한 시각적 단서가 필요한데, LLM 에이전트는 시각 장애가 있기 때문에 이러한 특권이 없습니다. 텍스트 쿼리의 난이도. 쿼리가 더 복잡해질수록 LLM 에이전트는 성능을 향상시키지만 어느 정도까지만 가능합니다. 문장에 있는 명사의 수를 세어 쿼리 복잡성을 측정할 수 있습니다. 설명에 명사가 많을수록 특정 객체를 접지하기가 더 어려워집니다. 그림 5에서 LLM 에이전트의 도움이 있든 없든 문장 복잡성이 증가함에 따라 성능이 감소하는 것을 볼 수 있습니다. 그러나 LLM 에이전트를 사용하고 사용하지 않는 것의 성능 차이를 분석한 결과, 쿼리 복잡성에 2차 종속성이 있음을 알 수 있습니다(그림 4). 이는 LLM이 더 높은 복잡성의 쿼리가 제시될 때 접지에 이점을 제공하지만, 특정 임계값에 도달한 후에는 성능 이점이 감소함을 시사합니다. 쿼리 복잡성이 낮을 때 LLM이 없는 모델은 객체를 효과적으로 접지할 수 있으므로 LLM은 최소한의 이점을 제공합니다. 복잡성이 증가함에 따라 기준 모델의 성능이 떨어지고 LLM은 더 큰 이점을 제공합니다. 그러나 참조 표현의 복잡성이 증가함에 따라 LLM의 공간 추론 기능은 noLLM 기준의 성능을 능가하지 못할 수 있습니다. 이러한 더 높은 복잡성 범위에서 이점을 얻으려면 더 강력한 LLM이 필요할 수 있습니다. V. 결론 및 제한 사항 우리는 접지 프로세스를 조율하기 위한 중심 에이전트로 대규모 언어 모델(LLM)을 활용하는 3D 시각적 접지를 위한 새로운 접근 방식인 LLM-Grounder를 소개했습니다. 경험적 평가에 따르면 LLM-Grounder는 복잡한 텍스트 쿼리를 처리하는 데 특히 뛰어나 3D 시각적 접지 작업을 위한 견고하고, 제로샷, 오픈 어휘 솔루션을 제공합니다. 그러나 고려해야 할 몇 가지 제한 사항이 있습니다. 비용: GPT 기반 모델을 핵심 추론 에이전트로 활용하는 것은 계산적으로 비용이 많이 들 수 있으며, 이는 리소스가 제한된 환경에서의 배포를 제한할 수 있습니다. 지연: GPT 모델의 고유한 지연으로 인해 추론 프로세스가 느릴 수 있습니다. 이 지연은 빠른 의사 결정이 중요한 실시간 로봇 애플리케이션에 상당한 병목 현상이 될 수 있습니다. 이러한 제한에도 불구하고 LLM-Grounder는 3D 시각적 접지에서 새로운 벤치마크를 설정하고 LLM을 로봇 시스템과 통합하는 미래 연구의 길을 열어줍니다. 참고문헌 [1] S. Peng, K. Genova, CM Jiang, A. Tagliasacchi, M. Pollefeys, T. Funkhouser, &quot;Openscene: 오픈 어휘를 사용한 3차원 장면 이해&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식(CVPR) 컨퍼런스 논문집, 2023. [2] L. Zhao, D. Cai, L. Sheng, D. Xu, &quot;3DVG-Transformer: 포인트 클라우드의 시각적 접지를 위한 관계 모델링&quot;, ICCV, 2021, 2928-2937쪽. [3] J. Roh, K. Desingh, A. Farhadi, D. Fox, &quot;Languagerefer: 3차원 시각적 접지를 위한 공간 언어 모델&quot;, 로봇 학습 컨퍼런스. PMLR, 2022, 1046-1056쪽. [4] D. Cai, L. Zhao, J. Zhang, L. Sheng, D. Xu, “3djcg: 3d 포인트 클라우드에서 공동 고밀도 캡션 및 시각적 접지를 위한 통합 프레임워크,&quot; IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2022, pp. 16464–16473. [5] J. Chen, W. Luo, R. Song, X. Wei, L. Ma, W. Zhang, “3d 시각적 접지를 위한 포인트-언어 계층적 정렬 학습,&quot; 2022. [6] DZ Chen, Q. Wu, M. Nießner, AX Chang, “D3net: rgb-d 스캔에서 반지도 고밀도 캡션 및 시각적 접지를 위한 화자 청취자 아키텍처,&quot; 2021. [7] Z. Yuan, X. Yan, Y. Liao, R. 영어: Zhang, S. Wang, Z. Li 및 S. Cui, &quot;Instancerefer: 인스턴스 다중 레벨 문맥적 참조를 통한 포인트 클라우드의 시각적 접지를 위한 협력적 전체론적 이해&quot;, IEEE/CVF 컴퓨터 비전 국제 컨퍼런스 논문집, 2021, pp. 1791-1800. [8] E. Bakr, Y. Alsaedy 및 M. Elhoseiny, &quot;주변을 둘러보고 참조: 3차원 시각적 접지를 위한 2차원 합성 의미론 지식 증류&quot;, Advances in Neural Information Processing Systems, vol. 35, pp. 37 146-37 158, 2022. [9] H. Liu, A. Lin, X. Han, L. Yang, Y. Yu, 및 S. Cui, &quot;Refer-it-in-rgbd: rgbd 이미지에서 3D 시각적 접지를 위한 하향식 접근 방식&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2021, pp. 6032–6041. [10] A. Jain, N. Gkanatsios, I. Mediratta, 및 K. Fragkiadaki, &quot;이미지 및 포인트 클라우드의 언어 접지를 위한 하향식 감지 변환기&quot;, 유럽 컴퓨터 비전 컨퍼런스.Springer, 2022, pp. 417-433. [11] S. Huang, Y. Chen, J. Jia, 및 L. Wang, &quot;3D 시각적 접지를 위한 다중 시점 변환기&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 논문집, 2022, pp. 15 524-15 533. [12] DZ Chen, AX Chang, 및 M. Nießner, &quot;Scanrefer: 자연어를 사용한 rgb-d 스캔에서 3D 개체 위치 파악&quot;, 제16회 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2020. [13] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, 및 LJ Guibas, &quot;ReferIt3D: 실제 장면에서 세밀한 3D 객체 식별을 위한 신경 청취기&quot;, 제16회 유럽 컴퓨터 비전 컨퍼런스(ECCV), 2020. [14] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, MS Ryoo, A. Stone, 및 D. Kappler, &quot;실제 세계 계획을 위한 오픈 어휘 쿼리 가능 장면 표현&quot;, 2023 IEEE 로봇 및 자동화 국제 컨퍼런스(ICRA). IEEE, 2023, pp. 11 509-11 522. [15] R. Ding, J. Yang, C. Xue, W. Zhang, S. Bai, and X. Qi, &quot;Pla: 언어 기반 개방형 어휘 3D 장면 이해&quot;, IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 2023, pp. 7010-7019. [16] SY Gadre, M. Wortsman, G. Ilharco, L. Schmidt, and S. Song, &quot;목초지의 소: 언어 기반 제로샷 객체 탐색을 위한 기준선 및 벤치마크&quot;, CVPR, 2023. [17] C. Huang, O. Mees, A. Zeng, and W. Burgard, &quot;로봇 탐색을 위한 시각적 언어 맵&quot;, 2023 IEEE 국제 로봇 및 자동화 컨퍼런스 (ICRA). IEEE, 2023, pp. 10608-10615. [18] KM Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, A. Maalouf, S. Li, GS Iyer, S. Saryazdi, NV Keetha 외, &quot;Conceptfusion: 오픈 세트 멀티모달 3D 매핑,&quot; ICRAWorkshop on Pretraining for Robotics(PT4R), 2023. [19] K. Mazur, E. Sucar, AJ Davison, &quot;실시간 오픈 세트 장면 이해를 위한 특징적 사실적 신경 융합,&quot; 2023 IEEE 국제 로봇 및 자동화 컨퍼런스(ICRA). IEEE, 2023, pp. 8201-8207. [20] NMM Shafiullah, C. Paxton, L. Pinto, S. Chintala, 및 A. Szlam, &quot;클립 필드: 로봇 메모리를 위한 약한 감독 의미 필드&quot;, ICRA2023 로봇 사전 훈련 워크숍(PT4R), 2023. [21] A. Takmaz, E. Fedele, RW Sumner, M. Pollefeys, F. Tombari, 및 F. Engelmann, &quot;OpenMask3D: 오픈 어휘 3D 인스턴스 분할&quot;, arXiv 사전 인쇄본 arXiv:2306.13631, 2023. [22] H. Ha 및 S. Song, &quot;의미 추상화: 2D 비전 언어 모델에서 오픈 월드 3D 장면 이해&quot;, 제6회 로봇 학습 연례 컨퍼런스, 2022. [온라인]. 사용 가능: https://openreview.net/forum?id=1V-rNbXVSaO [23] Y. Hong, C. Lin, Y. Du, Z. Chen, JB Tenenbaum, and C. Gan, &quot;3d concept learning and reasoning from multi-view images,&quot; IEEE/CVF Conference on Computer Vision and Pattern Recognition의 회의록, 2023. [24] A. Radford, JW Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., &quot;Learning transferable visual models from natural language supervisor,&quot; in International conference on machine learning. PMLR, 2021, pp. 8748-8763. [25] M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou, &quot;When 영어: 그리고 왜 시각 언어 모델이 단어 모음처럼 동작하며, 이에 대해 무엇을 할 수 있을까요?&quot; 제11회 국제 학습 표현 컨퍼런스, 2022. [26] OpenAI, &quot;Gpt-4 기술 보고서,&quot; 2023. [27] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, QV Le, D. Zhou et al., &quot;사고의 사슬 프롬프트는 대규모 언어 모델에서 추론을 이끌어냅니다.&quot; 신경 정보 처리 시스템의 발전, 제11권, 2호, 2022년 11월. 35, pp. 24 824-24 837, 2022. [28] S. Yao, D. Yu, J. Zhao, I. Shafran, TL Griffiths, Y. Cao, and K. Narasimhan, &quot;생각의 나무: 대규모 언어 모델을 통한 의도적인 문제 해결,&quot; arXiv 사전 인쇄본 arXiv:2305.10601, 2023. [29] A. Zeng, M. Attarian, KM Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, MS Ryoo, V. Sindhwani, J. Lee et al., &quot;소크라테스 모델: 언어로 제로샷 다중 모드 추론 구성,&quot; 제11회 학습 표현 국제 컨퍼런스, 2022. [30] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, &quot;Toolformer: Language models can teach myself to use tools,&quot; 2023. [31] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman et al., &quot;Do as i can, not as i say: Grounding language in robotic affordances,&quot; arXiv 사전 인쇄본 arXiv:2204.01691, 2022. [32] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar et al., &quot;Inner monologue: Embodied reasoning through planning with language models,&quot; in Conference on Robot 학습. PMLR, 2023, pp. 1769–1782. [33] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, A. Zeng, &quot;정책으로서의 코드: 구체화된 제어를 위한 언어 모델 프로그램&quot;, 2023 IEEE 로봇 및 자동화 국제 컨퍼런스(ICRA). IEEE, 2023, pp. 9493-9500. [34] D. Shah, B. Osinski, B. Ichter, S. Levine, &quot;LM-nav: 언어, 비전 및 동작의 대규모 사전 훈련된 모델을 사용한 로봇 탐색&quot;, 제6회 로봇 학습 연례 컨퍼런스, 2022. [온라인]. 사용 가능: https://openreview.net/forum?id=UW5A3SweAH [35] J. Kerr, CM Kim, K. Goldberg, A. Kanazawa, M. Tancik, &quot;Lerf: 언어가 포함된 광도장&quot;, 국제 컴퓨터 비전 컨퍼런스(ICCV), 2023. [36] J. Hsu, J. Mao, J. Wu, &quot;Ns3d: 3d 객체 및 관계의 신경 상징적 근거&quot;, 2023 IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스(CVPR), pp. 2614-2623, 2023. [온라인]. 사용 가능: https://api.semanticscholar.org/CorpusID:[37] A. Dai, AX Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nieẞner, &quot;Scannet: Richly-annotated 3d reconstructions of indoor scenes,&quot; Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. [38] B. Li, KQ Weinberger, S. Belongie, V. Koltun, and R. Ranftl, &quot;Language-driven semantic segmentation,&quot; International Conference on Learning Representations, 2022. [온라인]. 사용 가능: https://openreview.net/forum?id=RriDjddCLN [39] G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin, &quot;이미지 수준 레이블을 사용한 오픈 어휘 이미지 분할 확장&quot;, European Conference on Computer Vision에서. Springer, 2022, 540-557쪽. [40] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang, P. Vajda, D. Marculescu, &quot;마스크 적응 클립을 사용한 오픈 어휘 의미 분할&quot;, IEEE/CVF Conference on Computer Vision and Pattern Recognition 회의록, 2023, 7061-7070쪽. [41] S. Kobayashi, E. Matsumoto, V. Sitzmann, &quot;특징 필드 증류를 통한 편집을 위한 nerf 분해&quot;, Advances in Neural Information Processing Systems, vol. 35, 2022. [온라인]. 영어: 사용 가능: https://arxiv.org/pdf/2205.15585.pdf [42] T. Brown, B. Mann, N. Ryder, M. Subbiah, JD Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., &quot;언어 모델은 소수 학습자입니다.&quot; 신경 정보 처리 시스템의 발전, 제 4권. 33, pp. 1877-1901, 2020. [43] J. Wei, M. Bosma, V. Zhao, K. Guu, AW Yu, B. Lester, N. Du, AM Dai, QV Le, &quot;Finetuned language models are zero-shot learners,&quot; 국제 학습 표현 컨퍼런스, 2021. [44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., &quot;인간의 피드백을 통해 지시를 따르도록 언어 모델 훈련하기&quot;, Advances in Neural Information Processing Systems, vol. 35, pp. 27 730-27744, 2022. [45] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and PJ Liu, &quot;통합 텍스트-텍스트 변환기를 사용한 전이 학습의 한계 탐구,&quot; The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. [46] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., &quot;Llama: 개방적이고 효율적인 기초 언어 모델,&quot; arXiv 사전 인쇄본 arXiv:2302.13971, 2023. [47] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao, &quot;React: 언어 모델에서 추론과 행동의 시너지 효과,&quot; arXiv 사전 인쇄본 arXiv:2210.03629, 2022. [48] N. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, 및 S. Yao, &quot;Reflexion: 언어적 강화 학습을 갖춘 언어 에이전트,&quot; arXiv 사전 인쇄본 arXiv:2303.11366, 2023. [49] H. Liu, C. Sferrazza, 및 P. Abbeel, &quot;후견의 사슬은 언어 모델을 피드백과 일치시킵니다,&quot; arXiv 사전 인쇄본 arXiv:2302.02676, vol. 3, 2023. [50] E. Jang, &quot;Ilms가 자체 출력을 비판하고 반복할 수 있습니까?&quot; evjang.com, 2023년 3월. [온라인]. 사용 가능: https://evjang.com/2023/03/26/self-reflection.html [51] E. Karpas, O. Abend, Y. Belinkov, B. Lenz, O. Lieber, N. Ratner, Y. Shoham, H. Bata, Y. Levine, K. Leyton-Brown et al., “Mrkl systems: A modules, neuro-symbolic architecture that combine large language models, external knowledge sources and discrete reasoning,&quot; arXiv preprint arXiv:2205.00445, 2022. [52] A. Parisi, Y. Zhao, and N. Fiedel, &quot;Talm: Tool augmented language models,&quot; arXiv preprint arXiv:2205.12255, 2022. [53] H. Chase, “LangChain,” 2022년 10월. [온라인]. 사용 가능: https: //github.com/hwchase17/langchain [54] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, 및 Y. Zhuang, &quot;Hugginggpt: huggingface에서 chatgpt 및 그 친구들을 사용하여 AI 작업 해결,&quot; arXiv 사전 인쇄본 arXiv:2303.17580, 2023. [55] M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, 및 Y. Li, &quot;Api-bank: 도구 증강 LLM을 위한 벤치마크,&quot; arXiv 사전 인쇄본 arXiv:2304.08244, 2023. [56] &quot;Auto-gpt,&quot; https://github.com/Significant-Gravitas/Auto-GPT, 2013. [57] &quot;Gpt-engineer,&quot; 한국어: https://github.com/AntonOsika/gpt-engineer, 2013. [58] CR Qi, L. Yi, H. Su, 및 LJ Guibas, “Pointnet++: 측정 공간의 포인트 집합에 대한 심층 계층적 특징 학습,&quot; 신경 정보 처리 시스템의 발전, vol. 30, 2017. [59] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “노이즈가 있는 대규모 공간 데이터베이스에서 클러스터를 발견하기 위한 밀도 기반 알고리즘,&quot; kdd, vol. 96, no. 34, 1996, pp. 226–231.
