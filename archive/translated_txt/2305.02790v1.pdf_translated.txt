--- ABSTRACT ---
최근 DeepNorm은 Transformers를 매우 깊게(즉, 1000개 레이어) 확장하고 깊은 확장의 유망한 잠재력을 보여줍니다. 깊은 모델의 학습을 안정화하기 위해 DeepNorm(Wang et al., 2022a)은 모델 업데이트를 일정한 값으로 제한하려고 시도합니다. 이러한 제약 조건을 적용하면 모델 학습의 초기 단계에 도움이 될 수 있지만 전체 학습 절차 동안 모델이 충분히 학습되지 않을 수 있습니다. 이 논문에서는 Transformer의 비잔류 분기를 학습 기간에 따라 동적으로 재조정하는 BranchNorm을 제안합니다. BranchNorm은 이론적으로 초기 단계에서 부드러운 그래디언트 규범으로 학습을 안정화할 뿐만 아니라 후속 학습 단계에서 더 나은 수렴을 촉진합니다. 여러 변환 작업에 대한 실험 결과는 BranchNorm이 학습 안정성과 수렴 성능 사이에서 더 나은 균형을 이룬다는 것을 보여줍니다. 1
--- METHOD ---
BranchNorm. 3.1 기울기의 관점 불균형한 기울기는 주로 Transformer³의 불안정성에 책임이 있습니다(Wang et al., 2019; Shleifer et al., 2021; Zhu et al., 2021). 먼저 Wang et al.(2019)에 따라 기울기와 모델 깊이 간의 관계를 살펴봅니다. L개의 하위 계층과 학습 손실 E가 있는 Transformer가 주어지면 1번째 하위 계층의 기울기는 체인 규칙4에 따라 계산됩니다. 크거나 매우 작아서 기울기가 사라지거나 폭발할 수 있습니다. 마찬가지로, 우리는 방정식 (2)에 기반하여 DeepNorm의 기울기를 분석하고 1번째 하위 계층의 기울기가 다음과 같이 계산됨을 얻습니다.기약 L-θε θε δαι მxL, II OLN (axk + F (xk; 01)) a (axk + F (xk; 01)) × k=l LN JF (xk; k) дхк L-II k=l + residual (5) DeepNorm에서 a는 모델 깊이에 따라 증가하고 학습 안정성에 도움이 됩니다.이론적으로 a는 DeepNorm 안정성의 상한을 나타내기 위해 무한대로 갈 수 있습니다. 여기서 우리는 도출을 단순화하기 위해 이 가정을 도입합니다. a가 충분히 커지면, 즉 a → ∞이면 LN 항목을 L-1 ƏLN(axk)로 근사화할 수 있고 잔여 항목 ǝ(αxk)를 II-a로 근사화할 수 있습니다. 이를 방정식 (5)에 넣고 다음과 같이 단순화할 수 있습니다. &quot; LLL-θε θε Эхи II OLN (axk) (axk) ☑ II k=l θαι 기약 LN k=l α 잔여 LL-θε ✗ OLN(k) дхк ☑ II a α k=l ƏXL 기약 θε ᎥᏆ Ꮮ . 기약 II k=l LN ĐLN(2k) дхк L-☑ II k=l LN 잔여 (00+00) θε L-OLN (F (xk; Ok)) JF (xk; Ok) k=l LN (4) θε θαι axL 기약 L-Π II (¹ k=l × II + JF (xk; k) дхк 잔차 여기서 그래디언트는 3개의 항으로 구성되고 마지막 두 항목은 모델 계층 L의 수에 대해 곱셈적입니다. L이 커지면 마지막 두 항목의 값이 매우 3 최근 몇 년 동안 이 지점에 의문을 제기하고 다양한 관점을 제공하는 연구자도 있습니다(Liu et al., 2020b; Wang et al., 2022a). 이를 알아내기 위해 더 많은 탐색과 논의가 필요하므로 이 논문에서는 여전히 그래디언트 노름의 관점에서 분석을 수행합니다. 4 더 자세한 도출은 부록 A에 있습니다. (6) 방정식 (4)의 Post-LN의 그래디언트와 비교할 때 DeepNorm은 최종 잔차 항목을 대략적으로 제거할 수 있으므로 그래디언트 소실 또는 폭발의 위험을 효과적으로 완화할 수 있습니다. DeepNorm에서 더 큰 a는 더 안정적인 그래디언트를 생성하지만, 위에서 언급했듯이 최종 수렴 성능을 희생해야 할 수도 있습니다. 불균형 그래디언트는 일반적으로 초기 학습 단계에서 발생한다는 점을 고려하면, 학습 기간에 따라 a를 변경할 수 있다면 더 적절할 수 있습니다. 3.2 BranchNorm 이 섹션에서는 이전 섹션의 관찰 내용을 요약하고 BranchNorm을 소개합니다. x1+= LN(x1+αF (x1; 01)) (7) 25.22.20.17.515.12.10.DeepNet nllloss - BranchNorm nllloss 학습 단계. DeepNet gnormloss BranchNorm gnormloss그림 3: 학습 시작 시 그래디언트 노름(실선)과 음수 로그 우도 손실(nllloss, 점선). 이는 DeepNorm의 이중 형태와 유사하지만 두 가지 주요 차이점이 있습니다.첫째, BranchNorm은 동적 요인(즉, a)을 활용하여 초기 학습 단계에서 기울기를 정규화하고 이후 단계에서 이러한 정규화의 부정적인 효과를 점진적으로 제거합니다.둘째, BranchNorm의 a는 Transformer의 비잔차 분기에 따라 조정되므로 방정식(5)에서 DeepNorm의 강력한 가정 없이 초기 단계의 기울기를 정확하게 정규화할 수 있습니다.특히, 방정식(7)의 a는 학습 단계 t의 수에 대한 간단한 요인입니다.여기서 우리는 간단한 선형 증분 접근 방식을 사용합니다.at = min(1.0,t/T)(8) 여기서 T는 BranchNorm을 수행하는 최대 단계 수입니다.학습의 맨 처음에 at은 0에 접근하는데, 이는 모델이 상수 변환을 근사하고 부드러운 기울기로 업데이트됨을 의미합니다. 위 섹션의 분석에 따라 1번째 하위 계층에 대한 BranchNorm의 그래디언트를 얻습니다. L-θε θε ☑ дх ax L II k=l = irreducible L-II ПI (1 + ak=l θε ƏXL irreducible ☑ QLN(2k+aF (2k;0)) Ə (xk + αF (xk; 01)) JF (xk; 01) residual L-II k=l дхк OLN(2k) дхк LN LN (a = 0) (9) 학습 시작 시에 BranchNorm은 그래디언트 노름을 안정화할 수 있는 반면 DeepNorm ✗은 방정식 (6)에서 비교적 강력한 가정(a → ∞)이 필요합니다.
--- EXPERIMENT ---
여러 변환 작업에 대한 결과는 BranchNorm이 학습 안정성과 수렴 성능 간에 더 나은 균형을 이룬다는 것을 보여줍니다.1 서론 최근 몇 년 동안 Transformers(Vaswani et al., 2017)가 빠르게 개발되어 광범위한 작업에서 최첨단(SOTA) 성능을 달성했습니다.한편, 모델 차원을 넓히면 모델 용량이 상당히 확장됩니다(Devlin et al., 2019; Liu et al., 2019; Goyal et al., 2021; Lin et al., 2021; Smith et al., 2022).심층 신경 모델이 여러 계층의 추상화로 피처 표현을 학습한다는 점을 감안할 때(LeCun et al., 2015), 너비보다 깊이를 확장하여 모델 용량을 늘리는 것이 더 매력적입니다. 불행히도 Transformers의 학습 불안정성으로 인해 이러한 SOTA 모델의 깊이는 여전히 비교적 얕습니다(Kaplan et al., 2020; Hoffmann et al., 2022). Transformers의 학습을 안정화하기 위해 더 나은 아키텍처에 대한 다양한 노력이 있었습니다.* 동등한 기여. + 책임 저자. WMT14 En-Fr BLEU 0.0.0.0.0. 바닐라 Transformer를 사용한 깊이 DeepNorm 갭에서의 성능 그림 1: 모델이 완전히 수렴한 후 WMT2014 En-Fr 데이터 세트에서 BLEU(%) 점수. &#39;갭&#39;은 바닐라 Transformer에 DeepNorm을 적용한 후 관찰된 성능 감소를 나타냅니다. 영어: tures(Wang et al., 2019; Shleifer et al., 2021; Wang et al., 2022b) 또는 적절한 초기화 구현(Zhang et al., 2019a; Huang et al., 2020; Wang et al., 2022a)이 있습니다. 이 중 가장 대표적인 접근 방식은 DeepNorm(Wang et al., 2022a)으로, 먼저 Transformers를 레이어로 확장하고 기존의 얕은 대응 모델보다 상당히 우수한 성능을 보입니다. 구체적으로 DeepNorm은 Transformer의 잔여 연결을 상향 조정하고 매개변수 초기화의 분산을 줄임으로써 모델 업데이트를 일정한 수준으로 제한하는 것을 목표로 합니다. 결과적으로 Transformers의 안정성이 초기 학습 단계에서 향상됩니다. 그러나 후속 학습 단계에서 DeepNorm이 부과한 매개변수 업데이트의 크기 제한으로 인해 궁극적으로 학습이 부족한 모델이 생성될 수 있습니다. 위의 추측을 검증하기 위해 먼저 얕은 Transformers에서 실험을 수행하여 수렴을 보장합니다. 그림 1에서 볼 수 있듯이 DeepNorm은 vanilla Transformers에서 어느 정도 성능 저하를 가져오는 것으로 관찰되었으며, 이 문제는 모델이 더 깊어질수록 더 심화되는 경향이 있습니다. 위의 문제를 해결하기 위해 BranchNorm이라는 매우 깊은 Transformers를 견고하게 확장하는 간단하면서도 효과적인 접근 방식을 제안합니다. 구체적으로, Transformer의 비잔류 브랜치¹는 학습 기간에 따라 동적으로 재조정됩니다. 모델 학습의 초기 단계에서 BranchNorm은 이론적으로 부드러운 그래디언트 규범으로 학습을 안정화합니다. 이후 학습 단계에서 BranchNorm은 더 나은 수렴을 촉진하기 위해 vanilla Post-LayerNorm(즉, PostLN)으로 점진적으로 퇴화합니다. 광범위한 변환 작업에 대한 실험 결과 BranchNorm은 DeepNorm보다 일관된 개선을 가져오고 위의 학습 부족 문제를 효과적으로 완화합니다. 게다가 BranchNorm은 DeepNorm보다 일부 주요 하이퍼파라미터(예: 워밍업)에서 더 견고하게 수행되므로 매우 깊은 Transformers를 확장하기 위한 이식 가능한 대안이 될 가능성이 높습니다. 이 논문의 기여는 다음과 같이 요약할 수 있습니다. • 우리는 BranchNorm이라는 간단하면서도 효과적인 정규화 접근 방식을 제안하여 매우 깊은 Transformers의 학습을 안정화합니다. • BranchNorm은 학습 안정성과 광범위한 변환 작업의 성능 수렴 간에 더 나은 균형을 이룹니다. • BranchNorm은 활성화 함수의 유사성과 희소성을 표현하는 관점에서 매우 깊은 모델에서 매개변수 중복 문제를 완화하는 것으로 입증되었습니다. 2 배경 이 섹션에서는 먼저 Post-LN과 Pre-LN의 차이점에 대한 간략한 개요를 제공하고 이어서 DeepNorm의 접근 방식을 소개합니다. Post-LN과 Pre-LN. 먼저, Wang et al. (2019); Nguyen 및 Salazar (2019)는 LayerNorm(Ba et al., 2016)의 위치가 학습 안정성에 상당한 영향을 미친다는 것을 관찰하고 원래 Post-LN(Vaswani et al., 2017)과 비교할 때 더 안정적인 Pre-LN 변형을 제안합니다. 영어: An &#39;Note that we named residual connections in Transformer in &#39;residual branch&#39; and the other branch that we called &#39;nonresidual branch&#39; in this paper. Pх Layer-Norm + &lt; Feed Forward Layer-Norm Self-Attention Post-Norm Pх Layer-Norm Feed Forward Layer-Norm 4 MH Self-Attention Layer-Norm Pre-Norm 그림 2: Pre-Norm(즉, Pre-LN) 및 Post-Norm(즉, Post-LN) Transformers의 아키텍처. 이 두 아키텍처의 예가 그림 2에 나와 있습니다. 이어서 Liu et al.(2020b)은 Pre-LN이 잔여 연결에 지나치게 의존하여 모델이 잠재력을 최대한 발휘하지 못할 수 있다고 추가로 분석했습니다. 위의 관찰에서 동기를 얻어 나머지 실험에서는 Post-LN을 기반으로 접근합니다. 형식적으로, 1번째 하위 계층 x1의 하위 계층의 입력이 주어졌을 때, 출력 x1+를 계산하는데, 이는 다음과 같이 Post-LN에 의해 계산된다: X1+1 = LN(x1+ F (x1; 01)) (1) 여기서 LN은 LayerNorm²의 약자이고, F는 현재 하위 계층(어텐션 또는 피드포워드)의 함수를 나타내고, Or는 하위 계층의 해당 매개변수를 나타낸다. DeepNorm. DeepNorm은 Post-LN Transformer 아키텍처를 따르고 스칼라 인자 a &gt; 1로 잔차 분기를 재조정한다. 마찬가지로, l번째 하위 계층은 다음과 같이 DeepNorm에 의해 계산된다: X1+= · LN(αx₁ + F (x1; 01)) (2) 또한, DeepNorm은 초기 매개변수의 분산을 스케일링 인자 ß &lt; 1만큼 줄인다. a와 ẞ는 모두 모델 깊이의 함수이며, 이는 일정한 모델 업데이트 가정에서 파생되었다. N-계층 인코더와 M-계층 디코더를 갖춘 표준 Transformer의 경우, DeepNorm은 2a와 3a를 다음과 같이 계산합니다. aencoder 0.81 (N¹M)= Bencoder = 0.87(N¼M) = adecoder (3M) ẞdecoder = (12M)¯(3) ẞ는 모델 초기화에만 영향을 미치는 반면, a는 전체 절차에서 사용되고 고정됩니다. 게다가, 모델이 깊어짐에 따라 DeepNorm은 a에 더 큰 값을 할당하는데, 이로 인해 모델 출력 x1+1이 잔차 분기 ax₁에 지나치게 의존하게 되어 궁극적으로 방정식 (2)에서 학습이 부족한 모델 매개변수 Oɩ가 생성됩니다. 3가지 접근 방식 이 섹션에서는 먼저 그래디언트 노름의 관점에서 Post-LN의 불안정성을 분석한 다음, DeepNorm이 불균형 그래디언트를 어느 정도 완화할 수 있는 방법을 보여주고, 마지막으로 제안하는 방법인 BranchNorm을 소개합니다. 3.1 그래디언트의 관점 불균형 그래디언트는 주로 Transformer³의 불안정성에 책임이 있습니다(Wang et al., 2019; Shleifer et al., 2021; Zhu et al., 2021). 먼저 Wang et al.(2019)에 따라 그래디언트와 모델 깊이 간의 관계를 살펴봅니다. L개의 하위 레이어와 학습 손실 E가 있는 Transformer가 주어지면 1번째 하위 레이어의 그래디언트는 체인 규칙4에 따라 계산됩니다. 크거나 매우 작아 그래디언트가 사라지거나 폭발할 수 있습니다. 마찬가지로, 우리는 방정식 (2)에 기반하여 DeepNorm의 기울기를 분석하고 1번째 하위 계층의 기울기가 다음과 같이 계산됨을 얻습니다.기약 L-θε θε δαι მxL, II OLN (axk + F (xk; 01)) a (axk + F (xk; 01)) × k=l LN JF (xk; k) дхк L-II k=l + residual (5) DeepNorm에서 a는 모델 깊이에 따라 증가하고 학습 안정성에 도움이 됩니다.이론적으로 a는 DeepNorm 안정성의 상한을 나타내기 위해 무한대로 갈 수 있습니다. 여기서 우리는 도출을 단순화하기 위해 이 가정을 도입합니다. a가 충분히 커지면, 즉 a → ∞이면 LN 항목을 L-1 ƏLN(axk)로 근사화할 수 있고 잔여 항목 ǝ(αxk)를 II-a로 근사화할 수 있습니다. 이를 방정식 (5)에 넣고 다음과 같이 단순화할 수 있습니다. &quot; LLL-θε θε Эхи II OLN (axk) (axk) ☑ II k=l θαι 기약 LN k=l α 잔여 LL-θε ✗ OLN(k) дхк ☑ II a α k=l ƏXL 기약 θε ᎥᏆ Ꮮ . 기약 II k=l LN ĐLN(2k) дхк L-☑ II k=l LN 잔여 (00+00) θε L-OLN (F (xk; Ok)) JF (xk; Ok) k=l LN (4) θε θαι axL 기약 L-Π II (¹ k=l × II + JF (xk; k) дхк 잔차 여기서 그래디언트는 3개의 항으로 구성되고 마지막 두 항목은 모델 계층 L의 수에 대해 곱셈적입니다. L이 커지면 마지막 두 항목의 값이 매우 3 최근 몇 년 동안 이 지점에 의문을 제기하고 다양한 관점을 제공하는 연구자도 있습니다(Liu et al., 2020b; Wang et al., 2022a). 이를 알아내기 위해 더 많은 탐색과 논의가 필요하므로 이 논문에서는 여전히 그래디언트 노름의 관점에서 분석을 수행합니다. 4 더 자세한 도출은 부록 A에 있습니다. (6) 방정식 (4)의 Post-LN의 그래디언트와 비교할 때 DeepNorm은 최종 잔차 항목을 대략적으로 제거할 수 있으므로 그래디언트 소실 또는 폭발의 위험을 효과적으로 완화할 수 있습니다. DeepNorm에서 더 큰 a는 더 안정적인 그래디언트를 생성하지만, 위에서 언급했듯이 최종 수렴 성능을 희생해야 할 수도 있습니다. 불균형 그래디언트는 일반적으로 초기 학습 단계에서 발생한다는 점을 고려하면, 학습 기간에 따라 a를 변경할 수 있다면 더 적절할 수 있습니다. 3.2 BranchNorm 이 섹션에서는 이전 섹션의 관찰 내용을 요약하고 BranchNorm을 소개합니다. x1+= LN(x1+αF (x1; 01)) (7) 25.22.20.17.515.12.10.DeepNet nllloss - BranchNorm nllloss 학습 단계. DeepNet gnormloss BranchNorm gnormloss그림 3: 학습 시작 시 그래디언트 노름(실선)과 음수 로그 우도 손실(nllloss, 점선). 이는 DeepNorm의 이중 형태와 유사하지만 두 가지 주요 차이점이 있습니다.첫째, BranchNorm은 동적 요인(즉, a)을 활용하여 초기 학습 단계에서 기울기를 정규화하고 이후 단계에서 이러한 정규화의 부정적인 효과를 점진적으로 제거합니다.둘째, BranchNorm의 a는 Transformer의 비잔차 분기에 따라 조정되므로 방정식(5)에서 DeepNorm의 강력한 가정 없이 초기 단계의 기울기를 정확하게 정규화할 수 있습니다.특히, 방정식(7)의 a는 학습 단계 t의 수에 대한 간단한 요인입니다.여기서 우리는 간단한 선형 증분 접근 방식을 사용합니다.at = min(1.0,t/T)(8) 여기서 T는 BranchNorm을 수행하는 최대 단계 수입니다.학습의 맨 처음에 at은 0에 접근하는데, 이는 모델이 상수 변환을 근사하고 부드러운 기울기로 업데이트됨을 의미합니다. 위 섹션의 분석에 따라 1번째 하위 계층에 대한 BranchNorm의 그래디언트를 얻습니다.L-θε θε ☑ дх ax L II k=l = irreducible L-II ПI (1 + ak=l θε ƏXL irreducible ☑ QLN(2k+aF (2k;0)) Ə (xk + αF (xk; 01)) JF (xk; 01) residual L-II k=l дхк OLN(2k) дхк LN LN (a = 0) (9) 학습 시작 시 BranchNorm은 그래디언트 노름을 안정화할 수 있는 반면 DeepNorm ✗은 방정식 (6)에서 비교적 강력한 가정(a → ∞)이 필요합니다.실험적으로 그림 3에서 볼 수 있듯이 학습 시작 시 BranchNorm의 해당 더 부드러운 그래디언트를 관찰합니다.학습 단계 t가 미리 정의된 최대 단계 T에 도달하면 BranchNorm vanilla Post-LN으로 퇴화하여 더 나은 수렴을 달성합니다. 섹션 5.1에서 BranchNorm의 하이퍼파라미터 무감각성을 추가로 검증합니다. 4 실험 및 결과 우리는 접근 방식을 검증하기 위해 이중 언어 번역 및 다중 언어 번역 작업 모두에 대한 광범위한 실험을 수행합니다. 이 섹션에서는 실험 설정을 설명하고 결과를 제시합니다. 4. 데이터 세트 및 평가 이중 언어 작업에 표준 WMT 2017 영어-독일어(En-De), WMT 2014 영어-프랑스어(En-Fr), IWSLT 2014 독일어-영어(De-En) 데이터 세트를 사용하며, 이는 fairseq³의 공식 스크립트에 따라 처리됩니다. 다중 언어 작업의 경우 OPUS-100(Zhang et al., 2020) 및 MultiUN(Gu et al., 2019) 데이터 세트에 대한 실험을 수행하고 기존 연구에서 해당 처리를 따릅니다. 평가를 위해 추론 중 빔 크기를 4로, 길이 패널티를 0.6으로 설정했습니다. multibleu.perl을 사용하여 WMT 2017 En-Deб 및 WMT 2014 En-Fr에 대한 케이스별 민감한 BLEU 점수를 계산합니다. 또한 sacreBLEU를 사용하여 Wang et al.(2021)에 따라 OPUS에 대한 케이스별 민감한 BLEU 점수와 MultiUN에 대한 케이스별 비감응 BLEU 점수를 계산합니다. 4.2 학습 설정 실험은 fairseq 코드베이스(Ott et al., 2019)를 기반으로 합니다. 모든 실험에서 특별히 언급하지 않은 경우 숨겨진 차원을 512로, 피드포워드 내부 표현을 2048로 설정하는 표준 Transformer 기본 설정을 사용합니다. DeepNorm(Wang et al., 2022a)에 따라 모델 매개변수를 초기화합니다. 모든 실험은 각각 약 16,384개 토큰의 배치 크기로 할당된 32개의 NVIDIA A100 GPU에서 수행됩니다. 모든 Transformer 모델은 소규모 S에 대한 조기 중단과 함께 100k 단계로 훈련되었습니다.https://github.com/facebookresearch/fairseq &quot;엄격한 비교를 위해 DeepNorm을 사용한 동일한 테스트 세트, 즉 newstest2014를 사용합니다. https://github.com/mjpost/sacrebleu 모델 LN 6L-6L 18L-18L 50L-50L 100L-100L 250L-250L Vanilla Post-LN (Vaswani et al., 2017) Post 28.diverged DS-Init (Zhang et al., 2019a) Post 27.diverged Admin (Liu et al., 2020b) Post 27.28.diverged ReZero (Bachlechner et al., 2020) No 26.diverged R-Fixup (Zhang 등, 2019b) 번호 27.28.27.diverged 분기 T-Fixup (Huang 등, 2020) 번호 27.28.27.diverged 분기 Vanilla Pre-LN (Vaswani 등, 2017) Pre 27.28.28.27.27.DLCL (Wang 등, 2019) Pre 27.28.diverged 27.27.NormFormer (Shleifer 등, 2021) Pre 27.28.27.diverged 분기 Sub-LN (Wang 등, 2022b) + Pre 27.28.28.27.27.DeepNorm (Wang 등, 2022a) Post 27.28.29.28.DeepNorm (Wang et al., 2022a) † 게시물 28.29.29.29.29.29.BranchNorm(저희) 게시물 29.30.30.7* 29.29.표 1: 깊이 스케일링을 적용한 WMT-17 En-De 테스트 세트의 BLEU 점수(%). †는 저희의 재구현을 나타냅니다. AL-BL은 A 계층 인코더와 B 계층 디코더가 있는 Transformer를 나타냅니다. **&#39;는 BranchNorm이 p &lt; 0.03으로 DeepNorm보다 상당히 우수함을 의미합니다. 모델 LN 6L-6L 18L-18L 50L-50L 100L-100L 250L-250L 500L-500L 바닐라 Post-LN (2017) Pre 41.43.diverged 바닐라 Pre-LN (2017) Pre 40.42.42.43.43.43.DLCL (2019) Pre 41.42.43.diverged Sub-LN (2022b) † Pre 41.42.43.43.43.43.DeepNorm (2022a) † Post 41.42.43.43.43.43.MixNorm (우리) BranchNorm (우리) Post 41.43.43.43.43.43.Post 41.43.43.44.44.30* 44.표 2: 깊이 스케일링을 적용한 WMT-14 En-Fr 테스트 세트의 BLEU 점수(%). †는 재구현을 나타냅니다. AL-BL은 A 계층 인코더와 B 계층 디코더가 있는 Transformer를 나타냅니다. ***는 BranchNorm이 p &lt;0.03으로 DeepNorm보다 상당히 우수함을 의미합니다. WMT2017 En-De에서의 BLEU(%) 성능 곡선 31.30.30.29.29.28.DeepNorm(기준선) BranchNorm(저희) 28. 학습 단계 그림 4: 학습 단계가 증가함에 따라 WMT 2017 En-De에서 50L-50L 모델의 BLEU 점수 곡선. 모든 실험에서 방정식(8)에서 BranchNorm의 최대 노름 단계 T는 4,000으로 설정됩니다. 자세한 내용은 부록 B에 나와 있습니다.4.3 이중 언어 번역 과제 DeepNorm(Wang 등, 2022a), Sub-LN(Wang 등, 2022b), NormFormer(Shleifer 등, 2021), ReZero(Bachlechner 등, 2020) 등을 포함하여 딥 트랜스포머에 대한 여러 최신 접근 방식을 비교합니다.실험을 수행할 때 소스 코드가 공개적으로 제공되지 않았기 때문에 DeepNorm을 구현하고 원래 논문(Wang 등, 2022a)을 따랐습니다.다른 접근 방식에서 학습 프레임워크가 동일한지 확인하기 위해 Sub-LN 및 NormFormer의 공식 소스 코드를 따르고 Fairseq에서 다시 구현했습니다.다른 결과는 해당 논문에서 직접 인용했습니다.WMT17 En-De의 결과. 표 1은 WMT에 대한 기준선과 접근 방식의 결과를 보고합니다.(a) IWSLT2014 De-En 0.16M (b) WMT2017 En-De 4.5M (c) WMT2014 En-Fr 36MDeepNormDeepNorm BranchNorm(저희) BranchNorm(저희) SubLN DeepNorm BranchNorm(저희) SubLN100500100500100Model Depths Model Depths Model Depths 그림 5: 로그 스케일로 표시된 다양한 모델 깊이에 따른 이중어 번역의 성능. 2017 En-De 데이터 세트. 대부분의 경우 바닐라 Post-LN Transformer의 훈련은 자체 훈련 불안정성으로 인해 발산됩니다. 한편, 이전 접근 방식은 다양한 정도로 딥 Transformer의 훈련을 안정화할 수 있습니다. 영어: 재구현한 DeepNorm의 결과는 원래 논문에서 보고된 결과보다 약간 더 우수하며, 접근 방식의 개선을 더욱 설득력 있게 만드는 더 강력한 기준선 역할을 합니다. 모든 접근 방식에서 200개 레이어 이후 BLEU 점수가 다르게 저하되는 것은 주목할 만한 사실입니다. 이 현상은 모델 심화 후 소규모 WMT17 En-De에서 과적합으로 인해 발생한다고 추측합니다. 요약하자면, BranchNorm은 다양한 심도에서 일관되게 최상의 결과를 달성하고 위에서 언급한 성능 저하 문제를 완화합니다. 게다가 BranchNorm은 동일한 모델 심도를 고려할 때 이전의 최첨단 심층 모델보다 최대 +1.2 BLEU 더 우수한 성능을 보입니다. 그림 4에서 볼 수 있듯이 BranchNorm은 DeepNorm보다 수렴 속도가 빠르고 수렴 성능이 더 좋습니다. 기준선 결과 WMT14 En-Fr.의 결과 및 더 큰 WMT 2014 En-Fr 데이터 세트에 대한 BranchNorm이 표 2에 보고되어 있습니다. WMT 2014 En-De에서도 유사한 결과를 관찰했습니다. 즉, BranchNorm은 다양한 깊이를 가진 모델에서 일관된 개선을 가져왔습니다. 주목할 점은 500개 레이어 모델이 기존 딥 모델보다 성능이 뛰어나고 44.3 BLEU의 새로운 SOTA 성능을 달성했다는 것입니다. 데이터 규모의 효과. 그림 5에 서로 다른 규모의 세 가지 데이터 세트에 대한 자세한 성능을 그렸습니다. 전반적으로 모델이 깊어짐에 따라 더 작은 데이터에 대한 성능은 저하되는 반면 더 큰 데이터 세트는 깊이의 스케일링으로부터 계속 이점을 얻는 것을 관찰했습니다. 이는 더 깊은 모델이 적합하기 위해 더 큰 데이터가 필요한 경향이 있음을 나타내며, 이는 대규모 사전 학습(Hoffmann et al., 2022)의 결과와 일치합니다. 학습 단계의 효과. 그림 4에서 WMT2017 En-De에서 50L-50L 모델에 대한 BranchNorm 및 DeepNorm의 학습 곡선을 그렸습니다. 결과는 BranchNorm이 딥 모델의 잠재적 성능을 효과적으로 발휘하고 최종적으로 더 나은 수렴 성능을 낼 수 있음을 보여줍니다. 반면 DeepNorm은 학습 부족 문제로 어려움을 겪고 있으며, 더 큰 학습 단계에서는 성능이 어느 정도 저하됩니다. 4.4 다국어 번역 과제 OPUS-100 및 MultiUN 데이터 세트에 대한 다양한 모델의 결과는 표 3에 나와 있습니다. 깊이가 12에서 1000으로 증가함에 따라 OPUS 데이터 세트와 MultiUN 데이터 세트에서 BLEU 점수가 각각 +7.8 및 +6.1포인트 증가합니다. 기본 Pre-LN Transformer를 200 및 1000개 레이어로 확장하는 것은 효과가 없는 것으로 나타났으며, 기본 Pre-LN Transformer가 딥 모델에서 충분히 효과적이지 않음을 나타냅니다. BranchNorm은 모든 깊이에서 DeepNorm보다 지속적으로 성능이 우수하며, 이는
--- CONCLUSION ---
영어: s of the bilingual translations. 5 분석 이 섹션에서는 먼저 BranchNorm의 하이퍼파라미터에 대한 견고성을 확인한 다음 매개 변수 중복성을 분석합니다. 5.1 다른 T의 하이퍼파라미터 민감도 효과. 우리는 방정식 (8)에서 다른 최대 표준 단계 T를 변경하는 것이 BranchNorm에 미치는 효과를 평가하기 위해 실험을 수행합니다. OPUSMultiUN 모델 #레이어 #매개변수 X→En En→X 평균 X→En En→X 평균133M 27.21.24.43.52.48.Baseline(Zhang 등, 2020)173M 29.22.26.46.53.254M 31.24.27.863M 34.26.30.49.56.52.Pre-LN(Vaswani 등, 2017)3.8B 34.28.31.50.56.53.863M 33.29.31.DeepNorm(Wang 등, 2022a)3.8B 33.30.32.863M 33.28.31.49.56.53.DeepNorm† (2022a)3.8B 34.29.32.50.57.53.863M 34.28.31.4* 49.57.53.4* BranchNorm(저희)3.8B 35.29.32.3* 50.57.54.2* &#39;*&#39; 표 3: OPUS-100 및 MultiUN 테스트 세트에서 다양한 깊이를 가진 다양한 모델의 평균 BLEU 점수(%). †는 저희의 재구현을 나타냅니다. 굵은 글씨로 표시된 점수는 동일한 깊이에서 가장 좋은 점수에 해당합니다. BranchNorm이 p &lt; 0.05로 DeepNorm보다 훨씬 우수함을 의미합니다. BLEU (%)(a) 워밍업의 효과 BLEU (%)(b) 학습률의 효과 DeepNorm(기준선) BranchNorm(우리의) 25DeepNorm(기준선) BranchNorm(우리의) 5e-8e-1e-1.5e-워밍업 단계 학습률 그림 6: WMT 2014 En-Fr 데이터 세트에서 100L-100L 모델을 학습할 때 주요 하이퍼파라미터(예: 워밍업 및 학습률)의 효과. 다양한 전략에 대한 알파 1.0.0.0.0.Exp k=0.선형 T-시그모이드 u=0.0-학습 단계 t그림 7: BranchNorm에서 a의 다양한 성장 전략. 모든 전략에서 a가 1.0으로 잘렸다는 점에 유의하세요. T의 값이 클수록 BranchNorm이 vanilla Post-LN으로 느리게 저하되며 일반적으로 더 안정적인 학습 프로세스를 생성합니다. 우리는 TЄ [100, 400, 4000, 20000]을 변화시키고 BranchNorm이 T의 변화에 민감하지 않다는 것을 관찰합니다.다른 워밍업 및 학습 속도의 효과.우리는 WMT14 En-Fr 데이터 세트에서 200개 계층(즉, 100L-100L) Transformer에 대한 이러한 핵심 하이퍼파라미터의 효과를 조사하고 그림 6에 결과를 제시합니다.우리의 관찰 결과에 따르면 BranchNorm은 워밍업을 사용하지 않고도 200개 계층 Transformer를 안정적으로 훈련할 수 있으며 DeepNorm과 비교했을 때 더 큰 학습 속도에 대한 내성이 더 뛰어납니다.a의 다른 성장 전략의 효과. 우리는 그림 7에 나와 있는 기본 선형 전략을 포함한 다양한 성장 전략의 효과를 조사합니다.WMT14 En-Fr의 200개 계층 변환기의 경우, 우리는 각각 선형, exp 및 Similarity (a) 인코더 계층 1.0.0.0.0.0.0.---- Similarity 1.(b) 디코더 계층 0.0.0.0.DeepNorm(기준선) 0.BranchNorm(우리의 것) 0.DeepNorm(기준선) BranchNorm(우리의 것) 1 24 5 6 7 8 9 10 11 12 13 14 15 16Layers3 4 5 6 7 8 9 10 11 12 13 14 15 16Layers 그림 8: 인접한 층 사이의 표현 유사성. BranchNorm의 값은 대부분 층에서 DeepNorm보다 낮습니다. 희소성 0.(a) 인코더 층 (b) 디코더 층 0.0.0.0.0.0.0. 희소성 0.090.080.070.0.0.040.DeepNorm(기준선) BranchNorm(우리의) 0.0.27 8 9 10 11 12 13 14 15 16 1715층 DeepNorm(기준선) BranchNorm(우리의) 8 9 10 11 12 13 14 15 16 17층 그림 9: DeepNorm 및 BranchNorm 모델의 활성화 함수의 희소성. BranchNorm 모델은 모든 층에서 DeepNorm 모델보다 희소합니다. 시그모이드 전략은 우리 방법이 이러한 전략 변형에 강력하다는 것을 나타내므로 모든 실험에서 가장 단순한 선형 전략을 사용합니다.5.2 매개변수 중복성 표현 유사성.이전 연구(Liu et al., 2020a)에서는 Pre-LN Transformer가 잔여 분기에 비례적으로 큰 가중치를 두어 모델이 심화됨에 따라 잠재적인 성능을 저해할 수 있다고 가정했습니다.저희의 가설은 DeepNorm이 심층 모델 학습의 안정성을 높이기 위해 잔여 분기의 가중치를 직접 증가시키지만 심층 모델의 잠재력을 저해할 수도 있다는 것입니다.이 가정을 검증하기 위해 DeepNorm과 BranchNorm으로 각각 학습한 200개 계층(즉, 100L-100L) 모델에서 인접한 계층 간 표현의 코사인 유사성을 결정하는 방법론을 사용했습니다. 인코더와 디코더 계층의 표현 유사성은 그림 8에 나와 있습니다. DeepNorm의 유사성 점수가 BranchNorm의 유사성 점수를 지속적으로 초과하는 것으로 관찰되었는데, 이는 잔여 브랜치의 가중치 증가로 인해 모델이 Pre-LN Transformer에 더 가까워졌음을 나타냅니다. 깊이와 데이터가 다른 모델에서도 희소성에 대한 유사한 결과가 지속적으로 관찰되었습니다. 이러한 특성은 이후 딥 모델의 저하에 기여하고 성능에 부정적인 영향을 미칠 수 있습니다. 따라서 BranchNorm 구현에서와 같이 가중치를 원래 값으로 되돌리는 것이 필수적입니다. 활성화 함수의 희소성. Li et al. (2022)은 Transformer의 활성화 함수 희소성을 연구하고 희소한 모델이 더 나은 일반화와 견고성을 제공한다는 것을 보여줍니다. 희소성은 활성화 함수 뒤에 있는 0이 아닌 항목의 백분율로 정량화됩니다. 그림 9에서 볼 수 있듯이 DeepNorm과 BranchNorm으로 각각 학습한 두 모델의 희소성을 관찰하고 BranchNorm의 희소성이 비교적 작다는 것을 발견했습니다. 모델의 견고성과 일반화에 대한 희소성의 효과를 확인하기 위해 MTNT(Michel and Neubig, 2018)에 대한 추가 실험을 수행했습니다.MTNT(노이즈가 있는 텍스트의 기계 번역)는 Reddit(www.reddit.com)의 노이즈가 있는 댓글과 전문적으로 공급된 번역으로 구성되며 견고한 번역을 위한 테스트베드입니다.이 노이즈 데이터 세트에서 DeepNorm과 BranchNorm으로 훈련된 두 개의 영어-프랑스어 모델을 평가합니다.BranchNorm은 DeepNorm보다 1.0 BLEU가 상당히 개선되어 우리 방법이 모델의 희소성을 높여 견고성을 개선할 수 있음을 나타냅니다.6 결론 이 논문에서는 먼저 DeepNorm의 과소 훈련 문제를 탐구하고 이론적으로 초기 단계에서 부드러운 그래디언트 규범으로 훈련을 안정화하는 보다 유연한 표준적 접근 방식인 BranchNorm을 제안합니다.훈련 불안정성의 위험한 단계를 지나면 BranchNorm은 표준 Post-LN으로 퇴화하여 더 나은 수렴 성능을 촉진할 수 있습니다. 여러 변환 작업에 대한 실험 결과 BranchNorm은 학습 안정성과 수렴 성능 간에 더 나은 균형을 이룬다는 것을 보여줍니다.제한 사항 딥 트랜스포머의 학습에는 일반적으로 많은 GPU 리소스가 필요합니다.예를 들어, 1,000개 레이어의 WMT14 En-Fr 변환 모델을 학습하려면 1000 GPU 일이 필요합니다.또한 더 깊은 디코더는 추론 속도를 느리게 만들 수 있으며, 딥 모델을 애플리케이션에 실질적으로 배포할 수 있도록 더 많은 모델 아키텍처 설계 또는 압축 기술을 추가로 탐색해야 합니다.참고 문헌 Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton.2016. Layer normalization.stat, 1050:21. Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, Julian J. McAuley.2020. Rezero만 있으면 됩니다: 큰 깊이에서 빠르게 수렴합니다.CoRR, abs/2003.04887. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: 언어 이해를 위한 딥 양방향 변환기의 사전 학습. NAACL-HLT 2019, 4171-4186쪽. Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau. 2021. 다국어 마스크 언어 모델링을 위한 대규모 변환기. CORR, abs/2105.00572. Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2019. 잘못된 상관 관계를 무시하여 개선된 제로샷 신경망 기계 번역. Association for Computational Linguistics의 제57회 연례 회의록, 1258-1268쪽. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022. Training Compute-Optimal Large Language Models. arXiv 사전 인쇄본 arXiv:2203.15556. Xiao Shi Huang, Felipe Pérez, Jimmy Ba, and Maksims Volkovs. 2020. Improving transformer optimization through better initialization. ICML 2020, Proceedings of Machine Learning Research의 119권, 4475-4483페이지. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv 사전 인쇄 arXiv:2001.08361. 얀 르쿤, 요슈아 벤지오, 제프리 힌튼. 2015. 딥러닝. 자연, 521(7553):436–444. Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo 등. 2022. 대형 모델은 인색한 학습자입니다. 훈련된 변압기의 활성화 희소성. arXiv 사전 인쇄 arXiv:2210.06313. Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou 및 Hongxia Yang. 2021. M6-10T: 효율적인 수조 개 매개변수 사전 학습을 위한 공유-연결 해제 패러다임. CORR, abs/2110.03888. Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han. 2020a. 적응 학습 속도의 분산 및 그 이상. ICLR 2020에서. Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Jiawei Han. 2020b. 변환기 학습의 어려움 이해. 2020년 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 57475763페이지, 온라인. 계산 언어학 협회. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: 견고하게 최적화된 BERT 사전 학습 접근법. CoRR, abs/1907.11692. Paul Michel and Graham Neubig. 2018. MTNT: 노이즈가 있는 텍스트의 기계 번역을 위한 테스트베드. 2018년 자연어 처리 경험적 방법에 대한 컨퍼런스 회의록, 543553페이지, 벨기에 브뤼셀. Association for Computational Linguistics. Toan Q. Nguyen and Julian Salazar. 2019. 눈물이 없는 변압기: 자기 주의의 정규화 개선. CORR, abs/1910.05895. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli. 2019. fairseq: 시퀀스 모델링을 위한 빠르고 확장 가능한 툴킷. 2019년 북미 컴퓨터 언어학 협회 지부 회의록: 인간 언어 기술, NAACL-HLT 2019, 미네소타주 미니애폴리스, 2019년 6월 2-7일, 데모, 48-53페이지. 컴퓨터 언어학 협회. Sam Shleifer, Jason Weston, Myle Ott. 2021. Normformer: 추가 정규화를 통한 개선된 변환기 사전 학습. CoRR, abs/2110.09456. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti 등 2022. deepspeed와 megatron을 사용하여 대규모 생성 언어 모델인 megatron-turing nlg 530b를 훈련합니다. arXiv 사전 인쇄 arXiv:2201.11990. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser 및 Illia Polosukhin. 2017. 관심만 있으면 됩니다. NeurIPS 2017, 페이지 5998-6008. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang 및 Furu Wei. 2022a. DeepNet: Transformer를 1,000개 레이어로 확장합니다. CORR, ABS/2203.00555. Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song 및 Furu Wei. 2022b. 파운데이션 트랜스포머. CORR, ABS/2210.06423. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong 및 Lidia S. Chao. 2019. 기계 번역을 위한 딥 트랜스포머 모델 학습. ACL 2019, 1810~1822페이지. Weizhi Wang, Zhirui Zhang, Yichao Du, Boxing Chen, Jun Xie, Weihua Luo. 2021. 제로샷 신경망 기계 번역 재고: 잠재 변수의 관점에서. 계산 언어학 협회의 연구 결과: EMNLP 2021, 4321-4327페이지, 도미니카 공화국 푼타카나. 계산 언어학 협회. Biao Zhang, Ivan Titov, Rico Sennrich. 2019a. 깊이 확장 초기화 및 병합된 어텐션을 사용한 딥 트랜스포머 개선. EMNLP-IJCNLP 2019, 898-909페이지. Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich. 2020. 대규모 다국어 신경망 기계 번역 및 제로샷 번역 개선. ACL 2020, 1628-1639페이지. Association for Computational Linguistics. Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. 2019b. Fixup 초기화: 정규화 없는 잔여 학습. ICLR 2019에서. Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. 2021. Gradinit: 안정적이고 효율적인 학습을 위한 신경망 초기화 학습. Advances in Neural Information Processing Systems, 34:16410-16422. A 이론적 증명 A.1 사후 LN의 그래디언트 L개의 하위 레이어와 학습 손실 E를 갖는 변압기가 주어지면, 1번째 하위 레이어의 그래디언트는 체인 룰에 의해 계산됩니다. θε θαι δε θαι θαι θαι (10) axL 위 방정식에서 재귀적으로 분해하면, JXL δαι XL XL-XL-1 XL-0х1+(11) θαι 사후 LN이 주어지면 x1+1을 다음과 같이 계산합니다. x1+1 = LN(x1+ F (x1; 01)) (12) 잔차 연결의 출력을 Y₁ = x + F (x1; 01)이라고 하면, 두 인접한 레이어의 편미분을 다음과 같이 계산할 수 있습니다. 0х1+0х1+1 ду θαι მყმუ (13) ƏLN (yi) IF (x1; 01) 1+ дуг Οχι 방정식 (13)과 방정식 (11)을 방정식 (10)에 대입하면 다음을 얻습니다. zk = axk로 두고 연쇄 법칙을 사용하면 다음을 얻습니다. × Πα L-θε θε θαι SONXL II SONLN (yk) дук irreducible k=l L-irreducible θε LN (14) = ✗ Lk=l (II ( 1 + OF (xk; k) дхк 잔여 θαι irreducible θε τXL irreducible ✓ II k=l LN OLN(2k) дхк LN k=1 잔차 L-1) × II a Π α k=residual L-II ĐLN(2k) дхк 위의 그래디언트는 세 개의 항으로 구성되고 마지막 두 항목은 모델 계층 L의 개수에 대한 곱셈입니다. L이 커지면 Post-LN의 그래디언트는 사라지거나 폭발할 위험에 직면하게 됩니다. A.2 DeepNorm의 그래디언트 DeepNorm은 스칼라 승수 a &gt; 1로 잔차 분기를 재조정하고 다음과 같이 하위 계층을 계산합니다. k=l LN (18) 방정식 (14)에서 Post-LN의 그래디언트와 비교할 때 DeepNorm은 최종 곱셈 항목을 대략적으로 제거할 수 있으므로 그래디언트 사라지거나 폭발할 위험을 어느 정도 완화할 수 있습니다. A.3 BranchNorm의 그래디언트 BranchNorm은 Transformer에서 비잔여 분기를 직접 재조정하고 1번째 하위 계층에 대한 계산을 다음과 같이 수행합니다.X1+= LN(x1+αF (x1; 01)) (19) 이전 분석 프로세스와 유사하게 BranchNorm의 그래디언트를 다음과 같이 계산할 수 있습니다.X1+1 = LN(ax+F(x1; 01)) (15) A.1의 위 프로세스를 따르면 DeepNorm의 그래디언트가 나옵니다.L-θε θε ☑ II дхі ax L irreducible L-θε θε &#39;OLN (axk + F (xk; 01)) θαι axL II × k=l irreducible Ə (axk + F (xk; 01)) LN L-II ( k=lk=l +a OLN (xkaF (xk; 01)) 0 (2k+aF (2k;0)) OF (xk; 01) дхк LN ✗ L-JF (xk; Ok) II (a + ߘхк k=l residual θε ☑ axL II residual L-ĐLN(k) (a = 0) Jxk (16) irreducible DeepNorm이 잔차 연결의 출력 비율을 증폭시키기 위해 a에 상대적으로 더 큰 값을 할당한다는 점을 감안할 때, 여기서는 파생을 단순화하기 위한 가정을 도입합니다. a가 충분히 커지면 위 방정식을 다음과 같이 근사할 수 있습니다. L-II θε ≈θε θαι axL irreducible k=l OLN (axk) (axk) LN L-☑ II k=l α residual (17) k=l LN (20) BranchNorm은 그래디언트 노름을 로 안정화할 수 있는 반면 DeepNorm은 방정식 (6)에서 비교적 강력한 가정이 필요합니다. 실험적으로, 그림 3에서, 우리는 훈련의 아주 시작 부분에서 BranchNorm의 상응하는 더 부드러운 기울기를 관찰합니다.B 하이퍼파라미터 하이퍼파라미터 학습률 학습률 스케줄러 워밍업 업데이트 워밍업 초기 학습률 최대 토큰 Adam € 소규모 중규모 대규모 5e-역 sqrtle-128 ×Adam B 1e-(0.9, 0.98) 레이블 평활화 0. 훈련 업데이트 100K 기울기 클리핑 0. 드롭아웃 0.0.0. 가중치 감소 0. 은닉 크기 FFN 내부 은닉 크기 어텐션 헤드 표 4: 다양한 데이터 크기에 대한 Transformer 기반 실험의 하이퍼파라미터. &#39;소규모&#39;: IWSLTDe-En 및 WMT17 En-De. &#39;중규모&#39;: WMT14 En-Fr. &#39;대규모&#39;: OPUS-100 및 MultiUN 데이터 세트.
