--- ABSTRACT ---
우리는 사용자가 자유형 자연어로 음악 선택을 안내할 수 있도록 하면서 입력 비디오에 음악을 추천하는 방법을 제안합니다. 이 문제 설정의 핵심 과제는 기존 뮤직 비디오 데이터 세트가 필요한 (비디오, 음악) 학습 쌍을 제공하지만 음악에 대한 텍스트 설명이 부족하다는 것입니다. 이 연구는 다음 세 가지 기여를 통해 이 과제를 해결합니다. 첫째, 우리는 사전 학습된 음악 태거 출력과 소수의 인간 텍스트 설명이 주어진 대규모 언어 모델(BLOOM-176B)에서 자연어 음악 설명을 생성하기 위해 유추 기반 프롬프트 절차에 의존하는 텍스트 합성 접근 방식을 제안합니다. 둘째, 우리는 이러한 합성된 음악 설명을 사용하여 텍스트와 비디오 입력 표현을 융합하여 음악 샘플을 쿼리하는 새로운 삼중 모달 모델을 학습합니다. 학습을 위해 우리는 모델 성능에 중요한 텍스트 드롭아웃 정규화 메커니즘을 도입합니다. 우리의 모델 설계는 검색된 음악 오디오가 비디오에 묘사된 시각적 스타일과 자연어 쿼리에 설명된 음악 장르, 분위기 또는 악기를 일치시켜 두 가지 입력 모달리티와 일치하도록 합니다. 셋째, 접근 방식을 평가하기 위해 YT8M-Music Video 데이터 세트의 4k 클립 하위 세트에 자연어 음악 설명을 주석으로 달아 문제에 대한 테스트 데이터 세트를 수집합니다. 이 설명은 공개적으로 제공됩니다. 텍스트 안내를 사용할 때 검색 정확도를 크게 향상시키는 동시에 비디오-음악 검색에서 이전 방법의 성능과 동일하거나 이를 능가할 수 있음을 보여줍니다. 언어 안내 음악 검색 검색된 뮤직 비디오+언어 쿼리 기타가 있는 포크 음악 경쾌한 팝 경쾌한 팝 사용자 입력에는 대상 음악의 비디오와 텍스트 설명이 포함됩니다. ViML 여성 보컬리스트, 기타, 탬버린이 등장하는 음울한 포크 발라드 ViML 여성 보컬리스트가 활기찬 신스 멜로디를 선보이는 ViML 팝 기타, 드럼이 반주하는 남성 보컬리스트가 등장하는 경쾌한 팝송 저자가 제공한 검색된 음악 트랙에 대한 설명은 예시 목적으로만 제공됩니다. 그림 1. 언어 안내 음악 검색. ViML 모델은 비디오와 텍스트 프롬프트를 입력으로 사용하여 데이터베이스에서 적합한 음악 트랙을 검색합니다. 이 모델은 검색을 안내하기 위해 비디오와 언어 표현을 융합하는 법을 배웁니다. 우리의 접근 방식이 비디오와 언어 콘텐츠 모두와 일치하는 오디오를 검색하는 방식에 주목하세요. 동일한 비디오 쿼리(위쪽 두 행)의 경우, 언어 쿼리와 일치하도록 음악 스타일을 변경할 수 있으며, 동일한 Upbeat 팝 쿼리(아래쪽 두 행)의 경우 비디오 콘텐츠와 일치하도록 보컬리스트를 변경할 수 있습니다. 결과를 충분히 이해하려면 웹사이트에서 동반 비디오를 보고 들어보세요. 1.
--- INTRODUCTION ---
크리에이터를 위한 비디오 편집 프로세스에서 중요한 부분은 음악 사운드트랙을 선택하는 것입니다. 특히 소셜 미디어 플랫폼에서 단편 비디오가 증가함에 따라 자동화된 음악 추천 시스템이 비디오 편집 애플리케이션에서 점점 더 일반적이고 중요한 부분이 되었습니다. 이러한 시스템은 관련 음악을 찾는 데 도움이 될 수 있지만 종종 추천되는 음악 유형에 대한 사용자 제어 기능이 제한적입니다. 이전 작업에서는 비디오의 시각적 콘텐츠와 스타일만을 기반으로 음악을 검색했습니다[31,36]. 그러나 음악 자체는 비디오를 어떻게 인식해야 하는지에 대한 중요한 정보를 전달할 수 있습니다. 음악 선택만으로도 시각적 장면을 행복, 무섭거나 슬픈 것으로 인식되는 장면으로 바꿀 수 있습니다¹. 결과적으로 입력된 비디오에 대한 대상 음악을 설명하는 사용자 입력 기능이 부족한 것은 현재 음악 추천 방법의 유용성에 대한 주요 제한 사항입니다. 이 연구에서는 사용자가 그림 1에 표시된 분위기, 장르 또는 악기 연주를 포함한 특정 음악 속성에 대한 추천을 안내할 수 있는 보다 유연한 음악 대 비디오 추천 접근 방식을 제안합니다. 유연성과 사용자 편의성을 극대화하기 위해 자유형 자연어(예: 그림 1의 &quot;기타가 있는 포크 음악&quot;) 형태로 사용자 음악 속성 설명을 사용하는 것을 제안합니다. 비디오에 대한 언어 안내 음악 추천 모델을 학습하는 데는 두 가지 주요 과제가 있습니다. 첫째, 음악+텍스트[5,7,19,29] 또는 음악+비디오[1]를 포함하는 데이터 세트가 있지만 음악, 비디오 및 텍스트를 함께 포함하는 사용 가능한 데이터 세트는 없습니다. 또한 텍스트와 음악을 포함하는 기존 데이터 세트는 자유형 텍스트가 아닌 제한된 태그 어휘에 초점을 맞춥니다. 둘째, 이전 연구에서는 시각적, 오디오 및 텍스트 임베딩[2-4, 33, 46]을 공동으로 학습하는 방법을 탐구했으며 신중한 정규화 없이는 네트워크가 과적합되어 입력 모달리티 중 하나를 무시하도록 학습할 수 있습니다. 우리는 네트워크를 통한 정보 흐름을 유지하고 모달리티 중 하나를 무시하지 않는 모델을 훈련하고자 합니다. 위에 설명된 과제를 충족하기 위해, 우리의 작업은 다음과 같은 기여를 합니다: (1) 우리는 뮤직 비디오 데이터 세트에 대한 자연어 설명을 자동으로 생성하는 새로운 접근 방식을 제안합니다. 이 접근 방식은 사전 훈련된 음악 태거와 대규모 언어 모델을 결합하여 그림 2(왼쪽)에 나와 있는 모든 음악 클립에 대한 자연어 설명을 출력합니다. 먼저, 태거는 음악 장르, 분위기 또는 악기를 설명하는 사전 정의된 어휘에서 태그를 예측합니다. 둘째, 이러한 예측된 태그는 확률과 함께 소수의 인간이 제공한 텍스트 설명(즉, A(태그): A&#39;(설명) :: B(태그): B&#39;(설명))과의 유추를 기반으로 신중하게 설계된 대규모 언어 모델 프롬프트 절차를 사용하여 뮤직 비디오에 대한 풍부한 자연어 설명으로 변환됩니다. 여기서 A와 B는 태거가 자동으로 제공한 음악 태그이고, A&#39;는 인간이 제공한 텍스트 설명이며, B&#39;는 대규모 언어 모델이 출력한 자연어 설명입니다. (2) 비디오-텍스트 융합 모듈을 갖춘 Transformer 기반 모델 아키텍처를 제안합니다. ViML(Video to Music with Language)이라고 하는 모델은 입력 비디오의 시각적 콘텐츠/스타일과 자연어 쿼리에 설명된 음악 장르, 분위기, 악기 연주와 모두 일치하는 음악을 검색할 수 있습니다. 이전 연구[16,27,38]와 유사하게, 우리는 정규화 메커니즘으로 텍스트 드롭아웃을 사용하여 훈련하는 것이 추가된 텍스트 입력에서 음악 검색 성능 향상을 달성하는 데 중요하다는 것을 발견했습니다.(3) 우리는 언어 기반 음악 추천을 평가하기 위해 YT8M-뮤직 비디오 데이터 세트[1]의 하위 집합에서 클립에 대한 4000개의 고품질 텍스트 주석 데이터 세트를 공개합니다. 우리는 우리의 방법이 텍스트 입력을 통합할 때 음악 검색에 대한 이전 연구보다 상당한 개선을 이룰 수 있음을 보여줍니다. 더욱이, 우리의 모델은 텍스트 입력을 무시할 때 기준 음악 대 비디오 추천 모델의 성능과 일치하거나 심지어 능가할 수 있습니다.2.
--- RELATED WORK ---
음악 및 언어. 분위기, 장르 또는 악기 연주와 같은 속성을 지정하는 태그가 포함된 수많은 음악 태그 데이터 세트가 있으며 [5, 7, 19, 29], 여러 연구에서 이러한 데이터 세트에서 자동화된 음악 태거를 훈련하는 방법을 연구했습니다 [10,20,21, 30, 42, 43]. 이러한 것 외에도
--- METHOD ---
사용자가 자유형 자연어로 음악 선택을 안내할 수 있도록 하면서 입력 비디오에 음악을 추천하는 것입니다. 이 문제 설정의 핵심 과제는 기존 뮤직 비디오 데이터 세트가 필요한 (비디오, 음악) 학습 쌍을 제공하지만 음악에 대한 텍스트 설명이 부족하다는 것입니다. 이 작업은 다음 세 가지 기여를 통해 이 과제를 해결합니다. 첫째, 사전 학습된 음악 태거 출력과 소수의 인간 텍스트 설명이 주어진 대규모 언어 모델(BLOOM-176B)에서 자연어 음악 설명을 생성하기 위해 유추 기반 프롬프트 절차에 의존하는 텍스트 합성 접근 방식을 제안합니다. 둘째, 이러한 합성된 음악 설명을 사용하여 텍스트와 비디오 입력 표현을 융합하여 음악 샘플을 쿼리하는 새로운 삼중 모달 모델을 학습합니다. 학습을 위해 모델 성능에 중요한 텍스트 드롭아웃 정규화 메커니즘을 도입합니다. 모델 설계를 통해 검색된 음악 오디오가 비디오에 묘사된 시각적 스타일과 자연어 쿼리에 설명된 음악 장르, 분위기 또는 악기를 일치시켜 두 입력 모달리티와 일치하도록 할 수 있습니다. 셋째, 접근 방식을 평가하기 위해 YT8M-Music Video 데이터 세트의 4k 클립 하위 세트에 자연어 음악 설명을 주석으로 달아 문제에 대한 테스트 데이터 세트를 수집합니다. 이 설명은 공개적으로 제공됩니다. 텍스트 안내를 사용할 때 검색 정확도를 크게 향상시키는 동시에 비디오-음악 검색에서 이전 방법의 성능과 동일하거나 이를 능가할 수 있음을 보여줍니다. 언어 안내 음악 검색 검색된 뮤직 비디오+언어 쿼리 기타가 있는 포크 음악 경쾌한 팝 경쾌한 팝 사용자 입력에는 대상 음악의 비디오와 텍스트 설명이 포함됩니다. ViML 여성 보컬리스트, 기타, 탬버린이 등장하는 음울한 포크 발라드 ViML 여성 보컬리스트가 활기찬 신스 멜로디를 선보이는 ViML 팝 기타, 드럼이 반주하는 남성 보컬리스트가 등장하는 경쾌한 팝송 저자가 제공한 검색된 음악 트랙에 대한 설명은 예시 목적으로만 제공됩니다. 그림 1. 언어 안내 음악 검색. ViML 모델은 비디오와 텍스트 프롬프트를 입력으로 사용하여 데이터베이스에서 적합한 음악 트랙을 검색합니다. 모델은 검색을 안내하기 위해 비디오와 언어 표현을 융합하는 법을 배웁니다. 우리의 접근 방식이 비디오와 언어 콘텐츠 모두와 일치하는 오디오를 검색하는 방식에 주목하세요. 동일한 비디오 쿼리(위쪽 두 행)에 대해 언어 쿼리와 일치하도록 음악 스타일을 변경할 수 있고 동일한 Upbeat 팝 쿼리(아래쪽 두 행)에 대해 비디오 콘텐츠와 일치하도록 보컬리스트를 변경할 수 있습니다. 결과를 충분히 이해하려면 웹사이트에서 동반 비디오를 보고 들어보세요. 1. 서론 크리에이터를 위한 비디오 편집 프로세스의 핵심 부분은 음악 사운드트랙을 선택하는 것입니다. 특히 소셜 미디어 플랫폼에서 단편 비디오가 증가함에 따라 자동화된 음악 추천 시스템은 비디오 편집 애플리케이션에서 점점 더 일반적이고 중요한 부분이 되었습니다.*Adobe Research에서 인턴으로 수행한 작업 ing 이러한 시스템은 관련 음악을 찾는 데 도움이 될 수 있지만 종종 추천되는 음악 유형에 대한 사용자 제어 기능이 제한적입니다. 이전 작업에서는 비디오의 시각적 콘텐츠와 스타일만을 기반으로 음악을 검색했습니다[31,36]. 그러나 음악 자체는 비디오를 어떻게 인식해야 하는지에 대한 중요한 정보를 전달할 수 있습니다. 음악 선택만으로도 시각적 장면을 행복, 무서움 또는 슬픔으로 인식되는 장면으로 바꿀 수 있습니다¹. 결과적으로 입력된 비디오에 대한 대상 음악을 설명하는 사용자 입력 기능의 부족은 현재 음악 추천 방법의 유용성에 대한 주요 제한입니다. 이 연구에서는 사용자가 그림 1에 표시된 분위기, 장르 또는 악기 연주를 포함한 특정 음악 속성에 대한 추천을 안내할 수 있는 보다 유연한 음악 대 비디오 추천 접근 방식을 제안합니다. 유연성과 사용자 편의성을 극대화하기 위해 자유형 자연어(예: 그림 1의 &quot;기타가 있는 포크 음악&quot;) 형태로 사용자 음악 속성 설명을 사용하는 것을 제안합니다. 비디오에 대한 언어 안내 음악 추천 모델을 학습하는 데는 두 가지 주요 과제가 있습니다. 첫째, 음악+텍스트[5,7,19,29] 또는 음악+비디오[1]를 포함하는 데이터 세트가 있지만 음악, 비디오 및 텍스트를 함께 포함하는 사용 가능한 데이터 세트는 없습니다. 또한 텍스트와 음악을 포함하는 기존 데이터 세트는 자유형 텍스트가 아닌 제한된 태그 어휘에 초점을 맞춥니다. 둘째, 이전 연구에서는 시각적, 오디오 및 텍스트 임베딩[2-4, 33, 46]을 공동으로 학습하는 방법을 탐구했으며 신중한 정규화 없이는 네트워크가 과적합되어 입력 모달리티 중 하나를 무시하도록 학습할 수 있습니다. 우리는 네트워크를 통한 정보 흐름을 유지하고 모달리티 중 하나를 무시하지 않는 모델을 훈련하고자 합니다. 위에 설명된 과제를 충족하기 위해, 우리의 작업은 다음과 같은 기여를 합니다: (1) 우리는 뮤직 비디오 데이터 세트에 대한 자연어 설명을 자동으로 생성하는 새로운 접근 방식을 제안합니다. 이 접근 방식은 사전 훈련된 음악 태거와 대규모 언어 모델을 결합하여 그림 2(왼쪽)에 나와 있는 모든 음악 클립에 대한 자연어 설명을 출력합니다. 먼저, 태거는 음악 장르, 분위기 또는 악기를 설명하는 사전 정의된 어휘에서 태그를 예측합니다. 둘째, 이러한 예측된 태그는 확률과 함께 소수의 인간이 제공한 텍스트 설명(즉, A(태그): A&#39;(설명) :: B(태그): B&#39;(설명))과의 유추를 기반으로 신중하게 설계된 대규모 언어 모델 프롬프트 절차를 사용하여 뮤직 비디오에 대한 풍부한 자연어 설명으로 변환됩니다. 여기서 A와 B는 태거가 자동으로 제공한 음악 태그이고, A&#39;는 인간이 제공한 텍스트 설명이며, B&#39;는 대규모 언어 모델이 출력한 자연어 설명입니다. (2) 비디오-텍스트 융합 모듈을 갖춘 Transformer 기반 모델 아키텍처를 제안합니다. ViML(Video to Music with Language)이라고 하는 모델은 입력 비디오의 시각적 콘텐츠/스타일과 자연어 쿼리에 설명된 음악 장르, 분위기, 악기 연주와 모두 일치하는 음악을 검색할 수 있습니다. 이전 연구[16,27,38]와 유사하게, 우리는 정규화 메커니즘으로 텍스트 드롭아웃을 사용하여 훈련하는 것이 추가된 텍스트 입력에서 음악 검색 성능 개선을 달성하는 데 중요하다는 것을 발견했습니다.(3) 우리는 언어 기반 음악 추천을 평가하기 위해 YT8M-뮤직 비디오 데이터 세트[1]의 하위 집합에서 클립에 대한 4000개의 고품질 텍스트 주석 데이터 세트를 공개합니다. 우리는 우리의 방법이 텍스트 입력을 통합할 때 음악 검색에 대한 이전 연구보다 상당한 개선을 이룰 수 있음을 보여줍니다. 게다가, 우리의 모델은 텍스트 입력을 무시할 때 기준 음악 대 비디오 추천 모델의 성능과 일치하거나 심지어 능가할 수 있습니다.2. 관련 연구 음악과 언어. 영어: 기분, 장르 또는 악기 연주와 같은 속성을 지정하는 태그가 포함된 수많은 음악 태그 데이터 세트가 있으며 [5, 7, 19, 29], 여러 연구에서 이러한 데이터 세트에서 자동 음악 태거를 훈련하는 방법을 연구했습니다 [10, 20, 21, 30, 42, 43]. 제한된 태그 어휘에 국한된 이러한 방법 외에도 일부 연구에서는 음악과 자유형 자연어를 함께 임베딩하는 방법도 연구했습니다 [9, 14, 26, 43]. 그러나 이러한 접근 방식 중 어느 것도 비디오 모달리티를 통합하지 않습니다. 비디오에 대한 음악 추천. 다른 사람들은 입력 비디오의 스타일과 콘텐츠를 기반으로 음악을 자동으로 추천하는 방법을 조사했습니다 [13, 23, 31, 36, 48]. Prétet et al. [31]은 수작업으로 만든 기능 대신 학습된 오디오 기능을 통합하여 이전의 자체 감독 방법 [13]을 기반으로 구축했습니다. 최근에는 Surís et al. [36]은 자기 감독 대조 손실과 Transformer[37] 아키텍처를 채택하여 주어진 입력 비디오에 적합한 음악을 검색하기 위해 장거리 시간적 맥락 모델링을 크게 개선하는 MVPt 모델을 제안합니다. 그러나 이러한 접근 방식 중 어느 것도 이 작업에서 중점을 두는 자연어 모달리티를 통합하지 않습니다. 비디오, 오디오 및 언어. 다양한 작업에서 시청각 또는 시각 언어 주제를 탐구했지만 소수의 작업이 비디오, 오디오 및 언어를 공동으로 내장하는 데 중점을 둡니다[2, 4, 12, 33, 44, 46]. 특히 Alayrac et al.[3]은 오디오와 비디오를 텍스트 표현과 가장 잘 결합하는 방법을 조사합니다. VATT 모델[2]은 모달리티 전체에서 단일 공유 Transformer 백본을 사용할 수 있는 완전한 엔드투엔드 3모달 모델입니다. 마지막으로 두 가지 최근 방법[12, 44]은 CLIP[32]을 확장하여 오디오를 공동으로 내장합니다. 관련성이 있지만 이러한 모든 접근 방식은 음악보다는 &quot;환경적&quot; 또는 &quot;일상적&quot; 소리에 공통적으로 초점을 맞추고 있으며 결과적으로 음악 추천에 중요한 장거리 시간적 맥락 모델링이 부족합니다. 또한 이러한 작업 중 어느 것도 두 가지 모달리티(비디오, 텍스트)를 결합하여 다른 모달리티(음악)의 결과를 쿼리하는 다운스트림 문제를 다루지 않습니다. Few-shot 언어 모델 프롬프팅. 최근의 대규모 언어 모델은 read→ Music Tagger Music Tagger I. prompt2text 합성 BLOOM-176B에 대한 프롬프트 입력: [입력]: 장르: 일렉트로닉(82.6%), 댄스(63.9%); 분위기: 다이내믹(46.8%), 드라마틱 A(33.3%); 악기: 신시사이저 키보드(81.9%), 일렉트로닉 드럼셋(77.6%), 신스 베이스(68.4%) [출력]: 높은 A&#39; 에너지 신스 라인과 오토튠된 여성 보컬.➡B [입력]: 장르: 컨트리(47.2%), 록(30.8%); 분위기: 행복(40.2%), 휴식(31.7%), 향수(30.9%); 악기: 드럼셋(63.9%), 일렉트릭 기타(50.5%), 남성 보컬(49.7%), 일렉트릭 베이스(49.1%), 어쿠스틱 기타(33.7%) [출력]: 출력: 향수를 불러일으키는 느낌의 컨트리 록 트랙입니다. B&#39; 곡은 어쿠스틱 기타, 일렉트릭 기타, 일렉트릭 베이스, 드럼, 남성 보컬이 특징입니다. … 장르: 컨트리, 록 II. data2text 합성 음악 태거 분위기: 행복, 휴식, 향수 악기: 드럼셋, 남성 보컬, 어쿠스틱 기타 템플릿 문장에 태그 입력 이것은 컨트리와 록 음악입니다. 음악은 행복하고, 휴식적이며, 향수를 불러일으키는 분위기를 줍니다. 사운드트랙에는 드럼셋, 남성 보컬, 어쿠스틱 기타가 있습니다.제로샷 D2T 파이프라인: 순서, 집계 및 압축 이것은 컨트리와 록 음악입니다.사운드트랙에는 어쿠스틱 기타, 드럼셋, 남성 보컬이 있어 행복하고, 편안하고, 향수를 불러일으키는 분위기를 줍니다.뮤직 태거 III.태그 합성 어쿠스틱 기타, 컨트리, 행복, 드럼셋, 휴식, 일렉트릭 베이스, 남성 보컬, 록, 향수, 일렉트릭 기타 그림 2. 우리 작업에서 탐구한 세 가지 텍스트 합성 접근 방식 개요.모두 사전 학습된 음악 태거 모델의 태그 예측에 의존합니다.각 방법의 출력 텍스트를 녹색으로, 태거의 입력을 파란색 글꼴로, 인간 주석자의 입력을 빨간색 글꼴로 강조 표시합니다.왼쪽: 자동으로 예측된 음악 태그와 소수의 인간 설명이 주어진 경우 자연어 설명을 생성하기 위한 prompt2tags 접근 방식을 소개합니다. 우리는 대규모 언어 모델(BLOOM-176B)에 음악 태그(A, B)와 설명(A&#39;, B&#39;) 사이의 유추 작업(A : A&#39; :: B : B&#39;)을 완료하도록 요청합니다. 오른쪽 위: data2text 파이프라인은 각 태그 범주에 해당하는 무작위로 선택된 템플릿 문장에 샘플링된 태그를 삽입합니다. 그런 다음 Zero-shot D2T 모델[17]은 이러한 템플릿을 정렬, 집계 및 압축하여 최종 출력 설명을 만듭니다. 오른쪽 아래: 태그 접근 방식은 신뢰도가 높은 태그를 직접 연결하여 음악의 텍스트 설명을 형성하는 것을 포함합니다. 이해 및 QA[8,40], 추론[18,41] 또는 심지어 데이터 증강[11,39,45]에 이르기까지 다양합니다. 일부 연구에서는 이러한 성공을 멀티모달 애플리케이션으로 확장했습니다. 예를 들어, Zeng et al.[47]은 언어 모델이 이러한 문제를 대규모 시각적 또는 오디오 모델의 입력을 사용하여 독해 이해 또는 QA 작업으로 재구성하여 비디오 이해 또는 이미지 캡션 작업을 해결할 수 있음을 보여줍니다. 다른 연구에서는 다중 모달 작업[24] 또는 로봇 계획[15,34]을 위한 텍스트 주석 생성 또는 검색을 돕기 위해 언어 모델을 사용했습니다. 이 연구에서 우리는 완전히 새로운 few-shot 언어 쿼리 모델링 응용 프로그램을 제안합니다. 음악 태그에서 자유형 음악 텍스트 설명을 생성합니다. 3. 접근 방식 우리의 목표는 그림 3에서 설명한 대로 비디오 및 음악 텍스트 설명(v, t)의 입력 쌍과 음악 클립 m 사이의 유사도 s(fut, fm)를 예측할 수 있는 한 쌍의 피처 인코더 fut 및 ƒm을 학습하는 것입니다. 이러한 모델을 지도 학습 방식으로 학습하려면 해당 트리플릿(v, m, t)의 데이터 세트가 필요합니다. 음악이 쌍으로 있는 비디오의 대규모 데이터 세트가 있지만, 쌍으로 된 음악 트랙의 고품질 자연어 설명이 포함된 데이터 세트는 찾기 어렵습니다. 결과적으로 우리는 각 음악 트랙의 음악 태그 형태로 사용 가능한 구조화된 데이터에서 텍스트 설명을 생성하는 모델 G에 기반한 합성 접근 방식을 조사합니다. 다음 섹션에서는 언어 안내 비디오-음악 추천 모델을 훈련하는 방법을 설명하기 전에 먼저 음악 설명 합성 방법 G에 대해 논의합니다.3.1. 음악에 대한 텍스트 설명 합성 비디오 및 음악 오디오 쌍(vi, mi) 집합이 주어지고 음악 mi를 설명하는 구조화된 데이터 di ETD에도 액세스할 수 있다고 가정합니다.이 경우 이 구조화된 데이터는 신뢰도가 있는 음악 태그로 구성됩니다.각 음악 트랙 m¿는 자유형 인간 텍스트 설명 t¿ Є TT로 설명될 수 있습니다.그러나 대규모로 고품질 인간 설명을 얻는 것은 엄청나게 비쌀 수 있습니다.대신 생성기 G : TD → TT를 사용하여 이러한 텍스트 설명을 합성하는 것을 제안합니다.이 생성기는 오디오 트랙을 설명하는 구조화된 데이터를 자연스러운 인간 설명 공간에 매핑합니다. = 이러한 매핑 함수 G의 목표는 다음과 같습니다. (i) 예측 출력 ti = G(d)는 특정 음악 트랙에 해당하는 구조화된 데이터 d¿에 포함된 의미적 의미를 보존해야 하고 (ii) 예측 출력 t의 분포는 실제 인간 텍스트 주석 t₁ = TT의 분포를 따라야 합니다. 생성 함수 G가 되도록 완전 지도 모델을 학습하려면 대량의 인간 텍스트 설명이 필요합니다. 대신 생성 함수 G를 얻기 위해 제로샷 또는 퓨샷 접근 방식을 살펴봅니다. 특히, 모두 자동으로 예측된 음악 태그를 사용하는 세 가지 접근 방식을 설명합니다. 사전 학습된 언어 모델의 신중한 퓨샷 프롬프팅에 전적으로 의존하는 prompt2text 접근 방식, 사전 학습된 언어 모델을 사용하여 템플릿화된 문장을 다시 표현하는 제로샷 data2text 접근 방식, 자동으로 얻은 태그 집합을 통해 음악 트랙 설명을 직접 나타내는 제로샷 태그 베이스라인입니다. 자세한 내용은 다음과 같습니다. ~ I. 퓨샷 prompt2text 접근 방식. 우리는 먼저 신중한 few-shot 프롬프팅을 통해 전체 매핑 함수 G가 단일 대규모 언어 모델에 포함될 수 있는지 살펴봅니다. 이 접근 방식은 t; ~ TT인, ..., ty에 대한 작은 세트의 인간이 제공한 설명에 의존합니다. 우리는 각 예제 ti에 대해 자동 음악 태거가 제공하는 동일한 오디오 트랙을 설명하는 쌍의 구조화된 데이터 출력 d¿도 있다고 가정합니다. 무조건 생성기 G를 목표로 하는 이전의 프롬프트 기반 데이터 증강 작업[11, 39, 45]과 달리 우리는 인간 문장 t¿ Є TT의 분포를 따르도록 구조화된 데이터 d¿에 따라 텍스트 데이터 ti G(di)를 생성하는 것을 목표로 합니다. 그림 2(왼쪽)에서 볼 수 있듯이 구조화된 데이터 출력 di는 템플릿을 통해 텍스트 형태로 변환되고, 쌍 (do, to), ..., (dk, tk) 세트를 사용하여 프롬프트에서 k개의 입출력 구성 요소를 형성합니다. 프롬프트의 마지막 세그먼트는 새로운 음악 트랙에 해당하는 구조화된 데이터 d;입니다. di가 주어지면, 모델은 예시 입력에서 제안된 매핑 TD → TT에 따라 설명 ti를 출력하려고 시도합니다. 이 설정에서 텍스트를 생성하기 위해, 우리는 매우 다양한 1.5TB 텍스트 코퍼스에서 훈련된 BLOOM-176B [6] 모델을 사용합니다. prompt2text가 생성에서 가장 큰 자유를 허용한다는 점을 감안할 때, 모델은 대상 분포 Ț와 유사한 다양한 텍스트 세트를 더 쉽게 생성할 수 있습니다. prompt2text 접근 방식은 BLOOM과 같은 대규모 언어 모델이 태그와 해당 신뢰도 예측과 같은 다양한 구조화된 데이터 입력을 처리할 수 있기 때문에 매우 유연합니다. 그러나 이 모델은 구조화된 데이터에서 의미적 의미를 보존할 가능성이 낮을 수도 있습니다. II. 제로샷 data2text 접근 방식. 우리가 제안하는 두 번째 설정은 그림 2(오른쪽 상단)에 설명된 데이터-텍스트 생성 프로세스를 포함합니다. 높은 수준에서 이 방법의 목표는 미리 정의된 템플릿 문장에 구조화된 태그 데이터를 삽입하고 원래 의미적 의미를 보존하면서 언어 모델을 사용하여 이러한 템플릿 문장을 다시 표현하는 것입니다. 우리는 각 음악 트랙에 대해 예측된 태그를 장르, 분위기, 악기 카테고리로 그룹화하는 것으로 시작합니다. 우리는 태그에 대한 플레이스홀더가 있는 짧은 문장 형태의 카테고리별 템플릿 세트를 정의합니다. 우리는 각 카테고리에 대한 템플릿 문장을 무작위로 샘플링하고, 그 카테고리에 대한 높은 신뢰도의 예측 태그로 템플릿을 채웁니다. 이러한 문장을 보다 자연스러운 자유형 설명으로 형성하기 위해 사전 훈련된 대규모 언어 모델을 사용합니다. 구체적으로, 우리는 사전 훈련된 RoBERTa [25] 및 BART [22] 언어 모델을 기반으로 하는 순서 지정, 집계 및 압축 모듈로 구성된 Zero-shot D2T 접근 방식[17]을 따릅니다. 파이프라인 구성 요소는 먼저 채워진 개별 템플릿 문장의 순서를 설정하고 어떤 문장을 단일 문장으로 결합해야 하는지 지정합니다. 다음으로, 압축 모듈은 생성 텍스트 모델을 사용하여 순서 지정 및 집계 사양에 따라 입력 문장을 다시 작성합니다. 이 모듈은 의미적 의미를 보존하면서 정보를 다시 표현하는 것을 목표로 합니다. 이 D2T 파이프라인은 대규모 일반 텍스트 코퍼스에서 사전 학습된 모델을 활용하기 때문에 이러한 모듈은 제로샷 방식으로 음악 설명을 생성하는 데 우수한 성능을 발휘하는 것으로 나타났습니다.III. 태그 접근 방식.사용하는 마지막 설정은 예측 태그의 간단한 연결을 포함합니다.각 음악 트랙에 대해 가장 많이 필터링된 예측 태그 세트를 가져옵니다(이 세트는 일반적으로 총 10-15개의 태그입니다).그런 다음 모델의 순서 의존성을 방지하기 위해 이러한 태그를 무작위로 섞고 모든 태그를 쉼표로 구분된 음악 설명 목록(예: &quot;신디사이저 키보드, 전자 드럼세트, 팝, 댄스, 신스 베이스, 일렉트로닉, 해피, 일렉트릭 기타, 프랜틱, 다이내믹&quot;)으로 연결합니다.이 접근 방식은 의미적 의미를 강력하게 보존하지만 인간의 주석 분포 TT를 잘 나타낼 수 있는 다양한 어휘와 형식의 텍스트를 생성하지 못합니다.3.2. 음악 검색 학습을 위한 텍스트 드롭아웃 여기서 목표는 쿼리 비디오 v와 대상 음악 트랙을 설명하는 자연어 쿼리 t와 일치하는 음악 트랙 m을 검색하는 것입니다. 이것은 모델이 입력 비디오와 입력 언어 쿼리의 정보를 모두 융합하여 의미적으로 적절한 음악 트랙을 찾아야 하기 때문에 어려운 작업입니다.또한 오디오/비디오와 텍스트 간의 세분성 차이는 학습을 크게 방해할 수 있습니다.이 작업을 위해 ViML이라는 3중 모드 접근 방식을 설계하고 세분성 문제를 해결하기 위해 텍스트 드롭아웃을 도입합니다.드롭아웃이 개별 뉴런 간의 공동 적응을 줄여 과적합을 방지하는 방식[35]과 유사한 방식으로 텍스트 드롭아웃은 텍스트 입력에 대한 과적합을 방지하고 비디오와 텍스트 인코더 간의 공동 적응을 방지합니다.이 접근 방식은 그림 3에 설명되어 있으며 모델 아키텍처, 손실 및 텍스트 드롭아웃에 대한 세부 정보가 다음에 나와 있습니다.모델 아키텍처. 우리 모델은 (비디오, 음악, 텍스트) 페어링 세트(v, m, t)에 대해 학습되었으며, 이는 CLIP 이미지 인코더 g³ Visual Transformer 입력 비디오 CLIP 텍스트 인코더 xt 텍스트 변환기 퓨전 모듈 fvt 입력 텍스트 8 gt 텍스트 드롭아웃 입력 음악 DeepSim 음악 인코더 gm xm 음악 변환기 fm 특징 유사도 s(yut, ym) 그림 3에 해당합니다. 제안하는 ViML 모델은 세 가지 모달리티(비디오, 텍스트 및 오디오)의 입력을 임베딩 공간에 임베딩합니다. 음악 입력의 경우 DeepSim [20]을 사용하여 기본 특징을 추출하고 비디오 프레임 및 텍스트 설명의 경우 CLIP [32]에서 기본 특징을 추출합니다. 이러한 기본 특징은 각 모달리티의 Transformer 인코더에 입력됩니다. 비디오 및 텍스트 특징은 퓨전 모듈과 결합되어 공유 임베딩 공간에서 음악을 쿼리할 수 있습니다. 마지막으로 텍스트 드롭아웃을 사용하여 세 가지 모달리티 간의 세분성 차이를 해결합니다. 비디오는 보다 복잡한 입력 모드이기 때문에 텍스트 드롭아웃은 비디오와 텍스트 표현의 동시적 적응을 방지하여 향상된 비디오 표현을 강제로 실행합니다. 섹션 3.1에 설명된 대로, 뮤직 비디오 클립 v에는 뮤직 트랙 m에 대한 생성된 텍스트 설명 t가 지정됩니다. 우리는 이러한 입력을 시각적 비디오 특징의 경우 기본 특징 xv g&quot;(v), 음악 특징의 경우 xm = gm(m), 텍스트 특징의 경우 xt = gt(t)로 변환합니다. 이는 학습 중에 고정되는 사전 학습된 대규모 인코더 gº, gm 및 gt를 사용합니다. = 각 기본 특징 표현 x는 차원 nxd이며, 여기서 n은 비디오 클립을 나타내는 기본 특징의 시간 시퀀스의 길이이고 d는 기본 특징의 차원입니다. 우리 모델은 음악이나 비디오와 유사한 시간 텍스트 설명 시퀀스를 처리할 수 있지만 실제로는 트랙 수준의 텍스트 설명만 얻으므로 텍스트의 경우 n = 1입니다. 우리의 3중 모달 모델은 각 모달리티 fʊ, fm, ft에 해당하는 세 개의 별도 모듈과 비디오 및 텍스트 표현을 결합하는 네 번째 퓨전 모듈 fut로 구성됩니다. 모듈은 각각의 기본 특징을 취하고 임베딩 y = f(x²), ym = fm(xm), yt = ft(x²)를 출력합니다. 퓨전 모델은 다음을 출력합니다. 비디오 및 텍스트 임베딩 yʊt = fvt(yʊ, y²)에서 융합된 임베딩. 융합 손실. 학습을 위해 음악과 융합된 비디오-텍스트 임베딩 간의 InfoNCE 손실[28]을 사용합니다. Lvt→m iЄD exp(s(yot, ym)/T) ΣjED exp(s(yt, ym)/T) (1) 여기서 s는 유사도 함수이고, D는 데이터 배치이며, T는 7 = 0.03으로 설정한 온도 하이퍼파라미터입니다. 유사도 메트릭의 경우 s(x, y) = x²y/(||x|| ||y||)로 정의된 코사인 유사도를 사용합니다. 손실 Lut→m은 음악 임베딩에서만 음수가 샘플링되므로 대칭이 아님에 유의하세요. 손실이 대칭적이 되도록 대신 합산 손실 Lm,vt = Lvt→m + Lm→vt로 훈련합니다. 텍스트 드롭아웃. 오디오/비디오와 텍스트 간의 세분성 차이로 인해 발생하는 어려움을 해결하기 위해 정규화 메커니즘으로 텍스트 드롭아웃을 도입합니다. 확률 p로 입력 텍스트 임베딩 xt를 특정 값 x NULL로 설정합니다. 실제로 이 x NULL 입력을 빈 문자열에 대한 사전 훈련된 gt 모델에서 생성된 임베딩으로 할당합니다. 그러나 x NULL로 0 벡터를 사용하는 것도 비슷하게 작동합니다. 텍스트와 비디오를 함께 사용하여 음악 검색 성능을 개선하는 것 외에도 텍스트 드롭아웃으로 훈련하면 텍스트 입력에 대한 종속성을 제거하여 비디오만에서 검색하는 경우에도 우수한 성능을 보이는 모델을 얻을 수 있습니다. 3.3. 구현 세부 정보 음악 태그 생성. 텍스트 생성 프로세스의 첫 번째 주요 단계는 각 음악 트랙을 설명하는 구조화된 데이터를 얻는 것입니다. 이를 위해 우리는 태그의 고정된 사전 정의된 어휘로 수동으로 주석이 달린 음악 트랙 데이터 세트에서 훈련된 음악 태거를 사용합니다[20]. 구체적으로, 태거는 41개 악기 태그, 20개 장르 태그, 28개 분위기 태그에 대한 신뢰도를 예측합니다. 우리는 클립 또는 트랙 수준에서 이러한 예측을 집계하고 신뢰도를 기반으로 후속 세트를 필터링하여 특정 임계값(우리의
--- EXPERIMENT ---
s). ViML 모델. MVPt[36]에 따라 음악 및 비디오 인코더 fm 및 fv에 Transformer 아키텍처[37]를 사용합니다. Transformer는 비디오 및 음악 클립에서 장기 컨텍스트를 인코딩하여 모델 성능을 개선하는 데 중요한 역할을 합니다. 또한 텍스트 인코더 ft 및 비디오-텍스트 퓨전 레이어 fut에 유사한 2계층 Transformer 아키텍처를 사용합니다. 그러나 단일 선형 레이어와 같은 다른 퓨전 모듈 아키텍처가 유사한 결과를 산출한다는 것을 발견했습니다. 퓨전 모듈 아키텍처 연구에 대한 보충 자료를 참조하세요. 기본 피처의 경우 CLIP[32]를 사용하여 비디오 프레임 및 텍스트 입력에 대한 표현을 인코딩하고 DeepSim[20]을 사용하여 음악을 인코딩합니다. [36]의 저자와 소통한 후 비디오를 10초 세그먼트로 분할하고 초당 6프레임으로 계산된 CLIP 임베딩 피처를 평균하여 각 세그먼트에 대한 피처를 계산합니다. OpenAI의 CLIP ViT-B/32 모델을 사용하여 512차원 CLIP 임베딩 피처를 계산합니다. 우리는 각 모달리티에 대한 선형 투영 레이어를 사용하여 모든 입력 기반 피처를 d 256 크기의 임베딩으로 인코딩합니다. 또한 모델에서 인코딩된 비디오, 텍스트, 음악 및 융합된 비디오-텍스트 표현에 대한 출력 차원으로 d =를 선택합니다. = 폐쇄된 방에서 빠른 스트러밍 패턴으로 어쿠스틱 기타를 연주하는 여성 보컬리스트의 희미하고 단순한 어쿠스틱 곡으로, 라이브로 녹음되었습니다. 따라 부르기에 좋습니다. 악보. 남성 어그레시브 패드와 벨과 같은 시브 랩이 있는 엘비스 신스 패드의 커버 버전이 특징인 어두운 악기 트랙이 있는 힙합 트랙. 영화 다람쥐 목소리인 듯합니다. 보컬과 기타 악기가 특징인 여러 프레슬리 팝송이 있는 펀자브 트랙으로 남성 및 여성 보컬이며 전통적인 피아노 레이어에 좋습니다. 결혼 축하처럼 들립니다. 사랑에 대한 고백. 그림 4. 수집한 YouTube8M-MusicTextClips 데이터 세트의 예시 주석. 각 예제는 주석을 위해 오디오를 추출한 10초 소스 비디오 클립의 프레임을 보여줍니다. 주석 작성자에게는 뮤직 비디오의 오디오만 제공되었으므로 주석은 음악을 설명하지만 해당 비디오는 설명하지 않습니다. 그림의 각 예제에는 10초 대상 클립의 시작 부분에 타임스탬프가 있는 해당 YT8M 소스 비디오에 대한 하이퍼링크가 포함되어 있습니다. 비디오 프레임 이미지 위에 마우스를 올려놓고 클릭하여 링크를 따라가세요. 4. 실험 이 섹션에서는 실험 설정과 결과를 보고합니다. 먼저, 4.1절에서 데이터 세트와 평가 프로토콜을 설명합니다. 다음으로, 4.2절에서 최첨단 비디오-음악 검색 방법과 비교하여 태그 기반 비디오-음악 검색을 조사합니다. 4.3절에서는 자유형 텍스트 주석에 따라 비디오-음악 검색의 성능을 평가합니다. 마지막으로, 4.4절에서 텍스트 드롭아웃의 영향을 측정하기 위해 절제 연구를 수행합니다. 4.1. 데이터 세트 및 평가 프로토콜 YT8M-뮤직 비디오. 모든 실험에서 우리는 훨씬 더 큰 YouTube8M 데이터 세트[1]에서 &quot;뮤직 비디오&quot; 태그가 있는 약 100,000개의 비디오를 포함하는 YT8M-뮤직 비디오 데이터 세트를 사용하여 모델을 훈련합니다. 우리는 3.1절에서 설명한 접근 방식을 사용하여 전체 데이터 세트에 대한 태그와 각 비디오의 음악 트랙을 설명하는 자연어 텍스트를 합성합니다. 또한 YT8MMusic Video의 테스트 분할을 사용하여 4.2절에서 태그 기반 검색을 평가합니다. YT8M-MusicTextClips. 전체 YT8MMusic Video 데이터 세트 외에도 YT8M-뮤직 비디오의 4,000개 샘플 하위 세트 클립에 각 비디오와 함께 제공되는 음악 트랙에 대한 인간이 제공한 텍스트 설명으로 주석을 달았습니다. 이러한 주석을 만들기 위해 각 뮤직 비디오 중간에서 10초 오디오 클립을 샘플링하고 인간 주석자에게 오디오 클립을 듣고 난 후 들리는 음악을 설명하도록 요청합니다. 따라서 주석은 YT8M 샘플이며 주석 작성자는 해당 비디오를 보지 못합니다. 예시 주석은 그림 4에 해당 YouTube 비디오의 10초 클립의 시작 타임스탬프에 대한 링크와 함께 표시됩니다. 이 주석이 달린 세트는 주로 평가를 위한 것입니다. 결과적으로 주석은 YT8M-Music Video의 테스트 세트에서 3,000개 샘플의 더 큰 세트와 YT8M-Music Video의 훈련 세트에서 1,000개 샘플의 더 작은 세트로 분할되어 few-shot prompt2text 합성 프로세스에서 예로 사용됩니다. 우리는 주석이 달린 텍스트 설명을 동반 웹사이트²에서 공개적으로 제공합니다. 평가 설정 및 메트릭. 우리는 이전 연구[31, 36]와 일관되게 음악 검색 성능을 평가합니다. 그러나 우리의 경우 쿼리는 비디오만이거나 비디오와 해당 텍스트 주석이 함께 있을 수 있습니다. 각 질의에 대해 질의와 N개의 음악 트랙 풀 간의 피처 유사도를 계산합니다(트랙 수준 설정에서 N=2000, 클립 평가의 경우 N=500으로 설정). 풀에는 입력 질의에 해당하는 단일 기준 진실 음악 트랙(긍정적 예)이 포함되어 있고 풀의 나머지 음악 트랙은 일치하지 않습니다(즉, 부정적 예). 질의 풀의 음악 트랙을 피처 유사도에 따라 순위를 매기고 질의의 기준 진실 일치 음악 트랙(긍정적 예)의 순위를 찾습니다. 그런 다음 K=1,5,10에 대한 Recall@K(R@K로 줄임)와 Median Rank를 계산하여 전체 테스트 질의 세트에서 이러한 각 메트릭의 평균을 계산합니다. 4.2. 태그 기반 검색 첫 번째 실험 세트에서는 태그 기반 검색 설정을 살펴봅니다. 여기서 목표는 &quot;행복&quot;, &quot;피아노&quot;, &quot;재즈&quot;와 같은 사전 정의된 어휘의 태그 집합과 함께 쿼리 비디오가 주어진 음악 트랙을 검색하는 것입니다. 이 설정은 태그 기반 검색과 같은 일부 애플리케이션에서 실제로 흥미로울 수 있습니다. 이 설정을 해결하기 위해 태그 접근 방식으로 합성된 텍스트에서 모델을 학습합니다. 이 실험에서 트랙 수준 모델을 학습하고 이전 작업[31,36]과 일관된 방식으로 트랙 수준에서 검색을 수행합니다. 이전 작업과 결과를 직접 비교하기 위해 약 10K 샘플로 구성된 전체 YT8M 테스트 세트에서 검색을 수행합니다. 표 1에서 볼 수 있듯이 Pretét 등이 제안한 모델[31], MVPt 모델[36], MVPt+라고 하는 개선된 버전의 MVPt의 세 가지 기준선을 포함합니다. 여기서 온도 2https://www.danielbmckee.com/language-guidedmusic-for-video/index.html 방법 학습 텍스트 쿼리 텍스트 입력 중간 순위↓↓ R@1↑ R@5 ↑ R@ 10 ↑ a. Pretét et al. [31]0.3.5.b. MVPt [36]6.24.41.c. MVPt+ [36]27.50.60.d. ViML(저희) e. ViML(저희) 태그 태그29.62.75.태그49.81.89.f. 확률0.0.0.표 1. 전체 YouTube8M-Music Video 테스트 세트에서 태그 기반 음악 검색. 태그 쿼리 없이 비디오에서 음악으로 검색하는 이전 방법과 ViML을 비교합니다(행 d). 또한 테스트 시간에 (합성) 태그를 사용하여 비디오+텍스트에서 음악으로 검색하는 ViML을 평가합니다(행 e). 이러한 실험의 태그 접근 방식으로 학습 및 평가를 위한 텍스트 설명을 생성합니다. 방법 a. MVPt+ Train Text MR↓ R@1↑ R@5 ↑ R@10 ↑12.29.40.b. ViML tags11.30.42.data2text13.61 33.prompt2text0.14.09 35.1.46.47.2.c. ViML d. ViML Chance 표 2. YT8M-MusicTextClips 테스트 세트에서 자유형 자연어로 음악 검색. 텍스트 입력을 사용하는 모든 방법은 쿼리로 인간 텍스트 주석에 대해 평가됩니다. MVPt+ 모델은 텍스트 입력을 사용하지 않으므로 동일한 3k 비디오 클립 세트에 대해 비디오에서만 음악을 검색하여 평가합니다. MR은 중간 순위입니다. InfoNCE 손실에서 매개변수 7을 0.03으로 줄였습니다. 이 변경으로 성능이 더욱 크게 향상되었습니다. 다음으로 태그 접근 방식에서 생성된 데이터로 학습된 모델(ViML)을 소개합니다. 두 가지 설정에서 ViML 모델을 평가합니다. 첫째, 테스트 시간에 입력 텍스트 없이 평가합니다(대신 빈 텍스트 입력을 사용합니다). 둘째, 테스트 시간에 텍스트 입력으로 평가합니다. 전체 YT8M-Music Video 분할에 대해 트랙 수준의 인간이 제공한 음악 태그 주석이 없으므로 태그 접근 방식을 사용하여 합성적으로 생성된 태그에서 트랙 수준 모델을 평가합니다. 태그 합성 데이터에서 학습한 모델은 도메인 외부의 자유형 텍스트 입력으로 일반화되지 않을 수 있지만 태그 기반 프롬프트는 주요 원하는 속성(예: &quot;여성 보컬리스트, 기타, 행복&quot;)으로 음악 검색을 안내하는 편리한 방법이 될 수 있습니다. 보고하는 태그 기반 검색은 테스트를 위한 태그 기반 텍스트가 교육 데이터를 합성하는 데 사용한 것과 동일한 음악 태거 모델에서 나오기 때문에 이러한 유형의 사용자 태그 안내 검색에 대한 상한으로 사용할 수 있습니다. 합성 태그로 모델을 평가하면 각 리콜 메트릭에서 MVPt+보다 2030포인트의 매우 상당한 성능 증가가 발생합니다. 흥미롭게도, 테스트 시점에 텍스트 없이 평가한 ViML 모델은 MVT+의 비디오-음악 검색 성능과 일치할 뿐만 아니라 특히 Recall@과 Recall @10에서 MVPt+보다 상당히 향상되었습니다. 이 성능 향상은 단순히 퓨전 레이어에 매개변수를 추가한 결과가 아닙니다. 단일 선형 레이어로만 구성된 퓨전 모듈은 유사한 결과를 낳습니다(자세한 내용은 보충 자료 참조). 이 결과는 텍스트 도메인과 공동으로 학습하면 비디오 및 오디오 표현이 향상될 수 있음을 시사합니다. 언어와의 공동 학습은 제공된 태그에 해당하는 의미적으로 의미 있는 차원으로 비디오-오디오 공간을 풀어내는 데 도움이 되고, 일부 관련 없는 객체의 존재/부재에 해당하는 관련 없는 차원을 억제하는 데 도움이 된다고 가정합니다. 4.3. 자유형 자연어 검색 다음 실험에서는 자유형 자연어 입력을 사용한 검색으로 넘어갑니다. 목표는 입력 비디오와 쿼리 자유형 자연어 설명이 주어졌을 때 관련 음악 트랙을 검색하는 것입니다. 이 설정의 경우, 데이터 세트의 각 비디오에 해당하는 음악을 설명하는 자유형 휴먼 텍스트 주석이 포함된 YT8M-MusicTextClips 데이터 세트의 테스트 비디오를 평가합니다. 이 실험에서 우리는 Surís et al. [36]에서 보고한 &quot;세그먼트 수준&quot; 설정과 유사한 프로토콜을 사용하지만, 입력 비디오에는 인간 주석자가 레이블을 지정한 10초 오디오를 둘러싼 30초 클립만 포함됩니다. 반면에 모델은 Surís et al. [36]에서 보고한 이전 세그먼트 수준 설정에서 전체 소스 비디오에 걸친 대규모 컨텍스트에 액세스할 수 있었습니다. 이 설정에서 검색은 컨텍스트가 제한되어 [36]의 세그먼트 수준 설정이나 4.2에서 보고된 트랙 수준 설정보다 훨씬 더 어렵다는 점에 유의합니다. 그러나 이러한 검색은 소셜 미디어와 엔터테인먼트에서 단편 비디오가 증가하고 있다는 점을 감안할 때 특히 흥미롭습니다. 결과는 표 2에 요약되어 있습니다.기준선은 30초 세그먼트에서 훈련된 MVPt+ 모델입니다(전체 비디오에서 MVPt+를 훈련하고 30초 클립에서 테스트하면 성능이 훨씬 더 심각하게 떨어집니다).다음으로 ViML 모델에 대한 입력 쿼리로 비디오 및 자유형 휴먼 텍스트 설명을 사용하여 음악 검색을 보고합니다.표 2에서는 3.1절에 설명된 세 가지 접근 방식 각각에 의해 합성된 텍스트가 있는 YT8M 뮤직 비디오에서 훈련된 세 가지 변형을 보고합니다.첫 번째 태그 synMethod Train Text Dropout Text Inputs Median Rank↓ R@1↑ R@5↑ R@ 10 ↑ a. MVPt+12.20 29.40.b. ViML prompt 2text9.26.37.c. ViML prompt2text human11.45 30.42.d. ViML prompt2text12.27 30.41.e. ViML prompt2text human14.35.47.표 3. 텍스트 드롭아웃을 사용한 훈련이 검색 성능에 미치는 영향. YT8M-MusicTextClips 테스트 세트에서 평가. 논문 기준선(b.)은 비디오만 사용한 MVPt+ 검색(a.)보다 상당한 개선을 제공합니다. 다음으로, 태그 의미를 엄격하게 보존하면서 보다 자연스러운 구문을 생성하는 data2text 접근 방식(c.)을 평가합니다. 이 접근 방식은 ViML 태그 변형(b.)보다 일관된 개선을 제공합니다. 마지막으로, prompt2text 접근 방식(d.)은 신중한 few-shot 프롬프팅을 통해 대규모 언어 모델이 이 작업에서 강력한 주석자임을 보여주는 최상의 성능을 보여줍니다. 정성적 결과. 그림 5에서 YouTube8MMusic TextClips의 예제에 대한 정성적 검색 결과를 제공합니다. 첫 번째 예에서 두 모델 모두 입력 비디오의 스타일과 비트와 잘 일치하는 트랙을 검색합니다. 그러나 ViML만이 입력 텍스트를 사용하여 올바른 음악 스타일과 일치할 수 있습니다. 두 번째 예에서 ViML 결과만이 원하는 음악 장르와 비디오의 분위기와 올바르게 일치합니다.4.4. 텍스트 드롭아웃 분석 표 3에서 텍스트 드롭아웃이 있는 prompt2text 설명과 텍스트 드롭아웃이 없는 prompt2text 설명으로 학습된 ViML 모델의 성능을 비교합니다.이 모델을 두 가지 설정에서 음악 검색에 대해 평가합니다.(i) 비디오만 사용(빈 텍스트 입력, 행 b. 및 d.)하여 쿼리로 사용,(ii) 비디오와 인간 텍스트 설명을 함께 사용하여 쿼리로 사용(행 c. 및 e.).예상대로, 학습 중에 텍스트 드롭아웃을 추가하면(d.) 비디오만 사용한 검색 성능이 향상됩니다(b.).그러나 흥미롭게도 텍스트 드롭아웃은 쿼리에 자연어(e. 대 c.)가 포함된 경우에도 성능을 크게 향상시켜 텍스트 드롭아웃이 멀티모달 설정에서 매우 유용한 정규화 기술임을 시사합니다.텍스트 드롭아웃이 없으면 모델이 학습 텍스트 입력에 과적합되기 시작하면서 학습이 일찍 정체되기 시작한다는 것을 알 수 있습니다. 비디오는 훨씬 더 풍부하고 복잡한 모달리티이므로 훈련 중에 이 모달리티에 더 많은 주의를 기울이면 학습이 향상됩니다. 우리는 드롭아웃 기법이 0.8-0.95 범위의 높은 드롭아웃 비율에서 가장 효과적이라는 것을 발견했으며, 모든 실험에서 0.8의 드롭아웃 비율을 사용합니다. 5.
--- CONCLUSION ---
이 작업에서 우리는 비디오에 대한 언어 가이드 음악 추천을 허용하는 접근 방식을 도입했습니다. 우리는 proVideo+Language Query Retrieved Music MVPt+ ViML 남성 보컬과 박동하는 리듬이 있는 일렉트로닉 댄스 팝 트랙입니다. 나이트클럽에 가장 적합합니다. 힙합 비트와 프리스타일 분위기를 주는 일렉트로닉 음악이 있는 외국어로 된 남성 랩 곡입니다. 남성 보컬리스트, 신스 라인, 쿰비아 비트가 비디오와 일치하는 타악기적 라틴 음악입니다. 리듬감 있는 일렉트로닉 베이스 비트 위에 색소폰으로 연주되는 경쾌한 재즈 멜로디입니다. 남성 보컬이 오르는 신스 레이어와 박수가 비디오와 일치하는 활기찬 일렉트로닉 파티 곡입니다. 처리된 남성 보컬, 쿵쿵거리는 드럼 비트, 신스 패드 효과가 있는 프랑스 랩 곡입니다. 그림 5. YouTube8M-MusicTextClips 테스트 세트의 정성적 결과입니다. 우리는 MVPt+ 모델과 ViML 모델을 사용하여 두 가지 예에 대한 음악 검색 품질을 비교합니다. 왼쪽 열에는 입력 비디오의 프레임과 대상 음악을 설명하는 입력 텍스트 설명이 포함됩니다. MVPt+ 모델은 비디오만 입력으로 사용하는 반면 ViML 모델은 비디오와 해당 텍스트를 모두 사용합니다. 오른쪽의 두 열에는 각각 MVPt+와 ViML에 대해 검색된 음악이 포함되어 있습니다. 웹사이트의 동반 비디오에서 결과를 확인하세요. 텍스트와 비디오 입력을 융합하여 두 도메인과 일치하는 음악을 찾고 텍스트 드롭아웃 기술을 도입하여 학습을 개선하는 ViML 모델을 제시했습니다. 학습을 위한 데이터를 얻기 위해 대규모 언어 모델(BLOOM176B)과 사전 학습된 음악 태거의 출력을 사용하는 자유형 음악 설명 합성 방식을 제안했습니다. 결과에 따르면 대규모 언어 모델은 텍스트 데이터가 제한적이지만 다른 구조화된 데이터를 사용할 수 있는 도메인에서 학습 데이터 합성을 위한 강력한 도구를 제공합니다. 방법을 평가하기 위해 YT8M 비디오의 음악에 대한 고품질 자유형 인간 설명이 포함된 새로운 데이터 세트인 YouTube8MMusic TextClips도 도입했습니다. 이 작업을 기반으로 구축할 수 있는 흥미로운 방향이 많이 있는데, 여기에는 특정 음악 속성이나 언어 가이드 오디오-비디오 생성에 대한 보다 세부적인 제어를 허용하는 것이 포함됩니다. 참고문헌 [1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, Sudheendra Vijayanarasimhan. Youtube-8m: 대규모 비디오 분류 벤치마크. arXiv 사전 인쇄본 arXiv:1609.08675, 2016. 2,[2] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, Boqing Gong. Vatt: 원시 비디오, 오디오 및 텍스트에서 다중 모달 자기 감독 학습을 위한 변환기. 신경 정보 처리 시스템의 발전, 34:24206-24221, 2021.[3] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, Andrew Zisserman. 자기 감독 다중 모드 다용도 네트워크. 신경 정보 처리 시스템의 발전, 33:25-37, 2020.[4] Yusuf Aytar, Carl Vondrick, Antonio Torralba. 보고, 듣고, 읽고: 심층 정렬 표현. arXiv 사전 인쇄본 arXiv:1706.00932, 2017.[5] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, Paul Lamere. 백만 곡 데이터 세트. 제12회 국제 음악 정보 검색 컨퍼런스 회의록, 2011.[6] BigScience. Bigscience 대규모 오픈 사이언스 오픈 액세스 다국어 언어 모델, 2022. &quot;https:// hugingface.co/bigscience/bloom&quot;.[7] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, Xavier Serra. 자동 음악 태그 지정을 위한 mtg-jamendo 데이터 세트. Machine Learning for Music Discovery Workshop, International Conference on Machine Learning(ICML 2019), Long Beach, CA, United States, 2019.[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 언어 모델은 few-shot 학습기입니다. 신경 정보 처리 시스템의 발전, 33:1877-1901, 2020.[9] Jeong Choi, Jongpil Lee, Jiyoung Park, Juhan Nam. 오디오 기반 음악 분류 및 태그를 위한 제로샷 학습. 국제 음악 정보 검색 학회 컨퍼런스, 2019년.[10] Keunwoo Choi, George Fazekas, Mark Sandler. 딥 합성곱 신경망을 사용한 자동 태그 지정. 제12회 국제 음악 정보 검색 학회 컨퍼런스 회의록, 2016년.[11] Bosheng Ding, Linlin Liu, Lidong Bing, Canasai Kruengkrai, Thien Hai Nguyen, Shafiq Joty, Luo Si, Chunyan Miao. DAGA: 저자원 태그 작업을 위한 생성 접근 방식을 사용한 데이터 증강. 2020년 자연어 처리 경험적 방법(EMNLP) 컨퍼런스 회의록, 6045-6057쪽, 온라인, 2020년 11월. 계산 언어학 협회. 3,[12] Andrey Guzhov, Federico Raue, Jörn Hees, Andreas Dengel. Audioclip: 클립을 이미지, 텍스트, 오디오로 확장. ICASSP 2022-2022 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 976-980페이지. IEEE, 2022.[13] Sungeun Hong, Woobin Im, Hyun Seung Yang. Cbvmr: 소프트 인트라 모달 구조 제약을 사용한 콘텐츠 기반 비디오 음악 검색. 2018 ACM 국제 멀티미디어 검색 컨퍼런스 회의록, 2017.[14] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, Daniel PW Ellis. Mulan: 음악 오디오와 자연어의 공동 임베딩. 국제 음악 정보 검색 학회 컨퍼런스, 2022.[15] Wenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky Liang, Peter R. Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, Brian Ichter. 내면 독백: 언어 모델을 통한 계획을 통한 체화된 추론. 로봇 학습 컨퍼런스, 2022.[16] Ahmed Hussen Abdelaziz, Barry-John Theobald, Paul Dixon, Reinhard Knothe, Nicholas Apostoloff, Sachin Kajareker. 성과 중심의 대화 얼굴을 개선하기 위한 모달리티 드롭아웃. 2020년 멀티모달 상호작용 국제 컨퍼런스 회의록, 378386쪽, 2020.[17] Zdeněk Kasner와 Ondřej Dušek. 제로샷 데이터-텍스트 생성을 위한 신경 파이프라인. 60회 연례 회의록(제1권: 장문 논문), 3914-3932쪽, 2022년. 3,[18] Takeshi Kojima, Shixiang(Shane) Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa. 대규모 언어 모델은 제로샷 추론기입니다. 신경 정보 처리 시스템의 발전, 2022년.[19] Edith Law, Kris West, Michael I Mandel, Mert Bay, J Stephen Downie. 게임을 사용한 알고리즘 평가: 음악 태그의 경우. ISMIR, 387-392쪽, 2009년.[20] Jongpil Lee, Nicholas J. Bryan, Justin Salamon, Zeyu Jin, Juhan Nam. 음악 유사성을 위한 얽힘이 풀린 다차원 메트릭 학습. 국제 음향, 음성 및 신호 처리(ICASSP) 컨퍼런스 논문집. IEEE, 2020. 2,[21] 이종필, 박지영, 김근형, 남주한. 원시 파형을 사용한 음악 자동 태그 지정을 위한 샘플 수준 딥 합성곱 신경망. 제14회 사운드 및 음악 컴퓨팅 컨퍼런스 논문집, 2017.[22] 마이크 루이스, 류인한, 나만 고얄, 마르잔 가즈비니네자드, 압델 라흐만 모하메드, 오메르 레비, 베셀린 스토야노프, 루크 제틀모이어. 바트: 자연어 생성, 번역 및 이해를 위한 시퀀스 간 사전 학습의 노이즈 제거. 계산 언어학 협회 연례 회의, 2019.[23] 보첸 리, 아파르나 쿠마르. 비디오로 쿼리: 크로스 모달 음악 검색. ISMIR, 604-611페이지, 2019.[24] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, Lorenzo Torresani. 원격 감독을 통한 절차적 활동 인식 학습. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 13853-13863페이지, 2022.[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. Roberta: 견고하게 최적화된 bert 사전 학습 접근 방식. arXiv 사전 인쇄본 arXiv:1907.11692, 2019.[26] Ilaria Manco, Emmanouil Benetos, Elio Quinton, György Fazekas. 음악을 위한 대조적 오디오-언어 학습. 국제 음악 정보 검색 학회 컨퍼런스, 2022.[27] Natalia Neverova, Christian Wolf, Graham Taylor, Florian Nebout. Moddrop: 적응형 다중 모달 제스처 인식. IEEE 패턴 분석 및 머신 인텔리전스 저널, 38(8):1692-1706, 2015.[28] Aaron van den Oord, Yazhe Li, Oriol Vinyals. 대조적 예측 코딩을 통한 표현 학습. arXiv 사전 인쇄본 arXiv:1807.03748, 2018.[29] Sergio Oramas, Oriol Nieto, Francesco Barbieri, Xavier Serra. 심층적 특징을 사용하여 오디오, 텍스트 및 이미지에서 다중 레이블 음악 장르 분류. 국제 음악 정보 검색 컨퍼런스, 2017.[30] Jordi Pons 및 Xavier Serra. musicnn: 음악 오디오 태깅을 위한 사전 훈련된 합성 신경망. arXiv 사전 인쇄본 arXiv: 1909.06654, 2019.[31] Laure Prétet, Gael Richard 및 Geoffroy Peeters. 크로스모달 뮤직-비디오 추천: 디자인 선택에 대한 연구. 2021년 국제 신경망 공동 컨퍼런스(IJCNN), 1-9페이지. IEEE, 2021. 1, 2, 6,[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 자연어 감독에서 이전 가능한 시각적 모델 학습. International Conference on Machine Learning, 8748-8763페이지. PMLR, 2021. 2,[33] Andrew Rouditchenko, Angie Boggust, David F. Harwath, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Rogério Schmidt Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, James R. Glass. Avlnet: 교육 비디오에서 시청각 언어 표현 학습. Interspeech, 2020.[34] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg. ProgPrompt: 대규모 언어 모델을 사용하여 상황에 맞는 로봇 작업 계획 생성. International Conference on Robotics and Automation(ICRA), 2023.[35] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov. Dropout: 신경망의 과적합을 방지하는 간단한 방법. 머신 러닝 연구 저널, 15(1):1929–1958, 2014.[36] Dídac Surís, Carl Vondrick, Bryan Russell, Justin Salamon. 음악과 비디오에서 예술적 서신을 주고받을 때입니다. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 10564-10574쪽, 2022. 1, 2, 5, 6,[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin. 주의만 있으면 됩니다. 신경 정보 처리 시스템의 발전, 30, 2017. 2,[38] 웨이야오 왕, 두 트란, 맷 페이즐리. 멀티모달 분류 네트워크 훈련을 어렵게 만드는 요인은? IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 12695-12705페이지, 2020.[39] 위페이 왕, 캔 쉬, 칭펭 선, 황 후, 충양 타오, 시우보 갱, 다신 지앙. PromDA: 리소스가 부족한 NLU 작업을 위한 프롬프트 기반 데이터 증강. 제60회 연례 회의록(제1권: 장문 논문), 4242-4255페이지, 아일랜드 더블린, 2022년 5월. 계산 언어학 협회. 3,[40] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le. 미세 조정된 언어 모델은 제로샷 학습자입니다. 2022년 국제 학습 표현 컨퍼런스에서.[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou. 사고의 사슬 촉진은 대규모 언어 모델에서 추론을 이끌어냅니다. S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, A. Oh 편집자, 신경 정보 처리 시스템의 발전, 35권, 248~242~4837페이지. Curran Associates, Inc., 2022.[42] Minz Won, Keunwoo Choi, and Xavier Serra. 반지도 음악 태그 변환기. 국제 음악 정보 검색 학회 논문집, 2021.[43] Minz Won, Andres Ferraro, Dmitry Bogdanov, and Xavier Serra. CNN 기반 자동 음악 태그 모델 평가. 제17회 사운드 및 음악 컴퓨팅 논문집, 2020.[44] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip: 클립에서 강력한 오디오 표현 학습. ICASSP 2022-2022 IEEE 국제 음향, 음성 및 신호 처리 컨퍼런스(ICASSP), 4563-4567페이지. IEEE, 2022.[45] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, Doug Downey. 상식적 추론을 위한 생성적 데이터 증강. EMNLP 2020의 연구 결과: 1008-1025페이지, 온라인, 2020년 11월. EMNLP. 3,[46] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, Yejin Choi. Merlot Reserve: 시각, 언어 및 소리를 통한 신경 스크립트 지식. IEEE/CVF 컴퓨터 비전 및 패턴 인식 컨퍼런스 회의록, 16375-16387페이지, 2022.[47] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. 소크라테스 모델: 언어를 사용한 제로샷 멀티모달 추론 구성. arXiv 사전 인쇄본 arXiv: 2204.00598, 2022.[48] Donghuo Zeng, Yi Yu, Keizo Oyama. 지도 학습 심층 CCA를 통한 크로스 모달 뮤직 비디오 검색을 위한 오디오-비주얼 임베딩. 2018년 IEEE 멀티미디어 국제 심포지엄(ISM), 143-150페이지. IEEE, 2018.A. 퓨전 모듈 아키텍처 연구 부록 인코딩된 시각적 및 텍스트 입력을 결합하는 퓨전 모듈에 대해 다양한 아키텍처를 평가합니다. 표 4에서 다섯 가지 아키텍처 변형을 제시합니다. 먼저, 시각적 및 텍스트 Transformer 인코더 출력(a.)을 직접 추가하여 퓨전을 벤치마킹합니다. 이렇게 하면 퓨전 모듈에서 학습된 매개변수가 완전히 제거됩니다. 다음으로, 연결된 시각적 및 텍스트 피처를 입력으로 전달하는 세 가지 다른 학습된 퓨전 모듈 아키텍처를 평가합니다. (b.) 단일 선형 계층 (c.) 2계층 MLP, (d.) 1계층 Transformer 네트워크, (e.) 2계층 Transformer 네트워크. 퓨전 모듈의 크기가 성능을 크게 변경하지 않는다는 것을 알 수 있습니다. 우리는 주요 결과에서 약간 더 높은 리콜 메트릭 성능을 제공하는 2계층 Transformer 퓨전 아키텍처를 사용하지만, 학습된 매개변수를 포함하지 않는 &quot;추가&quot; 퓨전 모듈을 포함한 다른 퓨전 아키텍처에서도 유사한 성능을 얻을 수 있습니다.B. 비디오 없이 학습하기 우리는 또한 음악과 텍스트로만 학습된 모델을 실험했지만, 이러한 모델이 YT8MMusic TextClips 테스트 세트에서 음악 검색에서 다른 기준선보다 상당히 낮은 성능을 보이는 것을 발견했습니다. 이는 입력 비디오에 데이터 세트의 짧은 인간 텍스트 설명보다 훨씬 더 많은 정보가 포함되어 있기 때문에 놀라운 일이 아닙니다. YT8M-Music TextClips 테스트 세트에서 prompt2text 데이터로 학습하고 인간 텍스트로 평가한 음악+텍스트 모델의 성능은 리콜 @1/5/10=2.52/9.27/15.52 및 MR=56이었습니다(본 논문의 표 2 결과와 비교). 우리는 표 5(a.)에서 태그 입력을 MT로 학습한 트랙 수준 음악+텍스트 모델의 결과를 보고합니다(표 1 주요 논문의 결과). 이 모델은 또한 MVPt+ 또는 ViML보다 상당히 낮은 성능을 보였습니다. C. 앙상블 모델 주요 텍스트의 표 1에 보고된 기준선 외에도 MVPt+와 부록 B의 음악+텍스트 모델을 앙상블로 결합하여 더 강력한 기준선을 형성하는 것도 조사했습니다. 보다 구체적으로, 음악 트랙 m과 해당 비디오, 텍스트 쌍 (v, t)에 대해 총 유사도 점수를 가중 합 (1 − a) · s(y³‚ym)+a·s(zt, zm)으로 계산합니다. 여기서 y&quot;, ym은 MVPt+에서 생성된 비디오 및 음악 임베딩이고, zt, 2m은 음악+텍스트 모델에서 생성된 텍스트 및 음악 임베딩이며, a는 조정한 계수입니다. 표 5(d.)에서 볼 수 있듯이, 이 음악+텍스트 모델과 MVPt+ 앙상블은 강력한 성능을 달성하여 ViML의 Recall@1 성능을 능가하고 유사한 Recall @5/10 ViML 성능을 달성했습니다. 그러나 이러한 앙상블을 사용하여 ViML의 성능을 개선할 수도 있음을 발견했습니다. 특히 ViML과 MVPt+의 유사도 점수의 가중 합으로 음악 검색 점수를 계산하면 표 5(e.)에서 볼 수 있듯이 ViML 성능보다 상당히 개선되었습니다. ViML과 음악+텍스트 모델의 앙상블은 표 5(f.)에서 가장 높은 성능을 보였습니다.D. 비디오 속도에 맞는 음악 정성적 결과에서 음악의 분당 비트(BPM)가 비디오 속도와 맞지 않는 예는 많지 않았습니다.우리는 주어진 음악 장르가 제한된 템포 범위에 있다고 가정합니다.따라서 음악 장르를 효과적으로 맞출 수 있다면 잘 맞는 템포를 무료로 반환할 수 있습니다.세분화된 템포 정렬이 없다는 점에 유의하세요.예를 들어, 묘사된 댄스 동작이 음악과 완벽하게 동기화되지 않을 수 있습니다.가능한 미래 방향 중 하나는 비디오에서 음악과 묘사된 동작 간의 정렬을 개선하는 것입니다.E. 텍스트 합성 예제 출력 그림 6과 7에서 YouTube8M-Music Video 데이터 세트에서 무작위로 선택한 예제에 대한 실제 인간 주석과 함께 텍스트 합성 접근 방식에서 생성된 출력을 제시합니다.텍스트 합성 접근 방식은 태그 정확도와 형식/언어의 다양성 사이에서 서로 다른 상충 관계를 보여줍니다. prompt2text 설정은 가장 자유로운 형태의 텍스트 합성 접근 방식이지만 때때로 트랙에 대한 원래 태그 예측과 일치하지 않는 출력을 생성합니다. 일반적으로 prompt2text 설명은 더 짧은 경향이 있으며 종종 입력 태그의 정보를 생략합니다. prompt2text 접근 방식에서 사용되는 언어 모델은 때때로 완전히 잘못된 정보를 환각할 수도 있습니다(예: 그림 6의 예 3에서 &quot;여성 보컬과 피아노 섹션&quot;). 그러나 prompt2text에서 생성된 출력의 어휘와 구조의 다양성으로 인해 이 접근 방식은 실제 인간 주석과 가장 유사합니다. 방법 a. 추가 # 매개변수 중간 순위↓ R@1↑ R@5 ↑ R@ 10 ↑13.34.46.b. 선형 131K13.33.45.c. MLP 1.6M13.32.44.d. 변환기(1개 레이어) 1.4M13.34.46.e. 변압기(2계층) 2.8M14.09 35.47.표 4. 융합계층 아키텍처 연구.모든 모델은 합성된 prompt2text 데이터에서 학습되었으며, YT8M-Music TextClips 3k 테스트 세트에서 결과를 보고합니다.방법 학습 텍스트 쿼리 텍스트 입력 중간값 순위↓↓ R@1↑ R@5 ↑ R@10↑ a. MT 태그 태그11.30.42.b. MVPt+ [37]27.93 50.60.c. ViML(저희) 태그 태그49.81.89.d. MT &amp; MVPt+ Ens. 태그 태그55.81.88.e. ViML &amp; MVPt+ Ens. 태그 태그59.85.91.f. ViML &amp; MT Ens. 태그 태그63.91.96.g. Chance0.0.0.표 5. 음악+텍스트 모델과 모델 앙상블을 위한 전체 YouTube8M-Music Video 테스트 세트에서 태그 기반 음악 검색. 편리한 비교를 위해 본문에 표 2의 MVPt+ 및 ViML 결과도 보고합니다. 표에서 섹션 B에 설명된 음악+텍스트 모델을 MT로 표시합니다. 1) 2) 3) 4) https://youtu.be/xcZIwXABBKA?t=TAGS: 록, 리드 보컬, 일렉트릭 베이스, 광란적, 강력함, 보컬, 일렉트릭 기타, 화남, 느림, 남성 보컬, 어쿠스틱 드럼세트 DATA2TEXT: 사운드트랙은 강력하고 느리며 광란적인 분위기의 록입니다. 이 노래에서는 어쿠스틱 드럼, 남성 보컬, 보컬, 리드 보컬, 일렉트릭 기타가 연주됩니다. PROMPT2TEXT: 일렉트릭 기타, 일렉트릭 베이스, 드럼, 남성 보컬이 있는 록 곡입니다. 이 트랙은 강력하고 화난 분위기를 가지고 있습니다. HUMAN: 그런지한 남성 보컬, 강렬한 드럼 연주, 기타 리프, 강렬한 에너지를 전달하는 베이스가 있는 얼터너티브 하드 록 곡입니다. https://youtu.be/A_j0jhJnHmA?t=TAGS: 댄스, 신시사이저 키보드, 힙합, 남성 보컬, 슬픔, 일렉트로닉, 일렉트로닉 드럼셋, 보컬, 신스 베이스 DATA2TEXT: 이 음악은 힙합, 일렉트로닉, 댄스이며 보컬, 신시사이저 키보드, 일렉트로닉 드럼셋, 남성 보컬이 있습니다. PROMPT2TEXT: 남성 보컬이 있는 힙합 인스트루멘탈입니다. 랩 음악에 좋습니다. HUMAN: 일렉트로닉 음악과 강렬한 남성 보컬이 있는 반항적인 힙합 트랙입니다. https://youtu.be/ZUyZP9AS01c?t=TAGS: 팝, 힙합, 신스 베이스, 행복, 댄스, 보컬, 일렉트로닉, 슬픔, 남성 보컬, 신서사이저 키보드, 리드 보컬, 일렉트로닉 드럼셋, 몽환적 DATA2TEXT: 이 노래는 일렉트로닉과 힙합으로, 신서사이저 키보드, 일렉트로닉 드럼셋, 남성 보컬, 신스 베이스, 보컬, 리드 보컬이 있습니다. 행복하고 몽환적이며 슬픈 느낌입니다. PROMPT2TEXT: 여성 보컬과 피아노 섹션이 있는 힙합 트랙입니다. 슬픈 장면에 좋습니다. HUMAN: 일렉트로닉 음악과 그루비한 트랩 비트가 있는 지비한 남성 리드 트랙으로, 이 노래는 댄스 휴양지입니다. https://youtu.be/Z1UhPHPjE10?t=TAGS: 보컬, 신시사이저 키보드, 남성 보컬, 신스 베이스, 행복, 리드 보컬, 슬픔, 힙합, 화남, 일렉트로닉 드럼셋 DATA2TEXT: 이 곡은 슬픈 듯 들리지만 일렉트로닉 드럼셋, 남성 보컬, 신시사이저 키보드, 신스 베이스가 들어간 곡입니다. PROMPT2TEXT: 느린 비트, 남성 보컬, 신스 베이스가 들어간 힙합 트랙입니다. 이 트랙은 슬프고 우울한 느낌이 있습니다. HUMAN: 낮은 베이스 효과와 트랩 신스 라인이 들어간 외국의 헤비 랩입니다. 친구들과 함께 그루비하게 재밍합니다. https://youtu.be/kXPxUMv-S9I?t=TAGS: 일렉트릭 기타, 보컬, 느림, 록, 댄스, 리드 보컬, 일렉트릭 베이스, 블루스, 광란, 일렉트로닉, 어쿠스틱 드럼셋, 남성 보컬, 기발함, 화남, 5) 강력함 6) 7) 8) 9) DATA2TEXT: 보컬, 남성 보컬, 어쿠스틱 드럼셋, 리드 보컬, 일렉트릭 베이스의 사운드로 화나고, 강력하고, 광란적인 느낌을 주는 블루스와 일렉트로닉 음악입니다. PROMPT2TEXT: 일렉트릭 기타와 드럼이 있는 블루스 느낌의 록 송입니다. HUMAN: Greenday와 같은 그런지 록 반항 음악입니다. 왜곡된 기타, 베이스, 드럼이 있는 남성 보컬입니다. https://youtu.be/enHWyaXrcfl?t=TAGS: 힙합, 일렉트로닉 드럼셋, 일렉트로닉, 슬픔, 신서사이저 키보드, 남성 보컬, 리드 보컬, 댄스, 신스 베이스, 보컬 DATA2TEXT: 음악은 힙합, 일렉트로닉, 댄스, 슬픔과 같은 느낌이며, 신서사이저 키보드, 일렉트로닉 드럼셋, 남성 보컬, 신스 베이스, 리드 보컬이 포함됩니다. PROMPT2TEXT: 남성 보컬, 일렉트로닉 드럼셋, 신서사이저 키보드, 베이스가 있는 힙합 트랙입니다. HUMAN: 남성 래퍼가 나오는 트랩 곡으로, 매우 날카로운 하이햇 비트와 탐닉적이고 중독성 있는 베이스 라인이 특징입니다. 한국어: https://youtu.be/kE2wuQT4J14?t=TAGS: 일렉트릭 베이스, 블루스, 다이나믹, 어쿠스틱 드럼셋, 해피, 보컬, 남성 보컬, 프랜틱, 일렉트릭 기타, 록 DATA2TEXT: 이것은 어쿠스틱 드럼, 일렉트릭 베이스, 보컬, 남성 보컬, 일렉트릭 기타를 특징으로 하는 블루스와 록 음악으로, 다이나믹하고, 해피하고, 프랜틱한 느낌입니다. PROMPT2TEXT: 남성 보컬이 있는 블루스 록 트랙입니다. 이 노래는 해피하고 강렬한 일렉트릭 기타와 일렉트릭 베이스가 있습니다. HUMAN: 남성 보컬이 신스 레이어, 오버드라이브 일렉트릭 기타와 드럼으로 뒷받침되는 소울풀한 록 트랙입니다. 이 노래는 편안한 음조를 가지고 있습니다. https://youtu.be/2dFMqtk1ieM?t=TAGS: 신디사이저 키보드, 힙합, 보컬, 팝, 일렉트릭 드럼셋, 해피, 몽환적, 댄스, 일렉트로닉, 남성 보컬, 리드 보컬 DATA2TEXT: 이 노래는 일렉트로닉, 댄스, 힙합으로 몽환적으로 들립니다. 악기에는 남성 보컬과 전자 드럼 세트가 포함됩니다. PROMPT2TEXT: 강력한 베이스 라인과 귀에 잘 들리는 멜로디가 있는 몽환적인 팝 트랙입니다. 광고, 광고 및 비디오에 가장 적합합니다. HUMAN: 랩 남성 보컬과 드럼 그루브가 있는 중간 템포의 낮은 사운드 힙합 트랙입니다. https://youtu.be/PKJ7p_8tj1c?t=TAGS: 신스 베이스, 신디사이저 키보드, 전자, 전자 드럼 세트, 보컬, 행복, 여성 보컬, 느림, 강력함, 광란, 남성 보컬, 댄스, 리드 보컬 DATA2TEXT: 음악은 신디사이저 키보드, 전자 드럼 세트, 보컬, 남성 보컬 및 여성 보컬이 연주되어 느리고 행복하며 강력한 느낌을 줍니다. PROMPT2TEXT: 강력한 비트와 많은 신스 사운드 효과가 있는 댄스 트랙입니다. 파티에 이상적입니다. HUMAN: 강력한 베이스와 강렬한 전자 비트가 있는 일렉트로팝 트랙입니다. 디스코에 좋습니다. 그림 6. 접근 방식을 사용한 합성된 텍스트 예. YouTube8M-MusicTextClips 테스트 세트에서 무작위로 예를 선택합니다. 각 예제 비디오에 대한 태그, data2text 및 prompt2text 접근 방식의 출력을 보여줍니다. 또한 각 예제에 대해 수집된 실제 인간 주석도 보여줍니다. 텍스트 합성 접근 방식은 태그 정확도를 유지하고 어휘 및 구문 구조의 다양성을 높이는 것 사이에서 균형을 이룹니다. prompt2text 접근 방식은 가장 높은 다양성과 인간 주석과 가장 유사한 출력을 생성합니다. 그림 7에 추가 예가 나와 있습니다. 10) 11) 12) 13) 14) 15) 16) 17) 18) https://youtu.be/QJIw6v5uuYc?t=TAGS: 보컬, 신스 베이스, 일렉트로닉, 여성 보컬, 광란, 리드 보컬, 팝, 신디사이저 키보드, 댄스, 행복, 일렉트로닉 드럼셋, 남성 보컬 DATA2TEXT: 이것은 행복한 사운드의 댄스 및 일렉트로닉 음악처럼 들립니다. 악기 구성에는 남성 보컬, 리드 보컬, 일렉트로닉 드럼셋, 보컬, 여성 보컬, 신시사이저 키보드, 신스 베이스가 포함됩니다. PROMPT2TEXT: 신스 패드 레이어, 무거운 베이스 드롭 및 비트가 있는 EDM 팝 트랙입니다. 파티 트랙처럼 들립니다. HUMAN: 리듬 기타, 드럼 및 베이스가 있는 멜로디 팝송입니다. 이 트랙은 설정에 행복한 분위기를 더합니다. https://youtu.be/w3RzK9PEуHs?t=TAGS: 일렉트로닉, 신시사이저 베이스, 보컬, 힙합, 남성 보컬, 리드 보컬, 댄스, 행복, 신시사이저 키보드, 일렉트로닉 드럼셋 DATA2TEXT: 이 노래는 행복합니다. 이 노래에서는 일렉트로닉 드럼셋, 리드 보컬, 신시사이저 베이스, 보컬, 남성 보컬 및 신시사이저 키보드가 연주됩니다. PROMPT2TEXT: 일렉트로닉 드럼, 베이스 및 신시사이저가 있는 힙합 음악입니다. HUMAN: 눈에 띄는 신스 라인, 기타 및 드럼이 있는 강렬한 자유형 남성 랩입니다. https://youtu.be/Jkb-MeVp4cY?t=TAGS: 일렉트릭 기타, 몽환, 댄스, 신스 베이스, 힙합, 리드 보컬, 일렉트로닉, 팝, 신디사이저 키보드, 일렉트로닉 드럼셋, 보컬, 행복 DATA2TEXT: 행복한 사운드가 담긴 일렉트로닉, 댄스, 팝 음악입니다. 사운드트랙에는 신스 베이스, 보컬, 신디사이저 키보드, 일렉트릭 기타, 일렉트로닉 드럼셋, 리드 보컬이 있습니다. PROMPT2TEXT: 에너지가 넘치는 매우 경쾌한 트랙입니다. 파티나 나이트클럽에 좋습니다. 신스 베이스와 신스가 많이 있습니다. HUMAN: 어쿠스틱 기타, 풍경, 소프트 록 드럼 비트, 외국어로 노래하는 남성 아티스트가 있는 고전적인 코디네이트 트랙입니다. 매우 향수적이고 우아한 작품입니다. 한국어: https://youtu.be/Jvjgq9LpDZA?t=TAGS: 일렉트로닉 드럼셋, 슬픔, 리드 보컬, 힙합, 행복, 신시사이저 키보드, 보컬, 신스 베이스, 남성 보컬 DATA2TEXT: 남성 보컬, 신시사이저 키보드, 보컬, 리드 보컬, 일렉트로닉 드럼셋을 포함한 힙합 음악과 슬픔이 느껴집니다. PROMPT2TEXT: 남성 보컬, 신시사이저, 드럼, 베이스, 강렬한 드롭이 있는 힙합 곡입니다. HUMAN: 남성 보컬과 전자적으로 제작된 레이어가 있는 다크 랩 곡입니다. https://youtu.be/7SzppacIY1M?t=TAGS: 댄스, 힙합, 신시사이저 키보드, 일렉트로닉 드럼셋, 슬픔, 보컬, 일렉트로닉, 신시사이저 베이스, 리드 보컬 DATA2TEXT: 일렉트로닉 드럼셋과 신시사이저 베이스가 특징인 힙합과 일렉트로닉 음악입니다. 슬픈 느낌입니다. PROMPT2TEXT: 남성 리드, 베이스, 드럼, 신시사이저가 있는 힙합 트랙입니다. 이 노래는 스트레스를 해소해줍니다. HUMAN: Jay-Z 노래처럼 들리는 싱코페이트 비트와 신스 사운드 효과가 있는 힙합 트랙입니다. https://youtu.be/y2GXHr7P3D0?t=TAGS: 신스 베이스, 일렉트로닉, 참신함, 화남, 신디사이저 키보드, 다이내믹, 일렉트로닉 드럼셋, 댄스 DATA2TEXT: 이 노래는 신스 베이스와 일렉트로닉 드럼셋이 있는 노래입니다. 분위기는 역동적이고, 댄스, 참신함, 일렉트로닉입니다. PROMPT2TEXT: 로봇 같은 여성 보컬과 신스 베이스 라인이 있는 댄스 트랙입니다. 이 트랙은 클럽에 완벽합니다. HUMAN: 총을 쏘는 것처럼 들리는 혼란스러운 음악이 있는 데스메탈 트랙입니다. https://youtu.be/CzVcyff_gc4?t=TAGS: 보컬, 댄스, 신스 베이스, 일렉트로닉 드럼셋, 남성 보컬, 일렉트로닉, 해피, 팝, 신서사이저 키보드, 리드 보컬 DATA2TEXT: 이 음악의 장르는 일렉트로닉, 사운드 해피, 일렉트로닉 드럼셋, 신서사이저 키보드, 리드 보컬, 신서베이스, 이 노래에서 연주되는 남성 보컬입니다. PROMPT2TEXT: 중독성 있는 멜로디와 강렬한 비트가 있는 해피하고 경쾌한 댄스 트랙입니다. 보컬은 생소한 언어로 불립니다. HUMAN: 랩 남성 보컬, 드럼 앤 베이스가 있는 라운지 칠 힙합 음악입니다. 한국어: https://youtu.be/u6KD0-jBfx4?t=TAGS: 댄스, 여성 보컬, 리드 보컬, 행복, 신스 베이스, 일렉트로닉 드럼셋, 보컬, 팝, 신서사이저 키보드, 일렉트로닉, 다이내믹 DATA2TEXT: 이 음악은 팝, 일렉트로닉, 댄스 느낌이 나는 다이내믹하고 행복한 곡으로, 신서사이저 키보드, 리드 보컬, 신스 베이스, 여성 보컬이 등장합니다. PROMPT2TEXT: 강렬한 비트, 중독성 있는 멜로디, 넘치는 에너지가 있는 댄스 트랙입니다. HUMAN: 관능적인 여성 보컬, 건반, 기타, 드럼, 베이스가 등장하는 매우 열정적인 러브 발라드 트랙입니다. https://youtu.be/qwCWz4BFNuk?t=TAGS: 휴식, 남성 보컬, 리드 보컬, 몽환적, 향수적, 어쿠스틱 기타, 슬픔, 보컬, 피아노, 어쿠스틱 DATA2TEXT: 이 음악은 어쿠스틱 기타, 리드 보컬, 남성 보컬, 피아노를 포함하여 몽환적이고 향수적 사운드를 냅니다. PROMPT2TEXT: 여성 보컬과 피아노가 있는 어쿠스틱 발라드. HUMAN: 우울한 멜로디가 있는 슬픈 팝송. 열정적인 여성 보컬은 이별과 그리움의 느낌을 더합니다. 그림 7. 우리의 접근 방식을 사용한 합성 텍스트 예. 그림 6에서 계속.
