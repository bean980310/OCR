arXiv:2308.16582v2 [cs.CV] 11 Sep 2023
Any-Size-Diffusion: Toward Efficient Text-Driven
Synthesis for Any-Size HD Images
Qingping Zheng¹,
1,2*
,
Yuanfan Guo²*, Jiankang Deng³,
Jianhua Han², Ying Li¹ *, Songcen Xu², Hang Xu²*
Abstract
¹Northwestern Polytechnical University
2Huawei Noah's Ark Lab
3 Huawei UKRD
Stable diffusion, a generative model used in text-to-image
synthesis, frequently encounters resolution-induced compo-
sition problems when generating images of varying sizes.
This issue primarily stems from the model being trained on
pairs of single-scale images and their corresponding text de-
scriptions. Moreover, direct training on images of unlimited
sizes is unfeasible, as it would require an immense number
of text-image pairs and entail substantial computational ex-
penses. To overcome these challenges, we propose a two-
stage pipeline named Any-Size-Diffusion (ASD), designed
to efficiently generate well-composed images of any size,
while minimizing the need for high-memory GPU resources.
Specifically, the initial stage, dubbed Any Ratio Adaptability
Diffusion (ARAD), leverages a selected set of images with
a restricted range of ratios to optimize the text-conditional
diffusion model, thereby improving its ability to adjust com-
position to accommodate diverse image sizes. To support the
creation of images at any desired size, we further introduce a
technique called Fast Seamless Tiled Diffusion (FSTD) at the
subsequent stage. This method allows for the rapid enlarge-
ment of the ASD output to any high-resolution size, avoid-
ing seaming artifacts or memory overloads. Experimental
results on the LAION-COCO and MM-CelebA-HQ bench-
marks demonstrate that ASD can produce well-structured im-
ages of arbitrary sizes, cutting down the inference time by 2×
compared to the traditional tiled algorithm.
Introduction
In text-to-image synthesis, Stable Diffusion (SD) (Rombach
et al. 2022) has emerged as a significant advancement. Ex-
isting SD models (Ruiz et al. 2023; Meng et al. 2023) trans-
form text aligned with image components into high-quality
images, typically sized at 512 × 512 pixels. Despite these
models having the ability to handle varying sizes, they no-
ticeably struggle with resolution changes, resulting in poor
composition (e.g., improper cropping and unnatural appear-
ance), a problem demonstrated in Figure 1(a). The root of
this issue lies in the models trained mainly on pairs of text
and images of a uniform size, overlooking the complexities
of handling images at multiple resolutions. Consequently,
this leads to observed deficiencies in image composition.
In pursuit of generating well-structured images at arbi-
trary aspect ratios, guided by textual descriptions, the Mul-
* Equal Contribution
*Contact
A cute teddy bear in front of a plain white wall. The teddy bear has a warm,
brown fur that looks soft and fluffy, sitting on the brown wooden tabletop.
(c) Ours ASD
(b) MD2.1
(a) SD2.1
900 x 1024
1024 x 512
1024 x 1024
Figure 1: Resolution-induced Poor Composition. Given
the text, (a) SD2.1 and (b) MD2.1, a MultiDiffusion model,
raise poor composition issues in red boxes when synthesiz-
ing images of varying sizes, as opposed to (c) our ASD.
tiDiffusion methodology (Bar-Tal et al. 2023) leverages a
pretrained text-conditional diffusion (e.g., stable diffusion),
as a reference model and controls image synthesis through
the utilization of several reference diffusion processes. Re-
markably, the entire process is realized without requiring
further training or fine-tuning. While efficient, it does not
completely resolve the limitations associated with handling
the reference model's multi-resolution images. As a result,
the production of images may exhibit suboptimal composi-
tional quality. The underlying reason is also tied to the ref-
erence model's training on images constrained to a single-
scale size, as illustrated in Figure 1(b).
A direct and appealing solution to the problem is to
train the SD model to cope with every possible image size.
Yet, this approach encounters an immediate and signifi-
cant barrier: the infinite diversity of image ratios, which
makes it practically unfeasible. Furthermore, it's challeng-
ing to gather an extensive collection of high-resolution im-
ages and corresponding text pairs. Even with a plethora of
high-quality datasets available, the intrinsic pixel-based na-
ture of SD requires substantial computational resources, par-
ticularly when dealing with high-resolution images of var-
ious sizes. The problem is further aggravated when con-
sidering the use of megapixel images for SD training, as
this involves extensive repeated function equations and gra-
dient computations in the high-dimensional space of RGB
images (Ho, Jain, and Abbeel 2020). Even when a trained
model is ready, the inference step is also time-consuming
and memory-intensive. Through empirical observation, we
have found that attempts to generate 4K HD images using
the SD model trigger out-of-memory errors when executed
on a GPU with a 32GB capacity.
The key insight of this paper is to introduce a pioneering
Any-Size-Diffusion (ASD) model, executed in two stages,
which has the capability to synthesize high-resolution im-
ages of arbitrary sizes from text prompts. In its dual-
phase approach, our ASD not only efficiently handles the
resolution-induced poor composition but also successfully
circumvents out-of-memory challenges. At the outset, we
are faced with the complexity of accommodating all con-
ceivable image sizes, a challenge that might seem in-
tractable. To address this, in the first stage, we introduce
a multi-aspect ratio training strategy that operates within a
well-defined, manageable range of ratios. This strategy is
used to optimize our proposed Any Ratio Adaptability Diffu-
sion (ARAD) model. As a result, it enables the production of
well-composed images that are adaptable to any size within
a specified range, while also ensuring a reduced consump-
tion of GPU memory. In order to yield images that can fit
Iany
size, in the second stage, we propose an additional method
called Fast Seamless Tiled Diffusion (FSTD) to magnify the
image output originating from the preceding ARAD. Con-
trary to the existing tiled diffusion methods (Álvaro Bar-
bero Jiménez 2023), which address the seaming issue but
compromise on the speed of inference, our proposed FSTD
designs an implicit overlap within the tiled sampling dif-
fusion process. This innovation manages to boost inference
speed without the typical seaming problems, achieving the
high-fidelity image magnification. To sum up, the contribu-
tions of this paper are as follows:
• We are the first to develop the Any-Size-Diffusion (ASD)
model, a two-phase pipeline that synthesizes high-
resolution images of any size from text, addressing both
composition and memory challenges.
• We introduce a multi-aspect ratio training strategy, im-
plemented within a defined range of ratios, to optimize
ARAD, allowing it to generate well-composed images
adaptable to any size within a specified range.
• We propose an implicit overlap in FSTD to enlarge im-
ages to arbitrary sizes, effectively mitigating the seaming
problem and simultaneously accelerating the inference
time by 2× compared to the traditional tiled algorithm.
Related Work
Stable Diffusion. Building upon the foundations laid by
the Latent Diffusion Model (LDM) (Rombach et al. 2022),
diffusion models (Ho, Jain, and Abbeel 2020; Song et al.
2021) have achieved substantial success across various
domains, including text-to-image generation (Nichol et al.
2022; Ramesh et al. 2022; Saharia et al. 2022), image-to-
image translation (Dhariwal and Nichol 2021; Nichol and
Dhariwal 2021), and multi-modal generation (Ruan et al.
2023). Owing to their robust ability to capture complex
distributions and create diverse, high-quality samples,
diffusion models excel over other generative methods
(Goodfellow et al. 2014). In the field, Stable Diffusion (SD)
(Rombach et al. 2022) has emerged as a leading model for
generating photo-realistic images from text. While adept at
producing naturalistic images at certain dimensions (e.g.,
512 × 512), it often yields unnatural outputs with sizes
beyond this threshold. This constraint principally originates
from the fact that existing stable diffusion models are
exclusively trained on images of a fixed size, leading to a
deficiency in high-quality composition on other sizes. In
this paper, we introduce our Any-Size-Diffusion (ASD)
model, designed to generate high-fidelity images without
size constraints.
Diffusion-based Image Super-Resolution. The objective
of Image Super-Resolution (SR) is infer a high-resolution
(HR) image from a corresponding low-resolution (LR)
counterpart. The utilization of generative models to mag-
nify images often omits specific assumptions about degra-
dation, leading to challenging situations in real-world ap-
plications. Recently, diffusion-based methods (Sahak et al.
2023; Saharia et al. 2023; Li et al. 2023; Ma et al. 2023) have
shown notable success in real-world SR by exploiting gener-
ative priors within these models. Though effective, these ap-
proaches introduce considerable computational complexity
during training, with a quadratic increase in computational
demands as the latent space size increases. An optimized
method, known as StableSR(Wang et al. 2023), was devel-
oped to enhance performance while reducing GPU mem-
ory consumption. However, this method can become time-
inefficient when processing images divided into numerous
overlapping regions. In the next phase of our ASD pipeline,
we present a fast seamless tiled diffusion technique, aimed
at accelerating inference time in image SR.
Method
To resolve the issue of resolution-induced poor composition
when creating high-fidelity images of various sizes from any
text prompt, we propose a straightforward yet efficient ap-
proach called Any Size Diffusion (ASD). This approach sim-
plifies the process of text-to-image synthesis by breaking it
down into two stages (see Figure 2).
•
Stage-I, termed as Any Ratio Adaptability Diffusion
(ARAD), trains on multiple aspect-ratio images and gen-
erates an image conditioned on a textual description and
noise size, avoiding poor composition issues.
Input-texts
match resize
Stage-I: Any Ratio Adaptability Diffusion
Frozen LORA Trainer
Text-condition
L
ε
+0000 1000
D
T-steps
(a) Multi-aspect Ratio Training
Generated images
GT
A set of ratios
Text-condition
Gandalf from
the lord of rings
Input text & size
0000 0000;
T-steps
Embedding
WxH~N(0,1)
D
(b) Inference
Stage-II: Fast Seamless Tiled Diffusion
copy from step t to t-1
00000000
T-steps
D
Image of varying size
offset
- ε
upscale
Any-Size Image
t -1
t
(c) Implicit-Overlap in Tiled Sampling
Non-overlap tiles
Figure 2: The Any-Size-Diffusion (ASD) pipeline, including: 1) Stage-I, Any Ratio Adaptability Diffusion, translates text into
images, adapting to various aspect ratios, and 2) is responsible for transforming low-resolution images from the Stage-I into
high-resolution versions of any specified size. For procedure (c), the implicit overlap in tiled sampling, only the solid green line
region is sent to the UNetModel for current denoising. At Stage-II, the dashed green arrow represents regions that are directly
copied from the preceding denoised latents, potentially enhancing efficiency and consistency within the overall process.
•
Stage-II, known as Fast Seamless Tiled Diffusion
(FSTD), magnifies the image from Stage-I to a predeter-
mined larger size, ultimately producing a high-resolution
synthesized image, adjustable to any specified size.
Pipeline
As depicted in Figure 2, ARAD is implemented based on the
text-conditioned latent diffusion model (LDM) (Rombach
et al. 2022) for arbitrary ratio image synthesis. During the in-
ference process, ARAD receives a user-defined text prompt
and noise size (e.g., “Gandalf from the lord for the rings”).
Initially, the pre-trained text encoder (Cherti et al. 2023) is
adapted to process this prompt, subsequently generating a
contextual representation referred to as a textual embedding
Te (y). Then, a random noise of the base resolution size, de-
noted as e, is initialized. The noisy input conditioned on
the textual embedding p(e|y) is progressively denoised by
the UNetModel (Cherti et al. 2023). This process is iterated
through T times, leveraging the DDPM algorithm (Song,
Meng, and Ermon 2020) to continuously remove noises and
restore the latent representation z. Ultimately, a decoder D
is employed to convert the denoised latent back into an im-
age IE RH×W×3. Here, H and W represent the height and
width of the image, respectively.
Subsequently, the FSTD model accepts the image gen-
erated in the previous step as input and performs infer-
ence based on the image-conditional diffusion (Wang et al.
2023). In detail, the image is magnified by a specified size.
A pretrained visual encoder ε is employed to map the re-
sulting image I' Є RH'×'×³ into a latent representation
z = E(I'). A normal distribution-based noise ε ~ ·N(0, 1)
is then added to it, yielding the noisy latent variable z'
A(z). The image, conditioned on itself p(z'|2), undergoes
progressive iterations by the UNetModel, utilizing our pro-
posed tiled sampling I Є RH×W×³ for T cycles. Lastly, the
decoder D is employed to project the denoised latent vari-
able into the final output, effectively transforming the latent
space back into the image domain.
Any Ratio Adaptability Diffusion (ARAD)
In this stage, ARAD is proposed to make the model have
the capability of generating an image, adjustable to varying
aspect ratios, resolving the issue of resolution-induced poor
composition. This stage is mainly achieved by our designed
multi-aspect ratio training strategy.
Multi-aspect ratio training. Instead of directly training on
the original image and text pairs, we employ our aspect-
ratio strategy to map the original image into an image with
a specific ratio. To be more precise, we define a set of
ratios {1, 2, ..., rn}, each corresponding to specific sizes
{S1, S2, ..., Sn}, where n represents the number of prede-
fined aspect ratios. For each training image x = RH×W×³,
we calculate the image ratio as r = H/W. This ratior is
t=T
t=T
t-1
(a) w/o overlap
t-1
(b) explicitly overlap
Z
(x, y)
(x+Ax, y + Ay)
t=T
t-1
seam | fast
seamless | slow
-
seamless | fast
(c) Ours implicitly overlap
Figure 3: Comparison of various tiling strategies: (a)
without overlapping, (b) with explicit overlapping, and (c)
with implicit overlapping. Green tiles are explicit overlaps,
and the orange tile is our implicit overlap at step t-1.
then compared with each predefined ratio, selecting the one
m with the smallest distance as the reference ratio. The in-
dex m is determined by
arg min f(m) = {|r1−r|, · · ·,|rm−r|,···,|rn—r|}, (1)
m
where f(m) represents the smallest distance between the
current ratio and the predefined ratio. Therefore, if the image
has a ratio similar to the mth predefined size sm, the original
size of the training image is resized to sm.
Forward ARAD process. During the training process, a
pair consisting of an image and its corresponding text (x, y)
is processed, where x represents an image in the RGB space
RH×W×³, and y denotes the associated text. A fixed visual
encoder, Ɛ, is used to transform the resized image sm into
a spatial latent code z. Meanwhile, the corresponding text is
converted into a textual representation 70 (y) via OpenCLIP
(Cherti et al. 2023). For the total steps T, conditional dis-
tributions of the form p(zt|y), t = 1.T, can be modeled
using a denoising autoencoder ε0 (zt, t, y). Consequently, the
proposed ARAD can be learned using an objective function
LARAD = E(x),ue~N (0,1). [||— € (zt, t, To (y))||}]. (2)
~N(0,1),t
Fast Seamless Tiled Diffusion (FSTD)
In the second stage, we propose FSTD, a training-free ap-
proach built on StableSR (Wang et al. 2023) that ampli-
fies the ARAD-generated image to any preferred size. To
achieve efficient image super-resolution without heavy com-
putational demands during inference, we devise an implicit
overlap technique within the tiled sampling method.
Tiled sampling. For clarity, consider an upscaled image I Є
RH'W'x
‚ partitioned into M petite tiles, symbolized as
{Pi Є Rh×w×³ | 1 ≤ i ≤ M}, where w and h denote the
width and height of each tile. We initially encode each tile
'x3
P; using an encoder function ε, adding the random noise,
to generate a set of noisy latent representations Z = {Zi :
E(Pi) + €i | Ei ~ · N(0, 1), 1 ≤ i ≤ M}. Subsequently,
each noisy tile is processed by the UNetModel conditioned
on the original tile for T steps, resulting in a set of denoised
latents Z' {Z'i ei N(0, 1), 1 ≤ i ≤ M}. Finally,
a decoder fD is applied to convert them back into image
space, culminating in the reconstructed image
=
~
I' = {P} = Rh×w×³ | P{ = ƒd(Z;), 1 ≤i≤M}. (3)
Herein, P represents the ith tile decoded from its corre-
sponding denoised latent tile.
However, a seaming problem emerges when any two tiles
in the set are disjoint, as depicted in Figure 3(a). To tackle
this, we implement overlaps between neighboring tiles that
share common pixels (Figure 3(b)). While increasing ex-
plicit overlap can effectively mitigate this issue, it substan-
tially escalates the denoising time. As a consequence, the in-
ference time quadratically increases with the growth in over-
lapping patches. Indeed, it's practically significant to strike
a balance between inference time and the amount of overlap.
Implicit overlap in tiled sampling. To speed up the in-
ference time while avoiding the seaming problem, we pro-
pose an implicit overlap in tiled sampling. As depicted in
Figure 3(c), the magnified image is divided into L non-
overlapping tiles and we keep the quantity of disjoint noisy
latent variables constant during the reverse sampling pro-
cess. Prior to each sampling step, we apply a random offset
to each tile, effectively splitting Z into two components: Zs
(the shifted region with tiling) and Zº (the constant region
without tiling). This can be mathematically represented as
Z = ZSUZc. Take note that at the initial time step, Zº = 0.
At each sampling, the shifted part, Z³, is a collection of L
disjoint tiles, denoted as Zs {Z | 1 ≤ i ≤ L}. Here,
each Z symbolizes a shifted tile. The shifted portion, Z³,
comprises L disjoint tiles that change dynamically through-
out the sampling process. Within this segment, each tile is
expressed as Zx,y Zyi+Ayi,xi+Axi for 1 ≤ i ≤ L.
Here, Ax and y denote the random offsets for tile Z
implemented in the preceding step. As for the constant sec-
tion without tiling, denoted as ZC, the pixel value is sourced
from the corresponding latent variable in the previous sam-
pling step. It is worth noting that after each time step, Zc
is non-empty, symbolically represented as Z° ± Ø. This
approach ensures implicit overlap during tiled sampling, ef-
fectively solving the seaming issue.
=
Experiments
Experimental Settings
Datasets. The ARAD of our ASD is trained on a sub-
set of LAION-Aesthetic (Schuhmann 2022) with 90k text-
image pairs in different aspect ratios. It is evaluated on MA-
LAION-COCO with 21,000 images across 21 ratios (se-
lecting from LAION-COCO (Schuhmann et al. 2022)), and
MA-COCO built from MS-COCO (Lin et al. 2014) con-
taining 2,100 images for those ratios. A test split of MM-
CelebA-HQ (Xia et al. 2021), consisting of 2,824 face im-
age pairs in both low and high resolutions, is employed to
evaluate our FSTD and whole pipeline.
panoramic dream castle under the blue sky at the Disney.
[1024 x 2048]
(a) SR-Plus
(b) SR-Tile (c) SR-Tile-Plus
(d) AR-Plus
(e) AR-Tile
(f) ASD (Ours)
A white, grand and elegant victorian mansion with intricate details in a sunny day. [4096 x 1024]
(a) SR-Plus
(b) SR-Tile
(d) AR-Plus
(e) AR-Tile
(c) SR-Tile-Plus
(f) ASD (Ours)
Figure 4: Qualitative comparison of our proposed ASD method with other baselines, including (a) SR-Plus, (b) SR-Tile,
(c) SR-Tile-Plus, (d) AR-Plus, (e) AR-Tile and (f) our proposed ASD. The yellow box indicates the resolution-induced poor
composition. The orange box indicates better composition. The red solid line box is the zoom-in of the red dashed line box,
aiming to inspect if there are any seaming issues. Our ASD outperforms others in both composition quality and inference time.
Table 1: Quantitative evaluation against baselines. (a) SR-Plus, (b) SR-Tile, (c) SR-Tile-Plus, (d) AR-Plus, (e) AR-Tile
and (f) our ASD. 'S' and 'A' denote single and arbitrary ratios, respectively. All tests run on a 32G GPU. Notably, under the
same GPU memory, our ASD achieves at least 9× higher resolution than the original SD model.
Stage-I
Stage-II
Capability
Exp.
Ratio
Tile
Overlap
Composition
Max Resolution Seam
FID↓
MM-CelebA-HQ
IS ↑
CLIP↑
(a)
S
Poor
20482
N
118.83
2.11
27.22
S
Х
Poor
184322
Y
111.96 (- 6.87)
2.46 (+0.35)
27.46 (+ 0.24)
(c)
S
Poor
184322
N
111.06 (-7.77)
2.53 (+0.42)
27.55 (+ 0.33)
(d)
A
Excellent
20482
N
92.80 (- 26.03)
3.97 (+ 1.86)
29.15 (+ 1.93)
(e)
A
Excellent
184322
Y
85.66 (-33.17)
3.98 (+ 1.87)
29.17 (+ 1.95)
(f)
A
Excellent
184322
N
85.34 (33.49)
4.04 (+ 1.93)
29.23 (+ 2.01)
Implementation Details. Our proposed method is imple-
mented in PyTorch (Paszke et al. 2019). A multi-aspect ra-
tio training method is leveraged to finetune ARAD (using
LORA (Hu et al. 2021)) for 10,000 steps with a batch size of
8. We use Adam (Kingma and Ba 2014) as an optimizer and
the learning rate is set to 1.0e-4. Our FSTD (the second stage
model) is training-free and is built upon StableSR (Wang
et al. 2023). During inference, DDIM sampler (Song, Meng,
and Ermon 2020) of 50 steps is adopted in ARAD to gen-
erate the image according to the user-defined aspect ratio.
In the second stage, we follow StableSR to use 200 steps
DDPM sampler (Ho, Jain, and Abbeel 2020) for FSTD.
Evaluation metrics. For benchmarks, we employ common
perceptual metrics to assess the generative text-to-image
models, including FID (Heusel et al. 2017), IS (Salimans
et al. 2016) and CLIP (Radford et al. 2021). IS correlates
with human judgment, important to evaluate the metric on
a large enough number of samples. FID captures the distur-
bance level very well and is more consistent with the noise
level than the IS. CLIP score is used to measure the cosine
similarity between the text prompt and the image embed-
dings. Besides, the extra metrics (e.g., PSNR, SSIM (Wang
et al. 2004) and LPIPS (Zhang et al. 2018)) are employed
to assess the super-resolution ability of the second stage of
our ASD. PSNR and SSIM scores are evaluated on the lumi-
nance channel in the YCbCr color space. LPIPS quantifies
the perceptual differences between images.
Baseline Comparisons
Based on state-of-the-art diffusion models, we build the fol-
lowing six baselines for comparison.
• SR-Plus: employs SD 2.1 (Rombach et al. 2022) for the
direct synthesis of text-guided images with varying sizes.
⚫ SR-Tile: utilizes SD 2.1 for initial image generation,
magnified using StableSR (Wang et al. 2023) with a non-
overlap in tiled sampling(Álvaro Barbero Jiménez 2023).
⚫ SR-Tile-Plus: A two-stage method that initiates with SD
2.1 (Rombach et al. 2022) and refines the output using
our proposed FSTD, facilitating the synthesis of images
of arbitrary dimensions.
•
AR-Plus: deploys our proposed ARAD model for direct,
text-driven image synthesis across a spectrum of sizes.
• AR-Tile: commences with our ARAD model for ini-
tial image generation, followed by magnification via Sta-
bleSR employing a non-overlap in tiled sampling.
• ASD: is our proposed novel framework, integrating
ARAD in Stage I and FTSD in Stage II, designed to syn-
thesize images with customizable dimensions.
Quantitative evaluation. As reported in Table 1, our pro-
posed ASD method consistently outperforms the baseline
methods. Specifically, our ASD model shows a 33.49 reduc-
tion in FID score compared to (a) SR-Plus, and an increase
of 1.92 and 2.01 in IS and CLIP scores, respectively. On a
32GB GPU, SR-Plus fails to synthesize images exceeding
20482 resolution. In contrast, our ASD effectively mitigates
this constraint, achieving at least 9× higher resolution than
SR-Plus under identical hardware conditions. Additionally,
Table 2: Comparison of our ARAD and other diffusion-
based approaches. We compare their compositional ability
to handle the synthesis of images across 21 different sizes.
MA-COCO
Method
MA-LAION-COCO
FID↓ IS ↑ CLIP ↑
SD2.1
MD2.1
ARAD
14.32 31.25 31.92
14.57 28.95 32.11
13.98 34.03 32.60
FID↓ IS ↑ CLIP ↑
42.50 30.20 31.63
43.25 28.92 30.92
40.28 29.77 31.87
A mannequin in white suit.
576 X 768 512 X 1024
A fine style toy sport sedan, CG art.
1024 X 640
A Pomeranian dog sitting in front of a mini tipi tent.
(a) SD2.1
(b) MD2.1
1024 X 576
(c) ARAD
Figure 5: Comparison of visual results. Composition qual-
ity of the text-to-image synthesis using (a) SD2.1, a stable
diffusion 2.1, (b) MD2.1, a multi-diffusion based on SD 2.1,
and (c) our ARAD. Color boxes indicate poor composition.
we also have the following observations: (i) Utilizing multi-
aspect ratio training results in notable improvements across
various comparisons, specifically reducing FID scores from
118.83 to 92.80 in (a)-(d), 111.96 to 85.66 in (b)-(e), and
111.06 to 85.34 in (c)-(f). (ii) Introducing a tiled algorithm at
the second stage enables the generation of images with un-
limited resolution, while simultaneously enhancing perfor-
mance, e.g., FID scores improve from 92.80 to 85.66 when
comparing (a)-(b) and (d)-(c). (iii) Implementing overlap in
tiled sampling effectively addresses the seaming issue, as ev-
idenced by the comparisons between (b)-(c) and (e)-(f).
Qualitative comparison. As depicted in Fig. 4, the images
synthesized by ASD exhibit superior composition quality
(e.g. proper layout) when compared to other baseline meth-
ods. Additionally, ASD can generate 4K HD images that are
not only well-composed but also free from seaming artifacts.
Specifically, when guided by a text description, the AR-Plus
method is observed to generate a more complete castle than
SR-Plus, as demonstrated in Fig.4(a) vs. Fig.4(d). Compared
with SR-Plus, AR-Tile can produce realistic images but is
hindered by the presence of seaming issues (see Fig. 4(e)).
In contrast, Fig. 4(f) shows that our ASD successfully elim-
inates seaming artifacts and ensures the production of well-
composed images, while minimizing GPU memory usage.
MA-LAION-COCO
Types
3
FID↓
14.36
IS ↑ CLIP ↑
32.53 32.38
5
14.10 33.61
All
13.98 34.03
32.58
32.60
MA-COCO
FID↓ IS ↑ CLIP↑
41.28 29.58 31.71
40.25 29.63 31.80
40.28 29.77 31.87
Table 3: Performance on ARAD trained on the various
types of aspect ratios. “All” denotes the 9 aspect ratios.
Table 4: The versatility of tiled sampling in FSTD.
We conduct ablation on the MM-CelebA-HD benchmark.
"w/o", "explicit”, and “implicit” describe non-overlapping,
explicit, and implicit overlap in tile sampling respectively.
"fixed", and "random” refer to different tile offset strategies.
Here, the overlap of two adjacent tiles is 32×32.
MM-CelebA-HQ
Time
Method
Overlap & Offset
PSNR↑ SSIM↑ LPIPS↓↓ FID↓
per frame
w/o overlap
26.89
0.76
0.09
explicit
27.49
0.76
0.09
22.80
24.15
75.08s
166.8s
implicit & fixed
26.83
0.75
implicit & random
27.53
0.76
0.08 21.37 75.01s
0.08 22.25 75.19s
(a) Zoomed LR (b) w/o overlap (c) Explicit (d) Implicit
Figure 6: The super-resolution results of ×4 for different
methods. We visually compare the (a) Zoomed LR (bicubic
method), tiled diffusion with (b) non-overlap and (c) explicit
overlap tiles; and (d) our FSTD which uses implicit overlap
in tiled sampling. Notably, (d) is 2× faster than (c).
ARAD Analysis
To verify the superiority of our proposed ARAD in address-
ing resolution-induced poor composition issues, we conduct
the ablation study, specifically at the initial stage.
Impact of ARAD. Table 2 highlights the performance of
ARAD, showing improvements of 13.98, 34.03, and 32.60
in FID, IS, and CLIP, respectively, on MA-LAION-COCO
over original SD 2.1 and MultiDiffusion (Bar-Tal et al.
2023) (MD2.1). This superiority is further illustrated in
Fig. 5. While SD2.1 and MD2.1 exhibit composition prob-
lems, our ASD produces images that are consistent with
user-defined textual descriptions. For example, MD2.1 incor-
rectly generates two overlapped blue suits from a prompt for
a white suit, a mistake not present in our ASD's results.
Influence on the number of aspect ratios. Table 3 reveals
the model's performance across various aspect ratios. The
data shows that increasing the number of aspect ratios in
the training dataset improves performance, with FID scores
falling from 14.36 to 13.98. A comparison between 3 and
5 aspect ratios highlights a significant improvement, as the
FID score drops from 14.36 to 14.10. Further increasing the
aspect ratios continues this trend, reducing the FID score
to 13.98. This pattern emphasizes the importance of aspect
ratios in enhancing model performance.
FSTD Analysis
Although we have proved the effectiveness of the proposed
FSTD in Fig. 4 and Table 1, we now explore its design in the
image super-resolution performance on MM-CelebA-HD.
Table 4 report the ablation study on the versatility of tiled
sampling; see more details in the supplementary material.
Importance of tiles with overlap. The first two lines from
Table 4 reveal a comparison between the perceptual per-
formance of explicit overlap and non-overlap in tiled sam-
pling. Specifically, the explicit overlap exhibits superior per-
formance (e.g., 27.49 vs. 26.89 on PSNR). However, non-
overlap tiled sampling offers an approximately 2× faster in-
ference time compared to the explicit overlap. Despite this
advantage in speed, Fig. 6(b) clearly exposes the seaming
problem associated with non-overlap tiled sampling, high-
lighting the trade-off between performance and efficiency.
Implicit vs. explicit overlap. An analysis of the results pre-
sented in Table 4 and Fig.6(c)-(d) confirms that the use of
implicit overlap in tiled sampling yields the best perfor-
mance across both perceptual metrics and visual represen-
tation. Further examination of the last column in Table4
demonstrates that the inference time for implicit overlap in
tiled sampling is nearly equivalent to that of tiling without
overlap. Moreover, the implementation of implicit overlap
successfully reduces the inference time from 166.8s to ap-
proximately 75.0s. This ablation study validates the supe-
riority of our proposed FSTD method, accentuating its ca-
pacity to achieve an optimal balance between performance
quality and inference time.
Effect of various offset strategies. The last two lines of Ta-
ble 4 demonstrate the advantage of using a random offset in
implicit overlap tiled sampling. Specifically, when compar-
ing the fixed and random offset methods in implicit overlap,
the random offset yields a PSNR value of 27.53, outperform-
ing the fixed offset, which registered at 26.83. The results
for other perceptual metrics and visual performance indica-
tors are found to be nearly identical, further emphasizing the
preference for a random offset in this context.
Conclusion
In this study, we address the challenge of resolution-induced
poor composition in creating high-fidelity images from
any text prompt. We propose Any Size Diffusion (ASD), a
method consisting of ARAD and FSTD. Trained with multi-
aspect ratio images, ARAD generates well-composed im-
ages within specific sizes. FSTD, utilizing implicit overlap
in tiled sampling, enlarges previous-stage output to any size,
reducing GPU memory consumption. Our ASD is validated
both quantitatively and qualitatively on real-world scenes,
offering valuable insights for future work.
Appendix
Details of Our ASD Methodology
In this section, we present a comprehensive analysis of our
proposed Any Size Diffusion (ASD) pipeline. We first delin-
eate the multi-aspect ratio training strategy implemented in
Stage-I. Subsequently, we provide a thorough examination
of the implicit overlap characteristics inherent to the tiled
sampling approach adopted in Stage II.
Stage-I: Any Ratio Adaptability Diffusion (ARAD) Al-
gorithm 1 presents a detailed description of our proposed
multi-aspect ratio training strategy. We establish nine pre-
defined key-value pairs, where each key represents a dis-
tinct aspect ratio and each value corresponds to a specific
size. The training images, which vary in size, are processed
according to this multi-aspect ratio strategy, as specified in
lines 4-12 of Algorithm 1.
Algorithm 1: Multi-Aspect Ratio Training Strategy
1 Input: Image Ã € RH×W×³, Ratio-Size Dictionary
Dr→s {r1 : 81, T2 : 82, · · ·, 19 : 89};
r→s=
2 Output: Resized Image for Model Training ;
3 Compute image ratio r =
4 Initialize minValue = ∞ ;
5 Initialize minIndex = 0;
6 for each ratio r¿ in {1, 2,
Compute distance =
H
1
Algorithm 2: Procedures for our proposed FSTD.
Input: an image I Є RH×W×³
Output: an image T' Є RH×W×3
2 ► Step 1: Tiled Sampling Preparation
3 Divided the input image I into a set of M disjoint
tiles: {Phxw×3 | 1≤i≤M};
4 for each tile P¿ in {P1, P2, · ··‚Pм} do
/* add a random noise to latent */
L₁ = E (Pi) + €i ;
5
6
7 end;
8 Hence, we have M disjoint latents
{L1, L2,,LM};
9
10 Step 2: Implicit Overlap Tiled Sampling
11 Suppose that Z³ = {L1, L2,, LM} and Zc
and ZZS UZº, and the total step is T ;
12
13 Initialize Z = {L1, L2,
14 Initialize Z = 0;
15 Initialize ZT = ZU Z;
, r9} do
16
= | r — ri | ;
if distance minValue then
=
minIndex = i
7
8
9
min Value
10
11
i = i + 1 ;
12 end;
distance;
,LM};
17 Implicit Overlaps for Tiles
18 for each time step t in {T - 1,..., 0} do
19
20
21
22
23
24
Set a random offset (Ax, Ay);
for each tile Li in {L1, L2, …, Lм} do
Li‚xi‚yi = Li‚xi+^xi‚Yi+^Yi;
end;
/* Update Zs and Zc */
=
0,
Z$ = {L1,x1,y1, L2,x2,y2,···, LM,xm‚yм} and
the number of the tiles keeps constant ;
13 Retrieve rm, Sm from Dr→s using minIndex ;
14 Resize image I to sm for model training
Stage-II: Fast Seamless Tiled Diffusion (FSTD). Algo-
rithm 2 outlines the step-by-step procedure of our proposed
Fast Seamless Tiled Diffusion (FSTD) technique, designed
to efficiently upscale a low-resolution image to a target res-
olution of H x W. Initially, as delineated from line 1 to
line 7 in Algorithm 2, we partition the input low-resolution
image into M non-overlapping tiles, with each being rep-
resented as a separate latent variable {L1, L2, · · ·‚ĽM}·
These latent variables collectively form a set Z, as described
in line 8. In particular, Z is composed of two distinct com-
ponents, as assumed in line 11: the shifted region Zs and
the constant region Zº. The former is designated for pro-
cessing through tiled denoising, while the latter remains ex-
cluded from the tiling process at the current time step. At
25
Z = Zt+1 \ Z{ ;
26
/* Denoise updated latents */
27
for each tile Li,xi,yi in Zi do
28
3222233
29
30
| Apply UNetModel to denoise (Li,xi,yi ) ;
end;
Z₁ = ZUZ and Z Ø;
31 end;
33 After denoising for T timesteps, Z' = Zo;
34 Consequently, we have Z' and I' = D(Z').
the initial 7th time step, as illustrated in lines 13-15, Zs
is initialized as {L1, L2,,LM}, and Zc is initialized
as an empty set. This is due to the initial offsets of each
tile being set to zero. At the heart of the algorithm, as ex-
hibited from lines 17 to 31, is a novel mechanism termed
'Implicit Overlap in Tiled Sampling'. This mechanism is
conceived to significantly reduce both inference time and
GPU memory consumption compared to the non-tiled sam-
pling method used in stable diffusion processes. For each
time step, the algorithm randomly offsets the positions of
these latent variables while maintaining their quantities in-
variant. This results in an updated shifted region, denoted
as Z¾ € {L1,x1,y1, L2,x2,y2?' , LM,xм,чм}, and a novel
constant region Z₁ = Zt+1 \ Z. Notably, from the second
denoising step onwards, Z becomes a non-empty set, with
each pixel within this constant region retaining the same
value as the corresponding pixel in the preceding denoised
latent variables. After T time steps, the iterative procedure
obtains a new denoised latent set, denoted as Z'. In the final
stage, the algorithm decodes this latent set Z' back into an
image of the user-defined size H×W×3, thereby producing
the final super-resolved output image.
Ꮇ
Implementation Details
This section elaborates on the implementation details asso-
ciated with the multi-aspect ratio training approach, and de-
lineate configurations integral to the tiled sampling strategy.
Multi-aspect ratio training. We establish a set of nine
distinct aspect ratios, each associated with a corresponding
size specification, as enumerated in Table 5. Prior to the
commencement of training, images are systematically re-
sized to a predetermined dimensionality, as prescribed by
Algorithm 1. Subsequently, images exhibiting identical as-
pect ratios are agglomerated into a stochastic batch, which
is then utilized as the input for model training.
Explicit and implicit overlaps in tiled sampling. In the
context of tiled sampling, we explore two distinct strategies:
explicit and implicit overlap configurations. Notably, in both
strategies, the dimensions of the input image and tiles can be
parameterized by the user. For the explicit overlap configura-
tion, we mathematically formulate the relationship between
the number of tiles and their overlaps as follows:
Wimage
Х
Himage
-
Ntiles
], (4)
(Wtile - overlap) (Htile overlap)-
where Win
image and Himage represent the width and height of
the input image, respectively, while Wtile and Htile denote
the corresponding dimensions of each tile. In contrast, the
implicit overlap strategy conventionally configures the size
of each tile to 512x512 with zero overlaps between adjacent
tiles. Moreover, the spatial dimensions of latent variables are
64×64. To introduce a form of overlap, we employ a random
offset strategy, designed to control the translational shifts of
these tiled latent representations, thereby achieving implicit
overlap of tiles. Specifically on the MM-CelebA-HQ (Xia
et al. 2021) benchmark, we utilize high-resolution images
with dimensions of 1024 × 1024 alongside corresponding
low-resolution images sized 256 × 256.
Table 5: Dictionary of nine pre-defined ratio-size pairs.
Each ratio corresponds to a pre-defined image size.
Size
#
Ratio
1
1.0000
512 x 512
0.7500
576 x 768
1.3330
768 x 576
0.5625
576 x 1024
1.7778
1024 x 576
0.6250
640 × 1024
1.6000
1024 × 640
0.5000
512 × 1024
2.0000
1024 × 512
123456789
Table 6: The Effect of different random offset ranges in
FSTD. We conduct ablation on the MM-CelebA-HD bench-
mark by upscaling the low-resolution images of sizes 256 x
256 to 1024x1024. The range of random offset is 16, 32 and
48. The row in gray is the default setting of our method.
MM-CelebA-HQ
Offset
range
PSNR↑
SSIM↑ LPIPS↓
FID↓
Time
per image
16
27.51
0.76
0.09
22.58
75.39s
32
27.53
0.76
48
27.52
0.76
0.08
0.08 22.06
22.25
75.19s
75.21s
More Ablation Study
In this section, we present extensive ablation studies to con-
duct an in-depth analysis of our proposed model. Our analy-
sis is divided into two primary components: the first compo-
nent focuses specifically on our Fast Seamless Tiled Diffu-
sion (FSTD) technique, while the second component encom-
passes a comprehensive evaluation of the entire pipeline,
which we denote as Any Size Diffusion (ASD).
Impact of the random offset range in FSTD. In Table 6,
we present a detailed analysis, revealing that the range of the
random offset has minimal influence on the super-resolution
performance of images within the MM-CelebA-HQ dataset.
Specifically, with offset ranges of 16, 32, and 48, the PSNR
scores exhibit remarkable consistency, recording values of
27.51, 27.53, and 27.52, respectively. Furthermore, the in-
ference times across these distinct offset ranges remain sim-
ilarly uniform. This observation underscores the robustness
of our approach, as it performs consistently well under vary-
ing offset parameters, thereby demonstrating its resilience to
changes in this aspect of the configuration.
Table 7: Performance comparison across various num-
bers of explicit tiles. We conduct ablation on the MM-
CelebA-HD benchmark by upscaling the low-resolution im-
ages in 256 x 256 to 1024x1024. The number of tiles in total
with respect to overlap could be calculated by Eq 4.
Method
Overlap Ntiles
PSNR↑
w/o
162
26.89
0.76
16
212
27.50 (+0.61)
0.76
MM-CelebA-HQ
SSIM↑ LPIPS↓ FID↓↓
0.09 22.80
0.09 23.21
Time
per image
32
322
27.49
0.76
0.09 24.15
48
642
27.64
0.76
0.09
24.25
75.1s
148.7s
166.8s
182.6s (+ 33.9)
SR-Plus
Ours ASD
40
31.692 OOM OOM OOM OOM
32
31.824
30.802 31.154
28.514
27.73
CPU Memory Cost (G)
0
24
16
17.454
10.258
13.44
8 -10.258-
Emma Watson as a powerful
mysterious sorceress, casting
lightning magic, detailed clothing
5122 1024² 20482 4096² 81922 163842² 18432²
Generated Image Resolutions (Pixels)
Figure 7: Comparison of SR-Plus and ours ASD in terms
of GPU memory cost (G) vs. image resolution (pixels).
SR-Plus is the original SD 2.1 model used to generate im-
ages of varying sizes. OOM is the abbreviation of an out-of-
memory error. Experiments are conducted on a 32G GPU.
Influence on the number of tiles or overlap region in ex-
plicit overlap tiled sampling. Table 7 illustrates the trade-
off between perceptual quality and computational efficiency
in relation to tile overlap in image super-resolution. Notably,
as the number of tiles increases, there is a corresponding
improvement in perceptual metrics, although this comes at
the cost of increased computational time. For instance, tiles
with a 16-pixel overlap exhibit superior perceptual metrics
compared to non-overlap tiles, yielding a notable improve-
ment of 0.63 in PSNR score. However, this enhancement
comes with a substantial increase in inference time, which
increases from 75.1s to 148.7s. Further, compared to the re-
sults presented in the second row of the table, a tile overlap
of 48 pixels yields a PSNR score improvement from 27.50 to
27.64, while incurring an additional inference time of 33.9s.
Performance on the generation of images with different
resolutions. Fig. 7 highlights the comparative efficiency
of our proposed Any Size Diffusion (ASD) method relative
to the baseline SR-Plus model, operating under a 32G GPU
memory constraint. Specifically, in the context of generat-
ing images with dimensions of 10242 and 2048², our ASD
algorithm consistently exhibits more efficient GPU memory
usage than the SR-Plus model. For instance, when generat-
ing images of size 1024², ASD consumes 13.44G of mem-
ory compared to SR-Plus's 17.45G; for 20482 images, the
consumption is 27.73G for ASD versus 31.69G for SR-Plus.
Importantly, while the SR-Plus model is constrained to a
maximum resolution of 20482-beyond which it exceeds
available GPU memory—our ASD method is developed to
accommodate image resolutions of up to 18432². This rep-
resents a significant 9× increase over the SR-Plus model's
maximum capacity.
(a) SR-Plus
(b) AR-Plus
Figure 8: Visual comparison: SR-Plus (original SD 2.1) vs.
AR-Plus (fine-tuned SD 2.1 with multi-aspect ratio train-
ing). The yellow box means poor composition.
Achievement of 4K and 8K image-resolutions. Fig. 8
demonstrates the enhanced high-resolution image genera-
tion capabilities of our ASD method by contrasting it with
SR-Plus, the original SD 2.1 model (Rombach et al. 2022).
SR-Plus degrades in composition for resolutions exceed-
ing 512 x 512 pixels. In comparison, AR-Plus, developed
through multi-aspect ratio training of SD 2.1, addresses this
degradation but is bounded to a 2048 × 2048 pixel output un-
der a 32GB GPU constraint. A non-overlapping tiled algo-
rithm improves this limitation but introduces seaming arti-
facts, as shown in Fig. 9(a). Our solution, implementing im-
plicit overlap in tiled sampling, resolves this seaming issue,
with results depicted in Fig. 9(b). Thus, our ASD method
effectively generates high-quality 4K and 8K images.
References
Bar-Tal, O.; Yariv, L.; Lipman, Y.; and Dekel, T. 2023. Mul-
tiDiffusion: Fusing Diffusion Paths for Controlled Image
Generation. In ICML.
Cherti, M.; Beaumont, R.; Wightman, R.; Wortsman, M.;
Ilharco, G.; Gordon, C.; Schuhmann, C.; Schmidt, L.; and
Jitsev, J. 2023. Reproducible Scaling Laws for Contrastive
Language-Image Learning. In CVPR, 2818-2829.
Dhariwal, P.; and Nichol, A. 2021. Diffusion Models Beat
Gans on Image Synthesis. NIPS, 34: 8780–8794.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Emma Watson as a powerful mysterious sorceress, casting lightning magic, detailed clothing
(a) 4096 x 8192
(b) 4096 x 8192
(a) 2048 x 2048
(b) 2048 x 2048
(a) 2048 x 4096
(b) 2048 x 4096
(a) 2048 x 2048
(b) 2048 x 2048
Figure 9: Comparative visualization of tiled diffusion techniques: (a) AR-Tile without overlaps vs. (b) our ASD with implicit
overlaps. Our ASD method enables the generation of 4K and 8K images while effectively avoiding seam issues.
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
2014. Generative Adversarial Nets. In NIPS, volume 27.
Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and
Hochreiter, S. 2017. GANs Trained by a Two Time-Scale
Update Rule Converge to a Local Nash Equilibrium. NIPS,
30.
Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising Diffusion
Probabilistic Models. NIPS, 33: 6840-6851.
Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,
S.; Wang, L.; and Chen, W. 2021. LORA: Low-Rank
Adaptation of Large Language Models. arXiv preprint
arXiv:2106.09685.
Kingma, D. P.; and Ba, J. 2014. Adam: A Method for
Stochastic Optimization. arXiv preprint arXiv:1412.6980.
Li, R.; Zhou, Q.; Guo, S.; Zhang, J.; Guo, J.; Jiang, X.; Shen,
Y.; and Han, Z. 2023. Dissecting Arbitrary-scale Super-
resolution Capability from Pre-trained Diffusion Generative
Models. arXiv preprint arXiv:2306.00714.
Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-
manan, D.; Dollár, P.; and Zitnick, C. L. 2014. Microsoft
COCO: Common Objects in Context. In ECCV, 740–755.
Ma, Y.; Yang, H.; Yang, W.; Fu, J.; and Liu, J. 2023. Solv-
ing Diffusion ODEs with Optimal Boundary Conditions for
Better Image Super-Resolution. arXiv.
Meng, C.; Rombach, R.; Gao, R.; Kingma, D.; Ermon, S.;
Ho, J.; and Salimans, T. 2023. On Distillation of Guided
Diffusion Models. In CVPR, 14297-14306.
Nichol, A. Q.; and Dhariwal, P. 2021. Improved Denoising
Diffusion Probabilistic models. In ICML, 8162–8171.
Nichol, A. Q.; Dhariwal, P.; Ramesh, A.; Shyam, P.;
Mishkin, P.; Mcgrew, B.; Sutskever, I.; and Chen, M. 2022.
GLIDE: Towards Photorealistic Image Generation and Edit-
ing with Text-Guided Diffusion Models. In ICML, 16784-
16804.
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;
Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;
et al. 2019. Pytorch: An imperative style, high-performance
deep learning library. NIPS, 32.
Radford, A.; Wook Kim, J.; Hallacy, C.; Ramesh, A.; Goh,
G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark,
J.; Krueger, G.; and Sutskever, I. 2021. Learning Transfer-
able Visual Models From Natural Language Supervision. In
ICML, 8821-8831.
Ramesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen,
M. 2022. Hierarchical Text-Conditional Image Generation
with CLIP Latents. arXiv preprint arXiv:2204.06125.
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-
mer, B. 2022. High-resolution Image Synthesis with Latent
Diffusion Models. In CVPR, 10684–10695.
Ruan, L.; Ma, Y.; Yang, H.; He, H.; Liu, B.; Fu, J.; Yuan,
N. J.; Jin, Q.; and Guo, B. 2023. Mm-diffusion: Learning
Multi-modal Diffusion Models for Joint Audio and Video
Generation. In CVPR, 10219-10228.
Ruiz, N.; Li, Y.; Jampani, V.; Pritch, Y.; Rubinstein, M.;
and Aberman, K. 2023. DreamBooth: Fine Tuning Text-
to-Image Diffusion Models for Subject-Driven Generation.
In CVPR, 22500-22510.
Sahak, H.; Watson, D.; Saharia, C.; and Fleet, D. 2023.
Denoising Diffusion Probabilistic Models for Robust Im-
age Super-Resolution in the Wild. arXiv preprint
arXiv:2302.07864.
Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton,
E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan,
B.; Salimans, T.; et al. 2022. Photorealistic Text-to-
Image Diffusion Models with Deep Language Understand-
ing. NIPS, 35: 36479-36494.
Saharia, C.; Ho, J.; Chan, W.; Salimans, T.; Fleet, D. J.; and
Norouzi, M. 2023. Image Super-Resolution via Iterative Re-
finement. TPAMI, 45(4): 4713-4726.
Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Rad-
ford, A.; and Chen, X. 2016. Improved Techniques for
Training GANS. NIPS, 29.
Schuhmann, C. 2022. LAION-AESTHETICS. https://laion.
ai/blog/laion-aesthetics/. Accessed: 2022-8-16.
Schuhmann, C.; Köpf, A.; Vencu, R.; Coombes, T.; and
Beaumont, R. 2022. LAION COCO: 600M Synthetic Cap-
tions from LAION2B-EN. https://laion.ai/blog/laion-coco/.
Accessed: 2022-9-15.
Song, J.; Meng, C.; and Ermon, S. 2020. Denoising Diffu-
sion Implicit Models. In ICLR.
Song, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Er-
mon, S.; and Poole, B. 2021. Score-Based Generative Mod-
eling through Stochastic Differential Equations. In ICLR.
Wang, J.; Yue, Z.; Zhou, S.; Chan, K. C.; and Loy, C. C.
2023. Exploiting Diffusion Prior for Real-World Image
Super-Resolution. arXiv preprint arXiv:2305.07015.
Wang, Z.; Bovik, A.; Sheikh, H.; and Simoncelli, E. 2004.
Image Quality Assessment: from Error Visibility to Struc-
tural Similarity. TIP, 13(4): 600–612.
Xia, W.; Yang, Y.; Xue, J.-H.; and Wu, B. 2021. TediGAN:
Text-Guided Diverse Face Image Generation and Manipula-
tion. In CVPR, 2256–2265.
Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,
O. 2018. The Unreasonable Effectiveness of Deep Features
as a Perceptual Metric. In CVPR, 586–595.
Álvaro Barbero Jiménez. 2023. Mixture of Diffusers for
Scene Composition and High Resolution Image Generation.
arXiv preprint arXiv:2302.02412.
