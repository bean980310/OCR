arXiv:2305.07677v2 [cs.SD] 24 May 2023
Masked Audio Text Encoders are Effective Multi-Modal Rescorers
Jinglun Cai, Monica Sunkara, Xilai Li, Anshu Bhatia, Xiao Pan, Sravan Bodapati
AWS AI Labs
{cjinglun, sunkaral, xilaili, anshubha, panxx, sravanb}@amazon.com
Abstract
Masked Language Models (MLMs) have
proven to be effective for second-pass rescor-
ing in Automatic Speech Recognition (ASR)
systems. In this work, we propose Masked
Audio Text Encoder (MATE), a multi-modal
masked language model rescorer which incor-
porates acoustic representations into the input
space of MLM. We adopt contrastive learning
for effectively aligning the modalities by learn-
ing shared representations. We show that us-
ing a multi-modal rescorer is beneficial for do-
main generalization of the ASR system when
target domain data is unavailable. MATE re-
duces word error rate (WER) by 4%-16% on in-
domain, and 3%-7% on out-of-domain datasets,
over the text-only baseline. Additionally, with
very limited amount of training data (0.8 hours)
MATE achieves a WER reduction of 8%-23%
over the first-pass baseline.
1 Introduction
Performance of Automatic Speech Recognition
(ASR) systems has been traditionally improved
during inference time via either editing/refinement
(Leng et al., 2021; Chi et al., 2021; Cai et al.,
2023) or second-pass rescoring (Xia et al., 2017;
Sainath et al., 2019; Hu et al., 2020) using lan-
guage models. In recent studies, Transformer-
based pre-trained Large Language Models (LLMs)
have shown promising results when used as second-
pass rescorers. Previous works (Xu et al., 2022;
Salazar et al., 2020; Udagawa et al., 2022) have
shown that deep bidirectional Transformers (De-
vlin et al., 2019) perform better than their unidirec-
tional counterparts such as GPT-2 (Radford et al.,
2019b).
While LLMs are trained on giant text corpora,
they may not be representative of the specific do-
main of interest, in this case, speech transcriptions.
This may result in limited generalization ability
without domain-specific fine-tuning. Further, ASR
applications warrant robustness to noise and other
distortions, which text-only LLMs are incapable of
handling on their own at rescoring time.
A potential solution to mitigate these limitations
is to incorporate the speech input into LLM rescor-
ers. Recent studies have demonstrated the effec-
tiveness of leveraging audio information during
second-pass rescoring (Sainath et al., 2019; Gandhe
and Rastrow, 2020; Hu et al., 2020, 2022) to im-
prove performance. However, a tight integration of
rescorer, attending to a shared speech encoder used
in the first-pass, relies on ASR architecture, train-
ing mechanism and internal features, limiting the
flexibility of being applied to other ASR systems.
Inspired by recent multi-modal LLM
works (Tsimpoukelli et al., 2021; Gao et al.,
2022; Bapna et al., 2021; Chen et al., 2022), we
propose MATE, a multi-modal MLM rescorer,
which is compatible with encapsulated ASR
systems: our method by design can work with
any first-pass ASR models (Hybrid / CTC /
Transducer). The rescorer is agnostic to ASR
architecture, training mechanism and internal
features, leading to better generalization capability.
To the best of our knowledge, this is the first
work to integrate a pre-trained self-supervised
learning (SSL) speech representation model
(Baevski et al., 2019, 2020; Hsu et al., 2021;
Chen et al., 2021) into the second-pass rescoring.
One key challenge of incorporating acoustic
information into LLMs is to transform the speech
into a form that can be accepted by the language
model. We overcome this by using a cross-modal
adaptation module consisting of Convolutional
Neural Network (CNN) (LeCun et al., 1989)
and adapter network (Houlsby et al., 2019). We
experiment with different auxiliary alignment
losses for audio-text alignment, to effectively learn
shared representations across the two modalities,
and adopt contrastive learning which significantly
improves the model performance. Empirically, we
show that MATE transfers well to new domains
in zero-shot and few-shot settings, outperforming
text-only baselines.
LMLM
L = LMLM + αLCTR
Masked Language Model (BERT)
Concatenation
LCTR
OO
Adapter Net
CNN
Speech Encoder (WavLM)
回归
"It is eighty uh eighty two degrees"
Lexical
Embedding
It is eighty uh [MASK] two degrees
Figure 1: MATE is trained with two losses: (1) The
MLM which takes concatenated cross-modal representa-
tion as input and computes LMLM on masked text tokens.
(2) LCTR to align the audio and text latent representa-
tions.
2 Approach
MATE consists of a pre-trained masked language
model BERT, an self-supervised learning (SSL)
based speech encoder WavLM (Chen et al., 2021)
and a modality matching module (CNN and adapter
network), as illustrated in Figure 1.
2.1 System Architecture
Masked Language Model We use BERT, a pre-
trained bidirectional MLM, as the primary com-
ponent of our rescorer. In this work, we extend
BERT to incorporate speech data along with text.
The pre-trained embedding layers of BERT serve
as the text embedding module, while the interme-
diate encoder layers take both acoustic and lexical
representations as input.
Pre-trained Speech Encoder To extract the
acoustic representation, we use WavLM model, pre-
trained on masked speech prediction and speech
denoising tasks, achieving state-of-the-art perfor-
mance on various speech processing tasks and out-
performing other models like Wav2Vec2(Baevski
et al., 2020) and HuBERT(Hsu et al., 2021) on
SUPERB (Yang et al., 2021) benchmark.
Cross-modal Adaptation To align the acous-
tic and lexical representations in the same feature
space, we design a cross-modal adaptation module.
It is composed of two sub-modules: (i) Convolu-
tional Neural Network (CNN) based subsampling
component, to balance the sequence length be-
tween the modalities, and (ii) A bottleneck adapter
network to project the acoustic representations to
the BERT encoder input space. The outputs from
the adapter network a and lexical embedding 1
are concatenated¹ horizontally al, and passed
through the BERT encoder layers to fuse the infor-
mation from the two modalities.
2.2 Alignment Loss
Pre-trained Masked Language Models are trained
on text corpora (Devlin et al., 2019). To explicitly
align audio and text modalities, we propose intro-
ducing an explicit alignment loss function, thereby
further enhancing the quality of cross-modal learn-
ing.
We adopt a contrastive loss function to enforce
the mapping of acoustic representations a and lex-
ical representations / to a shared feature space.
We conduct average pooling at utterance level,
and denote the pooled vectors by (āį,īj), from
the acoustic or lexical representation a¿ and li re-
spectively. Given acoustic-lexical representations
(āi, i)1≤i≤N where N is the batch size, we use the
paired vectors (āį, į) as positive samples and the
unpaired vectors (āi, īj)i‡¡ in the same mini-batch
as negative samples. The training objective is to
minimize the following contrastive loss LCTR with
Negative Log-Likelihood (NLL) function:
LCTR
N
log:
i=1
exp(sim(ār, Ti))
N
Σ₁₁ exp(sim(ār, j))
j=1
(1)
where sim(,) is a similarity metric, imple-
mented as dot product in our experiments. Con-
trastive loss promotes a higher level of similarity
between paired acoustic and lexical representations,
as compared to unpaired representations, thus en-
hancing the alignment between the two modalities.
2.3 Training and Inference
Training MATE is trained jointly on the MLM
objective MLM, similar to that employed in the pre-
training of BERT, and the contrastive loss LCTR.
L
=
£MLM+a•
LCTR
(2)
Following BERT pre-training, a portion of tokens
in the text sequence are randomly selected for pre-
diction, and are replaced by the [MASK] token, a
¹We have also experimented with a cross-attention based
merging mechanism, which leads to inferior performance.
random token or left unchanged. In order to opti-
mize the model's performance, the model is trained
end-to-end and all the parameters are updated dur-
ing the training process.
=
Inference We use pseudo-log-likelihood (PLL)
scoring (Wang and Cho, 2019; Salazar et al.,
2020) to compute sequence level scores. Given
an acoustic sequence a (S1,, SR) and a
lexical sequence
(t1, ..., tT), let l\k
(t1,..., tk-1, [MASK], tk+1, ..., tT), PLL score is
computed by summing conditional log probabil-
ities log PMLM (lia, l₁₁) of each masked lexical to-
ken:
T
=
=
PLL(1) log PMLM (li|a, l\;) (3)
=
i=1
The final score of an utterance is computed as a
linear interpolation of the first-pass ASR confi-
dence score and second-pass PLL score, leveraging
the complementary information to improve perfor-
mance while allowing a trade-off between them.
3 Experiments
3.1 Datasets
Training Set The training corpora consist of
10K+ hours of paired audio-text data, sampled from
both public and in-house datasets. This data regime
is representative of a variety of ASR systems used
for various speech applications, with a mix of ac-
cents, speakers, sampling rates, and background
noise. Less than 5% of the data are synthetic au-
dios generated using AWS Polly Text-to-Speech
(TTS) 2 neural backend.
Evaluation Set We evaluate the proposed MATE
approach on both synthetic and real datasets from
various domains: MTDialogue (movie-twitter), Lib-
riSpeech (LS) (Panayotov et al., 2015) and Vox-
Populi (Wang et al., 2021) are in-domain sets, as
the training set includes their corresponding train
data splits. Wall Street Journal (WSJ) (Garofolo
et al., 1993), ConvAI (in-house), SLURP (Bas-
tianelli et al., 2020) datasets are out-of-domain
(OOD) datasets for zero-shot evaluation.
MTDialogue (movie-twitter) is based on a pub-
lic lexical dialogue corpus 3 which consists of
movie subtitles and twitter user interactions. The
2https://aws.amazon.com/polly/
https://github.com/Phylliida/Dialogue-Datasets
audios are generated from TTS system. MTDia-
logue dataset is a seen dataset for open-book evalua-
tion; i.e., all its data samples are covered in training
data. An subset of 1.2 hour is sampled for evalu-
ation. LibriSpeech(LS) (Panayotov et al., 2015)
is a read English speech corpus based on LibriVox
audiobooks. We consider the two official evalua-
tion sets: test-clean and test-other, each with 5.0
hours of test audios. VoxPopuli (Wang et al., 2021)
consists of public political speech, sampled from
2009-2020 European Parliament event recordings.
For our evaluation purpose, we utilize a 5-hour
subset of VoxPopuli English data.
We also evaluate MATE on OOD evaluation sets:
ConvAI, WSJ, and SLURP. The Wall Street Jour-
nal (WSJ) (Garofolo et al., 1993) corpus contains
conventional and spontaneous dictation by journal-
ists. The test_eval93 split of 0.4 hour is selected
for our evaluation. ConvAI is based on in-house
user utterances of a task-oriented conversational
AI system. The typical usage scenarios include
booking flights, ordering food and querying health
insurance information, etc. The 2.0 hours of audios
are generated from TTS system. SLURP (Bas-
tianelli et al., 2020) is a public dataset for smart
home virtual assistant development. Top usage sce-
narios include checking calendar, playing music,
and asking about time, etc. We utilized the 10 hr
test set for evaluation.
Ethical Considerations: We have reviewed all
licenses of public datasets, which allow the usage
for research and paper publication. The in-house
dataset ConvAI is internally approved for research
purposes. All datasets are sets are de-identified to
ensure anonymity. We also make sure the datasets
cover various English accents, speakers and back-
grounds.
3.2 Evaluation Metrics
We use word error rate (WER) and content word er-
ror rate (CWER) as the evaluation metrics. CWER
is computed on content words only (e.g., "pizza",
“parliament”, “airline”), where we apply rule based
method to filter out a predefined block-list of func-
tion words. Furthermore, we evaluate Spoken
Language Understanding (SLU) performance on
SLURP dataset using standard SLU metrics (ac-
curacy and F1 score); SLU predictions (scenario,
action and entity) are generated by a bi-directional
Long Short-Term Memory (BiLSTM) NLU mod-
ule (Appendix A).
In-domain
MTDialogue LS test-clean LS test-other Voxpopuli
WSJ
Out-of-domain
ConvAI
SLURP
(1)
No rescoring
(2)
GPT2-text
(3)
BERT-text
(4)
LAS rescoring
(6)
MATE-NA
(7)
MATE-MSE
(8)
MATE (ours)
WER CWER WER CWER WER CWER WER CWER WER CWER WER CWER WER CWER
9.47 14.63 6.75 8.07 11.98 15.61 11.06 10.33 8.16 8.75 5.89 9.00 24.91 29.53
9.32 14.37 6.45 7.78 11.70 15.11
8.40 5.76 8.66 24.91 29.53
9.05 13.88 5.50 7.20
10.70
14.45
9.27 14.15 6.7 7.99 11.97 15.59 11.02 10.21
(5) Multi-modal-GPT2 9.24 14.17
6.35
7.69 11.54 14.93 10.56 9.83
9.05 13.90 5.55
7.29
10.75 14.51 10.34 9.92
7.49 11.41 5.22 6.95 10.31 13.97 10.10 9.62
7.64 11.70 5.16 6.84 10.30 13.81 9.91 9.47
10.72
9.94
7.64
10.33
9.96
6.46
8.20 5.38
8.37 24.48 29.27
8.01 8.55 5.81
7.55 8.20 5.69
6.49 8.10 5.40
6.10 7.65 5.07
6.01 7.46 5.10
8.84 24.91 29.53
8.59 24.89 29.40
8.36 24.46 29.24
7.92 23.84 28.24
7.91 23.77 28.14
(9)
Frozen-ME
9.21 14.22 5.57 7.3.4 Parameter-Efficient Tuning 9.80 6.55 8.15 5.42 8.34 24.39 29.13
9.15 14.02 5.58
(10) WavLM-adapter
7.41 10.81 14.69 10.23 9.86 6.52 8.05 5.47 8.39 24.56 29.27
(11) ME-adapter 9.19 14.12 5.56 7.43 10.79 14.63 10.09 9.60 6.43 8.20 5.42 8.35 24.34 29.08
Table 1: Performance measured by WER↓ and CWER ↓. All models except (2-3) are multi-modal. (2) GPT2-text
(Radford et al., 2019a): Full fine-tuning of GPT2 on training corpora transcriptions. (3) BERT-text (Salazar et al.,
2020; Devlin et al., 2019): Full fine-tuning of BERT on training corpora transcriptions, also denoted as "text-only
baseline". (4) LAS rescoring (Sainath et al., 2019): A multi-modal baseline with LAS head rescoring (attention
based LSTM decoder) accepting acoustic information from WavLM. (5) Multi-modal-GPT2: A multi-modal
uni-directional baseline with GPT2, accepting acoustic information from WavLM. (6) MATE-NA: MATE without
additional alignment loss; (7) MATE-MSE: MATE trained with MSE loss instead of contrastive loss. (9) Frozen-ME
(Masked Encoder): Fine-tune all parameters in multi-modal system except masked encoder (BERT) layers with only
MLM objective. (10) WavLM-adapter: add bottleneck adapter to speech encoder (WavLM) and do adapter-tuning
on WavLM, all other parameters are frozen. (11) ME (Masked Encoder)-adapter: do adapter-tuning on masked
encoder (BERT), all other parameters are frozen.
4 Results and Analysis
We summarize the observations and analysis of the
results from our experiments 4 as follows:
MATE excels at both in-domain and out-of-
domain generalization: Table 1 summarizes the
performance of the proposed MATE and multiple
baseline models, under various settings, across
in-domain and OOD datasets. Overall, we ob-
serve that our proposed approach (row 8) signifi-
cantly outperforms text-only baseline (row 3) on
in-domain datasets indicating that audio informa-
tion helps even when we have sufficient target do-
main corpus for fine-tuning. Furthermore, results
on OOD datasets indicate that MATE generalizes
much better to new domains in the complete ab-
sence of domain data (zero-shot setting), when
compared to the text-only baseline, by utilizing
the rich information from audio.
MLMs are more effective multi-modal rescorers
than autoregressive LMs: Rows 2-5 indicate a
significant performance gap between BERT and
autoregressive rescorers (LAS/GPT-2). BERT-Text,
which is a text-only baseline, outperforms even
the multi-modal GPT2 indicating the root cause of
the gap is the lack of bi-directional (left and right)
context in GPT2 which is necessary for reliable
*Appendix B contains experimental setup details, includ-
ing hyperparameters and infrastructure setting.
and effective LLM scoring, hence validating the
choice of MLM in MATE.
Alignment loss gives significant performance
boost: To study the effect of alignment loss, we
train the multi-modal rescorer with two loss func-
tions: Mean squared error (MSE) loss and con-
trastive loss. Significant performance gains (row 6
vs. row 7-8) in Table 1 indicate that explicit align-
ment techniques greatly improve learning of multi-
modal representations. Specifically, contrastive
loss not only aligns relevant pairs like MSE loss,
but also promotes distancing irrelevant samples,
leading to improved generalization on OOD sets.
Parameter-efficient fine-tuning results in lim-
ited gains: Rows 9-11 study the performance
of a multi-modal rescorer under different parame-
ter efficient fine-tuning settings. We observe that
performance degrades as we move from full fine-
tuning to adapter-tuning and freezing the full BERT
encoder layers, indicating that fine-tuning BERT
encoder is the most beneficial in terms of perfor-
mance improvement. As expected, in comparison
to model with full fine-tuning (row 6), rows 9-11 ex-
hibit lower performance. This suggests that frozen
or parameter-efficient training methods may lack
the model capacity to fully leverage the acoustic
information present in the multi-modal data.
WERR
30.0%
25.0%
25.6%
23.0%
23.0%
23.8% 22.7%
21.9%
21.6%
20.0%
19.7%
20.5%
20.5%
20.5% 20.8%
15.0%
10.0%
8.0%
6.6%
5.0%
0.0%
0.0
0.28
0.83
2.78
8.33
27.78
3.9%
9.2%
6.7%- 6.7%
9.2%
6.9%
--BERT-text, Voxpopuli
-- BERT-text, WSJ
-MATE, Voxpopuli
--MATE, WSJ
Audio
duration (hr)
Figure 2: Relative WER reduction (over first-pass) ver-
sus domain specific training data size.
Scenario Action Entity
78.01 72.53 53.23
72.53 53.23
72.65 53.26
72.45 53.37
73.70 54.26
No rescoring
GPT2-text
78.01
Multi-modal-GPT2
78.07
BERT-text
77.72
MATE
78.76
Table 2: Zero-shot evaluation on SLURP SLU task:
Accuracy for Scenario/Action, and F1 score for Entity.
MATE is the most effective few-shot learner:
To study the effect of few-shot learning, we plot
the relative WER reduction (WERR) on Voxpopuli
and WSJ datasets across different resource condi-
tions as shown in Figure 2. We observe that MATE
transfers well to the new domains in the zero-shot
setting with no training or domain data at all. Few-
shot performance clearly improves with more ex-
amples and goes a reasonable way towards closing
the gap from zero-shot performance to full fine-
tuning performance. We also observe that MATE
consistently has superior performance to text-only
baseline across both datasets, confirming the abil-
ity to rapidly adapt to new domains by leveraging
additional information from the audio modality.
MATE achieves best zero-shot performance im-
provement on downstream SLU tasks: To eval-
uate the effectiveness of the proposed approach
on the end goals in a dialog system, we compare
it with other baselines using metrics such as sce-
nario/action accuracy and entity F1 score in a zero-
shot setting on SLURP dataset. From results in
Table 2, we observe that MATE consistently out-
performs the other baselines on end-to-end goals
indicating that the improvements are mainly on
5The SLURP is a challenging corpus, which mimics the
noisy use cases of smart home assistants. Hence, by improving
rescoring method alone, we achieve less than 2% absolute
improvement in WER and SLU metrics.
recognition of content words and slot entities. 6
5 Conclusions
We propose a novel multi-modal rescorer, MATE,
which achieves significant WER, CWER reduction
on in-domain and OOD datasets. In zero-shot and
few-shot settings, MATE performs well on unseen
domains and adapts rapidly with limited data. The
domain generalization capability of MATE makes
it an effective choice as a second-pass rescorer for
scaling ASR systems to new domains.
6 Limitations
One limitation of our approach is that incorporat-
ing acoustic features from an SSL speech encoder,
in our case WavLM, introduces extra latency over-
head, as we use a standalone ASR model for first-
pass. Therefore, our approach may not be appropri-
ate for certain applications that have exceptionally
low latency constraints.
Another limitation is that while multi-modal
LLMs have the potential to improve ASR perfor-
mance, they can be more complex and harder to
interpret than text-only LLMs. This makes it more
challenging to understand the model's decision
making process or debug any potential errors.
"Qualitative examples are presented in Appendix E.
References
Alexei Baevski, Steffen Schneider, and Michael Auli.
2019. vq-wav2vec: Self-supervised learning of dis-
crete speech representations. ArXiv, abs/1910.05453.
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,
and Michael Auli. 2020. wav2vec 2.0: A framework
for self-supervised learning of speech representations.
In Advances in Neural Information Processing Sys-
tems, volume 33, pages 12449–12460. Curran Asso-
ciates, Inc.
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk,
Philémon Brakel, and Yoshua Bengio. 2016. End-to-
end attention-based large vocabulary speech recog-
nition. In 2016 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
pages 4945-4949.
Ankur Bapna, Yu-an Chung, Nan Wu, Anmol Gulati,
Ye Jia, Jonathan H Clark, Melvin Johnson, Jason
Riesa, Alexis Conneau, and Yu Zhang. 2021. Slam:
A unified encoder for speech and language model-
ing via speech-text joint pre-training. arXiv preprint
arXiv:2110.10329.
Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojan-
ski, and Verena Rieser. 2020. SLURP: A spoken lan-
guage understanding resource package. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
7252-7262, Online. Association for Computational
Linguistics.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems,
volume 33, pages 1877-1901. Curran Associates,
Inc.
Jinglun Cai, Mingda Li, Ziyan Jiang, Eunah Cho, Zheng
Chen, Yang Liu, Xing Fan, and Chenlei Guo. 2023.
Kg-eco: Knowledge graph enhanced entity correc-
tion for query rewriting. In ICASSP 2023 2023
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP).
William Chan, Navdeep Jaitly, Quoc Le, and Oriol
Vinyals. 2016. Listen, attend and spell: A neural
network for large vocabulary conversational speech
recognition. In 2016 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP), pages 4960-4964.
Sanyuan Chen, Chengyi Wang, Zhengyang Chen,
Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki
Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long
Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Micheal
Zeng, and Furu Wei. 2021. Wavlm: Large-scale
self-supervised pre-training for full stack speech pro-
cessing. IEEE Journal of Selected Topics in Signal
Processing, 16:1505-1518.
Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana
Ramabhadran, Pedro J. Moreno, Ankur Bapna, and
Heiga Zen. 2022. MAESTRO: Matched Speech Text
Representations through Modality Matching. In Proc.
Interspeech 2022, pages 4093-4097.
Ethan A. Chi, Julian Salazar, and Katrin Kirchhoff.
2021. Align-refine: Non-autoregressive speech
recognition via iterative realignment. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1920–1927,
Online. Association for Computational Linguistics.
Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,
and Yoshua Bengio. 2014. End-to-end continuous
speech recognition using attention-based recurrent
nn: First results. In NIPS 2014 Workshop on Deep
Learning, December 2014.
Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon
Calders, and Bettina Berendt. 2022. Measuring fair-
ness with biased rulers: A comparative study on bias
metrics for pre-trained language models. In NAACL
2022: the 2022 Conference of the North American
chapter of the Association for Computational Lin-
guistics: human language technologies, pages 1693-
1706.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171-4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Ankur Gandhe and Ariya Rastrow. 2020. Audio-
attention discriminative language model for asr
rescoring. In ICASSP 2020.
Heting Gao, Junrui Ni, Kaizhi Qian, Yang Zhang,
Shiyu Chang, and Mark Hasegawa-Johnson. 2022.
WavPrompt: Towards Few-Shot Spoken Language
Understanding with Frozen Language Models. In
Proc. Interspeech 2022, pages 2738-2742.
John S. Garofolo, David Graff, Doug Paul, and David
Pallett. 1993. CSR-I (WSJ0) Complete LDC93S6A.
Linguistic Data Consortium.
Alex Graves. 2012. Sequence transduction with recur-
rent neural networks. ArXiv, abs/1211.3711.
Alex Graves and Navdeep Jaitly. 2014. Towards end-
to-end speech recognition with recurrent neural net-
works. In Proceedings of the 31st International Con-
ference on Machine Learning, volume 32 of Pro-
ceedings of Machine Learning Research, pages 1764-
1772. PMLR.
Anmol Gulati, James Qin, Chung Cheng Chiu, Niki
Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,
Zhengdong Zhang, Yonghui Wu, and Ruoming Pang.
2020. Conformer: Convolution-augmented trans-
former for speech recognition. In Proceedings of
the Annual Conference of the International Speech
Communication Association, INTERSPEECH.
Demi Guo, Alexander Rush, and Yoon Kim. 2021.
Parameter-efficient transfer learning with diff prun-
ing. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
4884-4896, Online. Association for Computational
Linguistics.
Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catan-
zaro, Greg Diamos, Erich Elsen, Ryan Prenger, San-
jeev Satheesh, Shubho Sengupta, Adam Coates, and
Andrew Y. Ng. 2014. Deep speech: Scaling up end-
to-end speech recognition. CoRR, abs/1412.5567.
Geoffrey Hinton, Li Deng, Dong Yu, George Dahl,
Abdel rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara
Sainath, and Brian Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech recogni-
tion. Signal Processing Magazine.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-
mundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In
ICML, volume 97 of Proceedings of Machine Learn-
ing Research, pages 2790-2799. PMLR.
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,
Kushal Lakhotia, Ruslan Salakhutdinov, and Abdel-
rahman Mohamed. 2021. Hubert: Self-supervised
speech representation learning by masked prediction
of hidden units. IEEE/ACM Transactions on Audio,
Speech, and Language Processing, 29:3451-3460.
Ke Hu, Ruoming Pang, Tara N. Sainath, and Trevor
Strohman. 2021. Transformer based deliberation for
two-pass speech recognition. In 2021 IEEE Spoken
Language Technology Workshop (SLT), pages 68–74.
Ke Hu, Tara N Sainath, Yanzhang He, Rohit Prab-
havalkar, Trevor Strohman, Sepand Mavandadi, and
Weiran Wang. 2022. Improving deliberation by text-
only and semi-supervised training. arXiv preprint
arXiv:2206.14716.
Ke Hu, Tara N Sainath, Ruoming Pang, and Rohit Prab-
havalkar. 2020. Deliberation model based two-pass
end-to-end speech recognition. In ICASSP 2020-
2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 7799-
7803. IEEE.
Diederik P. Kingma and Jimmy Ba. 2014.
A method for stochastic optimization.
abs/1412.6980.
Adam:
CORR,
Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.
Howard, W. Hubbard, and L. D. Jackel. 1989. Back-
propagation applied to handwritten zip code recogni-
tion. Neural Computation, 1(4):541–551.
Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian
Luo, Linquan Liu, Tao Qin, Xiangyang Li, Edward
Lin, and Tie-Yan Liu. 2021. Fastcorrect: Fast error
correction with edit alignment for automatic speech
recognition. In Advances in Neural Information Pro-
cessing Systems, volume 34, pages 21708–21719.
Curran Associates, Inc.
Erik McDermott, Hasim Sak, and Ehsan Variani. 2019.
A density ratio approach to language model fusion
in end-to-end automatic speech recognition. In 2019
IEEE Automatic Speech Recognition and Understand-
ing Workshop (ASRU), pages 434-441.
Yajie Miao, Mohammad Abdelaziz Gowayyed, and Flo-
rian Metze. 2015. Eesen: End-to-end speech recogni-
tion using deep rnn models and wfst-based decoding.
2015 IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU), pages 167–174.
Vassil Panayotov, Guoguo Chen, Daniel Povey, and
Sanjeev Khudanpur. 2015. Librispeech: An ASR
corpus based on public domain audio books. In IEEE
International Conference on Acoustics, Speech and
Signal Processing (ICASSP).
A. Radford, Jeffrey Wu, R. Child, David Luan, Dario
Amodei, and Ilya Sutskever. 2019a. Language mod-
els are unsupervised multitask learners.
Alec Radford and Karthik Narasimhan. 2018. Im-
proving language understanding by generative pre-
training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019b. Lan-
guage models are unsupervised multitask learners.
OpenAI blog, 1(8):9.
Navid Rekabsaz, Simone Kopeinik, and Markus Schedl.
2021. Societal biases in retrieved contents: Mea-
surement framework and adversarial mitigation of
bert rankers. In Proceedings of the 44th Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, SIGIR '21, page
306-316, New York, NY, USA. Association for Com-
puting Machinery.
Tara N. Sainath, Ruoming Pang, David Rybach,
Yanzhang He, Rohit Prabhavalkar, Wei Li, Mirkó
Visontai, Qiao Liang, Trevor Strohman, Yonghui Wu,
Ian McGraw, and Chung-Cheng Chiu. 2019. Two-
Pass End-to-End Speech Recognition. In Proc. Inter-
speech 2019, pages 2773-2777.
Julian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-
trin Kirchhoff. 2020. Masked language model scor-
ing. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pages
2699-2712, Online. Association for Computational
Linguistics.
R. Schwartz and Steve Austin. 1991. A comparison
of several approximate algorithms for finding multi-
ple (n-best) sentence hypotheses. ICASSP 91: 1991
International Conference on Acoustics, Speech, and
Signal Processing, pages 701-704 vol. 1.
Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and
Adam Coates. 2018. Cold Fusion: Training Seq2Seq
Models Together with Language Models. In Proc.
Interspeech 2018, pages 387–391.
Tzu-Wei Sung, Jun-You Liu, Hung-yi Lee, and Lin-
shan Lee. 2019. Towards end-to-end speech-to-text
translation with two-pass decoding. In ICASSP 2019
-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 7175-
7179.
Edmondo Trentin and Marco Gori. 2001. A survey of
hybrid ann/hmm models for automatic speech recog-
nition. Neurocomputing, 37(1):91–126.
Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi,
S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.
Multimodal few-shot learning with frozen language
models. In Advances in Neural Information Pro-
cessing Systems, volume 34, pages 200-212. Curran
Associates, Inc.
Takuma Udagawa, Masayuki Suzuki, Gakuto Kurata,
Nobuyasu Itoh, and George Saon. 2022. Effect and
Analysis of Large-scale Language Model Rescoring
on Competitive ASR Systems. In Proc. Interspeech
2022, pages 3919-3923.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems, volume 30. Curran Associates, Inc.
Alex Wang and Kyunghyun Cho. 2019. BERT has a
mouth, and it must speak: BERT as a Markov ran-
dom field language model. In Proceedings of the
Workshop on Methods for Optimizing and Evaluat-
ing Neural Language Generation, pages 30-36, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Changhan Wang, Morgane Rivière, Ann Lee, Anne Wu,
Chaitanya Talnikar, Daniel Haziza, Mary Williamson,
Juan Pino, and Emmanuel Dupoux. 2021. VoxPop-
uli: A large-scale multilingual speech corpus for rep-
resentation learning, semi-supervised learning and
interpretation. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing.
Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,
Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation
networks: Sequence generation beyond one-pass de-
coding. In Advances in Neural Information Process-
ing Systems, volume 30. Curran Associates, Inc.
Liyan Xu, Yile Gu, Jari Kolehmainen, Haidar Khan,
Ankur Gandhe, Ariya Rastrow, Andreas Stolcke,
and Ivan Bulyko. 2022. Rescorebert: Discrimi
native speech recognition rescoring with bert. In
ICASSP 2022 - 2022 IEEE International Confer-
ence on Acoustics, Speech and Signal Processing
(ICASSP), pages 6117-6121.
Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang,
Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin,
Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting
Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko tik
Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-
Wen Li, Shinji Watanabe, Abdelrahman Mohamed,
and Hung yi Lee. 2021. SUPERB: Speech Process-
ing Universal PERformance Benchmark. In Proc.
Interspeech 2021, pages 1194–1198.
Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi,
Erik McDermott, Stephen Koo, and Shankar Kumar.
2020. Transformer transducer: A streamable speech
recognition model with transformer encoders and
rnn-t loss. ICASSP 2020-2020 IEEE International
Conference on Acoustics, Speech and Signal Process-
ing (ICASSP), pages 7829-7833.
Ding Zhao, Tara N. Sainath, David Rybach, Pat Ron-
don, Deepti Bhatia, Bo Li, and Ruoming Pang. 2019.
Shallow-Fusion End-to-End Contextual Biasing. In
Proc. Interspeech 2019, pages 1418–1422.
Guolin Zheng, Yubei Xiao, Ke Gong, Pan Zhou, Xiao-
dan Liang, and Liang Lin. 2021. Wav-BERT: Coop-
erative acoustic and linguistic representation learning
for low-resource speech recognition. In Findings
of the Association for Computational Linguistics:
EMNLP 2021, pages 2765-2777, Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Appendix
A SLURP SLU semantics and NLU
module
SLURP dataset consists of user interactions with
smart home virtual assistants. The semantics are
annotated with three levels of semantics: Scenario,
Action and Entity. For example, ASR transcript
"how do I make a turkey” is the annotated with
semantics "scenario: cooking | action: recipe | en-
tities: [(type: food | filler: turkey)]". The SLU
semantics spans over 18 different scenarios, 46
defined actions and 55 different entity types (Bas-
tianelli et al., 2020).
In the NLU module, we treat semantics predic-
tion as a sequence-to-sequence problem. Specifi-
cally, given an ASR transcript after rescoring "how
do I make a turkey", the goal is to predict: "sce-
nario: cooking | action: recipe | entities: [(type:
food | filler: turkey)]”. The NLU module has an
encoder-decoder structure based on bi-directional
Long Short-Term Memory (Bi-LSTM). Both the
encoder and the decoder have hidden dimention
256. The encoder has 2 layers while the decoder
has 3 layers. We use Negative Log-Likelihood
(NLL) loss for as training objective for sequence
prediction. We train the model on ground truth
<transcript, NLU semantics> paris from SLURP
training dataset. The learning rate is set to 3e-4 and
the training is conducted for 20 epochs with batch
size 16.
B Experimental Setup
MATE has 217M parameters in total. For both
masked language model and speech encoder, we
utilize base size models for efficiency (BERT-Base
110M and WavLM-Base+ 95M respectively). The
convolutional network contains 3 layers with 768
channels with strides (2,1,2) and kernel widths
(3, 1, 1). The bottleneck adapter layer has compres-
sion factor 0.5.
The training experiment for MATE is conducted
end-to-end: we train all modules simultaneously.
We use Adam optimizer (Kingma and Ba, 2014)
with linear decay of learning rate. We set ini-
tial learning rate to 5e - 5 and batch size to 32.
We searched the hyperparameter a in Eq.2 with
(1.0, 3.0, 10.0), and the final value is set to 1.0.
The training was conducted for 88K steps. All
the experiments are performed with NVIDIA Tesla
V100 GPUs in a single run. The training for MATE
takes 39.7 hours on a Tesla V100 8-GPU machine.
Our first-pass ASR model has a conformer-
CTC (Gulati et al., 2020) architecture. which is
trained on 50K+ hours audio-transcript paired data.
The conformer encoder consists of 20 layers of con-
former blocks with hidden dimension 2048; while
the shallow decoder is a single Transformer-based
layer with the same hidden dimension of 2048. The
conformer-CTC model has approximately 140M
parameters,
We use SCTK package for WER and CWER
evaluation. CWER has the same logic as WER
computation except that we filter out function
words. We use SLURP toolkit for SLU semantics
evaluation.
C Attention Visualization
We visualize the learned self-attention plots ex-
tracted from the proposed MATE model in Figure 3.
The model has 12 Transformer layers and with 12
heads in each multi-head self-attention. We se-
lected 6 representative plot from the 144 total atten-
tion plots with a sample utterance from wsj_eval93
test set. The input utterance has 33 tokens and 77
frames for the acoustic feature, the acoustic fea-
tures are appended to the lexical embedding before
fed into the BERT model. Our observations are
listed as follows:
•
(a) (b) (c) and (d) The plots highlight the
border of the text input and audio input (the
vertical straight line on position 32). We can
conclude that even without feeding any modal-
ity border information to MATE, it can learn
the border of two modalities itself.
(a), (d), (e) and (f) The monotonic audio-to-
text position alignment is clearly shown in the
plots. This indicates that the acoustic and lexi-
cal representations are successfully mapped to
one unified feature space. Interestingly, plots
(a), (e) and (f) show that text-to-audio position
alignment can also be learned by MATE.
D Risks
The proposed system, MATE, incorporates both
pre-trained language model (BERT) and speech
model (WavLM) into its design. Such pre-trained
models can contain biases and stereotypes against
https://github.com/chinshr/sctk
https://github.com/pswietojanski/slurp
(a) Layer 4 head 3
(b) Layer 6 head 6
to the correct utterance “hoover the hallway”.
0
0
20
40
60
80
100
20
20
40
60
80-
100-
0
20
40
60
80
100
0
20
40
60
80
100
(c) Layer 7 head 5
(d) Layer 8 head 2
0
0
40
60
80
100
0
20-
40
60
80
100
20
40
60
80
100
0
20
40
60
80
100
0
20
40
60
80
100
(e) Layer 11 head 1
0
20
40
60
80
(f) Layer 11 head 8
100
0
60
80
100
0
20
40
60
80
100
Figure 3: Selected attention plots from the self-attention
layers of the 12-layer BERT encoder The sample utter-
ance (from wsj_eval93) contains 110 total frames: the
first 33 frames are lexical embedding, followed by 77
acoustic embedding frames. The utterance is: "last year
new hampshire enacted legislation enabling banks from
outside the state to acquire new hampshire banks but
restrictions in the bill discouraged potential buyers"
certain religion, race and gender groups (Rekabsaz
et al., 2021; Delobelle et al., 2022).
E
Qualitative Examples
To further understand why the proposed approach,
MATE yields more accurate prediction, we selected
several representative cases from the evaluation
sets. Table 3 clearly shows that MATE tends to
correct more vocabulary or grammar errors present
in the n-best list. We observe MATE is able to
correct many ASR errors which are not resolvable
by text information alone. In the example from
SLURP, both "who in the hallway” and “hoover
the hallway" are plausible utterances in an informal
style of daily speech. With the aid of acoustic
information, MATE is able to assign higher score
Dataset
SLURP
Ground Truth
Voxpopuli
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
MTDialogue Ground Truth
ConvAI
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Ground Truth
Rescored 1-best by BERT-text
Rescored 1-best by MATE
Utterance
remove tuesday alarm of nine a m
move to alarm of nine a m
remove tuesday alarm at nine a m
hoover the hallway
who in the hallway
hoover the hallway
cancel business meeting on wednesday
council business meeting on wednesday
cancel business meeting on wednesday
can you let delta know i am never using them again
can you let doctor know i am never using them again
can you let delta know i am never using them again
i want to play fifa seventeen
i want to leave for seventeen
i want to play fifa seventeen
what do you know about fringe in edinburgh next year
what do you know about french in edinburgh next year
what do you know about fringe in edinburgh next year
for example the report talks about the rule of law and corruption
for example the report talks about the rule of law on corruption
for example the report talks about the rule of law and corruption
i have met them they are young capable and visionary
i have met them they are young capable and missionary
i have met them they are young capable and visionary
it's muffled
it's muff
it's muffled
how much she got to pay
how much he got to pay
how much she got to pay
why did the noodle box in greensborough fail its health inspection
why did the noodle box in greensboro fail its health inspection
why did the noodle box in greensborough fail its health inspection
tell me about duty free shopping
tell me about duty free shop
tell me about duty free shopping
Table 3: Qualitative examples: We contrast the 1-best outputs of BERT-text model and MATE in reference to ground
truth. We can observe that MATE improves recognition of content words and slot entities.
