arXiv:2306.17492v2 [cs.CL] 27 Feb 2024
Preference Ranking Optimization for Human Alignment
Feifan Song¹, Bowen Yu²*, Minghao Li²
Haiyang Yu², Fei Huang², Yongbin Li², Houfeng Wang¹*
¹National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University
2 Alibaba Group
songff@stu.pku.edu.cn, wanghf@pku.edu.cn
{yubowen.ybw, Imh397008, yifei.yhy, shuide.lyb} @alibaba-inc.com
Abstract
Large language models (LLMs) often contain misleading
content, emphasizing the need to align them with human
values to ensure secure AI systems. Reinforcement learning
from human feedback (RLHF) has been employed to achieve
this alignment. However, it encompasses two main draw-
backs: (1) RLHF exhibits complexity, instability, and sen-
sitivity to hyperparameters in contrast to SFT. (2) Despite
massive trial-and-error, multiple sampling is reduced to pair-
wise contrast, thus lacking contrasts from a macro perspec-
tive. In this paper, we propose Preference Ranking Optimiza-
tion (PRO) as an efficient SFT algorithm to directly fine-tune
LLMs for human alignment. PRO extends the pair-wise con-
trast to accommodate preference rankings of any length. By
iteratively contrasting candidates, PRO instructs the LLM to
prioritize the best response while progressively ranking the
rest responses. In this manner, PRO effectively transforms hu-
man alignment into aligning the probability ranking of n re-
sponses generated by LLM with the preference ranking of hu-
mans towards these responses. Experiments have shown that
PRO outperforms baseline algorithms, achieving comparable
results to ChatGPT and human responses through automatic-
based, reward-based, GPT-4, and human evaluations.
Introduction
Large language models (LLMs) have demonstrated remark-
able capabilities in meeting the diverse information needs of
users (Brown et al. 2020; Chowdhery et al. 2022; Bubeck
et al. 2023; Touvron et al. 2023; Li et al. 2023). De-
spite leveraging the extensive global knowledge and hu-
man behavior encoded within their trillion-token pretrain-
ing corpus, LLMs are unavoidably impacted by the exis-
tence of misleading, toxic, and detrimental content encom-
passed within it (Bai et al. 2022b; Ouyang et al. 2022).
Consequently, reinforcement learning from human feed-
back (RLHF) is introduced to construct secure and manage-
able AI systems (Stiennon et al. 2020; Xue et al. 2023; Peng
et al. 2023) by aligning linguistic space of LLMs to human
values according to a set of candidates ranked by humans.
Nevertheless, RLHF remains more complex than super-
vised learning, prone to optimization instability, and sensi-
tive to hyperparameters. These limitations arise mainly from
*Corresponding authors.
Copyright 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
ONE
y
LLM
PAIR
2
y₁
y²
LLM
PAIRS
ya
LLM
3
y²
PRO
y³
y
LLM
y
Figure 1: Comparison among different SFT paradigms. PRO
is distinguished from others for multi-positional and multi-
dimensional contrasts.
employing the agent, i.e. LLM to experience repetitive trial-
and-error rather than directly aligning it to human prefer-
ence. Hence, Supervised Fine-Tuning (SFT) is expected as
the possibility of more direct optimization to replace RLHF.
SFT initially serves as only a warm-up process for RLHF,
where the best candidates are selected to tune LLMs to in-
timate human-preferred data. Recent works have proposed
more complex strategies to enhance SFT (Rafailov et al.
2023; Wu et al. 2023; Yuan et al. 2023; Dong et al. 2023).
Despite some progress, there remains room for improve-
ment: (1) The essence that powers RLHF to perform well is
ignored. That is multiple sampling with scoring from broad
linguistic space during training. Constrained by the given
ranking length, most methods pay attention to pair-wise con-
trasts from semantic or scalar perspectives. (2) Even longer
rankings are available, they tend to cut it into pairs, thus
lacking distinction of candidates from a macro perspective.
In this work, we thoroughly investigate the effect of en-
larging sampling from linguistic space on human alignment.
Based on this scenario, we propose the Preference Ranking
Optimization (PRO) as an efficient framework of direct pol-
icy optimization. Figure 1 shows how PRO stands out from
different SFT-based formulations. To be specific, we rethink
the essence of RLHF and extend pair-wise contrasts from
the Bradley-Terry model (Bradley and Terry 1952) to en-
compass one-to-N contrasts within a ranking of arbitrary
lengths. Then, given an input prompt x and a set of responses
ranked by humans, represented as y¹, y², …..‚y”,
the pro-
posed PRO algorithm begins by tuning the agent LLM to
treat the best response y¹ as the positive and the remaining as
negatives. This prioritization suggests that the LLM gener-
ates y¹ with a higher likelihood than those humans consider
inferior. It then repetitively ignores the current top response
and considers the next one as the positive against the rest,
until there are no more responses in the ranking.
Apart from focusing on obtaining more candidates, we
particularly deploy proxies of different levels to sample ut-
terances with various qualities and degrees of human align-
ment. Inspired by RLHF, we also add a self-bootstrapping
method to dynamically sample new candidates from the re-
cipient LLM and label it with an additional reward model.
The new candidate is added to the original set for training.
All of these extended rankings are designed to check the im-
pact of quantity, quality, and diversity of texts.
In general, PRO directly subjects the LLM to a n-length
human preference ranking. By equipping LLMs with multi-
positional and multi-dimensional contrasts among candi-
dates, PRO fully utilizes given ranking sequences of any
length. As n approaches infinity, the recipient LLM is ex-
posed to more and more samples with scores, and should
continuously become perfectly aligned with human prefer-
ences. Especially when n = 2, PRO effectively optimizes
the LLM using the pair-wise contrast.
We thoroughly evaluate PRO through numerous experi-
ments, including automatic reward model scoring, GPT-4
evaluation, and human evaluation. The main observations
are as follows: (1) With a 2-ranking, PRO surpasses the
current competitive baselines by a large margin. Also, the
high quality and diversity of candidates in preference rank-
ings can be crucial for ultimate performance. (2) The longer
the length is, the more prominent improvement PRO can ac-
quire. Even by adding responses generated by ChatGPT to
continuously increase the ranking length, PRO achieves re-
ward scores similar to ChatGPT, but with just 7B parame-
ters. (3) Heterogeneous candidates manage to bringing bet-
ter improvement of PRO than relatively homogeneous ones.
Preliminary
We commence by providing a brief review of RLHF. In order
to train LLMs to generate responses that align with human
preferences, RLHF consists of three stages:
(1) Supervised Fine-tuning (SFT) where labelers furnish
the desired behavior's response with t tokens, denoted as
y = Y1,...,t,
for a given prompt, denoted as x. Subsequently,
The policy LLM goes through direct fine-tuning (maximum
likelihood) on this data, resulting in a model denoted as "SFT.
(2) Reward Model (RM) Training where TSFT is utilized by
prompts x to generate pairs of responses, which are then de-
noted by human labelers as a more favored answer y¹ against
the other one y², i.e. y¹ > y² | x. To predict these prefer-
ences, previous works employ the Bradley-Terry (BT) RM,
which essentially constructs a pairwise contrast:
LRM
=
- log
exp (ro(x, y¹))
` exp (rò(x, y¹)) + exp (rø(x, y²))
(1)
(3) Reinforcement Learning (RL) stage where TSFT is further
optimized in a trial-and-error process containing repetitive
sampling from linguistic space and corresponding feedbacks
simultaneously from the RM and reference policy.
Regrettably, RLHF is criticized for several drawbacks, in-
cluding increased complexity compared to supervised learn-
ing, sensitivity to hyperparameters, and the requirement for
additional training of reward models and value networks.
Methodology
In this section, we first achieve a shift from the pair-wise
contrast to PRO that leverages multi-positional one-to-N
contrasts. With candidate rankings extending, PRO has ac-
cess to more samples from linguistic space, and efficiently
distinguishes the human-preferred feature of the positive
samples from other negative samples. The whole process is
completed in SFT settings, thus contributing to avoiding the
numerous drawbacks associated with RLHF. Furthermore,
we demonstrate the flexibility of our PRO in its ability to
integrate with RM, thereby attaining advantages such as af-
fordable preference ranking and differentiated contrast.
From RLHF to PRO
We re-examine the objective of the Bradley-Terry
RM (Equation 1), which helps LLMs understand y¹ > y²
through score contrast. The RM is trained in supervised
settings and is expected to capture human preference. For
a given prompt x and two responses y¹ and y², the RM
should prefer y¹. To directly optimize the policy model, i.e.
the LLM, we can similarly transfer the pair-wise contrast
to it. In this way, the LLM is considered as both RM and
policy network, denoted as r:
exp (r(x, y¹))
(2)
L = -log
exp (r(x, y¹)) + exp (r(x, y²))
Naturally, if we expand the candidate set, i.e., increase sam-
pling, r is able to reach more shots, which replaces the trial-
and-error experience. Considering there exist n candidate
responses {y}, the human-annotated order y¹,,n is y¹ >
y² > >yn. We first define the partial order between
y¹ and responses behind it as y¹,2:n = y¹ > {y², …..‚y"}.
With reference to InfoNCE Loss (He et al. 2020), we derive
Equation 2 to a multi-dimensional one-to-N contrast:
...
L = =
= — log
exp (r(x, y¹))
Σ²±₁ exp (r(x, y²))
(3)
However, it does not fully leverage the ranking since it only
characterizes y1, 2:n, disregarding the n-2 valuable rankings
Context
Human: How do I attract butterflies to my garden?
Assistant: How would you like them to be attracted?
Human: They are pretty.
Assistant:
positive
LLM
G
Candidate Responses
y
y² y³
y"
000 000 000
SFT
+ p1 > p² ... pr +
n
negative
n-1
Figure 2: The pipeline of PRO for Human Feedback Alignment learning. Each candidate is concatenated with the prompt first,
then processed by the LLM to estimate corresponding rewards, which are optimized by Equation 5.
such as y²,3:n and yn−1,”. Instead, recursive contrasts can
be imposed to exploit multi-positional patterns, which start
with the first response, treat the remaining responses as neg-
atives, drop the current response, and move to the next. This
procedure repeats until there are no candidates left. Conse-
quently, the further extension to Equation 3 is as follows:
n-1
L = − log II
n
k=1
exp (r(x, y))
Σk exp (r(x, y²))
i=k
(4)
Equation 4 provides a comprehensive alignment with human
feedback. In addition to adhering to human preferences, it is
also desirable for the model to generate fluent replies. There-
fore, the original supervised loss that requires the model to
fit the responses considered the best by humans can also
be incorporated. We conclude the above as the Preference
Ranking Optimization (PRO) algorithm, as is demonstrated
in Figure 2. Instead of optimizing the agent to approximate
the RM, PRO enables the agent LLM to be directly trained
by the following objective:
LPRO (y¹n | x)
= L+ BLSFT
(5)
where LSFT is the NLL loss of the top 1 candidate and ẞ
is the hyper-parameter to maintain the balance between text
quality and human preference. The policy agent (also RM)
г is parameterized as "PRO in PRO:
|yk|
1
TπPRO (x, y)
=
|yk|
log P(x, y)
t=1
(6)
PRO and RLHF share a similar objective, that is, under-
standing human preferences through more exposure to la-
beled samples. The difference is that RLHF relies on trial-
and-error experience and pair-wise contrasts, whereas PRO
learns by assembling multiple samples into long rankings,
which can be more efficient.
Surprisingly, Equation 4 has a similar formulation with
Plackett-Luce (PL) model (Plackett 1975; Luce 2012), a
classic algorithm for ranking aggregation. Believing it is
not a coincidence, we assume that PL model and PRO ac-
complish similar targets. PL model aims to acquire a global
ranking of fixed candidates by combining multiple rankings,
whose parameters correspond to these candidates, while
PRO aims to learn general human preference, but the in-
volved rankings contain different n candidates from each
other. With modeling language space, the parameters of
agent LLM should theoretically correspond to infinite can-
didates for each ranking (i.e. n = ∞). Although n is limited
in practice, the larger it is, the more perfect the LLM should
be. We accordingly implement experiments towards n in the
next section.
Grafting RLHF onto PRO
While PRO can be directly valid on the human-annotated
preference ranking sequence without the need for introduc-
ing concepts like the reward model in RLHF, grafting RLHF
onto PRO can bring more flexibility to PRO. We outline
three possible upgrades as follows:
Affordable Preference Ranking. PRO is highly flexible,
relying solely on ranked preference sequences. The source
of the sequence is unrestricted, allowing for various possibil-
ities. One approach involves requesting annotators to imag-
ine multiple responses of different quality. Alternatively,
a more efficient method entails utilizing different existing
LLMs, such as ChatGPT and Alpaca, to generate multiple
responses. These responses can be ranked using an addi-
tional reward model
ro,
similar to RLHF.
Differentiated Contrast. The formulation as shown in
Equation 4, treats all responses y² < yk as negative exam-
ples of y and applies the same penalty to them. However,
this approach may not be reasonable, especially when the
preference scores of different y² are similar. For instance,
when the preference of yk+1 is only slightly worse than yk,
while y" is significantly worse than yk, the model should
differentiate and apply different penalty strengths, slightly
penalizing y+1 and heavily penalizing yn compared to yk.
To address this, we propose using the score ro (x, y²) from a
reward model ro to indicate the numerical preference of y²,
and modify Equation 4 as follows:
n-1
c=-Σ 10g
==
k=1
where
Ti>k
=k
exp
TπPRO (x,y)
Tk
(7)
Σε exp
TAPRO (x,y)
1
ro(2,gh) – ro(x,y)
(8)
(9)
T = min Th
i>k
When the difference between r$ (x, yk) and rø(x, y²) in-
creases, the preference gap between yk and y² becomes more
evident. Consequently, the temperature TË decreases, ampli-
fying the penalty of positive example yk towards y², while
it decreases when the difference is smaller. T is defined as
the minimum temperature among all the negative examples
to maintain a balance between the numerator and denomina-
tor. Our experiments (§) reveal that the dynamic temperature
design significantly increases performance when optimizing
LPRO alone while excluding LSFT. It also provides some per-
formance gains when jointly optimizing LPRO and LSFT.
Data Prepration
Experiments
We choose HH-RLHF Bai et al. (2022a) as the experimental
dataset. It has 4 sub-sets, namely Harmlessbase, Helpfulɓase,
Helpfulonline and Helpfulrejection. Each sample contains two
different conversations rated by human annotators and is
grouped into train/test splits.
To further evaluate the performance of different methods
on longer rankings, we augment each sample with new can-
didates from diverse LLMs to expand the range of ranked
responses. These augmented datasets are denoted as HH-
RLHFLLM,i, where LLM represents the language models
used (e.g. Alpaca, ChatGPT), and i is the length of the rank-
ings. The unmodified dataset is referred to as HH-RLHFraw.
Disclaimer: Data augmentation and inference from
Curie/ChatGPT, as well as the following GPT-4 evaluation,
are completed where the related services are available.
Evaluation Metrics
We present the findings using various evaluation methods:
automatic, model-based, and human-based metrics. Specif-
ically, we utilize BLEU (Papineni et al. 2002) to assess the
text quality and RMs to measure the level of human pref-
erence gained. To avoid unfairness, we select two differ-
ent RMs for training and evaluation, which we denote as
RM train and RMeval, respectively. These metrics allow us to
automatically evaluate numerous models. Human evaluation
is the gold standard for assessing human preferences (Zhou
et al. 2023). An annotator judge is presented with a question
and two responses and tasked with determining the better
option or declaring a tie. Furthermore, recent studies have
shown that GPT-4 (OpenAI 2023) effectively evaluates the
responses of chat assistants and aligns with human prefer-
ences (Zheng et al. 2023; Wang et al. 2023). Consequently,
we involve GPT-4 to select the best from the two options.
To mitigate positional bias (Zheng et al. 2023; Wang et al.
2023), we evaluate each candidate in both positions during
two separate runs, and the final score is computed as the av-
erage of the two runs.
Implementation Details
We choose the popular LLaMA-7B (Touvron et al. 2023) as
the backbone model, and implement PRO using Transform-
ers (Wolf et al. 2020) and Accelerate (Gugger et al. 2022).
We assign ẞ, the weight SFT loss, to 0.05*(−1)² where l
is the ranking length. The sequence length, epoch, and learn-
ing rate are set to 512, 2, and 5e-6, respectively, while the
maximum number of new tokens generated during inference
is 128, and the total batch size is 112.
Moreover, the expanded candidate rankings in each aug-
mented dataset need to be re-sorted. However, the numer-
ous manual sortings are time-consuming and costly. There-
fore, we employ RM train to score and rearrange all candi-
date rankings during the pre-processing stage of training.
In addition, values from RMeval will be normalized through
Sigmoid function in case it occasionally provides extreme
values that excessively influence the overall performance.
RM train and RM eval are all implemented using open-source
checkpoints. More particulars can be found in our code¹.
Main Experiment
We compare PRO with several LLMs (zero-shot), as well as
baselines for fine-tuning LLaMA-7B (Touvron et al. 2023).
Table 1 contains the results. It can be found that each fine-
tuned LLaMA-7B gets a notable improvement on BLEU and
Reward against the initial version without any specific align-
ment with human preference. Also, even without fine-tuning
on HH-RLHF, LLMs can still show certain performance,
while ChatGLM (Du et al. 2022) and ChatGPT with RLHF
training beat LLaMA-7B, Curie (Brown et al. 2020), and
Alpaca-7B (Taori et al. 2023). All of these prove the signifi-
cance of Human Alignment.
Next, we compare PRO with strong baselines on the same
dataset using LLaMA-7B, including SFT, RLHF, CoH (Liu,
Sferrazza, and Abbeel 2023), DPO (Rafailov et al. 2023) and
RRHF (Yuan et al. 2023). In general, PRO can outperform
all baselines, or show competitive performance in terms of
Reward score while maintaining considerable BLEU scores.
Specifically, even in the basic HH-RLHF containing just
rankings of 2 candidates, PRO achieves a 6.52 improvement
of Reward over SFT, and 2.6 over the well-performed DPO.
CoH (Liu, Sferrazza, and Abbeel 2023) gets higher BLEU
scores but falls short of PRO in Reward, which is mediocre.
raw
PRO exhibits a distinct advantage in terms of Harm-
lessness compared to Helpfulness. We attribute this to the
fact that achieving Harmlessness is comparatively easier
github.com/AlibabaResearch/DAMO-ConvAI/tree/main/PRO
Training Set
Method
Harmlessbase
Helpful base
Helpful online
Helpful rejection
Total
BLEU Reward
BLEU Reward
LLAMA
Zero-shot
Curie
Alpaca
10.82 51.16
14.23 50.71
15.07 53.03
ChatGLM 15.39 63.30
ChatGPT 15.51 71.44
67.94
BLEU Reward
12.78 31.71
15.02 38.91 14.60 34.85 13.13 38.94
17.33 45.51 17.11 51.36 18.99 48.68 16.99 48.71
19.68 49.80 18.77 55.74 22.21 53.72 19.12 52.72
20.16 59.14 30.99 61.10 25.41 61.45 21.99 61.27
21.38 65.94 29.81
BLEU
Reward BLEU Reward
26.52
68.39
22.56 68.48
SFT
15.07
RLHF
14.54
COH
13.34
HH-RLHF raw
DPO
16.29
RRHF
PRO
55.96 20.40
55.05 19.86
45.47 23.17
54.43 21.37
13.49 53.98 18.76
12.05 62.96 20.83
41.36 29.36
42.16 28.04
39.03 33.84 52.63
50.13 27.73 54.09
48.23 30.68 56.44
48.51
54.08
25.54
47.08
21.80 48.83
53.40
25.11
47.73
21.19 48.93
29.79 46.57
24.06 45.00
26.91
53.04 22.62
52.75
24.95
52.51
20.91
52.25
28.75 59.02
27.17
53.28
21.54
55.35
BON
16.75
59.24
22.81
54.04
29.89 61.00 27.76
58.04
23.7
57.66
RLHF
16.33
56.61
23.12
54.85
COH
13.71
47.36
22.45
42.34
HH-RLHF Alpaca,3
DPO
16.37 63.93 21.82
55.86
30.54 60.97
33.17 53.19
27.84 58.49
27.94
58.4
23.82
57.28
28.76 48.61
23.54
47.15
27.53 58.60
22.98
59.27
RRHF
PRO
12.79 54.18
14.41
19.21
53.23
31.53
59.04
25.14 56.76
21.02
55.39
62.60 22.47
54.38
25.61
60.90
26.82
58.26 22.11
58.72
BON
RLHF
15.05 67.85
13.63 61.97 20.12
20.77
60.43
31.27
64.36
26.47
55.29
28.89
59.78 24.65
HH-RLHF ChatGPT,3
COH
DPO
RRHF
PRO
13.44 56.87 21.89
15.63 67.81 21.00
13.02 64.63 18.95
15.53 73.08 22.30
51.52
34.04
63.14 22.45
58.26 20.99 58.65
59.51 28.24 56.35 23.26
61.86 29.01 61.90 26.39 63.81 22.35
61.38 31.37 63.26 24.75 63.28 20.86
64.78 29.35 66.66 27.49 66.95 23.07
63.83
55.58
64.10
63.12
67.97
Table 1: Main Results. PRO consistently acquires more reward than all fine-tuned baselines, while is close to or even exceeding
ChatGLM and ChatGPT.
Ascending
67.1
Random
Alpaca
63.7
ChatGPT
60.3
56.9
2
3
4
5
Figure 3: Results of experiments on different lengths.
for PRO as it primarily involves significant features such
as adapting expression styles and maintaining politeness in
most conversations. On the other hand, Helpfulness typi-
cally demands specific suggestions, which pose a greater
challenge for language models due to their limited world
knowledge, thus increasing the difficulty in this aspect.
When expanding the ranking sequence using ChatGPT
and sorting it with RM train, the performance of each method
also increases. On the expanded sequences, we observe that
BON (selecting the response with the highest reward model
score for SFT) becomes a competitive baseline. This finding
aligns with Rafailov et al. 2023, who observed that RLHF is
less tuning-efficient than BoN. The effectiveness of RRHF
becomes less prominent because it relies on pairwise con-
trasts between candidates from given rankings. It fails to
capture global differences corresponding to human prefer-
ence in the long rankings, which can be achieved through
Equation 4. Overall, in the expanded ranking, PRO remains
the most competitive method, and the more powerful the
LLM used for ranking augmentation, the more pronounced
the improvement of PRO. This surprising characteristic fills
us with anticipation for PRO's future development.
Effect of Expanding Preference Ranking Sequence
In Table 1, we have observed that expanding the ranking se-
quence of HH-RLHF from length 2 to 3 using LLMs im-
proves the performance of all models. This leads us to won-
der how the effect would change if we further expand the
preference ranking sequence. Specifically, we simulate 4 ex-
pansion strategies, each introducing 3 additional responses
to extend the preference sequence to length 5, followed by
reranking using a reward model.
raw
Alpaca: Using Alpaca-7B, we generate 3 responses, adding
1, 2, and 3 responses, respectively, to form ranking se-
quences of lengths 3, 4, and 5.
ChatGPT: Using ChatGPT, we generate three responses,
adding 1, 2, and 3 responses, respectively, to form ranking
sequences of lengths 3, 4, and 5.
Ascending: We utilize three LLMs, namely Curie, Alpaca-
7B, and ChatGPT. Based on the zero-shot results in Table 1,
the quality of their responses can be ranked as ChatGPT >
Alpaca-7B > Curie. In this setting, we add the responses in
ascending order of quality, i.e. Curie's response in rankings
of length 3, Curie and Alpaca-7B's responses in rankings of
length 4 while Curie, Alpaca-7B, and ChatGPT's responses
in rankings of length 5.
Random: The order of response additions is unrelated to
response quality and is done randomly.
Figure 3 presents the impact of various expansion strate-
gies on the effectiveness of PRO after expanding sequences
of different lengths. Our observations are as follows:
Longer ranking, better results: Overall, longer rank-
ing sequences generally lead to improved performance for
most strategies, as longer rankings embrace more candi-
dates. It implies that more sampling from linguistic space
with feedback labels effectively helps LLMs align with hu-
man preference. This is an exciting finding because with a
well-performed RM, which is relatively easy to obtain, ex-
panding rankings can be a straightforward task compared to
brainstorming for new prompts.
Better added responses, better results: If a single model
is used to generate additional responses, supplementing one
response is sufficient when the quality is average, such as
with Alpaca, adding more responses provides limited im-
provement. However, when the quality of responses is high,
as with ChatGPT, adding more responses leads to consistent
performance gains. This could potentially offer new insights
for the design of future Human Alignment algorithms.
More diversified added responses, better results:
Incorporating lower-quality responses may better im-
prove the LLM compared to using only high-quality re-
sponses. Interestingly, when the sequence length is 4,
Ascending (blue line) = Curie + Alpaca surpasses the per-
formance of Alpaca (green line) Alpaca Alpaca, even
though Curie's response quality is not as good as Alpaca's.
We attribute it to the fact that diverse responses, even if they
are negative examples, help the language model become
more aware of behaviors that should be avoided, thereby en-
hancing overall performance. Lastly, by combining Curie,
Alpaca, and ChatGPT, we achieve a performance close to
using three ChatGPT responses, demonstrating the truth in
the saying, "Two heads are better than one."
Human and GPT-4 Evaluation
Compared with reward models which may have a distortion
in capturing human preferences, human annotation is con-
sidered the most accurate evaluation method, and recently,
GPT-4-as-a-judge has emerged as a scalable approach for
rapidly assessing human preference.
To verify whether PRO truly captures human preferences,
we provide comprehensive evaluations conducted by both
GPT-4 and humans. We hereby investigate the performance
of PRO vs. Golden, i.e. the 1st candidate provided by the
datasets. In detail, we aim to determine whether PRO trained
on HH-RLHF raw can achieve or surpass human-preferred re-
sponses provided by the raw dataset, which contains ranking
sequences of length 2 that do not fully exploit PRO's capa-
bilities. On the other hand, this comparison serves as evi-
dence to some extent for the validity of the reward model
we use in evaluation.
Evaluator
Sub-set
Tie Lose
Win
Harmlessbase
Helpful base
GPT-4
Helpful online
Helpful rejection
Average
60.00 5.00 35.00
77.50 0.00 22.50
27.50 12.50 60.00
55.00 0.00 45.00
55.00 4.37 40.63
Human
Harmlessbase
Helpful base
Helpful online
Helpful rejection
Average
20.00 60.00 20.00
20.00 50.00 30.00
30.00 60.00 10.00
22.50 56.25 21.25
20.00 55.00 25.00
Table 2: Results of GPT-4 and Human Evaluation.
For GPT-4 evaluation, we first sample contexts in test sets.
We assemble two corresponding responses from PRO and its
counterpart into a modified version of the prompt template
from Zheng et al. (2023) for GPT-4 scoring. We also refer to
Wang et al. (2023) to provide two candidates in binary direc-
tions respectively, to eliminate unfairness triggered by order.
For Human evaluation, we employ 3 annotators to estimate
the same samples with GPT-4 evaluation, and directly dis-
tinguish one from another between two shuffled responses.
Table 2 gives the detailed results, where both GPT-4 and
humans globally support PRO more, thus highlighting the
strengths of PRO. This suggests that PRO is able to effec-
tively capture the preferences of humans as reflected in the
annotated data. Furthermore, our evaluation using the re-
ward model yielded consistent results, with both humans and
GPT-4 significantly favoring PRO. This not only reaffirms
the effectiveness of PRO but also demonstrates that our re-
ward model can reasonably evaluate human preferences.
Ablation Study
In this part, we investigate the effectiveness of each part in
PRO. Table 3 presents results.
SFT Loss To avoid the model solely catering to the reward
model at the expense of text quality, we introduce LSFT.
Therefore, removing LSFT lowers BLEU scores.
PRO Loss Table 1 also demonstrates the influence of
LPRO, as excluding it in PRO essentially equals to SFT
(BON) that gets lower Reward.
Adequate Ranking To fully leverage the ranking y¹¨*
we employ n― 1 loss functions to model y1, 2:n,
y2,3:n
yn-1,n. Our objective is to adequately model all ranking or-
ders and enable LLM to better differentiate between samples
of different preferences. To validate this idea, we deactivate
LPRO except for its first term, L½ PRO. Experimental results
demonstrate a decrease in both BLEU and Reward scores,
thus confirming the effectiveness of Equation 4.
Temperature T slightly enhances overall performance.
However, we observe a significant drop in performance
when both LSFT and T are removed simultaneously, whereas
removing either one individually did not have such a notice-
able impact. We believe this is because temperature helps
Traning Set
Harmlessbase
Method
Helpfulbase
Helpfulonline
Helpfulrejection
Total
BLEU Reward BLEU Reward
PRO
HH-RLHF raw
-LSFT
-T
-LSFT
T
12.05
6.94
12.04
0.88
PRO
14.41
-Lk>1
13.38
62.6
62.88
HH-RLHF Alpaca, 3
-LSFT
9.06
-T
-LSFT - T
13.71
0.52
BLEU Reward
62.96 20.83 48.51 28.75 59.02 27.17 53.28
67.20 10.37 46.60 11.17 49.33 11.32
62.91 20.63 47.92 28.73 58.52 26.94
52.81 6.74 42.97 6.37 42.84 6.85
22.47 54.38 25.61
60.90 26.82 58.26
21.50 53.48 24.56 60.32 25.81 57.15
65.78 18.77 54.18 23.90
23.33 58.29
63.40 21.70 53.77 24.84 60.36 26.01 57.34
55.90 2.13 23.41 3.56 23.44 2.66 23.82
BLEU Reward BLEU Reward
21.54 55.35
48.84
9.85
53.25
53.08
44.71
21.41
55.04
5.14
46.17
22.11
58.72
21.10 58.11
62.26
18.29
59.71
21.34
58.40
2.05
32.33
PRO
-Lk>1
HH-RLHF ChatGPT,3
-LSFT
-T
-LSFT - T
15.53 73.08 22.30 64.78
15.20 72.64 21.94 64.44
13.81 73.18 21.28
15.77 72.99 22.13
5.93 69.61 5.22
29.35
29.17
64.20
27.90
65.34
29.03
33.92
9.33
66.66 27.49
66.97 27.29
67.15 26.57
67.48 27.28
31.81 6.11
66.95 23.07
67.97
66.80 22.80
67.75
66.76 21.84
67.54 22.98
33.52 6.25
67.84
68.40
43.16
Table 3: Ablation results. We investigate the effectiveness of L, LSFT and the dynamic temperature T.
the LLM understand that some negative examples are neu-
tral (with reward scores similar to positive examples), and
thus should not be overly penalized to avoid confusion dur-
ing LLM training. The inclusion of LSFT plays a similar role
by increasing the weight of the best response.
Related Work
Reinforcement Learning from Human Feedback
Fine-tuning LLMs to align with human preferences has
emerged as a critical research problem. It can be formu-
lated as given a context and corresponding suffixes ranked
or scored by human annotators without more detailed la-
bels, the agent is required to learn human preference and
provide human-like results. Reinforcement Learning (RL)
can be the most straightforward way to reach this goal, for
the agent just scarce supervision signal from reward models
as human proxies, then is modified through numerous trials
under RL framework, namely Reinforcement Learning from
Human Feedback (RLHF). Many explorations have been
done on this path (Christiano et al. 2017; MacGlashan et al.
2017; Warnell et al. 2018; Ziegler et al. 2019; Stiennon et al.
2020; Nakano et al. 2021; Lee, Smith, and Abbeel 2021; Lei
et al. 2022; Snell et al. 2022; Bai et al. 2022a; Ouyang et al.
2022; Zhu, Jiao, and Jordan 2023; Zhu et al. 2023). Stien-
non et al. (2020) and Nakano et al. (2021) investigate the
RLHF method for text summarization and question answer-
ing, respectively. Bai et al. (2022a) apply RLHF to enable
LLMs to be harmless and helpful while releasing a new di-
alogue dataset with human feedback. Zhu, Jiao, and Jordan
(2023) provide the bound of reward learning if formulated as
the Bradely-Terry model and Plackett-Luce model. Known
as a masterpiece, Ouyang et al. (2022) propose InstructGPT
which first goes through SFT, then is continually modified
with PPO algorithm (Schulman et al. 2017). This process is
cyclic, during which the performance of the trained agent
spirals upwards. The famous ChatGPT inherits it.
SFT for Human Preference Alignment
Despite appealing advantages, RLHF has obvious limita-
tions regarding training efficiency and complexity, conse-
quently driving researchers to focus on SFT without these
challenges. Liu, Sferrazza, and Abbeel (2023) combine de-
sirable and undesirable suffixes in a template prompted by
opposite keywords, fully depending on a highly semantic
understanding of LLMs. Dong et al. (2023) rely on RMs to
select data sampled from the tuned model itself, which in
turn are utilized to extend the process of fine-tuning. Yuan
et al. (2023) compose multiple pairwise contrasts between
suffixes in the given ranking, which forms a new algorithm
from the perspective of training objectives. Rafailov et al.
(2023) similarly transform LLMs as a BT model to measure
chosen and rejected candidates by human annotators. PRO
chooses the path of modifying the SFT objective, but is fur-
ther promoted from RLHF and inherits its straightforward-
ness. It transforms RL's indirect optimization into a direct
one and extends pair-wise contrasts to multi-positional and
multi-dimensional contrasts.
Conclusion
In this paper, we derive from pair-wise contrasts of reward
models in RLHF that human alignment can be modeled as
aligning the probability ranking of n responses generated
by the LLM and the preference ranking of these responses
by humans. Based on this derivation, we propose PRO al-
gorithms. PRO inherits the advantages of RLHF, and fur-
ther captures fine-grained distinction corresponding to hu-
man preference from multiple one-to-N contrasts. We con-
duct extensive experiments to verify the excellence of PRO
against baselines and investigate the impact of multifaceted
factors. Overall, the findings presented in this paper demon-
strate the significance of PRO in effectively and efficiently
aligning LLMs to human preference. This work can serve as
a stepping stone for further quantifiable explorations.
Ethics Statement
There exists sensitive and offensive content in the data used,
which aims for only research purposes. Views included in it
do not represent our attitudes. We hope our work can be used
to make AI technologies in line with ethical requirements.
Acknowledgments
This work was supported by the National Key R&D Program
of China (No. 2022ZD0116308), and the National Natural
Science Foundation of China (Grant No. 62036001).
References
Bai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; Das-
Sarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan,
T.; et al. 2022a. Training a helpful and harmless as-
sistant with reinforcement learning from human feedback.
arXiv:2204.05862.
Bai, Y.; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.;
Jones, A.; Chen, A.; Goldie, A.; Mirhoseini, A.; McKinnon,
C.; et al. 2022b. Constitutional AI: Harmlessness from AI
Feedback. arXiv:2212.08073.
Bradley, R. A.; and Terry, M. E. 1952. Rank analysis of
incomplete block designs: I. The method of paired compar-
isons. Biometrika, 39(3/4): 324–345.
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems, 33: 1877–
1901.
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.;
Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg,
S.; et al. 2023. Sparks of artificial general intelligence: Early
experiments with gpt-4. arXiv:2303.12712.
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;
Gehrmann, S.; et al. 2022. Palm: Scaling language modeling
with pathways. arXiv:2204.02311.
Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.;
and Amodei, D. 2017. Deep Reinforcement Learning from
Human Preferences. In Guyon, I.; Luxburg, U. V.; Bengio,
S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett,
R., eds., Advances in Neural Information Processing Sys-
tems, volume 30. Curran Associates, Inc.
Dong, H.; Xiong, W.; Goyal, D.; Pan, R.; Diao, S.;
Zhang, J.; Shum, K.; and Zhang, T. 2023. Raft: Reward
ranked finetuning for generative foundation model align-
ment. arXiv:2304.06767.
Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
Tang, J. 2022. GLM: General Language Model Pretraining
with Autoregressive Blank Infilling. In Proceedings of the
60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), 320–335. Dublin, Ire-
land: Association for Computational Linguistics.
Gao, L.; Schulman, J.; and Hilton, J. 2022. Scaling Laws for
Reward Model Overoptimization. arXiv:2210.10760.
Gugger, S.; Debut, L.; Wolf, T.; Schmid, P.; Mueller, Z.; and
Mangrulkar, S. 2022. Accelerate: Training and inference at
scale made simple, efficient and adaptable.
He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020.
Momentum Contrast for Unsupervised Visual Representa-
tion Learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR).
Lee, K.; Smith, L.; and Abbeel, P. 2021.
ble: Feedback-efficient interactive reinforcement learning
via relabeling experience and unsupervised pre-training.
arXiv:2106.05091.
Peb-
Lei, W.; Zhang, Y.; Song, F.; Liang, H.; Mao, J.; Lv, J.; Yang,
Z.; and Chua, T.-S. 2022. Interacting with Non-Cooperative
User: A New Paradigm for Proactive Dialogue Policy. In
Proceedings of the 45th International ACM SIGIR Con-
ference on Research and Development in Information Re-
trieval, SIGIR '22, 212–222. New York, NY, USA: Associ-
ation for Computing Machinery. ISBN 9781450387323.
Li, M.; Song, F.; Yu, B.; Yu, H.; Li, Z.; Huang, F.; and Li,
Y. 2023. Api-bank: A benchmark for tool-augmented llms.
arXiv:2304.08244.
Liu, H.; Sferrazza, C.; and Abbeel, P. 2023. Chain
of Hindsight aligns Language Models with Feedback.
arXiv:2302.02676.
Luce, R. D. 2012. Individual choice behavior: A theoretical
analysis. Courier Corporation.
MacGlashan, J.; Ho, M. K.; Loftin, R.; Peng, B.; Wang, G.;
Roberts, D. L.; Taylor, M. E.; and Littman, M. L. 2017.
Interactive Learning from Policy-Dependent Human Feed-
back. In Precup, D.; and Teh, Y. W., eds., Proceedings of
the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research,
2285-2294. PMLR.
Nakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.; Kim,
C.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.; et al.
2021. Webgpt: Browser-assisted question-answering with
human feedback. arXiv:2112.09332.
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;
et al. 2022. Training language models to follow instructions
with human feedback. Advances in Neural Information Pro-
cessing Systems, 35: 27730-27744.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
Bleu: a Method for Automatic Evaluation of Machine Trans-
lation. In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, 311–318. Philadel-
phia, Pennsylvania, USA: Association for Computational
Linguistics.
Peng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. 2023. In-
struction tuning with gpt-4. arXiv:2304.03277.
Plackett, R. L. 1975. The analysis of permutations. Journal
of the Royal Statistical Society Series C: Applied Statistics,
24(2): 193–202.
Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,
C. D.; and Finn, C. 2023. Direct Preference Optimiza-
tion: Your Language Model is Secretly a Reward Model.
arXiv:2305.18290.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov, O. 2017. Proximal policy optimization algorithms.
arXiv:1707.06347.
Snell, C.; Kostrikov, I.; Su, Y.; Yang, M.; and Levine, S.
2022. Offline rl for natural language generation with im-
plicit language q learning. arXiv:2206.11871.
Stiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.;
Voss, C.; Radford, A.; Amodei, D.; and Christiano, P. F.
2020. Learning to summarize with human feedback.
Advances in Neural Information Processing Systems, 33:
3008-3021.
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.;
Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stan-
ford Alpaca: An Instruction-following LLAMA model.
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.;
Azhar, F.; et al. 2023. Llama: Open and efficient founda-
tion language models. arXiv:2302.13971.
Wang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y.; Liu,
Q.; Liu, T.; and Sui, Z. 2023. Large language models are not
fair evaluators. arXiv:2305.17926.
Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N. A.;
Khashabi, D.; and Hajishirzi, H. 2022. Self-Instruct:
Aligning Language Model with Self Generated Instructions.
arXiv:2212.10560.
Warnell, G.; Waytowich, N.; Lawhern, V.; and Stone, P.
2018. Deep TAMER: Interactive Agent Shaping in High-
Dimensional State Spaces. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 32.
Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;
Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-
son, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu,
J.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest, Q.;
and Rush, A. 2020. Transformers: State-of-the-Art Natural
Language Processing. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language Process-
ing: System Demonstrations, 38-45. Online: Association for
Computational Linguistics.
Wu, Z.; Hu, Y.; Shi, W.; Dziri, N.; Suhr, A.; Ammanabrolu,
P.; Smith, N. A.; Ostendorf, M.; and Hajishirzi, H. 2023.
Fine-Grained Human Feedback Gives Better Rewards for
Language Model Training. arXiv:2306.01693.
Xue, W.; An, B.; Yan, S.; and Xu, Z. 2023. Re-
inforcement Learning from Diverse Human Preferences.
arXiv:2301.11774.
Yuan, Z.; Yuan, H.; Tan, C.; Wang, W.; Huang, S.;
and Huang, F. 2023. Rrhf: Rank responses to align
language models with human feedback without tears.
arXiv:2304.05302.
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judg-
ing LLM-as-a-judge with MT-Bench and Chatbot Arena.
arXiv:2306.05685.
Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y.; Ma, X.;
Efrat, A.; Yu, P.; Yu, L.; et al. 2023. Lima: Less is more for
alignment. arXiv:2305.11206.
Zhu, B.; Jiao, J.; and Jordan, M. I. 2023. Principled Rein-
forcement Learning with Human Feedback from Pairwise or
K-wise Comparisons. arXiv:2301.11270.
Zhu, B.; Sharma, H.; Frujeri, F. V.; Dong, S.; Zhu, C.;
Jordan, M. I.; and Jiao, J. 2023. Fine-Tuning Lan-
guage Models with Advantage-Induced Policy Alignment.
arXiv:2306.02231.
Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Rad-
ford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2019.
Fine-tuning language models from human preferences.
arXiv:1909.08593.
Qualitative Analysis of PRO
Comparing RLHF and PRO, we find that both PRO and
RLHF share the primary goal of human alignment. RLHF
achieves this by providing better responses through a re-
ward model with higher discrete scores, requiring RL tech-
niques. In contrast, PRO directly achieves this through rank-
ing scores, thereby avoiding many drawbacks associated
with RL. The second objective of both PRO and RLHF is
to ensure high-quality model outputs. PRO's alignment ob-
jective is differentiable, allowing for multi-task learning by
combining alignment and SFT objectives through single-
stage training. On the other hand, RL, due to the discrete
optimization problem, requires training the SFT model first
and then constraining the RL model from deviating ex-
cessively from SFT. Consequently, RLHF necessitates two-
stage training, which undoubtedly increases training costs.
Comparing SFT, Equation 1 and Equation 4, we can ob-
serve that PRO is more data-efficient. SFT can only lever-
age responses considered as desired in a preference rank-
ing, completely disregarding negative responses. We believe
negative examples are crucial in human alignment since
LLM should not only learn what is good but also discern
what is not. The critical component of RLHF, the reward
model, is trained through pairwise response contrasts, re-
quiring (2) comparisons for a ranking of length n. In con-
trast, PRO only needs n 1 contrasts and introduces more
negative examples in each contrast compared to RLHF.
Therefore, PRO provides better and more stable score es-
timates since more negative examples enlarge the response
space, making the ranking process for obtaining the desired
response more aligned with human expectations.
-
Data Preprocessing
We combine training data from 4 sub-sets to fine-tune mod-
els and evaluate them on each test set, while we do validation
with 280 samples randomly selected from all test data. Each
sample contains a chosen conversation and a rejected one,
which constitutes a relatively short ranking.
We refer to the code² released by OpenAssistant and fil-
ter all data to ensure that all candidates in one sample have
identical contexts but different responses.
2github.com/LAION-AI/Open-Assistant
Sub-set
# train
Raw
Harmlessbase
Helpful base
Helpful online
Helpfulrejection
# test
42537 2312
Filtered 42536 2312
Raw
43835 2354
Filtered 43835 2354
Raw
22007 1137
Filtered 22002 1137
Raw
52421 2749
Filtered 52420 2749
Table 4: Statistics of HH-RLHF raw.
Baselines
We compare PRO with zero-shot baselines, and models fine-
tuned on LLaMA-7B (Touvron et al. 2023) which share the
same backbone with PRO:
LLAMA (Touvron et al. 2023) is a collection of prevalent
foundation models released to enhance research on LLM
techniques of training, inference, and widespread applica-
tions. We evaluate the 7B version of LLAMA (LLaMA-7B)
to be consistent with other fine-tuned baselines.
Curie (Brown et al. 2020) is considered as the 6.7B ver-
sion of GPT-3, which has a similar size to LLaMA-7B. The
model name used in API calls is text-curie-001.
Alpaca (Taori et al. 2023) is an instruction-tuned version
of LLAMA based on 52K instruction-following data. It is
estimated to have a similar instruction-following compe-
tence with text-davinci-003 on the Self-Instruct evalua-
tion suite (Wang et al. 2022).
ChatGLM (Du et al. 2022) is an bilingual chatbot with 6.2B
parameters. Having been implemented on GLM architec-
ture (Du et al. 2022) and trained with SFT and RLHF on
a large-scale conversation dataset, it manifests great poten-
tial of being in line with human preference.
ChatGPT is an online chat platform developed by OpenAI,
which possesses great human-like abilities and allows ver-
satile uses completed in the conversation form, after RLHF
fine-tuning.
SFT is the basic method that naively selects the top 1 can-
didate to fine-tune language models. Note that if we choose
the best response in a preference ranking sequence sorted by
a reward model, known as best-of-n sampling, SFT evolves
into BoN.
RLHF is successively promoted by Ziegler et al. (2019) and
Ouyang et al. (2022) to align the core of language models
with human preference in Reinforcement Learning settings.
CoH (Liu, Sferrazza, and Abbeel 2023) enforces language
models to differentiate the most preferred candidate from the
least preferred with prompts, which aligns models with hu-
man preference from a semantic perspective.
DPO (Rafailov et al. 2023) transforms the pair-wise con-
trastive optimization of RMs into a two-stage way of super-
vised LLMs fine-tuning. DPO and PRO share a similar mo-
tivation but are completed independently of each other.
RRHF (Yuan et al. 2023) takes candidate ranking into ac-
count, and distinguishes different candidates through pair-
wise ranking losses.
Self-bootstrapping Augmentation
Since the effectiveness of incorporating new responses to
expand ranking length has been proved, a natural question
arises: Can we further improve the model's performance by
including responses from the LLM itself in the candidate
list? We notice RLHF where new candidates can be sam-
pled from the LLM, and then rewarded by RMs to bootstrap
the LLM itself. This allows us to consider grafting the self-
bootstrapping advantage of RLHF as a subset onto PRO.
This can be seen as a special approach to expanding pref-
erence ranking sequences:
Given the prompt x and the current model, we sample a
response ŷ and add it to the existing response set {y}. Then
we re-rank the responses using the reward model, yielding
p(ŷ¹‚ | x). Therefore, further optimization can be per-
formed by refreshing Equation 5:
1,...,n+1
1,...,n
1,...,n+1
LPRO (y¹, | x) = LPRO (ŷ¹‚· | x) (10)
We name the approach as Self-bootstrapping, whose ab-
stract training procedures are as follows:
Algorithm 1: Self-bootstrap PRO
Input: Language Model TLM, Reward Model
Output: The fine-tuned LM PRO
1: Split D into {D0, D1, ..., Dк−1}.
2: for D₁ = {Do, D1, ..., DK-1} do
for Sample d = Di do
Let x =
Prefix(d).
Let {y} = Candidates(d).
Sample ŷ from Tм(x).
3:
4:
5:
6:
7:
8:
9:
end for
i+1
10:
Add ŷ to {y}.
ro,
Score and re-rank {y} with x and ro.
Let TLM
11: end for
K
PRO(TLM, Di).
12: Let TPRO = "LM
13: return πPRO.
Dataset D
From Table 5, we find that self-bootstrapping³ exhibits
conflicting results. On HH-RLHF raw, self-bootstrapping
shows an improvement in BLEU but a slight decrease in
reward score. On HH-RLHF Alpaca,3, both BLEU and re-
ward score decrease. However, on HH-RLHF ChatGPT,3, self-
bootstrapping improves reward score while maintaining
BLEU value. We speculate that self-bootstrapping is effec-
tive only when the underlying language model is strong.
Furthermore, although self-bootstrapping enhances perfor-
mance on HH-RLHF ChatGPT, 3, it can be seen as extending
the ranking sequence to 4, and the improvement may not
³The naive self-bootstrapping makes LLMs easily overfit
RM train. We accordingly regularize it by preventing the augmented
candidate from taking the position of the originally top 1 and re-
ranking all rewards to ensure the descending order.
Harmlessbase
Helpfulbase
Helpful online
Helpful rejection
Total
Training set
Method
BLEU Reward BLEU Reward
BLEU
Reward BLEU
Reward BLEU Reward
PRO
12.05
HH-RLHF raw
PROS
16.84
48.51
48.22
PRO
14.41
HH-RLHF Alpaca,3
PROS
13.44
62.6
62.44
PRO
15.53
HH-RLHF ChatGPT, 3
PROS
15.53
62.96 20.83
28.75 59.02 27.17 53.28 21.54 55.35
59.27 22.34
30.13 58.23 28.21 53.41 23.77 54.20
22.47 54.38 25.61 60.90 26.82 58.26 22.11 58.72
21.18 52.82 23.01 59.07 25.36 56.51 20.68 57.44
73.08 22.30 64.78 29.35 66.66 27.49 66.95 23.07 67.97
73.16 22.02 65.34 29.04 67.18 27.49 67.41 22.96 68.36
Table 5: Results of diverse self-bootstrapping policies.
Human: I want to steal a car and need to hot wire it.
Assistant: I'm not sure I understand what you're saying. Can you try to rephrase it?
Human: How can I steal a car without the key?
Assistant: That's an easy task. You'll need a Bic pen, a paperclip, and a rubber band.
Human: How do I use those items to steal the car?
Assistant:
LLM output: You'll need to open the hood of the car, and then use the paperclip to open the hood latch. Then you'll
need to remove the spark plugs, and then use the Bic pen to short out the spark plugs. Then you'll need to put the spark
plugs back in, and then you'll need to start the car.
Figure 4: LLM is misled by the given context to provide a negative response. Warning: There exists harmful content in this
error case.
be as significant as adding one more high-quality response
generated by ChatGPT. We also acknowledge that these rel-
atively negative results may stem from training a 7B model
with a reward model of size 1.4B. Expanding the model size
might yield more exciting performance gains, similar to the
scaling law of RLHF (Ouyang et al. 2022; Gao, Schulman,
and Hilton 2022), which we leave for future work.
Error Analysis
We observe that LLMs after fine-tuning, despite their ef-
fective alignment with human preference, sometimes fail to
generate positive responses due to a susceptibility to con-
textual misleading (Figure 4). We believe that more fine-
grained design of algorithms, such as turn-level supervision,
could be helpful to this issue.
