arXiv:2306.01841v1 [cs.CL] 2 Jun 2023
Zechun Liu*
Reality Labs, Meta Inc.
zechunliu@meta.com
Binary and Ternary Natural Language Generation
Aasish Pappu
Barlas Oğuz*
Meta AI
barlaso@meta.com
Meta AI
aasish@fb.com
Yangyang Shi
Reality Labs, Meta Inc.
yyshi@meta.com
Abstract
Ternary and binary neural networks enable
multiplication-free computation and promise
multiple orders of magnitude efficiency gains
over full-precision networks if implemented
on specialized hardware. However, since
both the parameter and the output space are
highly discretized, such networks have proven
very difficult to optimize. The difficulties are
compounded for the class of transformer text
generation models due to the sensitivity of
the attention operation to quantization and the
noise-compounding effects of autoregressive
decoding in the high-cardinality output space.
We approach the problem with a mix of
statistics-based quantization for the weights
and elastic quantization of the activations
and demonstrate the first ternary and binary
transformer models on the downstream tasks
of summarization and machine translation. Our
ternary BART base achieves an R1 score of 41
on the CNN/DailyMail benchmark, which is
merely 3.9 points behind the full model while
being 16x more efficient. Our binary model,
while less accurate, achieves a highly non-
trivial score of 35.6. For machine translation,
we achieved BLEU scores of 21.7 and 17.6 on
the WMT16 En-Ro benchmark, compared with
a full precision mBART model score of 26.8.
We also compare our approach in the 8-bit
activation setting, where our ternary and even
binary weight models can match or outperform
the best existing 8-bit weight models in the
literature. Our code and models are available at:
https://github.com/facebookresearch/
Ternary_Binary_Transformer.
1 Introduction
Generative pre-trained transformers (Brown et al.,
2020; Lewis et al., 2020; Radford et al., 2018) have
emerged as powerful and generic tools, driving
breakthroughs not only in language understanding
but the field of AI in general. These models owe
*Equal contribution
Raghuraman Krishnamoorthi
Reality Labs, Meta Inc.
raghuraman@meta.com
their success mainly to their seemingly infinite abil-
ity to scale to ever-larger data and model sizes. Un-
fortunately, such scaling comes at the cost of large
computational requirements, putting extensively
large generative transformers out of reach of all but
the most resource-rich institutions. Even moder-
ately sized pre-trained transformers have limited
applications due to their size and computational
cost. Making generative transformers more effi-
cient is imperative for widening their use to more
devices and practical applications.
In this work, we explore making generative pre-
trained transformers more efficient via the quanti-
zation of their weights and activations. Quantizing
the weights of a neural network is useful for com-
pression and allows the model to be stored more ef-
ficiently. However, compression alone does not re-
duce computation costs since the network's activa-
tions need to be computed in full precision. Quan-
tizing both weights and activations allows compu-
tation to be performed with lower precision, po-
tentially leading to significant efficiency gains de-
pending on the quantization level and hardware im-
plementation. Quantizing neural networks have a
long history, and multiple works have attempted to
quantize pre-trained transformers at various quanti-
zation levels (Shen et al., 2020; Zhang et al., 2020;
Liu et al., 2022; Qin et al., 2021). Most of this work
focuses on encoder-only models (mainly BERT) for
sentence and token classification tasks. Quantizing
text generation models has generally been regarded
as a more difficult task (Behnke et al., 2021; Tao
et al., 2022) due to the large output vocabulary and
sequential decoding. Recent work has tackled this
problem, though only for mild quantization levels
(down to 8-bit activations) and with mixed success.
In contrast, we are interested in very low-bit
quantization, down to ternary and even binary
weights and activations. In order to achieve this,
we combine and unify best practices for weight
and activation quantization and present a frame-
work that uses gradient-matching quantization for
weights and elastic quantization for activations. We
apply our method to natural language generation
tasks and, for the first time, demonstrate low-bit
generative transformers of competitive accuracy.
Our ternary (weight and activation) model lags a
full-precision BART (Lewis et al., 2020) model by
only 4 points in ROUGE on the XSUM summariza-
tion dataset. In contrast, our model with ternary
weights and 8-bit activations comes within 1 point
and even outperforms comparable state-of-the-art
models with 8-bit weights. We also demonstrate a
fully binary (weights and activations) model. While
not as competitive, it is able to achieve a highly
non-trivial ROUGE-1 score of 31.7.
Our results also extend to machine translation
models. On the WMT16 En-Ro benchmark, we
quantize an mBART model to extend the ternary-
weight 8-bit activation SoTA by 1.2 points while
demonstrating fully ternary and fully binary trans-
lation models for the first time.
We summarize our contributions as follows:
• We propose a novel combination of statistics-
based weight quantization with learning-based ac-
tivation quantization, which enables stably training
transformer encoder-decoder models to converge
in the fully ternary/binary settings, which was not
previously possible.
• We significantly improve the state-of-the-art
text generation models in the 8-bit activation and
ternary/binary weight settings while setting the first
non-trivial baselines for the fully ternary and fully
binary settings.
2 Method
In this section, we first introduce the previous prac-
tices in binarization and ternarization. Then, we
introduce a unified statistic-based weight binariza-
tion / ternarization method that can alleviate the
gradient mismatch issue and enhance the quantized
weights entropy. Lastly, we analyze the difference
between weight quantization and activation quanti-
zation and propose an elastic ternarization method
for activations. We abbreviate our method as TBT,
short for "Ternary / Binary Transformer".
2.1 Preliminary
2.1.1 Ternarization
Ternary neural networks, where real values are
quantized to three levels, are first introduced in (Li
et al., 2016). Thus, these values can be repre-
sented in 2 bits, leading to a 16× reduction in size
and computation. Moreover, the computations can
be calculated multiplication-free, leading to even
further computation gains on suitable hardware.
The recent work integrates the ternarization algo-
rithm in natural language models for quantizing
the weights and activations in classification tasks
(Zhang et al., 2020) and ternarizing the weight (8-
bit activations are used) in generative models (Li
et al., 2022; Tao et al., 2022). The general for-
mula (Li et al., 2016) for ternarization is as follows:
X
0,
if X < -A
if - A≤ X ≤A
R
(1)
if XR > A
0.7 ||XR||11
Δ
(2)
NXR
αT
.
ΣXR 1XR>A
Σ1XR>A
33
Here XT denotes the ternary weights/activations,
and XR represents their real-valued counterparts.
NXR
denotes the total number of elements in the
tensor. A is the ternary threshold, and α is the
scaling factor that minimizes 12-loss between XT
and XR.
2.1.2
Binarization
The neural network binarization denotes represent-
ing the weights and/or activation with bi-level val-
ues. It is first proposed in BNN (Courbariaux
et al., 2016) and has evolved in the follow-up
works (Rastegari et al., 2016; Liu et al., 2018).
Rastegari et al. (2016) formulates binarization as:
X = α Sign(X)
(4)
-αB"
if XR < 0
+α, if X≥0
R
||XR| |11
(5)
ав
NXR
Here XB can represent binary weights or binary
activations. a denotes the scaling-factor that min-
imize the 12 loss between XR and α Sign(XR).
The acceleration and compression effect of
ternary/binary neural networks significant. By
representing the weights and activations with
{-1,0,1}, the network enjoys ~16× memory sav-
ing compared to its 32-bit floating-point counter-
part. When further binarize the weights and ac-
tivations to only 1-bit (i.e., {−1, 1}), up to 32×
model-size reduction and 58× speedup on CPUs
have been achieved (Rastegari et al., 2016), where
the matrix multiplication operations are replaced
with light-weighted bitwise XNOR operations.
Despite its appealing characteristics, naively bi-
narizing or ternarizing the transformer model for
natural language generation results in several accu-
racy drops or even a total failure in training. It has
been observed that the attention layers of the trans-
former network are difficult to quantize to low bits.
Also, the auto-regressive decoding tends to accu-
mulate errors due to quantization. Given the nature
of generative language networks that require high-
precision output, quantizing both the activations
and weights in these models to extreme bit values
is non-trivial and has not been explored before.
2.2 Stats-based max-entropy isometric weight
quantization
We propose a statistics-based method for weight
binarization/ternarization. Particularly, this novel
quantization method considers maximizing the en-
tropy of the quantized weights and reducing the
gradient mismatch in the backward pass. Previous
works (Courbariaux et al., 2016; Bai et al., 2021b;
Zhang et al., 2020) are mainly focused on minimiz-
ing the 12 loss between the quantized weights and
the real-valued weights to find the optimal quanti-
zation scheme,
a*
(6)
arg min ||aŴQ WR||12
where ŴQ denotes binary/ternary weights and a*
denotes the optimal scaling factor calculated. De-
spite the broad application and great success of the
classic quantization scheme, we found that merely
minimizing the 12 loss neglects several critical but
intractable issues in ultra-low-bit weight quantiza-
tion: (1) The information entropy of the quantized
weights is not considered. Eq. 1 and Eq. 4 cal-
culate the quantized weights to minimize the dis-
tance to the real-valued weights, which could lead
to imbalanced quantized weight distribution and
harm the quantized weights representation capacity.
(2) The quantization function Eq. 1 and Eq. 4 are
not isometric, meaning that it does not consider
the magnitude consistency between the quantized
weights and real-valued weights, while we find that
magnitude consistency contributes significantly to
accurate gradient estimation.
Considering the above two limitations in previ-
ous solutions, we are motivated to design a novel
quantization function that enhances information en-
tropy and reduces gradient mismatch. To boost the
weights representation capability, in information
theory, more information is preserved when the
quantized weights contain higher entropy:
N
= −pi log(pi), s.t. Pi
max H =
Pi
i=1
=
(7)
with på denoting the proportion of real-valued
weights being quantized to ith quantization level
in total N levels. Eq. 7 can be easily solved
with a Lagrange multiplier, and the optimal p
¼, i = {1, 2, ……., N}, suggesting the best quanti-
zation scheme to preserve maximum information
entropy is to distribute the real-valued weights in
all quantization levels as evenly as possible.
For reducing the gradient mismatch, as sug-
gested by the previous binarization work (Liu et al.,
2020b), the magnitude difference between the quan-
tized weight and the real-valued weight will greatly
influence the gradient scale and a mismatch in mag-
nitude will be amplified in back-propagation and
cause gradient vanishing or explosion during train-
ing. Thus it is important to ensure the magnitude
of real-valued weights and quantized weights are
consistent.
Combining two requirements discussed above,
we proposed max-entropy isometric weight quanti-
zation. In ternarization, it is formulated as
where μT
WT
=
W
α Clip(-
R-μT
1)]
ατ
WR
(8)
4 ||WRμT||11
NWR
αT
=
Where WT and WR refer to the ternary weights
and real-valued weights, respectively. The round-
ing function [.] and Clip() function quantize
weights to {-1,0,1}. μ is the mean of real-
valued weights and now denotes the number of
weights in the weight matrix. Scaling factor a is
calculated from the weight statistics and follows
the entropy rule to scale the real-valued weight
WR to be evenly distributed in quantization lev-
els. In the ternary case, the weights are quantized
to {-α, 0, α}. When the real-valued weights
are initialized as uniformly and symmetrically
distributed (He et al., 2015; Glorot and Bengio,
2010), the scaling factor will distribute
αT
to [-1.5, 1.5], such that the output ternary weights
Wi
ат
R
Stats-based
Weight
Quant Q
Quantized
Embeddings
Learning-based
Symmetric
Activation Quant
Stats-based
Weight
Quant K
Learning-based
Learning-based
Symmetric
Symmetric
Activation Quant
Activation Quant
Softmax
Learning-based Asymmetric
Activation Quant {0,1,...}
Learning-based
Symmetric
Activation Quant
Stats-based
Weight
Quant V
Learning-based
Symmetric
Activation Quant
Stats-based Weight
Quant FC
ReLU
Learning-based Asymmetric
Activation Quant {0,1,...}
Learning-based
Symmetric
Activation Quant
Stats-based
Weight →
Quant FC
Stats-based
Weight
Quant FC
Feed-Forward Network
Self-Attention
Transformer Block Output
Figure 1: Overview of TBT. A transformer block contains the multi-head self-attention and feed-forward network.
We propose a statistic-based quantization method for weights ternarization/binarization and adopt a learning-based
asymmetric quantization method for activation in ReLU/Softmax output (X = R2) and learning-based asymmetric
quantization method for activations that contain both positive and negative values in other layers (X = R^).
αT
will have near uniform distribution in three ternary
levels. Meanwhile, Eq. 8 is an isometric mapping
where the real-valued weights are scaled by 1 to
near [-1, 1] and time α to scale back after quanti-
zation. In this way, the magnitude is preserved.
Correspondingly, in the binary case we have,
WB
=
QB Sign(
where μB
ав
.
=
WR - μB)
WR,
ав
||WR-B||
Ꭱ
NWR
(9)
Here WB denotes the binary weights, where
substracting the average μ makes the real-
valued weight zero-centered before binarization
and thus encourages an even distribution in bi-
narized weights. Then the scaling factor QB
matches the magnitude between real-valued and
binary weights. Particularly, in Eq. 9, W
QB Sign(.
.
(WB) = α Sign (WR - HB), we
aB
explicitly include the α in the denominator to
keep the binarization function isometric and the
gradients w.r.t. weights can be calculated straight-
forwardly as:
OW
STE
≈ 1
ᎧᎳ .
R
wi
R -HB
aB
(10)
|<1
STE is abbreviated for straight-through estima-
tor (Bengio et al., 2013), which replaces the non-
differentiable Sign function with Clip function in
the backward pass. We show that the proposed max-
entropy isometric weight quantization improves the
accuracy of weight binarization / ternarization by
6.0/ 11.53 RougeL scores on the CNN/DailyMail
benchmark, respectively. More details can be found
in Sec. 3.2.
2.3
Learning-based activation quantization
In contrast to neural network weights that are stored
on the disk, activations are calculated on-the-fly.
The distribution of activations in a particular layer
depends on the network weights as well as the
corresponding input sequence, and thus varies from
batch to batch. In order to have the quantization
function better capture the underlying activation
distribution, we propose learning-based activation
quantization.
Inspired by BiT (Liu et al., 2022), we divide the
activation layers into two categories: the activation
layers with non-negative values (XR Є R+), i.e.,
Softmax/ReLU layer outputs and the rest of the
layers with both positive and negative activations
(XR ER). We binarize / ternarize the first acti-
vation category (XRER+) to {0, a} / {0, a, 2a},
and symmetrically quantize the later activation cat-
egory (XR ER) to {-a, a} and {-a, 0, a} in
binary and ternary cases respectively. In this way,
the activation distribution matches the original full-
precision activations and thus reduces the quantiza-
tion error. Further, we learn to scale the real-valued
activations to better fit quantization thresholds, and
this learnable scaling factor can be updated end-
to-end with the gradients from the network loss to
better account for overall network optimization.
In the ternary case, we propose the elastic ternar-
ization function formulated as,
= αT
ar Clip (✗,0,2)], if XÂЄR+
ar
αT
ER
Clip(1, 1)], if XR ЄR
αT
(11)
where XR and XT denote real-valued and ternary
activations, respectively. To keep the formula con-
cise, we set XR = XR – XR, denoting the zero-
mean real-valued activations. α is the scaling
factor. Different from the weight quantization, the
scaling factor in Eq. 11 is learned with the gradi-
ent update. We follow the practice in (Zhou et al.,
2016; Esser et al., 2019) to calculate the gradients
with straight-through estimation (STE) bypassing
the non-differentiable rounding function:
OX STE
θατ
ᎨᎢ
(12)
XR
XT
R.10≤x≤2αT
, if XRЄR+
if XRЄR
XT
ατ
The learnable scaling factor can dynamically adapt
to different activation distributions and improve
the ternarization accuracy. In the binary case, it is
formulated as.
X = α X
QB [Clip(
α B
"
,0,1)], if XRЄR+ (13)
QB - Sign (X),
if XR ER
Here XB denotes the binary activations.
Correspondingly, the gradients w.r.t. the scaling
factor a can be easily calculated as
OX STE
дав
XB
(14)
XR
ав
if XRЄR+
if XRЄR
Sign(X),
We demonstrate that with the learning-based ac-
tivation quantization method and statistics-based
weight quantization scheme, the proposed TBT for
the first time is able to quantize the BART model
for natural language generation tasks to ternary and
even binary weights and activations, and achieve
reasonable accuracy on summarization and transla-
tion benchmarks.
3 Experiments
In this section, we evaluate the effectiveness of our
low-bit quantization scheme for natural language
generative model on text summarization bench-
marks: CNN/DailyMail (Nallapati et al., 2016) and
XSUM (Narayan et al., 2018). We additionally
experiment on the machine translation task with
mBART on WMT16 English-Romanian (En-Ro)
dataset (Bojar et al., 2016a).
3.1
Experimental settings
We follow recent work (Li et al., 2022) in train-
ing the quantized network with initialization and
knowledge distillation from a full-precision pre-
trained model. Specifically, we use the BART-
base (Lewis et al., 2019) as our full-precision
baseline for summarization tasks and mBART-
large (Liu et al., 2020a) for the translation task.
We train the quantized models for 20 epochs on 8
GPUs with a batch size of 128 and a learning rate
of 2.5e-4 for 8-bit activation models and 5e-4 for
binary and ternary activation models.
3.2 Summarization
For the summarization task, we adopt the following
benchmarks:
con-
The XSUM dataset (Narayan et al., 2018)
sists of 226k documents sampled from the online
news website of BBC, together with short, one sen-
tence summaries. Since the summaries are very
short, abstractive methods tend to do better on this
dataset.
Table 1: Comparison of quantization methods for text summarization on XSUM and CNN/DailyMail benchmarks.
We use the "E-W-A (#bits)" notation referring to the number of bits of embeddings, weights and activations,
(specifically, 1 denotes binary, 2 denotes ternary). The results of QuantBart, DQ-BART and BlockPruning are
quoted from their paper. Additionally, we implement the algorithm developed in BinaryBert, BiBert and Ternary Bert
to the BART model and report the results, denoted with *. We use the rouge-{1,2,L} as evaluation metrics.
Method
BART
QuantBart (Tao et al., 2022)
DQ-BART (Li et al., 2022)
#Bits
Size
(E-W-A)
(MB)
32-32-32
532.0
8-8-8
138.1
XSUM
CNN/DailyMail
R1
FLOPS
R2 RL R1 R2 RL
1x 43.84 20.79 35.71 44.90 22.25 42.09
40.25 17.78 32.70
8-8-8
138.1
42.51 19.61 34.61 44.66 21.92 41.86
Ternary
Baseline (TWN) (Li et al., 2016)
2-2-8
39.6
QuantBart (Tao et al., 2022)
2-2-8
39.6
DQ-BART (Li et al., 2022)
2-2-8
39.6
TBT
2-2-8
39.6
Baseline (TWN) (Li et al., 2016)
2-2-2
TernaryBert* (Zhang et al., 2020)
2-2-2
TBT
2-2-2
39.6
39.6
39.6
0.25x
0.25x
0.25x 40.06 17.34 32.46 42.94 20.07 40.13
0.25x 42.40 19.54 34.51 43.46 20.52 40.58
0.0625× 12.80 1.21 11.4 12.92 0.32 12.42
0.0625× 14.03 2.23 11.79 10.95 0.52 8.56
0.0625× 36.21 14.38 29.07 41.03 18.18 38.30
39.99 17.13 31.99 42.99 20.05 40.18
39.15 16.72 31.72
Binary
Baseline (BWN) (Courbariaux et al., 2016)
1-1-8
BinaryBert* (Bai et al., 2021b)
1-1-8
BlockPruning (Lagunas et al., 2021)
TBT
1-1-8
Baseline (BWN) (Courbariaux et al., 2016)
BinaryBert* (Bai et al., 2021b)
1-1-1
1-1-1
BiBert* (Qin et al., 2021)
TBT
1-1-1
1-1-1
23.2
23.2
23
23.2
23.2
23.2
23.2
23.2
0.125x 1.90 0.01 1.78 2.78 0.08 2.48
0.125× 39.76 17.05 31.99 40.66 18.52 28.36
41.4 18.7 38.4
0.125 40.96 18.37 33.30 42.66 19.72 39.80
0.0156× 1.90 0.01 1.78 2.78 0.08 2.48
0.0156× 8.13 0.12 7.69 9.80 0.15 8.62
0.0156× 7.58 0.06 7.54 14.22 0.13 10.06
0.0156× 31.68 11.19 25.29 35.56 11.71 33.23
CNN/DailyMail (Nallapati et al., 2016) is an-
other news summarization benchmark, with longer
documents (~30 sentences) and longer, multi-
sentence summaries. The dataset contains close
to 300k document-summary pairs.
We use BART-base model (Lewis et al., 2019),
which is an English-only encoder-decoder trans-
former with 140 million parameters. We compare
using the standard ROUGE-{1,2,1} metrics for this
task.
For the ternary weights and 8-bit activations set-
ting, we compare with two state-of-the-art methods
QuantBart (Tao et al., 2022) and DQ-BART (Li
et al., 2022). For the fully ternary setting, and the
binary quantization experiments, there is no prior
art. Therefore we provide a naive quantization base-
line, using popular implementations from previous
work (Li et al., 2016; Courbariaux et al., 2016),
and adapt the binary and ternary methods proposed
for the BERT models (Bai et al., 2021b; Qin et al.,
2021; Zhang et al., 2020) to BART.
Our main results are summarized in Table 1. In
the ternary weights and 8-bit activations setting,
TBT improves previous SoTA by up to 2.3 points
in ROUGE score on XSUM, and up to 0.5 points
on CNN/DailyMail. Both improvements are signif-
icant.
Further quantizing weights to binary, while keep-
ing activations at 8-bit, we are still able to achieve
a ROUGE-L score of 33.3 on XSUM, which is 0.8
points higher than the previous ternary SOTA (DQ-
BART), and comparable on CNN/DailyMail. This
is the first demonstration of a binary-weight gen-
erative transformer model of competitive accuracy
to our knowledge. Additionally, TBT binary weight
BART model achieves 1.2 points higher ROUGE
score on CNN compared with the SOTA pruning
method with the same compressed model size.
Moving on to ternary and binary activations,
there is no prior art, and previous implementations
fail to produce meaningful results. Our method,
on the other hand, achieves ROUGE-L scores of
29.1 and 38.3 on XSUM and CNN/DailyMail in
the fully ternary setting, which are 6.6 and 3.8
points behind the full-precision baseline respec-
tively. Our fully binary (weights and activations)
model has a wider gap at 10.4 and 8.9 points, how-
ever still manages to produce highly non-trivial
output at ROUGE-L scores of 25.3 and 33.2 points
for XSUM and CNN/DailyMail.
3.3 Machine translation
We also evaluate our model on machine transla-
tion. We adopt the En-Ro benchmark from the
32-32-32
2.44
26.82
8-8-8
0.61 25.91
Table 2: Comparison of quantization methods on
mBART-large model for translation on WMT16 En-Ro.
Method
mBART (Liu et al., 2020a)
DQ-BART (Li et al., 2022)
#Bits (E-W-A) Size (GB) BLEU
Table 4: Generated average sequence length comparison
between baseline method and our method.
CNN/DailyMail
Method
#Bits (E-W-A)
XSUM
BART-base
32-32-32
30.73
99.89
Baseline
2-2-8
28.53
93.63
DQ-BART (Li et al., 2022) 2-2-8
0.31 23.48
TBT
2-2-8
32.04
95.78
TBT
2-2-8
0.31 24.63
Baseline
2-2-2
48.41
14.88
TBT
2-2-2
0.31 21.70
TBT
2-2-2
30.71
88.38
TBT
1-1-8
TBT
1 - 1 - 1
0.16 24.30
17.59
0.16
Baseline
1-1-8
62.0
128.0
TBT
1-1-8
31.57
97.08
Baseline
TBT
1-1-1
62.0
128.0
1 - 1 - 1
29.81
67.51
Table 3: Ablation study on the effects of the pro-
posed learning-based activation quantization method
and stats-based weight quantization method on XSUM
and CNN/DailyMail benchmark.
Method
1 Baseline (TWN)
2
+ Activation(learning-based)
4
+ Both
3 Weight(stats-based)
5 Baseline (BWN)
6 + Activation(learning-based)
7 +Weight(stats-based)
8
+ Both
9 Baseline (TWN)
XSUM
#Bits (E-W-A) R1 R2 RL
2-2-2
12.80 1.21 11.4
2-2-2 15.05 1.38 12.13
2-2-2 13.79 0.87 12.74
2-2-2 36.21 14.38 29.07
1-1-1
1 - 1 - 1
1-1-1
1-1-1
1.90 0.01 1.78
1.90 0.01 1.78
10.00
10.96 0.29
31.68 11.19 25.29
CNN/DailyMail
R1 R2 RL
2-2-2 12.92 0.32 12.42
10+ Activation(learning-based) 2-2-2 13.34 0.99 12.58
11+ Weight(stats-based)
12 + Both
2-2-2 19.34 0.42 18.42
2-2-2 41.03 18.18 38.30
13 Baseline (BWN)
1-1-1 2.78 0.08 2.48
14+ Activation(learning-based) 1-1-1 2.78 0.08 2.48
15+ Weight(stats-based)
1-1-1 15.05 0.35 14.01
16+ Both
1-1-1 35.56 11.71 33.23
WMT'16 shared task (Bojar et al., 2016b) to be
compatible with previous work. Our base model is
an mBART-large model (Liu et al., 2020a), a 680
million parameter multi-lingual encoder-decoder
transformer pre-trained on 25 languages.
Table 2 shows our results. In the ternary weight
setting with 8-bit activations, we improve the pre-
vious SoTA by 1.2 points, achieving 24.63 BLEU.
Remarkably our binary weight model also outper-
forms the previous ternary weight SOTA by al-
most a full point. It scores 24.3 BLEU – only
1.5 points behind a full mBART model while being
16× smaller.
-
In the fully ternary and binary settings, where
previous methods failed to converge, TBT models
are able to reach practical levels of performance,
with ternary TBT mBART achieving 21.7 BLEU,
and TBT binary mBART at 17.59.
3.4 Ablations
As stated earlier, our main proposed modeling
improvement is a combination of two methods:
statistics-based quantization for the weights, and
learning-based quantization for the activations.
We ablate the contribution of these methods and
present the results in Table 3.
The results clearly show that while each method
can give moderate gains by itself over the base-
line, these improvements are not sufficient by them-
selves to produce meaningful results. None of the
ablated models can achieve an R2 score above 1.5.
It's only the combination of the two, which to-
gether stabilize the training and result in good con-
vergence for fully ternary and binary models.
3.5 Sequence length analysis
In language generation tasks, the error compound-
ing issue in the recursive decoder generation pro-
cess will largely amplify the quantization error
or even lead to divergent results, and thus is an
harsh factor to test the robustness of a quantization
method. The average generated sequence length in-
dicates whether the quantized model can overcome
the compounding error and generate reasonable
length of text.
In Table 4 we compare the generated sequence
length between the proposed method and the base-
line method (i.e., TWN (Li et al., 2016) for ternary,
BWN (Courbariaux et al., 2016) for binary). Our
method successfully produces summarizations with
comparable length as the full-precision model on
XSUM benchmark, even when both weights and
activations are binarized.
Compared to XSUM dataset, for which the
document are summarized to only one sentence,
CNN/DailyMail is more challenging because it al-
lows longer summary. We can clearly see that,
the text generate with our 8-bit activation models
can maintain near the similar average length as the
full-precision BART model, while the binary and
ternary activation models deviate moderately. In
contrast, the baseline method is only able to derive
Ternary weights
(WT)
Weights
Activations
Real-valued activations
WT in 1st row
Ternary activations
Baseline (TWN)
(XT)
-0.1
0.0
0.1
-0.05
0.00
0.05
(b)
-0.025
0.000
0.025
0.00
0.05
(c)
Ours (TBT)
(XR)
0.10 0.15
(d)
-0.1 0.0 0.1
-0.1
0.0
0.1
0.00
0.01
0.00 0.05 0.10 0.15
(e)
(f)
(g)
(h)
Figure 2: Weight and activation histogram comparison between the baseline TWN method and TBT method for
ternarizing BART model on CNN/DailyMail benchmark. The weights are taken from the fully-connected layer of
the value matrix in 1st self-attention block in the decoder and activations are the attention outputs of the same layer.
reasonable summarization with 2-bit weight 8-bit
activations and fails at lower bit-width, showing the
difficult natural of the language generation tasks.
3.6 Visualization
To further understand the effectiveness of the pro-
posed method, we visualize weight and activation
histograms in the BART model ternarized with the
baseline method and the proposed method in Fig. 2.
Both the baseline method and our method use
per-row weight ternarization, and thus a tensor ten-
sor will have #row of scaling factors. As we can
see in Fig. 2 (b) and (g), the proposed method al-
lows the weights to be more evenly distributed in
three ternarization levels, which can allow higher
information entropy in quantized weights, as dis-
cussed in Sec. 2.2. Additionally, we calculate the
quantized weight distribution entropy (i.e., Eq. 7) in
96 fully-connected layers in the BART-base model
and found that the proposed TBT method achieves
consistently higher entropy in quantized weights
than the baseline method in all the layers. Further,
an interesting phenomenon we can see in Fig. 2 (a)
(e) is that ternary weights in a baseline model are
very close to the Gaussian distribution, in contrast,
weights ternarized with TBT are capturing a more
sophisticated distribution. This phenomenon im-
plies that the proposed method helps the weights
learn more informative patterns and thus better sat-
isfy the high demand for language generation tasks.
For activation quantization, it is evident that the
attention layer and the SoftMax output only con-
tain the positive activations (✗R = R+). If simply
ternarized to {-a, 0, a}, the ternary activations
will waste one representative level (Fig. 2(d)) and
therefore lead to lower accuracy. Instead, the pro-
posed method uses a two-set ternarization method
that ternarizes the non-negative activation layer
(XR Є R+) to {0, a, 2a), and learns the scaling
factor a to better fit the underlying real-valued dis-
tribution. This ternarization method greatly reduces
information loss and enhances the final accuracy.
4 Related Work
Quantization has long been studied to make neu-
ral networks more efficient (see (Hubara et al.,
2017) for a survey). Due to the popularity of
BERT, numerous works have studied quantization
for transformer models, starting with 8-bit quan-
tization (Zafrir et al., 2019; Fan et al., 2020), and
progressing to 4-bit (Shen et al., 2020; Zadeh et al.,
2020), ternary (Zhang et al., 2020) and binary Bai
et al. (2021b); Qin et al. (2021); Liu et al. (2022).
All of these works have focused on the encoder-
only setting.
In the generative setting, Prato et al. (2019);
Behnke et al. (2021) demonstrate quantized mod-
els for machine translation, and Fan et al. (2020);
Bai et al. (2021a) for language modeling, though
only for moderate quantization levels (4-8 bits).
Most recently, Tao et al. (2022) and Li et al. (2022)
pushed weight quantization down to 2 bits (with
8-bit activation quantization) and evaluated on lan-
guage modeling and summarization. However,
our method outperforms these works substantially,
while also demonstrating accurate generative trans-
formers with both weights and activations quan-
tized to 2-bit and even 1-bit for the first time.
5 Conclusion
We have demonstrated high accuracy ternary and
binary natural language generation models based
on a pre-trained transformer encoder-decoder back-
bone. Quantizing both the weights and the activa-
tions of the network allow these models to run on
special-purpose hardware using binary and ternary
arithmetic, which doesn't require multiplication
modules. Therefore our results promise multiple
orders of magnitude gains in efficiency while run-
ning these models, and can drastically expand the
use cases of such models beyond just high end gpu
servers. We are especially excited about the im-
plications of our results for larger text generation
models such as GPT-3 (Brown et al., 2020). These
models have both demonstrated impressive capabil-
ities, while also presenting enormous scaling and
computational challenges. Low-bit quantization
is a promising approach to mitigate some of these
issues. Whether our approach will scale to these
models is an open problem and an exciting future
research direction.
6 Limitations
We conduct experiments on public datasets of fi-
nite sentence length, while generalizability to ex-
tremely long sequences or even streaming data has
not been verified. Furthermore, the generalizabil-
ity of the proposed quantization method to other
tasks, including computer vision or speech recogni-
tion, remains to be tested. In addition, binarization
and ternarization require bit-packing to have actual
memory savings and dedicated hardware support
for real-time acceleration, which is more of a hard-
ware implementation aspect and not studied in this
paper.
7 Ethics Statement
We affirm that we contribute to society, avoid harm,
and are honest and trustworthy. We respect previ-
ous work and appropriately cite the methods and
datasets we are using. All data we use is public and
no private data is involved. There is some poten-
tial risk if the translation technique is maliciously
used by a third party and thus we are committed to
maintaining the compression techniques we have
developed and the general summarization/machine
translation techniques used correctly without incur-
ring any form of discrimination.
References
Haoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King,
and Michael R Lyu. 2021a. Towards efficient post-
training quantization of pre-trained language models.
arXiv preprint arXiv:2109.15082.
Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin,
Xin Jiang, Qun Liu, Michael R Lyu, and Irwin King.
2021b. Binarybert: Pushing the limit of bert quanti-
zation. In ACL/IJCNLP (1).
Maximiliana Behnke, Nikolay Bogoychev, Alham Fikri
Aji, Kenneth Heafield, Graeme Nail, Qianqian
Zhu, Svetlana Tchistiakova, Jelmer Van der Linde,
Pinzhen Chen, Sidharth Kashyap, et al. 2021. Ef-
ficient machine translation with model pruning and
quantization. In Proceedings of the Sixth Conference
on Machine Translation, pages 775-780.
Yoshua Bengio, Nicholas Léonard, and Aaron Courville.
2013. Estimating or propagating gradients through
stochastic neurons for conditional computation.
arXiv preprint arXiv:1308.3432.
Ondřej Bojar, Rajen Chatterjee, Christian Federmann,
Yvette Graham, Barry Haddow, Matthias Huck, An-
tonio Jimeno Yepes, Philipp Koehn, Varvara Lo-
gacheva, Christof Monz, Matteo Negri, Aurélie
Névéol, Mariana Neves, Martin Popel, Matt Post,
Raphael Rubino, Carolina Scarton, Lucia Specia,
Marco Turchi, Karin Verspoor, and Marcos Zampieri.
2016a. Findings of the 2016 conference on machine
translation. In Proceedings of the First Conference
on Machine Translation: Volume 2, Shared Task Pa-
pers, pages 131–198, Berlin, Germany. Association
for Computational Linguistics.
Ondřej Bojar, Yvette Graham, Amir Kamran, and Miloš
Stanojević. 2016b. Results of the wmt16 metrics
shared task. In Proceedings of the First Conference
on Machine Translation: Volume 2, Shared Task Pa-
pers, pages 199–231.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877-1901.
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran
El-Yaniv, and Yoshua Bengio. 2016. Binarized neu-
ral networks: Training deep neural networks with
weights and activations constrained to+ 1 or-1. arXiv
preprint arXiv:1602.02830.
Steven K Esser, Jeffrey L McKinstry, Deepika Bablani,
Rathinakumar Appuswamy, and Dharmendra S
Modha. 2019. Learned step size quantization. In
International Conference on Learning Representa-
tions.
Angela Fan, Pierre Stock, Benjamin Graham, Edouard
Grave, Rémi Gribonval, Herve Jegou, and Armand
Joulin. 2020. Training with quantization noise
for extreme model compression. arXiv preprint
arXiv:2004.07320.
Xavier Glorot and Yoshua Bengio. 2010. Understanding
the difficulty of training deep feedforward neural net-
works. In Proceedings of the thirteenth international
conference on artificial intelligence and statistics,
pages 249–256. JMLR Workshop and Conference
Proceedings.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification.
In Proceedings of the IEEE international conference
on computer vision, pages 1026-1034.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran
El-Yaniv, and Yoshua Bengio. 2017. Quantized neu-
ral networks: Training neural networks with low
precision weights and activations. The Journal of
Machine Learning Research, 18(1):6869–6898.
François Lagunas, Ella Charlaix, Victor Sanh, and
Alexander M Rush. 2021. Block pruning for faster
transformers. arXiv preprint arXiv:2109.04838.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: De-
noising sequence-to-sequence pre-training for natural
language generation, translation, and comprehension.
arXiv preprint arXiv:1910.13461.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:
Denoising sequence-to-sequence pre-training for nat-
ural language generation, translation, and comprehen-
sion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, pages
7871-7880.
Fengfu Li, Bo Zhang, and Bin Liu. 2016. Ternary
weight networks. arXiv preprint arXiv:1605.04711.
Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati,
Parminder Bhatia, Andrew Arnold, Bing Xiang, and
Dan Roth. 2022. Dq-bart: Efficient sequence-to-
sequence model via joint distillation and quantization.
In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers), pages 203–211.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020a. Multilingual denoising
pre-training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726-742.
Zechun Liu, Wenhan Luo, Baoyuan Wu, Xin Yang,
Wei Liu, and Kwang-Ting Cheng. 2020b. Bi-real
net: Binarizing deep network towards real-network
performance. International Journal of Computer
Vision, 128(1):202-219.
Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao,
Scott Yih, Meng Li, Raghuraman Krishnamoorthi,
and Yashar Mehdad. 2022. Bit: Robustly bina-
rized multi-distilled transformer. arXiv preprint
arXiv:2205.13016.
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei
Liu, and Kwang-Ting Cheng. 2018. Bi-real net: En-
hancing the performance of 1-bit cnns with improved
representational capability and advanced training al-
gorithm. In Proceedings of the European conference
on computer vision (ECCV), pages 722-737.
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing
Xiang, et al. 2016. Abstractive text summarization
using sequence-to-sequence rnns and beyond. arXiv
preprint arXiv:1602.06023.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don't give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1797-1807, Brussels, Bel-
gium. Association for Computational Linguistics.
Gabriele Prato, Ella Charlaix, and Mehdi Reza-
gholizadeh. 2019. Fully quantized trans-
former for machine translation. arXiv preprint
arXiv:1910.10485.
Haotong Qin, Yifu Ding, Mingyuan Zhang, YAN
Qinghua, Aishan Liu, Qingqing Dang, Ziwei Liu, and
Xianglong Liu. 2021. Bibert: Accurate fully bina-
rized bert. In International Conference on Learning
Representations.
Alec Radford, Karthik Narasimhan, Tim Salimans, and
Ilya Sutskever. 2018. Improving language under-
standing by generative pre-training.
Mohammad Rastegari, Vicente Ordonez, Joseph Red-
mon, and Ali Farhadi. 2016. Xnor-net: Imagenet
classification using binary convolutional neural net-
works. In European conference on computer vision,
pages 525-542. Springer.
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei
Yao, Amir Gholami, Michael W Mahoney, and Kurt
Keutzer. 2020. Q-bert: Hessian based ultra low
precision quantization of bert. In Proceedings of
the AAAI Conference on Artificial Intelligence, vol-
ume 34, pages 8815-8821.
Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin
Jiang, Qun Liu, Ping Luo, and Ngai Wong. 2022.
Compression of generative pre-trained language mod-
els via quantization. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 4821-
4836.
Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad,
and Andreas Moshovos. 2020. Gobo: Quantiz-
ing attention-based nlp models for low latency and
energy efficient inference. In 2020 53rd Annual
IEEE/ACM International Symposium on Microarchi-
tecture (MICRO), pages 811–824. IEEE.
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8bert: Quantized 8bit bert. In
2019 Fifth Workshop on Energy Efficient Machine
Learning and Cognitive Computing-NeurIPS Edition
(EMC2-NIPS), pages 36–39. IEEE.
Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao
Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert:
Distillation-aware ultra-low bit BERT. In EMNLP.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou,
He Wen, and Yuheng Zou. 2016. Dorefa-net:
Training low bitwidth convolutional neural net-
works with low bitwidth gradients. arXiv preprint
arXiv:1606.06160.
