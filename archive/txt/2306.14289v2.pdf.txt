arXiv:2306.14289v2 [cs.CV] 1 Jul 2023
FASTER SEGMENT ANYTHING: TOWARDS LIGHTWEIGHT SAM
FOR MOBILE APPLICATIONS
Chaoning Zhang*
Kyung Hee University
Sung-Ho Bae
Kyung Hee University
A PREPRINT
Dongshen Han
Kyung Hee University
Yu Qiao
Kyung Hee University
Jung Uk Kim
Kyung Hee University
Seungkyu Lee
Kyung Hee University
Choong Seon Hong
Kyung Hee University
July 4, 2023
ABSTRACT
Segment Anything Model (SAM) has attracted significant attention due to its impressive zero-shot
transfer performance and high versatility for numerous vision applications (like image editing with
fine-grained control). Many of such applications need to be run on resource-constraint edge devices,
like mobile phones. In this work, we aim to make SAM mobile-friendly by replacing the heavyweight
image encoder with a lightweight one. A naive way to train such a new SAM as in the original SAM
paper leads to unsatisfactory performance, especially when limited training sources are available. We
find that this is mainly caused by the coupled optimization of the image encoder and mask decoder,
motivated by which we propose decoupled distillation. Concretely, we distill the knowledge from
the heavy image encoder (ViT-H in the original SAM) to a lightweight image encoder, which can be
automatically compatible with the mask decoder in the original SAM. The training can be completed
on a single GPU within less than one day, and the resulting lightweight SAM is termed MobileSAM
which is more than 60 times smaller yet performs on par with the original SAM. For inference speed,
With a single GPU, MobileSAM runs around 10ms per image: 8ms on the image encoder and 4ms
on the mask decoder. With superior performance, our MobileSAM is around 5 times faster than the
concurrent FastSAM and 7 times smaller, making it more suitable for mobile applications. Moreover,
we show that MobileSAM can run relatively smoothly on CPU. The code for our project is provided
at MobileSAM), with a demo showing that MobileSAM can run relatively smoothly on CPU.
1 Introduction
ChatGPT Zhang et al. [2023a] has revolutionized the NLP field, marking a breakthrough in generative AI (AIGC,
a.k.a Artificial intelligence generated content) Zhang et al. [2023b]. What has made this possible lies in GPT-series
models Brown et al. [2020], Radford et al. [2018, 2019], which are foundation models Bommasani et al. [2021] trained
on web-scale text datasets. Following the success of foundation models in NLP, multiple works He et al. [2020],
Qiao et al. [2023a], Zhang et al. [2022a] have learned an image encoder together with a text encoder via contrastive
learning He et al. [2020], Zhang et al. [2022b]. Very recently, Meta Research team has released the "Segment Anything"
project Kirillov et al. [2023], where a prompt-guided vision foundation termed SAM has been proposed and is believed
to be a GPT moment for vision. SAM consists of two components: ViT-based image encoder and prompt-guided mask
decoder, which work in sequence (see Figure 1).
Since its advent, SAM has attracted significant attention for multiple reasons. First, it is the first to show that vision
can follow NLP to pursue a path that combines foundation model with prompt engineering. Second, it is the first to
perform label-free segmentation, a fundamental vision task that is in parallel to label prediction Zhang et al. [2023c].
Moreover, this fundamental task makes SAM compatible with other models to realize advanced vision applications,
*You are welcome to contact the authors through chaoningzhang1990@gmail.com
image
image
ViT-based image encoder
image
encoder
(632M)
Heavyweight
embedding prompt-guided mask decoder
Prompt
mask decoder
(3.87M)
↑
prompt encoder
(0.006M)
Lightweight
Figure 1: The overview of Segment Anything Model.
like text-guided segmentation and image editing with fine-grained control. Many of such use cases, however, need
to be run on resource-constrained edge-devices, like mobile apps. As shown in the official demo, with a processed
image embedding, the SAM can work on resource-constrained devices because the mask decoder is lightweight. What
makes the SAM pipeline computation heavy lies in the huge image encoder. In this work, we investigate how to obtain
a lightweight SAM suitable for resource-constrained mobile devices, which is therefore termed MobileSAM.
Table 1: Parameters SAM with different image encoders.
Parameters
ViT-based encoder
prompt-guided encoder
SAM (VIT-H) SAM (VIT-L) SAM (VIT-B)
632M
0.006M
307M
0.006M
86M
0.006M
Given that the default image encoder
in the SAM is based on ViT-H, a
straightforward way to obtain Mobile-
SAM is to follow the official pipeline
in Kirillov et al. [2023] to retrain a
new SAM with a smaller image en-
coder like replacing the ViT-H with a
smaller ViT-L or even smaller ViT-B.
The parameters of SAM with different variants of image encoder are summarized in Table 1. As stated in Kirillov
et al. [2023], training a new SAM with ViT-L or ViT-B as the image encoder requires 128 GPUs for multiple days.
Such resource-intensive retraining can be a non-trivial burden to reproduce or improve their results. This optimization
difficulty mainly comes from the coupled optimization of the image encoder and mask decoder. Motivated by this
understanding, we propose to decouple the optimization of the image encoder and mask decoder. Concretely, we
first distill the knowledge from the default image encoder ViT-H to a tiny ViT. After that, we can finetune the mask
decoder in the original SAM to better align with the distilled image encoder. It is worth highlighting that the alignment
optimization is optional because the fact that the lightweight image encoder is distilled from the default image encoder
guarantees its inherent alignment with the default mask decoder.
By turning the problem of seeking a new SAM pipeline into a decoupled distillation, our approach has the advantage of
being simple, and effective, while being reproducible at a low cost (on a single GPU with less than a day). The resulting
MobileSAM reduces the encoder parameters by 100 times and total parameters by 60 times yet. Surprisingly, such a
lightweight MobileSAM performs on par with the original heavyweight SAMs, which constitutes a significant step for
pushing SAM for mobile applications. For the inference with MobileSAM, a single image takes runs only around 10ms:
8ms on the image encoder and 4ms on the mask decoder. It is worth highlighting that our MobileSAM is around 5 times
faster and 7 times smaller than the concurrent FastSAM Zhao et al. [2023], while achieving superior performance.
2 Related work
SAM: generalization and versatility. Since its advent in early April of this year, numerous projects and papers have
emerged to investigate SAM from different perspectives. Given that SAM claims to segment anything, a line of works
has reported its performance in real-world situations, including medical images Ma and Wang [2023], Zhang et al.
[2023d], camouflaged objects Tang et al. [2023], and transparent objects Han et al. [2023]. The findings consistently
show that SAM works well in general setups, but not in the above challenging setups. Another significant research
direction has focused on enhancing SAM to improve its practicality. Attack-SAM Zhang et al. [2023e] has shown that
the output masks of SAM can be easily manipulated by adversarial attacks through maliciously generated adversarial
perturbations. Another work Qiao et al. [2023b] further conducts a comprehensive robustness evaluation of SAM,
ranging from style transfer and common corruptions to local occlusion and adversarial perturbation. It is found in Qiao
et al. [2023b] SAM has high robustness but not for adversarial perturbation, which aligns well with the finding in Zhang
et al. [2023e]. Another line of work focuses on demonstrating the versatility of SAM. Grounded SAM IDEA-Research
2
[2023] is the pioneering work to combine Grounding DINO Liu et al. [2023a] with SAM for segmenting anything with
text inputs. Specifically, it relies on Grounding DINO to generate a bounding box from text and then the generated
box can be used as a prompt to segment the mask. SAM predicts masks without labels and multiple works Chen
et al. [2023], Park [2023] combine SAM with other models such as CLIP Radford et al. [2021] to semantically
segment anything. Beyond object segmentation, multiple works have also shown its versatility in other fields, including
image editing Rombach et al. [2022], as well as inpainting tasks Yu et al. [2023], object tracking within videos Yang
et al. [2023], Zxyang [2023]. Beyond 2D vision, the investigation of SAM has also been extended to 3D object
reconstruction Shen et al. [2023], Kang et al. [2022], demonstrating its capabilities in assisting 3D model generation
from a single image. For a complete survey of SAM, the readers are suggested to refer to Zhang et al. [2023c].
ViT: lightweight and efficient. Early mobile vision applications have been mainly powered by lightweight CNNs,
such as MobileNet Howard et al. [2017] and its improved varinats Sandler et al. [2018], Howard et al. [2019]. The core
idea of MobileNet lies in separating a normal convolution block into depth-wise convolution and point-wise convolution,
which significantly reduces the mode parameters and computation time. Since the advent of ViT Dosovitskiy et al.
[2021], numerous works have attempted to make it lightweight and efficient. Complementary to ViT-Huge (ViT-H),
ViT-Large (ViT-L), ViT-Base (ViT-B) in the original ViT paper Dosovitskiy et al. [2021], smaller ViTs are introduced
in Touvron et al. [2020] and are denoted as Deit-Small (Deit-S) and Deit-Tiny (Deit-T) ViT-Small and ViT-Tiny.
Mobile ViT Mehta and Rastegari [2021] is a pioneering work to combine ViT with standard convolutions for improving
its performance, which outperforms MobileNet v2 Sandler et al. [2018]. The main motivation is to exploit the local
representation capability of CNN, and this practice is followed by multiple follow-up works which aim to enhance the
model speed, including EfficientFormer Li et al. [2022a], Efficient ViT Liu et al. [2023b], Next-ViT Li et al. [2022b]
and Tiny-ViT Wu et al. [2022]. The recent progress in lightweight and faster ViT is complementary to our proposed
decoupled distillation towards making the next-generation SAM suitable for resource-constrained mobile devices.
3 Mobile-Friendly SAM
3.1 Background and Project Goal
generates an
Background on SAM. Here, we first summarize the structure of SAM and how it works. SAM consists of a ViT-based
image encoder and a prompt-guided mask decoder. The image encoder takes the image as the input and
embedding, which is then fed to the mask decoder. The mask decoder generates a mask to cut out any object from the
background based on a prompt like a point (or box). Moreover, SAM allows generating multiple masks for the same
prompt for addressing the ambiguity issue, which provides valuable flexibility. Considering this, this work maintains
the pipeline of SAM to first adopt a ViT-based encoder to generate image embedding and then to adopt a prompt-guided
decoder to generate the desired mask. This pipeline is optimally designed for the “segment anything", which can be
used for the downstream task of “segment everything" (see Sec. 4.3 for more discussion).
Project goal. The goal of this project is to generate a mobile-friendly SAM (MobileSAM) that achieves satisfactory
performance in a lightweight manner and is much faster than the original SAM. The prompt-guided mask decoder
in the original SAM has less than 4M parameters and is thus considered lightweight. Given an image embedding
processed by the encoder, as shown in their public demo, SAM can work in resource-constrained devices since the
mask decoder is lightweight. However, the default image encoder in the original SAM is based on ViT-H with more
than 600M parameters, which is very heavyweight and makes the whole SAM pipeline incompatible with mobile
devices. Therefore, the key to obtaining a mobile-friendly SAM lies in replacing the heavyweight image encoder with
a lightweight one, which also automatically keeps all its functions and characteristics of the original SAM. In the
following, we elaborate on our proposed method for achieving this project goal.
3.2 Proposed Method
Coupled distillation. A straightforward way to realize our project goal is to follow the official pipeline in Kirillov
et al. [2023] to retrain a new SAM with a smaller image encoder. As stated in Kirillov et al. [2023], training a SAM with
ViT-H image encoder requires takes 68 hours on 256 A100 GPUs. Replacing the ViT-H with ViT-L or ViT-B reduces
the required GPUs to 128, which is still a non-trivial burden for many researchers in the community to reproduce or
improve their results. Following their approach, we can further adopt an even smaller image encoder and retrain a new
SAM with their provided segmentation dataset which is 11-T. Note that the masks in the provided dataset are given by
the pretrained SAM (with the ViT image encoder). In essence, this retraining process is knowledge distillation Hinton
et al. [2015], which transfers the knowledge from a ViT-H-based SAM to a SAM with a smaller image encoder (see
Figure 2 left).
3
image
VIT-based (large)
image encoder
Teacher SAM
prompt-guided
mask
mask decoder
distillation
image
VIT-based (large)
image encoder
Teacher SAM
prompt-guided
mask decoder
mask
copy
distillation
VIT-based (small)
image encoder
prompt-guided
mask decoder
mask
VIT-based (small)
image encoder
prompt-guided
mask decoder
mask
trainable
trainable
trainable
frozen
Figure 2: Coupled knowledge distillation of SAM. The left subfigure denotes the fully-coupled distillation, while the
right one represents the semi-coupled distillation.
From semi-coupled to decoupled distillation. When performing a KD from the original SAM to that with a smaller
image encoder, the difficulty mainly lies in a coupled optimization of the image encoder and combined decoder.
Intuitively, the optimization of the image encoder depends on the quality of the image decoder, and vice versa. When the
two modules in the SAM are both in a bad state, it is more challenging to train them both to a good state. Inspired by the
divide-and-conquer algorithm Zhang et al. [2022c], we propose to divide the KD task into two sub-tasks: image encoder
distillation and mask decoder finetuning. Concretely, we first perform the KD on the image encoder by transferring the
knowledge from ViT-H to a smaller encoder. Since the mask decoder in the original SAM is already lightweight, we
plan to keep its architecture. This brings a benefit of a readily used combined decoder for finetuning instead of training
it from scratch. To alleviate the optimization issue of coupled distillation, a straightforward way is to optimize the
image encoder with a copied and frozen mask decoder (see Figure 2 right). The freezing operation can help prevent the
quality of the mask decoder from being deteriorated by a poor image encoder. We call this distillation semi-coupled
because the optimization of the image encoder is still not fully decoupled from the mask decoder. Empirically, we find
that this optimization is still challenging because the choice of a prompt is random, which makes the mask decoder
variable and thus increases the optimization difficulty. Therefore, we propose to distill the small image encoder directly
from the ViT-H in the original SAM without resorting to the combined decoder, which is termed decoupled distillation
(see Figure 3). Another advantage of performing distillation on the image embedding is that we can adopt a simple
MSE loss instead of using a combination of focal loss Lin et al. [2017] and dice loss Milletari et al. [2016] for making
the mask prediction as in Kirillov et al. [2023].
Finetuning (optional)
VIT-based (large)
image encoder
image
embedding
prompt-guided
mask decoder
mask
distillation
VIT-based (small)
image encoder
image
embedding
image
Figure 3: Decoupled distillation for SAM.
copy
prompt-guided
mask decoder
mask
On the necessity of mask decoder finetuning. Unlike the semi-coupled distillation, the above decoupled distillation
yields a lightweight image encoder that might not align well with the original frozen mask decoder. Empirically, we
find that this is not true because the generated image encoding from the student image encoder can be sufficiently close
to that of the original teacher encoder, which renders finetuning on the combined decoder in the second stage optional.
4
It is expected that finetuning the mask decoder on the frozen lightweight image encoder or jointly finetuning them
together might further improve the performance.
Preliminary evaluation. Here, we conduct a preliminary investigation to compare coupled distillation and decoupled
distillation. Here, for performance evaluation, we compute the mIoU between the two masks generated by the teacher
SAM and student SAM on the same prompt point. Intuitively, a higher mIoU indicates a higher mask prediction
performance by assuming that the mask generated by ViT-H is ground-truth. For the coupled distillation, we adopt the
SAM with ViT-B provided in the original SAM Kirillov et al. [2023]. It was trained on SA-1B (11M images) on 128
GPUs (1 sample per GPU) for 180k iterations. By contrast, in our decoupled distillation setup, we train the model on 2
GPUs (two samples per GPU to save computation resources) on 0.1% samples of SA-1B dataset (11k) images for 55k
iterations. Overall, decoupled distillation takes less than 1% of the computation resources than coupled distillation,
while achieving a superior performance of mIoU of 0.75 vs 0.72 for the coupled sit (averaged on 200 samples). Since
ViT-B is still a non-trivial burden for mobile devices, therefore in the following we experiment with a Tiny ViT (with
5M parameters) Wu et al. [2022] based on our proposed decoupled distillation.
Table 2: Comparison of coupled distillation and decoupled distillation fro SAM with ViT-B as the image encoder.
Decoupled distillation performs better and requires less than 1% computation resources than coupled distillation.
SAM (coupled distillation) SAM (decoupled distillation)
MIOU
Training GPUs
Batch size
Iterations
Training Data
0.72
0.75
128
2
128
4
180k
55k
11M
11K
4 Experiments
4.1 Experimental Setup
Table 3: Comparison of the parameters and speed for the image encoder
in original SAM and MobileSAM. The inference speed is measured on a
single GPU.
Original SAM
MobileSAM
Parameters
Speed
5.78M
8ms
632M
452ms
Lightweight Image Encoder. The goal
of our project is to obtain an efficient
SAM by replacing the default ViT-H with
a lightweight image encoder for mobile de-
vices. As a ViT-based backbone, ViT-Tiny
has similar parameters as Deit-Tiny but per-
forms better. For example, on ImageNet-1K,
Deit-Yiny achieves an accuracy of 72.2%,
while ViT-Tiny achieves 79.1%. Therefore,
we adopt ViT-Tiny for the proof of concept
to demonstrate the effectiveness of our proposed decoupled distillation for training a lightweight MobileSAM that can
be much faster than the original SAM. The adopted lightweight image encoder consists of four stages which gradually
reduce the resolution. The first stage is constructed by convolution blocks with inverted residuals Sandler et al. [2018],
while the remaining three stages consist of transformer blocks. At the beginning of the model, there are 2 convolutions
blocks with a stride of 2 for downsampling the resolution. The downsampling operation between different stages is
processed by convolution blocks with the stride of 2. Different from Wu et al. [2022], we set the stride of 2 in the last
downsampling convolution to 1 for making the final resolution match that of the ViT-H image encoder of the original
SAM. The parameters inference speed of MobileSAM are summarized in Table 3. Note that other efficient image
encoders discussed in Section 2 can also be adopted as the image encoder.
Training and evaluation details. For the decoupled KD on the image encoder, we train the lightweight encoder with
1% of the SA-1B dataset Kirillov et al. [2023] for 8 epochs on a single GPU. We observe that more computation is spent
on the forward process on the teacher image encoder considering that it is significantly more heavy than our adopted
student image encoder (see above). To make the distillation faster, we follow the practice in Wu et al. [2022] to save the
image embeddings beforehand so that we only need to run the forward process once. With a single GPU, we can obtain
our MobileSAM in less than a day. Training our MobileSAM with more GPUs for a longer time is expected to yield
better performance. The initial investigation of performing mask decoder finetuning further improves the performance
5
of MobileSAM, however, we omit this step in this version of our paper for simplicity. For quantitative evaluation of the
distilled SAM, we compute the mIoU between the masks predicted by the original SAM and our MobileSAM.
(a) Original SAM
(b) MobileSAM
THE
THE
STAG
STAGE
(c) Original SAM
Figure 4: Mask prediction with a single point as the prompt.
4.2 MobileSAM performs on par with the orignal SAM
JATNOUE
(d) MobileSAM
For the main results, we report the predicted masks with two types of prompts: point and box. We do not report the
results with text prompt because the official github project of SAM does not provide pretrained models for text-guided
mask decoder. The results with point as the prompt are shown in Figure 4, and those with box as the prompt are shown
in Figure 5. We observe that MobileSAM makes a satisfactory mask prediction similar to that of the original SAM.
(a) Original SAM
(b) MobileSAM
(c) Original SAM
(d) MobileSAM
Figure 5: Mask prediction with a box as the prompt.
Ablation study. Here, we conduct an ablation study on the influence
of the training computation on the performance of SAM. The results
in Table 4 show that, under the same number of iterations, increasing
the batch size increases the model performance. Moreover, under the
batch size, the performance also benefits from more update iterations
by increasing the training epochs. Note that all the experiments are
conducted on a single GPU. We expect that increasing the number of
GPUs for allowing a larger batch size or further increasing the iterations
can further improve the performance.
4.3 MobileSAM outperforms FastSAM
Table 4: Ablation study on the influence of
training computation on the MobileSAM
performance.
batch size epochs
4
Iterations mIoU
2
50k
0.7057
8
4
50k
0.7286
8
8
100k
0.7447
Table 5: Comparison between segment any-
thing and segment everything.
# of objects
prompt-aware
anything everything
1
N
yes
no
Segment anything v.s. segment everything. Note that the title of the
original SAM paper Kirillov et al. [2023] is “segment anything" instead
of "segment everything". As highlighted in Kirillov et al. [2023], SAM
performs the task of promptable segmentation which “returns a valid
segmentation mask given any segmentation prompt" (quote from Kirillov
et al. [2023]). The role of the prompt is to specify what to segment in the
image. In theory, any object can be segmented as long as the prompt is set
properly, therefore, it is called “segment anything". By contrast, “segment
everything" is in essence object proposal generation Kirillov et al. [2023],
for which the prompt is not necessary. In Kirillov et al. [2023], “segment everything" (object proposal generation) is
chosen as one of the downstream tasks for demonstrating its zero-shot transfer performance. To summarize, “segment
anything" solves the foundation task of promptable segmentation for any object, while “segment everything" solves the
downstream task of mask proposal generation for all objects. Since “segment everything" does not necessarily require a
prompt, FastSAM directly generates the mask proposal with YOLO v8 in a prompt-free manner. To enable promptable
segmentation, a mapping algorithm is designed to select the mask from the proposal mask sets. It is worth highlighting
that the follow-up works that evaluate its generalization/robustness or investigate its versatility mainly focus on the
anything instead of everything mode because the former addresses the foundation task. Therefore, the comparison with
FastSAM mainly focuses on “segment anything", but we also provide a comparison regarding “segment everything" for
completeness.
MobileSAM is faster and smaller. FastSAM consists of a
YOLOV8-based detection branch and a YOLACT-based segmenta-
tion branch to perform a prompt-free mask proposal generation. It
has 68M parameters and takes 40ms to process an image. By con-
trast, MobileSAM has less 10M parameters, which is significantly
smaller. For the inference speed, on a single GPU, it takes 40ms
to process an image while ours only takes 10ms, which is 4 times
faster than FastSAM (see Table 6).
Table 6: Comparison between FastSAM and Mo-
bileSAM.
Size
Speed
FastSAM MobileSAM
68M
9.66M
64ms
12ms
Ratio
≈ 7
≈5
mIoU comparison under segment anything mode. We further
compare the moU between the predicted masks with that of the
original SAM. FastSAM is suggested to predict the mask with
multiple points, for which we choose one for the foreground and
the other for the background. The results in Table 7 show the mIoU
for FastSAM is much smaller than that for MobileSAM, suggesting
that the mask prediction of FastSAM is very different from that of
the original SAM. Moreover, the mIoU for the FastSAM decreases
very fast when the distance between the two prompt points. This
is mainly caused by the fact that FastSAM often fails to predict
the object when the foreground prompt point is set too close to the background prompt point.
Table 7: mIoU comparison. With the assumption
that the predicted mask from the original SAM
is ground-truth, a higher mIoU indicates a better
performance.
100 200 300 400 500
FastSAM 0.27 0.33 0.37 0.41 0.41
MobileSAM 0.73 0.71 0.74 0.73 0.73
Results for segment everything. The results for “segment everything" are shown in Figure 6. For completeness, we
also report the results of the original SAM, which generates a pleasing object proposal. We have two major observations.
First, the results of our MobileSAM align surprisingly well with that of the original SAM. By contrast, the results of
FastSAM are often less satisfactory. For example, FastSAM often fails to predict some objects, like the roof in the first
image. Moreover, the mask proposal is sometimes difficult to interpret (see the mask for the stage in the first image and
that for the sky in the second image). Second, FastSAM often generates masks that have non-smooth boundaries, for
7
(a) Image
(b) MobileSAM
(c) Original SAM
Figure 6: Comparison of segment everything results.
(d) FastSAM
which we suggest the reader zoom in to check the details in Figure 6. For example, the pillars in the third image have
non-smooth boundaries, while the original SAM and our MobileSAM do not have this issue.
5 Conclusion
In this work, we aim to make SAM mobile-friendly by replacing the heavyweight image encoder with a lightweight one.
We find that the naive way to train such a new SAM as in the original SAM paper leads to unsatisfactory performance,
especially under a setup of limited training sources. The coupled optimization of the image encoder and mask decoder
is the reason, and thus we propose decoupled distillation, whhere the knowledge is distilled from the image encoder
ViT-H in the original SAM to a lightweight image encoder. We show that the resulting lightweight image encoder
can be automatically compatible with the mask decoder in the original SAM. Our MobileSAM is more than 60 times
smaller yet performs on par with the original SAM. Moreover, we conduct a comparison with the concurrent FastSAM
and show that MobileSAM achieve superior performance. Our MobileSAM is also 4 times faster and 7 times smaller
than the concurrent FastSAM, making it more suitable for mobile applications. Since our MobileSAM keeps all the
pipeline of the original SAM and just replaces the image encoder, it can be plug-and-play for the existing SAM-based
projects to move from a heavyweight SAM to a lightweight one with almost zero effort.
References
Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang,
Jung Uk Kim, Seong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for agi: A complete
survey on chatgpt in aige era. arXiv preprint arXiv:2304.06488, 2023a.
Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam,
Chu Myaet Thwal, Ye Lin Tun, Le Luang Huy, et al. A complete survey on generative ai (aigc): Is chatgpt from
gpt-4 to gpt-5 all you need? arXiv preprint arXiv:2303.11717, 2023b.
8
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems, 2020.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by
generative pre-training. 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog, 2019.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv
preprint arXiv:2108.07258, 2021.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual
representation learning. In CVPR, 2020.
Yu Qiao, Md Munir, Apurba Adhikary, Huy Q Le, Avi Deb Raha, Chaoning Zhang, Choong Seon Hong, et al. Mp-fedcl:
Multi-prototype federated contrastive learning for edge intelligence. arXiv preprint arXiv:2304.01950, 2023a.
Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X Pham, Chang D Yoo, and In So Kweon. How does
simsiam avoid collapse without negative samples? a unified understanding with self-supervised contrastive learning.
In ICLR, 2022a.
Chaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. Dual temperature helps contrastive
learning without many negative samples: Towards understanding and simplifying moco. In CVPR, 2022b.
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer
Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.
Chaoning Zhang, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin,
Francois Rameau, Sung-Ho Bae, et al. A survey on segment anything model (sam): Vision foundation model meets
prompt engineering. 2023c.
Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment
anything. arXiv preprint arXiv:2306.12156, 2023.
Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023.
Yizhe Zhang, Tao Zhou, Peixian Liang, and Danny Z Chen. Input augmentation with sam: Boosting medical image
segmentation with segmentation foundation model. arXiv preprint arXiv:2304.11332, 2023d.
Lv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouflaged object detection. arXiv
preprint arXiv:2304.04709, 2023.
Dongsheng Han, Chaoning Zhang, Yu Qiao, Maryam Qamar, Yuna Jung, SeungKyu Lee, Sung-Ho Bae, and
Choong Seon Hong. Segment anything model (sam) meets glass: Mirror and transparent objects cannot be easily
detected. arXiv preprint, 2023.
Chenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, and In So Kweon. Attack-sam:
Towards evaluating adversarial robustness of segment anything model. arXiv preprint, 2023e.
Yu Qiao, Chaoning Zhang, Taegoo Kang, Donghun Kim, Shehbaz Tariq, Chenshuang Zhang, and Choong Seon Hong.
Robustness of sam: Segment anything under corruptions and beyond. arXiv preprint arXiv:2306.07713, 2023b.
IDEA-Research. Grounded segment anything, 2023. URL https://github.com/IDEA-Research/
Grounded-Segment-Anything. GitHub repository.
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499, 2023a.
Jiaqi Chen, Zeyu Yang, and Li Zhang. Semantic-segment-anything, 2023. URL https://github.com/
fudan-zvg/Semantic-Segment-Anything. GitHub repository.
Curt Park. segment anything with clip, 2023. URL https://github.com/Curt-Park/
segment-anything-with-clip. GitHub repository.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In
ICML, 2021.
9
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10684-10695, 2022.
Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment
anything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023.
Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything
meets videos. arXiv preprint arXiv:2304.11968, 2023.
Zxyang.
Segment and track anything, 2023.
Segment-and-Track-Anything. GitHub repository.
URL https://github.com/z-x-yang/
Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-3d: Towards single-view anything reconstruction in the
wild. arXiv preprint arXiv:2304.10261, 2023.
Minki Kang, Dongchan Min, and Sung Ju Hwang. Any-speaker adaptive text-to-speech synthesis with diffusion models.
arXiv preprint arXiv:2211.09383, 2022.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto,
and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861, 2017.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In CVPR, 2018.
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international
conference on computer vision, pages 1314-1324, 2019.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image
is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training
data-efficient image transformers & distillation through attention. arXiv preprint arXiv:2012.12877, 2020.
Sachin Mehta and Mohammad Rastegari. Mobilevit: light-weight, general-purpose, and mobile-friendly vision
transformer. arXiv preprint arXiv:2110.02178, 2021.
Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren.
Efficientformer: Vision transformers at mobilenet speed. Advances in Neural Information Processing Systems, 35:
12934-12949, 2022a.
Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan. Efficientvit: Memory efficient
vision transformer with cascaded group attention. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 14420–14430, 2023b.
Jiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xuefeng Xiao, Rui Wang, Min Zheng, and Xin Pan. Next-
vit: Next generation vision transformer for efficient deployment in realistic industrial scenarios. arXiv preprint
arXiv:2207.05501, 2022b.
Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fast pretraining
distillation for small vision transformers. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv,
Israel, October 23-27, 2022, Proceedings, Part XXI, pages 68–85. Springer, 2022.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.
Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon. Decoupled
adversarial contrastive learning for self-supervised adversarial robustness. In ECCV, pages 725–742. Springer, 2022c.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In
Proceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017.
Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 fourth international conference on 3D vision (3DV), pages 565–571. Ieee,
2016.
10
10
