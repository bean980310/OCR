arXiv:2307.06949v2 [cs.CV] 16 Oct 2024
Nataniel Ruiz
HyperDreamBooth: HyperNetworks for
Fast Personalization of Text-to-Image Models
Yael Pritch
Yuanzhen Li Varun Jampani
Wei Wei
Neal Wadhwa
Michael Rubinstein
Google Research
Tingbo Hou
Kfir Aberman
2 [sec] HyperNetwork
graffiti
22 [sec]
Fine-
tuning
Text-to-Image
"A [V] face as a..."
bark
skin
detailed
render
rock
star
psychedelic
οίι
painting
Figure 1. Using only a single input image, HyperDreamBooth is able to personalize a text-to-image diffusion model 25x faster than
DreamBooth [25], by using (1) a HyperNetwork to generate an initial prediction of a subset of network weights that are then (2) refined
using fast finetuning for high fidelity to subject detail. Our method both conserves model integrity and style diversity while closely
approximating the subject's essence and details.
Abstract
Personalization has emerged as a prominent aspect
within the field of generative AI, enabling the synthesis of
individuals in diverse contexts and styles, while retaining
high-fidelity to their identities. However, the process of per-
sonalization presents inherent challenges in terms of time
and memory requirements. Fine-tuning each personalized
model needs considerable GPU time investment, and stor-
ing a personalized model per subject can be demanding in
terms of storage capacity. To overcome these challenges, we
propose HyperDreamBooth-a hypernetwork capable of ef-
ficiently generating a small set of personalized weights from
a single image of a person. By composing these weights
into the diffusion model, coupled with fast finetuning, Hy-
perDreamBooth can generate a person's face in various
contexts and styles, with high subject details while also pre-
serving the model's crucial knowledge of diverse styles and
semantic modifications. Our method achieves personaliza-
tion on faces in roughly 20 seconds, 25x faster than Dream-
Booth and 125x faster than Textual Inversion, using as few
as one reference image, with the same quality and style di-
versity as DreamBooth. Also our method yields a model
that is 10,000x smaller than a normal DreamBooth model.
1. Introduction
Recent work on text-to-image (T2I) personalization [25]
has opened the door for a new class of creative applications.
Specifically, for face personalization, it allows generation of
new images of a specific face or person in different styles.
The impressive diversity of styles is owed to the strong prior
of pre-trained diffusion model, and one of the key proper-
ties of works such as DreamBooth [25], is the ability to im-
plant a new subject into the model without damaging the
model's prior. Another key feature of this type of method is
that subject's essence and details are conserved even when
applying vastly different styles. For example, when train-
ing on photographs of a person's face, one is able to gen-
erate new images of that person in animated cartoon styles,
where a part of that person's essence is preserved and rep-
resented in the animated cartoon figure - suggesting some
amount of visual semantic understanding in the diffusion
model. These are two core characteristics of DreamBooth
and related methods, that we would like to leave untouched.
Nevertheless, DreamBooth has some shortcomings: size
and speed. For size, the original DreamBooth paper fine-
tunes all of the weights of the UNet and Text Encoder of the
diffusion model, which amount to more than 1GB for Sta-
ble Diffusion. In terms of speed, notwithstanding inference
speed issues of diffusion models, training a DreamBooth
model takes about 5 minutes for Stable Diffusion (1,000 it-
erations of training). This limits the potential impact of the
work. In this work, we want to address these shortcomings,
without altering the impressive key properties of Dream-
Booth, namely style diversity and subject fidelity, as depc-
tied in Figure 1. Specifically, we want to conserve model
integrity and closely approximate subject essence in a fast
manner with a small model.
Our work proposes to tackle the problems of size and
speed of DreamBooth, while preserving model integrity,
editability and subject fidelity. We propose the following
contributions:
•
• Lighweight DreamBooth (LiDB) - a personalized text-
to-image model, where the customized part is roughly
100KB of size. This is achieved by training a Dream-
Booth model in a low-dimensional weight-space gener-
ated by a random orthogonal incomplete basis inside of a
low-rank adaptation [14] weight space.
• New HyperNetwork architecture that leverages the
Lightweight DreamBooth configuration and generates the
customized part of the weights for a given subject in a
text-to-image diffusion model. These provide a strong di-
rectional initialization that allows us to further finetune
the model in order to achieve strong subject fidelity within
a few iteration. Our method is 25x faster than Dream-
Booth while achieving similar performances.
• We propose the technique of rank-relaxed finetuning,
where the rank of a LORA DreamBooth model is relaxed
during optimization in order to achieve higher subject fi-
delity, allowing us to initialize the personalized model
with an initial approximation using our HyperNetwork,
and then approximate the high-level subject details using
rank-relaxed finetuning.
One key aspect that leads us to investigate a HyperNet-
work approach is the realization that in order to be able
to synthesize specific subjects with high fidelity, using a
given generative model, we have to “modify" its output
domain, and insert knowledge about the subject into the
model, namely by modifying the network weights.
Phase 1 - HyperNetwork Training (Large Scale)
"A [V] face"
HyperNetwork
Phase 2- Fast Fine-Tuning
"A [V] face"
Reconstruction Loss
Weights
Regularization
Weights
composition
Text-to-Image
Diffusion Model
Ground Truth
weights
(1)
Hyper-
Network
(2)
(2)
Text-to-Image
Diffusion Model
8
Figure 2. HyperDreamBooth Training and Fast Fine-Tuning.
Phase-1: Training a hypernetwork to predict network weights from
a face image, such that a text-to-image diffusion network out-
puts the person's face from the sentence "a [v] face" if the pre-
dicted weights are applied to it. We use pre-computed person-
alized weights for supervision, using an L2 loss, as well as the
vanilla diffusion reconstruction loss. Phase-2: Given a face im-
age, our hypernetwork predicts an initial guess for the network
weights, which are then fine-tuned using the reconstruction loss to
enhance fidelity.
2. Related Work
Text-to-Image Models Several recent models such as Im-
agen [26], DALL-E2 [22], Stable Diffusion (SD) [24],
Muse [7], Parti [33], etc., demonstrate excellent image gen-
eration capabilities given a text prompt. Some Text-to-
Image (T2I) models like SD and Muse also allow condition-
ing the generation with a given image via an encoder net-
work. Techniques such as ControlNet [35] propose ways to
incorporate new input conditioning such as depth. However,
current text and image-based conditioning in these models
do not capture sufficient subject details. For ease of exper-
imentation, we demonstrate our HyperDreamBooth on the
SD model, given its relatively small size. Yet, the proposed
technique is generic and applicable to any T2I model.
Personalization of Generative Models Personalized
generation aims to create varied images of a specific sub-
ject from one or a few reference images. Earlier approaches
utilized GANs to manipulate subject images into new con-
texts. Pivotal tuning [23] fine-tunes GANs with inverted
latent codes, while [20] fine-tunes StyleGAN with around
100 images for a personalized generative prior. Casanova et
al. [6] condition a GAN with an input image to produce vari-
HyperNetwork
K iterations
Visual
face
weight_1
A Layer_1
weights
weight_1
Transformer
Encoder
Zeros
(Init)
Transformer
Decoder
:
weight_N
weight_N
A Layer_L
weights
Figure 3. HyperNetwork Architecture: Our hypernetwork consists of a Visual Transformer (ViT) encoder that translates face images into
latent face features that are then concatenated to latent layer weight features that are initiated by zeros. A Transformer Decoder receives the
sequence of the concatenated features and predicts the values of the weight features in an iterative manner by refining the initial weights
with delta predictions. The final layer weight deltas that will be added to the diffusion network are obtained by passing the decoder outputs
through learnable linear layers.
ations. However, these GAN-based techniques often lack
subject fidelity or diverse context in generated images.
HyperNetworks, introduced as auxiliary networks pre-
dicting weights for neural networks [12], have been applied
in image generation tasks akin to personalization, such as
StyleGAN inversion [3], resembling methods that aim to
invert an image's latent code for editing in GAN spaces [2].
They have also been used in other tasks such as language
modeling [15, 19, 21].
T2I Personalization via Finetuning Recent techniques
enhance T2I models for improved subject fidelity and ver-
satile text-based recontextualization. Textual Inversion [10]
optimizes text embeddings on subject images for image
generation, while [30] explores a richer inversion space cap-
turing more subject details. DreamBooth [25] adapts entire
network weights for subject fidelity. Various methods, like
CustomDiffusion [18], SVDiff [13], LoRa [1, 14], Style-
Drop [28], and DreamArtist [9], optimize specific network
parts or use specialized tuning strategies. Despite their ef-
fectiveness, most of these techniques are slow, taking sev-
eral minutes per subject for high-quality results.
Fast T2I Personalization Several recent and concurrent
works aim for faster T2I model personalization. Some,
like E4T [11] and ELITE [31], involve encoder learning
followed by complete network finetuning, while our hy-
pernetwork directly predicts low-rank network residuals.
SUTI [8] creates a dataset for training a separate network
to generate personalized images, but lacks high subject fi-
delity and affects the original model's integrity. Concur-
rent work InstantBooth [27] and Taming Encoder [16] in-
troduce conditioning branches for diffusion models, requir-
ing training on large datasets. FastComposer [32] focuses
on identity blending in multi-subject generation using im-
age encoders. Techniques like [4], Face0 [29], and Celeb-
basis [34] explore different conditioning or guidance ap-
proaches for efficient T2I personalization. However, bal-
ancing diversity, fidelity, and adherence to image distri-
bution remains challenging. Our proposed hypernetwork-
based approach directly predicts low-rank network residu-
als for subject-specific adaptation, differing from existing
techniques.
3. Preliminaries
χω
Latent Diffusion Models (LDM). Text-to-Image (T2I) dif-
fusion models Do (ε, c) iteratively denoises a given noise
map € € Rhxw into an image I following the descrip-
tion of a text prompt T, which is converted into an input
text embedding c = (T) using a text encoder . In this
work, we use Stable Diffusion [24], a specific instatiation of
LDM [24]. Briefly, LDM consists of 3 main components:
An image encoder that encodes a given image into latent
code; a decoder that decodes the latent code back to image
pixels; and a U-Net denoising network D that iteratively de-
noises a noisy latent code. See [24] for more details.
DreamBooth [25] provides a network fine-tuning strat-
egy to adapt a given T2I denoising network De to generate
images of a specific subject. At a high-level, DreamBooth
optimizes all the diffusion network weights 0 on a few given
subject images while also retaining the generalization abil-
ity of the original model with class-specific prior preser-
vation loss [25]. In the case of Stable Diffusion [24], this
amounts to finetuning the entire denoising UNet has over
1GB of parameters. In addition, DreamBooth on a single
subject takes about 5 minutes with 1K training iterations.
Low Rank Adaptation (LoRA) [1, 14] provides a
memory-efficient and faster technique for DreamBooth.
Specifically, LoRa proposes to finetune the network weight
residuals instead of the entire weights. That is, for a layer
with weight matrix W Є Rnm, LoRa proposes to fine-
tune the residuals AW. For diffusion models, LoRa is
usually applied for the cross and self-attention layers of
the network [1]. A key aspect of LoRa is the decomposi-
tion of AW matrix into low-rank matrices A = Rnxr and
BERrxm: AW = AB. The key idea here is that r << n
LORA DreamBooth
Down
(nxr)
Up
(rxm)
A
B
Lightweight DreamBooth
r = 1
386k variables
1.6 MB
a 100, b 50
28k variables
120 KB
Down
Aux
Down
Train
(nxa)
Up Train
(rxb)
Up Aux
(bxm)
(axr)
A
A
B
B
Figure 4. Lightweight DreamBooth: we propose a new low-
dimensional weight-space for model personalization generated by
a random orthogonal incomplete basis inside LoRA weight-space.
This achieves models of roughly 100KB of size (0.01% of original
DreamBooth and 7.5% of LORA DreamBooth size) and, surpris-
ingly, is sufficient to achieve strong personalization results with
solid editability.
and the combined number of weights in both A and B is
much lower than the number of parameters in the original
residual AW. Priors work show that this low-rank residual
finetuning is an effective technique that preserves several
favorable properties of the original DreamBooth while also
being memory-efficient as well as fast, remarkably even
when we set r = 1. For stable diffusion 1.5 model, LORA-
DreamBooth with r = 1 has approximately 386K parame-
ters corresponding to only about 1.6MB in size.
4. Method
Our approach consists of 3 core elements which we explain
in this section. We begin by introducing the concept of the
Lightweight DreamBooth (LiDB) and demonstrate how the
Low-Rank decomposition (LoRa) of the weights can be fur-
ther decomposed to effectively minimize the number of per-
sonalized weights within the model. Next, we discuss the
HyperNetwork training and the architecture the model en-
tails, which enables us to predict the LiDB weights from a
single image. Lastly, we present the concept of rank-relaxed
fast fine-tuning, a technique that enables us to significantly
amplify the fidelity of the output subject within a few sec-
onds. Fig. 2 shows the overview of hypernetwork training
followed by fast fine-tuning strategy in our HyperDream-
Booth technique.
4.1. Lightweight DreamBooth (LiDB)
Given our objective of generating the personalized subset of
weights directly using a HyperNetwork, it would be benefi-
cial to reduce their number to a minimum while maintaining
strong results for subject fidelity, editability and style diver-
sity. To this end, we propose a new low-dimensional weight
space for model personalization which allows for personal-
ized diffusion models that are 10,000 times smaller than a
DreamBooth model and more than 10 times smaller than a
LORA DreamBooth model. Our final version has only 30K
variables and takes up only 120 KB of storage space.
The core idea behind Lightweight DreamBooth (LiDB)
is to further decompose the weight-space of a rank-1 LoRa
residuals. Specifically, we do this using a random or-
thogonal incomplete basis within the rank-1 LoRA weight-
space. We illustrate the idea in Figure 4. The approach
can also be understood as further decomposing the Down
(A) and Up (B) matrices of LoRA into two matrices each:
Aaux Atrain with Aaux Є Rnxa and Atrain E Rax and
B Btrain Baux with Btrain Є Rrxb and Baux Є Rbxm
where the aux layers are randomly initialized with row-wise
orthogonal vectors and are frozen; and the train layers are
learned. Two new hyperparameters are introduced: a and b,
which we set experimentally. Thus the weight-residual in a
LiDB linear layer is represented as:
A
b
=
=
AW x = Aaux Atrain Btrain Baux,
(1)
where r << min(n, m), a < n and b < m. Aaux and
Baux are randomly initialized with orthogonal row vectors
with constant magnitude - and frozen, and Btrain and Atrain
are learnable. Surprisingly, we find that with a = 100 and
:50, which yields models that have only 30K trainable
variables and are 120 KB in size, personalization results are
strong and maintain subject fidelity, editability and style di-
versity. We show results for personalization using LiDB in
the experiments section.
=
4.2. HyperNetwork for Fast Personalization of Text-
to-Image Models
We propose a HyperNetwork for fast personalization of a
pre-trained T2I model. Let 0 denote the set of all LiDB
residual matrices: Atrain and Btrain for each of the cross-
attention and self-attention layers of the T2I model. In
essence, the HyperNetwork Hn with parameters ŋ takes
the given image x as input and predicts the LiDB low-rank
residuals
Hn(x). The HyperNetwork is trained on a
dataset of domain-specific images with a vanilla diffusion
denoising loss and a weight-space loss:
L(x) = a||D(x+e, c) − x||¾½ + ß||ô – 0 ||2, (2)
where x is the reference image, 0 are the pre-optimized
weight parameters of the personalized model for image x,
D is the diffusion model conditioned on the noisy image
x + € and the supervisory text-prompt c, and finally a and
ẞ are hyperparameters that control for the relative weight of
each loss. Fig. 2 (top) illustrates the hypernetwork training.
Initial
Reference
Output
HyperNetwork
Prediction
HyperNetwork +
Fast Finetuning
w/o
HyperNetwork
Figure 5. HyperNetwork + Fast Finetuning achieves strong results. Each row displays outputs from initial HyperNetwork prediction
(HyperNetwork Prediction column) and after HyperNetwork prediction with fast finetuning (HyperNetwork + Fast Finetuning). Results
without the HyperNetwork component highlight its importance.
Supervisory Text Prompt We propose to eschew any
type of learned token embedding for this task, and our hy-
pernetwork acts solely to predict the LiDB weights of the
diffusion model. We simply propose to condition the learn-
ing process "a [V] face" for all samples, where [V] is a rare
identifier described in [25]. At inference time variations of
this prompt can be used, to insert semantic modifications,
for example "a [V] face in impressionist style".
HyperNetwork Architecture Concretely, as illustrated
in Fig. 3, we separate the HyperNetwork architecture into
two parts: a ViT image encoder and a transformer decoder.
We use a ViT-H for the encoder architecture and a 2-hidden
layer transformer decoder for the decoder architecture. The
transformer decoder is a strong fit for this type of weight
prediction task, since the output of a diffusion UNet or Text
Encoder is sequentially dependent on the weights of the
layers, thus in order to personalize a model there is inter-
dependence of the weights from different layers. In previ-
ous work [3, 12], this dependency is not rigorously modeled
in the HyperNetwork, whereas with a transformer decoder
with a positional embedding, this positional dependency is
modeled - similar to dependencies between words in a lan-
guage model transformer. To the best of our knowledge this
is the first use of a transformer decoder as a HyperNetwork.
Iterative Prediction We find that the HyperNetwork
achieves better and more confident predictions given an it-
erative learning and prediction scenario [3], where interme-
diate weight predictions are fed to the HyperNetwork and
the network's task is to improve that initial prediction. We
only perform the image encoding once, and these extracted
features f are then used for all rounds of iterative prediction
for the HyperNetwork decoding transformer T. This speeds
up training and inference, and we find that it does not affect
the quality of results. Specifically, the forward pass of T
becomes:
Ôk = T(f, Ôk−1),
(3)
where k is the current iteration of weight prediction, and ter-
minates once k = s, where s is a hyperparameter control-
ling the maximum amount of iterations. Weights 0 are ini-
tialized to zero for k = 0. Trainable linear layers are used to
convert the decoder outputs into the final layer weights. We
use the CelebAHQ dataset [17] for training the HyperNet-
work, and find that we only need 15K identities to achieve
strong results, much less data than other concurrent meth-
ods. For example 100k identities for E4T [11] and 1.43
million identities for InstantBooth [27].
4.3. Rank-Relaxed Fast Finetuning
=
We find that the initial HyperNetwork prediction is in great
measure directionally correct and generates faces with sim-
ilar semantic attributes (gender, facial hair, hair color, skin
color, etc.) as the target face consistently. Nevertheless,
fine details are not sufficiently captured. We propose a final
fast finetuning step in order to capture such details, which
is magnitudes faster than DreamBooth, but achieves virtu-
ally identical results with strong subject fidelity, editability
and style diversity. Specifically, we first predict personal-
ized diffusion model weights H(x) and then subse-
quently finetune the weights using the diffusion denoising
loss L(x) ||Ɗô(x + €, c) − x|| 2. A key contribution of
our work is the idea of rank-relaxed finetuning, where we
relax the rank of the LORA model from r = 1 to r > 1
before fast finetuning. Specifically, we add the predicted
HyperNetwork weights to the overall weights of the model,
and then perform LoRA finetuning with a new higher rank.
This expands the capability of our method of approximat-
ing high-frequency details of the subject, giving higher sub-
ject fidelity than methods that are locked to lower ranks of
=
Table 1. Comparisons. We compare our method for face iden-
tity preservation (Face Rec.), subject fidelity (DINO, CLIP-I) and
prompt fidelity (CLIP-T) to DreamBooth and Textual Inversion.
We find that our method preserves identity and subject fidelity
more closely, while achieving a higher score in prompt fidelity.
Table 3. HyperNetwork Ablation.
We ablate components:
No Hyper (without hypernetwork at test-time), Only Hyper (us-
ing hypernetwork prediction without fast finetuning), and our
full method without iterative prediction (k=1). Our full method
performs best for all fidelity metrics, with No Hyper achieving
slightly better prompt following.
Method
Ours
Face Rec. 1
DINO ↑ CLIP-I↑ CLIP-T↑
0.655
0.473
0.577
0.286
Method
Ours
Face Rec. ↑
DINO ↑ CLIP-I↑
CLIP-T↑
0.655
0.473
0.577
0.286
DreamBooth
0.618
0.441
0.546
0.282
No Hyper
0.647
0.392
0.498
0.299
Textual Inversion
0.623
0.289
0.472
0.277
Only Hyper
0.631
0.414
0.501
0.298
Ours (k=1)
0.648
0.464
0.570
0.288
Table 2. Comparisons with DreamBooth. We compare our
method to differently tuned versions of DreamBooth that mini-
mize optimization time. Altering hyperparameters by increasing
the learning rate and decreasing iterations leads to degraded re-
sults in DreamBooth. DreamBooth-Agg-1 uses 400 iterations and
DreamBooth-Agg-2 uses 40 iterations as opposed to the normal
1200 iterations used in our vanilla DreamBooth.
Method
DINO ↑ CLIP-I↑ CLIP-T↑
0.286
0.282
Face Rec. ↑
Ours
0.655
0.473
0.577
DreamBooth
0.618
0.441
0.546
DreamBooth-Agg-1 0.615
0.323
0.431
0.313
DreamBooth-Agg-2
0.616
0.360
0.467
0.302
weight updates. To the best of our knowledge we are the
first to propose such rank-relaxed LORA models.
We use the same supervision text prompt “a [V] face”
this fast finetuning step. We find that given the HyperNet-
work initialization, fast finetuning can be done in 40 itera-
tions, which is 25x faster than DreamBooth [25] and LORA
DreamBooth [1]. We show an example of initial, interme-
diate and final results in Figure 5.
5. Experiments
We implement our HyperDreamBooth on the Stable Diffu-
sion v1.5 diffusion model and we predict the LoRa weights
for all cross and self-attention layers of the diffusion UNet
as well as the CLIP text encoder. For privacy reasons, all
face images used for visuals are synthetic, from the SFHQ
dataset [5]. For training, we use 15K images from CelebA-
HQ [17].
5.1. Subject Personalization Results
Our method achieves strong personalization results for
widely diverse faces, with performance that is identically
or surpasses that of the state-of-the art optimization driven
methods [10, 11, 25]. Moreover, we achieve very strong
editability, with semantic transformations of face identi-
ties into highly different domains such as figurines and ani-
mated characters, and we conserve the strong style prior of
the model which allows for a wide variety of style genera-
tions. We show results in Figure 6.
5.2. Comparisons
Qualitative Comparisons We compare our method to
Textual Inversion [10], DreamBooth [25] and E4T [11]. Re-
sults are shown in Figure 7. We observe that our method
strongly outperforms both Textual Inversion and Dream-
Booth generally, in the one-input-image regime - and ob-
tains strong results compared to E4T, especially in cases
where E4T overfits to the reference face pose and realistic
appearance, even though the output should be highly styl-
ized.
Quantitative Comparisons and Ablations We compare
our method to Textual Inversion and DreamBooth using
face recognition metrics ("Face Rec." from a VGGFace2
Inception ResNet), along with DINO, CLIP-I, and CLIP-
T metrics [25]. Using 100 CelebAHQ identities and 30
prompts (style modification and recontextualization), total-
ing 30,000 samples, Table 1 illustrates our approach out-
performing in all metrics. However, face recognition met-
rics are relatively weak here due to network training limi-
tations (realistic face bias). To compensate, we conduct a
user study (details below).
We also conduct comparisons with more aggressive
Table 4. User Study. Given limitations of face recognition net-
works (stylized faces are OOD), we conduct an identity fidelity
user study comparing our stylized generations against DB and TI.
Our approach generally receives higher user preference.
Ours
ᎠᏴ Undecided Ours
TI Undecided
7.8%
Pref. ↑ 64.8% 23.3% 11.9% 70.6% 21.6%
Table 5. User Stylization and Identity Preference. We com-
pare the user preference of stylization and identity between our
approach and the SoTA approach E4T. Users generally prefer our
method.
Preference ↑
Ours E4T
60.0% 37.5%
Undecided
2.5%
Input image
Input image
Input image
Figure 6. Results Gallery: Our method can generate novel artistic and stylized results of diverse subjects (depicted in an input image, left)
with considerable editability while maintaining the integrity to the subject's key facial characteristics. The output images were generated
with the following captions (top-left to bottom-right): "An Instagram selfie of a [V] face", "A Pixar character of a [V] face", "A [V] face
with bark skin", "A [V] face as a rock star". Rightmost: "A professional shot of a [V] face".
DreamBooth training with altered iterations and learning
rates. Specifically, DreamBooth-Agg-1 (400 iterations) and
DreamBooth-Agg-2 (40 iterations) differ from our 1200-
iteration vanilla DreamBooth. Table 2 reveals that aggres-
sive DreamBooth training without our HyperNetwork ini-
tialization generally degrades results.
Additionally, we show an ablation study that explores
our method's components: removing the HyperNetwork
(No Hyper), utilizing only the HyperNetwork without fine-
tuning (Only Hyper), and our full setup without iterative
predictions (k=1). Table 3 demonstrates that our complete
setup achieves superior subject fidelity, albeit with a slightly
lower prompt following metric.
User Study We conduct a user study for face identity
preservation of outputs and compare our method to Dream-
Booth and Textual Inversion. Specifically, we present the
reference face image and two random generations using the
same prompt from our method and the baseline, and ask
the user to rate which one has most similar face identity to
the reference face image. We test a total of 25 identities,
and query 5 users per question, with a total of 1,000 sam-
ple pairs evaluated. We take the majority vote for each pair.
We present our results in Table 4, where we show a strong
preference for face identity preservation of our method.
Finally, we present a user study for overall preference of
both subject fidelity and style fidelity and compare our ap-
proach to the published state-of-the-art E4T method [11] on
a set of identities from the SFHQ dataset, with E4T kindly
run by the authors. We present both the reference subject
image as well as a reference style image and ask users which
output they prefer with respect to both identity preservation
Reference
OO
Outputs
B
HyperDreamBooth
E4T
Textual Inversion
DreamBooth
HyperDreamBooth
E4T
Textual Inversion
DreamBooth
Figure 7. Qualitative Comparison: We compare random samples from our method (HyperDreamBooth), E4T [11], DreamBooth [25] and
Textual Inversion [10] for two different identities and five different stylistic prompts. We observe that our method generally achieves very
strong editability while preserving identity, generally surpassing competing methods in the single-reference regime. E4T shows strong
performance but can tend to overfit to the reference head pose and realistic appearance, even when the image should be strongly stylized.
and style preservation. We test 10 identities, 4 prompts per
identity, and query 15 users per question, totaling 600 sam-
ples. Results are shown in Table 5, where we observe a pref-
erence of users for our method. Although E4T is a method
that achieves strong results and preserves identity well, we
observe slightly less qualitative editability as well as some
consistency errors with hard prompts. Note our method is
trained on 15k identities vs. 100k identities for E4T.
6. Conclusion
In this work, we presented HyperDreamBooth a new
method for fast and lightweight subject personalization of
diffusion models. It leverages a HyperNetwork to gener-
ate Lightweight DreamBooth (LiDB) parameters for a dif-
fusion model with a subsequent fast rank-relaxed finetuning
that achieves a sharp reduction in size and speed compared
to DreamBooth and other optimization-based personaliza-
tion work. We showed that it produces high-quality and
diverse images of faces with different styles and semantic
modifications, while preserving subject details and model
integrity.
References
[1] Low-rank adaptation for fast text-to-image diffusion fine-
tuning. https://github.com/cloneofsimo/
lora, 2022. 3,6
[2] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle:
A residual-based stylegan encoder via iterative refinement.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 6711-6720, 2021. 3.
[3] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and
Amit Bermano. Hyperstyle: Stylegan inversion with hy-
pernetworks for real image editing. In Proceedings of
the IEEE/CVF conference on computer Vision and pattern
recognition, pages 18511-18521, 2022. 3, 5
[4] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild,
Soumyadip Sengupta, Micah Goldblum, Jonas Geip-
ing, and Tom Goldstein. Universal guidance for diffusion
models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pages 843-852,
2023. 3
[5] David Beniaguev.
dataset.
Synthetic faces high quality (sfhq)
https://github.com/SelfishGene/
SFHQ-dataset, 2022. 6
[6] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal
Drozdzal, and Adriana Romero Soriano. Instance-
conditioned gan. Advances in Neural Information Process-
ing Systems, 34:27517-27529, 2021. 2
[7] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,
Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-
phy, William T Freeman, Michael Rubinstein, et al. Muse:
Text-to-image generation via masked generative transform-
ers. arXiv preprint arXiv:2301.00704, 2023. 2
[8] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz,
Xuhui Jia, Ming-Wei Chang, and William W Cohen.
Subject-driven text-to-image generation via apprenticeship
learning. arXiv preprint arXiv:2304.00186, 2023. 3
[9] Ziyi Dong, Pengxu Wei, and Liang Lin. Drea-
martist: Towards controllable one-shot text-to-image gen-
eration via contrastive prompt-tuning.
arXiv preprint
arXiv:2211.11337, 2022. 3
[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or.
An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint
arXiv:2208.01618, 2022. 3, 6, 8
[11] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,
Gal Chechik, and Daniel Cohen-Or. Designing an encoder
for fast personalization of text-to-image models. arXiv
preprint arXiv:2302.12228, 2023. 3, 5, 6, 7, 8
[12] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks.
arXiv preprint arXiv:1609.09106, 2016. 3,5
[13] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,
Dimitris Metaxas, and Feng Yang. Svdiff: Compact pa-
rameter space for diffusion fine-tuning. arXiv preprint
arXiv:2303.11305, 2023. 3
[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021. 2, 3
[15] Hamish Ivison, Akshita Bhagia, Yizhong Wang, Hannaneh
Hajishirzi, and Matthew Peters. Hint: Hypernetwork in-
struction tuning for efficient zero-shot generalisation. arXiv
preprint arXiv:2212.10315, 2022. 3
[16] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han
Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and
Yu-Chuan Su. Taming encoder for zero fine-tuning image
customization with text-to-image diffusion models. arXiv
preprint arXiv:2304.02642, 2023. 3
[17] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196, 2017. 5, 6
[18] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 1931-1941, 2023. 3
[19] Jesse Mu, Xiang Lisa Li, and Noah Goodman.
Learn-
ing to compress prompts with gist tokens. arXiv preprint
arXiv:2304.08467, 2023. 3
[20] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal
Yarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and
Daniel Cohen-Or. Mystyle: A personalized generative prior.
ACM Transactions on Graphics (TOG), 41(6):1-10, 2022. 2
[21] Jason Phang, Yi Mao, Pengcheng He, and Weizhu Chen. Hy-
pertuning: Toward adapting large language models without
back-propagation. In International Conference on Machine
Learning, pages 27854-27875. PMLR, 2023. 3
[22] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
2022. 2
[23] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel
Cohen-Or. Pivotal tuning for latent-based editing of real im-
ages. ACM Transactions on graphics (TOG), 42(1):1–13,
2022. 2
[24] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684-10695, 2022. 2, 3
[25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. 2022. 1, 3, 5, 6, 8
[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems, 35:36479-36494, 2022. 2
[27] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-
booth: Personalized text-to-image generation without test-
time finetuning. arXiv preprint arXiv:2304.03411, 2023. 3,
5
[28] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro
Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,
Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael
Rubinstein, and Dilip Krishnan. Styledrop: Text-to-image
generation in any style. arXiv preprint arXiv:2306.00983,
2023. 3
[29] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv
Leviathan. Face0: Instantaneously conditioning a text-to-
image model on a face. arXiv preprint arXiv:2306.06638,
2023. 3
[30] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir
Aberman. p+: Extended textual conditioning in text-to-
image generation. arXiv preprint arXiv:2303.09522, 2023.
3
[31] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei
Zhang, and Wangmeng Zuo. Elite: Encoding visual con-
cepts into textual embeddings for customized text-to-image
generation. arXiv preprint arXiv:2302.13848, 2023. 3
[32] Guangxuan Xiao, Tianwei Yin, William T Freeman, Frédo
Durand, and Song Han. Fastcomposer: Tuning-free multi-
subject image generation with localized attention. arXiv
preprint arXiv:2305.10431, 2023. 3
[33] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-
jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-
fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres-
sive models for content-rich text-to-image generation. arXiv
preprint arXiv:2206.10789, 2022. 2
[34] Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li,
Chenyang Qi, Xintao Wang, Ying Shan, and Huicheng
Zheng. Inserting anybody in diffusion models via celeb ba-
sis. arXiv preprint arXiv:2306.00926, 2023. 3
[35] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models. arXiv preprint
arXiv:2302.05543, 2023. 2
