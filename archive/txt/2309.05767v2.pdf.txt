arXiv:2309.05767v2 [cs.SD] 6 Feb 2024
NATURAL LANGUAGE SUPERVISION FOR GENERAL-PURPOSE AUDIO
REPRESENTATIONS
Benjamin Elizalde*, Soham Deshmukh*, Huaming Wang
Microsoft
{benjaminm, sdeshmukh, huawang}@microsoft.com
ABSTRACT
Audio-Language models jointly learn multimodal text
and audio representations that enable Zero-Shot inference.
Models rely on the encoders to create powerful representa-
tions of the input and generalize to multiple tasks ranging
from sounds, music, and speech. Although models have
achieved remarkable performance, there is still a gap with
task-specific models. In this paper, we propose a Contrastive
Language-Audio Pretraining model that is pretrained with a
diverse collection of 4.6M audio-text pairs employing two
innovative encoders for Zero-Shot inference. To learn audio
representations, we trained an audio encoder on 22 audio
tasks, instead of the standard training of sound event clas-
sification. To learn language representations, we trained an
autoregressive decoder-only model instead of the standard
encoder-only models. Then, the audio and language repre-
sentations are brought into a joint multimodal space using
Contrastive Learning. We used our encoders to improve
the downstream performance by a large margin. We ex-
tensively evaluated the generalization of our representations
on 26 downstream tasks, the largest in the literature. Our
model achieves state of the art results in several tasks out-
performing 4 different models and leading the way towards
general-purpose audio representations. Code is on GitHub¹.
Index Terms contrastive learning, general purpose au-
dio representation, zero-shot, language, sounds
1. INTRODUCTION
Recent research in the audio domain focuses on learning rep-
resentations that generalize to a wide range of downstream
tasks across different domains. The 2021 Holistic Evaluation
of Audio Representations (HEAR) [1] took a major step in
this direction by providing a comprehensive setup to bench-
mark audio representations. The models were pretrained on
a large dataset -AudioSet [2] (1.7M files)- using Supervised,
Self-Supervised or Unsupervised Learning. All the methods
have to undergo additional fine-tuning to use their representa-
tions on a given downstream task.
Zero-Shot models can be applied to any task directly
achieving flexibility and generalization. One of the most
successful type are Contrastive Language-Audio Pretraining
(CLAP) models that jointly learn multimodal text and audio
*Equal Contribution
https://github.com/microsoft/CLAP
representations. Authors in [3] introduced a CLAP model
that achieved state of the art (SOTA) in 16 downstream tasks.
Subsequent literature showed that the choice of audio and
text encoders are critical to generate powerful representations
and increase performance across tasks [4, 5, 6, 7]. For exam-
ple, upgrading from CNN to audio transformers (HTSAT) to
encode audio and from BERT to ROBERTa to encode text.
Another conclusion is that scaling up the number of train-
ing pairs improves overall performance. However, simply
adding pairs may result in a drop of performance in certain
domains and tasks [4, 5, 3, 6]. CLAP's performance is de-
pendent on the diversity of the text and audio training pairs
and how noisy they are. Wav2clip [8] and Audioclip [9] used
200k and 1.7M audio-text pairs respectively from AudioSet,
a dataset annotated for sound events. Authors paired audio
with class labels rather than with sentence-level descriptions,
potentially missing the context and language semantics of de-
scriptions, but with good Zero-Shot performance in 3 and 9
tasks respectively. CLAP [3] used 128k pairs but the text
were descriptions coming from audio captioning and a web-
sourced dataset. It was evaluated on 16 tasks and significantly
improved over its predecessors. LAION CLAP [4] used a
collection of 2.5M pairs, further improving performance in 8
tasks. Authors later added music and speech-related training
pairs, but performance in sound event classification (ESC50)
degraded by an absolute 1%. Wavcaps[6] used 500k pairs, but
cleaned up the noisy web-sourced descriptions with a Chat-
GPT language model. Results outperformed the literature in
8 tasks. Therefore, when scaling up pairs it is essential to ver-
ify performance trade offs by evaluating generalization across
different domains and tasks.
In this paper we make the following contributions. To
learn audio representations, we trained an audio encoder on
22 audio tasks. To learn language representations, we trained
an autoregressive decoder-only model. We pretrained our
CLAP model with an unprecedented 4.6 million audio-text
pairs and extensively evaluated the generalization of our
representations on 26 downstream tasks, the largest in the
literature, achieving SoTA results in several.
2. METHOD
Contrastive Language-Audio Pretraining (Fig 1) jointly trains
an audio an a text encoder to learn multimodal representations
which can be used for different types of inference.
Contrastive Pretraining
Paddling in the water
Text
Encoder
Text audio pairs
T₁
Classes
Dog barking
Zero-Shot Classification
Rain falling
Tw
Text
Encoder
A₁ A₁T, A₁₂ A₁T- AT
Siren wailing
…
Audio
Encoder
Az A A A
Ax
Art Art Axt
Text to Audio Retrieval
Text Query
A crow crying
Text
int the forest
Encoder
Audio
Database
AN ANTI ANT ANTS
ANTN
Testing audio
Audio
Encoder
AAT, AT A₁T
Audio Captioning
Dog barking
A campfire crackles as the flames burn branches
A₁ AT >>>>
Audio
Encoder
Testing audio
Az
A₂T
→
Audio
Encoder
Fig. 1: CLAP
A As
AN ANT
Mapper
GPT-2
Maps an audio embeddings to a
GPT2 input sequence
learns audio and a text embeddings that can be
compared in a multimodal space. The pretrained encoders can be
used for Zero-Shot Classification, Text to Audio and Audio to Text
Retrieval, and Audio Captioning.
2.1. Contrastive Language-Audio Pretraining
Let the processed audio be Xa s.t. Xa Є RF×T where F
are the number of spectral components (e.g. Mel bins) and T
are the number of time bins. Let the text be represented by
Xt. Each audio-text pair in a batch of N is represented as
{Xa, Xti where i Є [0, N]. For convenience, we drop the i
notation, and henceforth {Xa, Xt} will denote a batch of N.
From the pairs, the audio and text are passed to an au-
dio encoder fa(.) and a text encoder ft (.) respectively. For a
batch of N:
Ŷa = fa(Xa); Ŷt = ft(Xt)
(1)
where Ŷ a Є R³×V are the audio representations of dimen-
sionality V, and Âµ Є R³×ʊ are the text representations of
dimensionality U.
We brought audio and text representations, Ŷa and Ŷt,
into a joint multimodal space of dimension d by using a learn-
able projection layer:
Ea = La(Xa); Et = Lt(Xt)
(2)
where Ea Є Rxd, Et Є R³×d, La and Lɩ are the projec-
tions for audio and text respectively.
Now that the audio and text embeddings (Ea, Et) are
comparable, we can measure similarity:
C = T (Et · ET)
.
NXN
(3)
where 7 is a temperature parameter to scale the range of log-
its. The similarity matrix CERN has N matching pairs
in the diagonal and N² – N non-matching pairs in the off-
diagonal.
L = 0.5(ltext (C) + laudio (C))
=
A
(4)
where lk
log diag(softmax(C)) along text
and audio axis respectively. We used this symmetric cross-
entropy loss (L) over the similarity matrix to jointly train the
audio and text encoders along with their projection layers.
2.2. Audio and Text Encoders
Audio Encoder: To process audio, we trained a transformer-
based audio encoder (HTSAT [10]) on 22 audio tasks using a
similar method to this paper [11]. We called it HTSAT-22. We
hypothesized that an encoder trained on multiple audio tasks
would improve generalization and thus performance across
tasks. The method learns an audio encoder and a mapper net-
work to prompt a large language model to perform multiple
audio tasks, such as classification, captioning, retrieval and
audio Q&A. The architecture is trained essentially as a cap-
tioning system, where it learns to generate a free-form text
output c² in an autoregressive fashion conditioned on the au-
dio prompt p². Note that y denotes the model's trainable pa-
rameters. The loss function is Cross-Entropy:
N 1
L= log p(cpi, P2, C, 1) (5)
i=1 j=1
2k'
Text Encoder: To process text, we adapted GPT2 (base
124M), which is an autoregressive model that has exhibited
impressive abilities for text tasks. We addressed the chal-
lenge - How to make an autoregressive model produce a
sentence-level representation? Autoregressive models built
with transformer-decoder blocks, take an input text and out-
put the most likely sequence of words (tokens), one after
the other. In contrast, models built with transformer-encoder
blocks (BERT or ROBERTA) output a sentence-level repre-
sentation in a continuous space. To make GPT2 output a
sentence-level representation, we appended the special to-
ken|endoftext| > at the end of each input text. During
contrastive pretraining, we use the representations from this
token as sentence-level representations. This forces the token
to contain the aggregate information from the text input.
2.3. Evaluation
Zero-Shot Inference: We used CLAP's ability to determine
the similarity between audio and text. Let's consider a target
dataset with C class labels and N test audios. First, we com-
pute CLAP's audio and text embeddings for N audios and C
classes using the pretrained encoders. Second, we compute
the cosine similarity between each testing audio and all the
class labels. In the case of retrieval, we treat text queries as
classes. Each test audio will have as many logits as classes.
Third, logits are turned into a probability distribution by ap-
plying softmax for binary or multiclass classification; sigmoid
for multilabel classification; and left unaltered for retrieval.
Audio Captioning: In the architecture of Fig 1, a test audio is
passed to the pretrained audio encoder, then to a mapper net-
work, and then to GPT2 to generate a description. At training
time, only the weights of the mapper network are learned with
a captioning loss (Eq.5) and the training split.
Zero-Shot
Score ↑
Sound Event Classification ↑
Vocal Sound
Classification ↑
Surveillance
Sound Classif.↑
Action
Classification↑
Acoustic Scene
Classification↑
DCASE17
Vocal
ESC50
Model
Average
ESC50
FSD50K US8K
SESA
TUT 2017
Task 4
Sound
Actions
CNN14+BERT
0.428
0.826
0.302
0.732
0.300
0.495
0.749
0.495
0.296
HTSAT+CLIP
0.430
0.813
0.289
0.748
0.277
0.645
0.761
0.442
0.219
HTSAT+ROBERTa
0.431
0.811
0.322
0.757
0.226
0.610
0.745
0.475
0.285
HTSAT+GPT2
0.435
0.819
0.336
0.767
0.242
0.646
0.644
0.503
0.286
HTSAT-22+ROBERTa
0.454
0.879
0.388
0.767
0.209
0.682
0.656
0.481
0.369
HTSAT-22+CLIP
0.469
0.830
0.411
0.791
0.229
0.692
0.723
0.488
0.292
HTSAT-22+GPT2
0.480
0.882
0.403
0.750
0.337
0.692
0.762
0.475
0.317
Music Classification ↑
Instrument Classification ↑
Speech Emotion
KWS↑
Speaker
Classification↑
Counting↑
GTZAN
GTZAN
Model
Beijing
NS Instr.
Music Speech
Genres
Opera
family
CRE RAV
MA-D DESS
Speech
Commands
LibriCount 10
CNN14+BERT
HTSAT+CLIP
1
0.252
0.475
0.295
0.178 0.160
0.106
0.179
0.992
0.156
0.627
0.312
0.208
0.169
0.120
0.113
HTSAT+ROBERTa
0.992
0.178
0.436
0.352
0.263
0.2
0.098
0.149
HTSAT+GPT2
1
0.150
0.539
0.322
0.234
0.171
0.139
0.155
HTSAT-22+ROBERTa
1
0.209
0.309
0.402
0.301
0.278
0.129
0.207
HTSAT-22+CLIP
1
0.280
0.517
0.462
0.275
0.233
0.116
0.094
HTSAT-22+GPT2
1
0.289
0.487
0.425
0.297 0.217
0.089
0.254
Table 1: Zero-Shot performance on 16 downstream tasks and 119k training pairs. Our proposed encoders (HTSAT-22+GPT2) outperformed
the best combinations in the literature. Higher is better for all numbers. The metrics are mAP for FSD50k and ESC50-actions; F1-score for
DCASE17; all others use Accuracy. Zero-Shot score is the average of the metrics. This is the first comparison of encoders in literature with
16 tasks, usually only a couple of enocders and a handful of tasks are considered.
3. EXPERIMENTS
Training Datasets. Collecting pairs is perhaps the main
bottleneck of scaling up CLAP models. We gathered the
largest collection with 4.6 million audio and text pairs from
different datasets and web archives. The audios describe
human sounds and activities, environmental sounds, acous-
tic scenes, music, sound effects, and speech emotion. To
study the effect of encoders in Table 1, we used the same
training sets as CLAP [3]. Unlike the authors, we did
not include the test set of AudioCaps and Clotho, so the
number of pairs was 119k instead of 128k. The training
datasets for the 4.6M collection are: WavCaps [6], Au-
dioSet [2], FSD50K [12], Clotho [13], AudioCaps [14],
MACS [15], WavText5k [5], SoundDesc [16], NSynth [17],
FMA [18], Mosi [19], Meld [20], Iemocap [21], Mosei [22],
MSP-Podcast [23], CochlScene [24], LJspeech [25], EpicK-
itchen [26], Kinectics700 [27], findsounds.com. Details on
GitHub.
Downstream Tasks. We used 26 downstream tasks from dif-
ferent domains, several come from HEAR[1]: sound events,
vocal sounds, surveillance sounds, and acoustic scenes classi-
fication; audio captioning; retrieval; music, instruments, and
note attributes classification; speech emotions and language
classification; keyword spotting; and speaker counting. To
study the effect of encoders in Table 1, we used a subset of
16 tasks.
Pre-processing. We used log Mel spectrogram representa-
tions of audio with a sampling rate of 44.1 KHz, hop size of
320 frames, window size 1024 frames, and 64 Mel bins in the
range of 50-8000 Hz. During training, each audio clip is ran-
domly truncated to a continuous segment of 7 secs, or padded
if shorter. The batches with pairs are randomly sampled.
Encoders. For our proposed CLAP model, we used the au-
dio and text encoders HTSAT-22+GPT2 described in Sec.2.2.
For comparison, in Table 1 we used the two best combina-
tions of encoders in the literature CNN14+BERT and HT-
SAT+ROBERTa [3, 4, 6]. We also included the text encoder
from CLIP because it was used by different authors [9, 8, 4].
Both, the audio and text embeddings are projected into a mul-
timodal space with independent learnable projection layers
with an output dimension of 1024.
Training. We trained by unfreezing both encoders for 40
epochs, although the overall performance peaked in the first
10 epochs. We report the performance of the downstream
tasks corresponding to the epoch that yielded the best Zero-
Shot score (average of all tasks). We hypothesize that the
model corresponding to such epoch will generalize better to
unseen datasets and serve the community better. It is possible
that the performance of each task was higher or lower in a dif-
ferent epoch. Batch size was 1,536. We used Adam Optimiser
with an initial learning rate 10-3 and reduce the learning rate
on plateau by 10-1 with a patience of 15. The temperature
parameter 7 is learnable and initialised to 0.007.
4. RESULTS AND DISCUSSION
The results comparing different audio and text encoders are in
Table 1 and the results of our proposed CLAP are in Table 2.
4.1. Proposed audio and text encoder
Our proposed encoders HTSAT-22+GPT2 outperformed two
of the best combination of encoders in the literature, as
shown in Table 1. To compare overall performance, we
used Zero-Shot score, which is the average of the metrics
from all 16 tasks. HTSAT-22+GPT2 achieved 0.480, an
absolute 9% higher than the most common combinations
HTSAT+ROBERTa and CNN14+BERT with 0.431 and 0.428
respectively. All encoder combinations performed better
Model
Benchmark
HTSAT-22+GPT2
ESC50
[28]
0.948 [6]
0.939
FSD50K
[12]
0.302 [3]
Sound Event Classification ↑
US8K
0.485
[29]
0.806 [6]
0.823
DCASE17
Task 4 [30]
0.3 [3]
0.466
AudioSet
[2]
0.058 [3]
0.102
Vocal Sound
Classification ↑
Vocal
Surveillance
Sound Classif.↑
Action
Classification↑
Acoustic Scene
Classification↑
SESA
Sound [31]
[32]
ESC50
Actions [33]
0.495 [3]
0.8
0.25
0.045
TUT 2017
[30]
0.296 [3]
0.65
0.509
0.538
Speech Emotion
Music Classification ↑
Instrument Classification ↑
KWS↑
Classification↑
GTZAN
GTZAN
Model
Music
Speech [1]
Genres
NS
Pitch
NS
Velocity
NS
Qualities
Beijing
NS Instr.
CRE
RAV
Opera
family
MA-D
DESS
[1]
[17]
[17]
[17]
[1]
[17]
[1]
[34]
Speech
Commands
[1]
Speaker
Counting↑
Libri
Count 10
[1]
Benchmark
1 [3]
0.25 [3]
0.015
0.2
HTSAT-22+GPT2
0.992
0.584
0.444
0.222
0.1
0.489
0.4746 [3]
0.09
0.466
0.479
0.178 [3]
0.3
0.159 [3]
0.106 [3]
0.178 [3]
0.315
0.164
0.246
Audio Captioning ↑
Model
Benchmark
HTSAT-22+GPT2
AudioCaps
Clotho
[14]
[13]
0.438[35] 0.215[35]
0.455
0.271
AudioCaps
R@1
0.517[6]
0.425
Audio-Text Retrieval ↑
AudioCaps Clotho
MAP@10 R@1
0.457 [4] 0.234[6]
0.319
0.229
Text-Audio Retrieval ↑
Clotho
MAP@10
0.138[4]
0.155
AudioCaps
R@1
0.397[6]
0.356
AudioCaps Clotho Clotho
MAP@10 R@1 MAP@10
0.51[4] 0.195[6] 0.204[4]
0.51
0.157
0.257
Table 2: Performance on 26 downstream tasks using our proposed encoders and 4.6M training pairs. As the benchmark, we used the best
numbers in the literature, when no number was available we used random performance. Higher is better for all tasks. The evaluation metrics
are MAP for FSD50k, ESC50-Actions, AudioSet, and NS Qualities; F1-score for DCASE17; and SPIDER for Captioning; all others use
Accuracy.
than random. Although different combinations did better at
different tasks, none of them excelled at a specific domain.
Our HTSAT-22 audio encoder is the major contributor to
performance improvement. HTSAT-22 is pretrained on 22
audio tasks in contrast to HTSAT which is pretrained only
on sound event classification. Hence, suggesting that gen-
erating pretraining on multiple audio tasks can improve the
representations from the audio encoder. Comparing HTSAT-
22+GPT2 to HTSAT+GPT2 evidenced major improvements
such as LibriCount10 (absolute 10%), NS Instrument (abso-
lute 7%) and ESC50 (absolute 6%).
The proposed GPT2 autoregressive model improves upon
the popular ROBERTa. Using GPT2 with either HTSAT or
HTSAT-22 yielded the best performance over the other text
encoders. We hypothesize that the improvement comes from
two reasons. First, GPT2 has a larger vocabulary of 50k
tokens compared to BERT and ROBERTa with 30k. Sec-
ond, our modified GPT2 autoregressive predicts tokens till <
|endoftext > used for sentence-level representation. This
acts as self-supervision and forces the model to learn and put
emphasis on the ordering of words.
4.2. Scaling proposed CLAP architecture
Our CLAP model established new Zero-Shot SoTA on most
of the 26 downstream tasks as shown in Table 2, outperform-
ing 4 different SoTA models. To benchmark our model, we
used the best numbers in the literature coming from different
models. When no number was available, we used random per-
formance. In some cases, performance improvement is more
than double the benchmark literature. Some highlights are
Music Genres with 58.4% acc. vs 25%, Vocal Sounds with
80% acc. vs 49.5%, Acoustic Scenes with 53.8% acc. vs
29.6%. Some downstream tasks do not constitute a true Zero-
Shot setup as the audio files in the training set were part of the
4.6M pairs (see Sec.3). For instance, FSD50k audio and web
descriptions were used in training but not the class labels. We
did not fine-tune CLAP encoders for any downstream task.
We only fine-tune the audio encoder for ESC50 and were able
to improve performance from our previous CLAP model from
96.70% to 98.25% accuracy, thus establishing a new SoTA.
4.3. Generalization and individual domain performance
Adding diversity and scaling the audio-text pairs in training
presents a trade-off that increases performance in some tasks
but decreases it in others. As expected, adding training pairs
that resemble the domain from a given task helps, hence diver-
sity is essential for generalization. For example, CLAP [3] did
not include emotion recognition training pairs and achieved
17.1% acc. in RAVDESS and 23.4% in CREMAD. We added
emotion-related pairs and improved accuracy to 31.5% and
30% respectively. Nonetheless, more pairs can cause a distri-
bution shift, creating a mismatch between training and some
testing data. For example, our model achieved a slightly lower
score than a model [6] trained with 500k pairs on ESC50
(94.8% vs 93.9% acc.). Another example is with GTZAN
Music vs Speech, where a model [3] with 128k pairs achieved
100% acc. over ours with 99.2%. Even our model in Table 1
achieved 100% acc with 119k pairs. We should expect that
as we add training pairs, performance across tasks will vary.
Hence, zero-shot models should be evaluated across different
domains and tasks with focus on generalization rather than on
overfitting to specific tasks.
Audio-Text (A-T) and Text-Audio (T-A) Retrieval perfor-
mance fell short of the benchmark. We measured the tasks
with mAP@10, which is the ranking metric of IEEE DCASE,
and R@1. Our model outperformed the literature in terms of
mAP@10 for Clotho (A-T: 0.155 vs 0.138 and T-A: 0.257 vs
0.204), and struggled only with A-T AudioCaps (A-T: 0.319
vs 0.457 and T-A: 0.51 vs 0.51). Both datasets are sensitive
to out-of-domain training data and adding training pairs did
not translate into an improvement. This was demonstrated by
authors in [5] who unsuccessfully tried to add 39k files from
SounDesc or authors in [4] with 500k from Wavcaps or au-
thors in [6] with 1.7M from AudioSet.
5. CONCLUSION
We introduced a CLAP model with our proposed encoders
and 4.6M training pairs. Zero-shot models should be eval-
uated across different tasks with a focus on generalization
rather than on overfitting to specific tasks. We evaluated
CLAP on 26 tasks and established SOTA on most of them,
leading the way in general-purpose audio representations.
6. REFERENCES
[1] Joseph Turian, Jordie Shier, et al., “HEAR: Holistic
Evaluation of Audio Representations," in NeurIPS 2021
Competitions and Demonstrations Track, 2022.
[2] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman,
Aren Jansen, Wade Lawrence, et al., “Audio set: An
ontology and human-labeled dataset for audio events,"
in IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2017.
[3] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-
mail, and Huaming Wang, “Clap learning audio con-
cepts from natural language supervision,” in IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2023.
[4] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-
lor Berg-Kirkpatrick, and Shlomo Dubnov, "Large-scale
contrastive language-audio pretraining with feature fu-
sion and keyword-to-caption augmentation," arXiv
preprint arXiv:2211.06687, 2022.
[5] Soham Deshmukh, Benjamin Elizalde, and Huaming
Wang, "Audio Retrieval with WavText5K and CLAP
Training," in Proc. INTERSPEECH 2023, 2023.
[6] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang
Kong, Tom Ko, Chengqi Zhao, Mark Plumbley, et al.,
"Wavcaps: A chatgpt-assisted weakly-labelled audio
captioning dataset for audio-language multimodal re-
search," arXiv preprint arXiv:2303.17395, 2023.
[7] Benjamin Elizalde, Shuayb Zarar, and Bhiksha Raj,
"Cross modal audio search and retrieval with joint
embeddings based on text and audio," in ICASSP
2019-2019 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). IEEE,
2019, pp. 4095-4099.
[8] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar,
et al., "Wav2clip: Learning robust audio representations
from clip," in IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), 2022.
[9] Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas
Dengel, "Audioclip: Extending clip to image, text and
audio," in IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 2022.
[10] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor
Berg-Kirkpatrick, et al., “Hts-at: A hierarchical token-
semantic audio transformer for sound classification and
detection," in IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), 2022.
[11] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and
Huaming Wang, “Pengi: An audio language model for
audio tasks," arXiv preprint arXiv:2305.11834, 2023.
[12] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic
Font, and Xavier Serra, "Fsd50k: An open dataset of
human-labeled sound events," IEEE/ACM Transactions
on Audio, Speech, and Language Processing, 2022.
[13] Konstantinos Drossos, Samuel Lipping, and Tuomas
Virtanen, "Clotho: an audio captioning dataset,” in
IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2020.
[14] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,
and Gunhee Kim, "AudioCaps: Generating Captions
for Audios in The Wild," in NAACL-HLT, 2019.
[15] Irene Martín-Morató and Annamaria Mesaros, "What
is the ground truth? reliability of multi-annotator data
for audio tagging," in 29th European Signal Processing
Conference (EUSIPCO), 2021.
[16] A. Sophia Koepke, Andreea-Maria Oncescu, Joao Hen-
riques, Zeynep Akata, and Samuel Albanie, "Audio
retrieval with natural language queries: A benchmark
study,” IEEE Transactions on Multimedia, 2022.
[17] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander
Dieleman, et al., "Neural audio synthesis of musi-
cal notes with wavenet autoencoders," in International
Conference on Machine Learning. PMLR, 2017.
[18] Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst,
and Xavier Bresson, “Fma: A dataset for music analy-
sis," in 18th International Society for Music Information
Retrieval Conference, 2017.
[19] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-
Philippe Morency, “Mosi: multimodal corpus of sen-
timent intensity and subjectivity analysis in online opin-
ion videos," arXiv preprint arXiv:1606.06259, 2016.
[20] Soujanya Poria, Devamanyu Hazarika, Navonil Ma-
jumder, Gautam Naik, et al., “Meld: A multimodal
multi-party dataset for emotion recognition in conver-
sations," in Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics, 2019.
[21] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe
Kazemzadeh, Emily Mower, Samuel Kim, et al.,
"Iemocap: Interactive emotional dyadic motion capture
database," Language resources and evaluation, 2008.
[22] Amir Ali Bagher Zadeh, Paul Pu Liang, Soujanya Po-
ria, Erik Cambria, and Louis-Philippe Morency, "Multi-
modal language analysis in the wild: Cmu-mosei dataset
and interpretable dynamic fusion graph," in Proceedings
of the 56th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers), 2018.
[23] Reza Lotfian and Carlos Busso, “Building naturalistic
emotionally balanced speech corpus by retrieving emo-
tional speech from existing podcast recordings," IEEE
Transactions on Affective Computing, 2017.
[24] Il-Young Jeong and Jeongsoo Park, "Cochlscene: Ac-
quisition of acoustic scene data using crowdsourcing,”
in Asia-Pacific Signal and Information Processing As-
sociation Annual Summit and Conference, 2022.
[25] Keith Ito and Linda Johnson,
"The lj
speech dataset," https://keithito.com/
LJ-Speech-Dataset/, 2017.
[26] Dima Damen, Hazel Doughty, Giovanni Maria
Farinella,, Antonino Furnari, Jian Ma, Kazakos, et al.,
"Rescaling egocentric vision: Collection, pipeline
and challenges for epic-kitchens-100," International
Journal of Computer Vision (IJCV), 2022.
[27] Lucas Smaira, João Carreira, Eric Noland, Ellen Clancy,
Amy Wu, and Andrew Zisserman, "A short note on the
kinetics-700-2020 human action dataset," 2020.
[28] Karol J. Piczak,
"ESC: Dataset for Environmental
Sound Classification," in Proceedings of the 23rd An-
nual ACM Conference on Multimedia. pp. 1015–1018,
ACM Press.
[29] Justin Salamon, Christopher Jacoby, et al., "A dataset
and taxonomy for urban sound research,” in 22nd ACM
international conference on Multimedia, 2014.
[30] Annamaria Mesaros, Aleksandr Diment, Benjamin
Elizalde, Toni Heittola, et al., “Sound event detection
in the dcase 2017 challenge,” IEEE/ACM Transactions
on Audio, Speech, and Language Processing, 2019.
[31] Yuan Gong, Jin Yu, and James Glass, “Vocalsound: A
dataset for improving human vocal sounds recognition,"
in IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 2022.
[32] Tito Spadini, “Sound events for surveillance applica-
tions," Oct. 2019.
[33] Benjamin Elizalde, Radu Revutchi, Samarjit Das, Bhik-
sha Raj, Ian Lane, and Laurie M Heller, “Identifying
actions for sound event classification,” in 2021 IEEE
Workshop on Applications of Signal Processing to Au-
dio and Acoustics (WASPAA). IEEE, 2021, pp. 26–30.
[34] Steven R. Livingstone and Frank A. Russo, "The Ry-
erson Audio-Visual Database of Emotional Speech and
Song (RAVDESS)," Apr. 2018.
[35] Minkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh, “Pre-
fix tuning for automated audio captioning,” in IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing (ICASSP), 2023.
