arXiv:2309.04663v2 [cs.CL] 12 Sep 2023
FIAT:
FUSING
LEARNING PARADIGMS WITH
INSTRUCTION-ACCELERATED TUNING
Xinyi Wang, John Wieting, Jonathan H. Clark
Google DeepMind
{xinyiwang, jwieting, jhclark}@google.com
1
ABSTRACT
Learning paradigms for large language models (LLMs) currently tend to fall within
either in-context learning (ICL) or full fine-tuning. Each of these comes with their
own trade-offs based on available data, model size, compute cost, ease-of-use, and
final quality with neither solution performing well across-the-board. In this article,
we first describe ICL and fine-tuning paradigms in a way that highlights their natural
connections. Based on these connections, we propose a new learning paradigm
called FIAT¹ that fuses² the best of these paradigms together, enabling prompt-
engineered instructions and chain-of-thought reasoning with the very largest models
while also using similar methods to perform parameter updates on a modestly-sized
LLM with parameter-efficient tuning. We evaluate FIAT's effectiveness on a variety
of multilingual tasks³ and observe that FIAT performs better than both ICL and
fine-tuning at scales ranging from 100–10,000 training examples. We hope that
FIAT provides a practical way of harnessing the full potential of LLMs without
needing to make a hard choice between learning paradigms.
INTRODUCTION
Large language models (LLMs) show impressive generalization ability to new tasks and languages.
Some of their most exciting capabilities, such as producing logical reasoning to solve a problem, are
found to emerge only when the model size is over a certain threshold, often hundreds of billions of
parameters (Wei et al., 2022b;a). The impressive capabilities of these models to produce high-quality
responses without any task-specific tuning along with the very high cost of further tuning such models
has led much recent work to focus on the paradigm of In-Context Learning (ICL)—placing a few
task-specific examples and instructions into the model's input (Brown et al., 2020; Chowdhery et al.,
2022; Google et al., 2023; OpenAI, 2023).
Although prior work has seen that fine-tuning a model on task data can often lead to superior
performance on the downstream task compared to ICL (Scao & Rush, 2021; Schick & Schütze,
2020a;b; Asai et al., 2023), there are significantly fewer recent efforts on fine-tuning models for
tasks with limited data, perhaps because the time and compute costs associated with tuning a very
large model drives practitioners toward smaller models, abandoning the ability to take advantage of
emergent model capabilities.
ICL and model fine-tuning each come with their own trade-offs. ICL does not incur any training
cost and it allows one to utilize the most capable LLMs (Schick & Schütze, 2020b; OpenAI, 2023).
However, while ICL can achieve competitive performance on many tasks with a handful of annotated
examplars, it often requires very large models to work well and it cannot take advantage of additional
training examples if they do not fit into the context window. For many tasks, this leads to ignoring a
substantial amount of potentially-useful training examples. Fine-tuning, on the other hand, is not
constrained by the need to fit training examples into the model's input, and it can be quite effective
"We derive the name FIAT from Fusing Learning Paradigms with Instruction Accelerated Tuning.
2 FIAT fuses not only the learning paradigms but the models themselves.
3 We say that these tasks are naturally low-data because no additional data is available for such languages and
it's non-trivial to obtain more; we contrast this with artificially low-data scenarios where large data exists, but is
ignored.
1
Updated by humans
by prompt engineering
Instruction-tuned LLM
X
Input Sequence
A
Few-shot In-Context Learning
Instruction-tuned LLM
CoT
Reason
IB
x
θρ
Input Sequence
Updated by humans
by prompt engineering
y
x
Updated by gradient
descent on task data
LLM
Full Fine-tuning
Small parameter updated by
gradient descent on task data
Instruction-tuned LLM
ŷB
X
PEFT
0.
y
Input Sequence
y
FIAT using both Paradigms
Figure 1: Overall flow of FIAT and how it compares to ICL and fine-tuning. The colored components
are updated while building and learning a task-specific instance of FIAT, while other components are
fixed.0B is the parameters of the larger LLM and I ẞ are the instructions used to induce reasoning; +
are the parameters of a moderately-sized LLM to be tuned and I, is its instructions, which helps the
model predict the correct final answer.
even with smaller language models. These trade-offs tend to lead practitioners to arbitrarily pick a
paradigm or run costly experiments on these disparate methods in order to choose the best approach.
We instead take the view that these two model learning paradigms are in fact complementary. To
this end, we propose FIAT―Fusing Learning Paradigms with Instruction-Accelerated Tuning (FIAT),
which utilizes both ICL on very large models and parameter tuning on moderately-sized LLM while
fusing the common techniques associated with each paradigm. FIAT uses hand-engineering instruction
prompts that elicit chain-of-thought reasoning from a very large model, while also using the generated
reasoning and instruction prompts to tune a moderately-size LLM with parameter-efficient tuning.
Figure 1 shows the workflow of FIAT and how it compares to ICL and fine-tuning.
In the remainder of this article, we formally describe the connections between ICL and fine-tuning,
along with the various techniques that have developed within each paradigm (§2); we propose FIAT,
which fuses the best of these together and avoids many of the pitfalls of each of the individuals (§2.3);
we present experiments demonstrating how FIAT improves over both learning paradigms in data
scenarios ranging from 100–10,000 examples along with ablations detailing where these gains come
from (§3).
2
LEARNING PARADIGMS FOR LLMS
In this section, we review two popular learning paradigms for LLMs (ICL in §2.1 and parameter
tuning in §2.2) while considering their strengths and weaknesses, which directly lead to FIAT (§2.3).
2.1
IN-CONTEXT LEARNING
Instructed ICL keeps the parameters of the LLM fixed, but it instead selects an instruction prompt
(often through manual optimization) to improve the accuracy of the downstream task. Formally, a
model prediction is made by sampling a very large pre-trained LLM parameterized by fixed 0 and a
textual instruction I:
P(y|x; 0, 1)
(1)
*Typically, the sampling is a simple argmax with temperature 0, though this isn't always the case as in
techniques such as majority voting.
2
While the instructions I are prefixed onto the model input x in practice, we intentionally notate them
as an argument of the model, which we argue better reflects how they are conceptualized; we will
build on this later.
Chain-of-thought reasoning pushes instructed ICL a step further by crafting I to induce step-
by-step reasoning in the output of the model that improves the model's ability to arrive at a correct
prediction (Wei et al., 2022b). This allows auto-regressive inference to output observations about
the input or solve sub-problems of the overall task that future decoding steps can leverage when
predicting the final answer; it may also elicit textual patterns that the model saw during pre-training,
that would otherwise be difficult to access in the model's latent feature space (e.g. via fine-tuning).
Few-shot ICL Few-shot ICL differs from instructed ICL in that its instructions I are composed
of a small number of examplars selected among training examples D that have been formatted as a
textual input to the model via instructions.
Instruction-tuned Base Models Instruction-tuned models such as FLAN and TO (Sanh et al., 2021;
Chung et al., 2022; Longpre et al., 2023) often provide significant improvements on ICL compared to
using a pre-trained model. This is because instruction-tuning is essentially a second stage pretraining
using a set of multitask data whose distribution is closer to the downstream task.
The ICL paradigm achieves competitive results on various tasks with no or only a handful of annotated
examples. While it does not incur any additional model tuning cost, ICL often has high inference
cost because it requires LLMs over a certain size to work well, especially when using techniques
such as chain-of-thought. It also cannot take advantage of additional task data beyond what fits into
the context window of the model.
2.2 PARAMETER TUNING
Full-Parameter Fine-tuning Given pre-trained parameters 0 of a LLM to tune, standard fine-
tuning simply optimizes all parameters of the model on task-specific supervised training data D
according to:
P(y|x; 0)
(2)
The optimization of 0 is similar in purpose to the process of human prompt engineering of I in ICL.
Since model fine-tuning does not have to fit training data into the context window of the model, it is
more effective when there are slightly more training examples available. Fine-tuning also works well
on smaller language models with enough training examples, leading to faster inference. However,
fine-tuning incurs additional training cost and requires access to model parameters, while some of the
most capable LLMs are available for inference-only API access. The model could also easily overfit
to the training examples due to catastrophic forgetting (Goodfellow et al., 2013), especially for tasks
with limited data.
Parameter-efficient Fine Tuning (PEFT) improves the tuning procedure by using a learning
parameterization PEFT where |0 PEFT | <<< |0|. Besides reducing the danger of overfitting, this learning
technique also avoids forgetting features that may be useful for generalization beyond the training set.
Similarly, ICL avoids catastrophic forgetting by only modifying the input to the model while keeping
the parameters fixed.
2.3 FUSING LEARNING PARADIGMS WITH FIAT
In this section, we construct FIAT, motivating the purpose of each design choice in terms of modeling
capabilities. ICL and fine-tuning each have compelling strengths along with pitfalls, which we
summarize in Table 1. At a high level, we observe that these properties are largely complementary.
5 In practice, |0| tends to be much smaller for fine-tuning than for ICL.
3
ICL Fine-tuning
Strengths
Works well with small model
No
Yes
Supports large training data
No
Yes
Supports chain-of-thought reasoning
Yes
No
Usage of instruction prompts
Yes
No
Challenges
No parameter updates
Yes
Avoids catastrophic forgetting
Yes
ZZ
No
No
Table 1: Comparison of the ICL and fine-tuning learning paradigms, according to common usage
patterns.
•
Reflecting on these abilities of ICL and fine-tuning, we seek an approach that is capable of:
•
• Instruction following: follows human-engineered instructions to achieve high quality predictions;
Chain-of-thought reasoning: produces intermediate text that helps the model toward correct
predictions;
• Parameter tuning: refines its internal representation to align with a moderate to large number of
supervised training examples; and
• Data scaling: provides high quality models with data scales from 100 to 1000's of examples.
Model stacking via CoT-augmented Tuning We begin with the observation that chain-of-thought
prompting is typically not supervised, but rather induced via carefully-written instructions. Motivated
by this, we fuse two models for learning and inference: a big model ẞ with all the most powerful
emergent capabilities of LLMs, and a tunable model 7 whose size can be flexibly chosen depending
on the capacity needs of the task of interest. We assign the responsibility of chain-of-thought inference
to ẞ and then provide its textual predictions ŷß to the tunable model; it can then learn how to best
use these inputs (e.g. chain-of-thought explanations) based on how useful they are with regard to
predicting the supervised outputs. The parameters 0ẞ remain fixed as we do not have nor require any
directly supervised data for its sub-task.
Instruction-augmented Tuning Crafting a good instruction prompt is known to be essential to
high-quality ICL performance, and so we naturally include instructions Iẞ to generate reasoning and
explanations as a first step. Although instructions are typically not used for smaller tunable model IT,
we observe that instructions have the potential to benefit tuning as well. We speculate that instructions
help better align a task's inputs with the distribution seen during pre-training, allowing the model
to not only converge faster but also make fewer parameter updates. This, in turn, avoids the risk of
catastrophic forgetting associated with excessive parameter updates. Therefore, FIAT also provides
separate instructions I, for the tunable model.6
Pervasive Instruction-tuned Models Already, instruction-tuned models have become the standard
for ICL; we use such models as 0ẞ in all of our experiments. However, given FIAT's use of Instruction-
augmented Tuning, we also depart from the common practice of fine-tuning starting from models
pre-trained primarily on span corruption objectives and instead initialize with instruction-tuned
checkpoint (Longpre et al., 2023). This makes optimization easier since the model is already
expecting instructions; this can be especially beneficial in limited training data scenarios.
Parameter-efficient Tuning So far, we have added chain-of-thought reasoning, instruction follow-
ing in tuning, and instruction-tuned initialization to FIAT's design, all of which move the pre-tuning
model and the task definition toward each other in terms of increasing the probability of the desired
output. We hypothesize that parameter-efficient tuning is a particularly good fit for optimizing 0
in FIAT over the training data, because large changes to the model parameters 0, should not be
"In FIAT, instructions can be viewed as serving purpose analogous to a Bayesian prior in earlier statistical
learning methods: They allow encoding human knowledge into the learning procedure alongside supervised data
that empirically estimates parameters. However, textual instructions are a far more natural way of doing this
than the hyperparameters of a Dirichlet.
4
Algorithm 1: Model building with FIAT
Input: 0, 0, D
Output: 0, Iẞ, IT
// Write reasoning instructions & select exemplars.
IB = PROMPTENGINEERING (D, 0B)
// Write tuning instructions, based on large model.
IT
PROMPTENGINEERING (D, 0B)
// Initialize parameter-efficient tuning.
OPEFT
T
← INIT(07)
// Iterate over examples or batches of data.
for x, y ED do
end
// Generate expansions, explanations, reasoning.
Ув
= arg max, P(y|x; 0ß, Iß)
// Optimize using parameter-efficient update.
97 = ▼ PEFTP(y|x, ŷß; 07, 0PEFT, IT)
OPEFT ← UPDATE (OPEFT, gr)
// Apply PEFT updates to final tuned model.
04 — 07 OPEFT
Algorithm 2: Inference with FIAT
Input: x, I, I, 0ß, 0
Output: y
// Generate expansions, explanations, reasoning.
= arg max, P(y|x; ¤ß, Iß)
B₂
// Infer final output using tuned model.
y = arg max, P(y|x, ŷß; ¤, I+)
Figure 2: Model building and inference with FIAT. Left: Model building with FIAT begins with
interactive prompt engineering of the instructions I. Iẞ specifies how to perform reasoning using
few-shot exemplars on 08-i.e. behaviors for which we have no large-scale annotations, while I
specifies guidance to the tuned model 0, for using the generated reasoning and input to produce a
final output. Both 03 and 0, are instruction-tuned models and only ¤ is updated during training via
parameter-efficient tuning. Right: Inference with FIAT is very simple, requiring only: (1) a call to
the large generative model using the fixed pre-trained parameters 0ẞ and the reasoning instructions
IB; and (2) a call to the tuned model 0, along with the associated task instructions I.
necessary given a good initialization.7 Formalizing all the above modifications, we arrive at the final
formulation of FIAT used for fine-tuning and inference in Alg. 1 and Alg. 2.
3
EXPERIMENTS
Datasets One of our primary objectives in selecting datasets that naturally cover a broad variety
of training data sizes. We consider tasks ranging from classification to exercising a model's ability
to generate short answers, and we include a large number and variety of languages to evaluate the
generality of the method.
First, we use XOR-ATTRIQA (Muller et al., 2023), a classification task where model is asked to
predict whether the provided answer to the question is supported by the given passage context, which
includes 5 languages with 262 examples total. We refer to this as the O(100) data scenario.
We also study FIAT's behavior on the Cross-lingual QA task of XTREME-UP (Ruder et al., 2023).
This data is an expansion of the XOR QA³ dataset (Asai et al., 2020), a cross-lingual variant of the
TyDi QA (Clark et al., 2020) dataset. This task asks a model to predict the correct English answer span
given a non-English question and an English answer passage; this task also includes the possibility
that the passage does not contain a correct answer, making it more challenging. Cross-lingual QA is
a particularly important task for languages that have very little answer content as it enables providing
answers to questions that would otherwise be unanswerable using only in-language content. We
provide results on two focus sets. First, we use the subset of 20 Indic languages in XTREME-UP
Cross-lingual QA where each language has about 300 examples, to allow for studying a scenario with
7In FIAT, we use LoRA (Hu et al., 2021) to parameterize the tuning procedure because it does not induce
additional inference cost. Future work should consider other methods such as soft prompt tuning (Lester et al.,
2021).
8XOR QA stands for cross-lingual open-retrieval question answering; note the difference between XOR QA
and XOR-ATTRIQA.
5
XOR-ATTRIQA
XTREME-UP
Cross-lingual QA (Indic)
XTREME-UP
Cross-lingual QA (Full)
07
Өв
Method
O(100)
Acc/AUC-PR
O(1000)
O(10000)
F1
F1
L
ICL
78.6/—†
68.9
69.2
Fine-tune
90.5 / 52.1
63.5
75.5
XS
L
FIAT
94.0/78.1
73.6
77.8
Fine-tune
S
L
FIAT
90.6 / 54.5
93.9/77.5
67.1
77.8
77.3
79.3
Gain over best baseline +3.5 / +26.0 (vs S fine-tune)
+8.4 (vs ICL)
+1.5 (vs S fine-tune)
Table 2: Overall results of FIAT and typical baselines. While we provide improvements with regard
to the best baseline, we also point out that the best baseline often differs between ICL and fine-tuning,
especially at smaller model sizes; this leaves practitioners to empirically determine the best course of
action. AUC-PR is not computed for the ICL because outputs are text-only.
moderate data; we refer to this as the O(1000) data scenario. We also study the full XTREME-UP
Cross-lingual QA task which has 22,500 examples across 27 languages where the 5 high-resource
languages have more than 2500 examples each; we refer to this as the O(10,000) data scenario.⁹
Together, these tasks allow us to test our methods on three different data size scenarios from small
100's to over training 20,000 examples. Details of the languages and the dataset size can be found in
App. A.1.
Models We use PaLM-2 (Google et al., 2023) as our base model, and we experiment with
instruction-tuned models using the FLAN mixture (Chung et al., 2022). We use PaLM-2 L as
MB and we use PaLM-2 XS and S for Mr.
Baselines We compare to both ICL and fine-tuning baselines. For ICL, we use PaLM-2 L with chain-
of-thought reasoning (Wei et al., 2022b). We include 4 few-shot exemplars with hand-written chain-
of-thought explanations in English for each of the 5 languages in the XOR-ATTRIQA Attribution
task. 10 for a total of 20 exemplars. However, for XTREME-UP cross-lingual QA, it was not feasible
to hand-engineer prompts for each of the 27 languages. Therefore, we hand-write 4 chain-of-thought
explanations based on Bengali exemplars,¹¹ and use the same ICL examples for all 20 languages.
3.1 RESULTS
11
We present the performance of the baselines (ICL and fine-tuning) and our FIAT framework for all
three data settings in Table 2. We show the average scores across all languages in each dataset for
simplicity, and we provide the result for each language in App. A.2. Looking at the baselines, we find
that few-shot ICL using PaLM-2 L model is quite competitive without any additional model tuning,
but still lags behind PaLM-2 S fine-tuned on a relatively small amount of task data. However, we
find that the best baseline differs between ICL and fine-tuning PaLM-2 XS across different tasks and
data size settings. If one were choosing between just ICL or fine-tuning, this inconsistency makes it
difficult to determine the best course of action without empirical comparisons. On the other hand,
FIAT offers the best performance by combining the strengths of both ICL and fine-tuning.
4 ABLATIONS AND ANALYSIS
In this section, we study the effect of individual design decisions within FIAT and present the results
in Table 3, and drawing conclusions from them below. In the end, we find that while certain design
"We report the average result on the under-represented languages, following the recommendations of the
XTREME-UP benchmark.
1º During manual prompt engineering, we used Google Translate to assist with explanation annotation.
11 Note that while the exemplars have Bengali questions, we instruct the model to carry out its reasoning in
English.
6
XOR-ATTRIQA
XTREME-UP
XTREME-UP
Cross-lingual QA: Indics
Cross-lingual QA: Full
0
Өв
Method
O(100)
Acc/AUC-PR
O(1000)
O(10000)
F1
F1
L
Few-shot ICL
78.6/-
68.9
69.2
L
FIAT
94.0/78.1
73.6
77.8
XS
w/o CoT-augmentated tuning
w/o Instruction-augmented tuning
94.0/ 80.3
70.7
76.0
93.5/72.4
69.8
76.4
L
w/o Parameter-efficient tuning
w/o Instruction-tuned base model
FIAT
93.7/69.8
67.8
75.8
90.5 / 52.1
63.5
75.5
93.9/77.5
77.3
79.3
w/o CoT-augmentated tuning
94.7/80.7
76.7
79.8
S
w/o Instruction-augmented tuning
94.1/71.6
75.3
79.1
w/o Parameter-efficient tuning
w/o Instruction-tuned base model
94.7/76.2
90.6/54.5
72.3
78.5
67.1
77.8
Table 3: Ablations showing the contribution of each modification within the FIAT recipe; each
removal is cumulative with the one above. We observe that each modification tends to make a
substantial positive impact on at least one scenario. The bottom line in each block is equivalent to
traditional fine-tuning.
choices tend to have a larger effect on some settings than others, each tends to have substantial
contributions in some area, and together the overall modeling recipe is very effective as a whole.
Instructed-tuned base models improve final quality of fine-tuned models. The instruction-tuned
Flan XS model improves over the base model on all datasets, especially on XOR-ATTRIQA and
XTREME-UP Cross-lingual QA Indic, where the total amount of task data is around O(100) to
O(1000). This indicates that instruction-tuned models are not only beneficial for ICL, but can
also be beneficial for fine-tuning on limited data (Longpre et al., 2023). However, the advantage
of instruction-tuned model on XTREME-UP Cross-lingual QA decreases from the Indic (O(1000)
training examples) to Full (O(10000) training examples), indicating that instruction-tuned model is
less helpful when the fine-tuning dataset is large.
Instruction-augmented Tuning generally leads to significant improvements. Adding an appro-
priate prompted format to the task data is generally beneficial for all tasks. This result indicates that
prompt engineering is not only helpful for direct few-shot ICL, but also has a positive impact on
model fine-tuning. Prompted tuning is especially helpful for XOR-ATTRIQA and XTREME-UP Cross-
lingual QA Indic, where the amount of task data is very limited. This is because the prompt format
aligns the distribution of downstream task closer to the model pretraining distribution, which allows
the pretrained model to generalize to the downstream task with a small amount of task examples.
CoT-augmentated Tuning is helpful for most tasks. Our CoT-augmented Tuning can lead to
large improvement for Xtreme-UP Cross-lingual QA Indic task. Surprisingly, it does not help XOR-
ATTRIQA, which is contradictory to findings from prior works which show that explanations can be
especially helpful for classification tasks (Hsieh et al., 2023; Zhou et al., 2023). We hypothesize that
this is because the model already performs quite well on XOR-ATTRIQA without having access to
the explanations (over 90 percent accuracy) and this task may be reaching its saturation point.
CoT-augmented Tuning is even more helpful for tasks and languages with lower performance.
We analyze the relationship between the gains brought by CoT-augmentated Tuning on the XTREME-
UP Cross-lingual QA tasks. Figure 3 shows the improvement in F1 score of different languages
versus a baseline model's F1 score that lacks CoT-augmented Tuning. We can see that there is an
inverse relationship between the benefit of CoT-augmented Tuning and the baseline model score,
indicating that CoT is more beneficial for harder tasks or languages where the model could not
perform well without the help of the CoT augmentation. This means that while we see meaningful
gains in aggregate, for individual languages (or, more generally, individual tasks and use cases), CoT
can have an out-sized impact on quality.
7
F1
A F1 Score with CoT
10.0-
7.5
5.0
2.5
0.0
60
F1 Score
80
Figure 3: Gains in F1 on XTREME-UP Cross-
lingual QA with CoT-augmented Tuning. The
lower performing languages tend to benefit
more from CoT augmentation.
Method
F1
Gains
Baseline
70.7
Distilled CoT (Hsieh et al., 2023)
Our CoT-augmented Tuning
72.5
+1.8
73.6
+ 2.9
Figure 4: Performance on XTREME-UP
Cross-lingual QA Indic compared to the base-
line without CoT. Our CoT-augmented Tun-
ing method significantly outperforms previ-
ous methods on distilling CoT.
70-
70
60
60
50
Base
40
40
Prompted
30
0
500
Step
1000
Gain over baseline
1.5
1.0
XOR QA Indic
Attribution
Base
FLAN
Gain over baseline
0.5-
Base
FLAN
0.0
0
F1
AUC-PR
Figure 5: The validation F1 score through-
out training on XTREME-UP Cross-lingual
QA for methods with and without Instruction-
augmented Tuning. Instruction-augmented
Tuning out-performs baseline and it has much
better performance at step 0, before any
model optimization.
Figure 6: Improvement with Instruction-
augmented Tuning for the model with
and without instruction-tuning. Instruction-
augmented Tuning is generally helpful for
both types of models, and it tends to be more
beneficial for instruction-tuned models
CoT-augmented Tuning leads to better quality than CoT distillation. Recent work proposed
distilled CoT, which uses the explanation as a multitask output target, so that the model does not need
to generate additional explanations at test time (Hsieh et al., 2023). Here we compare the performance
of these two different ways of using the CoT explanations and list the performance on cross-lingual
QA tasks in Figure 4. Despite incurring higher inference cost, our CoT augmentation method further
out-performs the distilled CoT by a large margin on the harder XTREME-UP Cross-lingual QA Indic
task. In general, we view distillation as an orthogonal technique to FIAT, which is aimed at efficiency
over quality.
Adding instructions to tuning helps from beginning to end. In Figure 5, we plot the training
curves of Flan PaLM-2 S model with and without Instruction-augmented Tuning. We can see
that adding instructions to tuning leads to much better performance at step 0, before any model
optimization. This indicates that adding the instructions to the task data during fine-tuning 12 can
significantly improve the zero-shot performance of the model, probably because it makes the task
12 Note we use the term instruction-augmented tuning to differentiate from the separate concepts of
instruction-tuned base models, which creates base models that are better able to follow instructions of
specific tasks later, and prompt tuning, which learns soft prompt embeddings.
8
data more similar to the data used in the instruction tuning stage. Importantly, this also implies that
the model parameters don't need to move as far away from their starting point in order to achieve the
same level of quality, reducing the risk of catastrophic forgetting. However, the model does not only
reach the same level of quality with less steps, but also manages to exceed the quality of a model
without instructions.
Instruction-augmented Tuning helps more with an instruction-tuned base model.
We compare
the effect of prompted tuning on models with and without instruction tuning. Figure 6 shows that
prompted tuning generally brings improvements for both the base model without instruction tuning
and the Flan model with instruction tuning, while the gains on the instruction-tuned Flan model tend
to be slightly larger and more consistent. This is likely because the data format we used for prompted
tuning (task instructions followed by the input) is more similar to the Flan data mixture used for
instruction tuning.
5 RELATED WORK
Instruction Tuning Instruction-tuned models (Wei et al., 2021; Longpre et al., 2023) often have
better performance for few-shot ICL tasks than base language models since they are already primed
to following instructions due to being fine-tuned on a diverse set of tasks. Using instruction-tuned
models is a key component of FIAT.
In-Context Learning In in-context learning, the parameters of the LLM remain fixed and a prompt
containing a few examples along with reasoning steps is used to prime the model for solving similar
tasks (Nye et al., 2021; Wei et al., 2022b). In-context learning works best for large language models.
FIAT uses this capability of large language models, along with fine-tuning, to power small language
models in the low-data regime.
Knowledge Transfer from Larger to Smaller LLMs A popular prior method for transferring
knowledge from large models to smaller ones is model distillation (Hinton et al., 2015), where the
outputs of a larger model are used as a training signal for a smaller one. Other approaches include
using the larger language model to generate data and then using this data to train smaller models.
More recently, the latter has approach has been extended to generate reasoning steps which are
provided as fine-tuning data for the smaller language model (Magister et al., 2022; Huang et al., 2022;
Li et al., 2022; Ho et al., 2023; Hsieh et al., 2023; Fu et al., 2023; Zhu et al., 2023; Li et al., 2023).
Under-represented Languages Most work that trains large language model and uses them for
downstream tasks focus on English or the collection of 100 or so languages where there are large,
easily available corpora (ImaniGooghari et al., 2023). Tail languages have often been ignored by
language technologies due to lack of available corpora (Nayak & Joshi, 2022). Recent works has
focused on tail languages outside of these head languages (Bapna et al., 2022; Ruder et al., 2023). In
this work, we make the low-data regime the focus of our efforts, which is especially useful for tail
languages.
Fine-tuning smaller LLMs While fine-tuning with prompts has been studied for encoders pre-
trained with masked language modeling objectives (Scao & Rush, 2021), we show that it is also
important to fine-tuning generative language models. For example, some works show that fine-tuning
a smaller language model is a more competitive and efficient method for practical low-data learning
problems than few-shot ICL (Asai et al., 2023; Ruder et al., 2023). Agrawal et al. (2022) propose to
synthetic QA data generated from very large LLM to improve the performance of a smaller model.
6 CONCLUSION
We have presented FIAT, a method that fuses the ICL and fine-tuning learning paradigms and leads to
improved model predictions across a variety of data scenarios, ranging from 100–10,000 training
examples. We hope FIAT provides a practical way of harnessing the full potential of LLMs without
needing to make a hard choice between learning paradigms.
9
REFERENCES
Priyanka Agrawal, Chris Alberti, Fantine Huot, Joshua Maynez, Ji Ma, Sebastian Ruder, Kuzman
Ganchev, Dipanjan Das, and Mirella Lapata. Qameleon: Multilingual qa with only 5 examples.
arXiv preprint arXiv:2211.08264, 2022.
Akari Asai, Jungo Kasai, Jonathan H Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. Xor
qa: Cross-lingual open-retrieval question answering. arXiv preprint arXiv:2010.11856, 2020.
Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia
Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. Buffet: Benchmarking large language models
for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023.
Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng
Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, et al. Building machine translation
systems for the next thousand languages. arXiv preprint arXiv:2205.03983, 2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416, 2022.
Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev,
and Jennimaria Palomaki. Tydi qa: A benchmark for information-seeking question answering in ty
pologically di verse languages. Transactions of the Association for Computational Linguistics, 8:
454-470, 2020.
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language
models towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023.
Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investi-
gation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211,
2013.
Google, Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre
Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical
report. arXiv preprint arXiv:2305.10403, 2023.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In
Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 14852–14882, Toronto, Canada, July 2023. Association for Computational
Linguistics.
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner,
Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger
language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301,
2023.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685, 2021.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.
Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.
10
Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sa-
bet, Nora Kassner, Chunlan Ma, Helmut Schmid, André FT Martins, François Yvon, et al.
Glot500: Scaling multilingual corpora and language models to 500 languages. arXiv preprint
arXiv:2305.12182, 2023.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691, 2021.
Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Sym-
bolic chain-of-thought distillation: Small models can also" think" step-by-step. arXiv preprint
arXiv:2306.14050, 2023.
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,
Baolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners better.
arXiv preprint arXiv:2210.06726, 2022.
Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V
Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective
instruction tuning. arXiv preprint arXiv:2301.13688, 2023.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.
Teaching small language models to reason. arXiv preprint arXiv:2212.08410, 2022.
Benjamin Muller, John Wieting, Jonathan H Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Baldini
Soares, Roee Aharoni, Jonathan Herzig, and Xinyi Wang. Evaluating and modeling attribution for
cross-lingual question answering. arXiv preprint arXiv:2305.14332, 2023.
Ravindra Nayak and Raviraj Joshi. L3Cube-HingCorpus and HingBERT: A code mixed Hindi-
English dataset and BERT language models. In Proceedings of the WILDRE-6 Workshop within
the 13th Language Resources and Evaluation Conference, pp. 7–12, Marseille, France, June 2022.
European Language Resources Association.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David
Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:
Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,
2021.
OpenAI. Gpt-4 technical report, 2023.
Sebastian Ruder, Jonathan H Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia,
Shruti Rijhwani, Parker Riley, Jean-Michel A Sarr, Xinyi Wang, et al. Xtreme-up: A user-centric
scarce-data benchmark for under-represented languages. arXiv preprint arXiv:2305.11938, 2023.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables
zero-shot task generalization. arXiv preprint arXiv: 2110.08207, 2021.
Teven Le Scao and Alexander M Rush. How many data points is a prompt worth? NAACL, 2021.
Timo Schick and Hinrich Schütze. Exploiting cloze questions for few shot text classification and
natural language inference. arXiv preprint arXiv:2001.07676, 2020a.
Timo Schick and Hinrich Schütze. It's not just size that matters: Small language models are also
few-shot learners. arXiv preprint arXiv:2009.07118, 2020b.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652, 2021.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682, 2022a.
11
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
Neural Information Processing Systems, 35:24824–24837, 2022b.
Yangqiaoyu Zhou, Yiming Zhang, and Chenhao Tan. Flame: Few-shot learning from natural language
explanations. Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics, 2023.
Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou. Pad: Program-aided
distillation specializes large models in reasoning. arXiv preprint arXiv:2305.13888, 2023.
12
Split
bn
fi
ja
ru
te
Train
40
66
20
84
52
Validation 218
Test
150 578 136 174
2822 1318 1908 1268 2146
Table 4: Dataset size for XOR-ATTRIQA.
Split
as bho brx gbm gom gu
hi hne kn mai ml mni mr mwr or pa ps
sa
ta
ur
ar bn
fi
ja
ko
ru
te
Validation 356 358 359 365 366 370 310 372 373 360 375 330 335 386 386 385 384 385 386 387 388 3468 272 361 2530 368
Table 5: Dataset size for XTREME-UP Cross-lingual QA.
A APPENDIX
A.1 LIST OF LANGUAGES FOR EACH TASK
We provide the number of training, validation, and test examples for each task in Table 4 and Table 5.
A.2
LANGUAGE-WISE BREAKDOWN OF THE RESULTS
We provide the performance for each language in Table 6, Table 7, and Table 8.
Mr MB Method
L
Few-shot ICL
L
FIAT
XS
w/o CoT-Augmented Tuning
w/o Instruction-Augmented Tuning
w/o Parameter-efficient Tuning
w/o Instruction-tuned base model
L
FIAT
S
w/o CoT-Augmented Tuning
w/o Instruction-Augmented Tuning
w/o Parameter-efficient Tuning
w/o Instruction-tuned base model
bn
fi
ja
Acc/AUC-PR
ru
te
85.9/- 78.5/-
92.6/81.1 91.0/85.3
92.5/84.7
85.4/--
96.3 / 66.5
91.8/85.8
96.2/70.3
84.5/-
94.8/84.9 95.3/72.5
94.6/84.1 95.0/76.6
58.9/-
91.7/74.1
92.6/73.9
89.4 / 65.6
92.3/81.3
93.0/84.3
93.1/75.6
92.7/76.2
90.9 / 66.3
91.2/81.4
95.9/53.5
93.8/77.4 94.8/75.4
92.0/76.7
95.0/55.8
94.2/74.1 94.7/68.6
88.9/65.9 94.3 / 42.1
90.1/58.6 89.7/28.2
92.184.0 96.2/62.4 94.6/84.9 94.0/93.9
94.4/81.2 95.5/58.8 98.8/87.4 95.3/78.4
92.7/82.9 95.0/51.3 94.6/78.1
93.2/83.6 96.3/59.0 95.1/83.3
88.6/67.7 93.2/41.0 89.7/57.5
95.2/70.1
96.5/78.8
90.3 / 40.2
Table 6: Results on each language for XOR-ATTRIQA.
13
M, MB
Method
L
Few-shot ICL
L
FIAT
XS
w/o CoT-Augmented Tuning
w/o Instruction-Augmented Tuning
w/o Parameter-efficient Tuning
w/o Instruction-tuned base model
L
FIAT
S
w/o CoT-augmented Tuning
w/o Instruction-Augmented Tuning
w/o Parameter-efficient Tuning
w/o Instruction-tuned base model
_ as bho brx gbm gom gu hi han kn mai Filmni mr mwr or pa ps sa ta ur
72.5 61.8 43.0 60.3 72.3 70.6 61.5 70.8 72.9 73.3 72.2 57.1 71.5 69.5 71.4 73.7 70.6 72.6 71.5 69.4
75.9 73.9 47.2 72.7 76.1 76.1 79.3 76.2 76.6 75.5 76.3 61.1 75.4 73.3 76.0 75.6 76.6 77.4 75.4 73.3
65.6 64.7 49.3 60.3 62.6 65.7
73.2 71.5 39.1 67.8 71.7 73.7 78.5 70.3 74.0 71.2 74.7 50.1 73.9 71.4 70.9 72.2 72.8 71.8 74.5 72.48
70.7 69.5 49.2 65.7 70.7 80.5 67.4 69.9
69.7 70.9
51.6 70.0 67.8 66.8 69.5 69.7 68.7 70.9 69.8 67.8
76.9 63.2 65.2 63.7 65.4 52.8 64.2 63.5 63.8 65.8 64.3 63.7 65.4 64.4
80.2 77.8 52.2 77.2 78.3 80.6 82.2 79.5 79.7 78.8 79.8 64.5 79.4 77.4 79.4 80.7 80.0 80.4 79.8 78.0
79.1 78.4 50.3 75.6 78.7 79.9 84.6 77.8 79.2 78.3 79.2 62.4 77.8 77.7 79.6 79.2 78.8 79.9 80.1 78.0
74.3 71.2 506 717 72.7 746 818 727 75.1 74.1 74.9 51.9 73.9 721 758 73.5 73.5 726 73.6 735
Table 7: Results on each language for XTREME-UP Cross-lingual QA Indic.
ра
ps
sa
ta
ur
ar
fi
ja ko
ru
te
70.8 72.9 73.3 72.2 57.1 71.5 69.5 714 73.7 70.6 72.6 71.5 69.4 66.0 75.2 65.5 60.3 61.2 66.9 68.7
80.5 80.8 79.0 79.6 65.6 79.6 78.7 79.8 79.1 80.1 79.5 79.8 78.3 83.7 84.6 82.8 83.7 86.3 81.6 82.4
75 4 79.0 55.2 77.8 75.9 75.8 78.7 78.1 78.3 80.5
76.1 80 4 58.3 78.7 76.2
77 3 78.8 61.9 771
75.5
bho brx gbm gom
gu
hi hne kn mai ml mni mr
mwr
M-
Ма Method
F1
L
Few-shot ICL
L
FIAT
w/o CoT-augmented Tuning
XS
w/o Instruction-augmented Tuning
78.8
w/o Parameter-efficient Tuning
78.3
72.5 61.8 43.0 60.3 723 70.6 61.5
80.1 80.4 52.6 77.0 78.9 80.7 85.2
79.8 76.8 49.1 71.9 76.5 78.1 84 2
77.8 49.2 72.8 77.0 78.7 839
75.6 55.4
77.5 79.0
76.8 80.1
75.0 78.0 84.9 76.5 78 9
w/o Instruction-tuned base model
76.9 76.4 56.6 73.1
742 76.8 84.7 75.4 77.9
78.1 62.8 77.5 74.3
78.1 83.5
77.1 78.6 76.8 79.1 79.4 79 4 84.5
75.9 78.4 76.9 76.6 79.8 77.8 84.3
74.7 77.5 76.5 75.3 77.5 75.8 82.4
L
FIAT
S
w/o CoT-augmented Tuning
w/o Instruction-augmented Tuning
w/o Parameter-efficient Tuning
w/o Instruction-tuned base model
85.0 82.1 82.3 85.9 80.8 81.1
84 6 81.5 82.6 87.0 81.7 80 8
83.5 819 83.2 88.1 82.0 81.3
84 2 81.2 82.8 88.1 80.4 80.3
86.2 82.0 83.7 87.1 83.3 86.2
83 9
83.8 86.4 84.1
84.9 892 85.7
81.6 80.5 51.9 78.3 80.2 82.3 85.8 81.2 82.4 82.1 81.5 67.0 82.1 80.2 81.6 80.9 81.5 82.2 82.3 79.5 82.5
82.8 80.5 49.9 78.0 80.0 834 859 80.4 82.7 80.5 83.7 64 9 81.5 80.2 82.0 82.0 83.0 82.4 80.0 84.2 86.6 81.9 82.4 87.0
81.3 80.0 51.2 78.3 78 4
82.0 85.7 80.5 81.2 80.3 81 8 64.8 81.0 79.7 81.2 80.5 80.7 80.5 81.6 794 82.8 85.7 83.3
79.5 77.5 61.5 77.3 78.3 80.1 85.3 79.0 79.9 79.0 80.5 68.9 79.0 78.4 79.8 78.8 78.7 78.9
78.3 83.3 85.1 84.1
79.5 77.4 55.4 75.6 79.1 79.9 85.5 77.5 80.7 78.5 80.3 63.4 79.5 77.8 78.8 78.6 78.7 78.8 80.7 77.7 81.9 85.8 84.0
80.6
84 0
82.4
85.0 88.8 91.9 82.1
Table 8: Results on each language for XTREME-UP Cross-lingual QA All.
14
