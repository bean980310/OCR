arXiv:2307.11795v1 [eess.AS] 21 Jul 2023
Prompting Large Language Models with Speech
Recognition Abilities
Yassir Fathullah1,2* Chunyang Wu¹
Yuan Shangguan¹ Ke Li¹
Jay Mahadeokar¹ Ozlem Kalinli¹ Christian Fuegen¹
Meta AI¹, University of Cambridge²
yf286@cam.ac.uk, chunyang@meta.com
Egor Lakomkin¹
Jinxi Guo¹
Wenhan Xiong¹
Junteng Jia¹
Mike Seltzer¹
1
Abstract
Large language models have proven themselves highly flexible, able to solve a
wide range of generative tasks, such as abstractive summarization and open-ended
question answering. In this paper we extend the capabilities of LLMs by directly
attaching a small audio encoder allowing it to perform speech recognition. By
directly prepending a sequence of audial embeddings to the text token embeddings,
the LLM can be converted to an automatic speech recognition (ASR) system,
and be used in the exact same manner as its textual counterpart. Experiments on
Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into
the open sourced LLaMA-7B allows it to outperform monolingual baselines by
18% and perform multilingual speech recognition despite LLAMA being trained
overwhelmingly on English text. Furthermore, we perform ablation studies to
investigate whether the LLM can be completely frozen during training to maintain
its original capabilities, scaling up the audio encoder, and increasing the audio
encoder striding to generate fewer embeddings. The results from these studies
show that multilingual ASR is possible even when the LLM is frozen or when
strides of almost 1 second are used in the audio encoder opening up the possibility
for LLMs to operate on long-form audio.
Introduction
Large language models (LLMs) [4, 7, 23, 21] have been proven to be highly flexible models able
to solve a wide range of tasks. By being trained to predict the next token on a vast amount of
unsupervised text data, these systems learn to encode world knowledge in the network parameters,
useful in many downstream open-domain generative tasks such as abstractive summarization, question
answering, knowledge retrieval, text generation and machine translation.
However, interacting with LLMs purely through text can in many cases be limiting. There exists many
other structured modalities which encode information that is difficult to capture through text. For
example, audio can encode a wide range of emotions in a person's speech and images can represent
the geometry and location of objects that might be much harder to describe through text. Recently
published work have extended LLMs with the ability to ingest other modalities. The multi-modal
PALM-E [11] combined a large pretrained visual transformer [10] with the PaLM LLM [7] and were
able to achieve state-of-the-art performance on their robotics tasks. Similarly, the work of [24] utilize
a pretrained visual model and the large language model Vicuna, a derivative of LLaMA [5] in creating
an aligned model with the ability to reason with both visual and textual inputs. Furthermore [12]
propose LTU, an extension of LLaMA with an aligned audio encoder trained on an audio question
answering corpus, enabling it to reason with and understand sounds. However, LTU has limited
speech understanding and recognition abilities.
*Work done during internship at Meta AI.
1
Due to the immense number of parameters in these large language model oriented systems, it can
often be computationally impractical and expensive to adapt the whole system to new tasks. The
work of [24] trained a single projection layer which adapts the outputs of the visual encoder to be
aligned to the language model, representing a highly parameter efficient approach. However, this
severely limits the adaptability and performance of the system on new tasks. On the contrary, the
multi-modal PaLM-E [11] investigated training the whole visual encoder and language model jointly.
However, adapting the whole language model is extremely expensive and impractical. Alternative
approaches include: inserting adapter layers [20, 13] or prefix embeddings [18] which are trained on
the new task. While these approaches are effective parameter efficient approaches they increase the
inference costs. Low-rank Adaptation [14] solves these issues by using low-rank matrices to modify
some parameters of the system and has been shown to be highly promising. The approach is memory
efficient during training and does not impact inference runtime.
Contributions: In this paper we investigate equipping a large language model with speech recognition
abilities by conditioning the LLM on a variable length sequence of audio embeddings. We show that a
decoder-only large language model conditioned on the audio sequence is able to perform multilingual
speech recognition, outperforming monolingual supervised trained baselines. Furthermore, this paper
explores a range of factors that can enable better recognition performance such as the audio encoder
model size and frame rate, low-rank adaptation of LLM parameters, text token masking and the type
of large language model. Finally, by analysing the outputs of the audio encoder, we show that the
audio embeddings are similar and aligned to the text tokens.
2 Methodology
Our approach will be centered around the use of a large language model (LLM) to model sequences
of embeddings irrespective of the modality of the embedding. Inspired by the work of [11, 24] which
utilize a visual encoder to generate a fixed-length sequence of visual embeddings in the same space
as text embeddings, we utilize a pretrained audio encoder to generate a variable-length sequence
of audial embeddings. By conditioning on the audial embeddings, the large language model can be
allowed to perform speech recognition and other speech based tasks. Therefore, the only marginal
difference between a traditional LLM and the proposal is the mixing of embeddings of different
modalities.
2.1
Audial Embeddings
We use a conformer based audio encoder to produce a sequence of embeddings that will be used
to condition the LLM similar to a prompt, however, in embeddings space. To ensure the audio
encoder can extract useful embeddings it will initially be trained on a simple connectionist temporal
classification (CTC) loss. Since the sequence output of this encoder can be very long, one can further
reduce the length by stacking consecutive embeddings, resulting in larger but fewer embeddings, see
Figure 1 for the encoder structure. In this work we investigate different levels of stacking, ranging
up to embeddings that encode 960ms of audio which on average contains several tokens worth of
information in a single vector. The stacked embeddings are then projected to the hidden dimension of
the large language model to ensure they can be prepended to the text embeddings.
2.2 Large Language Model
Most experiments will utilize the smallest LLaMA-7B model [23]. The causal self-attention parame-
ters of this system will be adapted using a parameter efficient Low-rank Adaptation (LORA) [14],
keeping all other parameters frozen. In an ablation we will investigate whether any LLM parameters
need to be tuned at all to perform ASR. Furthermore, we investigate whether the choice of LLM
is important by replacing LLaMA with various BLOOM models [21]. The ASR-LLM problem
can possibly be reinterpreted as a copying/translation task where the LLM needs to regurgitate the
information in the audio sequence. If the audio encoder provides a sequence of embeddings aligned
with the text embeddings the problem collapses to a repetition task which should not require the
full capacity of an LLM. This interpretation will be investigated in Section 4. See Figure 2 for an
overview of the system.
2
10ms
80ms
百目
Filterbank Features
CNN
Conformer Encoder
HO-COD
Figure 1: Audio encoder architecture. The initial conformer is trained on a CTC loss. Thereafter the
outputs are stacked and projected to the dimension of the LLM to ensure compatibility. This figure
showcases a stacking factor of 3 resulting in 240ms embeddings.
240ms
Audio Encoder
<bos> I love playing the guitar and piano!
Text Embedding Matrix
Large Language Model
TTTTTTTT
I love playing the guitar and piano! <eos>
Figure 2: Model architecture. The embedding sequence generated from the audio encoder is directly
prepended to the text embeddings sequence. This is directly fed into the decoder-only LLM, tasked with
predicting the next token. The LLM can be frozen, adapted with parameter efficient approaches such as
LORA or fully finetuned. This work will investigate the former two.
3 Experimental Evaluation
3.1 Dataset
The Multilingual LibriSpeech (MLS) is a 50k hour ASR corpus derived from read audiobooks of
LibriVox [19]. Consisting of 8 languages: English (en), German (de), Dutch (nl), French (fr), Spanish
(es), Italian (it), Portuguese (pt) and Polish (pl) the dataset is predominately in English with 44.5k
hours. Some low-resource languages such as Portugese and Polish only have 161 and 103 hours
respectively. To account for the imbalance in the dataset we follow the strategy outlined in [9, 1] by
oversampling from the lower resource languages. Each utterance is up to 20 seconds long. None of
our reported word error rates include the use of the n-gram models provided by MLS.
3.2 Model Setup & Training Details
Audio Encoder The audio encoder operates on 80-d filterbank features with 10ms frame rate. It
consists of convolutional feature extractor with a coarse effective stride of 8 followed by linear layer
to project the output to 512 dimensions and 18 layers of non-macaron Conformer blocks. The blocks
have a hidden dimension of 512, a feed-forward net dimension of 2048, a convolutional kernel size
3
of 11 and 8 attention heads. A final linear layer is used to pretrain the audio encoder using a CTC
loss with a SentencePiece [16] vocabulary of size 1547. The final linear layer is discarded after
pretraining. Note that the effectiveness of this relatively small audio encoder of 72 million parameters
could be significantly improved by scaling the size up, reducing the level of striding and utilizing
a range of unsupervised and semi-supervised learning approaches [9, 1, 22, 2, 3, 6, 8]. However,
we restrict ourselves to a simpler setup and only use supervised learning to train our models. We
focus our attention on showing that an LLM can be conditioned to perform speech recognition and
investigate what factors improve its ability at performing this task.
Audial Embeddings The output of the encoder is a sequence of 512-d vectors with a frame rate
of 80ms. To reduce sequence length and memory consumption, every n consecutive frames are
stacked to form 512n-dimensional frames which are projected to 4096-d embeddings to match the
LLAMA-7B dimension, with a resulting frame rate of 80nms. We investigate producing embeddings
up to a frame rate of 960ms, corresponding to stacking 12 consecutive frames. These embeddings are
prepended to the text embeddings (as specified in Figure 2) and fed into the LLM, which is tasked
with predicting the next text based token.
Large Language Model Adaptation We use the Low-rank adaptation (LoRA) approach to adapt
the key, query, value and output layers of the self-attention mechanism leaving feed-forward nets,
embedding and final linear output layer unchanged. Unless specified otherwise, default LoRA
hyperparameters are set to a rank of R = 8 and a = 16. We investigate the impact of R in an ablation
study.
=
Training The audio encoders were initially trained using the Adam optimizer with ẞ₁ = 0.9, ẞ2
0.98 [15]. The learning rate was linearly warmed up over 20k training steps up to a peak value of
1e-3 followed by a exponential decaying schedule. This was done on 16 NVIDIA A100 40GBs with
4 gradient accumulations using a per-gpu batch size of up to 500 seconds of audio. The checkpoint
with the best validation loss was picked. The joint system with audio encoder and LLM was thereafter
trained with a similar schedule of 5k warmup steps up to a peak learning rate of 5e-4 decaying down
to 5e-6 over 250k steps. Training was often stopped early withing 100k steps. This was performed on
64 NVIDIA A100 40GBs with 4 gradient accumulations steps using batch sizes of up to 80 seconds.
The checkpoint with the lowest validation loss was picked for evaluation.
Evaluation All reported word error rates (WER) exclude the use of external language models
provided by [19]. Decoding is done using greedy search with a maximum output token length of 200.
3.3 Baselines
Our approach relies solely on supervised learning and so the most relevant baselines are the mono-
lingual models provided by MLS [19]. Since we follow the same data sampling strategy and
Table 1: Language specific and average WER performance on the MLS dataset. The first block monolingual
models refers to training a separate model for each language. The second block multilingual model refers
to training a single model on all languages concurrently. The last block refers to pretraining a model on all
languages, followed by finetuning a pretrained checkpoint for each language separately.
trainable
params
en de
nl
fr es
it
pt pl
Avg
supervised learning: monolingual models
36L Transformer CTC [19]
36L Transformer CTC [19] w/ LM
supervised learning: multilingual model
Decoder-only LLaMA-7B (960ms)
0.3B
0.3B
56
6.8 7.1 13.1 6.6 6.7 11.8 20.5 21.7 11.8
5.9 6.5 12.0 5.6 6.1 10.5 19.5 20.4 10.8
Decoder-only LLaMA-7B (480ms)
Decoder-only LLaMA-7B (240ms)
Decoder-only LLaMA-7B (160ms)
Decoder-only LLaMA-7B (80ms)
0.10B 7.6 7.4 11.9 7.0 6.1 11.4 18.6 19.1 11.1
0.09B 7.3 7.4 11.9 6.7 6.1 11.5 18.3 17.0 10.8
0.09B 7.0 7.2 11.4 6.4 6.0
11.5 17.5 16.7 10.5
0.08B 6.9 7.0 11.3 6.2 5.4 11.6 17.4 14.8 10.1
0.08B 6.2 6.7 11.3 5.5 5.2 10.8 16.2 15.9 9.7
self-supervised learning + monolingual finetuning
w2v2 XLSR-53 w/ LM
0.3B
7.0 10.8 7.6 6.3 10.4 14.7 17.2
10.6
4
setup as in [9] we will also include the self-supervised XLSR-53 with monolingual finetuning
as a baseline. There are many alternative and powerful audio encoders in literature that achieve
highly competitive results on the MLS benchmark, while relevant these systems are often trained
using self/semi-supervised approaches with significantly more compute and trainable parameters,
representing orthogonal contributions to our aims.
3.4 Main Results
Since we keep most parameters in the LLM frozen, and make use of a very small audio encoder, our
approach has much fewer trainable parameters compared to baselines, see Table 1. As expected, the
Decoder-only LLaMA with the highest frame rate (80ms) outperforms systems with lower frame rate,
also outperforming the monolingual models by 18% and 10% on average word error rate. Reducing
the frame rate degrades performance, however, even systems with large strides (480/960ms), reducing
the original filterbank sequence by a factor of up to 96, are able to compete with the monolingual
baselines. These high striding systems could also be one viable avenue for operating on long-form
audio, by compressing the audio sequence length orders of magnitude.
3.5 Ablation Studies
Larger Audio Encoders The level of audio encoder striding has a notable impact on the speech
recognition ability of LLAMA. Therefore, we also investigate the number of layers in the audio
encoder, scaling it from 72 up to 142 million parameters, see Table 2. The largest audio encoder with
Table 2: Investigating the impact of number of layers of the audio encoder on the MLS dataset.
pt pl Avg
trainable
params
en de nl
fr
es
it
18L Conformer (240ms) 0.09B 7.0 7.2 11.4
24L Conformer (240ms) 0.11B 6.6 6.6 10.8
36L Conformer (240ms) 0.16B 6.1 6.3 11.0
6.4 6.0
5.9 5.4
5.5 4.9
11.5 17.5 16.7 10.5
11.5 14.5 16.8 9.8
11.1 15.9 16.7 9.7
36 conformer layers and 240ms striding leads to an average WER of 9.7% matching the performance
of the 18 layer audio encoder with 80ms striding. This shows the importance of the audio encoder in
generating higher quality embeddings used in conditioning the LLM.
Low-rank Adaptation All experiments have fixed the low-rank adaptation parameter to R = 8
for adjusting the LLAMA self-attention parameters. We further investigate the impact of the LoRA
by adjusting RE [0, 8, 16, 32]; setting R = 0 is equivalent to completely freezing LLaMA. All
experiments in Table 3 use 240ms striding. Each rank adds approximately 1 million trainable
Table 3: Investigating the impact of rank R. Setting R = 0 is equivalent to freezing the LLM.
en de nl fr es it
7.5 7.4 12.0 6.8 5.9 11.8
pt
pl Avg
18.2 17.4 10.9
trainable
params
0.08B
0.09B 7.0 7.2 11.4 6.4 6.0 11.5 17.5 16.7 10.5
0.10B 6.3 6.8 11.4 5.7 5.5 10.8 16.3 15.0 9.7
0.11B 6.0 6.5 11.1 5.4 5.2 10.9 15.7 15.3 9.5
Decoder-only LLaMA-7B (240ms) R = 0
Decoder-only LLaMA-7B (240ms) R = 8
Decoder-only LLaMA-7B (240ms) R = 16
Decoder-only LLaMA-7B (240ms) R = 32
parameters. Interestingly, keeping LLaMA frozen and only training the audio encoder leads to
reasonable results with an average WER of 10.9%. This would also maintain the original capabilities
of the LLM; all other finetuning setups would negatively affect the ability of LLAMA in performing
text based tasks [11]. Furthermore, increasing the rank of the trainable parameters significantly
improves performance, where R 32 is able to achieve an average WER of 9.5%, outperforming
the best system in Table 1 which uses 80ms striding and R 8. Based on these results, parameter
tuning the whole LLM could lead to additional performance gains but is significantly more expensive
to train.
=
=
Masking Since the training task is based on causal next token prediction, but is conditioned
on the audio sequence which contains the needed information, masking text tokens could be
5
useful in boosting performance [17]. The table below shows performance when a fraction
F = [0.000, 0.125, 0.250, 0.375, 0.500] of the text tokens are randomly replaced with the <unk>
token during training. The introduction of masked text tokens during training can lead to notable
Table 4: Masking a fraction F of text tokens during training.
Decoder-only LLaMA-7B (240ms) F = 0.000
Decoder-only LLaMA-7B (240ms) F = 0.125
Decoder-only LLaMA-7B (240ms) F = 0.250
Decoder-only LLaMA-7B (240ms) F = 0.375
Decoder-only LLaMA-7B (240ms) F = 0.500
trainable
params
0.09B
en de
nl
fr
es
it
pt
pl
Avg
7.0 7.2
0.09B
6.7 7.0
0.09B
0.09B
0.09B
6.5 6.9
6.5 7.0
6.4 7.0
11.4 6.4 6.0 11.5 17.5 16.7 10.5
11.3 6.1 5.6 11.3 16.8 16.3 10.1
11.3 6.1 5.6 11.2 16.5 15.1 9.9
11.4 6.1 5.4 11.3 17.4 16.2 10.2
11.5 6.2 5.1 11.1 17.1 16.8 10.2
=
improvements in performance, with F 0.250 leading to a 5.7% average WER improvement
compared to the baseline F = 0.000. However, beyond this point, increasing the level of masking has
a negative impact on the low resource languages Portuguese and Polish. It is possible to set different
levels of masking depending on the amount of language specific data but we leave this investigation
to future work.
Large Language Model LLAMA was trained on predominantly English text with a small fraction
covering other languages [23]. BLOOM [21], on the other hand, was specifically designed to be
multilingual and has support for an order of magnitude more languages. Therefore, we replace
LLAMA-7B with a choice of {BLOOM-560M, BLOOM-1B7, BLOOM-7B1} to understand the
impact of LLM and how performance changes with increasing LLM scale, see Table 5. Comparing
Table 5: Replacing LLaMA-7B with various BLOOM language models.
trainable en de nl fr es it pt pl Avg
Decoder-only LLaMA-7B (240ms)
Decoder-only BLOOM-560M (240ms)
Decoder-only BLOOM-1B7 (240ms)
Decoder-only BLOOM-7B1 (240ms)
params
0.09B
7.0 7.2 11.4 6.4 6.0
0.07B 8.2 8.4 12.6 7.3 6.5 12.5 18.3
0.08B 7.5 8.3 12.2 6.7 5.8 12.2 16.6 19.0 11.0
0.08B 7.0 7.8 12.1 5.9 5.3 11.8 15.6 17.7 10.4
11.5
17.5
16.7 | 10.5
19.8 11.7
LLAMA-7B and the similarly sized BLOOM-7B1 we observe no significant difference in average
WER. Although BLOOM is multilingual it seems this ability is not as impactful once the system is
trained on a multilingual speech dataset. However, there is a clear trend showing significantly better
performance from scaling an LLM while keeping the conformer audio encoder fixed.
4 Analysing Audio Encoder Text Alignment
As hypothesized in Section 2.2 the speech recognition task can be interpreted as a regurgitation
task the language model is tasked with cleaning and repeating (in the same order) information
that is present in the audio encoder output sequence. Since the audio encoder is trained to generate
embeddings in the same semantic space as the text embeddings, this implies that the audio and text
embeddings should be monotonically aligned for a properly trained system.
We therefore, compute the cosine similarity between each possible pair of audio and text embedding
for an English test set example. This is done for the LLAMA models in 1 to understand the impact
of increased striding on the impact of alignment, see Figure 3. These alignment plots support the
hypothesis that the encoder is attempting to align the audio embeddings to the text in a monotonic
manner. As the striding is increase, the task of aligning audio to text becomes harder and harder.
Furthermore, this begs the question whether or not the audio encoder can benefit from further
supervision by training the output to be monotonically aligned to the text, instead of indirectly
training it through next token prediction via the language model.
6
5
Conclusion
text tokens
text tokens
text tokens
audio tokens
(a)
audio tokens
(b)
audio tokens
audio tokens
(d)
audio tokens
Figure 3: The pairwise cosine similarity between every pair of audio and text embeddings for a given test
example from the English set. The subfigures (a)-(e) represent the models in Table 1 with stridings ranging
from 80ms up to 960ms.
Overall this work has shown a simple procedure for enabling multilingual speech recognition with
a large language model. By prepending an audio embedding sequence, the large language model
can be triggered to perform speech recognition in a decoder-only fashion. Furthermore, this work
investigates a range of different factors that are key in enabling better recognition performance
including analysing the audio encoder stride & size. The paper also investigates the importance of
the LLM by comparing LLaMA against BLOOM, the importance of tuning the LLM with the use of
low-rank adapters and finally how the LLM can perform better recognition by augmenting the input
with masking. After joint training of the encoder and LLM it was shown that the audio embeddings
are tending to be aligned with the text embeddings. Future work can make use of this observation by
directly training the audio encoder to be aligned with the language model.
text tokens
text tokens
References
[1] Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika
Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al. “XLS-R: Self-supervised cross-lingual speech
representation learning at scale”. In: arXiv preprint arXiv:2111.09296 (2021).
[2] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. "Wav2vec 2.0: A Framework
for Self-Supervised Learning of Speech Representations". In: Proceedings of the 34th International
Conference on Neural Information Processing Systems. 2020.
[3] Junwen Bai, Bo Li, Yu Zhang, Ankur Bapna, Nikhil Siddhartha, Khe Chai Sim, and Tara N. Sainath.
"Joint Unsupervised and Supervised Training for Multilingual ASR”. In: 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP). 2022.
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language Models are Few-Shot
Learners”. In: Advances in Neural Information Processing Systems. 2020.
[5] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. “Vicuna: An Open-Source
Chatbot Impressing GPT-4 with 90%* ChatGPT Quality”. In: (2023).
[6] Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. "Self-supervised learning
with random-projection quantizer for speech recognition". In: Proceedings of the 39th International
Conference on Machine Learning. 2022.
[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. “Palm: Scaling language
modeling with pathways". In: arXiv preprint arXiv:2204.02311 (2022).
[8] Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu.
"w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised
Speech Pre-Training”. In: 2021 IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU). 2021.
[9] Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. "Unsu-
pervised Cross-lingual Representation Learning for Speech Recognition”. In: Interspeech. 2021.
[10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,
Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. "Scaling vision trans-
formers to 22 billion parameters". In: arXiv preprint arXiv:2302.05442 (2023).
[11] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. “Palm-e: An embodied multimodal language
model". In: arXiv preprint arXiv:2303.03378 (2023).
[12] Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass. “Listen, Think, and
Understand". In: arXiv preprint arXiv:2305.10790 (2023).
[13] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. "Parameter-Efficient Transfer Learning for NLP”. In:
Proceedings of the 36th International Conference on Machine Learning. Vol. 97. 2019.
[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. "LORA: Low-Rank Adaptation of Large Language Models”. In: International Conference
on Learning Representations. 2022.
[15] Diederik P. Kingma and Jimmy Ba. “Adam: A Method for Stochastic Optimization". In: International
Conference on Learning Representations (ICLR). 2015.
[16] Taku Kudo and John Richardson. "SentencePiece: A simple and language independent subword tokenizer
and detokenizer for Neural Text Processing”. In: Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System Demonstrations. 2018.
[17] Ke Li, Jay Mahadeokar, Jinxi Guo, Yangyang Shi, Gil Keren, Ozlem Kalinli, Michael L. Seltzer, and
Duc Le. "Improving fast-slow Encoder based Transducer with Streaming Deliberation". In: International
Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023.
[18] Xiang Lisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation”. In:
Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th
International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.
[19] Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. "MLS: A Large-
Scale Multilingual Dataset for Speech Research". In: Interspeech. 2020.
[20] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. “Learning multiple visual domains with
residual adapters”. In: Advances in Neural Information Processing Systems. Vol. 30. 2017.
[21] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. “Bloom: A 176b-parameter
open-access multilingual language model". In: arXiv preprint arXiv:2211.05100 (2022).
8
[22] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. "wav2vec: Unsupervised Pre-
training for Speech Recognition". In: Interspeech. 2019.
[23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. “Llama: Open and efficient foundation
language models". In: arXiv preprint arXiv:2302.13971 (2023).
[24] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. “Minigpt-4: Enhancing vision-
language understanding with advanced large language models". In: arXiv preprint arXiv:2304.10592
(2023).
