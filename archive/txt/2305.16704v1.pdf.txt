arXiv:2305.16704v1 [cs.LG] 26 May 2023
A Closer Look at In-Context Learning under
Distribution Shifts
Kartik Ahuja
David Lopez-Paz
FAIR (Meta AI)
FAIR (Meta AI)
Abstract
In-context learning, a capability that enables a model to learn from input examples on the
fly without necessitating weight updates, is a defining characteristic of large language models.
In this work, we follow the setting proposed in (Garg et al., 2022) to better understand the
generality and limitations of in-context learning from the lens of the simple yet fundamental
task of linear regression. The key question we aim to address is: Are transformers more adept
than some natural and simpler architectures at performing in-context learning under varying
distribution shifts? To compare transformers, we propose to use a simple architecture based
on set-based Multi-Layer Perceptrons (MLPs). We find that both transformers and set-based
MLPs exhibit in-context learning under in-distribution evaluations, but transformers more
closely emulate the performance of ordinary least squares (OLS). Transformers also display
better resilience to mild distribution shifts, where set-based MLPs falter. However, under
severe distribution shifts, both models' in-context learning abilities diminish.
1 Introduction
Transformers (Vaswani et al., 2017) form the backbone of modern large language models (LLMs)
including the likes of GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023). These LLMs
demonstrate remarkable capabilities, such as in-context learning and natural language-based
algorithmic reasoning. However, we are only beginning to understand the origins, limitations,
and generality of these capabilities, which is essential for developing safe and reliable LLMs.
In-context learning (ICL) refers to a model's capability to acquire knowledge on the fly from
examples provided at test time without requiring any weight updates. This ability is especially
useful when the model has to adapt to new tasks from a few demonstrations in the test prompt, for
example, adapting a model to drive in a new region with few demonstrations. Understanding ICL
for LLMs such as GPT-3 trained on raw text data is particularly challenging. In Garg et al. (2022),
the authors propose an insightful training setup, which abstracts away the raw nature of text
data. In their work, transformer models from GPT-2 family are trained on prompts comprising of
input, label demonstrations and shown to emulate the ordinary least squares (OLS) algorithm.
Certain natural questions arise at this point. What specifics of the transformer are responsible for
the emergence of this behvavior? Can simpler architectures exhibit the same capabilities? How
resilient is ICL to distribution shifts? These are the questions that motivate our work.
To compare with transformers, we propose a natural baseline that is based on set-based MLPs
Zaheer et al. (2017); Lopez-Paz et al. (2017) that exploit the permutation-invariant nature of
the task. Depending on the distribution of test prompts, we categorize in-context learning into
in-distribution ICL (ID-ICL) and out-of-distribution ICL (OOD-ICL). Under ID-ICL, the train
*Correspondance to kartikahuja@meta.com
1
distribution of the prompt is identical to the test distribution of the prompt. Under OOD-ICL, the
test distribution of prompt sequence is different from the train distribution. When evaluating
OOD-ICL, we are particularly interested in the case when the test distribution of prompts is
centered on the tail of the train distribution of prompts. We summarize our key contributions
below.
.
First, we derive conditions under which the the optimal model that predicts the label for
the current query based on the prompt coincide with the OLS or ridge regression. These
are based on known arguments, yet it is important to provide them for completeness.
Despite set-based MLPs being particularly suited for this permutation-invariant task, we
find that transformers (GPT-2 family) exhibit better ID-ICL abilities.
Under mild distribution shifts, we find that transformers degrade more gracefully than
set-based MLPs. Under more severe distribution shifts, both transformers and set-based
MLPs do not exhibit ICL abilities.
• ID-ICL performance is not predictive of OOD-ICL performance for both architecture choices.
Moving forward, several questions need to be answered. Why are transformers better than
set-based MLPs at ICL? How can we improve the OOD-ICL abilities of these architectures?
2 Related Works
Recent studies have offered intriguing insights into in-context learning (ICL). Olsson et al.
(2022) propose that the formation of “induction heads", which allow models to copy in-context
information, is key to ICL. Building on Garg et al. (2022)'s work, several researchers Akyürek
et al. (2022); von Oswald et al. (2022); Dai et al. (2022) demonstrated that transformer model's
ability to implicitly execute gradient descent steps during inference could also be central to ICL,
supporting their claims with empirical evidence. Li et al. (2023) explore this setup further by
analyzing generalization bounds for the learnability of algorithms. Lastly, Xie et al. (2021) focus
on data sampled from hidden Markov model and interpret in-context learning through the lens of
implicit Bayesian inference. They go on to provide conditions under which models can perform
ICL even when prompts have low probability under the training distribution.
Chan et al. (2022) studied the impact of inductive bias of pretraining the model on ICL. The authors
showed that pretrained transformers exhibit rule-based generalization, while those trained from
scratch use exemplar-based generalization, i.e., leverage information from the examples provided
in-context to carry out ICL. Kirsch et al. (2022) find that among factors determining the inductive
bias of the model, state-size is a more crucial parameter than the model size for ICL abilities. More
recently, Wei et al. (2023) showed that model size can be a crucial parameter as well. In particular,
they show that sufficiently large models such as PaLM-540B are capable of overriding semantic
priors if needed, while smaller counterparts are unable to do so.
3 In-context Learning under Distribution Shifts
We start with some standard notation. Inputs and labels are denoted as x Є Rd and y Є R
respectively. Each prompt p is a sequence of independent and identically distributed (i.i.d.) input,
label pairs, denoted as p = {(xi, Yi)}} =₁. Each prompt p is sampled independently as follows
ƒ ~ Pƒ,
xi ~Px, Ei Є Pɛ, Vi Є {1, , k},
Yif(xi) + Ei, Vi Є {1, ..., k},
(1)
2
where the labeling function f, which is fixed for the entire prompt p, is sampled from a
distribution Pf, inputs x; are sampled independently from Px, yi is generated by adding some
noise ɛ¿ to the labeling function's output f(x¿). For the prompt p, we define its prefix as
Pj = ((x1,y1), (x2, y2),···, x;), where j = {1, ..., k}. Define the support of prefix p; as Pj.
Define the risk for model M as R(M) = -1 E[l(M(p;), y;)], where I is the loss, M(p;)
looks at the prefixes p; and makes the prediction, the loss is computed w.r.t the true label Yj, E[.]
is the expectation over the joint distribution of (pj, yj). We want to find a model that minimizes
the risk R(M) i.e.,
M* = arg min R(M)
M
(2)
For the results to follow, we make some standard regularity assumptions that we state as follows.
The probability measure associated with p; is absolutely continuous w.r.t Lebesgue measure.
The conditional expectation and variance exists, i.e., |E[yj|pj]| < ∞ and Var[yj|pj] < ∞ for all
Pj Є Pj.
Lemma 1. If l is the square loss, then the solution to equation (2) satisfies, M*(pj)
EyjPj], almost everywhere in Pj, Vj € {1, ……‚k}.
==
While the above lemma is stated for square loss, an equivalent statement holds for cross-entropy
loss. We now turn to our case study, i.e., linear labeling functions f. Each prompt p is sampled as
follows
B~N(0, Σ), where Σ € Rdxd is invertible
Є
xi ~ Px, ɛi ~ N(0, o²), Vi Є {1, ……·‚k}
Yi ← ß³xi + ɛi, Vi € {1, . k}
(3)
where ẞ is drawn from a normal distribution with mean zero and covariance Σ and noise ε; is
sampled from a normal distribution with mean zero and variance σ². We break down prefix p; into
a matrix X; € R(j−1)×d and vector yj Є R³−¹ that stacks the first j − 1 x₁'s and y₁'s observed
in the prompt up to query xj. The tuple (X;, yj, x;) captures all the relevant information from
p; for predicting y;. Since p₁ has no inputs to look at in the past, we set X1, y₁ to zero. To
better understand the notation, consider the following example, p {(x1, Y1), (x2, Y2), (x3, Y3)}.
{(x1,y1), (x2, y2), x3}, X3 :
x1
=
, Y3 =
=
. Next, we derive the optimal models
Prefix p3=
M*(p;) for the data distribution in equation (3). The theorems derived below follows from
standard results on linear regression (See Dicker (2016); Richards et al. (2021)). We still state and
derive these for completeness.
Theorem 1. If l is the square loss and prompt generation follows equation (3), then the optimal
model from equation (2) satisfies,
-1 T
M*(pj) = x}√ (X;√¯ X; +o²Σ¯¹)¯¹X;˜¯yj
almost everywhere in P¡, Vj = {1, … … ·‚k}.
If is identity, then the above solution coincides with ridge regression (Hoerl and Kennard, 1970)
using o² as the ridge penalty. We now study the noiseless setting. To analyze the noiseless case,
we will look at the ridge solutions in the limit of σ going to zero.
3
Theorem 2. If l is the square loss and prompt generation follows equation (3) with Σ as identity,
then in the limit of σ →0 the optimal model from equation (2) satisfies
M*(pj) = x; X¡˜yj
1
almost everywhere in Pj, Vj € {1, · ,k}, where X is the Moore-Penrose pseudo-inverse of Xj.
In the above results (Lemma 1, Theorem 1, and Theorem 2) we do not use the fact that inputs
xi's are drawn independently. In Theorem 1, and Theorem 2, we assumed that ẞ is drawn from a
normal distribution. For distributions beyond normal, we now argue that if we restrict the search
space of models, then the same results continue to hold.
Constraint 1. M(pj) = x; m(X;)yj.
The above constraint restricts the model to be linear in test query and also to be linear in the label
seen up to that point. We do not impose any restrictions on m(.). In the absence of this constraint,
the risk R(M) depends on moments beyond the second order moments of the distribution of ẞ.
Thus the optimal model in the absence of this constraint may not coincide with OLS or ridge
regression.
Theorem 3. Suppose l is the square loss, B's and xi's are drawn from an arbitrary distribution with
a finite mean and invertible covariance, rest of the prompt generation follows equation (3). In this
setting, the solution to equation (2) under Constraint 1 satisfies
T
T
-1 T
M* (p;) = x;}√˜ (X;√˜ X; +o²Σ¯¹)¯¹X;˜yj
almost everywhere in Pj, Vj = {1, ………‚k}.
So far, we have characterized different conditions under which the optimal model emulates the
OLS or the ridge regression on the support of training distribution of the prompts. The study by
Garg et al. (2022) demonstrated that transformers, when trained with sufficient data, can emulate
OLS regression. Theorem 1, 2 suggest that sufficiently high capacity models (that can handle
input data of varying lengths) trained on sufficient amount of data should behave as well as
transformers on the prompts sampled from the same distribution as the train distribution. We
test this hypothesis in the experiments section. Outside the support of the training distribution
of prompts, performance is not guaranteed to be good, and it depends on the inductive biases -
architecture, optimizer, and the loss function. Our experiments will examine the bias from the
architecture. We now propose a natural architecture for the task in question.
A natural baseline for the above task We revisit the data generation in equation (1) and
parametrize the labeling function. Say the labeling process now is yi ← ƒ (xi, ß) + ɛi, where ẞ
is sampled from some distribution. E[yi|xi, ß] = f(xi, ß). Our model will first estimate ß from
the given set of samples X;, yj. The estimation of ẞ does not depend on the order of inputs and
thus estimation should be invariant w.r.t. to the order of inputs. Further, we want to work with
architectures that are capable of handling inputs of variable length. For this purpose, the most
natural architecture are the ones that accept sets as inputs. We revisit the Theorem 2 in Zaheer
et al. (2017). The theorem states
Theorem. Zaheer et al. (2017) A function operating on a set A having elements from a countable
universe is a valid set function iff it can be expressed as p(Σa; EA (a¿)).
The aforementioned theorem is stated for elements from a countable universe, with its extension to
uncountable sets provided in Zaheer et al. (2017), albeit for fixed-length sets. Since functions of the
form p(ΣaЄA (ai)) are uninversal representers of set-based functions we use them as the basis
for our architecture. We pick both p and as Multilayer Perceptrons (MLPs), and we use these to
¹If Σ is not identity, then the limit may or may not coincide with OLS; see the Appendix for further discussion.
4
* (P(A = 0 (xi, V ;)), x;
-1
;)..
j
where
estimate the parameter ẞ. The output from these MLPs is then input into another MLP together
with the query xj. The final architecture takes the form
(xi, yi) are input, label pairs seen up to xj. To manage sequences of variable length, we incorporate
a normalization term 11. Consider the noisy label scenario that we studied in Theorem 2, where
the optimal model is defined by x) (X, X; +0²Σ-¹)¯¹×,˜y;. Here, p(₁₁₁ = (x, y))
aims to output the best estimate for ẞ, which is Ŝ(X;, y;) = (X√Ƒ˜‍Xj+o²Σ¯¹)−¹X√ yj; note
how ẞ(x, y) is permutation-invariant. As per (Zaheer et al., 2017), sufficiently expressive p
and should be capable of expressing B(X;, y;). The final MLP, &, must approximate a linear
map. Next, we delve into the distribution shifts we consider and their underlying rationale.
j-1
=1
Distribution shifts for ICL. In both regression and classification problems, the concept of
covariate shift (Shimodaira, 2000) is well-understood. Covariate shift refers to the situation where
the distribution of the input features, denoted as P, changes between training and testing phases,
but the conditional distribution of the target variable given the features remains invariant. This
idea can be applied to the prompts p. When the distribution over prompts changes, but the
conditional distribution of the target variable (or response) given the prompt remains invariant,
this is referred to as “covariate shift over prompts". This is a particularly important setting
to test, as it helps us understand the model's ability to learn from novel types of prompts or
demonstrations at test time.
Consider two examples that leverage equation (3) as the underlying data generation process.
Suppose at train time, we generate prompt sequences with inputs xi's that are mostly positive
and then test on prompts comprised of negative inputs. If between train and test we do not alter
the label generation process, then this setting qualifies as covariate shift over prompts. On the
other hand, consider the setting, where the only difference from train to test is that during label
generation at test time is noisy. In this case, the prompt distribution changes but the conditional
distribution of the target conditional on the prompt also changes (E[y]p] at train time is the OLS
solution and at test time it is the ridge regression solution). As a result, this type of shift does
not qualify as covariate shift over prompts. We want to remark that the difference between two
models that perfectly minimize the expected loss in equation (2) is not apparent under all types
of covariate shifts but those that put much more weight on input sequences that are very low
probability at train time. This is one aspect in which our choice of distribution shifts differs from
Garg et al. (2022).
4 Experiments
=
=
In this section, we experiment with the set-based MLPs detailed earlier and transformers from
Garg et al. (2022). We generate data in line with the equation (3). The inputs x';s at train time
are sampled from N(0, Id), where Id is the d dimensional identity matrix, and at test time they
are sampled from N(μ, I). In one case, we set µ = 2.1 and refer to it as a mild distribution
shift, and in another case we setμ 4.1 as severe distribution shift, where 1 is a d dimensional
vector of all ones. The results are presented for d 10. The covariance of ẞ, i.e., Σ is identity.
We present results for both noiseless labels and noisy labels with σ² = 1. For the set-based MLPs,
which we refer to as MLP-set, we compare the performance of MLP-set under varying depths,
{4, 5, 10, 17, 26} (indexed from 0 to 4 in the increasing order of depth). The width was same for
all the layers at 500. We trained the MLP-set model with the Adam optimizer and a learning rate
of 0.001 except for the case of depth 26, where we had to lower the learning rate to 0.0001 to
LO
5
squared error
2
squared error
2
enable learning. We used ReLU activations and batch norm between any two hidden layers. For
training the transformer model, we adopt the same architecture used in (Garg et al., 2022), which
belongs to the GPT-2 family, and we include performances at two depths - 12 (Transformer 1)
and 16 (Transformer 2).
14
12
10
MLP-set 0
MLP-set 1
MLP-set 2
MLP-set 3
MLP-set 4
Transformer 1 (Garg et al.)
Transformer (Garg et al.)
OLS
Ridge
squared error
10
50
70
squared error
250
200
150
100
50
0
0
0
0
10
20
30
40
50
0
10
20
30
40
50
0
10
20
30
40
50
# in-context examples
# in-context examples
# in-context examples
(a)
(b)
(c)
Figure 1: Comparison of MLP-set and transformers for noiseless setting, i.e., σ = 0. a) ID-ICL
(µ = 0), b) OOD-ICL (Mild distribution shift with µ = 2. 1), c) OOD-ICL (Severe distribution
shift with = 4·1).
μ
4
16
14
12
MLP-set 0
50
MLP-set 1
MLP-set 2
MLP-set 3
MLP-set 4
Transformer 1 (Garg et al.).
Transformer
Ridgel
(Garg et al.)
quared error
10
40
30
squared error
250
200
150
100
50
0
0
0
10
20
30
40
50
0
10
20
30
40
50
0
10
20
30
40
50
# in-context examples
# in-context examples
# in-context examples
(a)
(b)
(c)
=
Figure 2: Comparison of MLP-set and transformers for noisy setting, i.e., σ 1. a) ID-ICL (u = 0),
b) OOD-ICL (Mild distribution shift with μ 2. 1), c) OOD-ICL (Severe distribution shift with
μ = 4 · 1).
=
With this experimental setup we ask these key questions: existing works studying this ICL
framework from (Garg et al., 2022) focused on transformers exhibiting this capabiltiy. Can this
ability exist in other models such as the set-based MLPs? How do the two architectures differ
under distribution shifts? In Figure 1, 2, we compare the two architectures for the noiseless and
noisy setting respectively. We describe our key findings below
•
•
We find that set-based MLPs exhibit ID-ICL capabilities but do not match the performance
of transformers; see Figure 1a, 2a. This is inspite of choosing an architecture that is well
suited for the task.
Under mild distribution shifts; see Figure 1b, 2b, transformers exhibit a more graceful
degradation as opposed set-based MLPs that become more erratic.
Under more severe distribution shifts; see Figure 1c, 2c, both the transformers and the
set-based MLPs do not exhibit OOD-ICL abilities.
Finally, the ranking of ID-ICL performance of either the set-based MLPs or the transformers
is not predictive of their OOD-ICL abilities.
The code for these experiments can be found at https://github.com/facebookresearch/iclmlp.
6
5 Discussion
This research reveals that transformers outperform natural baselines in approximating OLS
and ridge regression algorithms under mild distribution shifts. The question remains, why are
transformers superior? Further investigation is required to theorize why transformers when
optimized with familiar optimizers like stochastic gradient descent (SGD), can achieve better
approximations of algorithms than set-based MLPs. Additionally, it's crucial to explore if these
comparisons hold up for a broader set of algorithms (beyond OLS), architectures (beyond set-based
MLPs), and understand why. Some important steps towards these inquiries have been made by
Liu et al. (2022).
References
Akyürek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. (2022). What learning algorithm is
in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661.
Albert, A. E. (1972). Regression and the Moore-Penrose pseudoinverse. Academic press.
Ash, R. B. and Doléans-Dade, C. A. (2000). Probability and measure theory. Academic press.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,
P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in
neural information processing systems, 33:1877–1901.
Chan, S. C., Dasgupta, I., Kim, J., Kumaran, D., Lampinen, A. K., and Hill, F. (2022). Transformers
generalize differently from information stored in context vs in weights. arXiv preprint
arXiv:2210.05675.
Dai, D., Sun, Y., Dong, L., Hao, Y., Sui, Z., and Wei, F. (2022). Why can gpt learn in-context? language
models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559.
Dicker, L. H. (2016). Ridge regression and asymptotic minimax estimation over spheres of growing
dimension.
Garg, S., Tsipras, D., Liang, P., and Valiant, G. (2022). What can transformers learn in-context? a
case study of simple function classes. arXiv preprint arXiv:2208.01066.
Hoerl, A. E. and Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67.
Kirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L. (2022). General-purpose in-context learning
by meta-learning transformers. arXiv preprint arXiv:2212.04458.
Li, Y., Ildiz, M. E., Papailiopoulos, D., and Oymak, S. (2023). Transformers as algorithms:
Generalization and stability in in-context learning.
Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. (2022). Transformers learn shortcuts
to automata. arXiv preprint arXiv:2210.10749.
Lopez-Paz, D., Nishihara, R., Chintala, S., Scholkopf, B., and Bottou, L. (2017). Discovering causal
signals in images. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 6979-6987.
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell,
A., Bai, Y., Chen, A., et al. (2022). In-context learning and induction heads. arXiv preprint
arXiv:2209.11895.
7
OpenAI (2023). Gpt-4 technical report. arXiv.
Richards, D., Mourtada, J., and Rosasco, L. (2021). Asymptotics of ridge (less) regression under
general source condition. In International Conference on Artificial Intelligence and Statistics,
pages 3889-3897. PMLR.
Shimodaira, H. (2000). Improving predictive inference under covariate shift by weighting the
log-likelihood function. Journal of statistical planning and inference, 90(2):227–244.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and
Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing
systems, 30.
von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and
Vladymyrov, M. (2022). Transformers learn in-context by gradient descent. arXiv preprint
arXiv:2212.07677.
Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D.,
et al. (2023). Larger language models do in-context learning differently. arXiv preprint
arXiv:2303.03846.
Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. (2021). An explanation of in-context learning as
implicit bayesian inference. arXiv preprint arXiv:2111.02080.
Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R. R., and Smola, A. J. (2017).
Deep sets. Advances in neural information processing systems, 30.
8
A Appendix
Lemma. [Restatement of Lemma 1] If l is the square loss, then the solution to equation (2) satisfies,
M*(p;) = E[yj\pj], almost everywhere in Pj, Vj € {1, ……·‚k}.
Proof. We write
where
R;(M) = E[l(M(p;),Y;)].
k
✓
R(M) = R;(M),
j=1
We simplify R;(M) below
Rj (M)
= E[l(M(p;), 9j)] = Ep,Ey;\p; [(M(pj) — v;)²]
[(M(p;) − 1
-
Y ; ) ² ]
= Ep; Eyj|pj|(M(pj) − E[yj|pj]+E[Yj|Pj] — Yj)´
-
|Pj])²+
= E„Eun [(M(p) – Exp)] + E„‚Ev‚v, [(1 − By,b])³] +
'Pj
2Ep, Ey; p; (M(pj) — E[y;jpj]) (y; —EYjPj)
— E,, [(M(p;) — Ely, p,])²] + Ep, [Varly, 1;]]
=
(4)
Observe that R; (M) ≥ Ep; [Var[y;|p;]] and thus R(M) > Σ½ ½-1 Ep; [Var[yj|pj]]. If M* is a
minimizer of R(M), then it also has to minimize R;(M). If that were not the case, then M*
could be strictly improved by replacing M* for the jth query with the better model, thus leading
to a contradiction. Consider the model M(pj) = E[yj|p;] for all pj Є Pj, Vj = {1‚…‚k}.
This model M minimizes R(M) and each R.; (M). Observe that R.; (M) Ep; [Var[yj|Pj]].
Therefore, for any minima M*, R;(M*) = Ep; [Var[yj|Pj]]. From equation (4), we obtain that
= 0. From Theorem 1.6.6 in Ash and Doléans-Dade (2000), it follows
Ep; [(M* (p;) − E[y;¡p;])²]
that M*(p;) =E[y;\p;] almost everywhere in Pj.
=
Theorem. [Restatement of Theorem 1.] If l is the square loss and prompt generation follows equation
(3), then the optimal model from equation (2) satisfies,
T
T
-1 T
M*(pj) = x}√ (X;√¯ X; +o²Σ¯¹)¯¹X;˜¯yj
almost everywhere in P;, Vj Є {1, ……·‚k}.
Proof. From Lemma 1, we know that M*(pj) = E[yj|p;] almost everywhere in Pj. We now
simplify E[y; pj] for the data generation provided in equation (3). We follow standard steps of
computing the posterior in Bayesian linear regression to obtain the posterior of ẞ conditioned on
prefix pj
9
Yj
log (p(ß|p;)) = log p(ß|Xj, yj, xj) = log p(ß|Xj, yj)
=
= log (p(X;, yj|ß)) + log(p(ß)) + c
1
202
2
:(ẞ — µ)˜˜¯¯¹ (ß — µ) + c
с
(5)
where µ = ΣX√Ƒ˜y; and Σ = (X;√˜X; +o²Σ-1)-¹. Therefore, ẞ conditioned on p; is a Gaussian
distribution with mean μ and covariance Σ. Recall
Yj = ß³xj + ɛj
From the linearity of expectation and the expression above for the posterior, it follows
EYjPj]=E[YjXj,Yj, xj] = E[ß˜xj|Xj, Yj, xj] = µ˜xj
This completes the proof.
Theorem. [Restatement of Theorem 2] If l is the square loss and prompt generation follows equation
(3) with as identity, then in the limit of σ →0 the optimal model from equation (2) satisfies
M* (pj) =
= x Xyj
almost everywhere in Pj, Vj € {1,
...
,k}, where X is the Moore-Penrose pseudo-inverse of Xj.
Proof. For clarity, in this case we make the dependence of M* (p;) on σ explicit and instead write
it as M* (pj, σ) We calculate the limit of the ridge regression predictor as σ goes to zero. We
obtain
T
lim M*(p;,σ) = x√ lim(X;√˜¯X; +o²Σ¯¹)¯¹X}√˜yj = xƑ˜ X} yj
60
0+0
x.
In the simplification above, we used Σ is identity and also used the standard limit definition of
Moore-Penrose pseudo-inverse Albert (1972).
0<
-1
Implications for Theorem 2 when Σ is not identity Now consider the more general case
when Σ is not identity. In this case, suppose the inverse of X, X; exists, which can happen when
the rank of X, X; is d. In this case, limo →o(X;√˜¯X; +o²Σ¯¹)¯¹X = X;+. To see why this is
the case, observe that the map M* (pj, σ) is well defined for all σ including that at zero and it is
also continuous in σ. If the inverse of X, TX; does not exist, then the limit may not converge to
the Moore-Penrose pseudo-inverse. Consider the following example.
Let Xj
=
[10] and × 1 = [86].
where is invertible and c + 0.
(X;˜¯X; +o²Σ¯¹)¯¹x;
-1
=
1
c+o² (ac-62)
lim (X} X; + o²Σ−¹)¯¹× √ = [110]
-1
The limo→0(X,X; +o²Σ−¹)¯¹×; ‡ X;.
10
с 0
6
Theorem. [Restatement of Theorem 3] Suppose l is the square loss, ẞ's and x;'s are drawn from an
arbitrary distribution with a finite mean and invertible covariance, rest of the prompt generation
follows equation (3). In this setting, the solution to equation (2) under Constraint 1 satisfies
M*(p) = x(XX; +o²Σ−¹)¯¹×√ yj
almost everywhere in Pj, Vj = {1, … … ·‚k}.
Proof. Recall that R(M) = Σ; R;(M), where R;(M) = E[(M(pj) — y;)²]. Let us simplify one
of the terms R;(M).
R;(M) E(M
=
[(M(p;) — Yj)
- ²
= E[(M(p;) − v;)²] = E[(M(p;) — B˜x;)²³] + o²
=
E[(m(X;)y; ß³x;)
= E (m(X;)y; — ẞTx ;)² +σ²
'x 2
02
(7)
Suppose the covariance of x; is A. We write A½ to denote the symmetric positive definite square
root of A (Such a square root always exists, see Theorem 3 in 2). We use this to simplify the above
expression in equation (7) as follows
E[ (m(X)
R;(M) = E(m(X;)y; — ß™:
=
=
-
- BT x ;) ²] + 0 ²
E|[||\A\(m(X;)y; — ß³)||²] |
+02
(8)
= E
(m(X;)3
X; ) 
Y;) ||² |
Λ
2E y
[y]m(X;)™AB] +
+02
Let us simplfify the first and the third term in the above.
E[||Aª (m(X,),y,)||²] = E[y]m(X,)˜Am(X,)9;]
T
=
EẞXm(X) Am(X;)X;ß| +0²E[Trace[m(X;)˜^m(X;)]]
=
(9)
In the last simplification above, we use the fact that yj X;B+Ej, where X; = R(j-1)xd
stacks first j − 1 x₁'s and ɛ; Є R³-¹ stacks first j − 1 ε¿'s, and that each component of noise is
independent and zero mean.
-
Define O¹ =E[Xm(X;)˜^ƒ(X;)X;] and 0² = m(X;)¯\m(X;). Since X; is independent
of the above expression simplifies to
E[ß³¹ẞ] +0²Trace[0²] => 0¹²¡Σi‚j +σ²Trace[0²]
Now let us consider the third term in equation (9).
Αβ
=
i,j
2https://www.math.drexel.edu/~foucart/TeachingFiles/F12/M504Lect7.pdf
(10)
(11)
11
Define I = X₁m(X;) TA. Since X; is independent of ẞ the above expression simplifies to
Γβ
=
ΣΓ, Σε,
i,j
(12)
From the above simplifications it is clear that the loss depends on prior on ẞ through its mean and
covariance only. Therefore, if we use a Gaussian prior with same mean and covariance we obtain
the same loss. As a result, we can assume that prior is Gaussian with same mean and covariance
and leverage the previous result, i.e., Theorem 1. This completes the proof.
112
12
