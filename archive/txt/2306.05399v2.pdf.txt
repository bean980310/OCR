arXiv:2306.05399v2 [cs.CV] 16 Nov 2023
Matting Anything
Jiachen Li¹, Jitesh Jain¹, Humphrey Shi¹,2
¹SHI Labs @ Georgia Tech & Oregon & UIUC, 2Picsart AI Research (PAIR)
https://github.com/SHI-Labs/Matting-Anything
Semantic Matting
Instance Matting
Matting Anything
Semantic Matting Model
Instance Matting Model
Matting Anything Model
prompt
(box)
prompt
(point)
a dog in the
middle side
prompt
(text)
Image
(A)
Image
(B)
Image
(C)
Figure 1. Matting Anything Model (MAM) offers a versatile framework capable of addressing various types of image matting scenarios
with a single model. Compared to previous specialized models for (A) Semantic Matting, which outputs a single alpha matte of all instances
in the foreground; (B) Instance Matting, which returns alpha mattes of all human instances; (C) Matting Anything Model can estimate the
alpha matte of any target instance with user prompts as boxes, points, or text descriptions for interactive use by incorporating SAM [21].
It further reaches comparable performance to the specialized matting models on multiple benchmarks, and shows superior generalization
ability with fewer parameters as a unified image matting model.
Abstract
In this paper, we propose the Matting Anything Model
(MAM), an efficient and versatile framework for estimat-
ing the alpha matte of any instance in an image with flex-
ible and interactive visual or linguistic user prompt guid-
ance. MAM offers several significant advantages over pre-
vious specialized image matting networks: (i) MAM is ca-
pable of dealing with various types of image matting, in-
cluding semantic, instance, and referring image matting
with only a single model; (ii) MAM leverages the feature
maps from the Segment Anything Model (SAM) [21] and
adopts a lightweight Mask-to-Matte (M2M) module to pre-
dict the alpha matte through iterative refinement, which has
only 2.7 million trainable parameters. (iii) By incorporat-
ing SAM, MAM simplifies the user intervention required for
the interactive use of image matting from the trimap to the
box, point, or text prompt. We evaluate the performance
of MAM on various image matting benchmarks, and the
experimental results demonstrate that MAM achieves com-
parable performance to the state-of-the-art specialized im-
age matting models under different metrics on each bench-
mark. Overall, MAM shows superior generalization ability
and can effectively handle various image matting tasks with
fewer parameters, making it a practical solution for unified
image matting. Our code and models are open-sourced at
https://github.com/SHI-Labs/Matting-Anything.
1. Introduction
Image Matting, as a long-standing computer vision task,
aims to estimate the alpha matte a given an input image
I [45]. The matting target is mainly around human beings or
other objects at the semantic level [26,41,49]. Recent works
1
have extended the scope of image matting to more complex
scenarios like image instance matting [42], which requires
instance-aware alpha matte predictions and referring image
matting [28], which extracts the alpha matte given natural
language description.
Previous deep learning-based image matting meth-
ods [28, 29, 36, 37, 42, 47, 51, 54, 59] have been proposed
to address specific image matting tasks on correspond-
ing benchmarks. These methods are tailored to individual
datasets and lack the flexibility to handle various image
matting tasks due to their fixed model designs. This lim-
itation has hindered the development of more generalized
and versatile image matting models. As a result, there is a
growing interest in developing more adaptive and efficient
image matting frameworks that can handle different types
of image matting tasks with a single model.
Furthermore, previous image matting methods have re-
lied on user-guided trimaps as auxiliary inputs to achieve
accurate alpha matte predictions. Although some trimap-
free methods have been proposed that use mask guidance
or background images instead [39,55], they are unable to
estimate the alpha matte of the target instance based on the
user request for interactive use. Therefore, it is crucial to
develop a model that can achieve accurate alpha matte es-
timation without relying on user-guided trimaps, while also
being capable of handling simple user requests in a flexi-
ble and efficient manner for interactive use. Such a model
would significantly enhance the user experience by reduc-
ing the extra need for manual intervention.
Motivated by these limitations of image matting, we pro-
pose the Matting Anything Model (MAM), a versatile net-
work that can estimate the alpha matte of any target in-
stance with prompt-based user guidance in an image as
shown in Figure 1. MAM leverages the recent Segment
Anything Model (SAM) framework [21], which supports
flexible prompting and outputs segmentation masks of any
target instance for interactive use. Specifically, MAM takes
the feature maps and mask outputs from SAM as inputs and
adds a lightweight Mask-to-Matte (M2M) module to predict
the alpha matte of the target instance. We trained MAM on a
combination of five image matting datasets that cover differ-
ent classes of instances, allowing the M2M module to learn
generalizable features for image matting. During training,
we randomly place target instances onto background im-
ages and use a pre-trained SAM to output mask predictions
of the corresponding instances. The trainable M2M mod-
ule then refines the mask by predicting multi-scale alpha
mattes. Through an iterative refinement process based on
the mask or the alpha matte, the multi-scale predictions are
merged to obtain the final meticulous alpha matte.
We conducted extensive evaluations of MAM on six im-
age matting benchmarks, including semantic image mat-
ting benchmark PPM-100 [40], AM2K [25] PM-10K [25],
the instance image matting benchmark RWP636 [55],
HIM2K [42], and the referring image matting benchmark
RefMatte-RW 100 [28]. Our results demonstrate that MAM
achieves performance comparable to that of state-of-the-art
image matting models across all benchmarks under differ-
ent evaluation metrics. The experimental results highlight
the versatility and effectiveness of our proposed approach
for handling various image matting tasks in an interactive
and efficient manner.
2. Related Works
2.1. Image Matting
Given an image I, which can be view as a combination
of foreground image F and background image B with co-
efficient alpha matte a,
IaF + (1 - a)B
(1)
Image Matting is to estimate a given only I as inputs.
Traditional methods rely on a user-guided trimap, which
explicitly annotates the absolute foreground area, absolute
background area, and transition area. Then, sampling-based
image matting solutions use low-level features to distin-
guish the transition areas by measuring the similarities be-
tween foreground and background neighbors [1,2,5,8,9,11].
Recently, deep learning-based methods [27, 36, 47, 51, 54,
59] adopt neural networks to estimate the alpha matte in an
end-to-end manner with trimap as auxiliary inputs. Some
trimap-free methods use background image [39], mask
guidance [32,55], or segmentation data [4, 30] to make up
the absence of trimap. When the image I contains multiple
instances, the composition turns to
N
N
I = ΣaiFi + (1 - Σai) B
i
a¿ represents the alpha matte of instance i and InstMatt [42]
adopt the target and reference mask as guidance to predic-
tion instance-aware alpha matte prediction. Interactive mat-
ting methods [28, 48, 52] develop specialized models that
use point, boxes, or text input to estimate the alpha matte of
the target instance. MatAny [53] is a concurrent work that
also adopts SAM for semantic image matting. In terms of
video matting, trimap-free [22–24, 30, 40] methods are ex-
plored for real-time inference while the per-frame predic-
tion quality is not comparable to image matting methods.
However, these methods are designed for a certain scenario
with corresponding benchmarks, which limits their poten-
tial to handle various image matting tasks and benchmarks
simultaneously.
2.2. Image Segmentation
Image segmentation is a close research area to image
matting, while it predicts the binary mask of different in-
2
Segment
Anything Model
(SAM)
Image
Frozen Weights
Feature Maps
RB Refinement Block
Iterative Refinement
Prompt (box/point)
a girl with
black/red
sweater
Prompt (text)
mask
Mask-to-Matte (M2M) Module
RB
RB
RB
a osl
α 088
aos4
Figure 2. Matting Anything Model Architecture. The MAM architecture consists of a pre-trained SAM and an M2M module. Given an
input image I, SAM generates the mask prediction for the target instance based on the box or point user prompt. The M2M module takes
the concatenated inputs, including the image, mask, and feature maps, and produces multi-scale predictions aos8, dos4, and Cosl. The
iterative refinement process, detailed in Section 3, progressively improves the precision of the final meticulous alpha matte a, incorporating
information from the multi-scale outputs.
stances in the image. Similar to image matting, many im-
age segmentation methods are tailored for a specific im-
age segmentation task, like semantic segmentation [3, 15],
instance segmentation [13, 46], and panoptic segmenta-
tion [20, 44]. Recent works started to explore transformer-
based frameworks [6, 12, 16, 17] for unified image segmen-
tation. Language-guided segmentation frameworks [50,58]
look for text supervision to segment instance-aware masks.
OneFormer [16] adopts a single transformer model to learn
with a joint training strategy and performs universal seg-
mentation across semantic, instance and panoptic segmen-
tation and outperforms specialized models. SAM [21] takes
a further step recently, which supports flexible prompting
from users to segment any instance in an image for inter-
active use. Grounded-SAM [33] incorporates DINO with
SAM to add text prompt support. Foundation models like
SAM offer opportunities for other areas to develop versatile
frameworks to support a range of applications.
3. Matting Anything
In this section, we provide an overview of the Matting
Anything Model (MAM) architecture, which consists of
two main components: the frozen Segment Anything Model
(SAM) and the trainable Mask-to-Matte (M2M) module.
We first provide a brief review of the SAM, which is de-
signed to produce high-quality instance segmentation given
user-guided prompts. We then introduce the M2M mod-
ule, which enables the transformation of the binary masks
into high-quality alpha mattes. Finally, we describe how we
connect the M2M module with the SAM to gradually build
the end-to-end MAM.
3.1. Segment Anything Model
Segment Anything is a recently proposed foundation
model for segmentation. Given an image I Є R³×H×W¸
SAM uses a ViT-based image encoder to obtain deep feature
maps FЄ RC×6×6. Then, a variety of N input prompts
are encoded by the prompt encoder and sent to the mask de-
coder with the feature maps. The mask decoder returns a
set of mask candidates m¿ Є R¹×H×W, i Є N indicated by
the input prompts. With its flexible prompting mechanism,
SAM allows for interactive use and is easily adaptable for
downstream tasks.
3.2. Mask-to-Matte
The Mask-to-Matte (M2M) module is an integral com-
ponent of our Matting Anything Model (MAM) and is de-
signed to convert instance-aware mask predictions from
SAM into instance-aware alpha matte predictions efficiently
and smoothly. To achieve this, we utilize the feature maps
and mask predictions generated by SAM as auxiliary inputs
to M2M. To improve the accuracy of our predictions, we
adopt multi-scale branches for predicting the alpha matte
and merge these predictions through an iterative refinement
schedule.
Multi-Scale Prediction: Given an input image I Є
R³×HxW
the pre-trained SAM model produces feature
maps F € RC×× and mask prediction m = R¹×HxW
on the target instance with prompt guidance. We concate-
nate the rescaled image, mask, and feature maps to form
3
Task
Benchmark
Metric
Specialized Models
AM2K
Semantic Matting
PM-10K PPM-100
SADall MADall↓ MSE all↓
Instance Matting
HIM2K
IMQmat IMQ
RWP636
IMQmad IMQmse↑
Referring Matting
RefMatte-RW100
SADall↓
MSE all↓
GFM-R [25]
10.89
6.7
GFM-D [25]
10.26
6.9
MODNet [19]
4.4
MGMatting [55]
57.98
71.12
30.64
53.16
InstMatt [42]
70.26
81.34
51.10
73.09
CLIPMat-B [28]
107.81
59.5
CLIPMat-L [28]
85.83
47.4
Generalized Models
SAM [21]
25.00
MAM
17.30
25.7
15.4
10.8
4.6
61.15
74.01
49.87
56.92
33.51
17.9
68.78
81.67
54.40
76.45
29.24
15.1
Table 1. Comparisons between specialized matting models and MAM on various benchmarks. ↑↓ means higher / lower values indicate
better performance for the corresponding metric. Gray text refers to models specifically designed for these benchmarks. MAM shows clear
improvements over SAM and superior generalization ability as a unified image matting model.
the input Fm2m Є R(C+4)× ×W to the M2M module.
M2M employs several refinement blocks [7,55], which con-
tain connected self-attention layer [56], batchnorm layer,
and activation layer, to generate alpha matte predictions at
1/8 resolution, denoted as ass E R¹× ×W. The feature
maps are then upsampled to higher resolutions to make al-
pha matte predictions at 1/4 and full resolution, denoted
as αos4 € R¹×× and ɑos1 € R¹×H×W, respectively.
The multi-scale predictions enable MAM to handle objects
of varying scales and provide finer-grained alpha mattes for
detailed object extraction.
Iterative Refinement To improve the accuracy of global
and local predictions, we use an iterative refinement pro-
cess. We first compute weight maps Woss, Wos4, and Wosl
that highlight different areas of the image during training
like trimaps. These weight maps are used to compute losses
for each scale of prediction, with woss emphasizing the en-
tire image for aoss predictions, wos4 filtering out the back-
ground for aos4 predictions, and wos1 focusing only on the
transition areas. During inference, we gradually merge the
predictions of aos8, Qos4, and Cos1 with the mask predic-
tions m from SAM to obtain the final alpha matte prediction
a Є R¹×HxW
3.3. Matting Anything Model
After the development of the Mask-to-Matte (M2M)
module, we integrate it with the Segment Anything Model
(SAM) to enable end-to-end training and inference for the
Matting Anything Model (MAM). This integration allows
for a comprehensive and unified framework that handles
the entire matting process, from feature extraction to alpha
matte prediction.
Multi-Dataset Training To ensure the robustness and ver-
satility of our Matting Anything Model (MAM), we adopt
a multi-dataset training approach that encompasses diverse
=
foreground instances and background images from various
image matting datasets. This selection allows us to cover
a wide range of instance classes and background scenar-
ios, enhancing the model's ability to handle different types
of instances and backgrounds effectively. During the train-
ing process, we create composite images by combining a
foreground instance F € R³×H×W with its correspond-
ing ground truth alpha matte agt E R¹×H×W and a back-
ground image BЄ R³×H×W The composition is per-
formed using the equation I agt F (1 agt) B. We
then extract the bounding box (xo, Yo, x1,y1) that encap-
sulates the instance of interest within the composite im-
age. Then, we send the image I and the bounding box as
a prompt to the pre-trained SAM, which returns the mask
prediction of the instance. Then, we concatenate the image,
mask and feature maps, and send them to the M2M mod-
ule, which further returns the multi-scale alpha matte pre-
dictions aos8, Qos4, Qos1. The loss L is computed between
the multi-scale predictions and ground truth agt as
L(agt, dos1, dos4, Qo88) = AL₁ £1 + ALLap Lap
(3)
L₁ is L1 loss and LLap is Laplacian loss used in [14,30,43].
The coefficients 1 and XL Lap control the contribution of
each loss term, respectively. Both loss terms are computed
on multi-scale predictions as
L₁ = L1 (agt, αos1) + L1 (agt, αos4) + L1 (agt, αoss) (4)
LLap=LLap (agt, dos1)+L Lap (agt, Qos4) + L Lap (agt, dos8)
(5)
Multi-Benchmark Inference During the inference phase,
we conducted extensive evaluations of the Matting Any-
thing Model (MAM) on multiple image matting bench-
marks to assess its generality and adaptability. Given an
input image I, SAM produced the initial mask prediction
4
Method
SADall↓
SHM [4]
LFM [57]
HATT [37]
SHMC [32]
GFM-R [25]
17.81 / 16.64
36.12/37.51
28.01 / 22.66
61.50/57.85
6.8/6.9
11.6 / 15.2
5.5/3.8
27.0/29.1
GFM-D [25]
SAM [21]
MAM
10.89 / 11.52
10.26/11.89
25.00 / 44.11
17.30/25.82
2.9/3.8
2.9/4.1
10.8/28.8
3.5/9.2
10.2/9.7
21.0/ 15.2
16.1 13.1
35.6/34.0
6.4/6.7
5.9/6.9
14.8/25.7
10.1 / 15.4
MSE all↓
AM2K/PM-10K
MADall↓ Gradall↓
12.54 / 14.54
SADtri↓
10.26 / 8.53
19.68 / 16.36
13.36/9.32
35.23/23.04
9.15 / 8.00
21.06 / 21.82
18.29/15.16
37.00/37.28
10.00 / 13.07
8.82 / 12.90
8.24/7.80
60.01 / 24.56
10.65/14.22
20.72/31.96
15.67/23.99
Table 2. Results on the semantic image matting benchmark AM2K and PM-10K. Metrics with all and tri as subscript indicates the
evaluation of the whole image and the transition area, separately. ↓ means lower values indicate better performance for the metric.
Method
DIM [51]
MSE all↓
MADall↓
11.5
17.8
FDMPA [59]
10.1
16.0
LFM [57]
9.4
15.8
SHM [4]
7.2
15.2
HATT [37]
6.7
13.7
BSHM [32]
6.3
11.4
MODNet [19]
4.4
8.6
SAM [21]
MAM
10.8
13.8
4.6
9.9
Table 3. Results on the semantic image matting benchmark PPM-
100.
m Є R¹×H×W, which captured the rough delineation of
the instance. Subsequently, M2M contributed to the refine-
ment of the alpha matte prediction by providing multi-scale
predictions aos8, Qos4, and Qos1. Then, following the itera-
tive refinements, we progressively updated the predictions
by replacing the corresponding regions in the mask pre-
diction m with the respective multi-scale predictions that
demonstrated positive weight maps, while in some simple
cases the replacement is directly done upon aoss instead of
m. This iterative refinement allowed us to refine the alpha
matte estimation iteratively and enhance the precision of the
final prediction a Є R¹×H×W¸
4. Experiments
We extensively evaluate the performance of MAM on
six diverse image matting benchmarks. Through compre-
hensive evaluations using different metrics, we compare
the performance of MAM with state-of-the-art image mat-
ting models on each benchmark. The results demonstrate
that MAM consistently achieves comparable performance
to specialized state-of-the-art models, reaffirming its versa-
tility and effectiveness as a unified image matting solution.
4.1. Implementation Details
Training Datasets During the training process, we ran-
domly select foreground instances from several image mat-
ting datasets, including Adobe Image Matting dataset [51],
Distinctions-646 [54], AM2K [25], Human-2K [34], and
RefMatte [28], to ensure a diverse range of instance classes.
For background images, we select them from two datasets:
COCO [31] and BG20K [25] to provide a mix of both real-
world and synthetic backgrounds.
Evaluation Benchmarks To evaluate the adaptive ability
of MAM, we test it on a variety of image matting bench-
marks including the semantic image matting benchmarks
PPM-100 [40], AM2K [25], PM-10K [25], the instance im-
age matting benchmark RWP636 [55], HIM2K [42], and the
referring image matting benchmark RefMatte-RW100 [28].
The box prompt is used for all benchmarks and the point
prompt is only used in RefMatte-RW100. This comprehen-
sive evaluation allows us to assess the generalization ca-
pability of MAM across various image matting tasks and
benchmarks.
Evaluation Metrics We evaluate the accuracy of predicted
alpha matte for MAM with commonly adopted evaluation
metrics. Specifically, we employ Mean Absolute Differ-
ence (MAD), Sum of Absolute Difference (SAD), Mean
Squared Error (MSE), Gradient (Grad), and Connectivity
(Conn) [38] as corresponding evaluation metrics. We scale
MAD, MSE, Grad, and Conn by 103, 103, 10-3, and 10-³,
respectively. Lower values indicate better performance for
these metrics. Additionally, for instance-aware matting, we
utilize Instance Matting Quality (IMQ) [42], which takes
recognition and matting accuracy into consideration simul-
taneously. Higher values indicate better performance for the
IMQ metric.
Experimental Settings We trained MAM on a combina-
tion of training datasets using 8 RTX A6000 GPUs, with
a batch size of 10 images per GPU. Each image was a
combination of a randomly selected foreground instance
and a background image. Images were cropped to a size
of 1024 × 1024 and sent to a pre-trained ViT-B based
SAM [21] with a bounding box prompt of the target in-
stance. The feature maps and masks output by SAM were
then fed into the M2M module for alpha matte predic-
5
Model
Method
Size
IMQmad
Mask RCNN [13]
44.3 M
18.37
25.65
Synthetic Subset ↑
IMQmse IMQgrad IMQcon
Natural Subset ↑
IMQmad
IMQmse
IMQgrad IMQconn
0.45
19.07
24.22
33.74
2.27
26.65
CascadePSP [7]
67.7 M
40.85
51.64
29.59
43.37
64.58
74.66
60.02
67.20
GCA [29]
25.2 M
37.76
51.56
38.33
39.90
45.72
61.40
44.77
48.81
SIM [41]
46.5 M
43.02
52.90
40.63
44.29
54.43
66.67
49.56
58.12
FBA [10]
34.7 M
36.01
51.44
37.86
38.81
34.81
48.32
36.29
37.23
MGMatting [55]
+ 29.6 M
51.67
67.08
53.03
55.38
57.98
71.12
66.53
60.86
InstMatt [42]
SAM [21]
+ 29.7 M
63.59
78.14
64.50
67.71
70.26
81.34
74.90
72.60
93.7 M
49.69
61.44
4.34
51.84
61.15
74.01
13.64
65.85
MAM
+ 2.7 M
54.15
68.01
30.47
55.40
68.78
81.67
51.79
72.62
Table 4. Results on the instance image matting benchmark HIM2K. Metrics with mad, mse, grad, and conn as subscript indicates the
similarity metrics for IMQ are MAD, MSE, Gradient, and Connectivity, separately. ↑ means higher values indicate better performance for
the IMQ metric. MAM shows clear improvements over SAM under different metrics with only 2.7M extra trainable parameters, much
lighter compared to other mask-guided methods like MGMatting and InstMatt.
Method
IMQmad
IMQmse↑
Mask RCNN [13]
20.26
25.36
Method
MDETR [18]
Prompt SADall↓ MSEall↓ MADall
text
131.58
67.5
75.1
CascadePSP [7]
42.20
52.91
CLIPSeg [35]
text
211.86
117.8
122.2
GCA [29]
33.87
46.47
CLIPMat [28]
text
107.81
59.5
62.0
SIM [41]
34.66
46.60
SAM [21]
text
122.76
67.9
69.0
FBA [10]
35.00
47.54
MAM
text
120.10
65.9
67.5
MGMatting [55]
30.64
53.16
SAM [21]
point
214.19
123.8
124.9
InstMatt [42]
51.10
73.09
MAM
point
168.82
89.6
97.7
SAM [21]
49.87
56.92
SAM [21]
box
33.51
17.9
19.0
MAM
54.40
76.45
MAM
box
29.24
15.1
16.6
Table 5. Results on the instance matting benchmark RWP636.
tion. We employed the Adam optimizer with ẞ₁
= 0.5
and B2 = 0.99, trained for 20,000 iterations with warm-
up for the first 4,000 iterations. The weight map woss is
always 1 at all pixels during training, while wos4 changes to
the mask guidance from SAM after the 4,000 iterations and
Wos1 changes to the boundary of a os4 after the 4,000 itera-
tions as well. We set 3 refinement blocks for the prediction
of aos8, 3 refinement blocks for the prediction of
dos4, and
2 refinement blocks for the prediction of a osl. As a result,
the total trainable parameters of MAM is 2.7 million param-
eters. We applied cosine learning rate decay with an initial
learning rate of 0.001 during training. During inference, we
used a single GPU with a batch size of 1. Each image was
resized to have its longer side at 1024 pixels and its shorter
side was padded to 1024 pixels before being sent to MAM
for alpha matte prediction of the target instance.
4.2. Main Results
Specialized vs Unified Model We present a high-level
comparison between specialized image matting models and
MAM on the semantic, instance, and referring image mat-
ting benchmarks in Table 1. It shows that MAM has clear
improvements over SAM on all benchmarks. Furthermore,
MAM shows comparable performance to each specialized
Table 6. Results on the referring image matting benchmark
RefMatte-RW 100. MAM with box prompt can reach significantly
better performance than with the text prompt.
image matting model and even reaches better performance
on the HIM2K, RWP635, and RefMatte-RW100, which
makes it a practical and feasible solution to unified image
matting.
Semantic Image Matting We evaluate the performance of
MAM on three semantic image matting benchmarks: PPM-
100 [40], AM2K [25], and PM-10K [25], as presented in
Table 3 and Table 2. The iterative refinement process is
based on the doss prediction for all three benchmarks. On
the PPM-100 benchmark, MAM achieves improvements of
6.2 MSEall and 3.9 MAD all over SAM. Similarly, on the
AM2K benchmark, MAM outperforms SAM with enhance-
ments of 7.64 SAD all, 4.4 MSEall, 4.5 MADall, 41.54
Gradall, and 7.59 SADtri
Instance Image Matting In Table 4 and Table 5, We eval-
uate MAM on two instance image matting benchmarks:
HIM2K [42] and RWP636 [55]. For HIM2K, the iterative
refinement is based on prediction mask m since it contains
multiple instances per image and starting from m removes
false positive predictions. Compared to other state-of-the-
art methods on HIM2K, MAM reaches comparable perfor-
mance with only 2.7 M extra trainable parameters, which
6
Model
Natural Subset
Method
Size
IMQmad IMQmse
SAM [21]
93.7 M
50.47
61.66
+ Mask-Select
93.7 M
61.15
74.01
MAM Baseline
1.0 M
52.82
71.82
+ Multi-Scale Prediction
2.7 M
60.11
74.74
+ Iterative Refinement
+ Multi-Dataset Training
2.7 M
65.44
78.93
2.7 M
68.37
81.56
Table 7. Ablation study of MAM on the HIM2K benchmark. The
MAM Baseline is built upon the SAM model with the box prompt.
The other strategies are gradually added to the MAM Baseline and
end up with 2.7 M extra trainable parameters.
is only 10% of the specialized models like MGMatting and
InstMatt, which use the mask guidance from Mask RCNN.
On the RWP636 benchmark, we apply the iterative refine-
ment from aos8 and MAM reaches the new state-of-the-art
with 54.40 IMQmad and 76.45 IMQmse.
Referring Image Matting In Table 6, we present the eval-
uation of MAM on the RefMatte-RW 100 benchmark [28],
a recently introduced referring image matting benchmark.
While previous methods rely on text prompts for refer-
ring image matting, we leverage the bounding boxes and
text descriptions as the prompts for SAM. Considering the
text prompt for SAM has not been released yet, we use
Grounded-SAM [33] to support text prompt guidance. Re-
markably, MAM achieves superior performance when uti-
lizing the bounding box as the prompt for SAM, surpassing
the text-based methods CLIPSeg and CLIPMat by a sig-
nificant margin. Moreover, the use of bounding boxes as
prompts offers user-friendly and intuitive interactions, as
users find it easier to provide bounding boxes compared to
composing a fixed text paragraph. This observation sug-
gests that the bounding box prompt is more effective for
interactive image matting than the text or point prompt for
referring image matting.
4.3. Ablation Study
We conduct comprehensive ablation studies on the M2M
module of MAM, considering that SAM remains frozen
during the training process. To assess the performance
of MAM, we select the real-world subset of the HIM2K
benchmark.
SAM on HIM2K We begin by evaluating the pre-trained
ViT-B-based SAM using bounding boxes and points as
prompts for the target instance. SAM with box-based
prompts significantly outperforms the point-based prompts
and the final mask output is selected based on the mask
with the highest Intersection over Union (IoU) score with
the bounding box. SAM demonstrates strong performance
on the HIM2K benchmark, achieving 61.15 IMQmad and
74.01 IMQmse on the natural subset.
Building MAM We then construct the M2M baseline by
D
Image
GT
D
MAM
MG[51]
Figure 3. Visualizations of alpha matte predictions from MGMat-
ting and MAM. Improvements are highlighted in the red boxes.
integrating the M2M module, which takes SAM's mask and
feature maps, as well as the image, as inputs. This base-
line, comprising 3 connected refinement blocks and pre-
dicting at 1/16 resolution, yields inferior performance com-
pared to SAM, as the low-resolution predictions lack fine
details of the alpha matte. However, by gradually incor-
porating multi-scale predictions and iterative refinement, as
described in Section 3.2, the performance of MAM im-
proves. Additionally, the adoption of multi-dataset training,
as outlined in Section 3.3, further enhances performance,
resulting in 68.37 IMQmad and 81.56 IMQmse on the nat-
ural subset. Subsequently, we assess MAM's performance
on other benchmarks without retraining to validate its gen-
erality and adaptability.
4.4. Visualization
In Figure 3, we compare matting performance between
MGMatting and MAM of images that contain multiple in-
stances. They both leverage mask guidance from SAM.
It shows that MAM is able to give more accurate alpha
matte predictions with only 10% parameters compared to
MGMatting under the same mask guidance. It also has
fewer false positive predictions in other instances. In Fig-
ure 4, we further provide visualizations of the mask and al-
pha matte predictions from SAM and MAM. These images
are selected from the semantic image matting benchmarks
7
MAM
SAM[21]
GT
Image
MAM
SAM[21]
GT
Image
選
8 t c
意!
FFF
Figure 4. Visualizations of mask and alpha matte predictions from SAM and MAM. Improvements are highlighted in the red boxes.
and contain a single instance that can be a person, animal,
or transparent object. The visualizations demonstrate that
MAM achieves significantly improved predictions in the
transition areas without the trimap guidance, which high-
lights the superior performance of MAM in refining and en-
hancing the quality of alpha matte predictions.
5. Conclusion
In this paper, we introduce Matting Anything Model
(MAM), which uses the Segment Anything Model (SAM)
as a guidance module with a lightweight Mask-to-Matte
(M2M) module to refine the mask output into the alpha
matte of the target instance. M2M is designed to handle
various image matting tasks, including semantic, instance,
and referring image matting, using a single model based on
user prompts including points, boxes, and text. We evaluate
MAM on six image matting benchmarks and demonstrate
that it achieves comparable performance to the specialized
state-of-the-art methods under various evaluation metrics.
Our proposed model offers a more versatile and efficient
solution for interactive and unified image matting.
8
References
[1] Yagiz Aksoy, Tunc Ozan Aydin, and Marc Pollefeys. Design-
ing effective inter-pixel information flow for natural image
matting. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, 2017. 2
[2] Xue Bai and Guillermo Sapiro. A geodesic framework for
fast interactive image and video segmentation and matting.
In 2007 IEEE 11th International Conference on Computer
Vision. IEEE, 2007. 2
[3] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking atrous convolution for seman-
tic image segmentation. arXiv preprint arXiv:1706.05587,
2017. 3
[4] Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang,
Xinxin Yang, and Kun Gai. Semantic human matting. In
Proceedings of the 26th ACM international conference on
Multimedia, 2018. 2, 5
[5] Qifeng Chen, Dingzeyu Li, and Chi-Keung Tang. Knn mat-
ting. IEEE transactions on pattern analysis and machine
intelligence, 2013. 2
[6] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. arXiv preprint
arXiv:2112.01527, 2021. 3
[7] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung
Tang. Cascadepsp: Toward class-agnostic and very high-
resolution segmentation via global and local refinement. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, 2020. 4, 6
[8] Yung-Yu Chuang, Brian Curless, David H Salesin, and
Richard Szeliski. A bayesian approach to digital matting. In
Proceedings of the 2001 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition. CVPR 2001.
IEEE, 2001. 2
[9] Xiaoxue Feng, Xiaohui Liang, and Zili Zhang. A cluster
sampling method for image matting via sparse coding. In
European Conference on Computer Vision. Springer, 2016.
2
[10] Marco Forte and François Pitié. ƒ, b, alpha matting. arXiv
preprint arXiv:2003.07711, 2020. 6
[11] Leo Grady, Thomas Schiwietz, Shmuel Aharon, and Rüdiger
Westermann. Random walks for interactive alpha-matting.
In Proceedings of VIIP, 2005. 2
[12] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and
Humphrey Shi. Neighborhood attention transformer. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2023. 3
[13] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir-
shick. Mask r-cnn. In ICCV, 2017. 3, 6
[14] Qiqi Hou and Feng Liu. Context-aware image matting for si-
multaneous foreground and alpha estimation. In ICCV, 2019.
4
[15] Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao
Huang, Humphrey Shi, Wenyu Liu, and Thomas S. Huang.
Ccnet: Criss-cross attention for semantic segmentation. In
TPAMI, 2020. 3
[16] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita
Orlov, and Humphrey Shi. Oneformer: One transformer to
rule universal image segmentation. CVPR, 2023. 3
[17] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Ji-
achen Li, Steven Walton, and Humphrey Shi. Semask: Se-
mantically masked transformers for semantic segmentation.
arXiv preprint arXiv:2112.12782, 2021. 3
[18] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-
modulated detection for end-to-end multi-modal understand-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pages 1780-1790, 2021. 6
[19] Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Ryn-
son WH Lau. Modnet: Real-time trimap-free portrait mat-
ting via objective decomposition. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 36, pages
1140-1147, 2022. 4, 5
[20] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Dollár. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2019. 3
[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 3, 4, 5,
6,7
[22] Jiachen Li, Vidit Goel, Marianna Ohanyan, Shant
Navasardyan, Yunchao Wei, and Humphrey Shi. Vmformer:
End-to-end video matting with transformer. arXiv preprint
arXiv:2208.12801, 2022. 2
[23] Jiachen Li, Roberto Henschel, Vidit Goel, Marianna
Ohanyan, Shant Navasardyan, and Humphrey Shi. Video
instance matting. arXiv preprint arXiv:2311.04212, 2023. 2
[24] Jiachen Li, Marianna Ohanyan, Vidit Goel, Shant
Navasardyan, Yunchao Wei, and Humphrey Shi. VideoMatt:
A simple baseline for accessible real-time video matting. In
CVPR Workshops, 2023. 2
[25] Jizhizi Li, Jing Zhang, Stephen J Maybank, and Dacheng
Tao. Bridging composite and real: towards end-to-end deep
image matting. International Journal of Computer Vision,
2022. 2, 4, 5, 6
[26] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep automatic
natural image matting. arXiv preprint arXiv:2107.07235,
2021. 1
[27] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep image mat-
ting: A comprehensive survey. ArXiv, 2023. 2
[28] Jizhizi Li, Jing Zhang, and Dacheng Tao. Referring im-
age matting. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 22448-
22457, 2023. 2, 4, 5, 6, 7
[29] Yaoyi Li and Hongtao Lu. Natural image matting via guided
contextual attention. In Proceedings of the AAAI Conference
on Artificial Intelligence, 2020. 2, 6
[30] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip
Sengupta. Robust high-resolution video matting with tempo-
ral guidance. arXiv preprint arXiv:2108.11515, 2021. 2, 4
9
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision. Springer, 2014. 5
[32] Jinlin Liu, Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong
Xie, Changshui Zhang, and Xian-sheng Hua. Boosting se-
mantic human matting with coarse annotations. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2020. 2, 5
[33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun
Zhu, et al. Grounding dino: Marrying dino with grounded
pre-training for open-set object detection. arXiv preprint
arXiv:2303.05499, 2023. 3, 7
[34] Yuhao Liu, Jiake Xie, Xiao Shi, Yu Qiao, Yujie Huang, Yong
Tang, and Xin Yang. Tripartite information mining and inte-
gration for image matting. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2021. 5
[35] Timo Lüddecke and Alexander Ecker. Image segmenta-
tion using text and image prompts. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7086-7096, 2022. 6
[36] GyuTae Park, SungJoon Son, Jae Young Yoo, SeHo Kim,
and Nojun Kwak. Matteformer: Transformer-based image
matting via prior-tokens. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), June 2022. 2
[37] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang
Xu, Qiang Zhang, and Xiaopeng Wei. Attention-guided hi-
erarchical structure aggregation for image matting. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2020. 2,5
[38] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit
Gelautz, Pushmeet Kohli, and Pamela Rott. A perceptu-
ally motivated online benchmark for image matting. In 2009
IEEE Conference on Computer Vision and Pattern Recogni-
tion. IEEE, 2009. 5
[39] Soumyadip Sengupta, Vivek Jayaram, Brian Curless,
Steven M Seitz, and Ira Kemelmacher-Shlizerman. Back-
ground matting: The world is your green screen. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2020. 2
[40] Jiayu Sun, Zhanghan Ke, Lihe Zhang, Huchuan Lu,
and Rynson WH Lau. Modnet-v: Improving portrait
video matting via background restoration. arXiv preprint
arXiv:2109.11818, 2021. 2, 5,6
[41] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Semantic
image matting. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2021. 1,6
[42] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Human in-
stance matting via mutual guidance and multi-instance re-
finement. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, 2022. 2, 4, 5, 6
[43] Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, and
Yu-Wing Tai. Deep video matting via spatio-temporal align-
ment and aggregation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, 2021.
4
[44] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and
Liang-Chieh Chen. Max-deeplab: End-to-end panoptic
segmentation with mask transformers. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2021. 3
[45] Jue Wang and Michael F Cohen. Image and video matting:
a survey. 2008. 1
[46] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-
hua Shen. Solov2: Dynamic and fast instance segmentation.
Advances in Neural information processing systems, 2020. 3
[47] Yu Wang, Yi Niu, Peiyong Duan, Jianwei Lin, and Yuanjie
Zheng. Deep propagation based image matting. In IJCAI,
2018. 2
[48] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Han-
qing Zhao, Weiming Zhang, and Nenghai Yu. Improved im-
age matting via real-time user clicks and uncertainty estima-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 15374-15383,
2021. 2
[49] Bo Xu, Jiake Xie, Han Huang, Ziwen Li, Cheng Lu, Yong
Tang, and Yandong Guo. Situational perception guided im-
age matting. In Proceedings of the 30th ACM International
Conference on Multimedia, 2022. 1
[50] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit:
Semantic segmentation emerges from text supervision. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, 2022. 3
[51] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang.
Deep image matting. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2017. 2,
5
[52] Stephen DH Yang, Bin Wang, Weijia Li, YiQi Lin, and Con-
ghui He. Unified interactive image matting. arXiv preprint
arXiv:2205.08324, 2022. 2
[53] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu.
Matte anything: Interactive natural image matting with seg-
ment anything models. arXiv preprint arXiv:2306.04121,
2023. 2
[54] Haichao Yu, Ning Xu, Zilong Huang, Yuqian Zhou, and
Humphrey Shi. High-resolution deep image matting. AAAI,
2021. 2,5
[55] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe
Lin, Ning Xu, Yutong Bai, and Alan Yuille. Mask guided
matting via progressive refinement network. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, 2021. 2, 4, 5, 6
[56] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augus-
tus Odena. Self-attention generative adversarial networks. In
International conference on machine learning. PMLR, 2019.
4
[57] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing
Huang, Hujun Bao, and Weiwei Xu. A late fusion cnn for
digital matting. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 7469–
7478, 2019. 5
10
[58] Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin
Lmseg: Language-guided multi-
arXiv preprint arXiv:2302.13495,
Wang, and Fan Wang.
dataset segmentation.
2023. 3
[59] Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo
Zhang, and Ming Tang. Fast deep matting for portrait ani-
mation on mobile phone. In Proceedings of the 25th ACM
international conference on Multimedia, 2017. 2,5
11
