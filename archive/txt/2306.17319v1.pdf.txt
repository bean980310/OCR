arXiv:2306.17319v1 [cs.CV] 29 Jun 2023
ReMax: Relaxing for Better Training on Efficient
Panoptic Segmentation
Shuyang Sun¹* Weijun Wang² Qihang Yut Andrew Howard² Philip Torr¹ Liang-Chieh Chen
¹University of Oxford 2 Google Research
Abstract
This paper presents a new mechanism to facilitate the training of mask transformers
for efficient panoptic segmentation, democratizing its deployment. We observe
that due to its high complexity, the training objective of panoptic segmentation will
inevitably lead to much higher false positive penalization. Such unbalanced loss
makes the training process of the end-to-end mask-transformer based architectures
difficult, especially for efficient models. In this paper, we present ReMax that adds
relaxation to mask predictions and class predictions during training for panoptic
segmentation. We demonstrate that via these simple relaxation techniques during
training, our model can be consistently improved by a clear margin without any
extra computational cost on inference. By combining our method with efficient
backbones like MobileNetV3-Small, our method achieves new state-of-the-art
results for efficient panoptic segmentation on COCO, ADE20K and Cityscapes.
Code and pre-trained checkpoints will be available at https://github.com/
google-research/deeplab2.
1 Introduction
Panoptic segmentation [35] aims to provide a holistic scene understanding [61] by unifying instance
segmentation [20] and semantic segmentation [23]. The comprehensive understanding of the scene is
obtained by assigning each pixel a label, encoding both semantic class and instance identity. Prior
works adopt separate segmentation modules, specific to instance and semantic segmentation, followed
by another fusion module to resolve the discrepancy [69, 10, 34, 68, 52, 41]. More recently, thanks
to the transformer architecture [62, 4], mask transformers [64, 11, 73, 42, 70, 13, 71] are proposed
for end-to-end panoptic segmentation by directly predicting class-labeled masks.
Although the definition of panoptic segmentation only permits each pixel to be associated with
just one mask entity, some recent mask transformer-based works [11, 73, 12, 39] apply sigmoid
cross-entropy loss (i.e., not enforcing a single prediction via softmax cross-entropy loss) for mask
supervision. This allows each pixel to be associated with multiple mask predictions, leading to an
extremely unbalanced loss during training. As shown in Figure 1, when using the sigmoid cross-
entropy loss to supervise the mask branch, the false-positive (FP) loss can be even 10³× larger than
the false-negative (FN) loss. Surprisingly, such unbalanced loss leads to better results than using
softmax cross-entropy, which indicates that the gradients produced by the FP loss are still helpful for
better performance.
However, the radical imbalance in the losses makes it difficult for the network to produce confident
predictions, especially for efficient backbones [27, 56, 26], as they tend to make more mistakes given
the smaller model size. Meanwhile, the training process will also become unstable due to the large
scale loss fluctuation. To address this issue, recent approaches [4, 11, 12, 39] need to carefully clip
* Work done during internship at Google Research. Correspondence to: kevinsun@robots.ox.ac.uk
* Work done while at Google.
Preprint. Under review.
Count
200
Sigmoid CE
× sem
✗pan
Softmax CE
175-
150
125
100-
75
50
25
Semantic
head
Panoptic
head
mpan
Lsem
ms sem
stop grad
0
FP / FN loss, log scale
Figure 1: The histogram shows the ratio of
false positives to false negatives for the cross-
entropy loss, on a logarithmic scale. When
using sigmoid as the activation function, the false
positive loss is always over 100× greater than
the false negative, making the total loss to be
extremely unbalanced.
р
Lpan
mpan
Figure 2: ReMask Operation. Modules, repre-
sentations and operations rendered in gray are
not used in testing. > and represent the matrix
multiplication and Hadamard multiplication and
+ means element-wise sum. The x symbol and
"stop grad" mean that there is no gradient flown
to msem from pan during training.
the training gradients to a very small value like 0.01; otherwise, the loss would explode and the
training would collapse. In this way, the convergence of the network will also be slower. A natural
question thus emerges: Is there a way to keep those positive gradients, while better stabilizing the
training of the network?
To deal with the aforementioned conflicts in the learning objectives, one naïve solution is to apply
weighted sigmoid cross entropy loss during training. However, simply applying the hand-crafted
weights would equivalently scale the losses for all data points, which means those positive and helpful
gradients will be also scaled down. Therefore, in this paper, we present a way that can adaptively adjust
the loss weights by only adding training-time relaxation to mask-transformers [71, 64, 11, 13, 42, 73].
In particular, we propose two types of relaxation: Relaxation on Masks (ReMask) and Relaxation on
Classes (ReClass).
The proposed ReMask is motivated by the observation that semantic segmentation is a relatively
easier task than panoptic segmentation, where only the predicted semantic class is required for each
pixel without distinguishing between multiple instances of the same class. As a result, semantic
segmentation prediction could serve as a coarse-grained task and guide the semantic learning of
panoptic segmentation. Specifically, instead of directly learning to predict the panoptic masks, we
add another auxiliary branch during training to predict the semantic segmentation outputs for the
corresponding image. The panoptic prediction is then calibrated by the semantic segmentation outputs
to avoid producing too many false positive predictions. In this way, the network can be penalized less
by false positive losses.
The proposed ReClass is motivated by the observation that each predicted mask may potentially
contain regions involving multiple classes, especially during the early training stage, although each
ground-truth mask and final predicted mask should only contain one target in the mask transformer
framework [64]. To account for this discrepancy, we replace the original one-hot class label for each
mask with a softened label, allowing the ground-truth labels to have multiple classes. The weights of
each class is determined by the overlap of each predicted mask with all ground-truth masks.
By applying such simple techniques for relaxation to the state-of-the-art kMaX-DeepLab [71], our
method, called ReMax, can train the network stably without any gradient-clipping operation with a
over 10× greater learning rate than the baseline. Experimental results have shown that our method
not only speeds up the training by 3×, but also leads to much better results for panoptic segmentation.
Overall, ReMax sets a new state-of-the-art record for efficient panoptic segmentation. Notably,
for efficient backbones like MobileNetV3-Small and MobileNetV3-Large [26], our method can
outperform the strong baseline by 4.9 and 5.2 in PQ on COCO panoptic for short schedule training;
while achieves 2.9 and 2.1 improvement in PQ for the final results (i.e., long schedules). Meanwhile,
2
our model with a Axial-ResNet50 (MaX-S) [63] backbone outperforms all state-of-the-art methods
with 3× larger backbones like ConvNeXt-L [46] on Cityscapes [16]. Our model can also achieve
the state-of-the-art performance when compared with the other state-of-the-art efficient panoptic
segmentation architectures like YOSO [28] and MaskConver [28] on COCO [43], ADE20K [74] and
Cityscapes [16] for efficient panoptic segmentation.
2 Related Work
Mask Transformers for image segmentation. Recent advancements in image segmentation has
proven that Mask Transformers [64], which predict class-labeled object masks through the Hungarian
matching of predicted and ground truth masks using Transformers as task decoders [62, 4], outperform
box-based methods [34, 68, 53] that decompose panoptic segmentation into multiple surrogate tasks,
such as predicting masks for detected object bounding boxes [22] and fusing instance and semantic
segmentation [47, 8] with merging modules [41, 52, 44, 69, 10, 40]. The Mask Transformer based
methods rely on converting object queries to mask embedding vectors [31, 60, 65], which are then
multiplied with pixel features to generate predicted masks. Other approaches such as Segmenter
[58] and MaskFormer [13] have also used mask transformers for semantic segmentation. K-Net [73]
proposes dynamic kernels for generating masks. CMT-DeepLab [70] suggests an additional clustering
update term to improve transformer's cross-attention. Panoptic Segformer [42] enhances mask
transformers with deformable attention [75]. Mask2Former [13] adopts masked-attention, along with
other technical improvements such as cascaded transformer decoders [4], deformable attention [75],
and uncertainty-based point supervision [36], while kMaX-DeepLab [71] employs k-means cross-
attention. OneFormer [30] extends Mask2Former with a multi-task train-once design. Our work
builds on top of the modern mask transformer, kMaX-DeepLab [71], and adopts novel relaxation
methods to improve model capacity.
The proposed Relaxation on Masks (ReMask) is similar to the masked-attention in Mask2Former [13]
and the k-means attention in kMaX-DeepLab [71] in the sense that we also apply pixel-filtering
operations to the predicted masks. However, our ReMask operation is fundamentally distinct from
theirs in several ways: (1) we learn the threshold used to filter pixels in panoptic mask predictions
through a semantic head during training, while both masked-attention [13] and k-means attention [71]
use either hard thresholding or argmax operation on pixel-wise confidence for filtering; (2) our
approach relaxes the training objective by applying a pixel-wise semantic loss on the semantic mask
for ReMask, while they do not have explicit supervision for that purpose; and (3) we demonstrate
that ReMask can complement k-means attention in Section 4.
Acceleration for Mask Transformers for efficient panoptic segmentation. DETR [4] successfully
proves that Transformer-based approaches can be used as decoders for panoptic segmentation,
however, it still suffer from the slow training problem which requires over 300 epochs for just one go.
Recent works [13, 71, 75, 49] have found that applying locality-enhanced attention mechanism can
help to boost the speed of training for instance and panoptic segmentation. Meanwhile, some other
works [73, 42, 32] found that by removing the bi-partite matching for stuff classes and applying a
separate group of mask queries for stuff classes can also help to speed up the convergence. Unlike
them, which apply architectural level changes to the network, our method only applies training-time
relaxation to the framework, which do not introduce any extra cost during testing. Apart from the
training acceleration, recent works [25, 28, 10, 54, 50] focus on how to make the system for panoptic
segmentation more efficient. However, all these works focus on the modulated architecutural design
while our approach focus on the training pipeline, which should be two orthogonal directions.
Coarse-to-fine refinement for image segmentation. In the field of computer vision, it is a common
practice to learn representations from coarse to fine, particularly in image segmentation. For instance,
DeepLab [6, 8] proposes a graph-based approach [37, 7] that gradually refines segmentation results.
Recently, transformer-based methods for image segmentation such as [64, 13, 73, 67, 42, 19] have also
adopted a multi-stage strategy to iteratively improve predicted segmentation outcomes in transformer
decoders. The concept of using coarse-grained features (e.g., semantic segmentation) to adjust fine-
grained predictions (e.g., instance segmentation) is present in certain existing works, including [9, 2, 3].
However, these approaches can lead to a substantial increase in model size and number of parameters
during both training and inference. By contrast, our ReMax focuses solely on utilizing the coarse-fine
3
hierarchy for relaxation without introducing any additional parameters or computational costs during
inference.
Regularization and relaxation techniques. The proposed Relaxation on Classes (ReClass) in-
volves adjusting label weights based on the prior knowledge of mask overlaps, which is analogous to
the re-labeling strategy employed in CutMix-based methods such as [72, 5], as well as label smooth-
ing [59] used in image classification. However, the problem that we are tackling is substantially
different from the above label smoothing related methods in image classification. In image classifica-
tion, especially for large-scale single-class image recognition benchmarks like ImageNet [55], it is
unavoidable for images to cover some of the content for other similar classes, and label smoothing
is proposed to alleviate such labelling noise into the training process. However, since our approach
is designed for Mask Transformers [64, 11, 13, 71, 70] for panoptic segmentation, each image is
precisely labelled to pixel-level, there is no such label noise in our dataset. We observe that other than
the class prediction, the Mask Transformer approaches also introduce a primary class identification
task for the class head. The proposal of ReClass operation reduces the complexity for the classifica-
tion task in Mask Transformers. Prior to the emergence of Mask Transformers, earlier approaches
did not encounter this issue as they predicted class labels directly on pixels instead of on masks.
3 Method
Before delving into the details of our method, we briefly recap the framework of mask transform-
ers [64] for end-to-end panoptic segmentation. Mask Transformers like [64, 13, 73, 67, 42] perform
both semantic and instance segmentation on the entire image using a single Transformer-based
model. These approaches basically divide the entire framework into 3 parts: a backbone for feature
extraction, a pixel decoder with feature pyramid that fuses the feature generated by the backbone,
and a transformer mask decoder that translates features from the pixel decoder into panoptic masks
and their corresponding class categories.
In the transformer decoder, a set of mask queries is learnt to segment the image into a set of masks
by a mask head and their corresponding categories by a classification head. These queries are
updated within each transformer decoder (typically, there are at least 6 transformer decoders) by the
cross-attention mechanism [62] so that the mask and class predictions are gradually refined. The set
of predictions are matched with the ground truth via bipartite matching during training; while these
queries will be filtered with different thresholds as post-processing during inference.
3.1
Relaxation on Masks (ReMask)
The proposed Relaxation on Masks (ReMask) aims to ease the training of panoptic segmentation
models. Panoptic segmentation is commonly viewed as a more intricate task than semantic seg-
mentation, since it requires the model to undertake two types of segmentation (namely, instance
segmentation and semantic segmentation). In semantic segmentation, all pixels in an image are
labeled with their respective class, without distinguishing between multiple instances (things) of
the same class. As a result, semantic segmentation is regarded as a more coarse-grained task when
compared to panoptic segmentation. Current trend in panoptic segmentation is to model things and
stuff in a unified framework and resorts to train both the coarse-grained segmentation task on stuff
and the more fine-grained segmentation task on things together using a stricter composite objective
on things, which makes the model training more difficult. We thus propose ReMask to exploit an
auxiliary semantic segmentation branch to facilitate the training.
Definition. As shown in Figure 2, given a mask representation Xpan Є R#W×Nɖ, we apply a
panoptic mask head to generate panoptic mask logits man Є RHWXNQ. A mask classification
head to generate the corresponding classification result p = R³×Nc is applied for each query
representation qЄRNQxda. A semantic head is applied after the semantic feature X sem ERHW x sem
from the pixel decoder to produces a pixel-wise semantic segmentation map msem Є RHWXNC
assigning a class label to each pixel. Here H, W represent the height and width of the feature, NQ is
the number of mask queries, No denotes the number of semantic classes for the target dataset, d₁ is
the number of channels for the query representation, and dsem is the number of channels for the input
of semantic head. As for the structure for semantic head, we apply an ASPP module [8] and a 1×1
4
Image
Ground Truth
ReClass
baseball
glove: 1.0
baseball
glove: 0.8
person:
0.2
Figure 3: Demonstration on How ReClass works. We utilize the mask rendered in blue as an
example. Our ReClass operation aims to soften the class-wise ground truth by considering the degree
of overlap between the prediction mask and the ground truth mask. The blue mask intersects with
both masks of "baseball glove" and "person", so the final class weights contain both and the activation
of "person" in the prediction will no longer be regarded as a false positive case during training.
convolution layer afterwards to transform dsem channels into No channels as the semantic prediction.
Note that the whole auxiliary semantic branch will be skipped during inference as shown in Figure 2.
Since the channel dimensionality between msem and mpan
is different, we map the semantic masks
into the panoptic space by:
msem = σ(mem)σ(p³),
(1)
where σ() function represents the sigmoid function that normalizes the logits into interval [0, 1].
Then we can generate the relaxed panoptic outputs m̃pan in the semantic masking process as follows:
mpan =
mpan (sem Ompan),
(2)
where the represents the Hadamard product operation. Through the ReMask operation, the false
positive predictions in man can be suppressed by msem, so that during training each relaxed mask
query can quickly focus on areas of their corresponding classes. Here we apply identity mapping
to keep the original magnitude of mpan so that we can remove the semantic branch during testing.
This makes ReMask as a complete relaxation technique that does not incur any overhead cost during
testing. The re-scaled panoptic outputs mpan will be supervised by the losses Lpan.
Stop gradient for a simpler objective to msem. In order to prevent the losses designed for panoptic
segmentation from affecting the parameters in the semantic head, we halt the gradient flow to msem,
as illustrated in Figure 2. This means that the semantic head is solely supervised by a semantic loss
sem, so that it can focus on the objective of semantic segmentation, which is a less complex task.
How does ReMask work? As defined above, there are two factors that ReMask operation helps
training, (1) the Hadamard product operation between the semantic outputs and the panoptic outputs
that helps to suppress the false positive loss; and (2) the relaxation on training objectives that trains
the entire network simultaneously with consistent (coarse-grained) semantic predictions. Since
the semantic masking can also enhance the locality of the transformer decoder like [13, 71], we
conducted experiments by replacing msem with ground truth semantic masks to determine whether it
is the training relaxation or the local enhancement that improves the training. When msem is assigned
with ground truth, there will be no sem applied to each stage, so that man is applied with the most
accurate local enhancement. In this way, there are large amount of false positive predictions masked
by the ground truth semantic masks, so that the false positive gradient will be greatly reduced. The
results will be reported in Section 4.
3.2 Relaxation on Classes (ReClass)
Mask Transformers [64, 13, 71, 42] operate under the assumption that each mask prediction corre-
sponds to a single class, and therefore, the ground truth for the classification head are one-hot vectors.
However, in practice, each imperfect mask predicted by the model during the training process may
intersect with multiple ground truth masks, especially during the early stage of training. As shown
in Figure 3, the blue mask, which is the mask prediction, actually covers two classes ("baseball
glove" and "person") defined in the ground truth. If the class-wise ground truth only contains the
5
59.0
53.5
54.0
Method
52.4
2x faster
51.0
52.2
53.0
Backbone Resolution FPS PQ
Panoptic-DeepLab [10] MNV3-L [26]| 641×641 26.3 30.0
48.8
Panoptic-DeepLab [10]
R50 [21]
641×641 20.0 35.1
48.0
Real-time [25]
R50 [21]
800x1333 15.9 37.1
44.6
45.0
MaskConver [54]
MN-MH [15]
45.0
43.2
MaskFormer [13]
R50 [21]
3x faster
42.5
42.0
YOSO [28]
R50 [21]
YOSO [28]
R50 [21]
39.0
38.0
40.4
40.0
kMaX-DeepLab [71]
R50 [21]
640×640 40.2 37.2
800x1333 17.6 46.5
800x1333 23.6 48.4
512×800 45.6 46.4
1281x1281 16.3 53.0
3x faster
36.0
37.4
37.1
ReMaX-T*
MNV3-S [26]
ReMax-St
33.0
32.5
30.0
ReMax-R50
ReMaX-MNV3-S
ReMaX-MNV3-L
100K
kMaX-R50
kMaX-MNV3-S
MNV3-L [26]
R50 [21]
R50 [21]
50K
150K
kMaX-MNV3-L
Iters
Figure 4: Performance on COCO val compared
to the baseline kMaX-DeepLab [71]. ReMax can
lead to 3× faster convergence compared to the
baseline, and can improve the baselines by a clear
margin. The performance of ResNet-50 can be
further improved to 54.2 PQ when the model is
trained for 200K iterations.
ReMaX-M+
ReMaX-B
641×641 108.7 40.4
641x641 80.9 44.6
641x641 51.9 49.1
1281x1281 16.3 54.2
efficient models (> 15 FPS) on COCO val set.
The Pareto curve is shown in Figure 5 (b). The
FPS of all models are evaluated on a NVIDIA
represent the
V100 GPU with batch size 1.
application of efficient pixel and transformer de-
coders. Please check the appendix for details.
Table 1: Comparison with other state-of-the-art
class "baseball glove", the prediction for “person” will be regarded as a false positive case. However,
the existence of features of other entities would bring over-penalization that makes the network
predictions to be under-confident.
To resolve the above problem, we introduce another relaxation strategy on class logits, namely Class-
wise Relaxation (ReClass), that re-assigns the class confidence for the label of each predicted mask
according to the overlap between the predicted and ground truth semantic masks. We denote the one-
hot class labels as y, the ground truth binary semantic masks as S
the supplement class weights is calculated by:
Ут
=
σ(mpan)'S
ΣΗ
HW
Si
=
[So, ..., SHw] ∈ {0,1}HW×Nc,
(3)
where Ут denotes the label weighted by the normalized intersections between the predicted and the
ground truth masks. With ym, we further define the final class weight ŷ € [0, 1] Nc as follows:
ŷ =
= nym + (1 - nym)y,
(4)
where the n denotes the smooth factor for ReClass that controls the degree of the relaxation applying
to the classification head.
4 Experimental Results
4.1 Datasets and Evaluation Metric.
Our study of ReMaX involves analyzing its performance on three commonly used image segmentation
datasets. COCO [43] supports semantic, instance, and panoptic segmentation with 80 “things” and 53
"stuff" categories; Cityscapes [16] consists of 8 “things” and 11 “stuff” categories; and ADE20K [74]
contains 100 “things” and 50 “stuff” categories. We evaluate our method using the Panoptic Quality
(PQ) metric defined in [35] (for panoptic segmentation), the Average Precision defined in [43] (for
instance segmentation), and the mIoU [18] metric (for semantic segmentation).
4.2 Results on COCO Panoptic
Implementation details. The macro-architecture of ReMax basically follows kMaX-DeepLab [71],
while we incorporate our modules introduced in Section 3 into the corresponding heads. Concretely,
we use the key in each k-means cross-attention operation as xsem defined in Figure 2. The semantic
head introduced during training consists of an ASPP module [8] and a 1 × 1 convolution that outputs
No number of channels. The specification of models with different size is introduced in the appendix.
PQ
54.0
54.2
53.0
51.0
COCO
PQ
69.0
◆ 68.7
Cityscapes
68.4
66.4
66.2
66.0
49.1
48.0
A48.4
45.0
46.5
.
46.4
63.0
64.3
62.1
62.5
44.6
59.759.7
42.0
42.5
40.4
60.0
59.7
+
39.0
59.3 58.8
57.0
X 37.1
37.2
60.2
57.7
56.1
36.0
35.1
ReMax
kMaX-DeepLab 37.1
ReMax
MaskFormer
33.0
Panoptic-DeepLab
-YOSO
MaskConver
54.0
30.0
30.0
Real-time
YOSO
-Real-time
51.0 XLPSNet
kMaX-DeepLab
Panoptic-DeepLab
UPSNet
Mask2Former
52.5
10
25
40
55
70
85
100 FPS
0
5
10
15
20
25 FPS
(b)
Figure 5: FPS-PQ Pareto curve on (a) COCO Panoptic val set and (b) Cityscapes val set. Details
of the corresponding data points can be found in Table 1 and 8. We compare our method with
other state-of-the-art efficient pipelines for panoptic segmentation including kMaX-DeepLab [71],
Mask2Former [13], YOSO [28], Panoptic-DeepLab [10], Real-time Panoptic Segmentation [25],
UPSNet [68], LPSNet [24], MaskFormer [11], and MaskConver [54].
Training details. We basically follow the training recipe proposed in kMaX-DeepLab [71] but
make some changes to the hyper-parameters since we add more relaxation to the network. Here
we high-light the necessary and the full training details and specification of our models can be also
found in the appendix. The learning rate for the ImageNet-pretrained [55] backbone is multiplied
with a smaller learning rate factor 0.1. For training augmentations, we adopt multi-scale training
by randomly scaling the input images with a scaling ratio from 0.3 to 1.7 and then cropping it into
resolution 1281 × 1281. Following [64, 70, 71], we further apply random color jittering [17], and
panoptic copy-paste augmentation [32, 57] to train the network. DropPath [29, 38] is applied to the
backbone, the transformer decoder. AdamW [33, 48] optimizer is used with weight decay 0.005 for
short schedule 50K and 100K with a batch size 64. For long schedule, we set the weight decay to
0.02. The initial learning rate is set to 0.006, which is multiplied by a decay factor of 0.1 when the
training reaches 85% and 95% of the total iterations. The entire framework is implemented with
DeepLab2 [66] in TensorFlow [1]. Following [64], we apply a PQ-style loss, a Mask-ID cross-entropy
loss, and the instance discrimination loss to better learn the feature extracted from the backbone.
For all experiments if not specified, we default to use ResNet-50 as the backbone and apply ReMask
to the first 4 stages of transformer decoder. The n for ReClass operation is set to 0.1. All models are
trained for 27 epochs (i.e., 50K iterations). The loss weight for the semantic loss applied to each
stage in the transformer decoder is set to 0.5.
ReMax significantly improves the training convergence and outperforms the baseline by a large
margin. As shown in Figure 4, we can see that when training the model under different training
schedules 50K, 100K and 150K, our method outperform the baselines by a clear margin for all
different schedules. Concretely, ReMax can outperform the state-of-the-art baseline kMaX-DeepLab
by a significant 3.6 PQ when trained under a short-term schedule 50K iterations (27 epochs) for
backbone ResNet-50. Notably, our model trained with only 50K iterations performs even better
than kMaX-DeepLab [71] trained for the 100K iterations (54 epochs), which means that our model
can speed up the training process by approximately 2×. We kindly note that the performance of
ResNet-50 can be further improved to 54.2 PQ for 200K iterations. ReMax works very well with
efficient backbones including MobileNetV3-Small [26] and MobileNetV3-Large [26], which surpass
the baseline performance by 4.9 and 5.2 PQ for 50K iterations, and 3.3 and 2.5 PQ respectively for
150K iterations. These results demonstrate that the proposed relaxation can significantly boost the
convergence speed, yet can lead to better results when the network is trained under a longer schedule.
ReMax vs. other state-of-the-art models for efficient panoptic segmentation. Table 1 and Figure
5 (a) compares our method with other state-of-the-art methods for efficient panoptic segmentation
on COCO Panoptic. We present 4 models with different resolution and model capacity, namely
ReMax-Tiny (T), ReMaX-Small (S), ReMaX-Medium (M) and ReMaX-Base (B). Due to the limit
of space, the detailed specification of these models is included in the appendix. According to the
Pareto curve shown in Figure 5 (a), our approach outperforms the previous state-of-the-art efficient
models by a clear margin. Specifically, on COCO Panoptic val set, our models achieve 40.4, 44.6,
7
50.4 51.9 52.4 51.5
w/ w/ grad-
Activation
ReMax?
PQ
clip?
#ReMasks
0
2
4
6
η
softmax
Х
48.8
PQ
0 0.01 0.05 0.1 0.2
PQ 51.7 51.7 51.9 52.4 51.5
softmax
×
49.5
sigmoid ×
X
50.4
sigmoid
✓
51.2
sigmoid
×
52.4
Table 2: The impact of acti-
Table 3: The effect of number of Table 4: The impact of dif-
ReMask applied. ReMax per- fernt ½ defined in Eq. 4 for
forms the best when ReMask is ap- ReClass. Here we observe
vation function and gradient plied to the first 4 stages of the trans- that the result reaches its peak
clipping.
former decoder.
when n = 0.1.
w/ identity w/ ReMask
mapping?
Method
Backbone FPS PQ
in test?
PQ
52.4
MaskFormer [11]
K-Net [73]
17.6 46.5
47.1
7.8 49.6
✓
52.4
52.1
51.9
26.3 53.0
16.8 53.0
26.3 54.2
N/A
☑
w/stop-grad? w/ gt? PQ
☑ 52.4
✓
45.1
36.6*
☑
Table 5: Effect of applying
identity mapping and auxil-
iary head for ReMask dur-
ing testing. Removing the
auxiliary semantic head will
not lead to performance drop
when pan is applied with
identity mapping.
PanSegFormer [42]
Mask2Former [13]
kMaX-DeepLab [71]
MaskDINO [39]
ReMaX
R50 [21] 8.6 51.9
Table 6: Comparison on COCO val
with other models using ResNet-
50 as the backbone. The FPS
here is evaluated under resolution
1200 × 800 on V100 and the model
is trained for 200K iterations. ‡ is
evaluated using a A100 GPU.
Table 7: The effect of stop
gradient and gt-masking.
The denotation w/ gt? means
whether we use ground-truth
semantic masks for msem
The result without the stop-
gradient operation does not
well converge in training.
49.1 and 54.2 PQ with 109, 81, 52 and 16 FPS for ReMaX-T, ReMaX-S, ReMaX-M and ReMaX-B
respectively. The speed of these models is evaluated under the resolution 641 × 641 except for
ReMax-Base, which is evaluated under resolution 1281 × 1281. Meanwhile, as shown in Table 6,
our largest model with the backbone ResNet-50 also achieves better performance than the other
non-efficient state-of-the-art methods with the same backbone.
Effect of different activation, and the use of gradient clipping. Table 2 presents the effect of using
different activation function (sigmoid vs. softmax) for the Mask-ID cross-entropy loss and the σ(.)
defined in Eq (1). From the table we observe that ReMask performs better when using sigmoid as
the activation function, but our method can get rid of gradient clipping and still get a better result.
Why does ReMask work due to relaxation instead of enhancing the locality? As discussed in
Section 3, to figure out whether it is the relaxation or the pixel filtering that improves the training, we
propose experiments replacing msem with the ground truth semantic masks during training. When
msem is changed into the ground truth, all positive predictions outside the ground-truth masks will
be removed, which means that the false positive loss would be significantly scaled down. The huge
drop (52.4 vs. 45.1 PQ in Table 7) indicates that the gradients of false positive losses can benefit the
final performance. Table 7 also shows that when enabling the gradient flow from the panoptic loss to
the semantic predictions, the whole framework cannot converge well and lead to a drastically drop
in performance (36.6 PQ). The semantic masks msem faces a simpler objective (i.e. only semantic
segmentation) if the gradient flow is halted.
The number of mask relaxation. Table 3 shows the effect of the number of ReMask applied to
each stage, from which we can observe that the performance gradually increases and reaches its peak
at 52.4 PQ when the number of ReMask is 4, which is also our final setting for all other ablation
studies. Using too many ReMask (> 4) operations in the network may add too many relaxation to the
framework, so that it cannot fit well to the final complex goal for panoptic segmentation.
ReClass can also help improve the performance for ReMax. We investigate ReClass and its
hyper-parameter ŉ in this part and report the results in Table 4. In Table 4, we ablate 5 different ŋ
from 0 to 0.2 and find that ReClass performs the best when n = 0.1, leading to a 0.5 gain compared
n
to the strong baseline. The efficacy of ReClass validates our assumption that each mask may cover
regions of multiple classes.
Effect of the removing auxiliary semantic head for ReMask during testing. The ReMask
operation can be both applied and removed during testing. In Table 5, it shows that the models
8
MaX-S [26] 6.5 74M 68.7
Method
Backbone
FPS PQ
Method
Backbone
FPS #params PQ
Mask2Former [13]
R50 [21]
4.1 62.1
Mask2Former [71]
Swin-LT [45]
216M 66.6
Panoptic-DeepLab [10] Xception-71 [14]
5.7 63.0
kMaX-DeepLab [71]|
MAX-S* [64]
6.5
74M 66.4
LPSNet [24]
R50 [21]
7.7 59.7
Panoptic-DeepLab [10]|
R50 [21]
8.5 59.7
kMaX-DeepLab [71]
R50 [21]
9.0 64.3
kMaX-DeepLab [71] ConvNeXt-L* [46] 3.1
OneFormer [30] ConvNeXt-L [46]
ReMaX
232M 68.4
220M 68.5
Real-time [25]
R50 [21]
10.1 58.8
YOSO [28]
R50 [21]
11.1 59.7
kMaX-DeepLab [71]
MNV3-L [26]
22.8 60.2
ReMaX
9.0 65.4
ReMaX
640-2560
640-2560
-
34.7
-
39.7 46.1
ReMaX
R50 [21]
MNV3-L [26] 22.8 62.5
MNV3-S [26] 25.6 57.7
Table 8: Cityscapes val set results for lightweight
backbones. We consider methods without pre-
training on extra data like COCO [43] and Map-
illary Vistas [51] and test-time augmentation for
fair comparison. We evaluate our FPS with resolu-
tion 1025 × 2049 and a V100 GPU. The FPS for
other methods are evaluated using the resolution
reported in their original papers.
Table 9: Cityscapes val set results for larger back-
bones. Pre-trained on ImageNet-22k.
Method
MaskFormer [11]
Mask2Former [13]
YOSO [28]
kMaX-DeepLab [71]
kMaX-DeepLab [71]|
ReMaX
Backbone Resolution FPS PQ mIoU
R50 [21]
-
R50 [21] 640-2560 35.4 38.0
641x641 38.7 41.5 45.0
1281×1281 14.4 42.3 45.3
641x641 38.7 41.9 45.7
1281x1281 14.4 43.4 46.9
Table 10: ADE20K val set results. Our FPS is
evaluated on a NVIDIA V100 GPU under the
corresponding resolution reported in the table.
ReMaX
perform comparably under the two settings. In Table 5 we also show the necessity of applying
identity mapping to mpan during training in order to remove the auxiliary semantic head during
testing. Without the identity mapping at training, removing semantic head during testing would lead
to 0.5 drop from 52.4 (the first row in Table 5) to 51.9.
4.3 Results on Cityscapes
Implementation details. Our models are trained using a batch size of 32 on 32 TPU cores, with a
total of 60K iterations. The first 5K iterations constitute the warm-up stage, where the learning rate
gradually increases from 0 to 3 × 10−³. During training, the input images are padded to 1025 × 2049
pixels. In addition, we employ a multi-task loss function that includes four loss components with
different weights. Specifically, the weights for the PQ-style loss, auxiliary semantic loss, mask-id
cross-entropy loss, and instance discrimination loss are set to 3.0, 1.0, 0.3 and 1.0, respectively. To
generate feature representations for our model, we use 256 cluster centers and incorporate an extra
bottleneck block in the pixel decoder, which produces features with an output stride of 2. These
design are basically proposed in kMaX-DeepLab [71] and we simply follow here for fair comparison.
Results on Cityscapes. As shown in Table 8 and Figure 5 (b), it shows that our method can achieve
even better performance when using a smaller backbone MobileNetV3-Large (62.5 PQ) while the
other methods are based on ResNet-50. Meanwhile, our model with Axial-ResNet-50 (i.e., MaX-
S, 74M parameters) as the backbone can outperform the state-of-the-art models [30, 71] with a
ConvNext-L backbone (> 220M parameters). The Pareto curve in Figure 5 (b) clearly demonstrates
the efficacy of our method in terms of speed-accuracy trade-off.
4.4 Results on ADE20K
Implementation details. We basically follow the same experimental setup as the COCO dataset,
with the exception that we train our model for 100K iterations (54 epochs). In addition, we conduct
experiments using input resolutions of 1281 × 1281 pixels and 641 × 641 respectively. During
inference, we process the entire input image as a whole and resize longer side to target size then
pad the shorter side. Previous approaches use a sliding window approach, which may require more
computational resources, but it is expected to yield better performance in terms of accuracy and
detection quality. As for the hyper-parameter for ReMask and ReClass, we used the same setting as
what we propose on COCO.
Results on ADE20K. In Table 10, we compared the performance of ReMax with other methods,
using ResNet-50 as the backbone, and found that our model outperforms the baseline model by 1.6
in terms of mIOU, which is a clear margin compared to the baseline, since we do not require any
9
additional computational cost but only the relaxation during training. We also find that our model
can surpass the baseline model kMaX-DeepLab by 1.1 in terms of PQ. When comparing with other
frameworks that also incorporate ResNet-50 as the backbone, we show that our model is significantly
better than Mask2Former and MaskFormer by 3.7 and 8.7 PQ respectively.
5 Conclusion
The paper presents a novel approach called ReMax, comprising two components, ReMask and
ReClass, that leads to better training for panoptic segmentation with Mask Transformers. The
proposed method is shown to have a significant impact on training speed and final performance,
especially for efficient models. We hope that our work will inspire further investigation in this
direction, leading to more efficient and accurate panoptic segmentation models.
Acknowledgement. We would like to thank Xuan Yang at Google Research for her kind help and
discussion. Shuyang Sun and Philip Torr are supported by the UKRI grant: Turing AI Fellowship
EP/W002981/1 and EPSRC/MURI grant: EP/N019474/1. We would also like to thank the Royal
Academy of Engineering and FiveAI.
Appendix
A Loss Visualization of ReMax
Count
200
175
150
125
100
75
50
25
Sigmoid CE w/ ReMask, std=0.227
Sigmoid CE w/o ReMask, std=0.328
Method
Backbone
#Params FLOPS FPS PQ
kMaX-DeepLab [71] ConvNeXt-T* [64] 61M
ReMaX
ConvNeXt-T [64] 61M
Mask2Former [13]
Swin-B* [45] 107M
kMaX-DeepLab [71] ConvNeXt-S* [64] 83M
ReMaX
ConvNeXt-S [64] 83M
172G 21.8 55.3
172G 21.8 55.9
466G
56.4
251G 16.5 56.3
251G 16.5 56.6
-
0
1.75
2.00
2.25
2.50
FP/FN loss, log scale
2.75
3.00
3.25
3.50
Figure 6: The histogram shows the ratio
of false positives to false negatives for the
cross-entropy loss, on a logarithmic scale.
Table 11: Results for larger models on COCO val set.
FLOPS and FPS are evaluated with the input size 1200 ×
800 and a V100 GPU. †: ImageNet-22K pretraining.
We visualize the loss applied with ReMask and the loss applied without ReMask in Figure 6, from
which we can observe that ReMask can effectively reduce extremely high false positive losses;
therefore, our method can stabilize the training of the framework.
B Model Specification
Model
Backbone
Resolution
#Pixel #Transformer
Decoders Decoders
#FLOPS #Params FPS
ReMaX-T MNV3-S [26]
641 × 641
[1, 1, 1, 1]
ReMaX-S MNV3-L [26]
ReMaX-M
ReMaX-B
641 × 641
[1, 1, 1, 1]
R50 [21]
641 641
[1, 5, 1, 1]
[1, 1, 1]
[1, 1, 1]
[1, 1, 1]
[2, 2, 2]
18.8G 18.6M 109
20.9G 22.0M 81
67.8G 50.8M 52
294.7G 56.6M 26
R50 [21] 1281 x 1281 [1, 5, 1, 1]
Table 12: Specification of different models in ReMax family.
1
We provide the specification of our models and their corresponding number of parameters and FLOPs
in Table 12. We kindly note that the numbers of pixel decoders with the format [·, ·, ·, ·] represent the
numbers for features with [32, 16, 3, 1] times of the input size. We use Axial attention [63] for all
feature maps with resolution 32, 16 of the input size, and regular bottleneck residual blocks [21] for
1
10
the rest. The denotation [·, ·, ·] for the transformer decoders represents the numbers for resolution of
[1, ½,] times of the input size.
1
8"
C Performance for Larger Models
We also validate the performance of ReMax for larger models e.g. ConvNeXt-Tiny (T) and ConvNext-
Small (S). From Table 11 we can find that ReMaX can achieve better results compared to the baseline
kMaX-DeepLab [71] and Mask2Former [13]. However, the improvement of ReMax gets saturated
when the numbers become high. Notably, when using ConvNeXt-T backbone, ReMax can lead to 0.6
PQ increase over kMaX-DeepLab, while incurring no extra computational cost during inference. The
improvement is noticeable, as kMaX-DeepLab only further improves 1.0 PQ by using ConvNext-S
backbone, at the cost of extra 36% more parameters (22M) and 46% more FLOPs (79G).
D Limitations
Since we implement our method in TensorFlow, the baselines we can build upon is limited. We
will validate our approach on other baselines like Mask2Former [13] in PyTorch for future works.
Meanwhile, ReClass measures the weight of each class according to the size of each mask, which
may not be accurate and can be further improved in the future.
E Boarder Impact
Our method can help better train models for efficient panoptic segmentation. It can also be used to
develop new applications in areas such as autonomous driving, robotics, and augmented reality. For
example, in autonomous driving, efficient panoptic segmentation can be used to identify and track
other vehicles, pedestrians, and obstacles on the road. This information can be used to help the car
navigate safely. In robotics, efficient panoptic segmentation can be used to help robots understand
their surroundings and avoid obstacles. This information can be used to help robots perform tasks
such as picking and placing objects or navigating through cluttered environments. In augmented
reality, efficient panoptic segmentation can be used to overlay digital information on top of the real
world. This information can be used to provide users with information about their surroundings or to
help them with tasks such as finding their way around a new city. Overall, our method can be used to
boost a variety of applications in the field of computer vision and robotics.
References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga,
Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorflow: A system for large-scale machine learning. In
Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, 2016. 7
[2] Anurag Arnab and Philip HS Torr. Bottom-up instance segmentation using deep higher-order crfs. In
BMVC, 2016. 3
[3] Anurag Arnab, Sadeep Jayasumana, Shuai Zheng, and Philip HS Torr. Higher order conditional random
fields in deep neural networks. In ECCV, 2016. 3
[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey
Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 1, 3
[5] Jie-Neng Chen, Shuyang Sun, Ju He, Philip HS Torr, Alan Yuille, and Song Bai. Transmix: Attend to mix
for vision transformers. In CVPR, 2022. 4
[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic
image segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015. 3
[7] Liang-Chieh Chen, Alexander Schwing, Alan Yuille, and Raquel Urtasun. Learning deep structured models.
In ICML, 2015. 3
11
[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
IEEE TPAMI, 2017. 3, 4, 6
[9] Bowen Cheng, Liang-Chieh Chen, Yunchao Wei, Yukun Zhu, Zilong Huang, Jinjun Xiong, Thomas S
Huang, Wen-Mei Hwu, and Honghui Shi. Spgnet: Semantic prediction guidance for scene parsing. In
ICCV, 2019. 3
[10] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-
Chieh Chen. Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmenta-
tion. In CVPR, 2020. 1, 3, 6, 7,9
[11] Bowen Cheng, Alexander G Schwing, and Alexander Kirillov. Per-pixel classification is not all you need
for semantic segmentation. In NeurIPS, 2021. 1, 2, 4, 7, 8, 9
[12] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexander Kirillov, Rohit Girdhar, and Alexander G
Schwing. Mask2former for video instance segmentation. In CVPR, 2022. 1
[13] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-
attention mask transformer for universal image segmentation. In CVPR, 2022. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
11
[14] François Chollet. Xception: Deep learning with depthwise separable convolutions. In CVPR, 2017. 9
[15] Grace Chu, Okan Arikan, Gabriel Bender, Weijun Wang, Achille Brighton, Pieter-Jan Kindermans, Hanxiao
Liu, Berkin Akin, Suyog Gupta, and Andrew Howard. Discovering multi-hardware mobile models via
architecture search. In CVPR workshop, 2021. 6
[16] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson,
Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding.
In CVPR, 2016. 3, 6
[17] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning
augmentation policies from data. In CVPR, 2019. 7
[18] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes (voc) challenge. IJCV, 88:303–338, 2010. 6
[19] Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan, Xuan Yang, Xingyi Zhou, Golnaz Ghiasi,
Weicheng Kuo, Huizhong Chen, Liang-Chieh Chen, and David A Ross. Dataseg: Taming a universal
multi-dataset multi-task segmentation model. arXiv preprint arXiv:2306.01736, 2023. 3
[20] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Jitendra Malik. Simultaneous detection and
segmentation. In ECCV, 2014. 1
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In CVPR, 2016. 6, 8, 9, 10
[22] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 3
[23] Xuming He, Richard S Zemel, and Miguel Á Carreira-Perpiñán. Multiscale conditional random fields for
image labeling. In CVPR, 2004. 1
[24] Weixiang Hong, Qingpei Guo, Wei Zhang, Jingdong Chen, and Wei Chu. Lpsnet: A lightweight solution
for fast panoptic segmentation. In CVPR, 2021. 7,9
[25] Rui Hou, Jie Li, Arjun Bhargava, Allan Raventos, Vitor Guizilini, Chao Fang, Jerome Lynch, and Adrien
Gaidon. Real-time panoptic segmentation from dense detections. In CVPR, 2020. 3, 6, 7, 9
[26] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,
Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In ICCV, 2019. 1, 2, 6, 7,
9, 10
[27] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco
Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision
applications. arXiv preprint arXiv:1704.04861, 2017. 1
[28] Jie Hu, Linyan Huang, Tianhe Ren, Shengchuan Zhang, Rongrong Ji, and Liujuan Cao. You only segment
once: Towards real-time panoptic segmentation. In CVPR, 2023. 3, 6, 7, 9
12
[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic
depth. In ECCV, 2016. 7
[30] Jitesh Jain, Jiachen Li, Mang Tik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One
transformer to rule universal image segmentation. In CVPR, 2023. 3, 9
[31] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. In NeurIPS,
2016. 3
[32] Dahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu, Hong-Seok Kim, Hartwig Adam, In So
Kweon, and Liang-Chieh Chen. TubeFormer-DeepLab: Video Mask Transformer. In CVPR, 2022. 3,7
[33] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 7
[34] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollár. Panoptic feature pyramid networks. In
CVPR, 2019. 1,3
[35] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation.
In CVPR, 2019. 1,6
[36] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as
rendering. In CVPR, 2020. 3
[37] Philipp Krähenbühl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge
potentials. In NeurIPS, 2011. 3
[38] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks
without residuals. arXiv preprint arXiv:1605.07648, 2016. 7
[39] Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni, Heung-Yeung Shum, et al. Mask dino: Towards
a unified transformer-based framework for object detection and segmentation. In CVPR, 2023. 1, 8
[40] Qizhu Li, Xiaojuan Qi, and Philip HS Torr. Unifying training and inference for panoptic segmentation. In
CVPR, 2020. 3
[41] Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-
guided unified network for panoptic segmentation. In CVPR, 2019. 1,3
[42] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Tong Lu, and Ping
Luo. Panoptic segformer: Delving deeper into panoptic segmentation with transformers. In CVPR, 2022.
1, 2, 3, 4, 5, 8
[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 3, 6, 9
[44] Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, Gang Yu, and Wei Jiang. An end-to-end
network for panoptic segmentation. In CVPR, 2019. 3
[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 9, 10
[46] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A
convnet for the 2020s. In CVPR, 2022. 3, 9
[47] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-
tion. In CVPR, 2015. 3
[48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 7
[49] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong
Wang. Conditional detr for fast training convergence. In ICCV, 2021. 3
[50] Rohit Mohan and Abhinav Valada. Efficientps: Efficient panoptic segmentation. IJCV, 129(5):1551–1579,
2021. 3
[51] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas
dataset for semantic understanding of street scenes. In ICCV, 2017. 9
[52] Lorenzo Porzi, Samuel Rota Bulò, Aleksander Colovic, and Peter Kontschieder. Seamless scene segmenta-
tion. In CVPR, 2019. 1, 3
13
[53] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature
pyramid and switchable atrous convolution. In CVPR, 2021. 3
[54] Abdullah Rashwan, Yeqing Li, Xingyi Zhou, Jiageng Zhang, and Fan Yang. Maskconver: A universal
panoptic and semantic segmentation model with pure convolutions. OpenReview, 2023. 3, 6, 7
[55] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large
scale visual recognition challenge. IJCV, 115:211–252, 2015. 4, 7
[56] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:
Inverted residuals and linear bottlenecks. In CVPR, 2018. 1
[57] Inkyu Shin, Dahun Kim, Qihang Yu, Jun Xie, Hong-Seok Kim, Bradley Green, In So Kweon, Kuk-Jin
Yoon, and Liang-Chieh Chen. Video-kmax: A simple unified approach for online and near-online video
panoptic segmentation. arXiv preprint arXiv:2304.04694, 2023. 7
[58] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic
segmentation. In ICCV, 2021. 3
[59] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
inception architecture for computer vision. In CVPR, 2016. 4
[60] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In ECCV,
2020. 3
[61] Zhuowen Tu, Xiangrong Chen, Alan L Yuille, and Song-Chun Zhu. Image parsing: Unifying segmentation,
detection, and recognition. IJCV, 63:113–140, 2005. 1
[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 3, 4
[63] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-
DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. In ECCV, 2020. 3, 10
[64] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end
panoptic segmentation with mask transformers. In CVPR, 2021. 1, 2, 3, 4, 5, 7, 9, 10
[65] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. SOLOv2: Dynamic and fast instance
segmentation. In NeurIPS, 2020. 3
[66] Mark Weber, Huiyu Wang, Siyuan Qiao, Jun Xie, Maxwell D. Collins, Yukun Zhu, Liangzhe Yuan, Dahun
Kim, Qihang Yu, Daniel Cremers, Laura Leal-Taixe, Alan L. Yuille, Florian Schroff, Hartwig Adam, and
Liang-Chieh Chen. DeepLab2: A TensorFlow Library for Deep Labeling. arXiv: 2106.09748, 2021. 7
[67] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:
Simple and efficient design for semantic segmentation with transformers. In NeurIPS, 2021. 3, 4
[68] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, and Raquel Urtasun. Upsnet:
A unified panoptic segmentation network. In CVPR, 2019. 1, 3, 7
[69] Tien-Ju Yang, Maxwell D Collins, Yukun Zhu, Jyh-Jing Hwang, Ting Liu, Xiao Zhang, Vivienne Sze,
George Papandreou, and Liang-Chieh Chen. Deeperlab: Single-shot image parser. arXiv preprint
arXiv:1902.05093, 2019. 1,3
[70] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan
Yuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In
CVPR, 2022. 1, 3, 4, 7
[71] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and
Liang-Chieh Chen. k-means Mask Transformer. In ECCV, 2022. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11
[72] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.
Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019. 4
[73] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy. K-net: Towards unified image
segmentation. In NeurIPS, 2021. 1, 2, 3, 4, 8
[74] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing
through ade20k dataset. In CVPR, 2017. 3, 6
[75] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable
transformers for end-to-end object detection. In ICLR, 2021. 3
14
