arXiv:2305.10320v1 [cs.CV] 17 May 2023
CostFormer: Cost Transformer for Cost Aggregation in Multi-view Stereo
Weitao Chen*1, Hongbin Xu*1,2, Zhipeng Zhou¹, Yang Liu¹, Baigui Sun¹¹, Wenxiong
Kang, Xuansong Xie¹
1 Alibaba Group
2South China University of Technology
{hillskyxm, hongbinxu1013}@gmail.com, baigui.sbg@alibaba-inc.com, auwxkang@scut.edu.cn
Abstract
The core of Multi-view Stereo(MVS) is the match-
ing process among reference and source pixels.
Cost aggregation plays a significant role in this pro-
cess, while previous methods focus on handling it
via CNNs. This may inherit the natural limitation
of CNNs that fail to discriminate repetitive or incor-
rect matches due to limited local receptive fields.
To handle the issue, we aim to involve Transformer
into cost aggregation. However, another problem
may occur due to the quadratically growing com-
putational complexity caused by Transformer, re-
sulting in memory overflow and inference latency.
In this paper, we overcome these limits with an
efficient Transformer-based cost aggregation net-
work, namely CostFormer. The Residual Depth-
Aware Cost Transformer(RDACT) is proposed to
aggregate long-range features on cost volume via
self-attention mechanisms along the depth and spa-
tial dimensions. Furthermore, Residual Regression
Transformer(RRT) is proposed to enhance spatial
attention. The proposed method is a universal plug-
in to improve learning-based MVS methods.
1 Introduction
Given a series of calibrated images from different views in
one scene, Multi-view Stereo (MVS) aims to recover the
3D information of the observed scene. It is a fundamen-
tal problem in computer vision and widely applied to robot
navigation, autonomous driving, augmented reality, and etc.
Recent learning-based MVS networks [Yao et al., 2018;
Gu et al., 2020; Wang et al., 2021b] have achieved inspiring
success both in the quality and the efficiency of 3D recon-
struction. Generally, deep MVS approaches consist of the fol-
lowing five steps: feature extraction from multi-view images
via CNN network with shared weights, differentiable warping
to align all source features to the reference view, matching
cost computation from reference features and aligned source
features, matching cost aggregation or regularization, depth
or disparity regression.
*These authors contributed equally to this work.
*Corresponding authors.
Overall Error(mm)
0.46
0.46
PatchMatchNet
MVSNet
0.44
0.44
R-MVSNet
0.42
0.42
CVP-MVSNet
CasMVSNet
0.40
UCS-Net
Ours
0.38
0.40
0.38
0.36
0.36
*
0.34
0.34
2
6
8
10
0.2
0.4
0.6
0.8
1.0
1.2
GPU Mem. (GB)
Run-time (s)
Figure 1: Comparison with state-of-the-art MVS methods on DTU.
Relationship between error, GPU memory and run-time with image
size 1152x864.
Current progresses in learning-based MVS primarily con-
centrate on the limitation of reconstruction quality [Wei et al.,
2021; Yang et al., 2020a], memory consumption [Yan et al.,
2020; Wei et al., 2021], and efficiency [Wang et al., 2021b;
Wang et al., 2021a]. The basic network architecture of these
works is based on the pioneering backbone network called
MVSNet [Yao et al., 2018], which provides an elegant and
stable baseline. However, instead of taking the inheritance
of network design principle in MVSNet [Yao et al., 2018]
for granted, we can rethink the task of MVS problem as a
dense correspondence problem [Hosni et al., 2012] alterna-
tively. The core of MVS is a dense pixelwise correspondence
estimation problem that searches the corresponding pixel of
a specific pixel in the reference image along the epipolar
line in all warped source images. No matter which task this
correspondence estimation problem is applied to, the match-
ing task can be boiled down to a classical matching pipeline
[Scharstein and Szeliski, 2002]: (1) feature extraction, and (2)
cost aggregation. In learning-based MVS methods, the transi-
tion from traditional hand-crafted features to CNN-based fea-
tures inherently solves the former step of the classical match-
ing pipeline via providing powerful feature representation
learned from large-scale data. However, handling the cost
aggregation step by matching similarities between features
without any prior usually suffers from the challenges due to
ambiguities generated by repetitive patterns or background
clutters [Cho et al., 2021]. Consequently, a typical solution
in MVSNet and its variants [Yao et al., 2018; Gu et al., 2020;
Wang et al., 2021b] is to apply a 3D CNN or an RNN to reg-
ularize the cost volume among reference and source views,
rather than directly rely on the quality of the initial corre-
lation clues in cost volume. Although formulated variously
in previous methods, these methods either use hand-crafted
techniques that are agnostic to severe deformations or inherit
the limitation of CNNs, e.g. limited receptive fields, unable
to discriminate incorrect matches that are locally consistent.
In this work, we focus on the cost aggregation step of cost
volume and propose a novel cost aggregation Transformer
(CostFormer) to tackle the issues above. Our CostFormer
is based on Transformer [Vaswani et al., 2017], which is
renowned for its global receptive field and long-range de-
pendent representation. By aggregating the matching cost in
the cost volume, our aggregation network can explore global
correspondences and refine the ambiguous matching points
effectively with the help of the self-attention (SA) mecha-
nism in Transformer. Though the promising performances
of Vision Transformers have been proven in many applica-
tions [Dosovitskiy et al., 2020; Sun et al., 2021], the time
and memory complexity of the key-query dot product inter-
action in conventional SA grow quadratically with the spatial
resolution of inputs. Hence, replacing 3D CNN with Trans-
former may result in unexpected extra occupancy in memory
and latency in inference. Inspired by [Wang et al., 2021b],
we further introduce the Transformer architecture into an it-
erative multi-scale learnable PatchMatch pipeline. It inherits
the advantages of the long-range receptive field in Transform-
ers, improving the reconstruction performance substantially.
Meantime, it also maintains a balanced trade-off between ef-
ficiency and performance, which is competitive in the infer-
ence speed and parameters magnitude compared with other
methods.
Our main contributions are as follows:
(1) In this paper, we propose a novel Transformer-based
cost aggregation network called CostFormer, which can be
plugged into learning-based MVS methods to improve cost
volume effectively. (2) CostFormer applies an efficient Resid-
ual Depth-Aware Cost Transformer to cost volume, extending
2D spatial attention to 3D depth and spatial attention. (3)
CostFormer applies an efficient Residual Regression Trans-
former between cost aggregation and depth regression, keep-
ing spatial attention. (4) The proposed CostFormer brings
benefits to learning-based MVS methods when evaluating
DTU [Aanæs et al., 2016], Tanks & Temples [Knapitsch et
al., 2017] ETH3D [Schöps et al., 2017] and BlendedMVS
[Yao et al., 2020] datasets.
2 Related Work
2.1 Learning-based MVS Methods
Powered by the great success of deep learning-based tech-
niques, many learning-based methods have been proposed to
boost the performance of Multi-view Stereo. MVSNet [Yao
et al., 2018] is a landmark for the end-to-end network that in-
fers the depth map on each reference view for the MVS task.
Feature maps extracted by a 2D CNN on each view are repro-
jected to the same reference view to build a variance-based
cost volume. A 3D CNN is further used to regress the depth
map. Following this pioneering work, lots of efforts have
been devoted to boosting speed and reducing memory occu-
pation. To relieve the burden of huge memory cost, recurrent
neural networks are utilized to regularize the cost volume in
AA-RMVSNet [Wei et al., 2021]. Following a coarse-to-fine
manner to develop a computationally efficient network, a re-
cent strand of works divide the single cost volume into several
cost volumes at multiple stages, like CasMVSNet [Gu et al.,
2020], CVP-MVSNet [Yang et al., 2020a], UCSNet [Cheng
et al., 2020], and etc. Inspired by the traditional PatchMatch
stereo algorithm, PatchMatchNet [Wang et al., 2021b] inher-
its the pipeline in PatchMatch stereo in an iterative manner
and extend it into a learning-based end-to-end network.
2.2 Vision Transformer
The success of Transformer [Vaswani et al., 2017] and its
variants [Dosovitskiy et al., 2020; Liu et al., 2021] have
motivated the development of Neural Language Processing
in recent years. Borrowing inspiration from these works,
Transformer has been successfully extended to vision tasks
and proven to boost the performance of image classification
[Dosovitskiy et al., 2020]. Following the pioneering work,
many efforts are devoted to boosting the development of var-
ious vision tasks with the powerful representation ability of
Transformer.
In [Li et al., 2021], the application of Transformer in
the classic stereo disparity estimation task is investigated
thoughtfully. Swin Transformer [Liu et al., 2021] involves
the hierarchical structure into Vision Transformers and com-
putes the representation with shifted windows. Consider-
ing Transformer's superiority in extracting global content in-
formation via attention mechanism, many works attempt to
utilize it in the task of feature matching. Given a pair of
images, CATS [Cho et al., 2021] explore global consensus
among correlation maps extracted from a Transformer, which
can fully leverage the self-attention mechanism and model
long-range dependencies among pixels. LoFTR [Sun et al.,
2021] also leverages Transformers with a coarse-to-fine man-
ner to model dense correspondence. STTR [Li et al., 2021]
extends the feature matching Transformer architecture to the
task of stereo depth estimation task in a sequence-to-sequence
matching perspective. TransMVSNet [Ding et al., 2021] is
the most relevant concurrent work compared with ours, which
utilizes a Feature Matching Transformer (FMT) to lever-
age self-attention and cross-attention to aggregate long-range
context information within and across images. Specifically,
the focus of TransMVSNet is on the enhancement of feature
extraction before cost aggregation, while our proposed Cost-
Former aims to improve the cost aggregation process on cost
volume.
3 Methodology
In this section, we introduce the detailed architecture of the
proposed CostFormer which focuses on the cost aggrega-
tion step of cost volume. CostFormer contains two spe-
cially designed modules called Residual-Depth Aware Cost
Transformer (RDACT) and Residual Regression Transformer
(RRT), which are utilized to explore the relation between pix-
els within a long range and the relation between different
Reference
Image
Source
Images
INIT&PROP
STAGE3
DATL
DASTL
DATL
W
→>>
→3D →
INIT&PROP
STAGE2
W
INIT&PROP
STAGE1
W->
DASTL
REC
RDACT
→3D->
→ 3D
RDACT
RRT
RDACT
RRT
RDACT
RRT
W Warping
C
Cost Computation
3D 3D CNN
CA Cost Aggregation
D
D
Depth Regression
D
Warped Features
Cost Volume
Regularized Cost Volume
Transformed Cost Volume
Aggregated Cost Volume
Transformed Aggregated Cost Volume
INIT&PROP
Initialization and Propagation
Reference Image
RTL
RSTL
RER
RDACT
RRT
Refinement
STAGEO
RRT
Residual Depth-Aware Cost Transformer
Residual Regression Transformer
Figure 2: Structure of CostFormer based on PatchMatchNet.
depth hypotheses during the evaluation process. In Section
Preliminary, we give a brief preliminary on the pipeline of
our method. Then we show the construction of RDACT and
RRT respectively. Finally, we show experiments.
3.1
Preliminary
In general, the proposed RDACT and RRT can be integrated
with arbitrary cost volume of learning-based MVS networks.
Based on the patch match architecture [Wang et al., 2021b],
we further explore the issue of cost aggregation on cost vol-
ume. As shown in Figure 2, CostFormer based on Patch-
MatchNet [Wang et al., 2021b] extracts feature maps from
multi-view images and performs initialization and propaga-
tion to warp the features maps in source views to reference
view. Given a pixel p at the reference view and its correspond-
ing pixel på, at the i-th source view under the j-th depth hy-
pothesis dj is defined as:
-1
Pi,j = Ki · (Ro,i ·(Kõ¹· p⋅ d;) + to,i)
.
.
(1)
where Roi and to, i denote the rotation and translation be-
tween the reference view and i-th source view. Ko and Ki are
the intrinsic matrices of the reference and i-th source view.
The warped feature maps at the i-th source view Fi(pi,j)
are bilinearly interpolated to remain the original resolution.
Then, a cost volume is constructed from the similarity of fea-
ture maps, and 3D CNNs are applied to regularize the cost
volume. Warped features from all source views are integrated
into a single cost for each pixel p and depth hypothesis dj by
computing the cost per hypothesis S₁ (p, j)9 via group-wise
correction as follows:
G
Si(p, j) = < Fo(p)⁹, Fi(pi,j)' >ERG
(2)
where G is the group number, C is the channel number, <
·, · > is the inner product, Fo(p)9 and Fi(pi,j)9 are grouped
reference feature map and grouped source feature map at the
i-th view respectively. Then they aggregate over the views
with a pixel-wise view weight wi(p) to get S(p, j).
Taking no account of Transformer at the cost aggregation
(CA) step, a CA module firstly utilizes a small network with
3D convolution with 1×1×1 kernels to obtain a single cost, C
Ke
k=1
Ke
Є RH×W×D. For a spatial window of Ke pixels {pk} can
be organized as a grid, per pixel additional offsets {Apk}k=1
can be learned for spatial adaptation. The aggregated spatial
cost C(p, j) is defined as:
C(p, j) =
=
Ке
1
-Σ Wkdk C (p + Pk + Apk, j) (3)
Σ±1 Wkdk k=1
Ke
k=1
Ke
where wк and dê weight the cost C based on feature and depth
similarity. Given the sampling positions (p + Pk + Apk) k±1,
corresponding features from Fo are extracted via bilinear in-
terpolation. Then group-wise correlation is applied between
the features at each sampling location and p. The results are
concatenated into a volume on which 3D convolution layers
with 1×1×1 kernels and sigmoid non-linearities are applied to
output normalized weights {k} 1. The absolute difference
in inverse depth between each sampling point and pixel p with
their j-th hypotheses are collected. Then a sigmoid function
Ke
on the inverted differences is applied to obtain {d}/1
k=1
Ke
k=1
The remarkable thing is that such cost aggregation in-
evitably suffers from challenges due to ambiguities gener-
ated by repetitive patterns or background clutters. The local
mechanisms in ambiguities exist in many operations, such as
local propagation and spatial adaptation by small learnable
slight offset. CostFormer significantly alleviates these prob-
lems through RDACT and RRT. The original CA module is
also repositioned between RDACT and RRT.
After RRT, soft argmin is applied to get the regressed
depth. Finally, a depth refinement module is designed to re-
fine the depth regression.
For CascadeMVS and other cascade architectures, Cost-
Former can be plugged into similarly.
3.2 Residual Depth-Aware Cost Transformer
In this section, we explore the details of the Residual Depth-
Aware Cost Transformer (RDACT). Each RDACT consists
of two parts. The first part is a stack of Depth-Aware Trans-
former layer (DATL) and Depth-Aware Shifted Transformer
layer (DASTL), which deal with the cost volumes to ex-
plore the relations sufficiently. The second part is the Re-
UCSNet
PatchmatchNet
Ours
Ground Truth
10
TUBORG
10 STK
18'
TUF
10 STIC
TUFORG
10 STK
18
TUF
10TER
TUFORG
10 ST
181
TUF
TI
Figure 3: Comparison of different methods on the DTU evaluation set. The backbone of CostFormer is PatchMatchNet here.
Embedding Cost layer (REC) which recovers the cost volume
from the first part.
Given a cost volume Co ERH×W×D×G¸ temporary inter-
mediate cost volumes C1,C2,...,CL Є RHXWXDXE
extracted by DATL and DASTL alternatively:
=
are firstly
Ck DASTLk (DATLk (Ck−1)), k = 1, 2, ..., L (4)
where DATL is the k-th Depth-Aware Transformer layer
with regular windows, DASTLk is the k-th Depth-Aware
Transformer layer with shifted windows, E is the embedding
dimension number of DATLk and DASTLk.
Then a Re-Embedding Cost layer is applied to the last Ck,
namely CL, to recover G from E. The output of RDACT is
formulated as:
Cout
=
REC(CL) + Co
(5)
where REC is the Re-Embedding Cost layer, and it can be a
3D convolution with G output channels. If E = G, Cout can
be simply formulated as:
Cout=CL+Co
(6)
This residual connection allows the aggregation of different
levels of cost volumes; Cout instead of Co is then aggregated
by the original aggregation network described in section 3.1.
The whole RDACT is shown in the red window in Figure 2.
Before introducing the construction of DATL and DASTL,
we dive into the details of core constitutions called Depth-
Aware Multi-Head Self-Attention (DA-MSA) and Depth-
Aware Shifted Multi-Head Self-Attention (DAS-MSA). Both
DA-MSA and DAS-MSA are based on Depth-Aware Self-
Attention Mechanism. In order to explain Depth-Aware
Self-Attention Mechanism, we supply the knowledge about
Depth-Aware Patch Embedding and Depth-Aware Windows
as preliminary.
Depth-Aware Patch Embedding: Obviously, directly apply-
ing the attention mechanism for feature maps at pixel-wise
level is quite costly in terms of GPU memory usage. In order
to tackle this issue, we propose a Depth-Aware Patch Embed-
ding to reduce the high memory cost and get an additional
regularization. Specifically, given a grouped cost volume be-
fore aggregation CE RHXWXDXG a depth-aware patch em-
bedding is firstly applied to C to get tokens. It consists of a 3D
convolution with kernel size hxwxd and a layer normaliza-
tion. To downsample the spatial sizes of cost volume and keep
the depth hypotheses, we set h and w to more than 1 and d
as 1. So the sample ratio is adaptive for memory cost and run
time. Before convolution, cost volume will be padded to fit
the spatial sizes and downsampling ratio. After layer normal-
ization(LN), these embedded patches are further partitioned
by depth-aware windows.
Depth-Aware Windows: Beyond the nonlinear and linear
global self-attention, local self-attention within a window has
been proven to be more effective and efficient. As an exam-
ple of 2D windows, Swin Transformer [Liu et al., 2021] di-
rectly applies multi-head self-attention mechanisms on non-
overlapping 2D windows to avoid the big computation com-
plexity of global tokens. Extended from the 2D spatial win-
dow, an embedded cost volume patch = RH**W**D* ×G
with depth information is partitioned into non-overlapping
3D windows. These local windows are then transposed and
reshaped to local cost tokens. Assuming the sizes of these
windows are hs × ws × ds, the total number of tokens is
0
COLMAP
UCSNet
PatchmatchNet
Ours
2T
37 = 9mm
Figure 4: Comparison of different methods on Tanks&Temples. The Recall reported by official benchmark is presented.
ատ
[*] × [W] × [D]. These local tokens are further pro-
cessed by the multi-head self-attention mechanism.
Depth-Aware Self-Attention Mechanism: For a cost win-
dow token X Є Rh, xwxd, XG, the query, key, and value
matrices Q, K and VЄ RhsxwsxdsxG are computed as:
Q=XPQ, K = XPк,V = XPv
(7)
where PQ, PK, and Pv Є RGXG are projection ma-
trices shared across different windows. By introduc-
ing depth and spatial aware relative position bias B1 €
R(hsxhs)x(wsxws)x(dsxds) for each head, the depth-aware
self-attention(DA-SA1) matrix within a 3D local window is
thus computed as:
DA-SA1 = Attention 1(Q1, K1, V1) = SoftMax(
Q1K1T
√G
+ B1)V1
(8)
Where Q1, K1 and V1 € Rh¸wsds ×G are reshaped from
Q, K and VЄ RhsxwxdsxG. The process of DATL with
LayerNorm(LN) and multi-head DA-SA1 at the current level
is formulated as:
Â¹ = DA-MSA1((LN(X²−¹)) + X¹−1
(9)
By introducing depth-aware relative position bias B2 €
Rdsds for each head, the depth-aware self-attention(DA-
SA2) matrix along the depth dimension is an alternative mod-
ule to DATL and thus computed as:
DA-SA2 = Attention2(Q2, K2, V2)
=
Soft Max(
Q2K2T
√G
+ B2)V2
-
-
(10)
Where Q2, K2 and V2 € Rhs wsxdsxG are reshaped from
Q, K and VЄ Rhsxwxd,×G. B1 and B2 will be along the
depth dimension and lie in the range of [-d, +1,ds - 1].
Along the height and width dimension, B1 lies in the range
of [−hs + 1,h¸ − 1] and [−ws + 1, Ws
1]. In prac-
tice, we parameterize a smaller-sized bias matrix B1 €
R(2h-1)x(2ws-1)x(2ds-1) from B1 and perform the atten-
tion functionfor f times in parallel, and then concatenate
the depth-aware multi-head self-attention (DA-MSA) out-
puts. The
process of DATL with LayerNorm(LN), multi-head
DA-SA1, and DA-SA2 at the current level is formulated as:
Î¹ = DA-MSA1(LN(DA-MSA2(LN(X¹¯¹)))) + X²−1
(11)
Then, an MLP module that has two fully-connected layers
with GELU non-linearity between them is used for further
feature transformations:
X¹ = MLP(LN(Â¹))) + Â¹
(12)
Compared with global attention, local attention makes it pos-
sible for computation in high resolution.
However, there is no connection across local windows with
fixed partitions. Therefore, regular and shifted window par-
titions are used alternately to enable cross-window connec-
tions. So at the next level, the window partition configu-
ration is shifted along the height, width, and depth axes by
hs Ws
2). Depth-aware self-attention will be computed
2, 27
in these shifted windows(DAS-MSA); the whole process of
DASTL can be formulated as:
(쁨,
Ŷ²+1 = DAS-MSA1(LN(DAS-MSA2(LN(X¹)))) + X¹
X²+1 = MLP(LN(Â¹+¹)) + Ŷ¹+1
(13)
(14)
DAS-MSA1 and DAS-MSA2 correspond to multi-head
Attention and Attention2 within a shifted window, respec-
tively. Assuming the number of stages is n, there are n
RDACT blocks in CostFormer.
3.3 Residual Regression Transformer
After aggregation, the cost CE RHXWXD will be used for
depth regression. To further explore the spatial relation under
some depth, a Transformer block is applied to С before soft-
max. Inspired by the RDACT, the whole process of Residual
Regression Transformer(RRT) can be formulated as:
Ck = RSTk (RTk (Ck−1)), k = 1, 2, ..., L
Cout = RER(CL) + Co
(15)
(16)
Methods
Mean
Fam.
Fra.
MVSNet [Yao et al., 2018]
43.48
55.99
28.55
25.07
CasMVSNet [Gu et al., 2020]
56.84
76.37 58.45
UCS-Net [Cheng et al., 2020]
54.83
CVP-MVSNet [Yang et al., 2020b]
54.03
PVA-MVSNet [Yi et al., 2020]
54.46
AA-RMVSNet [Wei et al., 2021]
61.51
PatchmatchNet [Wang et al., 2021b]
53.15
64.36
56.93
63.52
Intermediate Group (F-score ↑)
Hor.
Lig.
M60 Pan.
Pla. Tra.
50.79 53.96 50.86 47.90 34.69
46.26 55.81 56.11 54.06 58.18 49.51
76.09 53.16 43.03 54.00 55.60 51.49 57.38 47.89
76.50 47.74 36.34 55.12 57.28 54.28 57.43 47.54
69.36 46.80 46.01 55.74 57.23 54.75 56.70 49.06
77.77 59.53 51.53 64.02 64.05 59.47 60.85 54.90
66.99 52.64 43.24 54.87 52.87 49.54 54.21 50.81
81.20 66.34 53.11 63.46 66.09 64.84 62.23 57.53
76.92 59.82 50.16 56.73 56.53 51.22 56.58 47.48
80.92 65.83 56.94 62.54 63.06 60.00 60.20 58.67
Mean
Aud.
Advanced Group (F-score ↑)
Bal. Cou.
Mus.
Pal. Tem.
31.12
19.81
38.46 29.10 43.87
27.36 28.11
56.27(+3.12) 72.46 52.59 54.27 55.83 56.80 50.88 55.05 52.32
57.10(+3.95) 74.22 56.27 54.41 56.65 54.46 51.45 57.65 51.70
64.40(+0.04) 81.45 66.22 53.88 62.94 66.12 65.35 61.31 57.90
64.51(+0.15) 81.31 65.51 55.57 63.46 66.24 65.39 61.27 57.30
33.53
20.96 40.15 32.05 46.01 29.28 32.71
32.31
23.69 37.73 30.04 41.80 28.31 32.29
38.96
28.33 44.36 39.74 52.89 33.80 34.63
32.85
22.83 39.04
33.87 45.46 27.95 27.97
37.00
24.84 44.59 34.77 46.49 34.69 36.62
37.53
26.68 42.14 35.65 49.37 32.16 39.19
34.07(+1.76) 24.05 39.20 32.17 43.95 28.62 36.46
34.31(+2.00) 26.77 39.13 31.58 44.55 28.79 35.03
39.55(+0.59) 28.61 45.63 40.21 52.81 34.40 35.62
39.43(+0.47) 29.18 45.21 39.88 53.38 34.07 34.87
UniMVSNet [Peng et al., 2022]
MVSTR [Zhu et al., 2021]
TransMVS [Ding et al., 2022]
MVSTER [Wang et al., 2022]
CostFormer(PatchMatchNet)
CostFormer(PatchMatchNet*)
CostFormer(UniMVSNet)
CostFormer(UniMVSNet*)
Table 1: Quantitative results of different methods on the Tanks & Temples benchmark (higher is better). * is pretrained on DTU and fine-tuned
on BlendedMVS. - is not pretrained on DTU and trained from scratch on Blended MVS
where RT is the k-th Regression Transformer layer with reg-
ular windows, RSTÅ is the k-th Regression Transformer layer
with shifted windows, RER is the re-embedding layer to re-
cover the depth dimension from CL, and it can be a 2D con-
volution with D output channels.
RRT also computes self-attention in a local window. Com-
pared with RDACT, RRT focuses more on spatial relations.
Compared with regular Swin [Liu et al., 2021] Transformer
block, RRT treats the depth as a channel, the number of chan-
nels is actually 1 and this channel is squeezed before the
Transformer. The embedding parameters are set to fit the cost
aggregation of different iterations. If the embedding dimen-
sion number equals D, Cout can be simply formulated as:
Cout = CL + Co
(17)
As a stage may iterate many times with different depth hy-
potheses, the number of RRT blocks should be set the same
as the number of iterations. The whole RRT is shown in the
yellow window in Figure 2.
4 Training
4.1 Loss function
Final loss combines with the losses of all iterations at all
stages and the loss from the final refinement module:
s n
Loss = ΣΣΙ + Lref
k=1i=1
(18)
where L is the regression or unification loss of the i-th iter-
ation at k-th stage. Lref is the regression or unification loss
from refinement module. If refinement module does not exist,
the Lref loss is set to zero.
4.2 Common training settings
CostFormer is implemented by Pytorch [Paszke et al., 2019].
For RDACT, we set the depth number at stages 3, 2, 1 as 4,
2, 2; patch size at height, width and depth axes as 4, 4, 1;
window size at height, width and depth axes as 7, 7, 2. If
the backbone is set as PatchMatchNet, embedding dimension
number at stages 3, 2, 1 are set as 8, 8, 4. For RRT, we set
the depth number as 2 at all stages, patch size as 1 at all axes;
window size as 8 at all axes. If the backbone is set as Patch-
MatchNet, embedding dimension number at iteration 2, 2, 1
at stages 3, 2, 1 as 32, 64, 16, 16, 8. All models are trained
on Nvidia GTX V100 GPUs. After depth estimation, we re-
construct point clouds similar to MVSNet [Yao et al., 2018].
5
Experiments
In this section, we introduce multiple MVS datasets and eval-
uate our method on these datasets. The results will be further
reported in detail.
5.1
DATASETS
The datasets used in the evaluation are DTU [Aanæs et al.,
2016], BlendedMVS [Yao et al., 2020], ETH3D [Schöps et
al., 2017], Tanks & Temples [Knapitsch et al., 2017], and
YFCC-100M [Thomee et al., 2016]. The DTU dataset is an
indoor multi-view stereo dataset with 124 different scenes,
there are 49 views under seven different lighting conditions
in one scene. Tanks & Temples is collected in a more com-
plex and realistic environment, and it's divided into the in-
termediate and advanced set. ETH3D benchmark consists of
calibrated high-resolution images of scenes with strong view-
point variations. It is divided into training and test datasets.
While the training dataset contains 13 scenes, the test dataset
contains 12 scenes. BlendedMVS dataset is a large-scale syn-
thetic dataset, consisting of 113 indoor and outdoor scenes
and split into 106 training scenes and 7 validation scenes.
5.2
Main Settings and Results on DTU
For the evaluation on the DTU [Aanæs et al., 2016] evalua-
tion set, we only use the DTU training set. During the train-
ing phase, we set the image resolution to 640 × 512. We
compare our method to recent learning-based MVS methods,
including CasMVSNet [Gu et al., 2020] and PatchMatchNet
[Wang et al., 2021b] which are also set as backbones of Cost-
Former. We follow the evaluation metrics provided by the
DTU dataset. The quantitative results on the DTU evaluation
set are summarized in Table 2, which indicates that the plug-
and-play CostFormer improves the cost aggregation. Partial
visualization results of Table 2 are shown in Figure 3.
Complexity Analysis: For the complexity analysis of Cost-
Former, we plug it into PatchMatchNet [Wang et al., 2021b]
and first compare the memory consumption and run-time with
Overall (mm)
0.777
Methods
Acc. (mm)
Comp. (mm)
Furu [Furukawa and Ponce, 2010]
0.613
0.941
Tola [Tola et al., 2012]
0.342
1.190
0.766
Gipuma [Galliani et al., 2015]
0.283
0.873
0.578
Colmap [Schönberger and Frahm, 2016]
0.400
0.644
0.532
SurfaceNet [Ji et al., 2017]
0.450
1.040
0.745
MVSNet [Yao et al., 2018]
0.396
0.527
0.462
R-MVSNet [Yao et al., 2019]
0.383
0.452
0.417
P-MVSNet [Luo et al., 2019]
0.406
0.434
0.420
Point-MVSNet [Chen et al., 2019]
0.342
0.411
0.376
Fast-MVSNet [Yu and Gao, 2020]
0.336
0.403
0.370
CasMVSNet [Gu et al., 2020]
0.325
0.385
0.355
UCS-Net [Cheng et al., 2020]
0.338
0.349
0.344
CVP-MVSNet [Yang et al., 2020b]
0.296
0.406
0.351
PVA-MVSNet [Yi et al., 2020]
0.379
0.336
0.357
PatchMatchNet [Wang et al., 2021b]
0.427
0.277
0.352
AA-RMVSNet [Wei et al., 2021]
0.376
0.339
0.357
UniMVSNet [Peng et al., 2022]
0.352
0.278
0.315
CostFormer(Based on PatchMatchNet)
0.424
0.262
0.343 (+0.0093)
CostFormer(Based on CasMVSNet)
0.378
0.313
CostFormer(Based on UniMVSNet)
0.301
0.322
0.345 (+0.0097)
0.312 (+0.0035)
Table 2: Quantitative results of different methods on DTU.
this backbone. For a fair comparison, a fixed input size of
1152 x 864 is used to evaluate the computational cost on
a single GPU of NVIDIA Telsa V100. Memory consump-
tion and run-time of PatchMatchNet [Wang et al., 2021b] are
2323MB and 0.169s. They are only increased to 2693MB and
0.231s by the plug-in.
Based on the reports of PatchMatchNet [Wang et al.,
2021b], we then get the comparison results of other state-of-
the-art learning-based methods. Memory consumption and
run-time are reduced by 61.9% and 54.8% compared to Cas-
MVSNet [Gu et al., 2020], by 48.8% and 50.7% compared
to UCSNet [Cheng et al., 2020] and by 63.5% and 77.3%
compared to CVP-MVSNet [Yang et al., 2020b]. Combining
the results(lower is better) are shown in Table 3 and Figure 1,
GPU memory and run-time of CostFormer are set as 100%.
Method
Overall (mm)
0.355
0.344
GPU Memory (%)
CasMVSNet [Gu et al., 2020]
UCSNet [Cheng et al., 2020]
CVP-MVSNet [Yang et al., 2020b]
262.47%
Run-time (%)
221.24%
195.31%
202.84%
273.97%
440.53%
0.351
Ours
100.00%
100.00%
0.343
Table 3: Comparison with other SOTA learning-based MVS meth-
ods on DTU. Relationship between overall performance, GPU mem-
ory and run-time.
Comparison with Transformers We also compare Cost-
Former with other Transformers [Zhu et al., 2021; Wang et
al., 2022; Ding et al., 2021; Liao et al., 2022] which are used
in MVS methods and not plug-and-play. For a fair compari-
son, only direct improvements(higer is better) and incremen-
tal cost of run time(low is better) from pure Transformers un-
der similar depth hypotheses are summarized in Table 4.
the Blended MVS [Yao et al., 2020] dataset. We compare
our method to those recent learning-based MVS methods, in-
cluding PatchMatchNet [Wang et al., 2021b] and UniMVS-
Net [Peng et al., 2022] which are also set as backbones of
CostFormer. The quantitative results on the Tanks & Temples
[Knapitsch et al., 2017] set are summarized in Table 1, which
indicates the robustness of CostFormer. Partial visualization
results of Table 1 are shown in Figure 4. We would like to
clarify that UniMVSNet¯ in Table 1 only uses BlendedMVS
for training which uses less data (no DTU) than the UniMVS-
Net baseline.
5.4 Main Settings and Results on ETH3D
We use the PatchMatchNet [Wang et al., 2021b] as back-
bone and adopt the trained model used in the Tanks & Tem-
ples dataset [Knapitsch et al., 2017] to evaluate the ETH3D
[Schöps et al., 2017] dataset. As shown in Table 5, our
method outperforms others on both the training and partic-
ularly challenging test datasets(higher is better).
Methods
MVE [Fuhrmann et al., 2014]
Gipuma [Galliani et al., 2015]
Training
Testing
F1 score↑
20.47
Time(s)↓
13278.69
Fl score ↑
30.37
Time(s)↓
10550.67
36.38
587.77
45.18
689.75
PMVS [Furukawa and Ponce, 2010]
46.06
836.66
44.16
957.08
COLMAP [Schönberger and Frahm, 2016]
67.66
2690.62
73.01
1658.33
PVSNet [Xu and Tao, 2020]
67.48
-
72.08
829.5
IterMVS [Wang et al., 2021a]
66.36
74.29
PatchMatchNet [Wang et al., 2021b]
PatchMatch-RL [Lee et al., 2021]
CostFormer(Ours)
64.21
452.63
73.12
492.52
67.78
68.92(+4.71) 566.18
72.38
75.24(+2.12) 547.64
Table 5: Quantitative results of different methods on ETH3D.
5.5 Main Settings and Results on BlendedMVS
dataset
We use the model used in ETH3D. On BlendedMVS [Yao et
al., 2020] evaluation set, we set N = 5 and image resolution
as 576 x 768. End point error (EPE), 1 pixel error (e1), and 3
pexels error (e3) are used as the evaluation metrics. Quanti-
tative results(lower is better) of different methods are shown
in Table 6.
Method
EPE
el (%)
e3 (%)
MVSNet [Yao et al., 2018]
1.49
21.98
8.32
MVSNet-s [Darmon et al., 2021]
1.35
25.91
8.55
CVP-MVSNet [Yang et al., 2020a]
1.90
19.73
10.24
VisMVSNet [Zhang et al., 2020]
1.47
18.47
7.59
CasMVSNet [Gu et al., 2020]
1.98
15.25
7.60
EPPMVSNet [Ma et al., 2021]
1.17
12.66
6.20
TransMVSNet [Ding et al., 2021]
0.73
8.32
3.62
CostFormer(Based on PatchmatchNet)
0.84
12.37
4.59
CostFormer(Based on UniMVSNet)
0.43
7.05
2.70
Method
MVSTR [Zhu et al., 2021]
TransMVS [Ding et al., 2021]
Trans Improvement (mm)
+0.0140
Delta Time (s)
+0.359s
+0.0160
+0.367s
Delta Time (%)
+78.21%
+135.42%
WT-MVSNet(CT) [Liao et al., 2022]
MVSTER(CNN Fusion) [Wang et al., 2022]
CostFormer(CNN Fusion)
+0.0130
+0.265s
+0.0040
+0.0097
+0.016s
+0.062s
+13.34%
+36.69%
Table 6: Quantitative results of different methods on BlendedMVS
Table 4: Quantitative improvement of performance and incremental
cost of run time of different Transformers on DTU evaluation set.
5.3 Main Settings and Results on Tanks & Temples
For the evaluation on Tanks & Temples [Knapitsch et al.,
2017], we use the DTU [Aanæs et al., 2016] dataset and
6
Conclusion
In this work, we explore whether cost Transformer can im-
prove the cost aggregation and propose a novel CostFormer
with the cascade RDACT and RRT modules. The experimen-
tal results on DTU [Aanæs et al., 2016], Tanks & Temples
[Knapitsch et al., 2017], ETH3D [Schöps et al., 2017], and
BlendedMVS [Yao et al., 2020] show that our method is com-
petitive, efficient, and plug-and-play. Cost Transformer can
be your need for better cost aggregation in multi-view stereo.
References
[Aanæs et al., 2016] Henrik Aanæs, Rasmus Ramsbøl
Jensen, George Vogiatzis, Engin Tola, and An-
ders Bjorholm Dahl. Large-scale data for multiple-view
stereopsis. Int. J. Comput. Vis., 120(2):153–168, 2016.
[Chen et al., 2019] Rui Chen, Songfang Han, Jing Xu, and
Hao Su. Point-based multi-view stereo network. In ICCV,
pages 1538-1547. IEEE, 2019.
[Cheng et al., 2020] Shuo Cheng, Zexiang Xu, Shilin Zhu,
Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao
Su. Deep stereo using adaptive thin volume representation
with uncertainty awareness. In CVPR, pages 2521–2531.
IEEE, 2020.
[Cho et al., 2021] Seokju Cho, Sunghwan Hong, Sangryul
Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong
Kim. Cats: Cost aggregation transformers for visual cor-
respondence. Advances in Neural Information Processing
Systems, 34, 2021.
[Darmon et al., 2021] François Darmon, Bénédicte Bascle,
Jean-Clément Devaux, Pascal Monasse, and Mathieu
Aubry. Deep multi-view stereo gone wild. CORR,
abs/2104.15119, 2021.
[Ding et al., 2021] Yikang Ding, Wentao Yuan, Qingtian
Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and
Xiao Liu. Transmvsnet: Global context-aware multi-
view stereo network with transformers. arXiv preprint
arXiv:2111.14600, 2021.
[Ding et al., 2022] Yikang Ding, Wentao Yuan, Qingtian
Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and
Xiao Liu. Transmvsnet: Global context-aware multi-view
stereo network with transformers. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8585-8594, 2022.
[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,
Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Min-
derer, Georg Heigold, Sylvain Gelly, et al. An image is
worth 16x16 words: Transformers for image recognition
at scale. arXiv preprint arXiv:2010.11929, 2020.
[Fuhrmann et al., 2014] Simon Fuhrmann, Fabian Langguth,
and Michael Goesele. Mve - a multi-view reconstruction
environment. In GCH, pages 11–18. Eurographics Asso-
ciation, 2014.
[Furukawa and Ponce, 2010] Yasutaka Furukawa and Jean
Ponce. Accurate, dense, and robust multiview stereopsis.
IEEE Trans. Pattern Anal. Mach. Intell., 32(8):1362–1376,
2010.
[Galliani et al., 2015] Silvano Galliani, Katrin Lasinger, and
Konrad Schindler. Massively parallel multiview stereop-
sis by surface normal diffusion. In ICCV, pages 873-881.
IEEE Computer Society, 2015.
[Gu et al., 2020] Xiaodong Gu, Zhiwen Fan, Siyu Zhu,
Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost
volume for high-resolution multi-view stereo and stereo
matching. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 2495-
2504, 2020.
[Hosni et al., 2012] Asmaa Hosni, Christoph Rhemann,
Michael Bleyer, Carsten Rother, and Margrit Gelautz. Fast
cost-volume filtering for visual correspondence and be-
yond. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 35(2):504-511, 2012.
[Ji et al., 2017] Mengqi Ji, Juergen Gall, Haitian Zheng,
Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d
neural network for multiview stereopsis. In ICCV, pages
2326-2334. IEEE Computer Society, 2017.
[Knapitsch et al., 2017] Arno Knapitsch, Jaesik Park, Qian-
Yi Zhou, and Vladlen Koltun. Tanks and temples: bench-
marking large-scale scene reconstruction. ACM Trans.
Graph., 36(4):78:1-78:13, 2017.
[Lee et al., 2021] Jae Yong Lee, Joseph DeGol, Chuhang
Zou, and Derek Hoiem. Patchmatch-rl: Deep mvs with
pixelwise depth, normal, and visibility. In Proceedings of
the IEEE/CVF International Conference on Computer Vi-
sion, October 2021.
[Li et al., 2021] Zhaoshuo Li, Xingtong Liu, Nathan
Drenkow, Andy Ding, Francis X Creighton, Russell H
Taylor, and Mathias Unberath. Revisiting stereo depth
estimation from a sequence-to-sequence perspective with
transformers. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision, pages 6197-6206,
2021.
[Liao et al., 2022] Jinli Liao, Yikang Ding, Yoli Shavit, Dihe
Huang, Shihao Ren, Jia Guo, Wensen Feng, and Kai
Zhang. Wt-mvsnet: Window-based transformers for
multi-view stereo, 2022.
[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,
Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
Swin transformer: Hierarchical vision transformer using
shifted windows. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 10012-
10022, 2021.
[Luo et al., 2019] Keyang Luo, Tao Guan, Lili Ju, Haipeng
Huang, and Yawei Luo. P-mvsnet: Learning patch-wise
matching confidence aggregation for multi-view stereo. In
ICCV, pages 10451–10460. IEEE, 2019.
[Ma et al., 2021] Xinjun Ma, Yue Gong, Qirui Wang, Jing-
wei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolar-
assembling based depth prediction for multi-view stereo.
In Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 5732-5740, 2021.
[Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco
Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,
Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank
Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-
performance deep learning library. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox,
and R. Garnett, editors, Advances in Neural Information
Processing Systems 32, pages 8024-8035. Curran Asso-
ciates, Inc., 2019.
[Peng et al., 2022] Rui Peng, Rongjie Wang, Zhenyu Wang,
Yawen Lai, and Ronggang Wang. Rethinking depth esti-
mation for multi-view stereo: A unified representation. In
Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), 2022.
[Scharstein and Szeliski, 2002] Daniel Scharstein and
Richard Szeliski. A taxonomy and evaluation of dense
two-frame stereo correspondence algorithms. Interna-
tional journal of computer vision, 47(1):7-42, 2002.
[Schönberger and Frahm, 2016] Johannes L. Schönberger
and Jan-Michael Frahm. Structure-from-motion revis-
ited. In CVPR, pages 4104-4113. IEEE Computer Society,
2016.
[Schöps et al., 2017] Thomas Schöps, Johannes L.
Schönberger, Silvano Galliani, Torsten Sattler, Kon-
rad Schindler, Marc Pollefeys, and Andreas Geiger. A
multi-view stereo benchmark with high-resolution images
and multi-camera videos. In CVPR, pages 2538-2547.
IEEE Computer Society, 2017.
[Sun et al., 2021] Jiaming Sun, Zehong Shen, Yuang Wang,
Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free lo-
cal feature matching with transformers. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 8922-8931, 2021.
[Thomee et al., 2016] Bart Thomee, David A. Shamma, Ger-
ald Friedland, Benjamin Elizalde, Karl Ni, Douglas
Poland, Damian Borth, and Li-Jia Li. Yfcc100m: the new
data in multimedia research. Commun. ACM, 59(2):64-73,
2016.
[Tola et al., 2012] Engin Tola, Christoph Strecha, and Pascal
Fua. Efficient large-scale multi-view stereo for ultra high-
resolution image sets. Mach. Vis. Appl., 23(5):903–920,
2012.
[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you
need. Advances in neural information processing systems,
30, 2017.
[Wang et al., 2021a] Fangjinhua Wang, Silvano Galliani,
Christoph Vogel, and Marc Pollefeys. Iterms: Itera-
tive probability estimation for efficient multi-view stereo.
arXiv preprint arXiv:2112.05126, 2021.
[Wang et al., 2021b] Fangjinhua Wang, Silvano Galliani,
Christoph Vogel, Pablo Speciale, and Marc Pollefeys.
Patchmatchnet: Learned multi-view patchmatch stereo. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 14194–14203,
2021.
[Wang et al., 2022] Xiaofeng Wang, Zheng Zhu, Fangbo
Qin, Yun Ye, Guan Huang, Xu Chi, Yijia He, and Xingang
Wang. Mvster: Epipolar transformer for efficient multi-
view stereo, 2022.
[Wei et al., 2021] Zizhuang Wei, Qingtian Zhu, Chen Min,
Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adap-
tive aggregation recurrent multi-view stereo network. In
Proceedings of the IEEE/CVF International Conference
on Computer Vision, pages 6187-6196, 2021.
[Xu and Tao, 2020] Qingshan Xu and Wenbing Tao. Pvs-
net: Pixelwise visibility-aware multi-view stereo network.
CORR, abs/2007.07714, 2020.
[Yan et al., 2020] Jianfeng Yan, Zizhuang Wei, Hongwei
Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping
Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-
view stereo net with dynamic consistency checking. In Eu-
ropean Conference on Computer Vision, pages 674–689.
Springer, 2020.
[Yang et al., 2020a] Jiayu Yang, Wei Mao, Jose M Alvarez,
and Miaomiao Liu. Cost volume pyramid based depth
inference for multi-view stereo. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 4877-4886, 2020.
[Yang et al., 2020b] Jiayu Yang, Wei Mao, Jose M. Alvarez,
and Miaomiao Liu. Cost volume pyramid based depth in-
ference for multi-view stereo. In CVPR, pages 4876-4885.
IEEE, 2020.
[Yao et al., 2018] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang,
and Long Quan. Mvsnet: Depth inference for unstructured
multi-view stereo. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pages 767-783, 2018.
[Yao et al., 2019] Yao Yao, Zixin Luo, Shiwei Li, Tianwei
Shen, Tian Fang, and Long Quan. Recurrent mvsnet
for high-resolution multi-view stereo depth inference. In
CVPR, pages 5525-5534. Computer Vision Foundation /
IEEE, 2019.
[Yao et al., 2020] Yao Yao, Zixin Luo, Shiwei Li, Jingyang
Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan.
Blendedmvs: A large-scale dataset for generalized multi-
view stereo networks. Computer Vision and Pattern
Recognition (CVPR), 2020.
[Yi et al., 2020] Hongwei Yi, Zizhuang Wei, Mingyu Ding,
Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing
Tai. Pyramid multi-view stereo net with self-adaptive view
aggregation. In Andrea Vedaldi, Horst Bischof, Thomas
Brox, and Jan-Michael Frahm, editors, ECCV (9), volume
12354 of Lecture Notes in Computer Science, pages 766-
782. Springer, 2020.
[Yu and Gao, 2020] Zehao Yu and Shenghua Gao. Fast-
mvsnet: Sparse-to-dense multi-view stereo with learned
propagation and gauss-newton refinement. In CVPR,
pages 1946-1955. IEEE, 2020.
[Zhang et al., 2020] Jingyang Zhang, Yao Yao, Shiwei Li,
Zixin Luo, and Tian Fang. Visibility-aware multi-view
stereo network. British Machine Vision Conference
(BMVC), 2020.
[Zhu et al., 2021] Jie Zhu, Bo Peng, Wanqing Li, Haifeng
Shen, Zhe Zhang, and Jianjun Lei. Multi-view stereo with
transformer. ArXiv, abs/2112.00336, 2021.
