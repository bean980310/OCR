arXiv:2309.00986v1 [cs.CL] 2 Sep 2023
ModelScope-Agent: Building Your Customizable Agent System with
Open-source Large Language Models
Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu
Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi
Ji Zhang, Fei Huang, Jingren Zhou
DAMO Academy, Alibaba Group, China
Abstract
Large language models (LLMs) have recently
demonstrated remarkable capabilities to com-
prehend human intentions, engage in reason-
ing, and design planning-like behavior. To
further unleash the power of LLMs to accom-
plish complex tasks, there is a growing trend to
build agent framework that equips LLMs, such
as ChatGPT, with tool-use abilities to connect
with massive external APIs.
In this work, we introduce ModelScope-Agent,
a general and customizable agent framework
for real-world applications, based on open-
source LLMs as controllers. It provides a user-
friendly system library, with customizable en-
gine design to support model training on mul-
tiple open-source LLMs, while also enabling
seamless integration with both model APIs and
common APIs in a unified way. To equip
the LLMs with tool-use abilities, a compre-
hensive framework has been proposed span-
ning over tool-use data collection, tool retrieval,
tool registration, memory control, customized
model training, and evaluation for practical
real-world applications. Finally, we showcase
ModelScopeGPT, a real-world intelligent as-
sistant of ModelScope Community based on
the ModelScope-Agent framework, which is
able to connect open-source LLMs with more
than 1000 public AI models and localized
community knowledge in ModelScope. The
ModelScope-Agent library¹ and online demo²
are now publicly available.
1 Introduction
Large language models (OpenAI, 2022, 2023;
Touvron et al., 2023; Chowdhery et al., 2022)
have gradually become common AI assistants
that demonstrate great potential in comprehend-
ing human intentions, performing complex rea-
soning tasks, and enabling content creation. De-
spite the rapid advancements of open-source LLMs,
e.g., LLAMA (Touvron et al., 2023) and Chat-
GLM (THUDM, 2023), they still remain limited
in performing complex tasks, such as following
user instructions to use external tools and capture
up-to-date information.
To further unleash the power of LLMs for real-
world practical applications, a rising trend of cur-
rent research (Schick et al., 2023; Shen et al., 2023;
Yang et al., 2023; Qin et al., 2023; Patil et al., 2023)
begins to enable LLMs with tool-use abilities to-
wards building an AI Agent. These include Hug-
gingGPT (Shen et al., 2023), Visual-ChatGPT (Wu
et al., 2023) and Gorilla (Patil et al., 2023) for
connecting with HuggingFace models, ToolAl-
paca (Tang et al., 2023) and ToolLLAMA (Qin et al.,
2023) for using massive common APIs such as
weather forecast and search engine. These methods
either directly rely on closed-source counterparts
like ChatGPT or focus on certain types of API tools.
Recently, there have also been public releases of
AI agents, such as Auto-GPT³, LangChain and
Transformers Agent (Huggingface, 2023), which
enable LLMs, such as ChatGPT or GPT-4, to use
tools and solve complex AI tasks. However, these
agents are mainly built with closed-source LLMS
and how to build a customizable agent system with
open-source LLMs remains largely unexplored.
In this work, we present ModelScope-Agent, a
general and customizable agent system for real-
world applications, based on open-source LLMs
as controllers. ModelScope³ is a public ML com-
munity, which seeks to bring together the most ad-
vanced machine learning models from the AI com-
munity, and streamlines the process of leveraging
Al models in real-world applications. ModelScope-
Agent provides a flexible and user-friendly sys-
tem library, with customizable engine design to
*Corresponding author: <ym119608@alibaba-inc.com>
https://github.com/modelscope/modelscope-agent
2https://modelscope.cn/studios/damo/ModelScopeGPT/summary "https://modelscope.cn/models
https://github.com/Significant-Gravitas/Auto-GPT
https://github.com/langchain-ai/langchain
support model training on multiple open-source
LLMs, while also enabling seamless integration
with both model APIs and common APIs in a uni-
fied way. It features an LLM-centric system de-
sign, which includes open-source LLMs as core
controller, and further interact with a tool-use mod-
ule and a memory module to accomplish complex
tasks. At the core of ModelScope-Agent, the li-
brary supports flexible selection and training on var-
ious open-source LLMs, such as LLAMA (Touvron
et al., 2023), ChatGLM (THUDM, 2023), Chat-
PLUG (Tian et al., 2023) and other customized
LLMs in ModelScope. For tool use, ModelScope-
Agent provides a default tool library, which sup-
ports diverse AI model APIs across NLP, CV, Au-
dio and Multi-model fields, as well as massive com-
mon APIs such as search engine. It also supports
registering new self-defined API plugins and auto-
matic API retrieval from the large tool library. It is
easy for users to customize their most appropriate
LLMs, local API tools and functions to develop
real-world applications. Moreover, a memory mod-
ule is also introduced to better store and manage the
system message, user history, in-context examples,
tool message and localized knowledge.
To enable the open-source LLMs to better con-
trol the whole agent system, we further propose
a comprehensive framework of tool-use data col-
lection, customized model training, evaluation and
deployment. Notably, we release a comprehen-
sive tool-enhanced dataset MSAgent-Bench, which
consists of 598k dialogues with various API cat-
egories, multi-turn API calls, API-Oriented QA,
and API-Agnostic instructions in both English and
Chinese. A simple training strategy of Weighted
LM, that enhances the training of generation of
API name and parameters, is used to better ensure
the correctness of API calls. Besides, an evalua-
tion framework is also supported in our library to
examine the tool-use abilities of the trained mod-
els in different aspects. Furthermore, we applied
ModelScope-Agent in a real-world application of
ModelScope Community namely ModelScopeGPT,
which is able to connect open-source LLMs with
more than 1000 public AI models and access lo-
calized community knowledge in ModelScope for
community QA.
To summarize, ModelScope-Agent is a general
and customizable agent system designed for devel-
opers to harness the power of open-source LLMs.
The library targets the following goals:
•
•
Agent based on Open-Source LLMs: the con-
troller of ModelScope-Agent can be flexibly
selected from open-source LLMs that are opti-
mized through our agent training framework.
Support and Customization of Diverse Tools:
Dozens of diverse model APIs and common
APIs are given by default. The library sup-
ports registering new self-defined APIs and
automatic API retrieval from the toolset.
• Customizable of Applications: ModelScope-
Agent can be flexibly applied in various in-
dustry applications. The agent and training
framework are documented describing its us-
age, construction and optimization.
ModelScope-Agent is in continual development
by the engineers at ModelScope and is released
under an Apache 2.0 license. Full documentation
is available through the project website.
2 The ModelScope Agent
ModelScope-Agent is designed to facilitate devel-
opers in building customizable agent systems based
on open-source LLMs. The overall system architec-
ture is shown in Figure 1. includes open-source
LLMs as controller, a tool-use module and a mem-
ory module to interact with. Given human instruc-
tion, the Agent, which adopts the selected LLM as
the controller, will automatically plan tasks, selec-
tively uses tools, leverage knowledge in memory,
and finally provides helpful responses to users.
2.1 LLMs as Brain
LLMs serve as the brain of the agent, responsible
for planning and decomposing user requests, se-
lectively calling tools, performing retrieval, and
integrating all the information from previous steps
to generate the final response. In order to make it
easier for users to customize the agent with their
own LLMs, we have added support for various
open-source LLMs by default, such as LLAMA,
ChatGLM and ChatPLUG, which have been op-
timized through our tool learning pipeline. The
details of training strategy and tool-use datasets
can be referred to Section 3. ModelScope-Agent
has integrated the LLM inference pipeline of the
ModelScope community, and replacing LLMs can
be done by simply setting the model_name and
model_config. In model_config, the model_id,
model_revision, and model parameter settings such
as max sequence length, should be configured.
Training Framework
Data Collection
Model API
Common API
API-Oriented QA
⚫ API-Agnostic
Agent Pipeline
Agent Execution
Tool Retrieval
LLM Training
0%
Memory Control
LLM as
Brain
Task Planning
ChatGLM
Deploy
Weighted LM
Tool Use
Evaluation
Automatic Eval
EM
Rouge-L
F1
Human Eval
API Execution
Response Generation
Memory Control
Knowledge Retrieval
Prompt Generator
embedding
System prompt
API schemas
• Knowledge
System Module
Tool Use
Tool Library
AI Models
• Text-to-Image
• Text-to-Video
• Text-to-Audio
•Image-Chat
• Text-translation
• Universal-IE
Common APIs
• Weather
Web-Search
• Calculator
• Map
• Music-Player
Shopping
Tool Retrieval
Tool Customization
Figure 1: The overall system architecture of ModelScope-Agent.
# LLM config "cfg_file"
from modelscope.utils.config import Config
model_cfg = Config.from_file(cfg_file)
11m Local LLM (model_name, model_cfg)
Furthermore, the ModelScope-Agent also pro-
vides a standard way to integrate new LLM. Users
can add their own LLMs, by integrating the LLM
pipeline into ModelScope. After that, the agent can
select the new LLMs for training and inference.
2.2 Tool Use
Tool Library The tool library is used to config-
ure and manage various collections of APIs used in
the agent. ModelScope-Agent can support a wide
range of both common APIs such as search APIs,
and AI model APIs across NLP, CV, Audio and
Multi-modal models in ModelScope and Hugging-
Face. Each tool API consists of the API name, de-
scription, parameters and request functions. Users
can easily choose and configure proper APIs in
the library to build their own agent. The default
APIs supported in the library can be referred to
Appendix A.1.
#23 tool default config file "default_file"
tool_cfg = Config.from_file(default_file)
Register and Customize New Tool The agent
allows users to register and customize new tools,
while also supporting quick integration of newly
registered tools into the agent, enabling LLMs to
selectively use the additional self-defined tools for
specific applications. This can be simply done
by inheriting from a base class, namely Tool, and
defining a new CustomTool with the API-related
schema of API name, description, parameters, and
request functions. More details about Custom Tool
can be referred in Appendix A.2.
from modelscope_agent. tools import Tool
class Custom Tool (Tool):
#23 logic added here
# refer example in Appendix A.2
tool_list = {'customo-tool': Custom Tool () }
Tool Retrieval and Execution Due to the large
amount of tool APIs in the tool library, a tool
retrieval module is further introduced to recom-
mend appropriate APIs for each instruction prompt.
Specifically, we use the dense vector retrieval
method based on the unified multilingual text-
embedding API 6. We vectorize both the text de-
scriptions of the APIs and the instruction prompt
using the text-embedding API. The top-3 most rel-
evant APIs with the highest vector product scores
are selected for tool use. As a result, the schema
information of the retrieved APIs will be concate-
nated with other system prompts in the subsequent
memory module and sent to LLMs as input. With
the concatenated instruction prompt, the LLMs will
plan and generate the API request, which will be
executed by the agent. The agent will then return
the results to the LLMs for continuous generation.
2.3 Memory Control
The memory module is used to retrieve, and assem-
ble a series of contextual information as input to the
LLMs. It consists of a knowledge retrieval submod-
ule and a prompt generator submodule, which are
responsible for external knowledge retrieval and
instruction prompt generation, respectively.
"https://help.aliyun.com/zh/dashscope/getting-started-1
Knowledge Retrieval It enables the agent to
get access to up-to-date and localized information
related with query prompt, thereby augmenting
LLMs with dynamic and domain-specific knowl-
edge. We follow the same dense vector retrieval
method as the previous tool retrieval module, and
support large-scale knowledge retrieval from local-
ized document corpus. Similarly, it allows users
to customize by changing to other open-source re-
trieval frameworks.
Prompt Generator The prompt generator is used
to assemble all available contextual information
such as system prompt, API schema, retrieved
knowledge, conversation history, and few-shot ex-
amples. According to the type of user query and
the maximum length of the LLM, the users can
selectively choose proper contextual information
and assemble the required input to the LLM. In our
agent, the prompt generator needs to be defined
before the agent is constructed.
2.4 Agent Pipeline
In summary, we build the agent by combining all
the modules: LLM controller, tool-use module, and
memory module. With agent.run, the agent can ef-
ficiently execute and complete the instruction in
a one-step generation. First, the agent retrieves
query-related tools through the tool retrieval and
combines the retrieved API schema with other con-
textual prompts in memory module, to construct
a new instruction prompt. Then, the agent sends
this new prompt to the LLM, who plans whether
and which API to call and generate an API request.
Next, the agent will execute the selected API with
the extracted API parameters and return the API
results to the LLMs, which will continue to plan
whether to call other APIs. If another API call
is needed, the process is repeated, otherwise, the
LLMs generate the final response and the agent
returns the final result to the user.
agent = AgentExecutor (1lm, tool_cfg,
additional_tool_list = tool_list)
agent.run("Draw a logo image of agent")
3 Training
3.1 Dataset
To facilitate building an agent with the ability
to use tools while upholding an optimal level of
user engagement, we release a comprehensive tool
dataset, MSAgent-Bench7, utilizing ChatGPT syn-
thetic data and the existing instruction-following
datasets. Our released dataset encompasses 598k
dialogues. Table 1 outlines the key differences
between the released dataset and other public avail-
able tool learning datasets, while the data distribu-
tion of our dataset is illustrated in Figure 2. As
demonstrated in the Table and Figure, we have
made certain efforts to construct a comprehensive
dataset which enables the effective training of an
agent:
Multilingual: We collect instances in both Chi-
nese and English, ensuring that the trained agent is
capable of functioning in both languages.
Various API Categories: Our dataset supports
Common APIs that have been registered by users
or applied through online API platforms, as well as
model APIs that can call neural models.
Multi Turn Dialog: In real-life scenarios, agents
may need to request more specific clarification
from users to complete a task or receive additional
instructions after completing a previous task. Our
dataset accounts for these scenarios and supports
multi-turn user-agent interactions when using tools.
API-Oriented QA: An effective agent should pos-
sess knowledge of APIs. Our dataset incorporates
API document QA tasks and task planning tasks
which requires agents to offer appropriate sugges-
tions to users on how to use various APIs to solve
complex tasks.
API-Agnostic Instructions: To enhance the
agent's ability to follow common instructions and
increase user engagement, we have incorporated
both Chinese and English API-agnostic instructions
within our dataset. These instructions place greater
emphasis on the agent's inherent capabilities rather
than reliance on API invocation.
The data was collected by prompting ChatGPT
(gpt-3.5-turbo) to generate instructions, API re-
quests, and answers based on the API calling re-
sults, more details can be accessed in Appendix D.
3.2 Model Training
We use the MSAgent-Bench to fine-tune multi-
ple open-source LLMs, including LLAMA (Tou-
vron et al., 2023), Qwen (QwenLM, 2023), Chat-
PLUG (Tian et al., 2023) etc. We train all the open-
source LLMs in a multi-round conversation mode
and concatenate all the prompts and answers. Com-
"https://modelscope.cn/datasets/damo/MSAgent-
Bench/summary
Dataset
Language
Instance Type
# Instances
API type
Avg. Turn
Avg. Step
API-Bank (Li et al., 2023)
English
Tool Use
ToolAlpaca (Tang et al., 2023)
English
Tool Use
264
3.9 K
Common API
3.27
1.92
Common API
1
1.66
Gorilla (Patil et al., 2023)
English
Tool Use
16.4 k
Model API
1
1
GPT4Tools (Yang et al., 2023)
English
Tool Use
71.4 K
Model API
1
1
ToolBench (Qin et al., 2023)
MSAgent-Bench (ours)
English
Tool Use
26.9 K
English + Chinese Tool Use + Common Chat
598 K
Common API
Common API + Model API
1
4.1
1.52
1.31
Table 1: The statistics of MSAgent-Bench and other existing tool learning datasets.
Model API
Text-to-Image
Text-to-Video
API-Oriented QA
Document QA
Task Planning
•
•
Text-to-Audio
Translation
Image Chat
Universal IE
Common API
.
Weather
Web Search
Calculator
Map
MSAgent-
Bench
•
API-Agnostic Instructions
Story Generation
Open QA
Code
Chit Chat
Paraphrase
STEM
Role Play
Figure 2: The instance types and distribution of our collected MSAgent-Bench.
pared to common instruction tuning data, the tool
learning samples focus more heavily on the accu-
racy of tool selection and API parameter prediction.
Therefore, we propose a simple training strategy,
Weighted LM, which enhances the training of gen-
eration of API name and parameters, while zero-
out the loss of tokens from the user prompt and the
tool execution. More details can be be referred to
Appendix B.3.
kwargs = dict(model = model, ...)
trainer: Epoch Based Trainer = build_trainer
measures the similarity between the generated re-
sponse and the gold answer. Additionally, we intro-
duce a novel metric called Argument F1 for fully
evaluating the quality of API requests. To com-
pute Argument F1, we categorize the arguments in
agent's API request into two cases, namely Half
match (HM) and Full match (FM), representing
correct argument but with wrong value and correct
argument with correct value, respectively. Suppose
the gold argument number in the API is |A|, and
the number of arguments in the agents API request
(name= args.trainer, default_args=kwargs) is |A*|, we compute the new Recall and Precision
trainer. train ()
4 Evaluation
Our evaluation system, MSAgent-Eval, comprises
two modules: an automatic evaluation framework
which comprehensively evaluates API usability of
the agents, and a human evaluation framework im-
plemented by an agent arena which reflects the
preferences of human users.
4.1 Automatic Evaluation Framework
In automatic evaluation, we mainly focus on eval-
uating agent's ability to generate accurate API re-
quest and the proper answers according to the API
calling results. Specifically, we use the action ex-
actly match score (Action EM) which measures
whether the agent uses the correct API as the ref-
erence gold API, and the ROUGE-L score which
as follows:
R = (0.5 x #HM + # FM)/|A|
P = (0.5 × # HM + # FM)/|A*|
(1)
(2)
and the final argument F1 is computed as:
F1 = 2(RP)/(R+P).
(3)
A sample code for the automated evaluation of
agents is provided below:
from tool_agent_finetune import evaluation
EM, F1, ROUGE = evaluation (refs, preds)
Expert annotators were engaged to annotate the
evaluation instances, with the task of providing
diverse instructions, manually documenting cor-
rect API calling requests, and writing appropriate
responses. The statistics of our currently assem-
bled test data is in Appendix B.1, and the auto-
matic evaluation scores of our trained agents can
Can you write a short promotional copy about vision pro VR glasses in 20 words, and
reading the generated content with a female voice
Experience a world beyond your wildest dreams with vision pro VR glasses.
Selecting the "modelscope speech-generation" API, the API is currently being executed.
The API call has been completed.
0:00/005
С
I will call an image generation api to generate image
{
9
And can you draw me an image of the VR glasses.
10
11
Selecting the "modelscope_image-generation" API, the API is currently being executed.
The API call has been completed.
-API Register
[1v
2 "plugin_name": "RenewInstance",
3v plugin_schema_for_model": {
"name": "RenewInstance",
"description": "Renew a Subscription ECS ins
"url": "http://123.56.163.126:8120/",
"paths": [
"name": "RenewInstance",
"method": "post",
"description": "Renew a Subscription ECS
"parameters": [
"name": "instance_id",
"description": "ECS instance id,
"required": true
13
14
15
16
17
18
19
"name": "period",
20
21
22
23
24
25
"description": "Renewal duration
"required": true
Please help me renew an ECS instance with the instance ID of i-
rj90a7e840y5csv9nh2a for 10 months.
▸ Selecting the "RenewInstance" API, the API is currently being executed...
The API call has been completed.
I am happy to help you renew your ECS instance with the instance ID of i-
rj90a7e840y5csv9nh2a for 10 months. The renewal process has been
successfully completed.
Another ECS instance, ID: i-rj90a7e840y5csv9nh4b, for 12 months
▸ Selecting the "RenewInstance" API, the API is currently being executed...
►The API call has been completed.
I am happy to help you renew your ECS instance with the instance ID of i-
rj90a7e840y5csv9nh4b for 12 months. The renewal process has been
successfully completed.
clear image
send regenerate
(a) ModelScope Intelligent Assistant
Figure 3: Demo cases of ModelScopeGPT based on ModelScope-Agent.
(b) Register and Use New Tools on Alibaba Cloud
be found in Appendix B.2. We also guarantee the
users to upload their own annotated test examples
to accurately evaluate the performance of agents in
customized scenarios.
8
4.2 Human Evaluation with Agent Arena
Inspired by the Arena for ChatBots (Zheng et al.,
2023), we have built an accessible Agent Arena
that allows users to furnish instructions to two
anonymous agents, based on the provided APIs.
Subsequently, users have the opportunity to vote
on which Agent performs better in tackling the in-
struction with the given APIs. In accordance with
the framework presented by Zheng et al. (2023),
we adopt a system of ELO ratings and leaderboard
maintenance for the participating Agents.
5 Usage Example of ModelScopeGPT
In this section, we showcase a successful
application of ModelScope Community, Mod-
elScopeGPT9, based on our ModelScope-Agent.
ModelScope Intelligent Assistant Based on
ModelScope-Agent, we have developed an intel-
ligent assistant for the ModelScope Community,
namely ModelScopeGPT. It uses LLMs as a con-
troller to connect dozens of domain-specific AI
models in the ModelScope open-source community,
covering NLP, CV, Audio, and Multi-Modal fields.
To make the pipeline more practical, we have in-
cluded API retrieval and knowledge retrieval tool to
automatically select proper APIs and get access to
the local ModelScope knowledge. As shown in Fig-
ure 3a, ModelScopeGPT can support API calls in
multi-turn conversations and generate correct API
https://modelscope.cn/studios/LLMZOO/Chinese-
Arena/summary
"https://modelscope.cn/studios/damo/ModelScopeGPT
/summary
call parameters using information from previous
conversations. More cases can refer to Appendix C.
As a result, ModelScopeGPT has achieved a total
request number of over 170k from 40k user visits
within one month after its release.
Register and Use New Tools Another key fea-
ture of an agent is its generalization capability to
unseen APIs. This allows users to quickly register
their own APIs and customize their specific applica-
tions. Therefore, we test the generalization ability
of ModelScopeGPT by applying it to an Alibaba
Cloud application scenario. As shown in Figure 3b,
we first found an API for renewing an ECS in-
stance on Alibaba Cloud. Then, we registered the
API schema defined in the tool library to the agent.
Finally, we entered the prompt "Please help me re-
new an ECS..." in the demo. The agent generated a
request through planning, selected the appropriate
API, called the API to renew the instance success-
fully, and provided a reply to inform the user that
the renewal was completed. This test demonstrates
that the open-source LLM optimized based on the
released API dataset has a strong generalization
ability towards unseen APIs.
6 Conclusion
ModelScope-Agent aims to facilitate building AI
Agent applications and research based on open-
source LLMs by providing a general and customiz-
able agent framework covering flexible system de-
sign, data collection, model training, evaluation
and usage example in real-world application. It
provides an open-source, community-driven library
towards AI Agent learning and best practices for
building an agent system with open-source LLMs.
We hope ModelScope-Agent can help pave the way
towards a new era of AI Agent.
Ethics Statement
Intended Use. ModelScope-Agent is designed
to facilitate building AI Agent applications and
research based on open-source LLMs, by providing
a general and customizable agent system.
Potential Misuse. Although we have only trained
with the tool-use datasets and gone through certain
data filtering rules, it is still possible that the cus-
tomized model may generate some biased, fake,
and unsafe information. Our agent framework also
provides users with the freedom to select proper
LLMs and upload their own clean data for training.
It is also important to design specific methods to
improve the safety of the agent framework in the
future.
References
Michael Ahn, Anthony Brohan, Noah Brown, Yev-
gen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu,
Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,
Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-
month, Nikhil J Joshi, Ryan Julian, Dmitry Kalash-
nikov, Yuheng Kuang, Kuang-Huei Lee, Sergey
Levine, Yao Lu, Linda Luu, Carolina Parada, Pe-
ter Pastor, Jornell Quiambao, Kanishka Rao, Jarek
Rettinghouse, Diego Reyes, Pierre Sermanet, Nico-
las Sievers, Clayton Tan, Alexander Toshev, Vincent
Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,
Mengyuan Yan, and Andy Zeng. 2022. Do as i can,
not as i say: Grounding language in robotic affor-
dances. arXiv preprint arXiv:2204.01691.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Merouane Debbah, Etienne Goffinet, Daniel Heslow,
Julien Launay, Quentin Malartic, et al. 2023. Falcon-
40b: an open large language model with state-of-the-
art performance.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. CORR, abs/2204.02311.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks,
Johannes Welbl, Aidan Clark, et al. 2022. Train-
ing compute-optimal large language models. arXiv
preprint arXiv:2203.15556.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Liang, Pete Florence, Andy Zeng, Jonathan Tompson,
Igor Mordatch, Yevgen Chebotar, Pierre Sermanet,
Tomas Jackson, Noah Brown, Linda Luu, Sergey
Levine, Karol Hausman, and brian ichter. 2023. In-
ner monologue: Embodied reasoning through plan-
ning with language models. In Proceedings of The
6th Conference on Robot Learning, volume 205 of
Proceedings of Machine Learning Research, pages
1769-1782. PMLR.
Huggingface. 2023.
Transformers agent. Website.
https://huggingface.co/docs/transformers/
transformers_agents.
Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu,
Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-
bank: A benchmark for tool-augmented llms. arXiv
preprint arXiv:2304.08244.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
Adam Roberts, Stella Biderman, Teven Le Scao,
M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey
Schoelkopf, et al. 2022. Crosslingual generaliza-
tion through multitask finetuning. arXiv preprint
arXiv:2211.01786.
OpenAI. 2022. Chatgpt: Optimizing language models
for dialogue.
OpenAI. 2023.
abs/2303.08774.
GPT-4 technical report. CORR,
Shishir G. Patil, Tianjun Zhang, Xin Wang, and
Joseph E. Gonzalez. 2023. Gorilla: Large language
model connected with massive apis. arXiv preprint
arXiv:2305.15334.
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen,
Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,
Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,
Huadong Wang, Cheng Qian, Runchu Tian, Kunlun
Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen
Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,
Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,
Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,
Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng
Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and
Maosong Sun. 2023. Tool learning with foundation
models. arXiv preprint arXiv:2304.08354.
QwenLM. 2023. Qwen-7b.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, et al. 2021. Scaling language models:
Methods, analysis & insights from training gopher.
arXiv preprint arXiv:2112.11446.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
arXiv preprint arXiv:2302.04761.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugging-
gpt: Solving ai tasks with chatgpt and its friends in
hugging face. arXiv preprint arXiv:2303.17580.
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,
Qiao Liang, and Le Sun. 2023. Toolalpaca: Gener-
alized tool learning for language models with 3000
simulated cases. arXiv preprint arXiv:2306.05301.
THUDM. 2023. Chatglm. https://github.com/
THUDM/ChatGLM-6B.
Junfeng Tian, Hehong Chen, Guohai Xu, Ming Yan,
Xing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu,
Wenshen Xu, Haiyang Xu, Qi Qian, Wei Wang, Qing-
hao Ye, Jiejing Zhang, Ji Zhang, Fei Huang, and
Jingren Zhou. 2023. Chatplug: Open-domain gen-
erative dialogue system with internet-augmented in-
struction tuning for digital human. arXiv preprint
arXiv:2304.07849.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. arXiv
preprint arXiv:2302.13971.
Chenfei Wu, Shengming Yin, Weizhen Qi, Xi-
aodong Wang, Zecheng Tang, and Nan Duan.
2023. Visual chatgpt: Talking, drawing and edit-
ing with visual foundation models. arXiv preprint
arXiv:2303.04671.
Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge,
Xiu Li, and Ying Shan. 2023. Gpt4tools: Teaching
large language model to use tools via self-instruction.
arXiv preprint arXiv:2305.18752.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judg-
ing llm-as-a-judge with mt-bench and chatbot arena.
arXiv preprint arXiv:2306.05685.
A Library
A.1 Tool List
API Name (language)
Text-to-Image(en)
Text-to-Image(zh)
Text-to-Video(en)
Text-to-Audio(en)
Text-to-Audio(zh)
Image-Chat(en)
Translation-zh2en
Translation-en2zh
Universal-IE(zh)
Text-to-Geographic(zh)
NER(zh)
API-Retrieval
ModelScope-Retrieval
Description
Converts text to an image.
Converts text to an image.
Converts text to a video.
Туре
Model API
Model API
dataset consisted of 360 conversations with 2059
text snippets as the references to be compared with
the agent prediction, which comprise 798 API re-
qusts and 1261 plain text answers according to the
previous calling results.
Converts text to audio.
Converts text to audio.
Image chat.
Translates Chinese text to English.
Translates English text to Chinese.
Extracts structured information.
Extracts geographic information.
Recognizes named entities in text.
Retrieves relevant APIs
Retrieves modelscope docs.
Model API
Model API
Model API
Model API
B.2 Evaluation Results
Model API
Model API
Model API
Model API
Model
ROUGE-L
Model API
Common API
Common API
Action EM Argument F1
ChatGPT (2-shot)*
36.70
34.82
25.51
LLAMA
39.16
58.60
44.98
ChatPLUG¹¹
MSAgent-Qwen¹2
46.45
68.29
55.12
51.35
87.23
68.09
Table 2: The statistics of default tool list. Supported
input languages for the APIs are listed in parentheses.
A.2
Custom Tool
User can customize their own tools by inheriting a
base tool and defining the tool names, descriptions,
and parameters according to a pre-defined schema.
Moreover, you can implement _local_call() or _re-
mote_call() depending on your specific require-
ments. To illustrate, below is an example of a
custom tool:
class Custom Tool (Tool):
description = 'xxx'
name = 'XXX'
parameters: list =
}]
'name': 'XXX'
[ {
'description': 'xxx',
'required': True
def _local_call():
def _remote_call():
Table 3: Automatic evaluation results. * represents that
we do not fine-tune ChatGPT but use in-context learning
with 2 demonstrations.
We compare the models trained in our proposed
ModelScopeGPT. The automaction evaluation re-
sults are shown in Table 3. Based on the findings
obtained from our experimentation, it is evident
that ChatGPT with in-context learning yielded infe-
rior results as compared to other models that were
subjected to finetuning. Furthermore, LLAMA un-
derperformed when compared to other finetuned
models. Our error study revealed that the lower
performance of ChatGPT and LLAMA could be at-
tributed to a large proportion of Chinese test cases
in our test set. The models (ChatPLUG, Qwen) that
performed better were those that predominantly fo-
cused on Chinese data. Our investigation revealed
that ChatGPT and LLAMA exhibited limitations
in user intent recognition, which ultimately led
to their suboptimal performance on Action EM.
Among the models examined, Qwen displayed the
most favorable performance, which could be at-
tributed to the superior performance of its basic
model.
B Experiment Setup
Evaluation Benchmark
B.1
To assess the generalization of the trained agent,
we include 10 in-domain APIs that appear in the
training set of ModelScope-Agent and 10 real un-
seen APIs 10. We also account for the multi-turn
ability of the agent by annotating several multi-turn
scenarios in our evaluation benchmark. Our test
instances were annotated by asking the human ex-
perts to write diverse instructions first. Then the
human experts were ask to write the JSON API
request and answer the instructions properly after
obtaining the API calling results. Our final testing
10 In progress, we will include more APIs in the future.
B.3 Weighted LM
We give an example of the training strategy
Weighted LM. As show in Figure 4, tokens with
different colors have different loss weights. For the
user input prompt, we set the loss weight to 0, so
that the model does not calculate the loss for the
prompt. For the API-Agnostic text of the assistant,
we keep the loss weight as 1. Finally, for the im-
portant text of the API calling, such as API name,
parameters, URL, etc., we set the loss weight to 2,
which can improve the generation accuracy of API
calling.
User: Write a brief introduction about VR glasses in 10 words
Assistant: Wearable devices for immersive virtual reality experience.
User: read it out loud in a female voice.
Assistant <startofthink> { "api_name": "modelscope_speech-generation", "url": "http://33.57.174.141:5000/damo/speech sambert-
hifigan tts zh-cn 16k", "parameters":{"text": "Wearable devices for immersive virtual reality experience.", "gender": "woman"}}<endofthink>
<starofexec>{"result":"<audio id=audio controls= preload=none> <source id=wav src=ursfe2.wav> </audio>"}<endofexec>
<audio id audio controls= preload=none> <source id=wav src=ursfe2.wav> </audio>
Loss weight
0.0
1.0
2.0
Figure 4: Example of training strategy for weighted LM. Different colored tokens have different loss weights.
Generate a video: two cats are playing
I will call ModelScope's video generation model api to generate a
video
▸ Selecting the "modelscope_video-generation" API, the API is currently
being executed...
The API call has been completed.
clear
image
生成一个两个猫在玩耍的视频
國目前 ModelScope 只有支持英文的视频生成模型,所以我将首先把您的问题利
用 ModelScope 的中英翻译模型转化为英文,然后再调用英文的视频生成模型
▸ Selecting the "modelscope_text-translation-zh2en" API, the API is currently
being executed...
► The API call has been completed.
翻译后的指令: Video of two cats playing
调用视频生成模型生成视频………………
▶ Selecting the "modelscope_video-generation" API, the API is currently
being executed...
► The API call has been completed.
send regenerate
clear
image
send regenerate
Figure 5: Single-step tool-use instructions, text-to-video cases. We have captured a few frames of the video to
display. Testing the model using the same semantic instruction in both English (left) and Chinese (right).
I will call a ModelScope's multimodal dialog model api.
<->
what's in this picture
▸ Selecting the "modelscope_image-chat" API, the API is currently being
executed...
►The API call has been completed.
The image features a blue and purple pixelated face, with a large,
white eye in.
Currently, ModelScope only supports the LLM of the multilingual ChatGPT model
that is compatible with English, which is named mPLUG-owl. Therefore, I will first
use ModelScope's Chinese-to-English translation model to convert your prompt into
English, and then call the English mPLUG-wl model. Calling the Chinese-to-
English translation model to convert the prompt into English.
<->
这幅图里有什么
國 目前 ModelScope 只有支持英文的多模态类 ChatGPT 的 LLM,名字叫
mPLUG-owl,所以我将首先把您的问题利用 ModelScope 的中英翻译模型转
化为英文,然后再调用英文的 mPLUG-Owl 模型
调用中英翻译模型转化 prompt为英文
▸ Selecting the "modelscope_text-translation-zh2en" API, the API is currently
being executed...
►The API call has been completed.
N: What's in this picture?
然后调用 mPLUG-owl 返回结果
►Selecting the "modelscope_image-chat" API, the API is currently being
executed...
►The API call has been completed.
The image features a blue and purple pixelated face, which appears to
be a cartoon.
clear image
send regenerate
clear
image
send regenerate
Figure 6: Single-step tool-use instructions, image-chat cases. Testing the model using the same semantic instruction
in both English (left) and Chinese (right).
C Cases
In this section, we show the qualitative results
about ModelScopeGPT implementation based on
ModelScope-Agent.
Single-step Tool Use As shown in Figure 5 and
6, the instruction expects the model to generate a
video and chat about the image respectively. These
instructions can be completed with a single step of
tool use.
Multi-step Tool Use As shown in Figure 7, the
instruction expects the model to write the promo-
tional copy first, then read it, and finally generate a
video. These instructions require the model to have
the ability of multi-step Tool use. In the Chinese
case, our model accurately completed the three-
step tool use.
Multi-turn Tool Use As shown in Figure 8, the
instruction requires the model to have the ability to
multi-turn conversation and use the history conver-
sation. Our model can accurately call the API and
capture the content of the previous conversation to
generate API parameters.
写一篇关于 Vision Pro VR 眼镜的 20字宣传文案,并用女声读出来,同时生成个视频看看
Write a 20-word promotional copy about Vision Pro VR glasses, have a female voice read it out,
and create a video to see how it looks.
Experience a whole new world with Vision Pro VR. Say goodbye to limitations and immerse
yourself in vivid, lifelike visuals. Get your hands on these cutting-edge glasses today!
►Selecting the "modelscope_speech-generation" API, the API is currently being executed....
The API call has been completed.
► 0:00 / 0:12
►Selecting the "modelscope_video-generation" API, the API is currently being executed....
The API call has been completed.
Vision Pro VR Glasses: Realistic Scenes, Immersive Experience, Extraordinary Sensations!
② Vision Pro VR 眼镜:真实场景,身临其境,超凡体验!!
►Selecting the "modelscope.speech-generation" API, the API is currently being executed..
The API call has been completed.
0:00/0:04
Currently, ModelScope only supports video generation models for English language, so I will first use ModelScope's English-Chinese
translation model to translate your guestion into English and then call the English video generation model.
目前 ModelScope 只有支持英文的视频生成模型,所以我将首先把您的问题利用 Modelscope 的中英翻译
模型转化为英文,然后再调用英文的视频生成模型
▸ Selecting the "modelscope_text-translation-zh2en" API, the API is currently being executed...
The API call has been completed.
EN: Vision Pro VR glasses: real scene, immersive, extraordinary experience!
Translated instruction
Call the video generation model to generate video.
调用视频生成模型生成视频中
Selecting the "modelscope_video-generation" API, the API is currently being executed...
The API call has been completed.
send regenerate
send
Fegenerate
clear
image
clean
image
Figure 7: Multi-step tool-use instructions. We have captured a few frames of the video to display. Testing the model
using the same semantic instruction in both English(left) and Chinese(right).
Write a brief story using only 20 words
写一个 20 字左右简短小故事
Lost in the city, he met her. Together they overcame adversity and
found their way back home.
从前,有一只聪明的狐狸,它善于骗取其他动物的食物。一天,它看到了一只可
爱的小兔子,于是它决定装成受伤的样子,向小兔子求救。小兔子心地善良,把
狐狸带回家给它治疗。狐狸趁机逃走,并骗取了小兔子的食物。
read the story with a female voice
Selecting the "modelscope_speech-generation" API, the API is currently
being executed...
Once upon a time, there was a clever fox who was good at tricking other animals to get their
food. One day, it saw a cute little rabbit and decided to pretend to be injured, asking for help.
from the rabbit. The kind-hearted rabbit took the fox home and treated its injuries. However,
the fox took advantage of the situation and ran away, stealing the rabbit's food.
用女生念出来
The API call has been completed.
0:00/0:06
►Selecting the "modelscope_speech-generation" API, the API is currently
being executed...
The API call has been completed.
► 0:00 / 0:17
And could you draw me a picture of the story
►Selecting the "modelscope_image-generation" API, the API is currently
being executed...
The API call has been completed.
clear
image
给这个故事配一张图
好的,我将调用 ModelScope 魔搭社区的图片生成模型为您作画
►Selecting the "modelscope_image-generation" API, the API is currently
being executed...
The API call has been completed.
send regenerate
clear
image
send regenerate
Figure 8: Multi-turn tool-use instructions, text-to-speech and text-to-image cases. Testing the model using the same
semantic instruction in both English(left) and Chinese(right).
介绍下魔搭社区
Introduce ModelScope
►Selecting the "modelscope_search" API, the API is currently being executed...
►The API call has been completed.
ModelScope 社区成立于2022年6月,是一个模型开源社区及创新平台,由阿里巴巴达摩院,联合 CCF
开源发展委员会,共同作为项目发起方。社区联合国内 AI 领域合作伙伴与高校机构,致力于通过开放的社区
合作,构建深度学习相关的模型开源,并开源相关模型服务创新技术,推动模型应用生态的繁荣发展。
ModelScope Community was established in June 2022 as an open-source community and innovation
platform for model development. It is initiated by Alibaba DAMO Academy and the Open Source
Development Committee of the China Computer Federation (CCF). The community collaborates.
►Selecting the "modelscope_search" API, the API is currently being executed...
The API call has been completed.
那魔搭社区上面有哪些模型
What models are on ModelScope?
魔搭社区是一个开源的模型开源社区,旨在通过开放的社区合作,构建深度学习相关的模型开源,并开源相
关模型服务创新技术,推动模型应用生态的繁荣发展。在魔搭社区上,你可以找到许多领域的模型,例如自
然语言处理、计算机视觉、语音等。
ModelScope is an open-source model development community that aims to build open-source deep
learning models through open community cooperation. It also promotes the prosperous development of 有没有语音合成的模型呢
the model application ecosystem by open-sourcing innovative technologies related to model services.
Selecting the "modelscope_search" API, the API is currently being executed...
The API call has been completed.
Any speech synthesis models available?
当然有了,我们推荐使用 SAMBERT 模型,它能够在生成具有连贯性和韵律的语音方面发挥出色。
Certainly, we recommend using the SAMBERT model, which excels in generating coherent and rhythmic
speech.
give me the link to the speech synthesis model
►Selecting the "modelscope_search" API, the API is currently being executed...
The API call has been completed.
魔搭社区有开放域对话模型吗
Does ModelScope have open-domain dialogue?
►Selecting the "modelscope.search" API, the API is currently being executed...
The API call has been completed.
当然有,我们有 ChatPLUG 模型,它可以接收用户对话历史和对话相关的外部知识、用户人设、模型人设
信息,输出合适的文本回复。
Yes, we have the ChatPLUG model, which can take in the user's dialogue history, external
knowledge related to the dialogue, user persona, and model persona information and output
appropriate replies
这个模型怎么使用,有没有代码
How can I use this model? Is there any code available?
►Selecting the "modelscope_search" API, the API is currently being executed..
The API call has been completed.
当然有,我们提供了示例代码,你可以根据你的需求和实际情况来使用这个模型。
Certainly, we provide sample code that you can use according to your needs and actual situation with this model.
示例代码:
Example code
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks
from modelscope.models import Model
model_id = 'damo/ChatPLUG-3.7B
pipeline_ins pipeline (Tasks.fid_dialogue, model-model_id,model_revision='v1.0
preprocess_params = {
'max_encoder_length': 380, encoder KAKE
发个语音合成模型链接
'context_turn': 3 # context
input str = 'Q:我想去北京旅游,有什么好的推荐吗?
result pipeline_ins (input, **kwargs)
print (result)
https://www.modelscope.cn/models/damo/speech_sambert-hifigan_tts_zhishuo_zh-
cn 16k/summary.
clear
image
send regenerate
image
send regenerati
Figure 9: Multi-turn tool-use instructions, text-to-speech and text-to-image cases. Testing the model using the same
semantic instruction in both English(left) and Chinese(right).
In-domain Knowledge QA As shown in Figure
9, the instruction requires the model to retrieve in-
domain knowledge and use the retrieved knowledge
to answer questions.
Instruction or
Clarification
API request
as User
as Agent
API Gallery
EE CAP
Follow-up or
Final Answer
loop would continue until the agent determined
that it was appropriate to terminate the conversa-
tion with the final answer. After acquiring the raw
dataset, we applied filtering mechanisms to elim-
inate instances in which ChatGPT generated API
requests containing hallucinated API names and
parameters that were absent from the retrieved API.
Additionally, we excluded instances in which Chat-
GPT generated illegal API requests, thus resulting
in a refined and finalized dataset.
As introduced in Section 3.1, we collect in-
stances across different languages and topics, the
detailed statistics of our collected data are shown
in Table 4.
Result
Instance Type
Chinese
# Instances
532,436
English
66,444
Common API
211,026
Model API
58,338
5,000
329,776
Figure 10: The data collection procedure of MSAgent-
Bench.
D Data Collection Procedure
We collected our dataset by using prompt engineer
to simulate the agent scenarios with two ChatG-
PTs (gpt-3.5-turbo). One of the ChatGPTs was
prompted to act as the user, while the other was
assigned to act as the agent. In order to expand
the domains and functionalities of APIs presented
in the training data, rather than the exsisting real
APIs, we also included a number of synthetic APIs
that were generated by ChatGPT. When these syn-
thetic APIs were incorporated into the dialogues,
we prompted another ChatGPT to serve as the API
and return the relevant calling outcomes.
The data collection procedure is shown in Fig-
ure 10. Initially, a set of random in-context demon-
strations were provided to ChatGPT for generating
an instruction. This instruction could either be a
regular one or one that requires solving with APIs,
depending on the demonstrations provided. Subse-
quently, ChatGPT was prompt to act as an agent by
first thinking about which action to undertake. If
no API calls were deemed necessary, or if the user
clarification is needed, the agent would respond
with a follow-up response to the user. Otherwise
the agent will send API request to the API gallery.
After receiving the result of the API call, the agent
would assess the situation and decide on the next ac-
tion. This iterative process of the "user-agent-API"
E
API-Oriented QA
API-Agnostic Instruction
Table 4: The statistics of our collected dataset.
Related Work
E.1 Large Language Models
Recent years have witnessed rapid development in
the field of Large Language Models (LLMs). Typ-
ical models, such as GPT3 (Brown et al., 2020),
Gopher (Rae et al., 2021), Chinchilla (Hoffmann
et al., 2022), PaLM (Chowdhery et al., 2022) and
LLAMA (Touvron et al., 2023), have shown im-
pressive zero and few-shot generalization abilities
on a wide range of NLP tasks, by scaling up the
model and data size. A remarkable milestone is the
release of ChatGPT (OpenAI, 2022) or GPT4 (Ope-
nAI, 2023), which has greatly revolutionized the
paradigm of AI development. As a result, a rising
trend of open-source LLMs has emerged to chal-
lenge and catch up their closed-source counterparts
like ChatGPT and Claude, such as BLOOM (Muen-
nighoff et al., 2022), LLAMA (Touvron et al.,
2023), Falcon (Almazrouei et al., 2023), Chat-
GLM (THUDM, 2023). Despite the great break-
through, LLMs are trained as text generators over
plain text corpora, thus performing less well on
other tasks such as multi-modal tasks. It also falls
short on tasks that require up-to-date information,
which are beyond the pretraining data. Using tools
or external APIs can help overcome the limitations
and harnesses the power of LLMs to facilitate seam-
less connections with downstream applications. In
ModelScope-Agent, we provide the whole cus-
tomizable framework and best practices for build-
ing an agent system, which enables open-source
LLMs to use tools and external APIs.
E.2 Agent & Tool Learning
The utilization of Large Language Models (LLMs)
as a controller to construct an agent system has
emerged as a prominent research area. Several re-
lated works employ prompt engineering techniques
on closed-source LLMs, such as ChatGPT (Ope-
nAI, 2022) and Claude, to enable their applica-
tion in specific domains. For instance, Visual-
ChatGPT (Wu et al., 2023) and HuggingGPT (Shen
et al., 2023) facilitate the HuggingFace model call-
ings accessible to OpenAI LLMs. SayCan (Ahn
et al., 2022) and inner monologue (Huang et al.,
2023) integrate LLMs with robots to achieve
robotic systems. Notably, recent works such
as Langchain and Auto-GPT encompass a wide
range of tools, including common APIs and neu-
ral models, and enhance long-term reasoning
and human-agent interaction whilst solving tasks,
which demonstrate the immense potential for build-
ing a generalized agent.
Numerous endeavors have also been made
to enable open-source LLMs to utilize tools.
For instance, Gorilla (Patil et al., 2023) and
GPT4Tools (Yang et al., 2023) generate training
data using self-instruction techniques to train open-
source LLMs to effectively utilize neural mod-
els. ToolAlpaca (Tang et al., 2023) and ToolL-
LaMA (Qin et al., 2023) train LLAMA using com-
mon APIs, with the distinction that ToolAlpaca
employs synthetic APIs from LLMS, whereas Tool-
LLAMA utilizes real APIs.
Overall, compared to the above-mentioned meth-
ods, ModelScope-Agent differs in the following
aspects. Firstly, our method includes a universal
training framework that supports user-customized
agent learning for open-source models to meet in-
dustrial needs. Secondly, ModelScope-Agent can
support various APIs in different fields, including
model APIs and common APIs, while previous
works only support certain specific APIs.
