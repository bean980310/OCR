arXiv:2309.05767v2 [cs.SD] 6 FebNATURAL LANGUAGE SUPERVISION FOR GENERAL-PURPOSE AUDIO REPRESENTATIONS Benjamin Elizalde*, Soham Deshmukh*, Huaming Wang Microsoft {benjaminm, sdeshmukh, huawang}@microsoft.com ABSTRACT Audio-Language models jointly learn multimodal text and audio representations that enable Zero-Shot inference. Models rely on the encoders to create powerful representations of the input and generalize to multiple tasks ranging from sounds, music, and speech. Although models have achieved remarkable performance, there is still a gap with task-specific models. In this paper, we propose a Contrastive Language-Audio Pretraining model that is pretrained with a diverse collection of 4.6M audio-text pairs employing two innovative encoders for Zero-Shot inference. To learn audio representations, we trained an audio encoder on 22 audio tasks, instead of the standard training of sound event classification. To learn language representations, we trained an autoregressive decoder-only model instead of the standard encoder-only models. Then, the audio and language representations are brought into a joint multimodal space using Contrastive Learning. We used our encoders to improve the downstream performance by a large margin. We extensively evaluated the generalization of our representations on 26 downstream tasks, the largest in the literature. Our model achieves state of the art results in several tasks outperforming 4 different models and leading the way towards general-purpose audio representations. Code is on GitHub¹. Index Terms contrastive learning, general purpose audio representation, zero-shot, language, sounds 1. INTRODUCTION Recent research in the audio domain focuses on learning representations that generalize to a wide range of downstream tasks across different domains. The 2021 Holistic Evaluation of Audio Representations (HEAR) [1] took a major step in this direction by providing a comprehensive setup to benchmark audio representations. The models were pretrained on a large dataset -AudioSet [2] (1.7M files)- using Supervised, Self-Supervised or Unsupervised Learning. All the methods have to undergo additional fine-tuning to use their representations on a given downstream task. Zero-Shot models can be applied to any task directly achieving flexibility and generalization. One of the most successful type are Contrastive Language-Audio Pretraining (CLAP) models that jointly learn multimodal text and audio *Equal Contribution https://github.com/microsoft/CLAP representations. Authors in [3] introduced a CLAP model that achieved state of the art (SOTA) in 16 downstream tasks. Subsequent literature showed that the choice of audio and text encoders are critical to generate powerful representations and increase performance across tasks [4, 5, 6, 7]. For example, upgrading from CNN to audio transformers (HTSAT) to encode audio and from BERT to ROBERTa to encode text. Another conclusion is that scaling up the number of training pairs improves overall performance. However, simply adding pairs may result in a drop of performance in certain domains and tasks [4, 5, 3, 6]. CLAP's performance is dependent on the diversity of the text and audio training pairs and how noisy they are. Wav2clip [8] and Audioclip [9] used 200k and 1.7M audio-text pairs respectively from AudioSet, a dataset annotated for sound events. Authors paired audio with class labels rather than with sentence-level descriptions, potentially missing the context and language semantics of descriptions, but with good Zero-Shot performance in 3 andtasks respectively. CLAP [3] used 128k pairs but the text were descriptions coming from audio captioning and a websourced dataset. It was evaluated on 16 tasks and significantly improved over its predecessors. LAION CLAP [4] used a collection of 2.5M pairs, further improving performance intasks. Authors later added music and speech-related training pairs, but performance in sound event classification (ESC50) degraded by an absolute 1%. Wavcaps[6] used 500k pairs, but cleaned up the noisy web-sourced descriptions with a ChatGPT language model. Results outperformed the literature in 8 tasks. Therefore, when scaling up pairs it is essential to verify performance trade offs by evaluating generalization across different domains and tasks. In this paper we make the following contributions. To learn audio representations, we trained an audio encoder on 22 audio tasks. To learn language representations, we trained an autoregressive decoder-only model. We pretrained our CLAP model with an unprecedented 4.6 million audio-text pairs and extensively evaluated the generalization of our representations on 26 downstream tasks, the largest in the literature, achieving SoTA results in several. 2. METHOD Contrastive Language-Audio Pretraining (Fig 1) jointly trains an audio an a text encoder to learn multimodal representations which can be used for different types of inference. Contrastive Pretraining Paddling in the water Text Encoder Text audio pairs T₁ Classes Dog barking Zero-Shot Classification Rain falling Tw Text Encoder A₁ A₁T, A₁₂ A₁T- AT Siren wailing … Audio Encoder Az A A A Ax Art Art Axt Text to Audio Retrieval Text Query A crow crying Text int the forest Encoder Audio Database AN ANTI ANT ANTS ANTN Testing audio Audio Encoder AAT, AT A₁T Audio Captioning Dog barking A campfire crackles as the flames burn branches A₁ AT >>>> Audio Encoder Testing audio Az A₂T → Audio Encoder Fig. 1: CLAP A As AN ANT Mapper GPT-Maps an audio embeddings to a GPT2 input sequence learns audio and a text embeddings that can be compared in a multimodal space. The pretrained encoders can be used for Zero-Shot Classification, Text to Audio and Audio to Text Retrieval, and Audio Captioning. 2.1. Contrastive Language-Audio Pretraining Let the processed audio be Xa s.t. Xa Є RF×T where F are the number of spectral components (e.g. Mel bins) and T are the number of time bins. Let the text be represented by Xt. Each audio-text pair in a batch of N is represented as {Xa, Xti where i Є [0, N]. For convenience, we drop the i notation, and henceforth {Xa, Xt} will denote a batch of N. From the pairs, the audio and text are passed to an audio encoder fa(.) and a text encoder ft (.) respectively. For a batch of N: Ŷa = fa(Xa); Ŷt = ft(Xt) (1) where Ŷ a Є R³×V are the audio representations of dimensionality V, and Âµ Є R³×ʊ are the text representations of dimensionality U. We brought audio and text representations, Ŷa and Ŷt, into a joint multimodal space of dimension d by using a learnable projection layer: Ea = La(Xa); Et = Lt(Xt) (2) where Ea Є Rxd, Et Є R³×d, La and Lɩ are the projections for audio and text respectively. Now that the audio and text embeddings (Ea, Et) are comparable, we can measure similarity: C = T (Et · ET) . NXN (3) where 7 is a temperature parameter to scale the range of logits. The similarity matrix CERN has N matching pairs in the diagonal and N² – N non-matching pairs in the offdiagonal. L = 0.5(ltext (C) + laudio (C)) = A (4) where lk log diag(softmax(C)) along text and audio axis respectively. We used this symmetric crossentropy loss (L) over the similarity matrix to jointly train the audio and text encoders along with their projection layers. 2.2. Audio and Text Encoders Audio Encoder: To process audio, we trained a transformerbased audio encoder (HTSAT [10]) on 22 audio tasks using a similar method to this paper [11]. We called it HTSAT-22. We hypothesized that an encoder trained on multiple audio tasks would improve generalization and thus performance across tasks. The method learns an audio encoder and a mapper network to prompt a large language model to perform multiple audio tasks, such as classification, captioning, retrieval and audio Q&A. The architecture is trained essentially as a captioning system, where it learns to generate a free-form text output c² in an autoregressive fashion conditioned on the audio prompt p². Note that y denotes the model's trainable parameters. The loss function is Cross-Entropy: NL= log p(cpi, P2, C, 1) (5) i=1 j=2k' Text Encoder: To process text, we adapted GPT2 (base 124M), which is an autoregressive model that has exhibited impressive abilities for text tasks. We addressed the challenge - How to make an autoregressive model produce a sentence-level representation? Autoregressive models built with transformer-decoder blocks, take an input text and output the most likely sequence of words (tokens), one after the other. In contrast, models built with transformer-encoder blocks (BERT or ROBERTA) output a sentence-level representation in a continuous space. To make GPT2 output a sentence-level representation, we appended the special token|endoftext| > at the end of each input text. During contrastive pretraining, we use the representations from this token as sentence-level representations. This forces the token to contain the aggregate information from the text input. 2.3. Evaluation Zero-Shot Inference: We used CLAP's ability to determine the similarity between audio and text. Let's consider a target dataset with C class labels and N test audios. First, we compute CLAP's audio and text embeddings for N audios and C classes using the pretrained encoders. Second, we compute the cosine similarity between each testing audio and all the class labels. In the case of retrieval, we treat text queries as classes. Each test audio will have as many logits as classes. Third, logits are turned into a probability distribution by applying softmax for binary or multiclass classification; sigmoid for multilabel classification; and left unaltered for retrieval. Audio Captioning: In the architecture of Fig 1, a test audio is passed to the pretrained audio encoder, then to a mapper network, and then to GPT2 to generate a description. At training time, only the weights of the mapper network are learned with a captioning loss (Eq.5) and the training split. Zero-Shot Score ↑ Sound Event Classification ↑ Vocal Sound Classification ↑ Surveillance Sound Classif.↑ Action Classification↑ Acoustic Scene Classification↑ DCASEVocal ESCModel Average ESCFSD50K US8K SESA TUTTaskSound Actions CNN14+BERT 0.0.0.0.0.0.0.0.0.HTSAT+CLIP 0.0.0.0.0.0.0.0.0.HTSAT+ROBERTa 0.0.0.0.0.0.0.0.0.HTSAT+GPT0.0.0.0.0.0.0.0.0.HTSAT-22+ROBERTa 0.0.0.0.0.0.0.0.0.HTSAT-22+CLIP 0.0.0.0.0.0.0.0.0.HTSAT-22+GPT0.0.0.0.0.0.0.0.0.Music Classification ↑ Instrument Classification ↑ Speech Emotion KWS↑ Speaker Classification↑ Counting↑ GTZAN GTZAN Model Beijing NS Instr. Music Speech Genres Opera family CRE RAV MA-D DESS Speech Commands LibriCountCNN14+BERT HTSAT+CLIP0.0.0.0.178 0.0.0.0.0.0.0.0.0.0.0.HTSAT+ROBERTa 0.0.0.0.0.0.0.0.HTSAT+GPT0.0.0.0.0.0.0.HTSAT-22+ROBERTa0.0.0.0.0.0.0.HTSAT-22+CLIP0.0.0.0.0.0.0.HTSAT-22+GPT0.0.0.0.297 0.0.0.Table 1: Zero-Shot performance on 16 downstream tasks and 119k training pairs. Our proposed encoders (HTSAT-22+GPT2) outperformed the best combinations in the literature. Higher is better for all numbers. The metrics are mAP for FSD50k and ESC50-actions; F1-score for DCASE17; all others use Accuracy. Zero-Shot score is the average of the metrics. This is the first comparison of encoders in literature with 16 tasks, usually only a couple of enocders and a handful of tasks are considered. 3. EXPERIMENTS Training Datasets. Collecting pairs is perhaps the main bottleneck of scaling up CLAP models. We gathered the largest collection with 4.6 million audio and text pairs from different datasets and web archives. The audios describe human sounds and activities, environmental sounds, acoustic scenes, music, sound effects, and speech emotion. To study the effect of encoders in Table 1, we used the same training sets as CLAP [3]. Unlike the authors, we did not include the test set of AudioCaps and Clotho, so the number of pairs was 119k instead of 128k. The training datasets for the 4.6M collection are: WavCaps [6], AudioSet [2], FSD50K [12], Clotho [13], AudioCaps [14], MACS [15], WavText5k [5], SoundDesc [16], NSynth [17], FMA [18], Mosi [19], Meld [20], Iemocap [21], Mosei [22], MSP-Podcast [23], CochlScene [24], LJspeech [25], EpicKitchen [26], Kinectics700 [27], findsounds.com. Details on GitHub. Downstream Tasks. We used 26 downstream tasks from different domains, several come from HEAR[1]: sound events, vocal sounds, surveillance sounds, and acoustic scenes classification; audio captioning; retrieval; music, instruments, and note attributes classification; speech emotions and language classification; keyword spotting; and speaker counting. To study the effect of encoders in Table 1, we used a subset of 16 tasks. Pre-processing. We used log Mel spectrogram representations of audio with a sampling rate of 44.1 KHz, hop size of 320 frames, window size 1024 frames, and 64 Mel bins in the range of 50-8000 Hz. During training, each audio clip is randomly truncated to a continuous segment of 7 secs, or padded if shorter. The batches with pairs are randomly sampled. Encoders. For our proposed CLAP model, we used the audio and text encoders HTSAT-22+GPT2 described in Sec.2.2. For comparison, in Table 1 we used the two best combinations of encoders in the literature CNN14+BERT and HTSAT+ROBERTa [3, 4, 6]. We also included the text encoder from CLIP because it was used by different authors [9, 8, 4]. Both, the audio and text embeddings are projected into a multimodal space with independent learnable projection layers with an output dimension of 1024. Training. We trained by unfreezing both encoders forepochs, although the overall performance peaked in the first 10 epochs. We report the performance of the downstream tasks corresponding to the epoch that yielded the best ZeroShot score (average of all tasks). We hypothesize that the model corresponding to such epoch will generalize better to unseen datasets and serve the community better. It is possible that the performance of each task was higher or lower in a different epoch. Batch size was 1,536. We used Adam Optimiser with an initial learning rate 10-3 and reduce the learning rate on plateau by 10-1 with a patience of 15. The temperature parameter 7 is learnable and initialised to 0.007. 4. RESULTS AND DISCUSSION The results comparing different audio and text encoders are in Table 1 and the results of our proposed CLAP are in Table 2. 4.1. Proposed audio and text encoder Our proposed encoders HTSAT-22+GPT2 outperformed two of the best combination of encoders in the literature, as shown in Table 1. To compare overall performance, we used Zero-Shot score, which is the average of the metrics from all 16 tasks. HTSAT-22+GPT2 achieved 0.480, an absolute 9% higher than the most common combinations HTSAT+ROBERTa and CNN14+BERT with 0.431 and 0.respectively. All encoder combinations performed better Model Benchmark HTSAT-22+GPTESC[28] 0.948 [6] 0.FSD50K [12] 0.302 [3] Sound Event Classification ↑ US8K 0.[29] 0.806 [6] 0.DCASETask 4 [30] 0.3 [3] 0.AudioSet [2] 0.058 [3] 0.Vocal Sound Classification ↑ Vocal Surveillance Sound Classif.↑ Action Classification↑ Acoustic Scene Classification↑ SESA Sound [31] [32] ESCActions [33] 0.495 [3] 0.0.0.TUT[30] 0.296 [3] 0.0.0.Speech Emotion Music Classification ↑ Instrument Classification ↑ KWS↑ Classification↑ GTZAN GTZAN Model Music Speech [1] Genres NS Pitch NS Velocity NS Qualities Beijing NS Instr. CRE RAV Opera family MA-D DESS [1] [17] [17] [17] [1] [17] [1] [34] Speech Commands [1] Speaker Counting↑ Libri Count[1] Benchmark 1 [3] 0.25 [3] 0.0.HTSAT-22+GPT0.0.0.0.0.0.0.4746 [3] 0.0.0.0.178 [3] 0.0.159 [3] 0.106 [3] 0.178 [3] 0.0.0.Audio Captioning ↑ Model Benchmark HTSAT-22+GPTAudioCaps Clotho [14] [13] 0.438[35] 0.215[35] 0.0.AudioCaps R@0.517[6] 0.Audio-Text Retrieval ↑ AudioCaps Clotho MAP@10 R@0.457 [4] 0.234[6] 0.0.Text-Audio Retrieval ↑ Clotho MAP@0.138[4] 0.AudioCaps R@0.397[6] 0.AudioCaps Clotho Clotho MAP@10 R@1 MAP@0.51[4] 0.195[6] 0.204[4] 0.0.0.Table 2: Performance on 26 downstream tasks using our proposed encoders and 4.6M training pairs. As the benchmark, we used the best numbers in the literature, when no number was available we used random performance. Higher is better for all tasks. The evaluation metrics are MAP for FSD50k, ESC50-Actions, AudioSet, and NS Qualities; F1-score for DCASE17; and SPIDER for Captioning; all others use Accuracy. than random. Although different combinations did better at different tasks, none of them excelled at a specific domain. Our HTSAT-22 audio encoder is the major contributor to performance improvement. HTSAT-22 is pretrained onaudio tasks in contrast to HTSAT which is pretrained only on sound event classification. Hence, suggesting that generating pretraining on multiple audio tasks can improve the representations from the audio encoder. Comparing HTSAT22+GPT2 to HTSAT+GPT2 evidenced major improvements such as LibriCount10 (absolute 10%), NS Instrument (absolute 7%) and ESC50 (absolute 6%). The proposed GPT2 autoregressive model improves upon the popular ROBERTa. Using GPT2 with either HTSAT or HTSAT-22 yielded the best performance over the other text encoders. We hypothesize that the improvement comes from two reasons. First, GPT2 has a larger vocabulary of 50k tokens compared to BERT and ROBERTa with 30k. Second, our modified GPT2 autoregressive predicts tokens till < |endoftext > used for sentence-level representation. This acts as self-supervision and forces the model to learn and put emphasis on the ordering of words. 4.2. Scaling proposed CLAP architecture Our CLAP model established new Zero-Shot SoTA on most of the 26 downstream tasks as shown in Table 2, outperforming 4 different SoTA models. To benchmark our model, we used the best numbers in the literature coming from different models. When no number was available, we used random performance. In some cases, performance improvement is more than double the benchmark literature. Some highlights are Music Genres with 58.4% acc. vs 25%, Vocal Sounds with 80% acc. vs 49.5%, Acoustic Scenes with 53.8% acc. vs 29.6%. Some downstream tasks do not constitute a true ZeroShot setup as the audio files in the training set were part of the 4.6M pairs (see Sec.3). For instance, FSD50k audio and web descriptions were used in training but not the class labels. We did not fine-tune CLAP encoders for any downstream task. We only fine-tune the audio encoder for ESC50 and were able to improve performance from our previous CLAP model from 96.70% to 98.25% accuracy, thus establishing a new SoTA. 4.3. Generalization and individual domain performance Adding diversity and scaling the audio-text pairs in training presents a trade-off that increases performance in some tasks but decreases it in others. As expected, adding training pairs that resemble the domain from a given task helps, hence diversity is essential for generalization. For example, CLAP [3] did not include emotion recognition training pairs and achieved 17.1% acc. in RAVDESS and 23.4% in CREMAD. We added emotion-related pairs and improved accuracy to 31.5% and 30% respectively. Nonetheless, more pairs can cause a distribution shift, creating a mismatch between training and some testing data. For example, our model achieved a slightly lower score than a model [6] trained with 500k pairs on ESC(94.8% vs 93.9% acc.). Another example is with GTZAN Music vs Speech, where a model [3] with 128k pairs achieved 100% acc. over ours with 99.2%. Even our model in Tableachieved 100% acc with 119k pairs. We should expect that as we add training pairs, performance across tasks will vary. Hence, zero-shot models should be evaluated across different domains and tasks with focus on generalization rather than on overfitting to specific tasks. Audio-Text (A-T) and Text-Audio (T-A) Retrieval performance fell short of the benchmark. We measured the tasks with mAP@10, which is the ranking metric of IEEE DCASE, and R@1. Our model outperformed the literature in terms of mAP@10 for Clotho (A-T: 0.155 vs 0.138 and T-A: 0.257 vs 0.204), and struggled only with A-T AudioCaps (A-T: 0.vs 0.457 and T-A: 0.51 vs 0.51). Both datasets are sensitive to out-of-domain training data and adding training pairs did not translate into an improvement. This was demonstrated by authors in [5] who unsuccessfully tried to add 39k files from SounDesc or authors in [4] with 500k from Wavcaps or authors in [6] with 1.7M from AudioSet. 5. CONCLUSION We introduced a CLAP model with our proposed encoders and 4.6M training pairs. Zero-shot models should be evaluated across different tasks with a focus on generalization rather than on overfitting to specific tasks. We evaluated CLAP on 26 tasks and established SOTA on most of them, leading the way in general-purpose audio representations. 6. REFERENCES [1] Joseph Turian, Jordie Shier, et al., “HEAR: Holistic Evaluation of Audio Representations," in NeurIPSCompetitions and Demonstrations Track, 2022. [2] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, et al., “Audio set: An ontology and human-labeled dataset for audio events," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017. [3] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang, “Clap learning audio concepts from natural language supervision,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. [4] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov, "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation," arXiv preprint arXiv:2211.06687, 2022. [5] Soham Deshmukh, Benjamin Elizalde, and Huaming Wang, "Audio Retrieval with WavText5K and CLAP Training," in Proc. INTERSPEECH 2023, 2023. [6] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark Plumbley, et al., "Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research," arXiv preprint arXiv:2303.17395, 2023. [7] Benjamin Elizalde, Shuayb Zarar, and Bhiksha Raj, "Cross modal audio search and retrieval with joint embeddings based on text and audio," in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 4095-4099. [8] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, et al., "Wav2clip: Learning robust audio representations from clip," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. [9] Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas Dengel, "Audioclip: Extending clip to image, text and audio," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. [10] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, et al., “Hts-at: A hierarchical tokensemantic audio transformer for sound classification and detection," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. [11] Soham Deshmukh, Benjamin Elizalde, Rita Singh, and Huaming Wang, “Pengi: An audio language model for audio tasks," arXiv preprint arXiv:2305.11834, 2023. [12] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra, "Fsd50k: An open dataset of human-labeled sound events," IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2022. [13] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen, "Clotho: an audio captioning dataset,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. [14] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim, "AudioCaps: Generating Captions for Audios in The Wild," in NAACL-HLT, 2019. [15] Irene Martín-Morató and Annamaria Mesaros, "What is the ground truth? reliability of multi-annotator data for audio tagging," in 29th European Signal Processing Conference (EUSIPCO), 2021. [16] A. Sophia Koepke, Andreea-Maria Oncescu, Joao Henriques, Zeynep Akata, and Samuel Albanie, "Audio retrieval with natural language queries: A benchmark study,” IEEE Transactions on Multimedia, 2022. [17] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, et al., "Neural audio synthesis of musical notes with wavenet autoencoders," in International Conference on Machine Learning. PMLR, 2017. [18] Michaël Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson, “Fma: A dataset for music analysis," in 18th International Society for Music Information Retrieval Conference, 2017. [19] Amir Zadeh, Rowan Zellers, Eli Pincus, and LouisPhilippe Morency, “Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos," arXiv preprint arXiv:1606.06259, 2016. [20] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, et al., “Meld: A multimodal multi-party dataset for emotion recognition in conversations," in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [21] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, et al., "Iemocap: Interactive emotional dyadic motion capture database," Language resources and evaluation, 2008. [22] Amir Ali Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency, "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph," in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018. [23] Reza Lotfian and Carlos Busso, “Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings," IEEE Transactions on Affective Computing, 2017. [24] Il-Young Jeong and Jeongsoo Park, "Cochlscene: Acquisition of acoustic scene data using crowdsourcing,” in Asia-Pacific Signal and Information Processing Association Annual Summit and Conference, 2022. [25] Keith Ito and Linda Johnson, "The lj speech dataset," https://keithito.com/ LJ-Speech-Dataset/, 2017. [26] Dima Damen, Hazel Doughty, Giovanni Maria Farinella,, Antonino Furnari, Jian Ma, Kazakos, et al., "Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100," International Journal of Computer Vision (IJCV), 2022. [27] Lucas Smaira, João Carreira, Eric Noland, Ellen Clancy, Amy Wu, and Andrew Zisserman, "A short note on the kinetics-700-2020 human action dataset," 2020. [28] Karol J. Piczak, "ESC: Dataset for Environmental Sound Classification," in Proceedings of the 23rd Annual ACM Conference on Multimedia. pp. 1015–1018, ACM Press. [29] Justin Salamon, Christopher Jacoby, et al., "A dataset and taxonomy for urban sound research,” in 22nd ACM international conference on Multimedia, 2014. [30] Annamaria Mesaros, Aleksandr Diment, Benjamin Elizalde, Toni Heittola, et al., “Sound event detection in the dcase 2017 challenge,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2019. [31] Yuan Gong, Jin Yu, and James Glass, “Vocalsound: A dataset for improving human vocal sounds recognition," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. [32] Tito Spadini, “Sound events for surveillance applications," Oct. 2019. [33] Benjamin Elizalde, Radu Revutchi, Samarjit Das, Bhiksha Raj, Ian Lane, and Laurie M Heller, “Identifying actions for sound event classification,” in 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2021, pp. 26–30. [34] Steven R. Livingstone and Frank A. Russo, "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)," Apr. 2018. [35] Minkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh, “Prefix tuning for automated audio captioning,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023.