arXiv:2306.05399v2 [cs.CV] 16 NovMatting Anything Jiachen Li¹, Jitesh Jain¹, Humphrey Shi¹,¹SHI Labs @ Georgia Tech & Oregon & UIUC, 2Picsart AI Research (PAIR) https://github.com/SHI-Labs/Matting-Anything Semantic Matting Instance Matting Matting Anything Semantic Matting Model Instance Matting Model Matting Anything Model prompt (box) prompt (point) a dog in the middle side prompt (text) Image (A) Image (B) Image (C) Figure 1. Matting Anything Model (MAM) offers a versatile framework capable of addressing various types of image matting scenarios with a single model. Compared to previous specialized models for (A) Semantic Matting, which outputs a single alpha matte of all instances in the foreground; (B) Instance Matting, which returns alpha mattes of all human instances; (C) Matting Anything Model can estimate the alpha matte of any target instance with user prompts as boxes, points, or text descriptions for interactive use by incorporating SAM [21]. It further reaches comparable performance to the specialized matting models on multiple benchmarks, and shows superior generalization ability with fewer parameters as a unified image matting model. Abstract In this paper, we propose the Matting Anything Model (MAM), an efficient and versatile framework for estimating the alpha matte of any instance in an image with flexible and interactive visual or linguistic user prompt guidance. MAM offers several significant advantages over previous specialized image matting networks: (i) MAM is capable of dealing with various types of image matting, including semantic, instance, and referring image matting with only a single model; (ii) MAM leverages the feature maps from the Segment Anything Model (SAM) [21] and adopts a lightweight Mask-to-Matte (M2M) module to predict the alpha matte through iterative refinement, which has only 2.7 million trainable parameters. (iii) By incorporating SAM, MAM simplifies the user intervention required for the interactive use of image matting from the trimap to the box, point, or text prompt. We evaluate the performance of MAM on various image matting benchmarks, and the experimental results demonstrate that MAM achieves comparable performance to the state-of-the-art specialized image matting models under different metrics on each benchmark. Overall, MAM shows superior generalization ability and can effectively handle various image matting tasks with fewer parameters, making it a practical solution for unified image matting. Our code and models are open-sourced at https://github.com/SHI-Labs/Matting-Anything. 1. Introduction Image Matting, as a long-standing computer vision task, aims to estimate the alpha matte a given an input image I [45]. The matting target is mainly around human beings or other objects at the semantic level [26,41,49]. Recent workshave extended the scope of image matting to more complex scenarios like image instance matting [42], which requires instance-aware alpha matte predictions and referring image matting [28], which extracts the alpha matte given natural language description. Previous deep learning-based image matting methods [28, 29, 36, 37, 42, 47, 51, 54, 59] have been proposed to address specific image matting tasks on corresponding benchmarks. These methods are tailored to individual datasets and lack the flexibility to handle various image matting tasks due to their fixed model designs. This limitation has hindered the development of more generalized and versatile image matting models. As a result, there is a growing interest in developing more adaptive and efficient image matting frameworks that can handle different types of image matting tasks with a single model. Furthermore, previous image matting methods have relied on user-guided trimaps as auxiliary inputs to achieve accurate alpha matte predictions. Although some trimapfree methods have been proposed that use mask guidance or background images instead [39,55], they are unable to estimate the alpha matte of the target instance based on the user request for interactive use. Therefore, it is crucial to develop a model that can achieve accurate alpha matte estimation without relying on user-guided trimaps, while also being capable of handling simple user requests in a flexible and efficient manner for interactive use. Such a model would significantly enhance the user experience by reducing the extra need for manual intervention. Motivated by these limitations of image matting, we propose the Matting Anything Model (MAM), a versatile network that can estimate the alpha matte of any target instance with prompt-based user guidance in an image as shown in Figure 1. MAM leverages the recent Segment Anything Model (SAM) framework [21], which supports flexible prompting and outputs segmentation masks of any target instance for interactive use. Specifically, MAM takes the feature maps and mask outputs from SAM as inputs and adds a lightweight Mask-to-Matte (M2M) module to predict the alpha matte of the target instance. We trained MAM on a combination of five image matting datasets that cover different classes of instances, allowing the M2M module to learn generalizable features for image matting. During training, we randomly place target instances onto background images and use a pre-trained SAM to output mask predictions of the corresponding instances. The trainable M2M module then refines the mask by predicting multi-scale alpha mattes. Through an iterative refinement process based on the mask or the alpha matte, the multi-scale predictions are merged to obtain the final meticulous alpha matte. We conducted extensive evaluations of MAM on six image matting benchmarks, including semantic image matting benchmark PPM-100 [40], AM2K [25] PM-10K [25], the instance image matting benchmark RWP636 [55], HIM2K [42], and the referring image matting benchmark RefMatte-RW 100 [28]. Our results demonstrate that MAM achieves performance comparable to that of state-of-the-art image matting models across all benchmarks under different evaluation metrics. The experimental results highlight the versatility and effectiveness of our proposed approach for handling various image matting tasks in an interactive and efficient manner. 2. Related Works 2.1. Image Matting Given an image I, which can be view as a combination of foreground image F and background image B with coefficient alpha matte a, IaF + (1 - a)B (1) Image Matting is to estimate a given only I as inputs. Traditional methods rely on a user-guided trimap, which explicitly annotates the absolute foreground area, absolute background area, and transition area. Then, sampling-based image matting solutions use low-level features to distinguish the transition areas by measuring the similarities between foreground and background neighbors [1,2,5,8,9,11]. Recently, deep learning-based methods [27, 36, 47, 51, 54, 59] adopt neural networks to estimate the alpha matte in an end-to-end manner with trimap as auxiliary inputs. Some trimap-free methods use background image [39], mask guidance [32,55], or segmentation data [4, 30] to make up the absence of trimap. When the image I contains multiple instances, the composition turns to N N I = ΣaiFi + (1 - Σai) B i a¿ represents the alpha matte of instance i and InstMatt [42] adopt the target and reference mask as guidance to prediction instance-aware alpha matte prediction. Interactive matting methods [28, 48, 52] develop specialized models that use point, boxes, or text input to estimate the alpha matte of the target instance. MatAny [53] is a concurrent work that also adopts SAM for semantic image matting. In terms of video matting, trimap-free [22–24, 30, 40] methods are explored for real-time inference while the per-frame prediction quality is not comparable to image matting methods. However, these methods are designed for a certain scenario with corresponding benchmarks, which limits their potential to handle various image matting tasks and benchmarks simultaneously. 2.2. Image Segmentation Image segmentation is a close research area to image matting, while it predicts the binary mask of different in-Segment Anything Model (SAM) Image Frozen Weights Feature Maps RB Refinement Block Iterative Refinement Prompt (box/point) a girl with black/red sweater Prompt (text) mask Mask-to-Matte (M2M) Module RB RB RB a osl αaosFigure 2. Matting Anything Model Architecture. The MAM architecture consists of a pre-trained SAM and an M2M module. Given an input image I, SAM generates the mask prediction for the target instance based on the box or point user prompt. The M2M module takes the concatenated inputs, including the image, mask, and feature maps, and produces multi-scale predictions aos8, dos4, and Cosl. The iterative refinement process, detailed in Section 3, progressively improves the precision of the final meticulous alpha matte a, incorporating information from the multi-scale outputs. stances in the image. Similar to image matting, many image segmentation methods are tailored for a specific image segmentation task, like semantic segmentation [3, 15], instance segmentation [13, 46], and panoptic segmentation [20, 44]. Recent works started to explore transformerbased frameworks [6, 12, 16, 17] for unified image segmentation. Language-guided segmentation frameworks [50,58] look for text supervision to segment instance-aware masks. OneFormer [16] adopts a single transformer model to learn with a joint training strategy and performs universal segmentation across semantic, instance and panoptic segmentation and outperforms specialized models. SAM [21] takes a further step recently, which supports flexible prompting from users to segment any instance in an image for interactive use. Grounded-SAM [33] incorporates DINO with SAM to add text prompt support. Foundation models like SAM offer opportunities for other areas to develop versatile frameworks to support a range of applications. 3. Matting Anything In this section, we provide an overview of the Matting Anything Model (MAM) architecture, which consists of two main components: the frozen Segment Anything Model (SAM) and the trainable Mask-to-Matte (M2M) module. We first provide a brief review of the SAM, which is designed to produce high-quality instance segmentation given user-guided prompts. We then introduce the M2M module, which enables the transformation of the binary masks into high-quality alpha mattes. Finally, we describe how we connect the M2M module with the SAM to gradually build the end-to-end MAM. 3.1. Segment Anything Model Segment Anything is a recently proposed foundation model for segmentation. Given an image I Є R³×H×W¸ SAM uses a ViT-based image encoder to obtain deep feature maps FЄ RC×6×6. Then, a variety of N input prompts are encoded by the prompt encoder and sent to the mask decoder with the feature maps. The mask decoder returns a set of mask candidates m¿ Є R¹×H×W, i Є N indicated by the input prompts. With its flexible prompting mechanism, SAM allows for interactive use and is easily adaptable for downstream tasks. 3.2. Mask-to-Matte The Mask-to-Matte (M2M) module is an integral component of our Matting Anything Model (MAM) and is designed to convert instance-aware mask predictions from SAM into instance-aware alpha matte predictions efficiently and smoothly. To achieve this, we utilize the feature maps and mask predictions generated by SAM as auxiliary inputs to M2M. To improve the accuracy of our predictions, we adopt multi-scale branches for predicting the alpha matte and merge these predictions through an iterative refinement schedule. Multi-Scale Prediction: Given an input image I Є R³×HxW the pre-trained SAM model produces feature maps F € RC×× and mask prediction m = R¹×HxW on the target instance with prompt guidance. We concatenate the rescaled image, mask, and feature maps to formTask Benchmark Metric Specialized Models AM2K Semantic Matting PM-10K PPM-SADall MADall↓ MSE all↓ Instance Matting HIM2K IMQmat IMQ RWPIMQmad IMQmse↑ Referring Matting RefMatte-RWSADall↓ MSE all↓ GFM-R [25] 10.6.GFM-D [25] 10.6.MODNet [19] 4.MGMatting [55] 57.71.30.53.InstMatt [42] 70.81.51.73.CLIPMat-B [28] 107.59.CLIPMat-L [28] 85.47.Generalized Models SAM [21] 25.MAM 17.25.15.10.4.61.74.49.56.33.17.68.81.54.76.29.15.Table 1. Comparisons between specialized matting models and MAM on various benchmarks. ↑↓ means higher / lower values indicate better performance for the corresponding metric. Gray text refers to models specifically designed for these benchmarks. MAM shows clear improvements over SAM and superior generalization ability as a unified image matting model. the input Fm2m Є R(C+4)× ×W to the M2M module. M2M employs several refinement blocks [7,55], which contain connected self-attention layer [56], batchnorm layer, and activation layer, to generate alpha matte predictions at 1/8 resolution, denoted as ass E R¹× ×W. The feature maps are then upsampled to higher resolutions to make alpha matte predictions at 1/4 and full resolution, denoted as αos4 € R¹×× and ɑos1 € R¹×H×W, respectively. The multi-scale predictions enable MAM to handle objects of varying scales and provide finer-grained alpha mattes for detailed object extraction. Iterative Refinement To improve the accuracy of global and local predictions, we use an iterative refinement process. We first compute weight maps Woss, Wos4, and Wosl that highlight different areas of the image during training like trimaps. These weight maps are used to compute losses for each scale of prediction, with woss emphasizing the entire image for aoss predictions, wos4 filtering out the background for aos4 predictions, and wos1 focusing only on the transition areas. During inference, we gradually merge the predictions of aos8, Qos4, and Cos1 with the mask predictions m from SAM to obtain the final alpha matte prediction a Є R¹×HxW 3.3. Matting Anything Model After the development of the Mask-to-Matte (M2M) module, we integrate it with the Segment Anything Model (SAM) to enable end-to-end training and inference for the Matting Anything Model (MAM). This integration allows for a comprehensive and unified framework that handles the entire matting process, from feature extraction to alpha matte prediction. Multi-Dataset Training To ensure the robustness and versatility of our Matting Anything Model (MAM), we adopt a multi-dataset training approach that encompasses diverse = foreground instances and background images from various image matting datasets. This selection allows us to cover a wide range of instance classes and background scenarios, enhancing the model's ability to handle different types of instances and backgrounds effectively. During the training process, we create composite images by combining a foreground instance F € R³×H×W with its corresponding ground truth alpha matte agt E R¹×H×W and a background image BЄ R³×H×W The composition is performed using the equation I agt F (1 agt) B. We then extract the bounding box (xo, Yo, x1,y1) that encapsulates the instance of interest within the composite image. Then, we send the image I and the bounding box as a prompt to the pre-trained SAM, which returns the mask prediction of the instance. Then, we concatenate the image, mask and feature maps, and send them to the M2M module, which further returns the multi-scale alpha matte predictions aos8, Qos4, Qos1. The loss L is computed between the multi-scale predictions and ground truth agt as L(agt, dos1, dos4, Qo88) = AL₁ £1 + ALLap Lap (3) L₁ is L1 loss and LLap is Laplacian loss used in [14,30,43]. The coefficients 1 and XL Lap control the contribution of each loss term, respectively. Both loss terms are computed on multi-scale predictions as L₁ = L1 (agt, αos1) + L1 (agt, αos4) + L1 (agt, αoss) (4) LLap=LLap (agt, dos1)+L Lap (agt, Qos4) + L Lap (agt, dos8) (5) Multi-Benchmark Inference During the inference phase, we conducted extensive evaluations of the Matting Anything Model (MAM) on multiple image matting benchmarks to assess its generality and adaptability. Given an input image I, SAM produced the initial mask predictionMethod SADall↓ SHM [4] LFM [57] HATT [37] SHMC [32] GFM-R [25] 17.81 / 16.36.12/37.28.01 / 22.61.50/57.6.8/6.11.6 / 15.5.5/3.27.0/29.GFM-D [25] SAM [21] MAM 10.89 / 11.10.26/11.25.00 / 44.17.30/25.2.9/3.2.9/4.10.8/28.3.5/9.10.2/9.21.0/ 15.16.1 13.35.6/34.6.4/6.5.9/6.14.8/25.10.1 / 15.MSE all↓ AM2K/PM-10K MADall↓ Gradall↓ 12.54 / 14.SADtri↓ 10.26 / 8.19.68 / 16.13.36/9.35.23/23.9.15 / 8.21.06 / 21.18.29/15.37.00/37.10.00 / 13.8.82 / 12.8.24/7.60.01 / 24.10.65/14.20.72/31.15.67/23.Table 2. Results on the semantic image matting benchmark AM2K and PM-10K. Metrics with all and tri as subscript indicates the evaluation of the whole image and the transition area, separately. ↓ means lower values indicate better performance for the metric. Method DIM [51] MSE all↓ MADall↓ 11.17.FDMPA [59] 10.16.LFM [57] 9.15.SHM [4] 7.15.HATT [37] 6.13.BSHM [32] 6.11.MODNet [19] 4.8.SAM [21] MAM 10.13.4.9.Table 3. Results on the semantic image matting benchmark PPM100. m Є R¹×H×W, which captured the rough delineation of the instance. Subsequently, M2M contributed to the refinement of the alpha matte prediction by providing multi-scale predictions aos8, Qos4, and Qos1. Then, following the iterative refinements, we progressively updated the predictions by replacing the corresponding regions in the mask prediction m with the respective multi-scale predictions that demonstrated positive weight maps, while in some simple cases the replacement is directly done upon aoss instead of m. This iterative refinement allowed us to refine the alpha matte estimation iteratively and enhance the precision of the final prediction a Є R¹×H×W¸ 4. Experiments We extensively evaluate the performance of MAM on six diverse image matting benchmarks. Through comprehensive evaluations using different metrics, we compare the performance of MAM with state-of-the-art image matting models on each benchmark. The results demonstrate that MAM consistently achieves comparable performance to specialized state-of-the-art models, reaffirming its versatility and effectiveness as a unified image matting solution. 4.1. Implementation Details Training Datasets During the training process, we randomly select foreground instances from several image matting datasets, including Adobe Image Matting dataset [51], Distinctions-646 [54], AM2K [25], Human-2K [34], and RefMatte [28], to ensure a diverse range of instance classes. For background images, we select them from two datasets: COCO [31] and BG20K [25] to provide a mix of both realworld and synthetic backgrounds. Evaluation Benchmarks To evaluate the adaptive ability of MAM, we test it on a variety of image matting benchmarks including the semantic image matting benchmarks PPM-100 [40], AM2K [25], PM-10K [25], the instance image matting benchmark RWP636 [55], HIM2K [42], and the referring image matting benchmark RefMatte-RW100 [28]. The box prompt is used for all benchmarks and the point prompt is only used in RefMatte-RW100. This comprehensive evaluation allows us to assess the generalization capability of MAM across various image matting tasks and benchmarks. Evaluation Metrics We evaluate the accuracy of predicted alpha matte for MAM with commonly adopted evaluation metrics. Specifically, we employ Mean Absolute Difference (MAD), Sum of Absolute Difference (SAD), Mean Squared Error (MSE), Gradient (Grad), and Connectivity (Conn) [38] as corresponding evaluation metrics. We scale MAD, MSE, Grad, and Conn by 103, 103, 10-3, and 10-³, respectively. Lower values indicate better performance for these metrics. Additionally, for instance-aware matting, we utilize Instance Matting Quality (IMQ) [42], which takes recognition and matting accuracy into consideration simultaneously. Higher values indicate better performance for the IMQ metric. Experimental Settings We trained MAM on a combination of training datasets using 8 RTX A6000 GPUs, with a batch size of 10 images per GPU. Each image was a combination of a randomly selected foreground instance and a background image. Images were cropped to a size of 1024 × 1024 and sent to a pre-trained ViT-B based SAM [21] with a bounding box prompt of the target instance. The feature maps and masks output by SAM were then fed into the M2M module for alpha matte predic-Model Method Size IMQmad Mask RCNN [13] 44.3 M 18.25.Synthetic Subset ↑ IMQmse IMQgrad IMQcon Natural Subset ↑ IMQmad IMQmse IMQgrad IMQconn 0.19.24.33.2.26.CascadePSP [7] 67.7 M 40.51.29.43.64.74.60.67.GCA [29] 25.2 M 37.51.38.39.45.61.44.48.SIM [41] 46.5 M 43.52.40.44.54.66.49.58.FBA [10] 34.7 M 36.51.37.38.34.48.36.37.MGMatting [55] + 29.6 M 51.67.53.55.57.71.66.60.InstMatt [42] SAM [21] + 29.7 M 63.78.64.67.70.81.74.72.93.7 M 49.61.4.51.61.74.13.65.MAM + 2.7 M 54.68.30.55.68.81.51.72.Table 4. Results on the instance image matting benchmark HIM2K. Metrics with mad, mse, grad, and conn as subscript indicates the similarity metrics for IMQ are MAD, MSE, Gradient, and Connectivity, separately. ↑ means higher values indicate better performance for the IMQ metric. MAM shows clear improvements over SAM under different metrics with only 2.7M extra trainable parameters, much lighter compared to other mask-guided methods like MGMatting and InstMatt. Method IMQmad IMQmse↑ Mask RCNN [13] 20.25.Method MDETR [18] Prompt SADall↓ MSEall↓ MADall text 131.67.75.CascadePSP [7] 42.52.CLIPSeg [35] text 211.117.122.GCA [29] 33.46.CLIPMat [28] text 107.59.62.SIM [41] 34.46.SAM [21] text 122.67.69.FBA [10] 35.47.MAM text 120.65.67.MGMatting [55] 30.53.SAM [21] point 214.123.124.InstMatt [42] 51.73.MAM point 168.89.97.SAM [21] 49.56.SAM [21] box 33.17.19.MAM 54.76.MAM box 29.15.16.Table 5. Results on the instance matting benchmark RWP636. tion. We employed the Adam optimizer with ẞ₁ = 0.and B2 = 0.99, trained for 20,000 iterations with warmup for the first 4,000 iterations. The weight map woss is always 1 at all pixels during training, while wos4 changes to the mask guidance from SAM after the 4,000 iterations and Wos1 changes to the boundary of a os4 after the 4,000 iterations as well. We set 3 refinement blocks for the prediction of aos8, 3 refinement blocks for the prediction of dos4, and 2 refinement blocks for the prediction of a osl. As a result, the total trainable parameters of MAM is 2.7 million parameters. We applied cosine learning rate decay with an initial learning rate of 0.001 during training. During inference, we used a single GPU with a batch size of 1. Each image was resized to have its longer side at 1024 pixels and its shorter side was padded to 1024 pixels before being sent to MAM for alpha matte prediction of the target instance. 4.2. Main Results Specialized vs Unified Model We present a high-level comparison between specialized image matting models and MAM on the semantic, instance, and referring image matting benchmarks in Table 1. It shows that MAM has clear improvements over SAM on all benchmarks. Furthermore, MAM shows comparable performance to each specialized Table 6. Results on the referring image matting benchmark RefMatte-RW 100. MAM with box prompt can reach significantly better performance than with the text prompt. image matting model and even reaches better performance on the HIM2K, RWP635, and RefMatte-RW100, which makes it a practical and feasible solution to unified image matting. Semantic Image Matting We evaluate the performance of MAM on three semantic image matting benchmarks: PPM100 [40], AM2K [25], and PM-10K [25], as presented in Table 3 and Table 2. The iterative refinement process is based on the doss prediction for all three benchmarks. On the PPM-100 benchmark, MAM achieves improvements of 6.2 MSEall and 3.9 MAD all over SAM. Similarly, on the AM2K benchmark, MAM outperforms SAM with enhancements of 7.64 SAD all, 4.4 MSEall, 4.5 MADall, 41.Gradall, and 7.59 SADtri Instance Image Matting In Table 4 and Table 5, We evaluate MAM on two instance image matting benchmarks: HIM2K [42] and RWP636 [55]. For HIM2K, the iterative refinement is based on prediction mask m since it contains multiple instances per image and starting from m removes false positive predictions. Compared to other state-of-theart methods on HIM2K, MAM reaches comparable performance with only 2.7 M extra trainable parameters, whichModel Natural Subset Method Size IMQmad IMQmse SAM [21] 93.7 M 50.61.+ Mask-Select 93.7 M 61.74.MAM Baseline 1.0 M 52.71.+ Multi-Scale Prediction 2.7 M 60.74.+ Iterative Refinement + Multi-Dataset Training 2.7 M 65.78.2.7 M 68.81.Table 7. Ablation study of MAM on the HIM2K benchmark. The MAM Baseline is built upon the SAM model with the box prompt. The other strategies are gradually added to the MAM Baseline and end up with 2.7 M extra trainable parameters. is only 10% of the specialized models like MGMatting and InstMatt, which use the mask guidance from Mask RCNN. On the RWP636 benchmark, we apply the iterative refinement from aos8 and MAM reaches the new state-of-the-art with 54.40 IMQmad and 76.45 IMQmse. Referring Image Matting In Table 6, we present the evaluation of MAM on the RefMatte-RW 100 benchmark [28], a recently introduced referring image matting benchmark. While previous methods rely on text prompts for referring image matting, we leverage the bounding boxes and text descriptions as the prompts for SAM. Considering the text prompt for SAM has not been released yet, we use Grounded-SAM [33] to support text prompt guidance. Remarkably, MAM achieves superior performance when utilizing the bounding box as the prompt for SAM, surpassing the text-based methods CLIPSeg and CLIPMat by a significant margin. Moreover, the use of bounding boxes as prompts offers user-friendly and intuitive interactions, as users find it easier to provide bounding boxes compared to composing a fixed text paragraph. This observation suggests that the bounding box prompt is more effective for interactive image matting than the text or point prompt for referring image matting. 4.3. Ablation Study We conduct comprehensive ablation studies on the M2M module of MAM, considering that SAM remains frozen during the training process. To assess the performance of MAM, we select the real-world subset of the HIM2K benchmark. SAM on HIM2K We begin by evaluating the pre-trained ViT-B-based SAM using bounding boxes and points as prompts for the target instance. SAM with box-based prompts significantly outperforms the point-based prompts and the final mask output is selected based on the mask with the highest Intersection over Union (IoU) score with the bounding box. SAM demonstrates strong performance on the HIM2K benchmark, achieving 61.15 IMQmad and 74.01 IMQmse on the natural subset. Building MAM We then construct the M2M baseline by D Image GT D MAM MG[51] Figure 3. Visualizations of alpha matte predictions from MGMatting and MAM. Improvements are highlighted in the red boxes. integrating the M2M module, which takes SAM's mask and feature maps, as well as the image, as inputs. This baseline, comprising 3 connected refinement blocks and predicting at 1/16 resolution, yields inferior performance compared to SAM, as the low-resolution predictions lack fine details of the alpha matte. However, by gradually incorporating multi-scale predictions and iterative refinement, as described in Section 3.2, the performance of MAM improves. Additionally, the adoption of multi-dataset training, as outlined in Section 3.3, further enhances performance, resulting in 68.37 IMQmad and 81.56 IMQmse on the natural subset. Subsequently, we assess MAM's performance on other benchmarks without retraining to validate its generality and adaptability. 4.4. Visualization In Figure 3, we compare matting performance between MGMatting and MAM of images that contain multiple instances. They both leverage mask guidance from SAM. It shows that MAM is able to give more accurate alpha matte predictions with only 10% parameters compared to MGMatting under the same mask guidance. It also has fewer false positive predictions in other instances. In Figure 4, we further provide visualizations of the mask and alpha matte predictions from SAM and MAM. These images are selected from the semantic image matting benchmarksMAM SAM[21] GT Image MAM SAM[21] GT Image 選 8 t c 意! FFF Figure 4. Visualizations of mask and alpha matte predictions from SAM and MAM. Improvements are highlighted in the red boxes. and contain a single instance that can be a person, animal, or transparent object. The visualizations demonstrate that MAM achieves significantly improved predictions in the transition areas without the trimap guidance, which highlights the superior performance of MAM in refining and enhancing the quality of alpha matte predictions. 5. Conclusion In this paper, we introduce Matting Anything Model (MAM), which uses the Segment Anything Model (SAM) as a guidance module with a lightweight Mask-to-Matte (M2M) module to refine the mask output into the alpha matte of the target instance. M2M is designed to handle various image matting tasks, including semantic, instance, and referring image matting, using a single model based on user prompts including points, boxes, and text. We evaluate MAM on six image matting benchmarks and demonstrate that it achieves comparable performance to the specialized state-of-the-art methods under various evaluation metrics. Our proposed model offers a more versatile and efficient solution for interactive and unified image matting.References [1] Yagiz Aksoy, Tunc Ozan Aydin, and Marc Pollefeys. Designing effective inter-pixel information flow for natural image matting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.[2] Xue Bai and Guillermo Sapiro. A geodesic framework for fast interactive image and video segmentation and matting. In 2007 IEEE 11th International Conference on Computer Vision. IEEE, 2007.[3] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.[4] Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, and Kun Gai. Semantic human matting. In Proceedings of the 26th ACM international conference on Multimedia, 2018. 2,[5] Qifeng Chen, Dingzeyu Li, and Chi-Keung Tang. Knn matting. IEEE transactions on pattern analysis and machine intelligence, 2013.[6] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. arXiv preprint arXiv:2112.01527, 2021.[7] Ho Kei Cheng, Jihoon Chung, Yu-Wing Tai, and Chi-Keung Tang. Cascadepsp: Toward class-agnostic and very highresolution segmentation via global and local refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 4,[8] Yung-Yu Chuang, Brian Curless, David H Salesin, and Richard Szeliski. A bayesian approach to digital matting. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001. IEEE, 2001.[9] Xiaoxue Feng, Xiaohui Liang, and Zili Zhang. A cluster sampling method for image matting via sparse coding. In European Conference on Computer Vision. Springer, 2016.[10] Marco Forte and François Pitié. ƒ, b, alpha matting. arXiv preprint arXiv:2003.07711, 2020.[11] Leo Grady, Thomas Schiwietz, Shmuel Aharon, and Rüdiger Westermann. Random walks for interactive alpha-matting. In Proceedings of VIIP, 2005.[12] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.[13] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 3,[14] Qiqi Hou and Feng Liu. Context-aware image matting for simultaneous foreground and alpha estimation. In ICCV, 2019.[15] Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and Thomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In TPAMI, 2020.[16] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. CVPR, 2023.[17] Jitesh Jain, Anukriti Singh, Nikita Orlov, Zilong Huang, Jiachen Li, Steven Walton, and Humphrey Shi. Semask: Semantically masked transformers for semantic segmentation. arXiv preprint arXiv:2112.12782, 2021.[18] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1780-1790, 2021.[19] Zhanghan Ke, Jiayu Sun, Kaican Li, Qiong Yan, and Rynson WH Lau. Modnet: Real-time trimap-free portrait matting via objective decomposition. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1140-1147, 2022. 4,[20] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019.[21] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 3, 4, 5, 6,[22] Jiachen Li, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, Yunchao Wei, and Humphrey Shi. Vmformer: End-to-end video matting with transformer. arXiv preprint arXiv:2208.12801, 2022.[23] Jiachen Li, Roberto Henschel, Vidit Goel, Marianna Ohanyan, Shant Navasardyan, and Humphrey Shi. Video instance matting. arXiv preprint arXiv:2311.04212, 2023.[24] Jiachen Li, Marianna Ohanyan, Vidit Goel, Shant Navasardyan, Yunchao Wei, and Humphrey Shi. VideoMatt: A simple baseline for accessible real-time video matting. In CVPR Workshops, 2023.[25] Jizhizi Li, Jing Zhang, Stephen J Maybank, and Dacheng Tao. Bridging composite and real: towards end-to-end deep image matting. International Journal of Computer Vision, 2022. 2, 4, 5,[26] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep automatic natural image matting. arXiv preprint arXiv:2107.07235, 2021.[27] Jizhizi Li, Jing Zhang, and Dacheng Tao. Deep image matting: A comprehensive survey. ArXiv, 2023.[28] Jizhizi Li, Jing Zhang, and Dacheng Tao. Referring image matting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2244822457, 2023. 2, 4, 5, 6,[29] Yaoyi Li and Hongtao Lu. Natural image matting via guided contextual attention. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020. 2,[30] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip Sengupta. Robust high-resolution video matting with temporal guidance. arXiv preprint arXiv:2108.11515, 2021. 2,[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision. Springer, 2014.[32] Jinlin Liu, Yuan Yao, Wendi Hou, Miaomiao Cui, Xuansong Xie, Changshui Zhang, and Xian-sheng Hua. Boosting semantic human matting with coarse annotations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2,[33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 3,[34] Yuhao Liu, Jiake Xie, Xiao Shi, Yu Qiao, Yujie Huang, Yong Tang, and Xin Yang. Tripartite information mining and integration for image matting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021.[35] Timo Lüddecke and Alexander Ecker. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7086-7096, 2022.[36] GyuTae Park, SungJoon Son, Jae Young Yoo, SeHo Kim, and Nojun Kwak. Matteformer: Transformer-based image matting via prior-tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022.[37] Yu Qiao, Yuhao Liu, Xin Yang, Dongsheng Zhou, Mingliang Xu, Qiang Zhang, and Xiaopeng Wei. Attention-guided hierarchical structure aggregation for image matting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020. 2,[38] Christoph Rhemann, Carsten Rother, Jue Wang, Margrit Gelautz, Pushmeet Kohli, and Pamela Rott. A perceptually motivated online benchmark for image matting. InIEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009.[39] Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Background matting: The world is your green screen. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020.[40] Jiayu Sun, Zhanghan Ke, Lihe Zhang, Huchuan Lu, and Rynson WH Lau. Modnet-v: Improving portrait video matting via background restoration. arXiv preprint arXiv:2109.11818, 2021. 2, 5,[41] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Semantic image matting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1,[42] Yanan Sun, Chi-Keung Tang, and Yu-Wing Tai. Human instance matting via mutual guidance and multi-instance refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 2, 4, 5,[43] Yanan Sun, Guanzhi Wang, Qiao Gu, Chi-Keung Tang, and Yu-Wing Tai. Deep video matting via spatio-temporal alignment and aggregation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.[44] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.[45] Jue Wang and Michael F Cohen. Image and video matting: a survey. 2008.[46] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. Solov2: Dynamic and fast instance segmentation. Advances in Neural information processing systems, 2020.[47] Yu Wang, Yi Niu, Peiyong Duan, Jianwei Lin, and Yuanjie Zheng. Deep propagation based image matting. In IJCAI, 2018.[48] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Hanqing Zhao, Weiming Zhang, and Nenghai Yu. Improved image matting via real-time user clicks and uncertainty estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15374-15383, 2021.[49] Bo Xu, Jiake Xie, Han Huang, Ziwen Li, Cheng Lu, Yong Tang, and Yandong Guo. Situational perception guided image matting. In Proceedings of the 30th ACM International Conference on Multimedia, 2022.[50] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.[51] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep image matting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2,[52] Stephen DH Yang, Bin Wang, Weijia Li, YiQi Lin, and Conghui He. Unified interactive image matting. arXiv preprint arXiv:2205.08324, 2022.[53] Jingfeng Yao, Xinggang Wang, Lang Ye, and Wenyu Liu. Matte anything: Interactive natural image matting with segment anything models. arXiv preprint arXiv:2306.04121, 2023.[54] Haichao Yu, Ning Xu, Zilong Huang, Yuqian Zhou, and Humphrey Shi. High-resolution deep image matting. AAAI, 2021. 2,[55] Qihang Yu, Jianming Zhang, He Zhang, Yilin Wang, Zhe Lin, Ning Xu, Yutong Bai, and Alan Yuille. Mask guided matting via progressive refinement network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 2, 4, 5,[56] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks. In International conference on machine learning. PMLR, 2019.[57] Yunke Zhang, Lixue Gong, Lubin Fan, Peiran Ren, Qixing Huang, Hujun Bao, and Weiwei Xu. A late fusion cnn for digital matting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 7469– 7478, 2019.[58] Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin Lmseg: Language-guided multiarXiv preprint arXiv:2302.13495, Wang, and Fan Wang. dataset segmentation. 2023.[59] Bingke Zhu, Yingying Chen, Jinqiao Wang, Si Liu, Bo Zhang, and Ming Tang. Fast deep matting for portrait animation on mobile phone. In Proceedings of the 25th ACM international conference on Multimedia, 2017. 2,