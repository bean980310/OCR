arXiv:2307.12854v1 [cs.CV] 24 JulMultiscale Video Pretraining for Long-Term Activity Forecasting Reuben Tan¹ Matthias De Lange² Kate Saenko1,Michael luzzolino Bryan A. Plummer¹ Karl Ridgeway³ Lorenzo Torresani³ ¹Boston University, 2KU Leuven, ³ Meta {rxtan,bplum, saenko}@bu.edu, {matthias.delange} @kuleuven.be {mliuzzolino, karl.ridgeway, torresani}@meta.com Abstract Long-term activity forecasting is an especially challenging research problem because it requires understanding the temporal relationships between observed actions, as well as the variability and complexity of human activities. Despite relying on strong supervision via expensive human annotations, state-of-the-art forecasting approaches often generalize poorly to unseen data. To alleviate this issue, we propose Multiscale Video Pretraining (MVP), a novel selfsupervised pretraining approach that learns robust representations for forecasting by learning to predict contextualized representations of future video clips over multiple timescales. MVP is based on our observation that actions in videos have a multiscale nature, where atomic actions typically occur at a short timescale and more complex actions may span longer timescales. We compare MVP to state-of-the-art self-supervised video learning approaches on downstream long-term forecasting tasks including longterm action anticipation and video summary prediction. Our comprehensive experiments across the Ego4D and Epic-Kitchens-55/100 datasets demonstrate that MVP outperforms state-of-the-art methods by significant margins. Notably, MVP obtains a relative performance gain of over 20% accuracy in video summary forecasting over existing methods. 1. Introduction Long-term forecasting of human activities (illustrated in Figure 1) is a key capability that is crucial for developing intelligent and collaborative machines. Machines that reason about future actions given some observations are better able to plan their own behavior accordingly and interact more effectively with other agents in dynamic environments. However, forecasting future actions is inherently challenging. To begin, the model has to understand the current state of the environment under partial observability. More importantly, the non-deterministic nature of the future compounds the Downstream long-term forecasting tasks Input partially observed video Predict future actions !! Retrieve correct summary ☑ Х Summary 3: CI kneads the dough and make a pizza. I I I repair close tighten board cover knob Action forecasting Summary 1: C Summary 2: C repairs the removes the Ilcircuit and turn circuit and put Ithe switch on. away the tool. Video summary forecasting -L Figure 1: Long-term activity forecasting tasks. We pretrain a video model and transfer its learnt representations to long-term action and video summary forecasting. 'C' denotes the camera-wearer in the summaries. difficulty of having to infer the relationships between actions and objects observed over time and also predict how these relationships will evolve in the future. State-of-the-art long-term forecasting methods (e.g., [16, 17]) have focused on learning more effective functions for modeling long-term temporal dynamics in videos by leveraging fully attentional models [29], but still rely on pretrained visual representations that are learnt using the standard objective developed for action recognition. However, this objective often encourages a video model to only understand short-term dependencies in a short clip instead of capturing long-term interactions and dynamics of the video. This may limit the generalizability of the pretrained visual representations to long-term forecasting tasks. Despite relying on strong training supervision from human-annotated action labels, the above-mentioned approaches still generalize poorly to unseen data [16], which lends support to our theory. To improve pretraining for long-term forecasting, we first make the observation that videos generally have a multiscale nature, where actions can be decomposed into subactions that occur at different levels of granularity. Consider Figure 2 that depicts a video of someone preparing a Video Encoder Video clip pair pretraining (prior work) Maximize Video similarity Encoder Predict contextualized future representations Prediction heads Video Encoder Observed video clips Maximize similarity Video Encoder Multiscale video pretraining (ours) Video Encoder Video Encoder Video Encoder Maximize similarity Future video clips over multiple timescales Figure 2: Multiscale Video Pretraining (MVP). In contrast to prior self-supervised methods [24, 13] that maximize the similarity between representations of clips from the same video, MVP trains a model to predict future contextual information over different time scales, helping it to generalize better to long-term forecasting tasks. meal. At the highest level of abstraction, the complex action of making an omelette comprises multiple actions, which generally occur at shorter timescales, such as cracking eggs and adding oil. We hypothesize that learning to understand this structure may be crucial for inferring the underlying goals and intentions of the agents involved, thus facilitating more accurate predictions of their subsequent actions. We endeavor to encode the multiscale nature of actions into the learnt video representations in a self-supervised manner during pretraining, which will generalize more effectively to downstream long-term forecasting tasks. To this end, we introduce a novel Multiscale Video Pretraining (MVP) approach (illustrated in Figure 2), which encourages a video model to learn to predict contextualized representations of future video clips that have been aggregated over different timescales using information from a partially observed video sequence. MVP draws inspiration from the required capability in long-term forecasting tasks, which necessitates being able to reason about the spatial and temporal structures of observed actions and predict future events that occur over multiple scales and temporal resolutions. During pretraining, MVP learns to infer the knowledge from an observed clip sequence that is required to predict the contextual information contained in future clips. Given the lack of ground-truth labels in our selfsupervised formulation, we generate prediction targets by computing contextualized representations of future video clips. This key aspect of MVP distinguishes it from the state-of-the-art video pretraining objective of maximizing the similarity between representations of different clips sampled from the same video [24, 13] (Figure 2 top). Feichtenhofer et al. [13] demonstrate that the latter objective encourages different clips of the same video to have similar representations over the spatiotemporal dimensions. While learning clip-invariant video representations may be beneficial to the task of short-term action recognition, they do not encode the high-level semantic structure of the observed video. In contrast, the MVP learning objective trains the video model to extrapolate future information at multiple scales from an observed video sequence. By recognizing the relations between different actions in long videos at different levels of granularity, the video model can better understand the underlying structure of videos and make more accurate predictions about what will happen next. We evaluate the effectiveness of MVP by transferring its pretrained representations to downstream long-term forecasting tasks including order-agnostic and specific action forecasting (Figure 1). Furthermore, we also introduce the novel multimodal task of video summary forecasting, where the goal is to retrieve the corresponding textual summary of the observed and future activities from a set of distractors. MVP significantly outperforms state-of-the-art video pretraining approaches across the Ego4D and Epic-Kitchens55/100 datasets. More importantly, we extract key insights on the contributions of different aspects of MVP through an extensive ablation study that we hope will be beneficial to future work on learning multiscale video representations. 2. Related work Self-supervised video pretraining. Self-supervised video pretraining [13, 18, 31] has been demonstrated to be beneficial for improving performance on downstream tasks such as activity recognition [10, 12, 13, 18, 19, 25, 31], video object segmentation [20], early action prediction [27] and unintentional action detection [18, 19] on target datasets including Kinetics-400/600 [2, 3, 21], HMDB-51 [22] and UCF101 [28]. Inspired by image-based self-supervised pretraining objectives [4, 5, 6], state-of-the-art video approaches [13, 24, 31, 33] often use a similar learning objective of maximizing the similarity between representations of two clips sampled from the same video. The Contrastive Video Representation Learning (CVRL) [24] approach also demonstrates that the applied transformations have to be consistent across all frames for optimal performance. Feichtenhofer et al. [13] also demonstrate that this objective of learning video clip-invariant representions can be extended beyond pairs of clips, which further improves the robustness of the learnt representations to the downstream task of action recognition. Additionally, the Contrastive Predictive Coding (CPC) [23] and Dense Predictive Coding (DPC) [18] approaches are also similar in spirit, where their learning objectives are to predict coarse clip-level and fine-grained spatiotemporal region representations of future clips given an observed sequence of clips for context, respectively. Han et al. [19] further build on this by introducing a memory bank of learnable vectors to account for the non-deterministic nature of predicting the future. However, in contrast to our MVP approach, the aforementioned approaches learn to predict the information in the future clips that directly follow after the observed sequence. More importantly, they only predict the base representations of future video clips instead of their contextualized representations, where their information has been aggregated over all preceding future clips in a causal manner. Additionally, BraVe [25] and LSTCL [31] embody a similar idea of learning to encode long-term temporal cues in clip-level representations by maximizing the similarity between a pair of short and long clips from the same video. The multiscale aspect of MVP distinguishes it from BraVe and LSTCL. While these methods help the video model to extrapolate the contextual information contained in the longer clip from the short clip, their learning objective does not explicitly encourage it to understand how the contextual information may change over different durations. This may limit the video model's ability to understand the relations between short actions that occur within a few frames and long-term actions that may span several seconds or more. In contrast, by learning to predict future contextual information over varying temporal spans, MVP may enable the trained video model to gain a deeper understanding of actions at different levels of abstraction and recognize complex actions by identifying their sub-actions. Action forecasting. State-of-the-art approaches [8, 15] are often aimed at addressing the short-term problem formulation where the goal is to anticipate the action that will occur in the next 7 seconds using the context of an observed video sequence of 7 seconds. Prior approaches have proposed to address this task by leveraging free additional information in the query videos either by aggregating past temporal context [14, 26] or predicting representations of future frames and video clips [30, 32]. Gong et al. [16] also leverage fully-attentional models to compute a more effective understanding of long-term temporal dynamics in the partially observed videos to generate more accurate predictions in the more challenging task of long-term forecasting [8, 11, 15, 17, 26]. However, these strongly-supervised approaches often leverage pretrained visual representations that do not encode the multiscale nature of actions in videos, which limits their effectiveness. As such, MVP is orthogonal to these methods since we aim to learn more efficient base representations for downstream long-term forecasting tasks. We leave it to future work to integrate multiscale representations into state-of-the-art forecasting approaches. 3. Multiscale Video Pretraining Our goal is to learn robust video representations that generalize well to downstream long-term forecasting tasks from a set of unlabeled videos. To this end, we introduce a self-supervised Multiscale Video Pretraining (MVP) objective, that aims to enable a video model to generate more accurate fine-grained action predictions of the forthcoming video clips given context from a partially observed clip sequence. Our approach is motivated by the reasoning that long-term forecasting requires the key capability of predicting the occurrences of future events at multiple timescales (e.g. near and distant future). Similarly, MVP requires a video model to infer the initial context of the video from an observed clip sequence and leverage the context to condition its predictions of information that is contained in future clips. Due to a lack of explicit annotations during pretraining, we propose to exploit the multiscale nature of complex actions in long videos for pseudo supervision. For example, the complex action of making an omelette can be decomposed into shorter atomic actions including cutting the onions and cracking the eggs. More specifically, MVP trains the video model to predict fine-grained spatiotemporal representations of the future that have been contextualized by aggregating information over varying numbers of future clips. We hypothesize that this objective encourages a video model to learn representations that encode future contextual information over multiple temporal spans. Unlike state-of-the-art video pretraining approaches [13, 23, 24, 31] which generally encourage different clips of the same video to have similar representations, MVP trains a video model to effectively represent the spatial and temporal structure of the observed video to extrapolate longterm information about future short and long actions. Intuitively, understanding the hierarchy of actions enables the video model to better reason about and recognize complex actions by identifying their sub-actions. Such an understanding may help the model to compute a more accurate prior distribution to condition its predictions on. 3.1. Temporal aggregation of video clip sequences While state-of-the-art video pretraining methods [13, 24] often utilize pairs of video clips from the same video, our MVP objective trains a video model with pairs of video clip sequences V and VF instead. MVP requires the video model to observe VO and infer the knowledge required to predict future contextual information that have been aggregated over the clips in VF at multiple timescales. To begin, we partition an input video into non-overlapping clips offrames each (about 0.8s) and randomly sample the observed as well as future clip sequences V° = {√₁°, VF = {VN+ V No +K,···, Vo+K+NÅ }, where No, NF, and K denote the number of observed, future, and temporal offset clips, respectively. We also define the temporal stride S as the difference in number of clips between two timescales. NF Thus, MVP makes Np predictions, where Np S = V} and Our video model (Figure 3) is comprised of a video clip Future video representation predictions Pred HeadPred HeadPred HeadTemporal Aggregator ho Multiscale predictions Maximize similarity Temporal Aggregator hμ Video Video Video Video encoder encoder encoder encoder 會 Video encoder Video encoder T-T-T T+K Observed video clips T+K+S T+K+2S Future video clips Video encoder T+K+3S Figure 3: Multiscale Video Pretraining. Given an observed sequence of video clips, MVP learns to extract information that is required to predict contextualized representations of future video clips over multiple timescales. encoding function go as well as temporal context aggregation functions ho and hμ. go is used to encode an input clip into a set of spatiotemporal region representations while ho and hμ are used to aggregate the temporal context of the observed and future clip sequences, respectively, by combining information over their constituent clip representations. Due to the computationally demanding nature of our MVP objective, we adopt the lightweight yet powerful Multiscale Vision Transformers (MViT) [10] as our base encoder ge without modifications, which has been shown to outperform prior video encoders in action recognition despite containing significantly fewer parameters. We encode the i-th video clip as: fi = go (Vi), fi Є RL×H×W×D where L, H, W and D denote the temporal, height, width and channel dimensions, respectively. Then, we compute contextualized representations for both input sequences by aggregating information over the clips: zº = No = F = h$(go(V°)), z³ = z¦µ = hµ(go(VF)), (1) where z and z are the contextualized representations for the observed and future sequences, respectively. 3.2. Spatiotemporal multi-head self-attention argue We that learning to predict fine-grained region representations over the spatial and temporal dimensions may be beneficial to understanding interactions between objects and actions in videos, unlike prior work focused on predicting global clip representations [23, 24, 31]. To this end, we train our model to predict spatiotemporal region representations of future clip sequences that have been contextualized over multiple timescales. This requires our temporal aggregation function to be able to compute contextual information between different spatial regions across multiple time steps. Intuitively, this objective can only be achieved with a strong understanding of the movement of objects over time and their relevance to different actions. A widely adopted convention for learning this function is to use multi-head self-attention (MHA) [1] over the entire set of spatiotemporal regions in the video clip sequence. However, since self-attention has quadratic complexity, the computational requirements increase rapidly even for short sequences of video clips. To address this, we only aggregate temporal context information between video clips by computing self-attention over all regions at the same spatial locations in the video clip sequence. This is motivated by our observation that the output region representations of MVIT for each time step have already been contextualized by aggregating information over other spatial regions, since the self-attention operation is an implicit function composited in the final video clip encoding function learnt by the MVIT model. We refer interested readers to [10] for more details on the MViT architecture. To begin, given an input spatiotemporal block SЄ RLxHxWxD, we project the set of temporal region features for the j-th spatial location S; E RLXD, where j = hw, into its queries, keys and values: Sj,q = SjWq, Sj‚k = SjWk, Sj,v = SjWv, where Wq, Wk and W₁, are the query, key and value projection weights of dimensions DxD. Then, we compute contextualized representations for the sequence using the MHA operation as follows: MHA(Sj,q, Sj,k, Sj,vw) = Sj,v Softmax SqSj,k (3) √D For a given spatiotemporal region representation Zi,t,h,w from the i-th video clip, we compute its contextualized representations as: Zi,t,h,w MHA(Zi,t,h,w). Finally, we predict the j-th future region representation at the k-th time step with a temporal stride of S by passing the contextualized spatiotemporal region representations through a two-layer multilayer perceptron (MLP), i.e., Zi,t,h,w MLPk (2,t,h,w). The entire set of predicted region representations is used in Section 3.3 to compute the training loss. Note that we use a different prediction head for each predicted timestep. 3.3. Multiscale targets and loss formulation = To compute the prediction targets for self-supervision, we apply the aggregation function hμ to VF in a causal manner, i.e. the set of contextualized spatial region representations St,j for the j-th spatial region at the 1-th time step is computed by attending only to the regions that precede it temporally. For the b-th sequence of future video clips in a sampled batch, we extract a set of target representations Z₁ = {¾‚k}, where k % S = 0 and Z₁ € RÑ³×LHW×D¸ Given a batch of unlabeled videos, we train the video model Pretraining Multiple Pretraining Ego4D ↑ EK55 ↑ EK100 ↑ approach clips used supervision Verb Action recognition No CVRL [24] No Self Strong 20.25.CPC [23] Yes Self 27.26.Noun 14.17.56 18.25.85 25.88 22.26.91 23.Mean Verb Noun Mean Verb Noun Mean 11.17.17.LSTCL [31] Yes Self 26.27.18.DPC [18] Yes Self 28.29.19.CVRL [24] Yes Self CONSTCL [33] Yes Self MVP (Ours) Yes Self 28.27 29.27.49 29.30.18 32.27.29 23.28.61 24.29.00 23.91 18.28.31 24.47 19.52 22.00 25.31.25 25.83 20.78 23.31 26.69 20.18 23.14.80 18.19.62 22.92 16.20.13 23.16 17.21.05 23.12.46 15.19.20.17.15 20.21.52 25.18.18 21.21.12 24.19.24 22.19.35 22.Table 1: Order-agnostic long-term forecasting. We report the mean average precision over all verb and noun classes. We see that self-supervised pretraining is generally more beneficial for long-term forecasting tasks than action recognition. end-to-end using a contrastive loss [23] formulation as: A = B Np LHW -ΣΣΣ - 10g b=1 j=1 n=exp(žb,j,n · Zb,j,n/T) exp(žb,j,n · Zb,j,n/T)+ Σ exp(b,j,n Zb',j',n' /T) (4) (b',j',n')!=(b,j,n) where 7 denotes the temperature value. 4. Experiments 4.1. Downstream tasks We compare our Multiscale Video Pretraining objective to state-of-the-art self-supervised video pretraining methods on the tasks of order-agnostic and specific long-term action forecasting as well as video summary forecasting. We pretrain the video models on Ego4D [17] and finetune them on both Ego4D and EpicsKitchen-55/100 [7, 8] for the downstream tasks. Additionally, we use a transformer encoder [29] and the meanpooling operation as our temporal context aggregators ho and hμ (Section 3.1), respectively. We refer readers to the supplemental for more details of these datasets, implementation and baseline models. Order-agnostic action forecasting. In order-agnostic long-term forecasting, we observe K% of a video of duration T and predict if an action will occur within the remaining video. Given a vocabulary of Nverb and Nnoun classes, we predict a Nverb-dimensional and Nnoun-dimensional binary vectors, where each dimension indicate the probability of the class occurring in the future. We formulate this as a multi-label prediction task and finetune all pretrained models by optimizing the binary cross-entropy loss computed over all verb and noun classes. We compute the mean average precision (mAP) over all verb and noun classes. Order-specific action forecasting. The order-specific task is a much more challenging setting, where the model is penalized even if it predicts the correct verb or noun but in the wrong order. Since the accuracy of the predicted actions depends on their temporal ordering, this can be formulated as a sequence prediction task. We finetune the pretrained " models by optimizing the total cross-entropy losses for both verbs and nouns computed over all time steps. We adopt the edit distance metric [17] to quantify how dissimilar the predicted and ground-truth action sequences are to each other. Video summary forecasting. In this multimodal task, for a video V of T temporal clips and an observed subsequence of length TO, the goal is to retrieve its corresponding summary from a set of distractor summaries. Given the video V and its summary L containing NL words, we first extract the contextualized representation for the observed clip sequence: CTO ha (gy (Vo:TO)). We extract a natural language representation fL = RLXDL for the summary using the pretrained BERT-Base [9] model: fL = k(L), where DL is the output dimension of the BERT model and ko denotes the BERT model that is parameterized by o. We use linear layers Wy and WL to project the video and language representations into the joint visual-semantic embedding space and finetune the models by optimizing the following contrastive loss formulation: B = L = Σ - log b=agg exp(Cb,To fb,L/T) еxp(СÛ‚Ã¤ · ƒÛ‚L/T)+ Σexp(ь, fm,L/T) m#b (5) Intuitively, this objective encourages the model to learn an alignment between the video and language representations by maximizing the similarity between corresponding pairs of videos and text summaries. Consistent with prior work in text-to-video retrieval [34], we adopt the Recall @K metric which computes the percentage of times the ground-truth summary is ranked in the top K retrievals. 4.2. Quantitative results 4.2.1 Order-agnostic long-term forecasting We aim to evaluate the effectiveness of our proposed MVP pretraining approach at learning video representations that encode future context over different temporal horizons. As such, we predict the future actions over the next 8 time steps Pretraining approach Action recognition No Multiple Pretraining clips used approach Strong Ego4D↓ CVRL [24] No Self CPC [23] Yes Self LSTCL [31] Yes Self DPC [18] Yes Self CVRL [24] Yes Self 0.CONSTCL [33] Yes Self MVP (Ours) Yes Self EK55↓ Verb Noun Action Verb Noun 0.754 0.901 0.977 0.741 0.0.746 0.845 0.960 0.719 0.0.735 0.838 0.956 0.719 0.0.752 0.846 0.963 0.721 0.0.734 0.821 0.950 0.708 0.0.822 0.952 0.719 0.0.735 0.818 0.951 0.704 0.0.724 0.809 0.943 0.690 0.EK100↓ Action Verb Noun Action 0.962 0.758 0.952 0.0.948 0.753 0.948 0.0.951 0.746 0.944 0.0.950 0.739 0.939 0.0.946 0.738 0.932 0.0.930 0.0.948 0.0.946 0.0.941 0.721 0.918 0.0.930 0.Table 2: Order-specific long-term forecasting evaluation. We use edit distance as the metric and report performance on verb, noun and action classes. An action class is a combination of its verb and noun classes. The results suggest that learning to understand the multiscale nature of videos is crucial for making accurate fine-grained predictions. and report the results on Ego4D, EK55 and EK100 in Table 1. We observe that self-supervised video pretraining is generally more beneficial to tasks requiring the key capability of long-term forecasting as compared to the strongly supervised variant of action recognition (first row of Table 1). Despite not requiring human-annotated labels during pretraining, our proposed MVP approach leads to approximately 14% improvement in future verb and noun predictions over its strongly-supervised counterpart when finetuned on the Ego4D task annotations. We hypothesize that the learning objective of predicting future clip representations is crucial for action anticipation. We also observe across all datasets that the state-ofthe-art pretraining objective of learning clip-invariant video representations [24, 13] does not generalize well to downstream tasks that require effective reasoning over clip sequences. In fact, simply extending the aforementioned pretraining objective to maximize the similarity between representations of two clip sequences sampled from the same video leads to significant improvements in future action predictions, especially over the longer temporal horizon ofclips. Our MVP approach also outperforms LSTCL [31] by a significant margin (e.g., we obtain a 3-5% improvement on Ego4D). Since LSTCL aims to encode long-term temporal cues in video representations of shorter clip sequences, our gains suggest that learning to predict contextual information of future clip sequences serves as an effective pretraining objective for long-term video understanding. 4.2.2 Order-specific long-term forecasting Table 2 reports the results across all three datasets on the more challenging task of predicting actions at specific time steps. Similar to our results for the order-unaware task in Section 4.2.1, we also observe that our proposed MVP approach generalizes better to a task that requires accurate fine-grained predictions. We note that pretraining approaches that learn to predict future clip representations at the fine-grained region-level such as DPC, CONSTCL and ours generally perform better under this challenging setting as compared to variants that predict global representations of future video clips including CPC and CVRL. One possible reason is that predicting fine-grained spatiotemporal region representations in the future is a much more challenging objective that necessitates the video model to understand the structure of different atomic actions in untrimmed videos. In particular, our gains across all three datasets suggest that learning to predict future region-level representations is especially crucial for verb predictions. This is evidenced by the much larger margins of improvement achieved by such approaches in predicting verbs in future clips as compared to nouns. For example, MVP reduces the edit distances by 0.029 and 0.018 on verb and noun predictions, respectively. In contrast to the order-agnostic task, we see that the improvements achieved by our MVP objective are smaller, which further emphasizes the difficulty of predicting actions precisely at specific timesteps. Additionally, we aim to understand the effectiveness of learning to predict future contextual information that is aggregated from video clips over different temporal horizons. In particular, we compare against CONSTCL [33], which also aims to reconstruct fine-grained spatiotemporal region representations of a future video clip sequence given the context of an observed clip sequence. Despite not relying on pretrained object detectors to identify location priors, our proposed MVP approach outperforms CONSTCL on both verb and noun predictions (e.g. reducing edit distance by 0.008 on Ego4D) while only using dense spatiotemporal feature maps. We hypothesize that our pretraining objective of predicting aggregated future spatiotemporal region representations helps a video model to better reason about the correlations between different atomic actions and how they contribute to the overarching goal in videos. 4.2.3 Video summary forecasting Finally, Table 3 reports our results on the multimodal video summary forecasting task. Besides video-only tasks, we Pretraining Multiple Pretraining approach clips Action recognition No supervision R@1↑ R@5↑ R@10+ Strong 0.90 5.00 8.CPC [23] Yes Self 9.70 28.60 41.DPC [18] Yes Self 10.10 29.70 43.CVRL [24] No Self 11.00 34.80 49.LSTCL [31] Yes Self 12.70 38.90 53.CONSTCL [33] Yes Self 11.40 41.80 53.CVRL [24] Yes Self MVP (Ours) Yes Self 15.90 40.70 56.19.30 50.70 65.downstream task of order-unaware forecasting on Ego4D. Order-unaware long-term forecasting on Ego4D Mean average precision (MAP)verbs nouns Table 3: Video summary forecasting on the Ego4D dataset. MVP helps the video model to learn more robust representations that generalize better than prior work to the multimodal task of text summary retrieval. note that the self-supervised pretraining approaches also generalize much better to a downstream task that involves the language modality than the strongly-supervised task of action recognition. Unlike the results on the previous tasks of order-unaware and specific long-term forecasting, we observe that the pretraining objective of learning clip-invariant video representations such as CVRL (single and multiple clips) and LSTCL outperforms DPC by a substantial margin of 15% in R@ 1 accuracy. We hypothesize that this may be due to the DPC pretraining approach training the video model to predict the representations of consecutive video clips in the future. In contrast, the aforementioned approaches sample the observed and predicted video clip sequences from the same video but at randomly determined times. This may encourage the video model to learn to extrapolate the contextual information further into the future instead of always predicting the immediate future as in the case of the DPC method. Interestingly, we also observe that learning to predict fine-grained spatiotemporal region representations during pretraining may be not be as critical for understanding the overarching context of a video as the previous evaluation tasks. This is evidenced by the fact that CVRL pretrained with multiple video clips actually outperforms CONSTCL by 4 % in R@1 accuracy. Lastly, the performance gains of approximately 3 – 8% in R@ 1 accuracy achieved by our proposed MVP approach over CVRL clip sequence, LSTCL and CONSTCL suggest that learning to reason about aggregated future contextual information over multiple time scales is especially beneficial to helping a model to extrapolate the semantics of the entire video. ~ 4.2.4 Ablation studies We ablate different aspects of MVP approach to determine their relative contributions to the robustness of the learnt representations. Specifically, we compare the effectiveness of the representations of different model variants on the45.0 47.60.0 62.50.0 52.5 55.0 57.MVP top-1 accuracy on Ego4D Figure 4: Benefit of MVP. We study the relation between self-supervised pretraining prediction accuracy and mean average precision on order-agnostic long-term forecasting. Temporal offset K Verb↑23.Noun ↑ 21.Mean↑ 22.27.15 26.26.27.26.27.26.25.26.27.26.26.Geometric 26.25.26.Random (ours) 30.32.33 31.Table 4: Temporal offset ablation on Ego4D. We ablate the effect of the temporal offset during pretraining on the downstream task of order-unaware long-term forecasting. Effectiveness of MVP. We evaluate the benefit of our Multiscale Video Pretraining approach in Figure 4 by studying the correlation between the prediction accuracy of the video model during pretraining and the downstream performance by using checkpoints at various stages of pretraining. While MVP uses a contrastive formulation, we compute the prediction accuracy as the percentage of predicted regions that have the highest similarity with their ground-truth counterparts. We observe a direct correlation between the prediction accuracy during pretraining and the mean mAP score over all verb and noun classes, which suggests that learning to encode the multiscale nature of videos in the base representations is beneficial for long-term forecasting tasks. Temporal offset K. In Table 4, we observe that verb and noun prediction accuracy increases as we increase K during pretraining. This is unsurprising since the video model should be able to better predict future actions by learning to reason about the contextual information further into the future during pretraining. However, we also see that using a temporal offset of 12 clips actually leads to a drop in performance. One possible reason is that the future is nondeterministic and predicting information too far into the future introduces a high degree of noise during pretraining. EMean MAPAggregation scale ablationMean MAP # Predicted steps = 4, Temporal stride s =Mean MAP # Input clips = 4, Temporal stride s =No agg Single-scale agg Multiscale agg (ours)(a) # Input clips (b) #Predicted clips (c)Mean MAP# Input clips = 4, # Predicted steps =Temporal stride S (d) Figure 5: Ablation of MVP. (a) The results suggest that learning to model the temporal dynamics in videos at multiple timescales is crucial for action forecasting. (b) Providing more context with more observed video clips is generally helpful for learning more robust representations. (c) Increasing the number of predicted steps helps the video model to make more accurate action predictions to a certain degree. (d) Using a small temporal stride to aggregate context in the future clip sequence over multiple timescales is more beneficial than higher values. We also hypothesize that sampling random temporal offset values works the best because learning to predict future contextual information over varying temporal horizons acts as a form of regularization and prevents the model from overfitting to predictions over a constant temporal period. Multiscale benefits. We investigate the importance of multiscale aggregation during pretraining on downstream performance (Fig 5(a)). Specifically, we train the video model with a variant of MVP where we only predict the uncontextualized representations of future clips (no aggregation) and another where the aggregation of context is computed over a single scale. To begin, we observe the importance of predicting contextualized representations, where predicting uncontextualized clip representations results in a drop of 2 ~ % in mean mAP. More importantly, we also see that learning to predict future clip representations that are aggregated over multiple timescales results in a significant improvement over predicting those that are only contextualized over a single timescale. These results may support our hypothesis that learning to understand the multiscale nature of actions helps the video model to better infer the underlying goals and thus, anticipate future actions. Number of input clips No. In Figure 5(b), we observe that increasing the number of clips in the observed sequence V during pretraining generally leads to better downstream performance. However, we see that the forecasting results drop when we use 8 input clips. One possible reason is that using more input clips results in more observed context which may ease the difficulty of the pretraining objective and consequently, reducing the robustness of the learnt representations to downstream forecasting tasks. Number of predicted clips Np. We also aim to understand the importance of varying the number of predicted clips during pretraining on downstream forecasting performance in Figure 5(c). Intuitively, setting a higher number of predicted future clips increases the difficulty of our MVP objective since the video has to learn to predict contextual information that is further out into the future. While increasing the number of predicted clips is generally beneficial for downstream performance, we also see that predicting 8 future clips results in a drop in performance. We theorize that it may be too hard to predict the contextualized information too far out into the future since it is non-deterministic. This may introduce some noise during pretraining which adversely affects the learnt video representations. Temporal stride S for aggregation. Last but not least, we ablate the effect of the temporal stride S during pretraining in Figure 5(d). We obtain the best downstream performance when we increase the temporal stride from 1 to 2, which may suggest that a higher temporal stride encourages the video model to learn to encode longer-term future contextual information. We hypothesize that larger strides actually results in a significant drop in performance because it may be too challenging for the video model to learn to understand the structure and relationships between different atomic actions if they are very distant in time. 4.3. Limitations The target representations in MVP are computed by aggregating information over future clips using a fixed temporal stride for different timescales. However, this may not always be realistic since different complex actions can consist of varying numbers of atomic actions. 5. Conclusion In summary, we introduce Multiscale Video Pretraining, a self-supervised approach that aims to learn robust video representations for downstream long-term forecasting tasks. Given an observed video clip sequence, we train a video model to predict aggregated representations of future clips over multiple timescales. We demonstrate empirically that learning to encode future contextual information helps the video model to generalize better to long-term forecasting tasks than prior work, which highlights the importance of multiscale pretraining to long-term video understanding. Last but not least, we extract key insights on different aspects of MVP, through an extensive ablation study, that we hope will be beneficial to further research on learning multiscale video representations. Some interesting avenues for future work may include further exploring the capabilities of these representations for other video and multimodal tasks such as action recognition and text-to-video retrieval. Acknowledgements: This material is based upon work supported, in part, by DARPA under agreement number HR00112020054. We would like to thank Gideon Stocek and Nishanth Alapati for their assistance with setting up the compute infrastructure for the experiments. References [1] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, volume 2, page 4, 2021. [2] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics600. arXiv preprint arXiv:1808.01340, 2018. [3] Joao Carreira, Eric Noland, Chloe Hillier, and Andrew Zisserman. A short note on the kinetics-700 human action dataset. arXiv preprint arXiv:1907.06987, 2019. [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597–1607. PMLR, 2020. [5] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. [6] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15750-15758, 2021. [7] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epickitchens dataset. In European Conference on Computer Vision (ECCV), 2018. [8] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision. arXiv preprint arXiv:2006.13256, 2020. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [10] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer. Multiscale vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6824-6835, 2021. [11] Yazan Abu Farha, Qiuhong Ke, Bernt Schiele, and Juergen Gall. Long-term anticipation of activities with cycle consistency. arXiv preprint arXiv:2009.01142, 2020. [12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202-6211, 2019. [13] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3299-3309, 2021. [14] Antonino Furnari and Giovanni Maria Farinella. Rollingunrolling Istms for action anticipation from first-person video. IEEE transactions on pattern analysis and machine intelligence, 43(11):4021-4036, 2020. [15] Rohit Girdhar and Kristen Grauman. Anticipative video transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 13505-13515, 2021. [16] Dayoung Gong, Joonseok Lee, Manjin Kim, Seong Jong Ha, and Minsu Cho. Future transformer for long-term action anticipation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30523061, 2022. [17] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995-19012, 2022. [18] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pages 0-0, 2019. [19] Tengda Han, Weidi Xie, and Andrew Zisserman. Memoryaugmented dense predictive coding for video representation learning. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part III 16, pages 312–329. Springer, 2020. [20] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk. Advances in neural information processing systems, 33:19545-19560, 2020. [21] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. [22] Hildegard Kuehne, Hueihan Jhuang, Estibaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In 2011 International conference on computer vision, pages 2556–2563. IEEE, 2011. [23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. [24] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6964-6974, 2021. [25] Adria Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Pătrăucean, Florent Altché, Michal Valko, et al. Broaden your views for self-supervised video learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1255-1265, 2021. [26] Fadime Sener, Dipika Singhania, and Angela Yao. Temporal aggregate representations for long-range video understanding. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XVI 16, pages 154–171. Springer, 2020. [27] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: A hierarchical video dataset for fine-grained action understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2616-2625, 2020. [28] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [30] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Anticipating visual representations from unlabeled video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 98-106, 2016. [31] Jue Wang, Gedas Bertasius, Du Tran, and Lorenzo Torresani. Long-short temporal contrastive learning of video transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14010-14020, 2022. [32] Yu Wu, Linchao Zhu, Xiaohan Wang, Yi Yang, and Fei Wu. Learning to anticipate egocentric actions by imagination. IEEE Transactions on Image Processing, 30:1143– 1152, 2020. [33] Liangzhe Yuan, Rui Qian, Yin Cui, Boqing Gong, Florian Schroff, Ming-Hsuan Yang, Hartwig Adam, and Ting Liu. Contextualized spatio-temporal contrastive learning with self-supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13977-13986, 2022. [34] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. In this supplemental, we provide the following additional material to the main submission: A. Training and evaluation datasets details B. Implementation details C. Spatiotemporal constrastive loss formulation D. Baseline models for comparisons A. Datasets Ego4D [17] is the largest dataset of egocentric videos spanning over 3600 hours of daily life activities ranging from household to outdoor leisure scenarios. These videos are collected by 931 camera-wearers from 9 different countries, who record their unscripted interactions as they engage in daily activities under a large variety of settings. In contrast to existing video recognition datasets, videos in Ego4D are generally much longer in duration since they span from 1 to 10 hours as compared to 10 seconds video clips in Kinetics 400/600 [2, 3]. Additionally, it is much larger in scale and diversity of activities than existing egocentric video datasets such as Epic-Kitchens 55/100 [7, 8]. Each video is also densely annotated by humans, who provide annotations describing notable interactions in the videos as well as highlevel summaries. This dataset facilitates the exploration and further research in a variety of downstream tasks such as audio-visual diarization and forecasting. We use the provided annotations to evaluate our proposed MTPL approach on long-term forecasting as well as video summary predictions. We adopt the same splits for training and evaluation on the target tasks as Grauman et al. [17]. In this dataset, we conduct our evaluations on the training and validation splits since the test evaluation is conducted on a held-out set via a submission to their challenge portal. We also note that the number of verb and noun classes present in all 3 provided splits are not consistent since each split contains some verb and noun classes that are not present in other splits. Please refer to the supplementary material for more details. EpicsKitchen-55/100. EpicKitchens-100 (EK100) [8] is another large dataset of egocentric videos. Similar to Ego4D, it also provides 700 long unscripted egocentric videos that span approximately 100 hours. It is less diverse than Ego4D since the participants only engage in daily activities in the kitchen. EpicKitchens-55 (EK55) [7] is an earlier and smaller version of EK100 but it provides the same types of videos and annotations. We use EK55 and EK100 to evaluate on the tasks of order-agnostic and orderspecific long-term forecasting. B. Implementation details B.1. Multiscale Video Pretraining We implement all models and experiments using the Pytorch deep learning library. We use the Multiscale Vision Transformer (MVIT) [10] as our base video encoder and 1 transformer encoder layers with 1 attention heads as our temporal context aggregator. The MVIT encoder typically accepts a video clip of 16 frames as input and outputs a global clip representation, which is the contextualized output of the classification token. However, in our case, we reduce the number of frames per clip to 8 due to memory constraints. Additionally, we discard the classification token during pretraining and perform our future feature predictions at the spatiotemporal region granularity. During the second stage of finetuning, we compute a global clip representation by performing meanpooling over the spatiotemporal region representations. Since we sample the video frames at 10 frames per second (FPS), the temporal duration of each clip is approximately 0.8 seconds long. Each input video clip is preprocessed by randomly scaling the height of the frames between 248 and 280 pixels and taking crops of 224 xpixels. During the first stage of pretraining on the Ego4D dataset, we also perform random augmentations to the video clips including random horizontal flipping and color jittering. The future feature prediction function is represented as a two-layer multilayer perceptron (MLP) with a non-linear ReLU operation and hidden dimension of 768. B.2. Downstream long-term forecasting tasks Figure 6 illustrates how our pretrained video model and its learnt representations are transferred to the orderagnostic and order-specific action forecasting as well as video summary forecasting. To begin, given the sequence of Ny observed video clips in each task V = {V₁, ···VNv }, we extract the contextualized representation of the last timestep as follows: ZNv = ho(90(Vz)), ZNv ЄRD (6) where D is the output channel dimension. For all downstream tasks, we finetune linear probes on top of the pretrained video model, which is kept frozen. Order-agnostic action forecasting. Given a vocabulary of and Nnoun classes, we predict a Nverb-dimensional and Nnoun-dimensional binary vectors as: Nverb Pverb=fverb (Ny), fnoun (NV), Pnoun (7) where each dimension in the predicted vectors indicates the probability of the verb or noun class occurring in the future. Order-agnostic long-term forecasting | Number of verb classes Prediction head Number of noun classes Video Model Order-specific long-term forecasting Verb: "cut" Noun: "banana" Verb: "pour" Noun: "milk" Prediction headPrediction headVideo Model Video summary forecasting Predict Verb: "wash" Noun: "spoon" Video summary representation Prediction head N Text summary representation Visual-semantic projection Visual-semantic projection Video Model Language Model Summary: "The camera wearer cleans the dishes before making a smoothie...' Observed video clip sequence Observed video clip sequence Observed video clip sequence Figure 6: Implementation for downstream long-term forecasting tasks. We finetune our pretrained video models on the downstream tasks of order-agnostic and order-specific action forecasting as well as video summary forecasting on the target datasets with strong supervision. We formulate this as a multi-label prediction task and finetune all pretrained models by optimizing the binary crossentropy loss computed over all verb and noun classes as: 2T+K+S Predicted representations В Nverb Nnoun L = = -Σ verb, b,¿ log (Pverb,b,i ) + Σ Ynoun,b,i ¿ log(Pnoun,b,¿)), Temporal negative b=1 i=i=(8) Positive negative Temporal Spatial negative Ground-truth representations where Yverb,b,i and Ynoun,b,i are the ground-truth verb and noun binary labels, respectively. Order-specific action forecasting. In this more challenging setting, the goal is to make fine-grained action predictions at specific timesteps. For simplicity, we adopt the same training and evaluation setup as in [17] and use separate prediction heads for different timesteps. For each timestep, we formulate the subtask as a multiclass prediction problem for both verbs and nouns. Consequently, we finetune the pretrained video models using the following loss formulation: L B Np ->> (verb,b,t log (Pverb,b,t)+(Ynoun,b,t log(Pnoun,b,t)). b=1 t=(9) Video summary forecasting. As shown in Figure 6 (right), we adopt the dual encoder architecture to address this multimodal task. Similar to prior work on learning joint visuallanguage representations including CLIP and ALIGN, we also use the late fusion mechanism where the semantic similarity between the final video and language representations are computed using a final dot product operation. I | I ZT+K ZT+K+S ----ZT+K+2S Figure 7: Spatiotemporal region predictions. Our MVP approach trains a video to predict future contextual information contained in fine-grained spatiotemporal regions. C. Spatiotemporal constrastive loss formulation We provide an illustration of how our proposed MVP objective trains a video model to predict fine-grained spatiotemporal region representations using the contrastive loss formulation in Figure 7. Given the predicted representation of the j-th spatial region at the t-th timestep 2t,j, we aim to maximize its semantic similarity with its ground-truth aggregated representation zt,; and the negative samples in the entire set of distractors consist of both hard negatives such as other spatial regions at the same timestep and easy negatives including representations that belong to clips from other videos in the sampled batch. D. Baseline models We briefly describe the self-supervised video pretraining baselines that we compare our proposed MVP objective against in our evaluations. Contrastive predictive coding (CPC). The Contrastive Predictive Coding (CPC) [23] approach aims to learn video representations that encode global information that is shared between different clips of a video. CPC uses the context from an observed clip sequence to predict the future uncontextualized information in the future clips that directly follow after the observed sequence. It also uses multiple prediction heads for representations of different timesteps that it tries to predict for. Dense predictive coding (DPC). The Dense Predictive Coding (DPC) [18] approach builds on top of CPC to learn video representations of predicting uncontextualized information but conditions its predictions for a given timestep with the context of the predicted information at the preceding timestep. Additionally, unlike CPC, the DPC objective aims to compute spatiotemporal representations instead of global clip representations. Contrastive video representation learning (CVRL). We also compare MVP to the Contrastive Video Representation Learning (CVRL) [24] approach, which is largely inspired by popular image-based self-supervised pretraining objectives [4, 5, 6]. CVRL trains a video model to maximize the similarity between representations of different clips that are randomly sampled from the same videos. While we compare to CVRL in its vanilla setting which uses pairs of video clips, we also train and evaluate a variant of CVRL which maximizes the similarity between representations of pairs of clip sequences. Long-Short Term Contrastive Learning (LSTCL). Similar to the CVRL approach, the Long-Short Term Contrastive Learning (LSTCL) [31] is initially proposed to learn video representations by maximizing the similarity between representations of video clip pairs. During pretraining, it accepts as input a short clip and another long clip which contains temporal information that is not present in the former. LSTCL trains a video model to extrapolate past and future information from a small observed temporal window. We also extend LSTCL to train on pairs of video clip sequences with the same total number of video clips per sample during pretraining to facilitate fair comparisons. Contextualized Spatio-Temporal Contrastive Learning with Self-Supervision (CONSTCL). Last but not least, we also compare to the Contextualized Spatio-Temporal Contrastive Learning with Self-Supervision (CONSTCL) [33] approach. CONSTCL aims to address the limitation of: spatiotemporal invariance [13] enforced by the CVRL objective. The CONSTCL objective leverages a region-based preraining task which trains the video model to transform video representations from one clip sequence to another, given the context from the first sequence.