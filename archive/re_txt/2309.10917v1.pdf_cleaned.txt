arXiv:2309.10917v1 [eess.AS] 19 SepEND-TO-END SPEECH RECOGNITION CONTEXTUALIZATION WITH LARGE LANGUAGE MODELS Egor Lakomkin, Chunyang Wu, Yassir Fathullah, Ozlem Kalinli, Michael L. Seltzer, Christian Fuegen Meta AI ABSTRACT In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoderonly fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on more than twenty five times larger speech dataset. Overall, we demonstrate that by only adding a handful number of trainable parameters via adapters, we can unlock contextualized speech recognition capability for the pretrained LLM while keeping the same text-only input functionality. Index Terms contextual biasing, large language models, speech recognition 1. INTRODUCTION In recent years, there has been growing interest in Large Language Models (LLMs) due to their remarkable efficacy in enhancing performance in tasks like question answering and summarization, surpassing specialized models [1, 2]. LLMs are trained on vast quantities of text data, thereby encapsulating a wealth of world knowledge within the network. This accumulated knowledge and contextual understanding prove to be particularly beneficial in the field of Automatic Speech Recognition (ASR), especially when additional context surrounding an utterance is available beyond the audio alone. For example, video titles and descriptions can provide insights into the topic of the video or offer clues about named entities that might be mentioned [3, 4]. Such contextual inWork done during internship at Meta AI. LLM decoder Text-text LoRa attention adapters <bos> Audio tokens Contextual text tokens (video title, description) Audio encoder Recognized spoken text Fig. 1. A speech recognition model with mixed-modal context consisting of audio and optional text tokens based on a pretrained LLM backbone. Speech encoder and LLM decoder are both initially pretrained. The LLM weights are frozen (orange blocks), while audio encoder and LoRa adapters are fine-tuned during training (blue blocks). formation can assist in disambiguating challenging pronunciations, as certain words, domain-specific terms, or named entities can often be inferred from context alone. Traditional approaches to ASR contextualization [4, 3, 5, 6] operate at the token or phrase level, employing techniques like biasing with weighted finite state transducers (WFSTs) or using specialized attention networks. These are typically either incorporated during the decoding stage or trained as separate components. Consequently, contextualization significantly improves the ASR system's ability to recognize named entities or specialized in-domain terms. However, there are some limitations to these approaches: The biasing is limited towards individual phrases or words, as opposed to contextualizing based on external information as a whole (for example, topic-based biasing). The biasing strength is usually controlled via a hyperparameter or requires specialized architectural changes and training procedures to ensure the system is not overbiased. Some of the contextualization methods influence only the decoder state without interacting with the encoder directly. In this work, we propose a Speech LLAMA - a decoderonly architecture inspired by recent developments in LLMs tailored towards speech recognition. It is trained to use the contextual information end-to-end without any additional hyperparameters. Specifically, 1) we prepend the whole available textual context as a prompt to an ASR system along with audio tokens. The Speech LLAMA hence have the full flexibility to look back and cross-corellate the contextual text tokens and the acoustic tokens when decoding the next spoken word. And 2) we employ the publicly available 7B LLAMA LLM [1] as a pretrained decoder for the Speech LLAMA. This simplifies the overall design of a contextual ASR as speech recognition can be considered as mixed-modal language model with next-token prediciton. Our intuition behind this is the pretrained LLMs already distill the linguistic information which should be particularly useful when reasoning which part of the context is relevant given the utterance. Our results on a competitive benchmark suggest a feasibility of this modelling approach. 2. RELATED WORK There have been several works on speech recognition models contextualization including deep and shallow biasing [8, 4]. Le et al. [4] introduced a weighted finite state transducer (WFST) composed from biasing strings which is attached dynamically during decoding and the scores of the RNN-T system and biasing WFST are interpolated. The advantage of such approaches is that they could be attached to any system after the training is completed. Another line of research is deep biasing methods that incorporate contextualization endto-end during the model training [9, 3, 6, 10, 11, 5]. A common limitation of these approaches is that the bias on the phrase level, rather than providing on the full context available. In addition, these approaches require a specialized biasing modules added to the main ASR architecture. In parallel to this reseach several approaches were presented incorporating LLMs for speech related tasks. Wu at al. [12] incorporated LLaMA LLM for speech translation by concatenating a textual prompt ("Translate audio to language X") with audio representations. AudioPalm [13] model was proposed mixing audio and text tokens for speech-to-text and speech to speech tasks. Fathullah et al. [14] presented results on enabling speech recognition capabilities for LLAMA model on the multi-lingual data. Recently a Whisper model [15] incorporated a biasing approach, where the previous segment's transcription was added to the prompt for the longform speech recognition. In difference to their work, we bias the system on the unstructed and sometimes unrelated textual context as not always video title and description match the context of speech. 3. EXPERIMENTAL SETUP Model: Figure 1 illustrates the overview of our proposed model. This speech LLM architecture consists of two main blocks: audio encoder and text decoder. The audio encoder firstly applies four downsampling blocks resuling in 16x time reduction of audio representations. After that a stack of Conformer [16] blocks with rotary positional embeddings [17] are applied with hidden dimensionality of 512 and kernel size of 9. At the end we add an additional downsampling block. As a result the decoder observes audio tokens sampled every 320ms with dimensionality of size 4,096. We pretrained the audio encoder with Connectionist Temporal Classification [18] criterion for 300k training steps on the same training data. We used a pretrained 7B LLaMA (v1) [1] as a decoder. To adapt text-only LLAMA to speech recognition task, we have added Low-Rank Adapters [19] to query, key, value, and output projection matrices in the self-attention layer of every decoder layer while keeping the rest of LLM parameters frozen throughout the training. We used the following LoRa parameters: rank of size 32, dropout rate of 5%, and 0.05 scaling parameter. Overall LoRa parameters add 30 million trainable parameters to the LLM decoder and the rest 6.billion are kept frozen. We used 80 dimensional log Mel features computed every 10ms with a window of 25ms. SpecAugment [20] with two frequency masks of width 27 and ten time masks with maximum width of 4% of the length of an utterance. We trained our models for 200,000 updates with mixed precision, linearly increasing the learning rate to 5e-4 in the first 20,updates and exponentially decaying to 1e-5 over the remaining updates. We use Adam with parameters ẞ1 = 0.9,0.98, weight decay 1e-5 and clip the gradient norm to 1. Our model is trained with 128 A100 GPUs for 3 days using Fairseq library [21]. = = Data: The models are trained on an in-house dataset that was de-identified with no personally identifiable information (PII) derived from public Facebook and Instagram videos. The data was further augmented with two distortion methods: speed perturbation [22] and randomly sampled additive background noise. For evaluation, we have sampled 3,200 videos comprising around 34 hours of speech that have context of at least 100 characters length with at least one non-frequent word from the context occurs in the transcription. Metrics: To evaluate our models, we report both the overall Word Error Rate (WER) and Rare WER, which considers only rare words. A word is considered rare if it does not occur in the 90% percentile of the most frequent words estimated on training data. Textual context: Similar to Xiao et al. [7] we incorporate video title and video description as an external context. We perform basic text post-processing like unicode character normalization and removal of all non-ascii symbols. Overall approximately 25% of videos from supervised video dataset have non-empty text context. When video title or description are present, we first concatenate and then tokenize them with the LLAMA tokenizer. After that, we prepend the <bos> token with the textual tokens. When both video title and descriptions are missing, the input corresponds to a traditional ASR setup without contextual information. The cross-entropy Table 1. Evaluation results of Speech LLaMA compared to large-scale RNN-T baseline on English speech data. We report overall WER and Rare WER. Rare WER specifically focuses on the accuracy of recognizing rare words in the dataset. Model Speech data (h) Trainable params (M) Context presence Training WER (%) SUB INS DEL Rare WER (%) Evaluation 1B RNN-T [7] 4M12.6.53 3.21 2.30.1B RNN-T [7] 4M12.6.23 3.2.28.Speech LLaMa 150k11.6.09 3.20 2.27.Speech LLaMa 150k11.Speech LLaMa 150k11.6.28 3.07 2.5.76 3.14 2.28.23.loss is masked for the contextual tokens and only computed for spoken tokens. In these experiments we limit the textual content to a maximum of 50 tokens for computational reasons. If the context is longer than the threshold, we perform a random crop of size 50 during training and crop the leading tokens during inference. Baseline: As a baseline we used a transformer based RNN-T system with one billion parameters [7], which is trained on four million hours of supervised and semi-supervised speech data. The RNN-T system architecture consists oftransformer layers in the encoder and 3 LSTM layers in the decoder. For contextualization it uses an WFST biasing method with neural language modelling shallow fusion [4], where the biasing FST is composed from video title and description. We are using exactly the same contextual information during decoding for the RNN-T baseline and our Speech LLAMA. 4. RESULTS Table 1 presents a summary of our decoding results on the evaluation set. We compare the Speech LLAMA against the offline RNN-T 1b model, considering two scenarios: with and without presenting contextualization information during decoding. The WER scores obtained for these scenarios using RNN-T are 12.34% and 12.13% respectively. Contextual biasing resuts in a relative WER reduction of approximately 1.7%. Even without the use of contextual information during training and evaluation, Speech LLAMA achieves a WER of 11.70%, a relative reduction of 5.2% over the RNN-T system trained on much more data. By incorporating context during training and evaluation, we achieve a significant improvement reaching an overall WER of 11.22% and resulting in 17% relative improvement in Rare WER, surpassing the performance of the RNN-T model with contextualization. It is worth noting that when we evaluate the Speech LLAMA trained with context but do not provide the context during inference, we obtain a WER of 11.98%. This corresponds to a slight WER gap compared to the model trained without context. We leave to address this minor performance difference to the future work, where adding a certain jitter to the context may improve the generalization of a model towards presence of the context. 4.1. Ablation studies 4.1.1. Context sensitivity To better understand how the model learns to use the context, we studied how receptive the model is to context perturbations. For this we tried a few ways to modify the prompt and measure its effect on the decoding. Specifically, we experimented with: 1. Replacing the actual context with words randomly sampled from the training data. 2. Replacing the context with the ground truth words. We filter out frequent words in this experiment as we assume that the model should not have significant issues in transcribing them. We expect a significant reduction of WER if the model is capable of copy-pasting the words from the context. 3. Replacing the contextual words with phonetical respellings of the words that appear in the transcripts. Our intuition is that such replacements are particularly challenging for the model and we should expect a bigger WER change compared to random substitutions. To generate re-spellings we employed a G2G [23] model. For every rare word in the ground truth we sample an alternative spelling from the G2G model and add it to the context. For example, if the word ball is present in the context and ground truth we replace it by bawl and use that as context instead of the original token. 4. In addition to the previous perturbation we probe appending a similar sounding word to the context (e.g. both tokens ball and bawl will be present in the context). This tests the ability of an ASR system to disambiguate the actual spoken word given a competitive word in context. Table 2. WER under different context perturbations during decoding stage. Table 4. Performance comparison of decoder-only Speech LLM and cross-attention Speech LLM. Context noise WER (%) Rare WER (%) (Original context) 11.23.(Remove all context) 11.28.Random 12.28.Respellings 11.28.Respellings (append) Ground Truth 11.10.25.19.We present our results in Table 2. We note that replacing the whole context with random words sampled from the training data results in only a marginal difference in WER compared to removing all external context (11.98% vs. 12.07%). This indicates that the model is robust against some contextual noise and can distinguish relevant from irrelevant context. Substituting rare words that match both the context and the ground truth with G2G respellings results in a significant drop in WER (11.22% → 11.89%), almost matching with not using any context. This hints that the majority of gains observed are due to the model being able to copy certain words from the context. In contrast, when we instead of replacing the matching contextual word rather append a competing similarsounding word, we observe a smaller WER drop (11.22% → 11.46%). This indicates that the model does not necessarily get confused by similarly pronounced words with different meanings. Furthermore, when we take the rare words from the ground truth into the context, the WER improves to 10.50% (6% relative change) and Rare WER improves by 18% relative. This further proves the ability of the model to utilize contextual information when present in order to better recognize rare entities. Table 3. Impact of the context masking structure on the WER. Masking Causal Full-Mask WER (%) 11.11.4.1.2. Causal vs Full Masking Traditionally causal masking is used in all self-attention layers for decoder-only language models to prevent future information leakage. However for offline speech recognition we have full audio and text context observed at the time of decoding and only transcription tokens are necessary to be masked causally. In this section we experiment the impact of applying causal masking on all input tokens and contrast it with applying full mask on the text and audio context followed by causal masking on transcription tokens. While the audio representations are fully contextualized already, we hypothesize Decoder WER (%) Decoder-only Encoder-decoder 11.11.that textual tokens may benefit from full masking. We present our results in Table 3. The full-mask shows only marginally better WER then causal masking (improving from 11.22% to 11.15%). This comes at a cost as efficient self-attention implementations are currently tailored towards causal masking (Flash-Attention v2) and using a custom masking slows down training by 10%. 4.1.3. Decoder-only vs Cross-attention Furthermore, we compared the decoder-only approach to a traditional encoder-decoder model by converting the Speech LLM architecture to Listen-Attend-Spell architecture [24]. To achieve that, instead of concatenating audio and text tokens we treated them separaterely. We added trainable cross-attention matrices to every LLM decoder layer. Table presents the results of this study. We observed that the two approaches perform similarly, with only minor improvement for the Encoder-Decoder architecture (11.22% 11.18%). This indicates that the decoder-only approach is a viable and straightforward method for performing ASR with or without contextualization.However, one limitation of the decoder-only approach is the quadratic attention complexity, which can impose restrictions on the overall sequence length. This limitation becomes significant as the context grows. To address this issue, we can employ techniques such as lower precision training (8 orbits) and linear attention approximation methods [25, 26]. 5. CONCLUSIONS AND FUTURE WORK In this work, we have presented to our knowledge the first results on utilizing pretrained LLMs to leverage contextual information in order to improve speech recognition. We have demonstrated that with a simple decoder-only architecture we can condition the ASR output on the unstructured text. Our approach shows superior performance against a strong baseline, proving the feasability of the proposed method at scale. End-to-end contextualization via text promping with LLMs shows better context utilization compared to our strong RNNT based baselines. In addition, our ablation studies show that the system is robust to noise perturbations and shows abilities to perform a phonetic disambiguation. As part of the future work, we plan to extend the methods towards long context and other modalities. 6. REFERENCES [1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, et al., "Llama: Open and efficient foundation language models," 2023. [2] OpenAI, "Gpt-4 technical report,” 2023. [3] Mahaveer Jain, Gil Keren, Jay Mahadeokar, Geoffrey Zweig, Florian Metze, and Yatharth Saraf, “Contextual RNN-T for open domain ASR," in INTERSPEECH. 2020, pp. 11-15, ISCA. [4] Duc Le, Mahaveer Jain, Gil Keren, Suyoun Kim, et al., "Contextualized streaming end-to-end speech recognition with trie-based deep biasing and shallow fusion,” in INTERSPEECH, 2021, pp. 1772–1776. [5] Kanthashree Mysore Sathyendra, Thejaswi Muniyappa, Feng-Ju Chang, et al., “Contextual adapters for personalized speech recognition in neural transducers,” in ICASSP. 2022, pp. 8537–8541, IEEE. [6] Golan Pundak, Tara N. Sainath, et al., “Deep context: End-to-end contextual speech recognition,” inIEEE Spoken Language Technology Workshop. 2018, pp. 418-425, IEEE. [7] Alex Xiao, Weiyi Zheng, Gil Keren, et al., “Scaling asr improves zero and few shot learning," in INTERSPEECH, 2021. [8] Ding Zhao, Tara N. Sainath, David Rybach, Pat Rondon, et al., "Shallow-fusion end-to-end contextual biasing," in INTERSPEECH. 2019, pp. 1418–1422, ISCA. [9] Duc Le, Gil Keren, Julian Chan, et al., “Deep shallow fusion for rnn-t personalization,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 251– 257. [10] Xuandi Fu, Kanthashree Mysore Sathyendra, Ankur Gandhe, et al., "Robust acoustic and semantic contextual biasing in neural transducers for speech recognition," CORR, vol. abs/2305.05271, 2023. [11] Tianyi Xu, Zhanheng Yang, Kaixun Huang, et al., "Adaptive contextual biasing for transducer based streaming speech recognition," 2023. [12] Jian Wu, Yashesh Gaur, et al., “On decoder-only architecture for speech-to-text and large language model integration," 2023. [13] Paul K. Rubenstein et al., “Audiopalm: A large language model that can speak and listen,” 2023. [14] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, et al., "Prompting large language models with speech recognition abilities," 2023. [15] Alec Radford, Jong Wook Kim, et al., “Robust speech recognition via large-scale weak supervision,” in ICML. 23-29 Jul 2023, vol. 202 of Proceedings of Machine Learning Research, pp. 28492–28518, PMLR. [16] Anmol Gulati, James Qin, et al., "Conformer: Convolution-augmented transformer for speech recognition," in INTERSPEECH. 2020, pp. 5036-5040, ISCA. [17] Jianlin Su, Yu Lu, et al., “Roformer: Enhanced transformer with rotary position embedding," CoRR, vol. abs/2104.09864, 2021. [18] Alex Graves et al., “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks," in ICML. 2006, vol. 148 of ACM International Conference Proceeding Series, pp. 369– 376, ACM. [19] Edward J. Hu, Yelong Shen, et al., “Lora: Low-rank adaptation of large language models," in ICLR. 2022, OpenReview.net. [20] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le, “Specaugment: A simple data augmentation method for automatic speech recognition," INTERSPEECH, Sep 2019. [21] Myle Ott, Sergey Edunov, Alexei Baevski, et al., “fairseq: A fast, extensible toolkit for sequence modeling," in ACL (Demonstrations), Minneapolis, Minnesota, June 2019, pp. 48–53, Association for Computational Linguistics. [22] Tom Ko, Vijayaditya Peddinti, et al., “Audio augmentation for speech recognition," in INTERSPEECH. 2015, pp. 3586-3589, ISCA. [23] Duc Le, Thilo Koehler, Christian Fuegen, and Michael L. Seltzer, "G2g: Tts-driven pronunciation learning for graphemic hybrid asr," in ICASSP, 2020, pp. 6869-6873. [24] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals, “Listen, attend and spell," CORR, vol. abs/1508.01211, 2015. [25] Tim Dettmers and Luke Zettlemoyer, “The case for 4bit precision: k-bit inference scaling laws," in ICML. 2023, vol. 202, pp. 7750–7774, PMLR. [26] Jiayu Ding, Shuming Ma, et al., “Longnet: Scaling transformers to 1,000,000,000 tokens," 2023.