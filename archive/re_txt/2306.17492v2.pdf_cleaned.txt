arXiv:2306.17492v2 [cs.CL] 27 FebPreference Ranking Optimization for Human Alignment Feifan Song¹, Bowen Yu²*, Minghao Li² Haiyang Yu², Fei Huang², Yongbin Li², Houfeng Wang¹* ¹National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University 2 Alibaba Group songff@stu.pku.edu.cn, wanghf@pku.edu.cn {yubowen.ybw, Imh397008, yifei.yhy, shuide.lyb} @alibaba-inc.com Abstract Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secure AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment. However, it encompasses two main drawbacks: (1) RLHF exhibits complexity, instability, and sensitivity to hyperparameters in contrast to SFT. (2) Despite massive trial-and-error, multiple sampling is reduced to pairwise contrast, thus lacking contrasts from a macro perspective. In this paper, we propose Preference Ranking Optimization (PRO) as an efficient SFT algorithm to directly fine-tune LLMs for human alignment. PRO extends the pair-wise contrast to accommodate preference rankings of any length. By iteratively contrasting candidates, PRO instructs the LLM to prioritize the best response while progressively ranking the rest responses. In this manner, PRO effectively transforms human alignment into aligning the probability ranking of n responses generated by LLM with the preference ranking of humans towards these responses. Experiments have shown that PRO outperforms baseline algorithms, achieving comparable results to ChatGPT and human responses through automaticbased, reward-based, GPT-4, and human evaluations. Introduction Large language models (LLMs) have demonstrated remarkable capabilities in meeting the diverse information needs of users (Brown et al. 2020; Chowdhery et al. 2022; Bubeck et al. 2023; Touvron et al. 2023; Li et al. 2023). Despite leveraging the extensive global knowledge and human behavior encoded within their trillion-token pretraining corpus, LLMs are unavoidably impacted by the existence of misleading, toxic, and detrimental content encompassed within it (Bai et al. 2022b; Ouyang et al. 2022). Consequently, reinforcement learning from human feedback (RLHF) is introduced to construct secure and manageable AI systems (Stiennon et al. 2020; Xue et al. 2023; Peng et al. 2023) by aligning linguistic space of LLMs to human values according to a set of candidates ranked by humans. Nevertheless, RLHF remains more complex than supervised learning, prone to optimization instability, and sensitive to hyperparameters. These limitations arise mainly from *Corresponding authors. Copyright 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. ONE y LLM PAIRy₁ y² LLM PAIRS ya LLMy² PRO y³ y LLM y Figure 1: Comparison among different SFT paradigms. PRO is distinguished from others for multi-positional and multidimensional contrasts. employing the agent, i.e. LLM to experience repetitive trialand-error rather than directly aligning it to human preference. Hence, Supervised Fine-Tuning (SFT) is expected as the possibility of more direct optimization to replace RLHF. SFT initially serves as only a warm-up process for RLHF, where the best candidates are selected to tune LLMs to intimate human-preferred data. Recent works have proposed more complex strategies to enhance SFT (Rafailov et al. 2023; Wu et al. 2023; Yuan et al. 2023; Dong et al. 2023). Despite some progress, there remains room for improvement: (1) The essence that powers RLHF to perform well is ignored. That is multiple sampling with scoring from broad linguistic space during training. Constrained by the given ranking length, most methods pay attention to pair-wise contrasts from semantic or scalar perspectives. (2) Even longer rankings are available, they tend to cut it into pairs, thus lacking distinction of candidates from a macro perspective. In this work, we thoroughly investigate the effect of enlarging sampling from linguistic space on human alignment. Based on this scenario, we propose the Preference Ranking Optimization (PRO) as an efficient framework of direct policy optimization. Figure 1 shows how PRO stands out from different SFT-based formulations. To be specific, we rethink the essence of RLHF and extend pair-wise contrasts from the Bradley-Terry model (Bradley and Terry 1952) to encompass one-to-N contrasts within a ranking of arbitrary lengths. Then, given an input prompt x and a set of responses ranked by humans, represented as y¹, y², …..‚y”, the proposed PRO algorithm begins by tuning the agent LLM to treat the best response y¹ as the positive and the remaining as negatives. This prioritization suggests that the LLM generates y¹ with a higher likelihood than those humans consider inferior. It then repetitively ignores the current top response and considers the next one as the positive against the rest, until there are no more responses in the ranking. Apart from focusing on obtaining more candidates, we particularly deploy proxies of different levels to sample utterances with various qualities and degrees of human alignment. Inspired by RLHF, we also add a self-bootstrapping method to dynamically sample new candidates from the recipient LLM and label it with an additional reward model. The new candidate is added to the original set for training. All of these extended rankings are designed to check the impact of quantity, quality, and diversity of texts. In general, PRO directly subjects the LLM to a n-length human preference ranking. By equipping LLMs with multipositional and multi-dimensional contrasts among candidates, PRO fully utilizes given ranking sequences of any length. As n approaches infinity, the recipient LLM is exposed to more and more samples with scores, and should continuously become perfectly aligned with human preferences. Especially when n = 2, PRO effectively optimizes the LLM using the pair-wise contrast. We thoroughly evaluate PRO through numerous experiments, including automatic reward model scoring, GPT-evaluation, and human evaluation. The main observations are as follows: (1) With a 2-ranking, PRO surpasses the current competitive baselines by a large margin. Also, the high quality and diversity of candidates in preference rankings can be crucial for ultimate performance. (2) The longer the length is, the more prominent improvement PRO can acquire. Even by adding responses generated by ChatGPT to continuously increase the ranking length, PRO achieves reward scores similar to ChatGPT, but with just 7B parameters. (3) Heterogeneous candidates manage to bringing better improvement of PRO than relatively homogeneous ones. Preliminary We commence by providing a brief review of RLHF. In order to train LLMs to generate responses that align with human preferences, RLHF consists of three stages: (1) Supervised Fine-tuning (SFT) where labelers furnish the desired behavior's response with t tokens, denoted as y = Y1,...,t, for a given prompt, denoted as x. Subsequently, The policy LLM goes through direct fine-tuning (maximum likelihood) on this data, resulting in a model denoted as "SFT. (2) Reward Model (RM) Training where TSFT is utilized by prompts x to generate pairs of responses, which are then denoted by human labelers as a more favored answer y¹ against the other one y², i.e. y¹ > y² | x. To predict these preferences, previous works employ the Bradley-Terry (BT) RM, which essentially constructs a pairwise contrast: LRM = - log exp (ro(x, y¹)) ` exp (rò(x, y¹)) + exp (rø(x, y²)) (1) (3) Reinforcement Learning (RL) stage where TSFT is further optimized in a trial-and-error process containing repetitive sampling from linguistic space and corresponding feedbacks simultaneously from the RM and reference policy. Regrettably, RLHF is criticized for several drawbacks, including increased complexity compared to supervised learning, sensitivity to hyperparameters, and the requirement for additional training of reward models and value networks. Methodology In this section, we first achieve a shift from the pair-wise contrast to PRO that leverages multi-positional one-to-N contrasts. With candidate rankings extending, PRO has access to more samples from linguistic space, and efficiently distinguishes the human-preferred feature of the positive samples from other negative samples. The whole process is completed in SFT settings, thus contributing to avoiding the numerous drawbacks associated with RLHF. Furthermore, we demonstrate the flexibility of our PRO in its ability to integrate with RM, thereby attaining advantages such as affordable preference ranking and differentiated contrast. From RLHF to PRO We re-examine the objective of the Bradley-Terry RM (Equation 1), which helps LLMs understand y¹ > y² through score contrast. The RM is trained in supervised settings and is expected to capture human preference. For a given prompt x and two responses y¹ and y², the RM should prefer y¹. To directly optimize the policy model, i.e. the LLM, we can similarly transfer the pair-wise contrast to it. In this way, the LLM is considered as both RM and policy network, denoted as r: exp (r(x, y¹)) (2) L = -log exp (r(x, y¹)) + exp (r(x, y²)) Naturally, if we expand the candidate set, i.e., increase sampling, r is able to reach more shots, which replaces the trialand-error experience. Considering there exist n candidate responses {y}, the human-annotated order y¹,,n is y¹ > y² > >yn. We first define the partial order between y¹ and responses behind it as y¹,2:n = y¹ > {y², …..‚y"}. With reference to InfoNCE Loss (He et al. 2020), we derive Equation 2 to a multi-dimensional one-to-N contrast: ... L = = = — log exp (r(x, y¹)) Σ²±₁ exp (r(x, y²)) (3) However, it does not fully leverage the ranking since it only characterizes y1, 2:n, disregarding the n-2 valuable rankings Context Human: How do I attract butterflies to my garden? Assistant: How would you like them to be attracted? Human: They are pretty. Assistant: positive LLM G Candidate Responses y y² y³ y" 000 000SFT + p1 > p² ... pr + n negative n-Figure 2: The pipeline of PRO for Human Feedback Alignment learning. Each candidate is concatenated with the prompt first, then processed by the LLM to estimate corresponding rewards, which are optimized by Equation 5. such as y²,3:n and yn−1,”. Instead, recursive contrasts can be imposed to exploit multi-positional patterns, which start with the first response, treat the remaining responses as negatives, drop the current response, and move to the next. This procedure repeats until there are no candidates left. Consequently, the further extension to Equation 3 is as follows: n-L = − log II n k=exp (r(x, y)) Σk exp (r(x, y²)) i=k (4) Equation 4 provides a comprehensive alignment with human feedback. In addition to adhering to human preferences, it is also desirable for the model to generate fluent replies. Therefore, the original supervised loss that requires the model to fit the responses considered the best by humans can also be incorporated. We conclude the above as the Preference Ranking Optimization (PRO) algorithm, as is demonstrated in Figure 2. Instead of optimizing the agent to approximate the RM, PRO enables the agent LLM to be directly trained by the following objective: LPRO (y¹n | x) = L+ BLSFT (5) where LSFT is the NLL loss of the top 1 candidate and ẞ is the hyper-parameter to maintain the balance between text quality and human preference. The policy agent (also RM) г is parameterized as "PRO in PRO: |yk|TπPRO (x, y) = |yk| log P(x, y) t=(6) PRO and RLHF share a similar objective, that is, understanding human preferences through more exposure to labeled samples. The difference is that RLHF relies on trialand-error experience and pair-wise contrasts, whereas PRO learns by assembling multiple samples into long rankings, which can be more efficient. Surprisingly, Equation 4 has a similar formulation with Plackett-Luce (PL) model (Plackett 1975; Luce 2012), a classic algorithm for ranking aggregation. Believing it is not a coincidence, we assume that PL model and PRO accomplish similar targets. PL model aims to acquire a global ranking of fixed candidates by combining multiple rankings, whose parameters correspond to these candidates, while PRO aims to learn general human preference, but the involved rankings contain different n candidates from each other. With modeling language space, the parameters of agent LLM should theoretically correspond to infinite candidates for each ranking (i.e. n = ∞). Although n is limited in practice, the larger it is, the more perfect the LLM should be. We accordingly implement experiments towards n in the next section. Grafting RLHF onto PRO While PRO can be directly valid on the human-annotated preference ranking sequence without the need for introducing concepts like the reward model in RLHF, grafting RLHF onto PRO can bring more flexibility to PRO. We outline three possible upgrades as follows: Affordable Preference Ranking. PRO is highly flexible, relying solely on ranked preference sequences. The source of the sequence is unrestricted, allowing for various possibilities. One approach involves requesting annotators to imagine multiple responses of different quality. Alternatively, a more efficient method entails utilizing different existing LLMs, such as ChatGPT and Alpaca, to generate multiple responses. These responses can be ranked using an additional reward model ro, similar to RLHF. Differentiated Contrast. The formulation as shown in Equation 4, treats all responses y² < yk as negative examples of y and applies the same penalty to them. However, this approach may not be reasonable, especially when the preference scores of different y² are similar. For instance, when the preference of yk+1 is only slightly worse than yk, while y" is significantly worse than yk, the model should differentiate and apply different penalty strengths, slightly penalizing y+1 and heavily penalizing yn compared to yk. To address this, we propose using the score ro (x, y²) from a reward model ro to indicate the numerical preference of y², and modify Equation 4 as follows: n-c=-Σ 10g == k=where Ti>k =k exp TπPRO (x,y) Tk (7) Σε exp TAPRO (x,y)ro(2,gh) – ro(x,y) (8) (9) T = min Th i>k When the difference between r$ (x, yk) and rø(x, y²) increases, the preference gap between yk and y² becomes more evident. Consequently, the temperature TË decreases, amplifying the penalty of positive example yk towards y², while it decreases when the difference is smaller. T is defined as the minimum temperature among all the negative examples to maintain a balance between the numerator and denominator. Our experiments (§) reveal that the dynamic temperature design significantly increases performance when optimizing LPRO alone while excluding LSFT. It also provides some performance gains when jointly optimizing LPRO and LSFT. Data Prepration Experiments We choose HH-RLHF Bai et al. (2022a) as the experimental dataset. It has 4 sub-sets, namely Harmlessbase, Helpfulɓase, Helpfulonline and Helpfulrejection. Each sample contains two different conversations rated by human annotators and is grouped into train/test splits. To further evaluate the performance of different methods on longer rankings, we augment each sample with new candidates from diverse LLMs to expand the range of ranked responses. These augmented datasets are denoted as HHRLHFLLM,i, where LLM represents the language models used (e.g. Alpaca, ChatGPT), and i is the length of the rankings. The unmodified dataset is referred to as HH-RLHFraw. Disclaimer: Data augmentation and inference from Curie/ChatGPT, as well as the following GPT-4 evaluation, are completed where the related services are available. Evaluation Metrics We present the findings using various evaluation methods: automatic, model-based, and human-based metrics. Specifically, we utilize BLEU (Papineni et al. 2002) to assess the text quality and RMs to measure the level of human preference gained. To avoid unfairness, we select two different RMs for training and evaluation, which we denote as RM train and RMeval, respectively. These metrics allow us to automatically evaluate numerous models. Human evaluation is the gold standard for assessing human preferences (Zhou et al. 2023). An annotator judge is presented with a question and two responses and tasked with determining the better option or declaring a tie. Furthermore, recent studies have shown that GPT-4 (OpenAI 2023) effectively evaluates the responses of chat assistants and aligns with human preferences (Zheng et al. 2023; Wang et al. 2023). Consequently, we involve GPT-4 to select the best from the two options. To mitigate positional bias (Zheng et al. 2023; Wang et al. 2023), we evaluate each candidate in both positions during two separate runs, and the final score is computed as the average of the two runs. Implementation Details We choose the popular LLaMA-7B (Touvron et al. 2023) as the backbone model, and implement PRO using Transformers (Wolf et al. 2020) and Accelerate (Gugger et al. 2022). We assign ẞ, the weight SFT loss, to 0.05*(−1)² where l is the ranking length. The sequence length, epoch, and learning rate are set to 512, 2, and 5e-6, respectively, while the maximum number of new tokens generated during inference is 128, and the total batch size is 112. Moreover, the expanded candidate rankings in each augmented dataset need to be re-sorted. However, the numerous manual sortings are time-consuming and costly. Therefore, we employ RM train to score and rearrange all candidate rankings during the pre-processing stage of training. In addition, values from RMeval will be normalized through Sigmoid function in case it occasionally provides extreme values that excessively influence the overall performance. RM train and RM eval are all implemented using open-source checkpoints. More particulars can be found in our code¹. Main Experiment We compare PRO with several LLMs (zero-shot), as well as baselines for fine-tuning LLaMA-7B (Touvron et al. 2023). Table 1 contains the results. It can be found that each finetuned LLaMA-7B gets a notable improvement on BLEU and Reward against the initial version without any specific alignment with human preference. Also, even without fine-tuning on HH-RLHF, LLMs can still show certain performance, while ChatGLM (Du et al. 2022) and ChatGPT with RLHF training beat LLaMA-7B, Curie (Brown et al. 2020), and Alpaca-7B (Taori et al. 2023). All of these prove the significance of Human Alignment. Next, we compare PRO with strong baselines on the same dataset using LLaMA-7B, including SFT, RLHF, CoH (Liu, Sferrazza, and Abbeel 2023), DPO (Rafailov et al. 2023) and RRHF (Yuan et al. 2023). In general, PRO can outperform all baselines, or show competitive performance in terms of Reward score while maintaining considerable BLEU scores. Specifically, even in the basic HH-RLHF containing just rankings of 2 candidates, PRO achieves a 6.52 improvement of Reward over SFT, and 2.6 over the well-performed DPO. CoH (Liu, Sferrazza, and Abbeel 2023) gets higher BLEU scores but falls short of PRO in Reward, which is mediocre. raw PRO exhibits a distinct advantage in terms of Harmlessness compared to Helpfulness. We attribute this to the fact that achieving Harmlessness is comparatively easier github.com/AlibabaResearch/DAMO-ConvAI/tree/main/PRO Training Set Method Harmlessbase Helpful base Helpful online Helpful rejection Total BLEU Reward BLEU Reward LLAMA Zero-shot Curie Alpaca 10.82 51.14.23 50.15.07 53.ChatGLM 15.39 63.ChatGPT 15.51 71.67.BLEU Reward 12.78 31.15.02 38.91 14.60 34.85 13.13 38.17.33 45.51 17.11 51.36 18.99 48.68 16.99 48.19.68 49.80 18.77 55.74 22.21 53.72 19.12 52.20.16 59.14 30.99 61.10 25.41 61.45 21.99 61.21.38 65.94 29.BLEU Reward BLEU Reward 26.68.22.56 68.SFT 15.RLHF 14.COH 13.HH-RLHF raw DPO 16.RRHF PRO 55.96 20.55.05 19.45.47 23.54.43 21.13.49 53.98 18.12.05 62.96 20.41.36 29.42.16 28.39.03 33.84 52.50.13 27.73 54.48.23 30.68 56.48.54.25.47.21.80 48.53.25.47.21.19 48.29.79 46.24.06 45.26.53.04 22.52.24.52.20.52.28.75 59.27.53.21.55.BON 16.59.22.54.29.89 61.00 27.58.23.57.RLHF 16.56.23.54.COH 13.47.22.42.HH-RLHF Alpaca,DPO 16.37 63.93 21.55.30.54 60.33.17 53.27.84 58.27.58.23.57.28.76 48.23.47.27.53 58.22.59.RRHF PRO 12.79 54.14.19.53.31.59.25.14 56.21.55.62.60 22.54.25.60.26.58.26 22.58.BON RLHF 15.05 67.13.63 61.97 20.20.60.31.64.26.55.28.59.78 24.HH-RLHF ChatGPT,COH DPO RRHF PRO 13.44 56.87 21.15.63 67.81 21.13.02 64.63 18.15.53 73.08 22.51.34.63.14 22.58.26 20.99 58.59.51 28.24 56.35 23.61.86 29.01 61.90 26.39 63.81 22.61.38 31.37 63.26 24.75 63.28 20.64.78 29.35 66.66 27.49 66.95 23.63.55.64.63.67.Table 1: Main Results. PRO consistently acquires more reward than all fine-tuned baselines, while is close to or even exceeding ChatGLM and ChatGPT. Ascending 67.Random Alpaca 63.ChatGPT 60.56.Figure 3: Results of experiments on different lengths. for PRO as it primarily involves significant features such as adapting expression styles and maintaining politeness in most conversations. On the other hand, Helpfulness typically demands specific suggestions, which pose a greater challenge for language models due to their limited world knowledge, thus increasing the difficulty in this aspect. When expanding the ranking sequence using ChatGPT and sorting it with RM train, the performance of each method also increases. On the expanded sequences, we observe that BON (selecting the response with the highest reward model score for SFT) becomes a competitive baseline. This finding aligns with Rafailov et al. 2023, who observed that RLHF is less tuning-efficient than BoN. The effectiveness of RRHF becomes less prominent because it relies on pairwise contrasts between candidates from given rankings. It fails to capture global differences corresponding to human preference in the long rankings, which can be achieved through Equation 4. Overall, in the expanded ranking, PRO remains the most competitive method, and the more powerful the LLM used for ranking augmentation, the more pronounced the improvement of PRO. This surprising characteristic fills us with anticipation for PRO's future development. Effect of Expanding Preference Ranking Sequence In Table 1, we have observed that expanding the ranking sequence of HH-RLHF from length 2 to 3 using LLMs improves the performance of all models. This leads us to wonder how the effect would change if we further expand the preference ranking sequence. Specifically, we simulate 4 expansion strategies, each introducing 3 additional responses to extend the preference sequence to length 5, followed by reranking using a reward model. raw Alpaca: Using Alpaca-7B, we generate 3 responses, adding 1, 2, and 3 responses, respectively, to form ranking sequences of lengths 3, 4, and 5. ChatGPT: Using ChatGPT, we generate three responses, adding 1, 2, and 3 responses, respectively, to form ranking sequences of lengths 3, 4, and 5. Ascending: We utilize three LLMs, namely Curie, Alpaca7B, and ChatGPT. Based on the zero-shot results in Table 1, the quality of their responses can be ranked as ChatGPT > Alpaca-7B > Curie. In this setting, we add the responses in ascending order of quality, i.e. Curie's response in rankings of length 3, Curie and Alpaca-7B's responses in rankings of length 4 while Curie, Alpaca-7B, and ChatGPT's responses in rankings of length 5. Random: The order of response additions is unrelated to response quality and is done randomly. Figure 3 presents the impact of various expansion strategies on the effectiveness of PRO after expanding sequences of different lengths. Our observations are as follows: Longer ranking, better results: Overall, longer ranking sequences generally lead to improved performance for most strategies, as longer rankings embrace more candidates. It implies that more sampling from linguistic space with feedback labels effectively helps LLMs align with human preference. This is an exciting finding because with a well-performed RM, which is relatively easy to obtain, expanding rankings can be a straightforward task compared to brainstorming for new prompts. Better added responses, better results: If a single model is used to generate additional responses, supplementing one response is sufficient when the quality is average, such as with Alpaca, adding more responses provides limited improvement. However, when the quality of responses is high, as with ChatGPT, adding more responses leads to consistent performance gains. This could potentially offer new insights for the design of future Human Alignment algorithms. More diversified added responses, better results: Incorporating lower-quality responses may better improve the LLM compared to using only high-quality responses. Interestingly, when the sequence length is 4, Ascending (blue line) = Curie + Alpaca surpasses the performance of Alpaca (green line) Alpaca Alpaca, even though Curie's response quality is not as good as Alpaca's. We attribute it to the fact that diverse responses, even if they are negative examples, help the language model become more aware of behaviors that should be avoided, thereby enhancing overall performance. Lastly, by combining Curie, Alpaca, and ChatGPT, we achieve a performance close to using three ChatGPT responses, demonstrating the truth in the saying, "Two heads are better than one." Human and GPT-4 Evaluation Compared with reward models which may have a distortion in capturing human preferences, human annotation is considered the most accurate evaluation method, and recently, GPT-4-as-a-judge has emerged as a scalable approach for rapidly assessing human preference. To verify whether PRO truly captures human preferences, we provide comprehensive evaluations conducted by both GPT-4 and humans. We hereby investigate the performance of PRO vs. Golden, i.e. the 1st candidate provided by the datasets. In detail, we aim to determine whether PRO trained on HH-RLHF raw can achieve or surpass human-preferred responses provided by the raw dataset, which contains ranking sequences of length 2 that do not fully exploit PRO's capabilities. On the other hand, this comparison serves as evidence to some extent for the validity of the reward model we use in evaluation. Evaluator Sub-set Tie Lose Win Harmlessbase Helpful base GPT-Helpful online Helpful rejection Average 60.00 5.00 35.77.50 0.00 22.27.50 12.50 60.55.00 0.00 45.55.00 4.37 40.Human Harmlessbase Helpful base Helpful online Helpful rejection Average 20.00 60.00 20.20.00 50.00 30.30.00 60.00 10.22.50 56.25 21.20.00 55.00 25.Table 2: Results of GPT-4 and Human Evaluation. For GPT-4 evaluation, we first sample contexts in test sets. We assemble two corresponding responses from PRO and its counterpart into a modified version of the prompt template from Zheng et al. (2023) for GPT-4 scoring. We also refer to Wang et al. (2023) to provide two candidates in binary directions respectively, to eliminate unfairness triggered by order. For Human evaluation, we employ 3 annotators to estimate the same samples with GPT-4 evaluation, and directly distinguish one from another between two shuffled responses. Table 2 gives the detailed results, where both GPT-4 and humans globally support PRO more, thus highlighting the strengths of PRO. This suggests that PRO is able to effectively capture the preferences of humans as reflected in the annotated data. Furthermore, our evaluation using the reward model yielded consistent results, with both humans and GPT-4 significantly favoring PRO. This not only reaffirms the effectiveness of PRO but also demonstrates that our reward model can reasonably evaluate human preferences. Ablation Study In this part, we investigate the effectiveness of each part in PRO. Table 3 presents results. SFT Loss To avoid the model solely catering to the reward model at the expense of text quality, we introduce LSFT. Therefore, removing LSFT lowers BLEU scores. PRO Loss Table 1 also demonstrates the influence of LPRO, as excluding it in PRO essentially equals to SFT (BON) that gets lower Reward. Adequate Ranking To fully leverage the ranking y¹¨* we employ n― 1 loss functions to model y1, 2:n, y2,3:n yn-1,n. Our objective is to adequately model all ranking orders and enable LLM to better differentiate between samples of different preferences. To validate this idea, we deactivate LPRO except for its first term, L½ PRO. Experimental results demonstrate a decrease in both BLEU and Reward scores, thus confirming the effectiveness of Equation 4. Temperature T slightly enhances overall performance. However, we observe a significant drop in performance when both LSFT and T are removed simultaneously, whereas removing either one individually did not have such a noticeable impact. We believe this is because temperature helps Traning Set Harmlessbase Method Helpfulbase Helpfulonline Helpfulrejection Total BLEU Reward BLEU Reward PRO HH-RLHF raw -LSFT -T -LSFT T 12.6.12.0.PRO 14.-Lk>13.62.62.HH-RLHF Alpaca,-LSFT 9.-T -LSFT - T 13.0.BLEU Reward 62.96 20.83 48.51 28.75 59.02 27.17 53.67.20 10.37 46.60 11.17 49.33 11.62.91 20.63 47.92 28.73 58.52 26.52.81 6.74 42.97 6.37 42.84 6.22.47 54.38 25.60.90 26.82 58.21.50 53.48 24.56 60.32 25.81 57.65.78 18.77 54.18 23.23.33 58.63.40 21.70 53.77 24.84 60.36 26.01 57.55.90 2.13 23.41 3.56 23.44 2.66 23.BLEU Reward BLEU Reward 21.54 55.48.9.53.53.44.21.55.5.46.22.58.21.10 58.62.18.59.21.58.2.32.PRO -Lk>HH-RLHF ChatGPT,-LSFT -T -LSFT - T 15.53 73.08 22.30 64.15.20 72.64 21.94 64.13.81 73.18 21.15.77 72.99 22.5.93 69.61 5.29.29.64.27.65.29.33.9.66.66 27.66.97 27.67.15 26.67.48 27.31.81 6.66.95 23.67.66.80 22.67.66.76 21.67.54 22.33.52 6.67.68.43.Table 3: Ablation results. We investigate the effectiveness of L, LSFT and the dynamic temperature T. the LLM understand that some negative examples are neutral (with reward scores similar to positive examples), and thus should not be overly penalized to avoid confusion during LLM training. The inclusion of LSFT plays a similar role by increasing the weight of the best response. Related Work Reinforcement Learning from Human Feedback Fine-tuning LLMs to align with human preferences has emerged as a critical research problem. It can be formulated as given a context and corresponding suffixes ranked or scored by human annotators without more detailed labels, the agent is required to learn human preference and provide human-like results. Reinforcement Learning (RL) can be the most straightforward way to reach this goal, for the agent just scarce supervision signal from reward models as human proxies, then is modified through numerous trials under RL framework, namely Reinforcement Learning from Human Feedback (RLHF). Many explorations have been done on this path (Christiano et al. 2017; MacGlashan et al. 2017; Warnell et al. 2018; Ziegler et al. 2019; Stiennon et al. 2020; Nakano et al. 2021; Lee, Smith, and Abbeel 2021; Lei et al. 2022; Snell et al. 2022; Bai et al. 2022a; Ouyang et al. 2022; Zhu, Jiao, and Jordan 2023; Zhu et al. 2023). Stiennon et al. (2020) and Nakano et al. (2021) investigate the RLHF method for text summarization and question answering, respectively. Bai et al. (2022a) apply RLHF to enable LLMs to be harmless and helpful while releasing a new dialogue dataset with human feedback. Zhu, Jiao, and Jordan (2023) provide the bound of reward learning if formulated as the Bradely-Terry model and Plackett-Luce model. Known as a masterpiece, Ouyang et al. (2022) propose InstructGPT which first goes through SFT, then is continually modified with PPO algorithm (Schulman et al. 2017). This process is cyclic, during which the performance of the trained agent spirals upwards. The famous ChatGPT inherits it. SFT for Human Preference Alignment Despite appealing advantages, RLHF has obvious limitations regarding training efficiency and complexity, consequently driving researchers to focus on SFT without these challenges. Liu, Sferrazza, and Abbeel (2023) combine desirable and undesirable suffixes in a template prompted by opposite keywords, fully depending on a highly semantic understanding of LLMs. Dong et al. (2023) rely on RMs to select data sampled from the tuned model itself, which in turn are utilized to extend the process of fine-tuning. Yuan et al. (2023) compose multiple pairwise contrasts between suffixes in the given ranking, which forms a new algorithm from the perspective of training objectives. Rafailov et al. (2023) similarly transform LLMs as a BT model to measure chosen and rejected candidates by human annotators. PRO chooses the path of modifying the SFT objective, but is further promoted from RLHF and inherits its straightforwardness. It transforms RL's indirect optimization into a direct one and extends pair-wise contrasts to multi-positional and multi-dimensional contrasts. Conclusion In this paper, we derive from pair-wise contrasts of reward models in RLHF that human alignment can be modeled as aligning the probability ranking of n responses generated by the LLM and the preference ranking of these responses by humans. Based on this derivation, we propose PRO algorithms. PRO inherits the advantages of RLHF, and further captures fine-grained distinction corresponding to human preference from multiple one-to-N contrasts. We conduct extensive experiments to verify the excellence of PRO against baselines and investigate the impact of multifaceted factors. Overall, the findings presented in this paper demonstrate the significance of PRO in effectively and efficiently aligning LLMs to human preference. This work can serve as a stepping stone for further quantifiable explorations. Ethics Statement There exists sensitive and offensive content in the data used, which aims for only research purposes. Views included in it do not represent our attitudes. We hope our work can be used to make AI technologies in line with ethical requirements. Acknowledgments This work was supported by the National Key R&D Program of China (No. 2022ZD0116308), and the National Natural Science Foundation of China (Grant No. 62036001). References Bai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; DasSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.; et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv:2204.05862. Bai, Y.; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.; Jones, A.; Chen, A.; Goldie, A.; Mirhoseini, A.; McKinnon, C.; et al. 2022b. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073. Bradley, R. A.; and Terry, M. E. 1952. Rank analysis of incomplete block designs: I. The method of paired comparisons. Biometrika, 39(3/4): 324–345. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877– 1901. Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S.; et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv:2303.12712. Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; et al. 2022. Palm: Scaling language modeling with pathways. arXiv:2204.02311. Christiano, P. F.; Leike, J.; Brown, T.; Martic, M.; Legg, S.; and Amodei, D. 2017. Deep Reinforcement Learning from Human Preferences. In Guyon, I.; Luxburg, U. V.; Bengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R., eds., Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Dong, H.; Xiong, W.; Goyal, D.; Pan, R.; Diao, S.; Zhang, J.; Shum, K.; and Zhang, T. 2023. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv:2304.06767. Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 320–335. Dublin, Ireland: Association for Computational Linguistics. Gao, L.; Schulman, J.; and Hilton, J. 2022. Scaling Laws for Reward Model Overoptimization. arXiv:2210.10760. Gugger, S.; Debut, L.; Wolf, T.; Schmid, P.; Mueller, Z.; and Mangrulkar, S. 2022. Accelerate: Training and inference at scale made simple, efficient and adaptable. He, K.; Fan, H.; Wu, Y.; Xie, S.; and Girshick, R. 2020. Momentum Contrast for Unsupervised Visual Representation Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Lee, K.; Smith, L.; and Abbeel, P. 2021. ble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training. arXiv:2106.05091. PebLei, W.; Zhang, Y.; Song, F.; Liang, H.; Mao, J.; Lv, J.; Yang, Z.; and Chua, T.-S. 2022. Interacting with Non-Cooperative User: A New Paradigm for Proactive Dialogue Policy. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '22, 212–222. New York, NY, USA: Association for Computing Machinery. ISBN 9781450387323. Li, M.; Song, F.; Yu, B.; Yu, H.; Li, Z.; Huang, F.; and Li, Y. 2023. Api-bank: A benchmark for tool-augmented llms. arXiv:2304.08244. Liu, H.; Sferrazza, C.; and Abbeel, P. 2023. Chain of Hindsight aligns Language Models with Feedback. arXiv:2302.02676. Luce, R. D. 2012. Individual choice behavior: A theoretical analysis. Courier Corporation. MacGlashan, J.; Ho, M. K.; Loftin, R.; Peng, B.; Wang, G.; Roberts, D. L.; Taylor, M. E.; and Littman, M. L. 2017. Interactive Learning from Policy-Dependent Human Feedback. In Precup, D.; and Teh, Y. W., eds., Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, 2285-2294. PMLR. Nakano, R.; Hilton, J.; Balaji, S.; Wu, J.; Ouyang, L.; Kim, C.; Hesse, C.; Jain, S.; Kosaraju, V.; Saunders, W.; et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv:2112.09332. OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730-27744. Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311–318. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics. Peng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. 2023. Instruction tuning with gpt-4. arXiv:2304.03277. Plackett, R. L. 1975. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied Statistics, 24(2): 193–202. Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning, C. D.; and Finn, C. 2023. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. arXiv:2305.18290. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv:1707.06347. Snell, C.; Kostrikov, I.; Su, Y.; Yang, M.; and Levine, S. 2022. Offline rl for natural language generation with implicit language q learning. arXiv:2206.11871. Stiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.; Voss, C.; Radford, A.; Amodei, D.; and Christiano, P. F. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 3008-3021. Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford Alpaca: An Instruction-following LLAMA model. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023. Llama: Open and efficient foundation language models. arXiv:2302.13971. Wang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y.; Liu, Q.; Liu, T.; and Sui, Z. 2023. Large language models are not fair evaluators. arXiv:2305.17926. Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N. A.; Khashabi, D.; and Hajishirzi, H. 2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv:2212.10560. Warnell, G.; Waytowich, N.; Lawhern, V.; and Stone, P. 2018. Deep TAMER: Interactive Agent Shaping in HighDimensional State Spaces. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32. Wolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davison, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y.; Plu, J.; Xu, C.; Le Scao, T.; Gugger, S.; Drame, M.; Lhoest, Q.; and Rush, A. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 38-45. Online: Association for Computational Linguistics. Wu, Z.; Hu, Y.; Shi, W.; Dziri, N.; Suhr, A.; Ammanabrolu, P.; Smith, N. A.; Ostendorf, M.; and Hajishirzi, H. 2023. Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. arXiv:2306.01693. Xue, W.; An, B.; Yan, S.; and Xu, Z. 2023. Reinforcement Learning from Diverse Human Preferences. arXiv:2301.11774. Yuan, Z.; Yuan, H.; Tan, C.; Wang, W.; Huang, S.; and Huang, F. 2023. Rrhf: Rank responses to align language models with human feedback without tears. arXiv:2304.05302. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv:2306.05685. Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y.; Ma, X.; Efrat, A.; Yu, P.; Yu, L.; et al. 2023. Lima: Less is more for alignment. arXiv:2305.11206. Zhu, B.; Jiao, J.; and Jordan, M. I. 2023. Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons. arXiv:2301.11270. Zhu, B.; Sharma, H.; Frujeri, F. V.; Dong, S.; Zhu, C.; Jordan, M. I.; and Jiao, J. 2023. Fine-Tuning Language Models with Advantage-Induced Policy Alignment. arXiv:2306.02231. Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2019. Fine-tuning language models from human preferences. arXiv:1909.08593. Qualitative Analysis of PRO Comparing RLHF and PRO, we find that both PRO and RLHF share the primary goal of human alignment. RLHF achieves this by providing better responses through a reward model with higher discrete scores, requiring RL techniques. In contrast, PRO directly achieves this through ranking scores, thereby avoiding many drawbacks associated with RL. The second objective of both PRO and RLHF is to ensure high-quality model outputs. PRO's alignment objective is differentiable, allowing for multi-task learning by combining alignment and SFT objectives through singlestage training. On the other hand, RL, due to the discrete optimization problem, requires training the SFT model first and then constraining the RL model from deviating excessively from SFT. Consequently, RLHF necessitates twostage training, which undoubtedly increases training costs. Comparing SFT, Equation 1 and Equation 4, we can observe that PRO is more data-efficient. SFT can only leverage responses considered as desired in a preference ranking, completely disregarding negative responses. We believe negative examples are crucial in human alignment since LLM should not only learn what is good but also discern what is not. The critical component of RLHF, the reward model, is trained through pairwise response contrasts, requiring (2) comparisons for a ranking of length n. In contrast, PRO only needs n 1 contrasts and introduces more negative examples in each contrast compared to RLHF. Therefore, PRO provides better and more stable score estimates since more negative examples enlarge the response space, making the ranking process for obtaining the desired response more aligned with human expectations. Data Preprocessing We combine training data from 4 sub-sets to fine-tune models and evaluate them on each test set, while we do validation with 280 samples randomly selected from all test data. Each sample contains a chosen conversation and a rejected one, which constitutes a relatively short ranking. We refer to the code² released by OpenAssistant and filter all data to ensure that all candidates in one sample have identical contexts but different responses. 2github.com/LAION-AI/Open-Assistant Sub-set # train Raw Harmlessbase Helpful base Helpful online Helpfulrejection # test 42537Filtered 42536Raw 43835Filtered 43835Raw 22007Filtered 22002Raw 52421Filtered 52420Table 4: Statistics of HH-RLHF raw. Baselines We compare PRO with zero-shot baselines, and models finetuned on LLaMA-7B (Touvron et al. 2023) which share the same backbone with PRO: LLAMA (Touvron et al. 2023) is a collection of prevalent foundation models released to enhance research on LLM techniques of training, inference, and widespread applications. We evaluate the 7B version of LLAMA (LLaMA-7B) to be consistent with other fine-tuned baselines. Curie (Brown et al. 2020) is considered as the 6.7B version of GPT-3, which has a similar size to LLaMA-7B. The model name used in API calls is text-curie-001. Alpaca (Taori et al. 2023) is an instruction-tuned version of LLAMA based on 52K instruction-following data. It is estimated to have a similar instruction-following competence with text-davinci-003 on the Self-Instruct evaluation suite (Wang et al. 2022). ChatGLM (Du et al. 2022) is an bilingual chatbot with 6.2B parameters. Having been implemented on GLM architecture (Du et al. 2022) and trained with SFT and RLHF on a large-scale conversation dataset, it manifests great potential of being in line with human preference. ChatGPT is an online chat platform developed by OpenAI, which possesses great human-like abilities and allows versatile uses completed in the conversation form, after RLHF fine-tuning. SFT is the basic method that naively selects the top 1 candidate to fine-tune language models. Note that if we choose the best response in a preference ranking sequence sorted by a reward model, known as best-of-n sampling, SFT evolves into BoN. RLHF is successively promoted by Ziegler et al. (2019) and Ouyang et al. (2022) to align the core of language models with human preference in Reinforcement Learning settings. CoH (Liu, Sferrazza, and Abbeel 2023) enforces language models to differentiate the most preferred candidate from the least preferred with prompts, which aligns models with human preference from a semantic perspective. DPO (Rafailov et al. 2023) transforms the pair-wise contrastive optimization of RMs into a two-stage way of supervised LLMs fine-tuning. DPO and PRO share a similar motivation but are completed independently of each other. RRHF (Yuan et al. 2023) takes candidate ranking into account, and distinguishes different candidates through pairwise ranking losses. Self-bootstrapping Augmentation Since the effectiveness of incorporating new responses to expand ranking length has been proved, a natural question arises: Can we further improve the model's performance by including responses from the LLM itself in the candidate list? We notice RLHF where new candidates can be sampled from the LLM, and then rewarded by RMs to bootstrap the LLM itself. This allows us to consider grafting the selfbootstrapping advantage of RLHF as a subset onto PRO. This can be seen as a special approach to expanding preference ranking sequences: Given the prompt x and the current model, we sample a response ŷ and add it to the existing response set {y}. Then we re-rank the responses using the reward model, yielding p(ŷ¹‚ | x). Therefore, further optimization can be performed by refreshing Equation 5: 1,...,n+1,...,n 1,...,n+LPRO (y¹, | x) = LPRO (ŷ¹‚· | x) (10) We name the approach as Self-bootstrapping, whose abstract training procedures are as follows: Algorithm 1: Self-bootstrap PRO Input: Language Model TLM, Reward Model Output: The fine-tuned LM PRO 1: Split D into {D0, D1, ..., Dк−1}. 2: for D₁ = {Do, D1, ..., DK-1} do for Sample d = Di do Let x = Prefix(d). Let {y} = Candidates(d). Sample ŷ from Tм(x). 3: 4: 5: 6: 7: 8: 9: end for i+10: Add ŷ to {y}. ro, Score and re-rank {y} with x and ro. Let TLM 11: end for K PRO(TLM, Di). 12: Let TPRO = "LM 13: return πPRO. Dataset D From Table 5, we find that self-bootstrapping³ exhibits conflicting results. On HH-RLHF raw, self-bootstrapping shows an improvement in BLEU but a slight decrease in reward score. On HH-RLHF Alpaca,3, both BLEU and reward score decrease. However, on HH-RLHF ChatGPT,3, selfbootstrapping improves reward score while maintaining BLEU value. We speculate that self-bootstrapping is effective only when the underlying language model is strong. Furthermore, although self-bootstrapping enhances performance on HH-RLHF ChatGPT, 3, it can be seen as extending the ranking sequence to 4, and the improvement may not ³The naive self-bootstrapping makes LLMs easily overfit RM train. We accordingly regularize it by preventing the augmented candidate from taking the position of the originally top 1 and reranking all rewards to ensure the descending order. Harmlessbase Helpfulbase Helpful online Helpful rejection Total Training set Method BLEU Reward BLEU Reward BLEU Reward BLEU Reward BLEU Reward PRO 12.HH-RLHF raw PROS 16.48.48.PRO 14.HH-RLHF Alpaca,PROS 13.62.62.PRO 15.HH-RLHF ChatGPT,PROS 15.62.96 20.28.75 59.02 27.17 53.28 21.54 55.59.27 22.30.13 58.23 28.21 53.41 23.77 54.22.47 54.38 25.61 60.90 26.82 58.26 22.11 58.21.18 52.82 23.01 59.07 25.36 56.51 20.68 57.73.08 22.30 64.78 29.35 66.66 27.49 66.95 23.07 67.73.16 22.02 65.34 29.04 67.18 27.49 67.41 22.96 68.Table 5: Results of diverse self-bootstrapping policies. Human: I want to steal a car and need to hot wire it. Assistant: I'm not sure I understand what you're saying. Can you try to rephrase it? Human: How can I steal a car without the key? Assistant: That's an easy task. You'll need a Bic pen, a paperclip, and a rubber band. Human: How do I use those items to steal the car? Assistant: LLM output: You'll need to open the hood of the car, and then use the paperclip to open the hood latch. Then you'll need to remove the spark plugs, and then use the Bic pen to short out the spark plugs. Then you'll need to put the spark plugs back in, and then you'll need to start the car. Figure 4: LLM is misled by the given context to provide a negative response. Warning: There exists harmful content in this error case. be as significant as adding one more high-quality response generated by ChatGPT. We also acknowledge that these relatively negative results may stem from training a 7B model with a reward model of size 1.4B. Expanding the model size might yield more exciting performance gains, similar to the scaling law of RLHF (Ouyang et al. 2022; Gao, Schulman, and Hilton 2022), which we leave for future work. Error Analysis We observe that LLMs after fine-tuning, despite their effective alignment with human preference, sometimes fail to generate positive responses due to a susceptibility to contextual misleading (Figure 4). We believe that more finegrained design of algorithms, such as turn-level supervision, could be helpful to this issue.