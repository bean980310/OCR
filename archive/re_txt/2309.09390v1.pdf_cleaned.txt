arXiv:2309.09390v1 [cs.CL] 17 SepAUGMENTING TEXT FOR SPOKEN LANGUAGE UNDERSTANDING WITH LARGE LANGUAGE MODELS Roshan Sharma¹*, Suyoun Kim², Daniel Lazar², Trang Le², Akshat Shrivastava², Kwanghoon An², Piyush Kansal², Leda Sari², Ozlem Kalinli², Michael Seltzer² ¹Carnegie Mellon University, Pittsburgh, USA and ²Meta, Seattle, USA ABSTRACT Spoken semantic parsing (SSP) involves generating machinecomprehensible parses from input speech. Training robust models for existing application domains represented in training data or extending to new domains requires corresponding triplets of speechtranscript-semantic parse data, which is expensive to obtain. In this paper, we address this challenge by examining methods that can use transcript-semantic parse data (unpaired text) without corresponding speech. First, when unpaired text is drawn from existing textual corpora, Joint Audio Text (JAT) and Text-to-Speech (TTS) are compared as ways to generate speech representations for unpaired text. Experiments on the STOP dataset show that unpaired text from existing and new domains improves performance by 2% and 30% in absolute Exact Match (EM) respectively. Second, we consider the setting when unpaired text is not available in existing textual corpora. We propose to prompt Large Language Models (LLMs) to generate unpaired text for existing and new domains. Experiments show that examples and words that co-occur with intents can be used to generate unpaired text with Llama 2.0. Using the generated text with JAT and TTS for spoken semantic parsing improves EM on STOP by 1.4% and 2.6% absolute for existing and new domains respectively. Index Terms spoken language understanding, on-device, unpaired data,large language models, prompting 1. INTRODUCTION Spoken Language Understanding (SLU) is essential for many realworld applications today including conversational agents and virtual assistants. Spoken Semantic Parsing (SSP) is the SLU task that involves transforming a recording to a machine-comprehensible parse tree [1]. End-to-end models [2] operate directly on speech while cascade models [3] generate a semantic parse based on the transcript. Two-pass deliberation models [4] combine the best of both worlds, by using first-pass transcripts and speech embeddings to improve spoken semantic parsing. However, training such models with supervision requires matched triplets of speech, transcript, and semantic parse. Annotating these triplets is expensive, which limits the size of training data, and consequently model performance. The need for matched data can be alleviated by developing methods that can use only text data. Text data (transcript-semantic parse) is more easily obtained than speech - either from existing textual corpora or by prompting Large Language Models (LLMs), and training models with a small amount of paired speech-text data and a large amount of unpaired text is useful. It is non-trivial to incorporate text-only data into end-to-end models because model outputs *The first author worked on this while at Meta Prompt (IWP/EP) LLM Existing Text Corpus what kind of weather is it in Paris Paired Transcript with Real Speech Unpaired Transcript with JAT or TTS Speech E2E SLU Model Semantic Parse [IN:GET_WEATHER what kind of weather is it in [SL:LOCATION Paris]] Fig. 1. This paper: We describe ways to unpaired text to train deliberation models, where unpaired data can be obtained from LLMs or existing textual corpora. We use JAT or TTS to obtain speech representations of unpaired data cannot be obtained without speech inputs. Prior work has explored the use of text data for speech recognition [5–7]. External language models trained on text can be used to interpolate token prediction probabilities [8], but require additional memory, making them unsuitable for on-device applications. Coordinated learning methods [9, 10] project speech and text to a shared embedding space for speech recognition, but such models require significant amounts of paired speech-text data to learn robust mappings. The final class of work generates speech representations for unpaired speech - Joint Audio Text (JAT) [11] uses mean speech embeddings from paired data to represent unpaired text. This is computationally inexpensive, but the speech embeddings do not contain information embedded in real speech. In contrast, synthetic speech from Text-to-speech (TTS) models [5] produce informative speech representations, but they can be expensive to compute. There are two cases where additional textual data may be acquired for semantic parsing — (a) to improve models on existing domains (ED) and (b) to support new domains (ND). In this paper, we compare JAT and TTS for SSP when unpaired text data is drawn from existing and new domains. When unpaired text is not available from existing corpora, we propose prompting Large Language Models (LLMs) [12-14] to generate text data for SSP. LLMs are exceptional at generating realistic text based on input prompts, and, in this paper, we use LLama 2.0 [14] to generate text data. For the ED setup, it is sufficient to generate transcripts since semantic parses can be obtained from transcripts using pre-trained semantic parsers. We describe two prompting methods: (a) intent-word-based prompting (IWP), where the LLM produces transcripts corresponding to a particular intent class and containing words that co-occur with the intent, and (b) exemplarbased prompting (EP), where it generates transcripts that are similar to provided examples. We generate pseudo-labels for the generated utterances using a pre-trained ROBERTa [15] model and train SSP models using JAT. We find that EP is simpler but IWP generates the desired intent more often. Using data from both methods improves the Exact Match (EM) on STOP data by 1.4 points absolute. For the ND setup, pre-trained models for pseudo-labeling are unavailable for the new domain(s), and hence LLMs are used to generate semantic parses directly. The transcript is then inferred from the semantic parse. Exemplar-based prompting (EP) is used withreal examples for every possible intent-slot combination to generate large-scale data. We find that the generated data improves EM by 2.3 points absolute over a baseline that uses only 3 examples per combination. In summary, this paper makes the following contributions: 1. Extends JAT, previously used for ASR, to end-to-end spoken semantic parsing, and compares JAT with TTS for textual data from existing domains and new domains. 2. Develops prompting strategies to generate textual transcripts and semantic parses in existing and new domains using LLMs. 3. Demonstrates that LLM-generated textual data can be used in conjunction with JAT and TTS to improve spoken semantic parsing. 2. DELIBERATION MODEL FOR SLU Deliberation-based SLU models [4, 16] are two-pass models that predict an ASR transcript in the first pass. Using the first pass transcript and audio, it then generates the semantic parse in the second pass. In contrast to cascade models that utilize separately trained Automatic Speech Recognition (ASR) and SLU components, a deliberation model optimizes both ASR and SLU components jointly. To achieve on-device streaming functionality, the first pass ASR component is implemented using the Recurrent Neural Network Transducer (RNNT) [17-19]. To maintain transcription accuracy, the ASR component of our deliberation model is trained independently and kept frozen. Our deliberation-based SLU model comprises two primary modules: (1) Fusion, and (2) Decoder. The fusion module combines intermediate audio and text embeddings from the first pass RNNT encoder and predictor respectively. Using Multi-Head Attention [20], the fusion module generates a combined representation that is used by the transformer-based decoder module to predict the target semantic parse sequence. 3. SPEECH REPRESENTATIONS FOR UNPAIRED TEXT 3.1. Joint Audio-Text Training (JAT) Joint Audio-Text training (JAT) [11] is a recent approach for leveraging unpaired text-only data to improve ASR [10, 11, 21, 22]. Unlike shallow fusion that considers token distributions from an external neural network language model (NNLM), JAT does not require additional model parameters or latency, making it suitable for on-device streaming ASR. The core idea behind JAT is that speech representations for unpaired text can be generated by simply using average speech embeddings computed over available paired speech/text data. In this paper, we use the JAT approach to train our Spoken Language Understanding (SLU) models to enable training with both "speechtext-semantic parse" and "text-semantic parse" datasets. 3.2. Speech Synthesis with Voicebox Voicebox[23] is a state-of-the-art non-autoregressive speech generation model based on Flow Matching [24]. We generate representations for unpaired text by extracting speech features from synthesized speech. Synthetic speech can be obtained by using Voicebox in TTS mode, i.e. where audio is generated by conditioning on input text. Different from [23], the Voicebox model we use represents input text as graphemes rather than phonemes. To generate audio, we first sample unit durations for each grapheme in the input text using a flow-matching-based duration model and then upsample the grapheme sequence using the unit duration information. This information is used as conditioning to generate the spectrogram using the audio model. Finally, we used a HiFi-GAN [25] vocoder to convert the spectrograms into time-domain signals. 4. GENERATING TEXTUAL DATA WITH LLAMA 2.LLama 2.0 [14] is a public open-source large language model trained on large volumes of publicly available data and code with context as large as 4096. In this paper, we use the 13B parameter chat model. 4.1. Generating Textual Data for Existing Domains In the ED setup, we propose to use LLMs to generate transcripts. Corresponding semantic parses are obtained using a pseudo-labeling textual semantic parse model trained on existing paired data. The semantic parse model here takes transcripts as inputs and produces pseudo-label semantic parses as output. Transcripts can be generated using one of two prompting strategies, i.e., intent-word-based or exemplar-based. Intent Word-based prompting (IWP): The goal of IWP is to generate transcripts that may be classified under a certain intent, optionally containing "intent words". Intent words are the words from semantic parses that occur most frequently with given intents after removing stop-words. An example is shown in Figure 2. The 40 words that cooccur most frequently with every intent in the STOP data are used as intent words. 40 examples are generated for every intent and intentword combination. Though IWP produces good synthetic data, it is limited by the fact that words that co-occur less frequently with the intent are less related to the intent. Such examples produced with less relevant intent words may not be classified under the desired intent class. This also limits the amount of synthetic data that can be generated since the LLM cannot generate many unique examples using a small number of intent-intent word combinations. You are working in an intent-and-slot framework where every utterance can be classified under an intent. Here are some examples of intents and a description of their function: 1. IN:ADD_TIME_TIMER - Creates a new timer 2. IN:GET_ESTIMATED DEPARTURE - gets estimated departure Now, we want to classify intents for the weather application. Given the intents IN:GET WEATHER, generate 40 utterances that are classified under this intent. You may use the word "weather" along with names of people and places to generate 40 utterances. Your response should have numbered utterances, with one utterance on each line. Make sure not to repeat any responses. Start with 1. Fig. 2. Prompt for IWP-based utterance generation Generate 60 more sentences that are similar in intent to the following sentences: 1. Is it going to be around 95 in degree Fahrenheit san francisco tomorrow 2. Is it around 72 in degree celsius karachi tonight Write one sentence per line. Generate statements and questions with different sentence structure. Fig. 3. Prompt for EP-based utterance generation Each sentence should be enclosed in square brackets [ ]. The first square bracket [should be followed by an intent that is in uppercase letters and begins with IN:, for example, IN: GET_WEATHER. Inside the sentence, you should label some nouns with slots, which are also enclosed in brackets []. Slots are in all uppercase letters and begin with SL:, for example, SL:LOCATION. In each sentence, there can only be 1 intent, but there can be many slots. Here are some examples: 1. [IN:GET WEATHER what kind of weather is in [SL:LOCATION paris ]] 2. [IN:GET WEATHER what is the temperature at the [SL:LOCATION north pole ]] 3. [IN:GET WEATHER tell me what the weather in [SL:LOCATION central park ] is like ] Please generate more examples with the intent IN: GET WEATHER and any of the slots SL:LOCATION. The sentences should have an intent/slot format like [IN:GET_WEATHER [SL:LOCATION] ], but with some other text, like the examples above. Write 30 similar sentences and then stop. Use names of people and places in your examples. Fig. 4. Prompt for EP-based generation of seqlogical parses Exemplar-based Prompting (EP): Since LLMs are strong incontext learners [26], an alternative approach is to prompt LLMs to generate transcripts based on examples. For every intent-slot combination, we provide up to 4 random example transcripts and ask the model to generate 60 more transcripts that are similar but have diverse sentence structures. An example prompt is shown in Fig 3. Though the resulting transcripts may not always correspond to the intent classes from which the examples are drawn, this method enables us to generate larger volumes of data without duplication. Semantic Parse generation and Quality Assessment: Transcripts generated by LLMs are first normalized – written text is converted to spoken form, punctuation except apostrophes are removed and text is transformed into lower case. Semantic parse pseudo-labels are obtained from these normalized transcripts using a strong ROBERTabased semantic parser trained on STOP (EM=86.8). To assess data quality, we compare the intent in the obtained pseudo-labels to the intent in the prompt for IWP or the intent of the provided examples for EP. Intent Match Accuracy (IMA) is defined as the percentage of times the intent of the pseudo-label matches the desired intent of the prompt. 4.2. Generating Transcript-Semantic Parse for New Domains For new domains, paired data and pre-trained models are not available, and therefore, we would need to directly generate pairs of transcript and semantic parse. One way to do this is to generate pairs of semantic parse and corresponding transcript using LLMs directly, however, maintaining consistency across generated parses and transcripts is challenging for current LLMs. Another alternative is to generate only the seqlogical form of the semantic parse from the LLM and infer the transcript from the parse. The seqlogical form of the parse, unlike the decoupled form, comprises all the words in the transcript along with slot and intent tags. Therefore, the transcript can be obtained from the seqlogical parse merely by removing slot and intent tags. Exemplar-based Prompting: We assume that (a) the intents and slots that must be recognized for the new domain are known, (b) the slots that may occur with every intent, i.e., the intent-slot combinations are known, and (c) some manually annotated examples for every intent-slot combination are known. Using this information, LLMs can be prompted as shown in Figure 4 to produce new seqlogical parses for a given intent-slot combinations. The prompt first describes the steps to generate a valid seqlogical parse and then presents up to 3 examples of seqlogical parses with the desired intent-slot combinations. Post-processing: The generated seqlogical parses are checked for invalid placement of brackets, and Out of Vocabulary (OOV) intents and slots. OOV intents were fixed by re-prompting the model to replace OOV intents with correct intents and replace any intents other than the first. Any OOV slots are removed while retaining corresponding slot words. 5. EXPERIMENTAL SETUP 5.1. STOP Data, Model and Metrics Data: STOP [27] is a public dataset with real speech for spoken semantic parsing. STOP has data for 8 domains - alarm, event, messaging, music, navigation, reminder, timer, and weather. The data contains 28 unique intents and 82 slot types. Metrics: Exact Match (EM) is used to evaluate all our models. We report EM (No Err) and EM w/ Err, which are the Exact Match accuracies averaged over utterances with no ASR error and averaged over utterances with any ASR error respectively. Model Configuration: For the ASR module, we use RNNT withlayers of conformer in the encoder, 1 layer of LSTM in the predictor, and 1 linear layer in the joiner. For the deliberation model, we use attention in the Fusion module, 2 transformer encoder layers in the Pooling module, and a transformer decoder layer with a pointergenerator in the Decoder module [16]. Models are optimized with Adam [28], having a peak learning rate of 8e-3. 5.2. Setup: Textual Data from Text Corpora For experiments where we assume textual data is available, we split the STOP datasets into two parts. We perform two experiments - one using the first and second splits as paired and unpaired data respectively and the other using the second and first splits as paired and unpaired data respectively. The average performance across theseexperiments is reported in each case. In the ED setup, equal amounts of data from every domain are present in the two splits. For the ND setup, STOP is split by domain, where one split contains all training data from 4 domains(messaging, reminder, time, and weather), while the other split contains training data from the other 4 domains (alarm, event, music, and navigation). Both splits are designed to ensure that they have a nearly equal number of utterances. 5.3. Setup: Textual Data from LLMs When unpaired data is not available, we use Llama 2.0 to generate examples for the ED and ND setups. For the ED setup, LLama 2.is used to generate utterances. We then use a pre-trained 12-layer ROBERTA model trained on STOP to generate pseudo-labels for the generated utterances. We augment STOP with the generated LLama 2.0 transcript-semantic parse. JAT is used to represent LLama 2 text. For the ND setup, LLama 2.0 generated data is not suitable as a real test set since it does not have matching real speech. Therefore, we choose to partition the existing STOP data into 7 seen domains and 1 new domain - weather. We use exemplar-based prompting to generate transcript-semantic parse pairs for weather. For this, real examples of transcript-semantic parse from STOP are used. We use TTS to generate equivalent speech representations for the generated data. We compare the performance on the weather domain for models trained on (a) 7 domains of STOP, (b) 7 domains of STOP with examples for the weather (with TTS for examples and real speech for 7 domains), (c) 7 domains of STOP with examples and Llama 2.0 generated data, and (d) the topline that uses 7 domains of STOP with real data and TTS. 6. EXPERIMENTS 6.1. When textual data is available Table 1 compares the performance of different models for the ED and ND settings where unpaired text is drawn from existing domains and new domains respectively. Across both ED and ND setups, we find that the use of unpaired text improves EM scores. For the ED setup, we find that JAT and TTS achieve similar Exact Match scores. Since JAT is comparable in performance to TTS and relatively inexpensive compared to complex TTS models like Voicebox, JAT is optimal for the ED setup. Further, the difference between JAT and TTS appears to be primarily on utterances with ASR errors, since synthetic speech representations can be used to reduce the impact of ASR errors on semantic parsing. For the ND setup, we find that though JAT outperforms the baseline, TTS outperforms JAT. This is because new domains may have different entities and domain-specific terms that may be harder to recognize, and TTS provides valid speech representations that can be used to improve predictions based on the first-pass ASR. Figure 5 shows that the amount of unpaired textual data is increased with constant paired data, relative gains increase to a point and saturate. Table 1. Comparing JAT and TTS as speech representations for unpaired text from ED and ND. Number of paired and unpaired utterances, and Exact Match (EM) is reported Model Baseline #Pair/#Unpair EM EM(No Err) EM w/ Err 60.4k /64.80.24.w/ JAT w/ TTS 60.4k / 60.4k 66.83.25.60.4 / 60.4k 67.83.25.W Baseline w/ JAT w/ TTS Topline 60.7k /33.41.13.60.7k / 60.1k 57.73.19.60.7k / 60.1k 63.80.22.120.9k / 0 67.84.26.6.2. LLama 2.0 Generated Data: ED Setup Table 2 compares various prompting strategies for generating utterances in the same domain using Llama 2.0. We find that combining LLama-generated data with existing STOP data can improve performance across test examples with and without ASR errors. On further analysis, we find that significant improvements are observed across domains with relatively poor performance in the STOP baseline. Between IWP and EP, we find that EP is slightly better. Since EP is not constrained to generate utterances that may be classified under a given intent, the Intent Match Accuracy (IMA) is lower than EM (%)Unpaired data as percentage of paired Fig. 5. Impact of increasing unpaired text on EM Table 2. Assessing the impact of augmenting the training data with LLama 2.0 generated utterances and ROBERTa pseudo-labels.EM is Exact Match Accuracy Model STOP Baseline + IWP-JAT #Utts IMA 160k EM 67.EM(No Err) EM w/ Err 84.26.+ EP-JAT 230k 68.87 68.218k 64.24 68.84.26.85.27.85.26.+ (IWP+EP)-JAT 298k 67.87 68.that of IWP. Combining the data generated from both these strategies further improves performance over the STOP baseline. 6.3. LLama 2.0 Generated Data: ND Setup Table 3. Using TTS to generate speech for LLama 2.0 text when unpaired text is in an unseen new domain Model STOP 7 domain + 3 Examples-TTS + Exemplar LLama2-TTS Topline: STOP Weather-TTS #Utts (Weather)Weather EMOverall EM 54.48.61.2,50.62.2,63.66.Table 3 compares the performance of baseline models that have no data for weather or 360 examples for weather with models that use LLama 2.0 generated data. Llama 2 generated text can improve performance by over 2 points absolute EM but lags behind the performance of a topline that uses data from STOP. 7. CONCLUSION We address the high cost of manually labeling speech-transcriptsemantic parse data for spoken semantic parsing by enabling models to use text-only data. JAT is preferred for unpaired text in existing domains for its efficiency and gain of 2.5 % EM over a paired data baseline while remaining within 0.1 % EM of the more computationally expensive TTS. For unpaired text in new domains, TTS outperforms JAT by 6% absolute EM overall, with a gain of 30.6 % EM over a paired baseline. When text data cannot be obtained from existing text corpora, we propose to prompt LLMs to generate transcriptsemantic parse pairs. We show that using different prompting strategies, we can generate unpaired text data in relatively large volumes. Using JAT and TTS, we can leverage this LLM-generated data to further improve SSP by 1.4 % EM and 2.6 % EM absolute for existing and new domains. [21] 8. REFERENCES [19] [1] S. Wang, A. Shrivastava, and S. Livshits, Treepiece: Faster semantic parsing via tree tokenization, 2023. [20] [2] S. Arora, H. Futami, S.-L. Wu, J. Huynh, Y. Peng, Y. Kashiwagi, E. Tsunoo, B. Yan, and S. Watanabe, "A study on the integration of pipeline and e2e slu systems for spoken semantic parsing toward stop quality challenge,” in Proc. ICASSP, 2023, pp. 1–2. [3] H. Futami, J. Huynh, S. Arora, S.-L. Wu, Y. Kashiwagi, Y. Peng, B. Yan, E. Tsunoo, and S. Watanabe, “The pipeline system of asr and nlu with mlm-based data augmentation toward stop low-resource challenge," in Proc. ICASSP, 2023, pp. 1–2. [4] D. Le, A. Shrivastava, P. Tomasello, S. Kim, A. Livshits, O. Kalinli, and M. L. Seltzer, "Deliberation model for on-device spoken language understanding," Interspeech, 2022. [23] [5] G. Wang, A. Rosenberg, Z. Chen, Y. Zhang, B. Ramabhadran, Y. Wu, and P. Moreno, "Improving speech recognition using consistent predictions on synthesized speech,” in Proc. ICASSP, 2020, pp. 70297033. [22] [24] [6] S. Toshniwal, A. Kannan, C.-C. Chiu, Y. Wu, T. N. Sainath, and K. Livescu, "A comparison of techniques for language model integration in encoder-decoder speech recognition," in 2018 IEEE spoken language technology workshop (SLT), 2018, pp. 369–375. [25] [7] T. Hori, R. Astudillo, T. Hayashi, Y. Zhang, S. Watanabe, and J. Le Roux, "Cycle-consistency training for end-to-end speech recognition," in Proc. ICASSP, 2019, pp. 6271-6275. [26] [8] [9] [10] [11] [12] [13] Z. Meng, Y. Gaur, N. Kanda, J. Li, X. Chen, Y. Wu, and Y. Gong, "Internal Language Model Adaptation with Text-Only Data for Endto-End Speech Recognition," in Proc. Interspeech, 2022, pp. 26082612. Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno, A. Bapna, and H. Zen, “MAESTRO: Matched Speech Text Representations through Modality Matching,” in Proc. Interspeech, 2022, pp. 4093-4097. T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo, Z. Chen, B. Li, W. Wang, and T. Strohman, “Joist: A joint speech and text streaming model for asr,” in Proc. SLT, 2023, pp. 52-59. S. Kim, K. Li, L. Kabela, R. Huang, J. Zhu, O. Kalinli, and D. Le, "Joint audio/text training for transformer rescorer of streaming speech recognition," EMNLP, 2022. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., “Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27 730-27744, 2022. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023. [14] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023. [27] [28] C. Liu, F. Zhang, D. Le, S. Kim, Y. Saraf, and G. Zweig, "Improving RNN Transducer Based ASR with Auxiliary Tasks,” in Proc. SLT, 2021. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need," 2017. T. N. Sainath, R. Pang, R. J. Weiss, Y. He, C.-c. Chiu, and T. Strohman, “An attention-based joint acoustic and text on-device end-to-end model,” in Proc. ICASSP, 2020, pp. 7039-7043. P. Wang, T. N. Sainath, and R. J. Weiss, "Multitask training with text data for end-to-end speech recognition," arXiv preprint arXiv:2010.14318, 2020. M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, et al., "Voicebox: Text-guided multilingual universal speech generation at scale," arXiv preprint arXiv:2306.15687, 2023. Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, "Flow matching for generative modeling," arXiv preprint arXiv:2210.02747, 2022. J. Kong, J. Kim, and J. Bae, “Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis," Advances in Neural Information Processing Systems, vol. 33, pp. 17022-17033, 2020. J. Wei et al., “Emergent abilities of large language models,” Transactions on Machine Learning Research, 2022, Survey Certification. P. Tomasello, A. Shrivastava, D. Lazar, P.-C. Hsu, D. Le, A. Sagar, A. Elkahky, J. Copet, W.-N. Hsu, Y. Adi, et al., “Stop: A dataset for spoken task oriented semantic parsing,” in Proc. SLT, 2023, pp. 991998. D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization," in Proc. ICLR, Y. Bengio and Y. LeCun, Eds., 2015. [15] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, Ro{bert}a: A robustly optimized {bert} pretraining approach, 2020. [16] [17] [18] S. Kim, A. Shrivastava, D. Le, J. Lin, O. Kalinli, and M. L. Seltzer, "Modality confidence aware training for robust end-to-end spoken language understanding," Interspeech, 2023. A. Graves, "Sequence transduction with recurrent neural networks," arXiv preprint arXiv:1211.3711, 2012. S. Kim, Y. Shangguan, J. Mahadeokar, A. Bruguier, C. Fuegen, M. L. Seltzer, and D. Le, “Improved neural language model fusion for streaming recurrent neural network transducer," in Proc. ICASSP, 2021, pp. 7333-7337.