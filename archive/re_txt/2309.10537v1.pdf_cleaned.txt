arXiv:2309.10537v1 [eess.AS] 19 SepFOLEYGEN: VISUALLY-GUIDED AUDIO GENERATION Xinhao Mei¹,2*, Varun Nagaraja¹, Gael Le Lan¹, Zhaoheng Ni¹, Ernie Chang¹, Yangyang Shi¹, Vikas Chandra¹ 1Meta AI, USA, 2 University of Surrey, Guildford, UK ABSTRACT Recent advancements in audio generation have been spurred by the evolution of large-scale deep learning models and expansive datasets. However, the task of video-to-audio (V2A) generation continues to be a challenge, principally because of the intricate relationship between the high-dimensional visual and auditory data, and the challenges associated with temporal synchronization. In this study, we introduce FoleyGen, an open-domain V2A generation system built on a language modeling paradigm. FoleyGen leverages an off-the-shelf neural audio codec for bidirectional conversion between waveforms and discrete tokens. The generation of audio tokens is facilitated by a single Transformer model, which is conditioned on visual features extracted from a visual encoder. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To address this, we explore three novel visual attention mechanisms. We further undertake an exhaustive evaluation of multiple visual encoders, each pretrained on either single-modal or multi-modal tasks. The experimental results on VGGSound dataset show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations. Index Terms― Sound generation, audio-visual learning, video-to-audio generation, multimodal learning 1. INTRODUCTION genRecent years have seen remarkable breakthroughs in audio eration, powered predominantly by the evolution of large-scale deep learning models and datasets. Despite great achievements in text-to-audio [1, 2] and text-to-music [3, 4] generation, video-to-audio (V2A) generation lags behind, standing as a promising yet under-explored area due to its inherent challenges. Video-to-audio generation is the task of generating congruent soundscapes for a given visual signal, which requires parsing visual data, identifying sound-emitting objects, and then crafting corresponding sounds. V2A models are useful in various applications, such as generating sound for movies as a computational Foley artist, enhancing immersive experiences in virtual reality applications, and assisting visually impaired individuals for better spatial awareness. * Work done during an internship at Meta. Generated Waveform Video Frames Encodec Decoder Visual Encoder Pretrained & Frozen Transformer Decoder Encodec Encoder Reference Waveform Fig. 1. Overview of the FoleyGen system. The dashed-line block shows the EnCodec encoder for converting waveforms into discrete tokens, utilized only during training. Achieving accurate and realistic V2A generation poses several challenges. First, the simultaneous interpretation of both visual and auditory data is intricate due to their respective high-dimensional natures. Second, real-world videos often contain visually irrelevant sounds where the objects emitting sound are absent from the visible frames. This discrepancy makes the generation of temporally synchronized audio extremely challenging. Finally, a single object can emit a diverse range of sounds depending on its interaction with varying environments, further complicating this task. Initial efforts in V2A generation has predominantly focused on constrained visual contexts and a limited set of sound classes to simplify the problem [5, 6, 7]. Such approaches commonly utilized class-aware strategies [6] or even trained separate models for distinct sound categories [7, 8]. Consequently, these methods fail to generalize to open-domain videos. Recent advancements, however, indicate a rising interest in open-domain, visually guided audio generation. SpecVQGAN [9] and IM2WAV [10] both employ a language modeling method, leveraging the Transformer model to capture the joint distribution of visual features and discrete audio tokens encoded by vector-quantized variational autoencoder (VQ-VAE). In SpecVQGAN, the VQ-VAE operates specifically on spectrograms and subsequently employs a neural vocoder to convert generated spectrograms back into waveforms. In contrast, IM2WAV directly operates on waveforms, partitioning the VQ-VAE's latent space into two levels and utilizing dual Transformer models to model their respective distributions. Additionally, Diff-Foley [11] introduces a latent diffusion method conditioned on contrastive audio-visual pretraining (CAVP) representations. Inspired by the pioneering work of AudioGen [2] and MusicGen [4], we introduce FoleyGen, a video-to-audio generation framework that adopts a language modeling paradigm. An overview of FoleyGen is provided in Figure 1. Specifically, our system encompasses three major components: a neural audio codec-EnCodec [12] for bidirectional conversion between audio and discrete tokens, a visual encoder for extracting visual features, and a Transformer model responsible for generating audio tokens conditioned on the visual context. Unlike SpecVQGAN [9], the introduction of EnCodec provides better reconstruction quality and alleviates fidelity loss that often occurs during the spectrogram-to-waveform conversion process. Additionally, it eliminates the need for deploying multiple Transformer models IM2WAV [10]. A prevalent problem in V2A generation is the misalignment of generated audio with the visible actions in the video. To enhance the temporal alignment between visible actions and corresponding audio events, we propose and explore three different visual attention mechanisms. Furthermore, we conduct an exhaustive evaluation of various visual encoders, pretrained on both single-modal and multi-modal tasks. The experimental results show that our proposed FoleyGen outperforms previous systems across all objective metrics and human evaluations. 2.1. Neural Audio Codec Modeling the distribution of time-domain waveforms presents significant challenges and computational inefficiencies, primarily due to their high-dimensional and lengthy characteristics. In audio generation systems, autoencoders are commonly utilized to encode audio waveforms into a latent space, which can be either continuous [1] or discrete [2]. Inspired by AudioLM [13] and AudioGen [2], we adopt EnCodec, a state-of-the-art neural audio codec [12], for our experiments. EnCodec comprises an encoder that compresses audio waveforms into latent vectors, a residual vector quantizer (RVQ) for converting these latent vectors into discrete tokens, and a symmetric decoder that reconverts these tokens back into audio waveforms. Given an audio clip a € Rt×fs, where t is the duration and f¸ is the sampling rate, the encoder first compresses a into a latent representation z = RL×d. Here, d is the dimensionality of the latent vector, and L is the number of down-sampled time steps. A RVQ with Ng codebooks then transforms the encoded latent vectors into N₁ × L discrete tokens. The discrete audio tokens are further used as the representation of audio in the language modeling stage. The EnCodec decoder converts the generated audio tokens to waveforms. The EnCodec encoder is used only during training. We adhere to the same hyperparameter settings as outlined in the EnCodec paper, please refer to [12] for details. The adoption of EnCodec offers a high compression rate while keeping high reconstruction quality. Unlike other autoencoders that operate on spectrograms [9, 11], EnCodec eliminates the need for an additional vocoder and thus obviates the potential fidelity loss that may occur when converting a generated spectrogram back to a waveform. 2. PROPOSED METHOD Given a video clip, a video-to-audio generation system is designed to produce an audio clip that is both semantically consistent with and temporally aligned to the accompanying video content. The video-to-audio generation process can be formulated as H: va, where v refers to the frames of a video input and a corresponds to the generated audio waveform. Figure 1 presents the architecture of FoleyGen, our proposed system. FoleyGen comprises three main components: a neural audio codec for the bidirectional conversion between waveforms and discrete tokens, a visual encoder for feature extraction from video frames, and an audio language decoder tasked with generating discrete audio tokens based on the extracted visual features. This section first provides a detailed introduction to each major component of FoleyGen. To improve the temporal alignment of the visual input and generated audio, we propose using different visual attention mechanisms, which are described at the end of this section. 2.2. Visual Encoder Given a visual input v € RT×C×H×W, where T represents the number of frames (which can be 1 for a single image), C is the number of channels, and H and W denote the height and width of the visual input, respectively, the visual encoder generates feature vectors FЄRTXD with D being the number of dimension of the language decoder. The quality of the extracted visual features F is critical for achieving semantically consistent and temporally aligned audio generation. A suboptimal visual encoder may lead to loss of important visual cues, resulting in an audio output that lacks fidelity or congruency with the original video content. To explore the efficacy of different visual encoders, we conducted a series of experiments using a variety of popular visual encoders trained with uni-modal and multi-modal tasks. These visual encoders include ViT [14], CLIP [15], ImageBind [16] and VideoMAE [17]. Allow to attend Prevent from attending Frame-Specific Visual Attention: In a more restrictive approach, we introduce "frame-specific visual attention", where the model's attention is confined strictly to visual features of the concurrent time frame during generation. This strict attention mechanism ensures that the model generates audio only based on the current visual context. A All-Frame Visual Attention Causal Visual Attention A Frame-Specific Visual Attention Fig. 2. Overview of the three visual attention mechanisms. For simplicity, here we assume we have 2 visual features 'V' and 4 audio tokens 'A' with a frame rate of 2 Hz. 2.3. Audio Language Decoder Audio is represented as discrete tokens after being encoded by EnCodec [12], therefore, the video-to-audio generation problem can be formulated as a conditional language modeling task. Given visual features extracted as conditional information, we employ a Transformer model [18] to generate discrete audio tokens autoregressively. The Transformer model is decoderonly and omits the cross-attention block. The visual features are prepended to the sequence of audio tokens for conditioning. Due to EnCodec's residual vector quantization, each timestep encodes multi-stream tokens using residual codebooks. To effectively capture these multi-stream tokens, we adopt the delay pattern introduced in MusicGen [4]. This approach parallelly models multiple streams of audio tokens while maintains offsets between the streams. The incorporation of the delay pattern ensures high efficiency and eliminates the need for predicting tokens in a flattened pattern. Moreover, it sidesteps the requirement of multiple Transformer models [13, 10]. 2.4. Visual Attention Mechanism Generating audio that is temporally aligned with a video presents significant challenges. To address this, we introduce and explore three distinct visual attention mechanisms. Figure 2 shows the overview of the three attention mechanisms. All-Frame Visual Attention: In our baseline setting, we employ the default causal attention mechanism inherent in the Transformer decoder. Given that the visual features are prepended to the discrete tokens, during the generation process, the audio tokens have the capability to attend to all visual features. While this provides a broad context, it might confuse the model regarding the exact timing for sound generation due to an overabundance of visual information. Causal Visual Attention: As a countermeasure, we investigate a "causal” approach wherein, during the audio token generation, the model is restricted to attending only to visual frames that precede and align with the current timestep. This sequential attention might help the model to better synchronize the audio with the visual cues. 3.1. Dataset 3. EXPERIMENTS We target at open-domain visually guided audio generation. Therefore, we use the VGGSound [19] dataset, which contains around 200k 10-second video clips sourced from YouTube with diverse contents. Since some video clips are not downloadable anymore, our version contains 159 318 samples in the train set and 13 161 samples in the test set. 3.2. Implementation Details All the audio clips in the dataset are sampled to 16k Hz monophonic audio. For the EnCodec, we follow the same downsampling strides [2, 4, 5, 8] in the encoder, which leads to a frame rate of 50 Hz. We employ four codebooks with a codebook size of 2048. For video data, we sample one frame per second and follow the prepocessing protocols (e.g., resize, normalize) in the visual encoders. A linear layer is used after the visual encoder to project the visual features to the same dimension of the Transformer model. The Transformer decoder consists of 24 layers with 16 heads and a dimension of 1024. A memory efficient flash attention [20] is used to improve the speed and memory usage. = The models are trained for 20k steps with a batch size of 256. AdamW optimizer with ẞ1 0.9, 62 = 0.95, and a weight decay of 0.1 is used. The learning rate is set to 1 × 10−and warm up is used in the first 4k steps. In addition, classifierfree guidance [21] is also employed to achieve better visual adherence. During training, the visual condition is dropped (i.e., replaced with null vectors) with a probability of 0.1. During inference, the classifier-free guidance scale of 3.0 is used, and we employ top-k sampling with k setting to 256. 3.3. Evaluation Metrics To evaluate the performance of FoleyGen, we carry out both objective and subjective evaluations. For objective evaluation, we employ Fréchet Audio Distance (FAD) [22], KullbackLeibler Divergence (KLD), and ImageBind (IB) score [16]. FAD calculates the distribution distance between the features of generated and reference audio clips, where the features are calculated using VGGish network [23] trained on AudioSet. KLD compares the label distribution of target and generated audio calculated by a pretrained PaSST model [24]. FAD demonstrates a strong correlation with human perception regarding audio quality, whereas KLD primarily captures the Table 1. Experimental results on VGGSound dataset. Here we use all-frame visual attention. Methods Visual Encoder FAD↓ KL↓ IB (%) ↑ OVR (%) ↑ REL (%) ↑ SpecVQGAN [9] ResNet-6.3.5.5.IM2WAV [10] CLIP 6.2.16.31.Ours CLIP 1.2.26.77.63.Table 2. Experimental results on VGGSound dataset with models trained using different visual encoders. IB(%) ↑ Visual Encoder FAD KL↓ CLIP 1.2.26.ᏙᎥᎢ 1.2.23.ImageBind 1.2.26.VideoMAE 2.3.17.Table 3. Experimental results on VGGSound dataset with models trained using different attention mechanisms. The visual encoder used is CLIP. ALI (%) ↑ Attention FAD↓↓ KL↓ All-frame 1.65 2.Causal 2.18 2.IB(%) ↑ OVR (%) ↑ 26.63.55.25.24.14.13.22.31.Frame-specific 2.49 2.audio concepts present in the recording [2]. To evaluate the relevance between the generated audio and video, we propose using the ImageBind model [16] to compute a relevance score. Since ImageBind is trained to learn a joint embedding across six distinct modalities, the cosine similarity of its embeddings for both video and generated audio can capture semantic relevance between them. For subjective evaluation, human listeners are asked to compare samples generated by distinct models and identify the one that demonstrated superior performance based on specific criteria, which included overall quality (OVR), relevance (REL) to the corresponding visual input. Temporal alignment (ALI) is considered when evaluating the attention mechanisms. 3.4. Results Table 1 presents the primary results of our study, where we benchmark our proposed FoleyGen system against two previous state-of-the-art methods, SpecVQGAN [9] and IM2WAV [10]. Given that IM2WAV utilized FAD and KLD as evaluation metrics, we adopted their scores directly. For subjective evaluation, we generated samples using their pretrained models. It's evident from the results that FoleyGen consistently surpasses both SpecVQGAN and IM2WAV in both objective and subjective metrics. Notably, there's a marked reduction in the FAD score. The trends in subjective evaluations are congruent with the objective metrics. Several factors can be attributed to this improvement. First, the integration of EnCodec facilitates a heightened compression ratio of audio tokens and leads to a enhanced reconstruction quality. This elevated compression ratio simplifies the modeling of its distribution for the language model. Second, the utilization of the delay pattern in token generation avoids the need for multiple Transformer models, culminating in superior performance. Table 2 shows the results of our models when trained using various visual encoders. It can be observed that visual encoders that are pre-trained via multi-modal tasks, (i.e., CLIP [15] and ImageBind [16]), exhibit comparable performances and surpass those trained solely on uni-modal tasks. ViT, which has been pre-trained through a discriminative task, outperforms VideoMAE. Since VideoMAE is trained using masked autoencoder with self-supervised learning, fine-tuning might be required when adopt it for downstream tasks. Table 3 presents the results achieved using different attention mechanisms. All-frame visual attention notably surpassed the other two, both in objective metrics and human evaluations. Interestingly, while the frame-specific attention lagged in objective evaluations, it demonstrated an enhanced performance in human evaluation as compared with causal visual attention. However, a critical insight from human evaluations reveals that the systems still struggle with temporal alignment, and sometimes fail to capture prominent actions within the video. 4. CONCLUSIONS In this paper, we introduced FoleyGen, a video-to-audio generation model following a language modeling paradigm. FoleyGen utilizes the EnCodec for bidirectional waveform-token conversion, a visual encoder for visual feature extraction and a Transformer decoder for conditioned audio token generation. Our evaluations demonstrate that FoleyGen surpasses prior methodologies in both objective metrics and human evaluations. Through our explorations, we observed that visual encoders trained on multimodal tasks exhibit superior performance. While we introduced visual attention mechanisms to enhance audio-video temporal alignment, it remains a persistent challenge in the domain. Future research should delve deeper into improving the temporal cohesion of video-to-audio generation systems. 5. REFERENCES [1] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley, “AudiOLDM: Text-to-audio generation with latent diffusion models," Proceedings of the International Conference on Machine Learning, 2023. [2] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi, "AudioGen: Textually guided audio generation," in The Eleventh International Conference on Learning Representations, 2023. [3] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D. Plumbley, "AudioLDM 2: Learning holistic audio generation with self-supervised pretraining," arXiv preprint arXiv:2308.05734, 2023. [4] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Défossez, "Simple and controllable music generation," arXiv preprint arXiv:2306.05284, 2023. [5] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward H. Adelson, and William T. Freeman, “Visually indicated sounds," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. [6] Kan Chen, Chuanxi Zhang, Chen Fang, Zhaowen Wang, Trung Bui, and Ram Nevatia, "Visually indicated sound generation by perceptually optimized classification,” in Proceedings of the European Conference on Computer Vision Workshops, 2018. [7] Yipin Zhou, Zhaowen Wang, Chen Fang, Trung Bui, and Tamara L Berg, “Visual to sound: Generating natural sound for videos in the wild," in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3550-3558. [8] Peihao Chen, Yang Zhang, Mingkui Tan, Hongdong Xiao, Deng Huang, and Chuang Gan, “Generating visually aligned sound from videos," IEEE Transactions on Image Processing, vol. 29, pp. 8292-8302, 2020. [9] Vladimir Iashin and Esa Rahtu, “Taming visually guided sound generation," in British Machine Vision Conference (BMVC), 2021. [10] Roy Sheffer and Yossi Adi, “I hear your true colors: Image guided audio generation,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1-5. [11] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao, “DiffFoley: Synchronized video-to-audio synthesis with latent diffusion models," arXiv preprint arXiv:2306.17203, 2023. [12] Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi, "High fidelity neural audio compression," arXiv preprint arXiv:2210.13438, 2022. [13] Zalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al., "Audiolm: a language modeling approach to audio generation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023. [14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby, “An image is worth 16xwords: Transformers for image recognition at scale,” in International Conference on Learning Representations, 2021. [15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al., “Learning transferable visual models from natural language supervision,” in International Conference on Machine Learning. PMLR, 2021, pp. 8748-8763. [16] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra, “ImageBind: One embedding space to bind them all,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 15180–15190. [17] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang, “VideoMAE: Masked autoencoders are data-efficient learners for selfsupervised video pre-training,” Advances in Neural Information Processing Systems, vol. 35, pp. 10078-10093, 2022. [18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin, "Attention is all you need,” in Advances in Neural Information Processing Systems. 2017, vol. 30, Curran Associates, Inc. [19] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman, "VggSound: A large-scale audio-visual dataset," in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 721–725. [20] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré, "FlashAttention: Fast and memory-efficient exact attention with io-awareness," Advances in Neural Information Processing Systems, vol. 35, pp. 16344-16359, 2022. [21] Jonathan Ho and Tim Salimans, "Classifier-free diffusion guidance," arXiv preprint arXiv:2207.12598, 2022. [22] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi, "Fréchet audio distance: A metric for evaluating music enhancement algorithms,” arXiv preprint arXiv:1812.08466, 2018. [23] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al., “CNN architectures for large-scale audio classification,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 131-135. [24] Khaled Koutini, Jan Schlüter, Hamid Eghbal-zadeh, and Gerhard Widmer, “Efficient training of audio transformers with patchout," in Interspeech. 2022, pp. 2753-2757, ISCA.