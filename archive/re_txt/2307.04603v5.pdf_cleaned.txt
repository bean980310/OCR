arXiv:2307.04603v5 [q-bio.BM] 31 JulSolvent: A Framework for Protein Folding Jaemyung Lee Kakao Brain james.brain@kakaobrain.com Kyeongtak Han† Department of Electrical and Computer Engineering, Inha University han00127@inha.edu Jaehoon Kim Kakao Brain jack.brain@kakaobrain.com Hasun Yu Kakao Brain shawn.yu@kakaobrain.com Youhan Lee Kakao Brain youhan.lee@kakaobrain.com Abstract Consistency and reliability are crucial for conducting AI research. Many famous research fields, such as object detection, have been compared and validated with solid benchmark frameworks. After AlphaFold2, the protein folding task has entered a new phase, and many methods are proposed based on the component of AlphaFold2. The importance of a unified research framework in protein folding contains implementations and benchmarks to consistently and fairly compare various approaches. To achieve this, we present Solvent, a protein folding framework that supports significant components of state-of-the-art models in the manner of an off-the-shelf interface Solvent contains different models implemented in a unified codebase and supports training and evaluation for defined models on the same dataset. We benchmark well-known algorithms and their components and provide experiments that give helpful insights into the protein structure modeling field. We hope that Solvent will increase the reliability and consistency of proposed models and give efficiency in both speed and costs, resulting in acceleration on protein folding modeling research. The code is available at https://github.com/kakaobrain/solvent, and the project will continue to be developed. 1 Introduction Deep-learning-based protein structure prediction task has become an essential research area after the witnesses of Alphafold2[1]. Additionally, protein folding methods without multiple sequence alignments (MSA) inputs such as ESMFold[2] and OmegaFold[3] have been proposed to model general proteins in high-speed from the removal of generating MSA. Especially, those models have shown promising results on specific applications, including antibody structure and de-novo structure IgFold[4] and tFold-Ab[5]. Each method has published papers and released inference code with pre-trained models. However, compared to the MSA-dependent method, which has a well-established Kyeongtak Han was an intern at Kakao Brain. preprint. open-sourced training framework, OpenFold [6], the released code of the mentioned MSA-free folding frameworks does not contain training codes, including the input data preparation pipeline. In addition to this, different implementations they used can produce slightly different results for the same components, making clear comparisons difficult. Furthermore, training and evaluating different datasets makes it difficult to compare methods fairly. Reproducing studies in a unified way and comparing them to other methods provides insights to researchers and allows them to propose new methods which are not easily accessible in protein folding. Because there have been similar difficulties in artificial intelligence fields, it is helpful to study how researchers have overcome them. We bring object detection as a representative example because a well-established framework achieves generalizability and fair comparison for the field. After the development of Faster R-CNN[7], various R-CNN-based object detection methods have been proposed. The individual method verified its contribution as a publication and source code. However, it was not easy to compare them across studies which is conducted on different codebases. Fortunately, the generalizability and consistency of experiments have been resolved since frameworks like Detectron2[8] and MMDetection [9] were proposed. Benchmarking was consistently and userfriendly performed within the same framework for various model variants. The framework provided unified datasets, evaluation metrics, and module implementations. With all other conditions fixed, the effect of the main contribution was clearly and definitely assessed as the frameworks have been used as a de-facto base. In addition, the framework generalized various models as meta-architecture, which is made up of abstracted components. It reduced the complexity of understanding the object detection pipeline and led to high-quality research, allowing researchers to focus on their ideas, and finally accelerating the field. The protein folding field needs to mature in the same direction as object detection. Like the emergence of Faster R-CNN in object detection, AlphaFold2 has emerged in protein structure prediction. Based on the modules from Alphafold2, single-sequence-based structure prediction models are being actively explored. To achieve acceleration on the protein folding research as Detectron2 or MMDetection, we present Solvent, the protein folding framework that contains major neural networks, which are the main parts of state-of-the-art models. In the Solvent, several methods are implemented with a unified codebase and represented under meta-architecture. In addition, well-defined datasets are provided for training and validating models. To make Solvent work as a framework, we borrow the pipeline of Detectron2[8], which guarantees the consistency and generalizability on Solvent. In the framework, we represent individual methods using the implementation of OpenFold[6], which is the most reliable and well-known Alphafold2 re-implementation project. We abstract the single sequence-based folding method into three Embedder, Trunk, and Folding module components. We design meta-architecture that one model of ESMFold[2], OmegaFold[3], and IgFold[4] can be called according to the specific types of Embedder, Trunk, and Folding module. Also, the specific algorithm of each component can be selected and combined interchangeably and user-friendly so that new model variants can be easily defined. Furthermore, a researcher can implement a new type of component and combine it with other built-in components in Solvent. For example, a new proposed protein language model can be applied as Embedder and experimented with the existing Trunk and Folding module in Solvent, allowing researchers efficiency in research. In addition to this, Solvent provides built-in support for several train and test datasets to benchmark model performance. Single-chain-based general protein and multi-chain-based antibody datasets are supported with the appropriate metrics. Especially to maximize the training and inference efficiency, we utilized the power of the recently proposed optimization technique, xformers [10]. Also, other optimizations are employed so that the training speed of Solvent is optimized by about 30 % compared to the original implementations. To confirm and show how Solvent works, we first experimented with reproducing ESMFold to check the reproducibility of the Solvent. We also experimented with combinations of methods for the components that comprise the meta-architecture to evaluate which Embedder and which Trunk are more effective. Furthermore, we conduct experiments that provide helpful insights for structure prediction studies, such as whether antibody-specific language models can be replaced by general protein language models and how effective Evoformers are. Solvent will be extended to support more algorithms and broader concepts beyond single-sequence protein folding.Embedder Meta-architecture ✓ SSPF ✓IGFold Trunk Folding Heads ✓ ESM ✔ OMEGAPLM ✓ Antiberty ✓ Evoformer GeoformerLite ✓IgFoldTrunk AlphafoldStructure IgFoldStructure AlphafoldHeads ✓IgFoldHeads Figure 1: Solvent generalizes model as meta-architecture. The components in meta-architecture is abstracted and their specifics determine folding algorithms. Table 1: Model abstraction Method Embedder Trunk Folding Heads ESMFold ESM Evoformer OmegaFold-lite IgFold OmegaPLM Antiberty GeoformerLite IgFoldTrunk AlphafoldStructure | AlphafoldHeads AlphafoldStructure AlphafoldHeads IgFoldStructure IgFoldHeads 2 Supporting Components Solvent is designed to train and evaluate arbitrary models on arbitrary data, with models and data managed as independent pipelines. This section describes each in more detail. 2.1 Models Solvent supports several different protein folding models, but each model is abstracted into a metaarchitecture. The meta-architecture is composed of the following components. Embedder Embedder takes a sequence as input and outputs a sequence embedding with their pair representation computed from pre-trained protein language models (PLM) such as ESM-2. Solvent supports ESM-2[11], OmegaPLM[3], Antiberty[12] as a built-in embedders. Trunk The trunk is the main building block of structure prediction. It exchanges information between sequence embedding and their pair representation computed by the previous component, Embedder, and includes a recycling embedder that utilizes predicted atom positions from the previous cycle. Evoformer[1], GeoformerLite, and IgFoldTrunk[4] are built-in supported in the Solvent. Due to GPU memory constraints, we provide GeoformerLite, a simplified version of Geoformer [3]. We note that it does not show the full performance of the original Geoformer. Folding The folding module takes single representation and pair representation computed from the previous component, Trunk, and directly predicts the 3D-coordinates of the structure. AlphafoldStructure[1], IgfoldStructure[4] is supported in the Solvent. Heads Heads perform task-specific prediction and loss calculation based on features obtained from the Trunk and Folding module(ex. pLDDT, distogram). Solvent includes all the auxiliary heads used in Alphafold2[1] and IgFold[4]. All mentioned components are listed in table 1. Researcher creates model variants easily by changing specific method in the component. For example, a new model can be defined by combining offthe-shelf components, such as the ESM-2 Embedder with the GeoformerLite Trunk, making a new model variant quickly and allowing an accurate comparison between Evoformer and GeoformerLite. In addition to built-in methods, researchers can add new custom methods as components and new models can be defined with built-in modules.2.2 Datasets Solvent supports single-chain based general protein dataaset and multi-chain antibody dataset. The datasets described below. 2.2.1 General protein datasets PDB The dataset is from Protein Data Bank[13] and we use the data before May, 2020 as same as the ESMFold[2] paper does. Uniref The dataset is basically from Alphafold predicted dataset(afdb)[1] and we use only the data corresponding to Uniref50. We use the samples with an average pLDDT of 70 or higher. CAMEO The dataset is from CAMEO[14] and we use it mainly as evaluation. We use only the data for the 3-months prior to June 25, 2022. 2.2.2 Antibody datasets SAbDab_20210331 Antibody dataset based on SAbDab[15] and uses data before March, 2021. Heavy-light chain paired samples and Heavy chain only nanobody samples are included in the dataset. SAbDab_igfold The selected dataset by IgFold[4] paper for the benchmark is a common set used in many papers. We use it mainly as an evaluation. Except for built-in datasets, custom datasets can be added to the Solvent easily and used to train models with built-in datasets. Any model can be trained and evaluated with well-registered datasets. 3 Benchmark First, we benchmark the reproduction of ESMFold[2] to check the reproducibility of Solvent. Then we experiment with combinations of different types of Embedder and Trunk for experiencing the convenience of the Solvent and benchmarking of simple model variants. Furthermore, we conduct additional experiments to gain insights to help with protein structure prediction. 3.1 Experimental settings Datasets For general protein, we use pdb and af2_uniref50 for training models. We use cameo dataset for evaluation. For antibody-specific models, we use sabdab_20210331 dataset and sabdab_igfold dataset for training and evaluating models, respectively. Training Details For general proteins, we followed the training scheme of ESMFold[2]. The crop size of the sequence is fixed at 256 for initial training and 384 for finetuning. Since the models we experiment with have low Trunk depth, we do not apply violation loss even when fine-tuning. The batch size is fixed at 128, as in AlphaFold2, and the batch size per GPU depends on the model size. Evaluation Metrics TMscore[16] is used for evaluating general proteins. Sequence alignment is used as a default option. For antibody models, region-specific RMSD is measured using PyRosetta[17]. 3.2 Benchmark existing models 3.2.Reproducibility of Solvent We define ESMFold through Solvent and benchmark the performance for checking reproducibility. Rather than reproducing ESMFold full model with 48 Evoformers, we use Trunk-off models. Different size(35M, 150M, 650M) of ESM-2 is used for experiments. As reported in Table 2, some models are reproduced with slightly lower TMscore and some with slightly better performance, but they are reproduced with comparable level to the paper[2]. We train models until finetuning phase only for reproducibility experiments and we train models until initial training for the other experiments.Table 2: Comparison ESMFold paper reported TMscore with reproduced ESMFold models by the Solvent. Method Solvent Paper reported ESM-2(35M) 0.0.ESM-2(150M) 0.0.ESM-2(650M) 0.0.Table 3: Simple model vairants by combining component from two different methods. Method Embedder Trunk TMscore ESMFold ESM-2(650M) 2 Evoformer 0.OmegaFold-lite CombinationsCombinationsOMEGAPLM(670M) 2 GeoformerLite 0.OMEGAPLM(670M) 2 Evoformer ESM-2(650M) 2 GeoformerLite 0.0.3.2.2 Combinations of models In Solvent, it is easy to combine components of different structure prediction models, making performance comparison between methods possible. Using this convenience, we conduct a combination study of the Embedder and Trunk of two different methods, ESMFold and OmegaFold-lite. Note that the original OmegaFold[3] does not share the IPA weight of the structure module, but we experimented with a weight-sharing IPA for solid comparison. As mentioned previous section, we use simplified Geoformer, Geoformer-lite, due to resource constraints. In other words, OmegaFold-lite is not a completely reproduced version of the original OmegaFold. Table 3 shows the performance of various model variants by permuting components from two different methods. These experiments give researchers a rigorous comparison framework to evaluate which Embedder or Trunk is better. 3.3 Further Study The Solvent provides an easy way to conduct various experiments objectively. This allows researchers to make meaningful comparisons between methods. We have conducted some further studies and hope the results give insights for structure prediction research. 3.3.1 Minimize number of Trunk with trainable language model A Trunk module, such as Evoformer, is essential to performance improvement in structure prediction. However, it is computationally expensive, requiring large GPU memory. Therefore, various engineering methods[18] have been proposed to achieve efficiency. A trunk contains an information exchange process between a single representation and a pair representation that contains structural information, and it is repeatedly performed through many blocks (e.g., 48 blocks in ESMFold). Meanwhile, most algorithms such as ESMFold, OmegaFold, and IgFold freeze their PLM layers and let only the Trunk learn the structure from the training data. If we unfreeze the parameters of PLMs and let the structure information be backpropagated into the language model, we can reduce the number of blocks of the Trunk. To prove this, we define four different ESMFold variants based on the type of PLMs and the number of Trunk. In the table 4, comparing variant1 and variant2 shows the effect of a trainable PLM. Variant1 and variant3 show the effect of the Evoformer. Trainable PLM results in a 7 percent TMscore performance improvement and the use of single Evoformer results in a 10 percent TMscore performance improvement. Adding a single Evoformer onto the trainable PLM model(variant2) has minor performance improvement. We expect the best model as variant4, but a trainable PLM with Evoformer leads to worse performance. All experiments are conducted using ESM-2(35M) as Embedder. 3.3.2 Use general protein language model on antibody structure predictions Various large-scale language models exist for general proteins, such as ESM-2(up to 15B) and OmegaPLM(670M). However, antibody-specific models are represented by Antiberty(25M), which is small compared to general proteins, and the size of the dataset to train them is also small comparedTable 4: Model variant based on Embedder status and the number Trunk. Trunk Method Embedder Status TMscore variantl Freeze variantTrainable 0 Evoformer 0.0 Evoformer 0.variant3 Freeze variantTrainable 1 Evoformer 0.1 Evoformer 0.Table 5: More datasets can be registered and used as testsets. Method CASPDe-novo Orphan variant1 0.0.0.variant2 0.0.0.variant3 0.0.0.variant4 0.0.0.Table 6: Antibody model variants based on PLM and their status. Method IgFold(reproduced) IgFold-variantl IgFold-variantIgFold-variantEmbedder Embedder Status Meta-arch Antiberty(25M) Freeze IGFold ESM-2(35M) Freeze IGFold ESM-2(650M) Freeze IGFold ESM-2(35M) Trainable IGFold Method Table 7: The performance of various antibody models. IDDT-Ca IgFold(paper) IgFold(reproduced) 0.IgFold-variant0.IgFold-variantIgFold-variant0.0.OCD H Fr H1 H2 H3 L Fr L1 L3.77 0.45 0.80 0.75 2.99 0.45 0.83 0.51 1.3.74 0.57 0.92 0.80 3.09 0.67 1.12 0.55 1.3.76 0.62 0.87 0.94 3.06 0.49 0.90 0.51 1.3.77 0.48 0.91 0.94 3.20 0.48 0.94 0.49 1.3.88 0.51 0.89 0.85 3.14 0.51 1.00 0.50 1.Lto ESM-2 and OmegaPLM. Currently, the general protein language model is publicly available and easy to use. It is worth investigating whether Antiberty, an antibody-specific language model, is still particularly unique for antibody structure prediction compared to general PLM. The details of antibody model variants are listed in Table 6. The performance of the listed model is reported in Table 7. From the comparison between IgFold(reproduced) and Igfold-variant1, the performance difference between using Antiberty and ESM-2(35M) is not very noticeable. The Antiberty model performs better on some CDRs but does not significantly. In fact, using a large general protein language model(IgFold-variant2) model seems more effective than using an antibody-specific language model. Using general language model with trainable parameters(IgFold-variant3) does not show performance improvement on most CDRs. 3.4 Custom datasets can be added and evaluated on different models In Solvent, any datasets can be registered and used for training and evaluating models. As an example, we register CASP14 datasets, de-novo proteins, and orphan proteins. In the case of CASP14, we used 33 publically released samples. T1044 is not included due to memory constraints. In the case of de-novo and orphan proteins, we referred to the target lists provided at RGN2[19] repository and used samples that were released after May 2020. These samples might be used when training ESM-2, which causes high performance for de novo proteins. All the samples are listed in the appendix. The model variants listed in Table 4 can be evaluated on these three different datasets(Table 5).4 Conclusion To support a consistent and easy-to-use research framework, we propose Solvent for protein folding research. We hope that efficient and rigorous experiments on top of the Solvent will further prove the strengths and weaknesses of each algorithm and finally accelerate structural prediction research. Currently, Solvent focuses on MSA-free protein structure prediction models. We will extend Solvent to a more general way that takes MSA and template as input and support more validation data such as orphan and de-novo protein. Acknowledgments We acknowledge the contributions of the Language Model Engineering Team at Kakao Brain, who have optimized Solvent. These optimizations make Solvent efficient in training speed and memory, so researchers can easily tap larger models. Their support has been essential in achieving the outcomes presented in this work. References [1] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex Bridgland, Clemens Meyer, Simon A A Kohl, Andrew J Ballard, Andrew Cowie, Bernardino Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska, Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W Senior, Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873):583–589, 2021. [2] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Salvatore Candido, and Alexander Rives. Evolutionary-scale prediction of atomic level protein structure with a language model. bioRxiv, 2022. [3] Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, Jianzhu Ma, and Jian Peng. High-resolution de novo structure prediction from primary sequence. bioRxiv, 2022. [4] Jeffrey A Ruffolo, Lee-Shin Chu, Sai Pooja Mahajan, and Jeffrey J Gray. Fast, accurate antibody structure prediction from deep learning on massive set of natural antibodies. Nature communications, 14(1):2389, 2023. [5] Jiaxiang Wu, Fandi Wu, Biaobin Jiang, Wei Liu, and Peilin Zhao. tfold-ab: Fast and accurate antibody structure prediction without sequence homologs. bioRxiv, 2022. [6] Gustaf Ahdritz, Nazim Bouatta, Sachin Kadyan, Qinghui Xia, William Gerecke, Timothy J O'Donnell, Daniel Berenberg, Ian Fisk, Niccolò Zanichelli, Bo Zhang, et al. Openfold: Retraining alphafold2 yields new insights into its learning mechanisms and capacity for generalization. bioRxiv, pages 2022-11, 2022. [7] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. [8] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019. [9] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Mmdetection: Open mmlab detection toolbox and benchmark. CoRR, abs/1906.07155, 2019.[10] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular and hackable transformer modelling library. https://github.com/ facebookresearch/xformers, 2022. [11] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022. [12] Jeffrey A Ruffolo, Jeffrey J Gray, and Jeremias Sulam. Deciphering antibody affinity maturation with language models and weakly supervised learning. arXiv, 2021. [13] Helen M. Berman, John Westbrook, Zukang Feng, Gary Gilliland, T. N. Bhat, Helge Weissig, Ilya N. Shindyalov, and Philip E. Bourne. The Protein Data Bank. Nucleic Acids Research, 28(1):235–242, 01 2000. [14] J'urgen Haas, Alessandro Barbato, Dario Behringer, Gabriel Studer, Steven Roth, Martino Bertoni, Khaled Mostaguir, Rafal Gumienny, and Torsten Schwede. Continuous automated model evaluation (cameo) complementing the critical assessment of structure prediction in casp12. Proteins, 86(S1):387–398, 2018. [15] James Dunbar, Konrad Krawczyk, Jinwoo Leem, Terry Baker, Angelika Fuchs, Guy Georges, Jiye Shi, and Charlotte M. Deane. SAbDab: the structural antibody database. Nucleic Acids Research, 42(D1):D1140-D1146, 11 2013. [16] Y. Zhang and J. Skolnick. Scoring function for automated assessment of protein structure template quality. Proteins, 57(4):702-710, 2004. [17] Sidhartha Chaudhury, Sergey Lyskov, and Jeffrey J. Gray. Pyrosetta: a script-based interface for implementing molecular modeling algorithms using rosetta. Bioinformatics, 26(5):689–691, 2010. [18] Shenggan Cheng, Ruidong Wu, Zhongming Yu, Binrui Li, Xiwen Zhang, Jian Peng, and Yang You. Fastfold: Reducing alphafold training time from 11 days to 67 hours, 2022. [19] Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Charlotte Rochereau, George M. Church, Peter K. Sorger, and Mohammed AlQuraishi. Single-sequence protein structure prediction using language models from deep learning. bioRxiv, 2021. A Appendix A.1 List of samples used in TableA.1.CASPWe used 33 publicaly released samples. T1044 is not included. T1024, T1025, T1026, T1027, T1029, T1030, T1031, T1032, T1033, T1035, T1036s1, T1037, T1038, T1039, T1040, T1041, T1042, T1043, T1046s1, T1046s2, T1049, T1050, T1054, T1056, T1064, T1067, T1073, T1074, T1079, T1080, T1082, T1090, T1099. A.1.2 De novo proteins From the RGN2 repository target lists, we used 111 samples released after May, 2020. 7BQS_A, 6X1K_A, 6W6X_A, 7A4Y_E, 7BQR_A, 7RX5_A, 7BQC_A, 7BQQ_A, 7DNS_A, 7BQB_A, 6YPI_A, 6Z35 A, 7BQE_A, 6ZOF_D, 7CZ0_E, 7BPN_A, 7BQD_A, 7CX4_N, 7BPM_A, 6WXO_B, 6XT4_A, 6XEH_A, 7BPL_A, 7BQN_A, 6XSS_A, 6WMK_A, 7AWY_A, 7AVA_A, 7A4D_E, 7B09_C, 7DI0_A, 6YB2_A, 7B08_B, 6YB1_A, 6VG7_A, 7BIM_A, 7BEY_A, 7AWZ_A, 6YB0_A, 7A8S_A, 7BQM_A, 6RLH_A, 6WXP_A, 6W9Y_A, 6UIB_C, 6RLI_A, 6UIS_A, 7RGR_B, 6WVS_A, 7BPP_A, 6W9Z_D, 7ARR_A, 6ZLI_C, 7AX0_A, 7ARS_A, 6X16_A, 6ZT1_A, 6WRV_D, 7AX2_A, 6Y7N_A, 6VFL_B, 6WRW_C, 6VGA_A, 6VFK_B, 6Z3X_B, 7BNT_B, 7CUV_A, 7N8J_B, 6VFJ_B, 7KBQ_A, 7CD8_A,6YAZ_E, 6VFI_B, 6X8N_A, 6VGB_A, 6VFH_A, 6W70_A, 7BAU_E, 6ZIE_A, 6ZOL_A, 6REO_A, 6ZV9_B, 6W90_A, 7BAT_A, 7JH6_A, 6Z0M_A, 7A50_B, 7JH5_A, 7BAW_A, 6YWC_C, 6WA0_B, 7DDR_A, 6Z2I_A, 7BWW_A, 7A48_B, 7BAV_C, 6WRX_C, 6V67_B, 6X9Z_A, 6YWD_C, 6XR1_A, 6XNS_B, 6VL5_D, 6VEH_A, 7AIT_C, 6XR2_E, 6VL6_A, 7BAS_B, 7CBC_A, 6XH5_B, 7B0A_C A.1.3 Orphan proteins From the RGN2 repository target lists, we used 6 samples released after May, 2020. 6M64_F, 6YNS_O, 7ALO_A, 7DGU_A, 7KEI_C, 70SC_A.