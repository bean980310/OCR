arXiv:2307.12612v1 [cs.CV] 24 JulLess is More: Focus Attention for Efficient DETR Dehua Zheng, 1,2 Wenhui Dong² Hailin Hu² Xinghao Chen² Yunhe Wang²* Huazhong University of Science and Technology 2Huawei Noah's Ark Lab dwardzheng@hust.edu.cn {wenhui.dong, hailin.hu, xinghao.chen, yunhe.wang} @huawei.com Abstract DETR-like models have significantly boosted the performance of detectors and even outperformed classical convolutional models. However, all tokens are treated equally without discrimination brings a redundant computational burden in the traditional encoder structure. The recent sparsification strategies exploit a subset of informative tokens to reduce attention complexity maintaining performance through the sparse encoder. But these methods tend to rely on unreliable model statistics. Moreover, simply reducing the token population hinders the detection performance to a large extent, limiting the application of these sparse models. We propose Focus-DETR, which focuses attention on more informative tokens for a better trade-off between computation efficiency and model accuracy. Specifically, we reconstruct the encoder with dual attention, which includes a token scoring mechanism that considers both localization and category semantic information of the objects from multi-scale feature maps. We efficiently abandon the background queries and enhance the semantic interaction of the fine-grained object queries based on the scores. Compared with the state-of-the-art sparse DETR-like detectors under the same setting, our Focus-DETR gets comparable complexity while achieving 50.4AP (+2.2) on COCO. The code is available at torch-version and mindspore-version. 1. Introduction Object detection is a fundamental task in computer vision that aims to predict the bounding boxes and classes of objects in an image, as shown in Fig. 1 (a), which is of great importance in real-world applications. DETR proposed by Carion et al.[1] uses learnable queries to probe image features from the output of Transformer encoders and bipartite graph matching to perform set-based box prediction. DETR-like models [18, 36, 14, 32, 21, 26, 2, 30, 37] have made remarkable progress and gradually bridged the gap *Corresponding author (a) Image (b) Sparse DETR (c) Focus-DETR (d) Focus-DETR foreground foreground object tokens Figure 1: Visualization and comparison of tokens selected by Sparse DETR [26] and our Focus-DETR. (a) is the original images, (b) and (c) represent the foreground selected by models. (d) indicates the object tokens with more fine-grained category semantic. Patches with smaller sizes come from higher-level features. with the detectors based on convolutional neural networks. Global attention in the DETR improves the detection performance but suffers from computational burden and inefficiency due to redundant calculation without explicit discrimination for all tokens. To tackle this issue, Deformable DETR [37] reduces the quadratic complexity to linear complexity through key sparsification, and it has developed into a mainstream paradigm due to the advantages of leveraging multi-scale features. Herein, we further analyze the computational burden and latency of components in these models (Fig. 2). As shown in Fig. 2, we observe that the calculation cost of the encoder is 8.8× that of the decoder in Deformable DETR [37] and 7.0× in DINO [36]. In addition, the latency of the encoder is approximately 4~8 times that of the decoder in Deformable DETR and DINO, which emphasizes the necessity to improve the efficiency in the encoder module. In line with this, previous works have generally discussed the feasibility of compressing tokens in the transformer encoder. For instance, PnP-DETR [29] abstracts the whole features into fine foreground object feature vectors and a small number of coarse background contextual feature vectors. IMFA [34] searches key points based on the prediction of decoder layer to sample multi-scale features and aggregates sampled features with single-scale features. Sparse DETR [26] proposes to preserve the 2D spatial structure of the tokens through query sparsity, which makes it applicable to Deformable DETR [37] to utilize multi-scale features. By leveraging the cross-attention map in the decoder as the token importance score, Sparse DETR achieves performance comparable to Deformable DETR only using 30% of queries in the encoder. Despite all the progress, the current models [29, 26] are still challenged by sub-optimal token selection strategy. As shown in Fig. 1 (b), the selected tokens contain a lot of noise and some necessary object tokens are obviously overlooked. In particular, Sparse DETR's supervision of the foreground predictor relies heavily on the decoder's cross-attention map (DAM), which is calculated based on the decoder's queries entirely from encoder priors. Preliminary experiments show severe performance decay when the Sparse DETR is embedded into the models using learnable queries due to weak correlation between DAM and the retained foreground tokens. However, state-of-the-art DETRlike models, such as DINO [36], have proven that the selected features are preliminary content features without further refinement and could be ambiguous and misleading to the decoder. In this case, DAM's supervision is inefficient. Moreover, in this monotonous sparse encoder, the number of retained foreground tokens remains numerous, and performing the query interaction without more fine-grained selection is not feasible due to computational cost limitations. To address these issues, we propose Focus-DETR to allocate attention to more informative tokens by stacking the localization and category semantic information. Firstly, we design a scoring mechanism to determine the semantic level of tokens. Foreground Token Selector (FTS) aims to abandon background tokens based on top-down score modulations across multi-scale features. We assign {1,0} labels to all tokens from the backbone with reference to the ground truth and predict the foreground probability. The score of the higher-level tokens from multi-scale feature maps modulates the lower-level ones to impose the validity of selection. To introduce semantic information into the token selection process, we design a multi-category score predictor. The foreground and category scores will jointly determine the more fine-grained tokens with strong category semantics, as shown in Fig. 1 (d). Based on the reliable scores and selection from different semantic levels, we feed foreground tokens and more fine-grained object tokens to the encoder with dual attention. Thus, the limitation of deformable attention in distant information mixing is remedied, and then the semantic information of foreground queries is enhanced by fine-grained token updates. To sum up, Focus-DETR reconstructs the encoder's cal-84.Latency(ms) GFLOPS 142.encoder decoderencoder decoder 27.49.20.20.9.Deformable DINO Focus-DETR DETR Deformable DETR DINO Focus-DETR 16.8.8.Figure 2: Distribution of calculation cost and latency in the Transformer part of the DETR-like models, e.g., Deformable DETR [37], DINO [36] and our Focus-DETR. culation process with dual attention based on obtaining more accurate foreground information and focusing on finegrained tokens by gradually introducing semantic information, and further enhances fine-grained tokens with minimal calculation cost. Extensive experiments validate FocusDETR's performance. Furthermore, Focus-DETR is general for DETR-like models that use different query construction strategies. For example, our method can achieve 50.4AP (+2.2) on COCO compared to Sparse DETR with a similar computation cost under the same setting. 2. Related work Transformer-based detectors. Recently, Carion et al.[1] proposed an end-to-end object detector named DETR (Detection Transformer) based on Vision Transformer [7]. DETR transforms object detection into a set prediction task through the backbone, encoder, and decoder and supervises the training process through Hungarian matching algorithms. A lot of recent works [18, 14, 37, 36, 21, 3, 35, 2, 4] have boosted the performance of Transformer-based detectors from the perspective of accelerating training convergence and improving detection precision. Representatively DINO[36] establishes DETR-like models as a mainstream detection framework, not only for its novel end-toend detection optimization, but also for its superior performance. Fang et al. [8] propose YOLOS and reveal that object detection can be accomplished in a pure sequenceto-sequence manner with minimal additional inductive biases. Li et al.[15] propose ViTDet to explore the plain, nonhierarchical ViT as a backbone for object detection. Dai et al.[5] propose a pretext task named random query patch detection to Unsupervisedly Pre-train DETR (UP-DETR) for object detection. IA-RED² [22] introduces an interpretable module for dynamically discarding redundant patches. Lightweight Vision Transformers. As we all know, vision Transformer (ViT) suffers from its high calculation complexity and memory cost. Lu et al. [23] propose an efficient ViT with dynamic sparse tokens to accelerate the inference process. Yin et al.[33] adaptively adjust the inference cost of ViT according to the complexity of different input images. Xu et al.[31] propose a structure-preserving token selection strategy and a dual-stream token update strategy to significantly improve model performance without changing the network structure. Tang et al. [28] presents a top-down layer by layer patch slimming algorithm to reduce the computational cost in pre-trained Vision Transformers. The core strategy of these algorithms and other similar works[11, 13, 19] is to abandon redundant tokens to reduce the computational complexity of the model. In addition to the above models focused on sparsity backbone structure applied on classification tasks, some works[26, 29] lie in reducing the redundant calculation in DETR-like models. Efficient DETR [32] reduces the number of layers of the encoder and decoder by optimizing the structure while keeping the performance unchanged. PnPDETR and Sparse DETR have achieved performance comparable to DETR or Deformable by abandoning background tokens with weak semantics. However, these methods are suboptimal in judging background information and lack enhanced attention to more fine-grained features. 3. Methodology We first describe the overall architecture of FocusDETR. Then, we elaborate on our core contributions: (a) Constructing a scoring mechanism that considers both localization and category semantic information from multiscale features. Thus we obtain two-level explicit discrimination for foreground and fine-grained object tokens; (b) Based on the scoring mechanism, we feed tokens with different semantic levels into the encoder with dual attention, which enhances the semantic information of queries and balances model performance and calculation cost. A detailed analysis of the computational complexity is provided. 3.1. Model Architecture As shown in Fig. 3, Focus-DETR is composed of a backbone, a encoder with dual attention and a decoder. The backbone can be equipped with ResNet [10] or Swin Transformer [20]. To leverage multi-scale features {fi}\/\_₁ (L = 4) from the backbone, where fi Є RC×H₁×W₁, we obtain the feature maps {ƒ1, ƒ2, ƒ3} in three different scales (i.e., 1/8, 1/16, 1/32) and downsample ƒ³ to get ƒ4 (i.e., 1/64). Before being fed into the encoder with dual attention, the multi-scale feature maps {fi}\/\₁ first go through a foreground token selector (Section 3.2) using a series of topdown score modulations to indicate whether a token belongs to the foreground. Then, the selected foreground tokens of each layer will pass through a multi-category score predictor to select tokens with higher objectiveness score by leveraging foreground and semantic information (Section 3.2). These object tokens will interact further with each other and complement the semantic limitation of the foreground queries through the proposed dual attention (Section 3.3). 3.2. Scoring mechanism Foreground Token Selector. Sparse DETR[26] has demonstrated that only involving a subset of tokens for encoders can achieve comparable performance. However, as illustrated in Fig. 4, the token selection provided by Sparse DETR [26] has many drawbacks. In particular, many preserved tokens do not align with foreground objects. We think the challenge from Sparse DETR lies in that its supervision of token selection relies on DAM. The correlation between DAM and retained foreground tokens will be reduced due to learnable queries, which brings errors during training. Instead of predicting pseudo-ground truth [26], we leverage ground truth boxes and labels to supervise the foreground selection inspired by [17]. To properly provide a binary label for each token on whether it appears in foreground, we design a label assignment protocol to leverage the multi-scale features for objects with different scales. In particular, we first set a range of sizes for the bounding boxes of different feature maps, and add the overlap of the adjacent interval by 50% to enhance the prediction near boundary. Formally, for each token t(i,j) with stride sı, where is the index of scale level, and (i, j) is the position in the feature map, we denote the corresponding coordinate (x, y) in the original image as ([ ¾ ] + i · 81, | ¾⁄1 ] + j · 81). Considering the adjacent feature map, our protocol determines the label ((i,j) according to the following rules, i.e., Ть l(i,j) = S1, 10. (x, y) & D Bbox V d) [] (x, y) = D Bbox Ad(i,j) = [rb, re] ¢ =²+= = ∞. (1) where DBbox (x, y, w, h) denotes the ground truth boxes, d(i,j) =max(1,2) = [rf, rl], represents the maximum checkerboard distance between (x, y) and the bounding box center, [r, r] represents the interval of object predicted by the l-layer features and r < rob+1 < r < r²+1 and +1 (r₁+r²), 1 = {0, 1, 2, 3}, ro O and r³ Another drawback of DETR sparse methods is the insufficient utilization of multi-scale features. In particular, the semantic association and the discrepancy in the token selection decisions between different scales are ignored. To fulfill this gap, we construct the FTS module with top-down score modulations. We first design a score module based on Multi-Layer Perceptron (MLP) to predict the foreground score in each feature map. Considering that high-level feature maps contain richer semantic than low-level features with higher resolution, we leverage the foreground score of high-level semantics as complement information to modulate the feature maps of adjacent low-level semantics. As shown in Fig. 5, our top-down score modulations only transmits foreground scores layer by layer through upsampling. Formally, given the feature map fɩ where l = {2, 3, 4}, Si-= MLPF (fi−1(1+ UP(αɩ * S₁))), (2) Top-down Score Modulations = {~ XN Encoder Image Foreground Token Selector Top K (layer-wise) Dual Attention Box Class Deformable Attention XN Decoder Update Self Attention) Multi-Category Top K Score Predictor Query Selection FFN Cross Attention Self Attention COCO-O Figure 3: The architecture overview of the proposed Focus-DETR. Our Focus-DETR comprises a backbone network, a Transformer encoder, and a Transformer decoder. We design a foreground token selector (FTS) based on top-down score modulations across multiscale features. And the selected tokens by a multi-category score predictor and foreground tokens go through the encoder with dual attention to remedy the limitation of deformable attention in distant information mixing. Sparse DETR Ours fffffi MLP S₁ Element-wise Multiplication Number Multiplication S1-UP Sample κα Figure 4: The foreground tokens preserved in different feature maps of Sparse DETR and our Focus-DETR. The red dots indicate the position of the reserved token corresponding to the original image based on the stride. where S indicates the foreground score of the 1-th feature map, UP() is the upsampling function using bilinear interpolation, MLPF (.) is a global score predictor for tokens in all the feature maps, {a} //--¹³ is a set of learnable modulation coefficients, and L indicates the layers of multi-scale feature maps. The localization information of different feature maps is correlated with each other in this way. Multi-category score predictor. After selecting tokens with a high probability of falling in the foreground, we then seek an efficient operation to determine more fine-grained tokens for query enhancement with minimal computational cost. Intuitively, introducing more fine-grained category information would be beneficial in this scenario. Following this motivation, we propose a novel more fine-grained token selection mechanism coupled with the foreground token selection to make better use of the token features. As shown in Fig. 3, to avoid meaningless computation of the background token, we employ a stacking strategy that considers both localization information and category semantic information. Specifically, the product of foreground score and category score calculated by a predictor MLPC (·) will be used as fl-→MLP → Figure 5: The operation of top-down score modulation. For multiscale feature maps, we use a shared MLP to calculate {S1, S2, ...}. S₁ is incorporated in the calculation of S1-1 by a dynamic coefficient a and feature map fi-1. our final criteria på for determining the fine-grained tokens involved in the attention calculation, i.e., Pj = 8; × Cj = 8; × MLPc(T³½³), (3) where and Sj c; represent foreground score and category probabilities of T respectively. Unlike the query selection strategy of two-stage Deformable DETR [37] from the encoder's output, our multi-category probabilities do not include background categories (Ø). We will determine the tokens for enhanced calculation based on the pj. 3.3. Calculation Process of Dual Attention The proposed reliable token scoring mechanism will enable us to perform more fine-grained and discriminatory calculations. After the foreground and fine-grained object tokens are gradually selected based on the scoring mechanism, we first exploit the interaction information of the finegrained object tokens and corresponding position encoding Algorithm 1 Encoder with Dual Attention Input: All tokens Ta, foreground tokens Tƒ, position embedding PEf, object token number k, foregroud score Sf, foreground token index If Output: all tokens T and foreground tokens T'½ after one encoder layer 1: category score Cf ← MLPc (Tƒ) 2: maximum of category score S. + max (Cf) 3: object token score Sp obj = 4: Idx TopK(Sp, k) Se Sf 5: To Tƒ[Idxº¹³], PE。 ← PEƒ[Idxobj] 6: q = k = PE。 + To, v = To 7: TMHSA(q, k, v) 8: ToNorm(v + To) 9: update T in Tƒ according to Idxobj 10: q q = T'ƒ, k = Ta + PEƒ, v =^ 11: TMSDeformAttn (q, k, v) 12: update T in To according to If Ta by enhanced self-attention. Then, the enhanced object tokens will be scattered back to the original foreground tokens. This way, Focus-DETR can leverage the foreground queries with enhanced semantic information. In addition, because of reliable fine-grained token scoring, dual attention in Encoder effectively boosts the performance with only a negligible increase in calculation cost compared to the unsophisticated query sparse strategy. We utilize Algorithm 1 to illustrate the fine-grained feature selection and enhancement process in the encoder with dual attention. 3.4. Complexity Analysis We further analyze the results in Fig. 2 and our claim that the fine-grained tokens enhanced calculation adds only a negligible calculation cost mathematically. We denote the computational complexity of deformable attention in the encoder and decoder as {GDA, GA), respectively. We calculate GDA with reference to Deformable DETR [37] as follows: GDA = O(KC +3MK +C+5K)NC, = (4) where N₁ (N₁ ≤ HW hiwi) is the number of queries in encoder or decoder, K is the sampling number and C is the embedding dims. For encoder, we set Ne as HW, where y is the ratio of preserved foreground tokens. For decoder, we set Nad to be a constant. In addition, the complexity of the self-attention module in decoder is O(2NqdC² + NC). For an image whose token number is approximately 1 × 104, GDA is approximately 7 under the common setting {K {K = 4,C = 256, Ngd = 900, y = GODA 1}. When y equals 0.3, the calculation cost in the Transformer part will reduce over 60%. This intuitive comparison demonstrates that the encoder is primarily responsible for redundant computing. Then we define the calculation cost of the fine-grained tokens enhanced calculation as GOEC: GOECO(2NC² + NC), (5) where No represents the number of fine-grained tokens that obtained through scoring mechanism. When No = 300, GOEC is only less than 0.025, which has a negligible (GDA+GA) impact on the overall model calculation. 3.5. Optimization Like DETR-like detectors, our model is trained in an end-to-end manner, and the loss function is defined as: L = \mÊmatch + \dÂdn + λƒÂƒ + λeÊenc, (6) where match is the loss for pair-wise matching based on Hungarian algorithm, Ian is the loss for denoising models, Ef is the loss for foreground token selector, Lenc is the loss for auxiliary optimization through the output of the last encoder layer, Am, λd, λf, λa are scaling factors. Loss for feature scoring mechanism. Focus-DETR obtains foreground tokens by the FTS module. Focal Loss [17] is applied to train FTS as follow: f = -aƒ(1 - p)"log(pƒ), = (7) where pf represents foreground probability, af 0.25 and Y = 2 are empirical hyperparameters. 4. Experiments 4.1. Experimental Setup Dataset: We conduct experiments on the challenging COCO 2017 [16] detection dataset, which contains 117K training images and 5K validation images. Following the common practice, we report the standard average precision (AP) result on the COCO validation dataset. Implementation Details: The implementation details of Focus-DETR mostly align with the original model in detrex [25]. We adopt ResNet-50 [10], which is pretrained using ImageNet [6] as the backbone and train our model with 8xNvidia V100 GPUs using the AdamW [12] optimizer. In addition, we perform experiments with ResNet-101 and Swin Transformer as the backbone. The initial learning rate is set as 1 × 10−5 for the backbone and 1 × 10-4 for the Transformer encoder-decoder framework, along with a weight decay of 1 × 10−4. The learning rate decreases at a later stage by 0.1. The batch size per GPU is set to 2. For the scoring mechanism, the loss weight coefficient of the FTS is set to 1.5. The MLP c() shares parameters with the corresponding in the decoder layer and is optimized along with the training of the entire network. In addition, we decrease Model Epochs Faster-RCNN[24]42.62.DETR(DC5)[1]43.3 63.AP AP50 AP44.45.9 22.APS APM APL Params GFLOPS FPS 20.45.61.42M25.47.61.41M11.Efficient-DETR[32]44.2 62.48.28.4 47.56.32MAnchor-DETR-DC5[30]44.64.47.24.48.60.19.PnP-DETR(a = = 0.33)[29]42.62.45.22.4 46.42.Conditional-DETR-DC5[21]45.65.48.25.49.62.44M11.Conditional-DETR-V2[3]44.8 65.48.25.48.62.46MDynamic DETR(5 scales) [4]47.65.51.28.49.59.58M DAB-Deformable-DETR[18]46.66.50.30.50.62.44M14.UP-DETR[5]42.63.45.20.47.61.SAM-DETR[35]45.0 65.47.26.49.63.58M24.Deformable DETR[37]46.65.50.28.49.61.40M19.Sparse DETR(a = 0.3)[26]46.65.49.29.49.60.41M23.DN-Deformable-DETR[14]48.67.52.31.52.63.48M18.DINO[36] + Sparse DETR(a = = 0.3) or + Focus-DETR (Ours)(a = 0.3)50.69.55.34.54.64.47M14.48.65.52.30.51.63.47M20.50.4 68.55.34.53.64.48M20.Table 1: Results for our Focus-DETR and other detection models with the ResNet50 backbone on COCO val2017. Herein, a indicates the keep ratio for methods that prune background tokens. All reported FPS are measured on a NVIDIA V100. the cascade ratio by an approximate arithmetic sequence, and the lower threshold is 0.1. We provide more detailed hyper-parameter settings in Appendix A.1.1, including the reserved token ratio in the cascade structure layer by layer and the object scale interval for each layer. Epochs AP AP 50 AP75 |Params GFLOPS Model Faster RCNN-FPN [24] DETR-DC5 [1] Anchor-DETR* [30] DN DETR [14] DN DETR-DC5 [14]108 44.0 63.9 47.500 44.9 64.7 47.45.1 65.45.2 65.60M 60M48.8 58M 48.63M 50 47.3 67.50.63MConditional DETR-DC5 [21] DAB DETR-DC5 [18] Focus-DETR (Ours)50 46.6 67.0 50.45.9 66.8 49.63M63M36 51.4 70.0 55.7 67M Table 2: Comparison of Focus-DETR (DINO version) and other models with ResNet101 backbone. Our Focus-DETR preserve 30% tokens after the backbone. The models with superscript * use 3 pattern embeddings. Model Deformable DETR (priori) + Sparse DETR (a = 0.3) or + Focus-DETR (a = 0.3) Deformable DETR (learnable) + Sparse DETR (a = 0.3) or Focus-DETR (a = 0.3) DN-Deformable-DETR (learnable) + Sparse DETR (a = 0.3) or + Focus-DETR (a = 0.3) DINO (mixed) + Sparse DETR (a = 0.3) or + Focus-DETR (a = 0.3) АР 46.Corr 46.0 0.7211±0.GFLOPS FPS23.46.23.45.43.5 0.5081±0.45.24.23.48.18.47.4 0.5176 0.23.48.23.50.14.48.2 0.5784±0.50.20.20.Table 3: Corr: the correlation of DAM and retained foreground(5k validation set). "priori”: position and content query (encoder selection); "learnable”: position and content query (initialization); "mixed”: position query (encoder selection), content query (initialization). 4.2. Main Results Benefiting from the well-designed scoring mechanisms towards the foreground and more fine-grained object tokens, Focus-DETR can focus attention on more fine-grained features, which further improves the performance of the DETR-like model while reducing redundant computations. Table 1 presents a thorough comparison of the proposed Focus-DETR (DINO version) and other DETR-like detectors [1, 32, 37, 30, 29, 21, 3, 9, 27, 4, 18, 14, 5, 35, 26], as well as Faster R-CNN [24]. We compare our model with efficient DETR-based detectors [29, 26], our Focus-DETR with keep-ratio of 0.3 outperforms PnP-DETR [29] (+7.AP). We apply the Sparse DETR to DINO to build a solid baseline. Focus-DETR outperforms Sparse DETR (+2.AP) when embedded into DINO. When applied to the DINO [36] and compared to original DINO, we lose only 0.5 AP, but the computational cost is reduced by 45% and the inference speed is improved 40.8%. In Fig. 7, we plot the AP with GFLOPS to provide a clear picture of the trade-off between accuracy and computation cost. Overall, Our Focus-DETR (DINO version) achieve state-of-the-art performance when compared with other DETR-like detectors. To verify the adaptability of Focus-DETR to the stronger backbone ResNet-101 [10] and the effect of the ratio of the preserved foreground on model performance, we perform a series of extensive experiments. As shown in Table 2, compared to other DETR-like models [18, 14, 30, 1, 9, 27, 24], Focus-DETR (DINO version) achieves higher AP with fewer GFLOPs. Moreover, using a Swin Transformer pretrained on ImageNet as backbone, we also achieve excellent performance, as shown in Appendix A.2.1. ImgImgImgfall f1 f2 ₤3 ₤ImgImglayerlayerforeground score layerlayerlayerlayer(a) (b) Figure 6: Visualization results of preserved foreground tokens distribution at multi-scale feature maps as shown (a) and k object tokens evolution at different encoder layers as shown (b). {Img1, Img2, Img3, Img4} represent four test images, {ƒ1, ƒ2, ƒ³, ƒ4} represent foreground tokens at four feature maps, {layer 1, layer 2 ...} represent different encoder layers.R101, 30% R50, 50% R50◆ R50, 30%R50, 10% * RR50, 30% R50, 50% +R50, 10% Swin-T R50, 50% DCE-RR50, 30% RDCSRRR50, 10% DCE-RRRRRRGFLOPS DCE-RDCS-RDC-RConditional DETR DAB DETR DN DETR Sparse DETR Deformable DETR DINO Sparse DETR+DINO Focus DETRFigure 7: Performance of recent object detectors in terms of average precision(AP) and GFLOPs. The GFLOPS is measured using 100 validation images. 4.3. Extensive Comparison Sparse DETR is state-of-the-art for lightweight DETRlike models. As mentioned earlier, sparse DETR will cause significant performance degradation when using learnable queries. To verify the universality of Focus-DETR, we compare our model with excellent and representative DETR-like models equipped with Sparse DETR, including Deformable DETR [37], DN-DETR [14] and DINO [36]. In addition to the Sparse DETR, we apply the Sparse DETR to Deformable DETR(two-stage off), DNDeformable DETR and DINO to construct three baselines. We retain all the Sparse DETR's designs for a fair enough comparison, including the auxiliary encoder loss and related loss weight. We also optimize these baselines by adjusting hyperparameters to achieve the best performance. As shown in Table 3, when applying Sparse DETR to Deformable DETR without two-stage, DN-Deformable-DETR and DINO, the AP decreases 1.9, 1.2 and 2.7. We calculate Corr proposed by Sparse DETR that denotes the correlation bewteen DAM and selected foreground token, we calculate the top 10% tokens to compare the gap more intuitively. As shown in Table 3, their Corrs are far lower than original Sparse DETR, which means foreground selector does not effectively learn DAM. Compared to Sparse DETR, Focus-DETR achieves 1.7, 1.1 and 2.2 higher AP with similar latency in Deformable DETR(two-stage off), DN-Deformable DETR and DINO. As shown in Fig. 3, it seems that our encoder using dual attention can be independently embedded into Sparse DETR or other DETR-like models. However, a precise scoring mechanism is critical to dual attention. We added the experiments of applying the encoder with dual attention to Sparse DETR in Appendix A.2.3. Results show us that fine-grained tokens do not bring significant performance gains. 4.4. Ablation Studies We conduct ablation studies to validate the effectiveness of our proposed components. Experiments are performed with ResNet-50 as the backbone using 36 epochs. Effect of foreground token selection strategy. Firstly, simply obtaining the token score using a foreground score predictor without supervision achieves only 47.8 AP and is lower than that (48.2 AP) of DINO pruned by Sparse DETR. As shown in the second row of Table 4, by adding supervision with our improved label assignment strategy, Focus-DETR yields a significant improvement of +1.0 AP. In addition, top-down score modulations optimize the performance of FTS by enhancing the scoring interaction between multi-scale feature maps. As shown in the third row of Table 4, Focus-DETR equipped with the top-down score modulation achieves +0.4 AP. As the visualization shown in Fig. 6 (a), we can observe that our method precisely select the foreground tokens. Moreover, feature maps in different levels tend to focus on objects with different scales. Furthermore, we find that that there is an overlap between the object scales predicted by adjacent feature maps due to our scale overlap setting. We provide more detailed overlap setting details in the Appendix A.1.2. FTS score predictor supervision modulations V cascade dual attention AP | AP50 AP75 FPS 47.8 65.2 52.1 20.20.20.20.50.4 68.5 55.0 20.Table 4: Ablation studies on the FTS and dual attention. FTS is the foreground token selector. Dual attention represents the our encoder structure. Supervision indicates the label assignment from the ground truth boxes. Effect of cascade token selection. When keeping a fixed number of tokens in the encoder, the accumulation of pre-selection errors layer by layer is detrimental to the detection performance. To increase the fault tolerance of the scoring mechanism, we design the cascade structure for the encoder to reduce the number of foreground tokens layer by layer (Section 3.2). As shown in Fig. 6 (b), we can see the fine-grained tokens focusing process in the encoder as the selecting range decreases, which enhances the model's fault tolerance and further improves the model's performance. As illustrated in the fourth row of Table 4, Focus-DETR equipped with cascade structure achieves +0.5 AP. Effect of the dual attention. Unlike only abandoning the background tokens, one of our contributions is reconstructing the encoder using dual attention with negligible computational cost. Tokens obtained after the enhanced calculation supplement the semantic weakness of the foreground queries due to the limitation in distant token mixing. We further analyze the effects of the encoder with dual attention. As shown in the fifth row of Table 4, the encoder with dual attention brings +0.8 AP improvement. These results demonstrate that enhancing fine-grained tokens is beneficial to boost detection performance and the effectiveness of our stacked position and semantic information for finegrained feature selection, as shown in Fig. 1. Top-down Bottom-up AP APAP49.66.54.50.68.55.50.68.54.Table 5: Association methods between scores of multi-scale feature maps. We try top-down and bottom-up modulations. Effect of top-down score modulation. We further analysis the effect of the multi-scale scoring guidance mechanisms in our method. As shown in Table 5, we can observe that utilizing multi-scale information for score prediction brings consistent improvement (+0.5 or +0.7 AP). We also conduct ablation experiments for different score modulation methods. The proposed top-down score guidance strategy (Section 3.2) achieves 0.2 higher AP than bottom-down strategy, which justifies our motivation that using high-level scores to modulating low-level foreground probabilities is beneficial for the final performance. Effect of pruning ratio. As shown in Table 6, we analyze the detection performance and model complexity when changing the ratio of foreground tokens retained by different methods. Focus-DETR achieves optimal performance when keeping the same ratio. Specifically, FocusDETR achieves +2.7 AP than Sparse DETR and +1.4AP than DINO equipped with Sparse DETR's strategies with similar computation cost at 128 GFLOPS. Model Sparse DETR [26] (epoch=50) DINO [36] α AP APS APM APL GFLOPS FPS 0.1 45.3 28.48.60.25.0.2 45.6 28.48.60.24.0.3 46.29.49.60.23.0.4 46.2 28.49.61.21.0.5 46.3 29.49.60.20.0.1 47.5 29.50.62.23.0.2 47.9 30.51.62.21.0.3 48.2 30.51.63.20.0.4 48.4 30.51.63.18.0.5 48.4 30.51.63.18.0.1 48.9 32.52.64.23.0.2 49.8 32.52.64.21.Focus-DETR 0.3 50.33.53.64.20.(epoch=36) 0.4 50.4 34.0 53.0.5 50.5 34.4 53.64.18.64.17.+ Sparse DETR [26] (epoch=36) Table 6: Experiment results in performance and calculation cost when changing the ratio of foreground tokens retained by FocusDETR, Sparse DETR, and DINO+Sparse DETR. 4.5. Limitation and Future Directions Although Focus-DETR has designed a delicate token scoring mechanism and fine-grained feature enhancement methods, more hierarchical semantic grading strategies, such as object boundaries or centers, are still worth exploring. In addition, our future work will be constructing a unified feature semantic scoring mechanism and fine-grained feature enhancement algorithm throughout the Transformer. 5. Conclusion This paper proposes Focus-DETR to focus on more informative tokens for a better trade-off between computation efficiency and model accuracy. The core component of Focus-DETR is a multi-level discrimination strategy for feature semantics that utilizes a scoring mechanism considering both position and semantic information. Focus-DETR achieves a better trade-off between computation efficiency and model accuracy by precisely selecting foreground and fine-grained tokens for enhancement. Experimental results show that Focus-DETR has become the SOTA method in token pruning for DETR-like models. Our work is instructive for the design of transformer-based detectors. References [1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European Conference of Computer Vision, 2020. 1, 2,[2] Qiang Chen, Xiaokang Chen, Gang Zeng, and Jingdong Wang. Group DETR: fast training convergence with decoupled one-to-many label assignment. CORR, abs/2207.13085, 2022. 1,[3] Xiaokang Chen, Fangyun Wei, Gang Zeng, and Jingdong Wang. Conditional DETR V2: efficient detection transformer with box queries. CoRR, abs/2207.08914, 2022. 2,[4] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-toend object detection with dynamic attention. In International Conference on Computer Vision, pages 2968-2977, 2021. 2,[5] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsupervised pre-training for object detection with transformers. In Computer Vision and Pattern Recognition, pages 1601-1610, 2021. 2,[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, pages 248-255, 2009. 5,[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In AAAI Conference on Artificial Intelligence. OpenReview.net, 2021.[8] Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, and Wenyu Liu. You only look at one sequence: Rethinking transformer in vision through object detection. arXiv preprint arXiv:2106.00666, 2021.[9] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai, and Hongsheng Li. Fast convergence of detr with spatially modulated co-attention. In International Conference on Computer Vision, pages 3601-3610, 2021.[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Computer Vision and Pattern Recognition, pages 770-778, 2016. 3, 5,[11] Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, and Kurt Keutzer. Learned token pruning for transformers. In Aidong Zhang and Huzefa Rangwala, editors, Knowledge Discovery and Data Mining, pages 784-794. ACM, 2022.[12] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, International Conference on Learning Representations, 2015.[13] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Bin Ren, Minghai Qin, Hao Tang, and Yanzhi Wang. Spvit: Enabling faster vision transformers via soft token pruning. ArXiv, abs/2112.13890, 2021.[14] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Computer Vision and Pattern Recognition, 2022. 1, 2, 6, 7,[15] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. arXiv preprint arXiv:2203.16527, 2022.[16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet, Tomás Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, European Conference of Computer Vision, volume 8693 of Lecture Notes in Computer Science, pages 740-755. Springer, 2014.[17] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):318–327, 2020. 3,[18] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. DAB-DETR: Dynamic anchor boxes are better queries for DETR. In International Conference on Learning Representations, 2022. 1, 2,[19] Xiangcheng Liu, Tianyi Wu, and Guodong Guo. Adaptive sparse vit: Towards learnable adaptive token pruning by fully exploiting self-attention. CoRR, abs/2209.13802, 2022.[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In International Conference on Computer Vision (ICCV), 2021. 3,[21] Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and Jingdong Wang. Conditional detr for fast training convergence. In International Conference on Computer Vision (ICCV), 2021. 1, 2,[22] Bowen Pan, Yifan Jiang, Rameswar Panda, Zhangyang Ia-red²: Wang, Rogério Feris, and Aude Oliva. Interpretability-aware redundancy reduction for vision transformers. CoRR, abs/2106.12620, 2021.[23] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. In Advances in Neural Information Processing Systems, 2021.[24] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015.[25] Tianhe Ren, Shilong Liu, Feng Li, Hao Zhang, Ailing Zeng, Jie Yang, Xingyu Liao, Ding Jia, Hongyang Li, He Cao, Jianan Wang, Zhaoyang Zeng, Xianbiao Qi, Yuhui Yuan, Jianwei Yang, and Lei Zhang. detrex: Benchmarking detection transformers, 2023.[26] Byungseok Roh, Jae Woong Shin, Wuhyun Shin, and Saehoon Kim. Sparse DETR: Efficient end-to-end object detection with learnable sparsity. In International Conference on Learning Representations, 2022. 1, 2, 3, 6, 8, 11, 12,[27] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set prediction for object detection. In International Conference on Computer Vision, pages 3591-3600, 2021.[28] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, and Dacheng Tao. Patch slimming for efficient vision transformers. In Computer Vision and Pattern Recognition (CVPR), pages 12155-12164, 2022.[29] Tao Wang, Li Yuan, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan. Pnp-detr: Towards efficient visual analysis with transformers. In International Conference on Computer Vision, 2021. 1, 2, 3,[30] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. In AAAI Conference on Artificial Intelligence, 2022. 1,[31] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu, and Xing Sun. Evo-vit: Slow-fast token evolution for dynamic vision transformer. In AAAI Conference on Artificial Intelligence, volume 36, pages 2964–2972, 2022.[32] Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient DETR: improving end-to-end object detector with dense prior. CORR, abs/2104.01318, 2021. 1, 3,[33] Hongxu Yin, Arash Vahdat, Jose M. Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. In Computer Vision and Pattern Recognition (CVPR), pages 10799-10808, 2022.[34] Gongjie Zhang, Zhipeng Luo, Zichen Tian, Jingyi Zhang, Xiaoqin Zhang, and Shijian Lu. Towards efficient use of multi-scale features in transformer-based object detectors. In CVPR, 2023.[35] Gongjie Zhang, Zhipeng Luo, Yingchen Yu, Kaiwen Cui, and Shijian Lu. Accelerating detr convergence via semanticaligned matching. In Computer Vision and Pattern Recognition (CVPR), pages 939-948, 2022. 2,[36] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection, 2022. 1, 2, 6, 7, 8, 11,[37] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In International Conference on Learning Representations, 2021. 1, 2, 4, 5, 6, 7,fi H/SxW/s ☑ max Bax-(w,h) HxW |₤1-H/SL-1 XW/SL-HXW (0.0) Model Figure 8: Visualization of the label assignment process. fi, fi-are feature maps with different scales (s1, S1-1). A. Appendix A.1. More Implementation Details A.1.1 Cascade Structure In order to increase the fault tolerance of our model, we gradually reduce the scope of foreground regions through a cascade structure. As we show in Section 3.4, the computational complexity of deformable attention [37] is linear with the number of preserved tokens. Therefore, there is no significant difference in complexity between the even structures (e.g., {0.4,0.4,0.4,0.4,0.4,0.4} and the cascade structures(e.g., {0.65,0.55,0.45,0.35,0.25,0.15}). Table 7 lists different average keep – ratio and corresponding ratios of different layers designed in this paper. Average keep· - ratio 0.0.0.0.0.Ratios {0.1, 0.1, 0.1, 0.1, 0.1, 0.1} {0.3, 0.3, 0.2, 0.2, 0.1, 0.1} {0.5, 0.4, 0.3, 0.3, 0.2, 0.1} {0.65,0.55,0.45,0.35,0.25,0.15} {0.75,0.65,0.55,0.45,0.35,0.25} Table 7: Detailed cascade keep-ratio desiged by Focus-DETR. A.1.2 Label Assignment Unlike the traditional label assignment scheme for multiscale feature maps, the ranges are allowed to overlap between the two adjacent feature scales to enhance the prediction near the boundary. This strategy increases the number of foreground samples while ensuring that the multiscale feature map predicts object heterogeneity. Intuitively, we assign the interval boundaries to be a series of integer power of two. As shown in Table 8, our overlapping interval setting improves the detection accuracy of the model when compared to non-overlapping ones using similar interval boundaries. As shown in Fig. 8, we present a visualization of the mapping between ground truth boxes in the original image and tokens from feature maps with different scales. Interval AP AP50 AP{[-1, 64], [64, 128], [128, 256], [256, ∞]} |50.2 68.2 54.non-overlapping {[-1, 128], [128,256], [256,512], [512, ∞]} 50.2 68.1 54.55.overlapping {[-1, 64], [64, 256], [128, 512], [256, ∞]} |50.4 68.Table 8: Effect of preset scale intervals of multi-scale feature maps on experimental performance. Interval represents different scale intervals of multi-scale feature maps and ∞ = 999999 in experiments. A.2. Supplementary Experiments A.2.1 Using Swin Transformer as the Backbone When using Swin Transformer [20] as the backbone, FocusDETR also achieves excellent performance. As shown in the following table, when Focus-DETR uses Swin-T as the backbone, the AP reaches 51.9 and achieve 56.0AP using Swin-B-224-22K and 55.9AP using Swin-B-38422K. Compared with Deformable DETR [37] and Sparse DETR [26], our model achieves significant performance improvements, as shown in Table 9. A.2.2 Convergence Analysis In order to better observe the changes in model performance with the training epoch, we measured the changes in Focus-DETR test indicators and compared them with DINO. Experimental results show that Focus-DETR outperforms DINO even at 12 epochs when using ResNet50 as the backbone, as shown in Table 10. In addition, we found that the Focus-DETR reached the optimal training state atepochs due to special foreground selection and fine-grained feature enhancement. A.2.3 Apply Dual Attention to Other Models As we mentioned in Section 4.3 of the main text, a precise scoring mechanism is critical to the proposed dual attention. We add the experiments of applying the encoder with dual attention to those models equipped with Sparse DETR, such as Deformable DETR [37], DN DETR [14] and DINO [36]. As shown in Table 11, the proposed dual attention for fine-grained tokens enhancement brings only +0.3AP in Deformable DETR(two-stage), 0.0AP in Deformable DETR(without two-stage), -0.1AP in DNDeformable-DETR and +0.3 AP in DINO. Results show us that untrusted fine-grained tokens do not bring significant performance gains, which is still inefficient compared to Focus-DETR. A.3. Visualization As shown in Fig. 10, we visualize eight test images with diverse categories, complex backgrounds, overlapping tarModel Epochs Backbone AP APAPAPS APM APL Params GFLOPS FPS Deformable-DETR Swin-T48.68.52.30.51.63.41MSparse DETR Swin-T49.69.53.31.52.65.41M18.Swin-T52.70.57.34.55.67.49M15.Focus-DETR Swin-B-224-22K56.74.61.40.59.72.109M15.Swin-B-384-22K56.75.61.38.60.72.109M8.Table 9: Results for our Focus-DETR using Swin Transformer as the backbone. Herein, Swin-T indicates the tiny version pretrained on ImageNet-1K [6]. Swin-B-224-22K represents the base version pretrained on ImageNet-22K [6] and the resolution of training set is 224. All reported FPS are measured on a NVIDIA V100 GPU. Model Backbone | Epochs | AP | AP50 | AP75 | APS | APM | APL DINO [36] R-R-Focus-DETR R-Swin-T 2222ELIEFIE 24 504 683 548 34.3 53.7 64.24 50.3 68.4 55.1 33.24 51.2 69.7 55.9 32.53.5 64.54.865.12 49.9 68.2 54.3 32.9 52.8 65.24 51.9 70.4 56.6 35.4 54.9 67.36 52.5 70.9 57.5 34.8 55.8 67.Table 10: Focus-DETR uses different backbones at different training epochs and provides comparison results with DINO [36]. R-and R-101 is ResNet backbone, Swin-T represents Swin Transformer of the tiny version. Model Deformable DETR (priori) epoch AP GFLOPS FPS46.+ Sparse DETR (a = 0.3)46.23.+ Sparse DETR(dual attention) (a = 0.3)46.23.or + Focus-DETR (a = 0.3)46.23.Deformable DETR (learnable)45.+ Sparse DETR (a = 0.3)43.24.+ Sparse DETR(dual attention) (a = 0.3)43.23.or + Focus-DETR (a = 0.3)45.23.DN-Deformable-DETR (learnable)48.18.+ Sparse DETR (a = 0.3)47.23.+ Sparse DETR(dual attention) (a = 0.3)47.23.or + Focus-DETR (a = 0.3) DINO (mixed)48.23.50.14.+ Sparse DETR (a = 0.3)48.20.+ Sparse DETR(dual attention) (a = 0.3)48.20.or + Focus-DETR (a = 0.3)50.20.Table 11: Apply dual attention to the classic models equipped with Sparse DETR and compare them with Focus-DETR. gets, and different scales. We analyze the foreground features retained by different encoder layers. Visualization results show that foreground areas focus on a more refined area layer by layer in the encoder. Specifically, the result of Layer-6 captures a more accurate foreground with fewer tokens. The final test results of Focus-DETR are also presented, as shown in the first column. In addition, we compare the differences of multi-scale feature maps retention object tokens due to our label assignment strategy. We also visualize Sparse DETR [26] to demonstrate the performance. As shown in first column of Fig. 9, Focus-DETR can obtain more precise foreground than Sparse DETR. According to the results of {f1, f2, f3, f4}, the multi-scale feature map of FocusDETR can retain tokens according to different object scales, which further proves the advantages of our tag allocation and top-down score modulations strategy. Focus DETR Sparse DETR Focus DETR Sparse DETR Focus DETR Sparse DETR fall fifz f3fxerc xerc Figure 9: Visualized comparison result of foreground tokens reserved in different feature maps. We analyze the difference between FocusDETR and Sparse DETR [26] by using three images with obvious object scale differences. fall is the tokens retained by all feature maps, {ƒ1, f2, ƒ3, ƒ4} represents different feature maps. Image Layer-Layer-2 Layer-Layer-4 Layer-5 Layer-N Figure 10: Visualization result of foreground tokens reserved at each encoder layer, and final detection results are provided. Layer{1, 2, 3, ...} indicates different encoder layers.