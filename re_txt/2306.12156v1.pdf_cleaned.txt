--- --2306.12156v1 [cs.CV] 21 JunarXiv Fast Segment Anything Xu Zhao !Tao Yu !? Wenchao Ding !? Min Li !? Yongqi An |? Ming Tang Yinglong Du !? 12° Jingiao Wang 12:Institute of Automation, Chinese Academy of Sciences, Beijing, China! School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China? Objecteye Inc., Beijing, China® Wuhan AI Research, Wuhan, China* {xu. zhao, yonggi.an,tangm, jqwang}@nlpr.ia.ac.cn {dingwenchao2021, duyinglong2022, yutao2022, limin2021}@ia.ac.cn Abstract The recently proposed segment anything model (SAM) has made a significant influence in many computer vision tasks. It is becoming a foundation step for many high-level tasks, like image segmentation, image caption, and image editing. However, its huge computation costs prevent it from wider applications in industry scenarios. The computation mainly comes from the Transformer architecture at highresolution inputs. In this paper, we propose a speed-up alternative method for this fundamental task with comparable performance. By reformulating the task as segmentsgeneration and prompting, we find that a regular CNN detector with an instance segmentation branch can also accomplish this task well. Specifically, we convert this task to the well-studied instance segmentation task and directly train the existing instance segmentation method using only 1/50 of the SA-1B dataset published by SAM authors. With our method, we achieve a comparable performance with the SAM method at 50x higher run-time speed. We give sufficient experimental results to demonstrate its effectiveness. The codes and demos will be released at https: //github.com/CASIA-IVA-Lab/Fast SAM. 1. Introduction Recently, the Segment Anything Model (SAM) [19] is proposed. It is regarded as a milestone vision foundation model. It can segment any object within the image guided by various possible user interaction prompts. SAM leverages a Transformer model trained on the extensive SA-1B dataset, which gives it the ability to deftly handle a wide range of scenes and objects. SAM opens the door to an exciting new task known as Segment Anything. This task, due to its generalizability and potentiality, has all the makings of becoming a cornerstone for a broad spectrum of future vision tasks. 4 FastSAM SAM 2099ms/img (a) BSDS500 R50 Evaluation vs FPS Speed ts coco, Box AR@1000 Evaluation vs FPS Speed SMeaocio asain fasameren Qos & — FastSaM (TRI) 50x Faster——— & yAMeReal-time Recall at 50% precision (%) Box Average Recall » | Real-time Frames per Second (img/s) Frames per Second (img/s) (b) (©) Figure 1. Comparative analysis of FastSAM and SAM. (a) Speed comparison between FastSAM and SAM on a single NVIDIA GeForce RTX 3090. (b) Comparison on the BSDS500 dataset [1, 28] for edge detection. (c) Box AR@1000 evaluation of FastSAM and SAM on the COCO dataset [25] for the object proposal. Both SAM and FastSAM are tested using PyTorch for inference, except FastSAM(TRT) uses TensorRT for inference. However, despite these advancements and the promising results shown by SAM and subsequent models in handling the segment anything task, its practical applications are still challenging. The glaring issue is the substantial computational resource requirements associated with Transformer (ViT) models, the main part of SAM’s architecture. When compared with their convolutional counterparts, ViTs stand out for their heavy computation resources demands, which presents a hurdle to their practical deployment, especially in real-time applications. This limitation consequently hinders the progress and potential of the segment anything task. Motivated by the high demand from the industrial appli --- --cations for the segment anything model, in this paper we design a real-time solution for the segment anything task, FastSAM. We decouple the segment anything task into two sequential stages which are all-instance segmentation and prompt-guided selection. The first stage hinges on the implementation of a Convolutional Neural Network (CNN)based detector. It produces the segmentation masks of all instances in the image. Then in the second stage, it output the the region-of-interest corresponding the prompt. By leveraging the computational efficiency of CNNs, we demonstrate that a real-time segment of anything model is achievable without much compromising on performance quality. We hope the proposed method would facilitate the industrial applications of the foundational task of segmenting anything. Our proposed FastSAM is based on YOLOv8-seg [16], an object detector equipped with the instance segmentation branch, which utilizes the YOLACT [4] method. We also adopt the extensive SA-1B dataset published by SAM. By directly training this CNN detector on only 2% (1/50) of the SA-1B dataset, it achieves comparable performance to SAM, but with drastically reduced computational and resource demands, thereby enabling real-time application. We also apply it to multiple downstream segmentation tasks to show its generalization performance. On the object proposal task on MS COCO [13], we achieve 63.7 at AR1000, which is 1.2 points higher than SAM with 32x 32 pointprompt inputs, but running 50 times faster on a single NVIDIA RTX 3090. The real-time segment anything model is valuable for industrial applications. It can be applied to many scenarios. The proposed approach not only provides a new, practical solution for a large number of vision tasks but also does so at a really high speed, tens or hundreds of times faster than current methods. It also offers new views for the large model architecture for the general vision tasks. We think for specific tasks, specific models still take advantage to get a better efficiencyaccuracy trade-off. Then, in the sense of model compression, our approach demonstrates the feasibility of a path that can significantly reduce the computational effort by introducing an artificial prior to the structure. Our contributions can be summarized as follow: ¢ A novel, real-time CNN-based solution for the Segment Anything task is introduced, which significantly reduces computational demands while maintaining competitive performance. ¢ This work presents the first study of applying a CNN detector to the segment anything task, offering insights into the potential of lightweight CNN models in complex vision tasks. *« A comparative evaluation between the proposed method and SAM on multiple benchmarks provides insights into the strengths and weaknesses of the approach in the segment anything domain. 2. Preliminary In this section, we give a review of the segment anything model and a clear definition of the segment anything task. Segment Anything Model. In the evolving field of image segmentation, the Segment Anything Model (SAM) [19] is a significant innovation due to its proposed training methodology and performance on large-scale visual datasets. SAM provides a high-precision, class-agnostic segmentation performance, exhibiting distinct capabilities in zero-shot tasks. As a foundation model, it expands the horizons of computer vision by showing not just powerful interactive segmentation methods, but also exceptional adaptability across a variety of segmentation tasks. SAM is a striking example of the potential of foundation models for open-world image understanding. However, while the model’s performance is satisfying, it is worth noting that SAM faces a significant limitation — the lack of real-time processing capability. This restricts its wide application in scenarios where immediate segmentation results are critical. Segment Anything Task. The Segment Anything task is defined as a process whereby an effective segmentation mask is produced given any form of the prompt. These prompts range from foreground/background point sets, rough boxes or masks, free-form text, or any information that indicates the content to be segmented within an image. We have discovered that the segment anything task can be effectively broken down into two stages in the majority of practical applications. The first stage involves detecting and segmenting all objects in the image, like a panoptic segmentation [18] process. The second stage depends on the provided prompts to separate the specific object(s) of interest from the segmented panorama. The decoupling of this task significantly reduces its complexity, thus providing the possibility to propose a real-time segment of anything model. 3. Methodology 3.1. Overview Fig. 2 gives the overview of the proposed method, FastSAM. The method consists of two stages, i.e. the Allinstance Segmentation and the Prompt-guided Selection. The former stage is a basis and the second stage is essentially the task-oriented post-processing. Different from the end-to-end transformers [7,8, 19], the overall method introduces many human priors which match the vision segmentation tasks, like the local connections of convolutions and --- --CNN FPN Backbone Detect Branch Mask Branch | Mask Coeff. Threshold » -0.C- 6846 = +0.‘| ae ios | Point-prompt Box-prompt Text-prompt ext Image Text \ Encoder / \ Encoder j Figure 2. The framework of FastSAM. It contains two stages: All-instance Segmentation (AIS) and Prompt-guided Selection (PGS). We use YOLOv8-seg [16] to segment all objects or regions in an image. Then we use various prompts to identify the specific object(s) of interest. It mainly involves the utilization of point prompts, box prompts, and text prompt. The text prompt is based on CLIP [31]. the receptive-field-relevant object assigning strategies. This makes it tailored for the vision segmentation task and can converge faster on a smaller number of parameters. 3.2. All-instance Segmentation Model Architecture. The architecture of YOLOv8 [16] develops from its predecessor, YOLOVS [15], integrating key design aspects from recent algorithms such as YOLOX [10], YOLOv6 [22], and YOLOv7 [35]. YOLOv8’s backbone network and neck module substitute YOLOv5’s C3 module with the C2f module. The updated Head module embraces a decoupled structure, separating classification and detection heads, and shifts from Anchor-Based to Anchor-Free. Instance Segmentation. YOLOv8-seg applies YOLACT [4] principles for instance segmentation. It begins with feature extraction from an image via a backbone network and the Feature Pyramid Network (FPN) [24], integrating diverse size features. The output consists of the detection and segmentation branches. The detection branch outputs category and bounding box, while the segmentation branch outputs k prototypes (defaulted to 32 in FastSAM) along with k mask coefficients. The segmentation and detection tasks are computed in parallel. The segmentation branch inputs a highresolution feature map, preserves spatial details, and also contains semantic information. This map is processed through a convolution layer, upscaled, and then passed through two more convolution layers to output the masks. The mask coefficients, similar to the detection head’s classification branch, range between -1 and 1. The instance segmentation result is obtained by multiplying the mask coefficients with the prototypes and then summing them up. YOLOv8 can be used in a variety of object detection tasks. With the instance segmentation branch, YOLOv8Seg is born suitable for the segment anything task, which aims to accurately detect and segment every object or region in an image, regardless of the object category. The prototypes and mask coefficients provide a lot of extensibility for prompt guidance. As a simple example, a simple prompt --- --encoder and decoder structure is additionally trained, with various prompts and image feature embeddings as input and mask coefficients as output. In FastSAM, we directly use the YOLOv8-seg method for the all-instance segmentation stage. The more artificial design might bring additional improvements, but we regard it as out of the scope of this work and leave it for future study. 3.3. Prompt-guided Selection Following the successful segmentation of all objects or regions in an image using YOLOv8, the second stage of the segment anything task is to use various prompts to identify the specific object(s) of interest. It mainly involves the utilization of point prompts, box prompts, and text prompts. Point prompt. The point prompt consists of matching the selected points to the various masks obtained from the first phase. The goal is to determine the mask in which the point is located. Similar to SAM, we employ foreground/background points as the prompt in our approach. In cases where a foreground point is located in multiple masks, background points can be utilized to filter out masks that are irrelevant to the task at hand. By employing a set of foreground/background points, we are able to select multiple masks within the region of interest. These masks will be merged into a single mask to completely mark the object of interest. In addition, we utilize morphological operations to improve the performance of mask merging. Box prompt. The box prompt involves performing Intersection over Union (loU) matching between the selected box and the bounding boxes corresponding to the various masks from the first phase. The aim is to identify the mask with the highest IoU score with the selected box and thus select the object of interest. Text prompt. In the case of text prompt, the corresponding text embeddings of the text are extracted using the CLIP [31] model. The respective image embeddings are then determined and matched to the intrinsic features of each mask using a similarity metric. The mask with the highest similarity score to the image embeddings of the text prompt is then selected. By carefully implementing these prompt-guided selection techniques, the FastSAM can reliably select specific objects of interest from a segmented image. The above approach provides an efficient way to accomplish the segment anything task in real-time, thus greatly enhancing the utility of the YOLOv8 model for complex image segmentation tasks. A more effective prompt-guided selection technique is left for future exploration. 4. Experiments In this section, we first analysis the run-time efficiency of FastSAM. Then we experiment with four zero-shot tasks, along with applications in real-world scenarios, efficiency, and deployment. In the first part of the experiments, our goal is to test the similarity in capabilities between FastSAM and SAM. Following SAM, we also experiment with four tasks with different levels: (1) low-level: edge detection, (2) mid-level: object proposal generation, (3) highlevel: instance segmentation, and finally, (4) high-level: segmenting objects with free-form text input. Our experiments also further validate FastSAM’s capabilities with real-world applications and speed. Implementation Details. Unless stated otherwise, the following conditions apply: (1) FastSAM employs a YOLOv8-x [16] model as the main part of its architecture, with the input size of 1024; (2) FastSAM’s training was carried out on 2% of the SA-1B dataset [19]; (3) We train the model for 100 epochs using the default hyper-parameter settings except that the reg_max in the bounding box regression module is changed from 16 to 26 for predicting large instances. 4.1. Run-time Efficiency Evaluation SAM uses the Transformer architecture to construct an end-to-end algorithm. The Transformer is a universal architecture that can represent many forms of mapping functions of various tasks. To segment anything, SAM learns the vision-oriented inductive bias through the learning process on large-scale data. On the contrary, with the human priori knowledge in structure designing, FastSAM obtains a relatively compact model. From Figure 3, The FastSAM generates relatively satisfying results. In Table 1, we report the running speed of SAM and FastSAM on a single NVIDIA GeForce RTX 3090 GPU. It can be seen that FastSAM surpasses SAM at all prompt numbers. Moreover, the running speed of FastSAM does not change with the prompts, making it a better choice for the Everything mode. 4.2. Zero-Shot Edge Detection Approach. FastS AM is assessed on the basic low-level task of edge detection using BSDS500 [1, 28]. Specifically, we select the mask probability map from the results of FastSAM’s all-instance segmentation stage. After that, Sobel filtering [33] is applied to all mask probability maps to generate edge maps. Finally, we conclude with the edge NMS [6] step. Results. The representative edge maps are illustrated in Figure 4. Upon qualitative observation, it becomes evident that despite FastSAM’s significantly fewer parameters (only 68M), it produces a generally good edge map. In comparison to the ground truth, both FastSAM and SAM tend to predict a larger number of edges, including some logical ones that aren’t annotated in the BSDS500. This bias --- --Running Speed under Different Point Prompt Numbers (ms) method params} 1 10 100 E(16x 16) E(32x32*) _-E(64x64) SAM-H [20] | 0.6G | 446 464 627 852 2099SAM-B [20] | 136M | 110 125 230 432 1383FastSAM (Ours) | 68MTable 1. Running Speed (ms/image) of SAM and FastSAM under different point prompt numbers. E: Everything Mode of SAM. *: 32xis the default setting of SAM for many tasks’. Figure 3. Segmentation Results of FastSAM method year | ODS OIS AP R50. HED [37] 2015 -788 -808 -840EDETR [30] 2022 -840 858 896 -zero-shot transfer methods: Sobel filter 1968 539 - Canny [6] 1986 -600 640 -Felz-Hutt [9] 2004. 610 640 560 SAM [19] 2023 -768 -786 -794FastSAM 2023 -750 -790 -793Table 2. Zero-shot transfer to edge detection on BSDS500. Evaluation Data of other methods is from [20]. is quantitatively reflected in Table 2. Table 2 shows that we achieve similar performance with SAM, specifically a higher R50 and a lower AP. 4.3. Zero-Shot Object Proposal Generation Background. Object proposal generation has long been a basic pre-processing step for many computer vision tasks, including general object detection, few-shot object detection, and image understanding. Many famous proposal generation methods witness the evolution of visual recognition in the past decades, as a role of the basic step of visual recognition methods. These proposal generation methods includes EdgeBox [38], Geodesic [21], Selective Search 'https: //github.com/ facebookresearch/ segment anything [34], MCG [2]. These years, many deep learning-based methods are proposed like DeepMask [29], OLN [17]. For example, RCNN-series object detection methods [1 1, 12] adopts the Seletive Search method, and the recently proposed open world detector, UniDetector [36], adopts the OLN method. Though RPN [32] is used by most existing object detectors, it can only generate object proposals of the learned categories, limiting its application in openvocabulary recognition tasks. Therefore, zero-shot object proposal generation is rather important. A good proposed method is important for the good performance of these visual recognition tasks. We directly use the generated bounding boxes of the first stage of FastSAM as the object proposals. To evaluate the performance, we test on LVIS [13] and COCO [25] dataset, following the existing evaluating strategies. Besides this, following the experimental settings of SAM, we also test the mask proposal accuracy by using the all-instance masks of the first stage. Details. We report the results of SAM, ViTDet [23], and our FastSAM on the LVIS dataset. As SAM does not publicize its detailed evaluation codes, we reproduced the categoryagnostic mask and box recall using the official LVIS evaluation code [13]. However, we fail to reproduce the Mask AR results of ViTDet and SAM presented in the SAM’s paper [20]. Nevertheless, we think our evaluation results still reflect several features of FastSAM compared with SAM. --- --_image ground truth Results. The results is shown in Table 3, 4, and 5. The results show that our method has a significant advantage on the box proposal generation tasks. Table 3 presents the Average Recall (AR) of various methods on the COCO validation set. Among these, EdgeBoxes [38], Geodesic [21], Sel.Search [34], and MCG [2] are methods that do not require training, whereas DeepMask [29] and OLN [17] are supervised methods that are trained on VOC categories within the COCO training set, and then tested across all categories. In contrast, our approach and SAM [20] implement a fully zero-shot transfer. From the table, it can be seen that our method and SAM [20] do not perform as well in AR@10 precision compared to previous supervised methods such as OLN [17]. However, in AR @ 1000, our method significantly outperforms OLN [17]. The reason for this is that previous methods were trained on certain categories in COCO, leading to a higher confidence level in these categories during inference. However, since our method and SAM are zero-shot, this results in balanced confidence levels across different categories, thereby recalling more categories not present in COCO. More comparisons can be seen in Figure 5. In Table 4, we report the bbox AR@1000 results of VitDet-H [23], SAM [20], and our method on the LVIS vl dataset. Our method substantially surpasses the most computationally intensive model of SAM, SAM-H E64, by over 5%. However, it falls short compared to VitDet-H [23], which was trained on the LVIS dataset. The reason for these results is that during our training process, we used the ground truth (gt) bounding box (bbox) information as a supervisory signal. SAM [20], on the other hand, only used the mask as a supervisory signal, and its bbox at inference is generated by extracting the outer box from the mask. From Table 5, our mask proposal generation is relatively lower on Recall. We find this mainly results from that our segmentation mask on small-sized objects is not fine FastSAM 4 Hog Rode ond Figure 4. Zero-shot edge prediction on BSDS500. FastSAM achieves comparable results to SAM. AR1IO ARI00 ARI1000 AUC EdgeBoxes [38] 74 17.8 33.8 13.Geodesic [21] 4.0 18.0 35.9 12.Sel.Search [34] 5.2 16.3 35.7 12.MCG [2] 10.1 24.6 39.8 18.DeepMask [29] 13.9 28.6 43.1 21.OLN-Box [17] 27.7 46.1 55.7 34.SAM-H E64 15.5 45.6 67.7 32.SAM-H E32 18.5 49.5 62.5 33.SAM-B E32 11.4 39.6 59.1 27.FastSAM (Ours) 15.7 47.3 63.7 32.Table 3. Comparison with learning-free methods on All Categories of COCO. We report average recall (AR) and AUC of learning free methods, deep learning methods (trained on VOC), and ours vs SAM on All generalization. The scores of competing methods are taken from [17], which test object proposal methods on allCOCO classes. coco AR H Fy 30 30 300 300 300Figure 5. Comparison with OLN [17] and SAM-H [20]. We test object proposal methods on all 80 COCO classes grained enough. We give more detailed discussions in Section 6. --- --bbox AR@method all | small med. large ViTDet-H [23] 65.0 | 532 833°zero-shot transfer methods: SAM-H E64 52.1 36.6 75.1 88.SAM-H E32 50.3 | 33.1 76.2 89.SAM-B E32 45.0 | 29.3 68.7 80.FastSAM (Ours) — 57.1 44.3 771 85.Table 4. Object proposal generation on LVIS vl. FastSAM and SAM is applied zero-shot, i.e. it was not trained for object proposal generation nor did it access LVIS images or annotations. mask AR@method all small | med. large freq. com. rare [23]results after our replication: ViTDet-H [23] 59.9 48.3 |78.1 848 - - SAM-H E64 54.2 39.6 177.9 83.SAM-H E32 51.8 35.2 |78.7 85.2 - - SAM-B E32 45.8 31.1 |70.5 73.6 - - FastSAM (Ours) 49.7 35.6 |72.7 77.6 - - Table 5. Object proposal generation on LVIS v1. FastSAM and SAM are applied zero-shot, i.e. it was not trained for object proposal generation nor did it access LVIS images or annotations. =... . ~—,,.. . | Image ‘Text prompt: “The yellow dog” ‘Text prompt: “The black dog” Figure 6. Segmentation results with text prompts 4.4. Zero-Shot Instance Segmentation Approach. Similarly to the SAM method, we accomplish the instance segmentation task by utilizing the bounding box (bbox) generated by ViTDet [23] as the prompt. As described by Section 3.3, we choose the mask with the highest Intersection over Union (IoU) with the bbox as the predicted mask. Results. Table 6 gives the evaluation results. On this task, we fail to achieve a high AP. We infer that this mainly because of the segmentation mask accuracy or the box-based mask selection strategy. Section 6 gives several examples. 4.5. Zero-Shot Object Localization with Text Prompts Approach. Finally, we consider an even high-level task, i.e. segmenting objects by free-form texts. This experiment is to show the FastSAM ability of processing text prompts like SAM. Different wit SAM, FastSAM doesn’t need to modify COCO [26] LVIS v1 [13] method AP APS APM APL |AP APS APM Apt ViTDet-H [23] 51.0 32.0 543 68.9 [46.6 35.0 58.0 66.zero-shot transfer methods (segmentation module only): SAM 46.5 30.8 51.0 61.7 |44.7 32.5 57.6 65.FastSAM 37.9 23.9 43.4 50.0 |34.5 24.6 46.2 50.Table 6. Instance segmentation results. Fastsam is prompted with ViTDet boxes to do zero-shot segmentation. The fully-supervised ViTDet outperforms SAM, but the gap shrinks on the higherquality LVIS masks. the training procedure. It directly runs text through CLIP’s text encoder and then uses the resulting text embedding to find the most similar mask at inference time. Results. We show qualitative results in Fig. 6. FastSAM can segment objects well based on the text prompts. Nevertheless, the running speed of the text-to-mask segmentation is not satisfying, since each mask region is required to be fed into the CLIP feature extractor. How to combine the CLIP embedding extractor into the FastSAM’s backbone network remains an interesting problem with respect to the model compression. 5. Real-world Applications In this section, we evaluate the performance of FastSAM across different application scenarios and analyze its advantages and limitations. We showcase visualizations of FastSAM’s segmentation using point-prompt, box-prompt, and everything modes, and compare it with SAM and ground truths. Anomaly Detection. As detailed in [3], anomaly detection is a task that aims to distinguish between defective and normal samples in the manufacture. FastSAM is evaluated using the MVTec AD dataset [3], with results displayed in Fig. 7. Under everything mode, FastSAM can segment nearly all regions similar to SAM, but with a lower level of precision compared to SAM. In addition, the mask for the background does not completely cover the entire background, which is an inherent characteristic of YOLACT [4]. By foreground/background points (yellow and magenta points in FastSAM-point respectively) or box-guided selection, FastSAM can segment on the exact defective regions. Salient Object Segmentation. The aim of salient object segmentation [5] is to segment the most attention-grabbing objects from an image. This task is class-agnostic, setting it apart from semantic segmentation. We apply FastSAM to the well-known saliency dataset, ReDWeb-S [27]. As presented in Fig. 8, FastSAM exhibited only a minor difference from SAM under everything mode, as it segment fewer background objects which are irrelevant to the task. By points-guided selection, such --- --original image SAM-point ground truth FastSAM-point SAM-box SAM-everything FastSAM-box FastSAM-everything Figure 7. Application on anomaly detection, where SAM-point/box/everything means using point-prompt, box-prompt, and everything modes respectively. original image SAM-point SAM-box SAM-everything ground truth FastSAM-point FastSAM-box FastSAM-everything Figure 8. Application on salient object segmentation, where SAM-point/box/everything mean using point-prompt, box-prompt, and everything modes respectively. as yellow points in FastSAM-point, we can obtain masks of all objects of interest. The segmentation result of FastSAMpoint is nearly identical to that of the SAM-point and the ground truth, with only minor details lost at the edges. The object of interest can also be selected by box prompt, such as the green box in FastSAM-box. However, it is impossible to select multiple objects with a single box, which even SAM-box cannot realize. Building Extracting. Building Extracting from optical remote sensing imagery has a wide range of applications, such as urban planning. We evaluate FastSAM on the dataset proposed by [14]. As demonstrated in Fig.9, FastSAM performs well in segmenting regularly shaped objects, but segments fewer regions related to shadows compared to SAM. We can also select regions of interest with point-prompt and boxprompt, as presented in FastSAM-point and FastSAM-box. It is worth noting that we position a point in a shadow region in FastSAM-point. However, the correct mask for the building can still be obtained by merging based on this point. This indicates that our method can resist the interference of noise to some extent. --- --ground truth FastSAM-point FastSAM-box FastSAM-everything Figure 9. Application on building extracting, where SAM-point/box/everything means using point-prompt, box-prompt, and everything modes respectively. image FastSAM SAM Figure 10. FastSAM generates finer segmentation masks on the narrow region of the large objects 6. Discussion Generally, the proposed FastSAM achieves a comparable performance with SAM and runs 50x faster than SAM (3232) and 170x faster than SAM (64x64). The running speed makes it a good choice for industrial applications, such as road obstacle detection, video instance tracking, and image manipulation. On some images, FastSAM even generates better masks for large objects, as shown in Figure 10. Weakness. However, as presented in the experiments, our box generation has a significant advantage, but our mask generation performance is below SAM. We visualize these examples in Figure 11. We find that FastSAM has the following features. ¢ The low-quality small-sized segmentation masks have large confidence scores. We think this is because the confidence score is defined as the bbox score of YOLOv8, which is not strongly related to the mask quality. Modifying the network to predict the mask IoU or other quality indicators is a way to improve that. * The masks of some of the tiny-sized objects tend to be near the square. Besides, the mask of large objects may have some artifacts on the border of the bounding boxes. This is the weakness of the YOLACT method. By enhancing the capacity of the mask prototypes or reformulating the mask generator, the problem is expected to solve. Moreover, since we only use 1/50 of all SA-1B datasets, the model’s performance can also be further enhanced by utilizing more training data. 7. Conclusion In this paper, we rethink the segment of anything task and the model architecture choosing, and propose an alternative solution with 50 times faster running speed than --- --SAM-ViT-H (32x32). SAM can solve multiple downstream tasks well. Figure 11. Some examples for the The experiments show that FastStill, there are several weaknesses that can be improved for FastSAM, like the scoring mechanism and the instance maskgenerating paradigm. These problems are left for future study. References i) 2] 3] 4] 5] 6]Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. JEEE transactions on pattern analysis and machine intelligence, 33(5):898-916, 2010. 1,Pablo Arbeldez, Jordi Pont-Tuset, Jonathan T Barron, Ferran Marques, and Jitendra Malik. Multiscale combinatorial grouping. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 328-335, 2014. 5,Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad—a comprehensive real-world dataset for unsupervised anomaly detection. In CVPR, pages 9592-9600, 2019.Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9157-9166, 2019. 2, 3,Ali Borji, Ming-Ming Cheng, Qibin Hou, Huaizu Jiang, and Jia Li. Salient object detection: A survey. Computational Visual Media, 5:117—150, 2019.John Canny. A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence, (6):679-698, 1986. 4,Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In Computer Vision— ECCV 2020: 16th European Conference, Glasgow, UK, Au-ad case of FastSAM. gust 23-28, 2020, Proceedings, Part I 16, pages 213-229. Springer, 2020.Bowen Cheng, Alex Schwing, and Alexander Kirillov. Perpixel classification is not all you need for semantic segmentation. Advances in Neural Information Processing Systems, 34:17864—-17875, 2021.Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based image segmentation. Jnternational journal of computer vision, 59:167—181, 2004.Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 1440-1448, 2015.Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Region-based convolutional networks for accurate object detection and segmentation. JEEE transactions on pattern analysis and machine intelligence, 38(1):142-158, 2015.Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356-5364, 2019. 2, 5,Shunping Ji, Shiqing Wei, and Meng Lu. Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set. JEEE Transactions on Geoscience and Remote Sensing, 57(1):574-586, 2018.Glenn Jocher. Yolov5 by ultralytics, 2020. https: // github.com/ultralytics/yolov5.Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Yolo by ultralytics, 2023. https: //github.com/ultralytics/ ultralytics. 2, 3,Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and Weicheng Kuo. Learning open-world object proposals without learning to classify. IEEE Robotics and Automation Letters, 7(2):5453-5460, 2022. 5,--- ---[26] [27] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollar. Panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9404-9413, 2019.Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 4,Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 5, 6,Philipp Krahenbiihl and Vladlen Koltun. Geodesic object proposals. In Computer Vision—ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 725-739. Springer, 2014. 5,Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangxiang Chu. Yolov6 v3.0: A full-scale reloading, 2023.Jingjing Li, Tianyu Yang, Wei Ji, Jue Wang, and Li Cheng. Exploring denoised cross-video contrast for weaklysupervised temporal action localization. In CVPR, pages 19914-19924, 2022. 5, 6,Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117-2125, 2017.Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision—ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014. 1,Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision—ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.Nian Liu, Ni Zhang, Ling Shao, and Junwei Han. Learning selective mutual attention and contrast for rgb-d saliency detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(12):9026-9042, 2021.David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, volume 2, pages 416-423. IEEE, 2001. 1,Pedro O O Pinheiro, Ronan Collobert, and Piotr Dollar. Learning to segment object candidates. Advances in neural information processing systems, 28, 2015. 5,Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, and Haibin Ling. Edter: Edge detection with transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1402-1412, 2022.(31]Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Jnternational conference on machine learning, pages 8748-8763. PMLR, 2021. 3,Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.Irwin Sobel, Gary Feldman, et al. A 3x3 isotropic gradient operator for image processing. a talk at the Stanford Artificial Project in, pages 271-272, 1968.Jasper RR Uijlings, Koen EA Van De Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. International journal of computer vision, 104:154-171, 2013. 5,Chien-Yao Wang, Alexey Bochkovskiy, and HongYuan Mark Liao. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. arXiv preprint arXiv:2207.02696, 2022.Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao, and Shengjin Wang. Detecting everything in the open world: Towards universal object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11433-11443, 2023.Saining Xie and Zhuowen Tu. Holistically-nested edge detection. In Proceedings of the IEEE international conference on computer vision, pages 1395-1403, 2015.C Lawrence Zitnick and Piotr Dollar. Edge boxes: Locating object proposals from edges. In Computer Vision— ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 391-405. Springer, 2014. 5,