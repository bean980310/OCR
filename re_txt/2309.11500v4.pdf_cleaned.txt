--- --2309.11500v4 [cs.SD] 9 SeparXiv Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning Luoyi Sun* CMIC, Shanghai Jiao Tong University Shanghai, China Shanghai Artificial Intelligence Laboratory Shanghai, China loiesun411@gmail.com Mengyue Wut X-LANCE, Shanghai Jiao Tong University Shanghai, China mengyuewu@sjtu.edu.cn i | Bird wings flap as rustling ' ' and birds chirping in the bac- | ' kground create a serene ambi- | | ance in a garden. ! ' panied by soft music . playing | ' ' ina church. ' | A roaring crowd erupts in che- | | ers and battle cries, creating | approaches, creating a loud: ' and powerful sound in a rail- | way environment. ' ' A train horn blares as a train | Xuenan Xu X-LANCE, Shanghai Jiao Tong University Shanghai, China wsntxxn@sjtu.edu.cn Weidi Xie* CMIC, Shanghai Jiao Tong University Shanghai, China Shanghai Artificial Intelligence Laboratory Shanghai, China weidi@sjtu.edu.cn heep bleat in the distance as Is the air as the musician plays | in a music studio, creating a pleasant ambiance. ' Ss people talk faintly, creating a : pastoral atmosphere in a whe- } at field. ' : Rain falls hard on a surface | | The sound of a bugle playing : 1 as people talk in the distance, | ' is accompanied by the power' creating a soothing ambiance | ' ' ful brass instruments of an ' ' of a rainy day. ' ' orchestra. ; Figure 1: Examples from our proposed Auto-ACD, that is a large-scale audio-language dataset of audio-text pairs (1.5M), with long sentences (18 words) and diverse vocabularies (23K), The captions for the audios in Auto-ACD are generated by an automatic pipeline, comprising elaborate sound descriptions, abundant auditory incidents and unique environmental information. The pivotal sound events are highlighted in bold. “Work done during internship at Shanghai Jiao Tong University. + Corresponding authors. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ’24, October 28-November 1, 2024, Melbourne, VIC, Australia. ABSTRACT Recently, the AI community has made significant strides in developing powerful foundation models, driven by large-scale multimodal datasets. However, for audio representation learning, existing datasets suffer from limitations in the following aspects: insufficient volume, simplistic content, and arduous collection procedures. To © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0686-8/24/10...$15.https://doi.org/10.1145/3664647.--- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. establish an audio dataset with high-quality captions, we propose an innovative, automatic approach leveraging multimodal inputs, such as video frames, audio streams. Specifically, we construct a large-scale, high-quality, audio-language dataset, named as AutoACD, comprising over 1.5M audio-text pairs. We exploit a series of pre-trained models or APIs, to determine audio-visual synchronisation, generate image captions, object detection, or audio tags for specific videos. Subsequently, we employ LLM to paraphrase a congruent caption for each audio, guided by the extracted multimodality clues. To demonstrate the effectiveness of the proposed dataset, we train widely used models on our dataset and show performance improvement on various downstream tasks, for example, audio-language retrieval, audio captioning, zero-shot classification. In addition, we establish a novel benchmark with environmental information and provide a benchmark for audio-text tasks. CCS CONCEPTS + Computing methodologies — Computer vision; + Information systems — Data structures; Information retrieval. KEYWORDS Audio-language Dataset, Audio-language Representation Learning, Audio Captioning ACM Reference Format: Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie. 2024. Auto-ACD: A Large-scale Dataset for Audio-Language Representation Learning. In Proceedings of the 32nd ACM International Conference on Multimedia (MM 24), October 28—November 1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3664647.1 INTRODUCTION In recent literature, foundation models, like CLIP [49], variants of GPT [50], DALL-E 2 [51] and Stable Diffusion [53], have shown tremendous success in various understanding and generation tasks. Despite being different in architectural or algorithmic designs, they are lying on a common basis: large-scale multimodal datasets, for example, MMC4 [66], LAION [55], HowTo100M [39], indicating an emerging transition from model-centric to data-centric representation learning. The former considers pushing the boundaries of model design within the constraints of a predetermined data budget, while the latter focuses on curating large-scale and high-quality datasets in a scalable manner. In the audio community, there have been recent endeavours on constructing audio-language datasets, as demonstrated in Fig. 2. However, existing datasets potentially suffer from two limitations, laborious and complicated collection processes and simplistic descriptions in text. On the one hand, Clotho [11] and AudioCaps [24], which contain audios typically comprising 1 to 3 sound events, accompanied by high-quality text descriptions provided by human annotators. This is are clearly challenging to scale up. On the other hand, LAION-Audio-630K [59] and WavCaps [38] collect large amounts of raw data from online foley websites, then employ sentence templates or keyword-to-caption models to convert the original audio labels into free-form sentences. It is obvious that the resulting language descriptions hardly offer additional information than simple prompts or sound tags. Therefore, models trained Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie Length 30K 57K 400K 630K Aut — Env. Ew. —— WavCaps —Clotho —— Auto-ACD(Ours) ——— LAION-Audio-630K — AudioCaps Figure 2: Comparison with other audio caption datasets. “Length” and “# Vocab.” refer to average length and vocabulary. “Env.” and “Auto.” refer to environmental information and automatic pipeline, respectively. on these datasets are incapable of learning robust audio-language representations. This paper presents our recent efforts for constructing a largescale, high-quality, audio-language dataset, with minimal manual efforts, termed Auto-ACD (Audio Captioning Dataset by Automatic Collection), with massive audio-text pairs (1.5M), long texts (words) and diverse vocabularies (23K). Specifically, an exemplary audio caption ought to encapsulate four varieties of information: the ‘what’ - the nature of the sound perceived, the ‘who’ - the entity producing the sound, the ‘how’ - the characteristics of the sound, and the ‘where’ - the location the sound occurs. Our key insight is that comprehensive understanding of the visual scene is expected to serve as a valuable information source and is sometimes necessary for understanding the audio content. Therefore, we build Auto-ACD on the prior of robust audio-visual correspondence in existing audio-visual datasets, for example, VGGSound [7], AudioSet [13]. Particularly, we initiate an automatic pipeline, that employs a range of publicly available tools or APIs across the general AI community, e.g., vision, language and audio models, to generate comprehensive language descriptions for the audio stream of the given video datasets. Lastly, we employ a large language model (LLM) to collectively assimilate all outputs, identify and eliminate any illogical information, and generate comprehensive descriptions for the audio. As a result, these descriptions not only depict the type of sound and its source, but also describe the auditory attributes and the specific location of its occurrence. --- --Auto-ACD To comprehensively validate auditory representation, for instance, audio events, and ambient information, learned from the text descriptions of Auto-ACD, we conduct experiments from four perspectives: First, we launch a joint audio-language representation learning using InfoNCE loss [18, 46, 65], and evaluate the model through a retrieval task between audio and language, showing noticeable improvement over existing datasets; Second, we conduct zero-shot classification experiments, demonstrating the effectiveness for learning environmental information with our dataset; Third, we benchmark on audio-language generation task, specifically, automatic audio captioning, by training a lightweight mapping network between the pre-trained audio backbone and GPT2 [50], showing superior performance on the widely used benchmark, e.g., Clotho [11]; Fourth, we manually filter a test set and introduce a novel benchmark for audio-language tasks. This benchmark assesses the ability of models to grasp information beyond mere audio tags, for example, the environment and fine-grained categories of sound, we set a baseline for future research in this field. 2 RELATED WORK 2.1 Audio-visual Learning Audio-visual events often occur simultaneously within in-the-wild videos, establishing a profound connection between sound and imagery. [1, 2, 23, 47] employ audio-visual self-supervised learning to leverage audio-visual correspondence for enhancing representation learning. Specifically, [15, 58, 62] learn audio-text representation based on such correspondence. Audio-visual localisation [6, 21, 40, 41, 56] concentrates on identifying the positions of visual sound sources within video. Audio-visual segmentation [12, 29, 32, 42, 64] aims to predict the pixel-wise segmentation masks of sounding objects in visual scenes precisely. Such studies have further demonstrated the intrinsic correlation between audio and visual events in in-the-wild videos, which inspires us to create an audio-language dataset anchored in visual information. 2.2 Audio-visual Dataset Large-scale audio-visual datasets are crucial for effective audio and video understanding. Two datasets are often involved in audiovisual learning: AudioSet and VGGSound. AudioSet [13] is a largescale audio-visual dataset with multiple audio events labelled for each audio clip. It contains over 2M 10-second audio clips. AudioSet is a manually annotated dataset, with the help of a well-structured hierarchical ontology consisting of 632 audio classes guided by literature and manual curation. VGGSound [7] comprises 200K 10second videos for 309 audio classes. This dataset was collected and annotated through an automated pipeline, with each video assigned only one label. In this paper, we aim to provide detailed description for audios, by exploiting both audio and visual cues. 2.3 Audio-language Learning The application of visual-language models in the audio-language arena marks a significant leap forward. Notably, [59] have adapted the CLIP model for audio-language contrastive learning, setting a precedent for innovative cross-modal research. Researchers are not merely focusing on extracting semantic information from audio MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. through tasks such as audio-text retrieval [25, 45], audio classification [20, 48], automatic audio captioning [37, 60], and audio question answering [14, 31]. They are also venturing into more nuanced aspects of auditory perception, including exploring temporal dynamics in sound through audio event detection [3, 28]. This broadening scope encompasses additional auditory attributes such as counting sounds within scenes [44] and classifying environments based on their acoustic characteristics[10]. Undoubtedly, it is paramount to construct a comprehensive, large-scale, high-quality and information-rich audio-language dataset. 2.4 Audio-language Dataset Audio-language tasks, including audio-text retrieval, audio captioning, audio question answering and text-guided audio generation, have greatly benefited from the availability of two widely-used audio captioning datasets: AudioCaps and Clotho. AudioCaps [24], a subset of AudioSet, consists of 50K 10-second-long audio clips, each with a single caption annotated. The annotators were provided with AudioSet tags as hints and videos if necessary. Clotho [11], on the other hand, comprises 6K audio clips lasting between 15 toseconds, each with five captions annotated through a three-step process involving captioning, grammar correction, and rating by human annotators. However, due to the human annotation process, these datasets are limited in size, expensive and time-consuming. LAION-Audio-630K [59] acquires audio and descriptions from online foley websites, including popular platforms like Freesound! and BBC Sound Effects”. WavCaps [38] utilizes ChatGPT to filter and paraphrase these raw descriptions, resulting in a dataset of 400K audio-text pairs with cleaned text data resembling human annotations. The sentence is mostly simple since there is often only one sound event in an audio clip. As a result, models trained on these datasets could only learn the category of sound. To enhance the comprehension capabilities of the audio-text model, we need a more diverse set of textual and audio data. 3 DATASET CONSTRUCTION To develop a large-scale, audio dataset with rich language descriptions, we base on the assumption that visual scene understanding serves as a strong prior. For instance, synchronized videos frequently showcase auditory cues, and visual information serves as a precise representation of the acoustic environment in which the sound happens. In an audio caption, it is desirable to incorporate sound attributes, location, and fine-grained labels. To achieve this, we can leverage publicly available tools or APIs to gather the necessary information for audio description and mutually verify the results. For instance, we can employ an object detection model to identify potential sources of sound, and an environmental classification model to extract scene categories. By extracting a wealth of information, we ensure the maximum coverage of accurate details, providing the language model with ample references. freesound.org/ 2https://sound-effects.bbcrewind.co.uk/ --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. = a . audio video visual-audio label ——WW———_—_*+ train [x¢:0.5921, ¥¢:0.5947, w:0.7879, h:0.3298] train [prob: 0.907], rail transport [prob: 0.858], railroad car, train wagon [prob: 0.833] Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie a train pulling into a station. prompt ©) train_station [prob:0.657] aye passenger_car A train horn blows as a train passes by, creating a loud and distinct sound in a railway station. a train horn blows. train horning Figure 3: Automatic pipeline for Auto-ACD collection. We utilize four open-source computer vision models to extract visual clues from the middle frame of videos, and two open-source audio understanding models to analyze the entirety of the audio content. Consequently, we combine the labels from the original dataset, and leverage Large Language Models (LLMs) to interpret and paraphrase these components into the final description. 3.1 Tools or APIs Given one sample from existing large-scale video datasets, for example, AudioSet or VGGSound [7, 13], i.e., denoted as V = {f;a;y}, where f, a and y correspond to frame sequence, audio stream, and visual or audio labels, respectively. Our goal is to adopt a range of publicly available tools or APIs across the general AI community, ie, using off-the-shelf vision, language and audio models to construct language descriptions for audios, as shown in Fig. 3. In this section, we describe these tools in detail. 3.1.1 Image Captioning. We employ the off-the-shelf BLIP-2 [27] model, which obtains competitive results for image captioning. This tool has the ability to generate captions that encompass the entire image and accurately depict the primary subject or environment. In our case, we input the middle frame of the video into this model. 3.1.2 Object Detection. We use the pre-trained Grounding DINO model [33], to identify objects within the middle frame, and preserve all the detected entities along with their corresponding prediction confidence scores to ensure a comprehensive analysis. 3.1.3 Image Labeling. We adopt the pre-trained OpenAI CLIP [49] model for image classification. In this application, we utilize the prompt: “a photo of a {label}" to generate textual embedding, leveraging the category ontology from ImageNet [9]. 3.1.4 Place Recognition. We employ the pre-trained PlaceCNN [63], to infer the environment context captured in videos. Given the robust correspondence between audio and visual signals, the environment depicted in the video is highly likely to represent the acoustic ambience in which the sound occurs. 3.1.5 Audio Tagging. We use the pre-trained PANNs [26] to predict the tags of sounds within the audio, and preserve the top three predictions with their confidence scores. This represents a crucial source of auditory temporal information, particularly for sounds emanating from entities not visible within the frame. 3.1.6 Audio Captioning. We use the existing AudioCaption [61] model, to generate concise and brief captions. These captions resemble the style of AudioCaps, focusing solely on the categorical information of audio events, devoid of any additional descriptive attributes about the sound. 3.1.7 Audio-visual Synchonisation. We employ the pre-trained Synchformer [22] to conduct synchronization detection between video and audio. This process could filter out samples consisting of irrelevant or unsynchronized video and audio content. In this case, we input both video and audio respectively into this model for analysis. 3.1.8 Existing Audio-Visual Labels. In addition to the predictions from models, we also incorporate the provided labels of existing datasets into our pipeline. For instance, VGGSound [7] gives a single label for each video, while AudioSet [13] provides multiple labels. These labels serve in the original dataset, offering accurate yet incomplete audio-visual information. 3.1.9 Summary. As for the language model, we use the OpenAI ChatGPT®, which demonstrates strong performance in reasoning and inductive summarization, to assemble the above-mentioned. descriptions or labels into comprehensive descriptions for audio. Many works, like BLIP-2[27], show that utilizing existing tools appropriately can significantly enhance the model’s performance. By leveraging audio-visual correspondence and the profound understanding capabilities of LLM, we generate precise audio captioning from the rich multi-modality clues acquired. In this case, we feed in a special prompt as shown in Section 3.2. 3.2 Caption Generation Based on the visual and acoustic clues present in the video, we craft a structured language paragraph, and use it to prompt ChatGPT to generate descriptions for audio. As illustrated in Fig. 4, the process begins with formulating the specific task and criteria for the desired outcome, followed by inputting seven distinctive audio-visual cues Shttps://openai.com/chatgpt --- --Auto-ACD [ Prompting ChatGPT to generate caption for audio I will give you some information from a video and an audio, this audio is separated from the video. There is a caption for an audio, simple audio caption, this sentence simply describe what happens in the audio. There are some audio tags: multiple audio tags, they indicate the audio events in this audio. number indicates the probability. The audio-visual label is dataset visual-audio label. Lextract a key frame from one video, and this is the image caption of this frame: image caption; this is the image label: imave labe\; this is the object detection: object detection; this is the place detection: place label. Now, please help me write one audio caption using common vocabulary and no more than 24 words, providing a description of what happened in the audio, and infer where the audio happened. You can refer the above information, and some visual information is inaccurate and can be ignored. please using the audio-visual label check the audio event in your caption. The sentence you write need to be like these following examples: A bell chimes thrice as birds chirp in the background in the for ‘A lawnmower engine buzzing and stopping to take a few breaks on the lawn. A machine being operated intermittently and people talking in the background in a factory. Figure 4: Detailed prompt provided to ChatGPT. For visualisation purposes, we use different colors to highlight diverse visual-audio cues. into the prompt, accompanied by their corresponding confidence score. Additionally, we provide three sentence examples from AudioCaps or Clotho as instruction. For visualisation purposes, we here use a colour-coded system to distinguish various cues. While generating captions, we explicitly ask ChatGPT to remove information that is inaudible, i.e., illogical and visually oriented elements, for example, colours. As a result, the large language model is able to analyze the scenario from all provided clues, and generate language description for audio, with sound category, and environment. The generated caption results are shown in Table. 1. 3.3 Dataset Filtering AudioSet is vast and diverse, while heavily marred by noise in many instances, for instance, gameplay live streams and explanatory videos. Conversely, VGGSound significantly emphasises the robust correlation between video and audio within the automated collection pipeline, thus requiring no further processing. As shown in Figure. 5, we formulate filtering criteria grounded in both the video-audio correspondence and the original labels. For each filter criterion, we conduct numerous trials followed by a manual verification, each filtering criterion achieves an accuracy rate exceeding 90%, resulting in the removal of 0.4 million videos in total. 3.3.1 Raw labels. AudioSet contains a plethora of explanatory videos with background music, wherein the visual and auditory information often do not correspond. Therefore, we eliminate videos from the multi-labels that encompass both speech and music. MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Table 1: The results of generated captions in Auto-ACD, with accurate content and ample surrounding information. Green and Yellow refer to “where" and “how" the audio sounds like. No. Generated Caption Loud pops and bangs resonate as timbales are being played, creating rhythmic music in a room. 2 Water gurgles and bubbles as a boat glides through, creating “a soothing and peaceful underwater ambience. 3 A woman speaks softly amidst the soothing sound of birds ‘chirping, creating a serene atmosphere in a garden. 4 A motorcycle engine idles before revving up, creating a loud sound in an urban environment. right Synchformer => tolerant right YW error ey video-audio music & speech © labels ——? labels analysis | ==> f others ‘ o Figure 5: Filtering process for AudioSet. We filter the dataset by assessing whether the video and audio are synchronized and analyzing the labels in the original dataset. 3.3.2 Audio-visual synchronisation. To obviate the possibility of fortuitous inference errors, we subject each video to five synchronization evaluations, featuring random variations in start time and offset, with a tolerance threshold established at 0.6 seconds. Synchformer[22] employs a 0.2s offset to ascertain the precise audiovisual synchronization, whereas we utilize a broader offset to determine the audio-visual correspondence. The outcomes are categorized as follows: (1) Predictions aligning with the ground truth are deemed “correct”; (2) Predictions that diverge from the ground truth while with a discrepancy within 0.6 seconds are designated as to be “tolerable”; (3) All other results are termed “error”. To preserve as much data as possible, videos classified as “error” in all five tests are removed from the dataset. 3.4 Dataset Statistics As depicted in Fig. 2, we collect 1.5 million audio-language pairs from AudioSet and VGGSound in total. To the best of our knowledge, Auto-ACD is the first million-level audio-language dataset to date, with train, validation and manually filtered test sets. Auto-ACD surpasses the other datasets in terms of data volume, average sentence length, and contains a relatively wide verbal vocabulary. LAIONAudio-630K[59] sources from user uploads, contains a plethora of noisy details, for instance, device and timestamps, and features an exceptionally extensive vocabulary. Additionally, Auto-ACD stands --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Frogs croaking and with insects Text vocalizing. Encoder 1d aleeAtt a} ede? | e8-e3ede? | eae? | eae? ea Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie prefix embeddings P! audio feature ef, Audio ' Encoder &A train running followed by a train horn. caption tokens Figure 6: Audio-language retrieval model and automatic audio captioning model frameworks. Similar to CLIP, the audiolanguage retrieval model consists of an audio encoder, text encoder, and contrastive loss. The automatic audio captioning model comprises a frozen audio encoder and language model, and a trainable mapping network. as the only audio-language dataset that encompasses environmental information, not only delineates the type and source of sounds but also specifies the location of their occurrence, increasing the richness of contextual details. In supplementary, we present a comparative analysis of captions from LAION-Audio-630K, WavCaps, and Auto-ACD for the same audio sample. Captions in LAION-Audio-630K and WavCaps are concise and contain minimal information beyond the audio tags. In particular, LAION-Audio-630K may include sentences that deviate from common sense, for example, describing “rapping a tree” for an audio tag of “rapping”. WavCaps, on the other hand, exhibits a monotonous sentence structure, such as “... sound can be heard”. In contrast, Auto-ACD features longer sentences that provide a richer depiction of the audio scenes. We conduct a manual check on randomly sampled 200 audiocaptions pairs from Auto-ACD, analyzing the clues from the different open-source tools and the generated captions. We define a clue that contradicts the audio to be erroneous, these tools possess high accuracy, the average accuracy is 81.3%. Furthermore, we conduct a manual check on randomly sampled 1000 audio-captions pairs, and find that 92.4% captions correspond with audio, just 5.3% incorrect words need to be modified, and only 4.4% captions contain inaudible information. These results indicate that our proposed approach enables high-quality, scalable caption generations, with few incorrect or inaudible information. 4 ARCHITECTURE We construct architectures targeting two general audio-language tasks, namely, audio-language contrastive learning, and automatic audio captioning, to further validate the effectiveness of Auto-ACD. In Section 4.1, we provide a detailed exposition of the architecture for audio-language contrastive learning. Further in Section 4.2, we introduce the framework for lightweight automatic audio captioning along with its loss function. 4.1 Audio-Language Constrastive Learning To validate the efficacy of our proposed dataset, we train an audiolanguage model with standard contrastive learning, e.g., infoNCE [49] loss, as shown in Fig.6. Specifically, we employ the pre-trained HTSAT [8] as the audio encoder, and the pre-trained RoBERTa [35] as the language encoder. Both encoders were initialised from the pre-trained CLAP model [59], and further finetuned on our dataset. We term our final model as Audio-Text Retrieval (ATR). Given an audio-text pair (a’, t'), we utilise audio encoder Aenc and text encoder Jenc to extract audio embedding e/, and text embedding el, respectively: fene(t') 4 = Aenc(a’), e} = The model is then trained with contrastive loss, wherein the paired audio and language embeddings are treated as positive, and unpaired ones as negative, with the following loss function: N DL exp where Tt represents the learnable temperature parameters. At training phase, we introduced word-level text masking, that is to randomly mask words within the sentences, before feeding into the text encoder. 4.2 Automatic Audio Captioning To demonstrate the effectiveness of our pre-trained audio backbone, we also use audio captioning for evaluation. Inspired by ClipCap [43] and AutoADs [16, 17], we adopt a lightweight audio captioning model, where both the audio backbone and language model (GPT-2) are fixed, and only a mapping network is trained, as shown in Fig. 6. --- --Auto-ACD MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Table 2: The audio-text retrieval results on AudioCaps, Clotho and ACD test sets. “basic”, “LA.” “Wav.” and “ACD" refer to the combination of AudioCaps and Clotho (basic), LAION-Audio-630K (LA), WavCaps (Wav) and Auto-ACD (ACD), respectively. “ACDys” is a subset of Auto-ACD, curated from VGGSound. “ * FT” refers to fine-tuning the model on the target dataset. AudioCaps Test Clotho Test Auto-ACD Test Train Set Model Audio—Text Text— Audio R@1 R@10 R@1 R@Audio—Text Text—Audio Audio—Text Text—Audio R@1 R@10 R@1 R@10 R@1 R@10 R@1 R@basic+LA.[59] HTSAT-RoBERTa 45.0 88.0 36.2 82.24.2 66.9 17.2 55.4 20.0 65.0 17.9 59.basic+Wav.[38] _ HTSAT-BERT 51.7 90.6 39.7 86.1 23.4 63.4 19.5 58.2 - - - basict ACDys HTSAT-RoBERTa 50.5 90.6 39.8 86.9 24.2 62.9 20.0 58.9 39.2 86.2 39.6 85.basictACD HTSAT-RoBERTa 53.7 91.7 39.5 85.4 17.7 52.6 15.3 52.1 47.1 91.2 49.0 92.basictACD*FT HTSAT-RoBERTa 56.3 93.9 42.7 88.5 26.2 67.5 21.7 61.7 - - - Given an audio-text pair (a',c'), we use the pre-trained audio encoder to extract audio features e) = Aenc(a'), and we convert the caption into a token sequence, cl... 13h where k indicates the maximal length of text. Then, we design a mapping network fmap to transform the extracted embedding into a set of prefix embeddings: P* = finap(€a)We take the prefix embedding set as the condition for predicting the next token with an auto-regressive language model. There fore, during training, we minimize the negative log-likelihood of predicting the correct word: N ¢ L= =D) Y tog po (6) | Phela--se5-1) i=l j=l where @ represents the trainable parameters. 5 EXPERIMENTS In this section, we evaluate three tasks, namely, audio-language retrieval, audio captioning and zero-shot classification. 5.1 Audio-language Retrieval 5.1.1 Dataset. We conduct audio-text retrieval experiments across several datasets: AudioCaps, Clotho, Auto-ACDys, and Auto-ACD. The distributions for the train, validation, and test sets in AudioCaps, Clotho, and Auto-ACD are 50K/495/975, 3.8K/1045/1045, and 1.5M/2K/1K data pairs, respectively. Auto-ACDys is a subset of Auto-ACD, containing 190K data pairs exclusively sourced from VGGSound. Notably, in the case of Clotho, and AudioCaps (validation and test set), each data pair consists of one audio sample accompanied by five corresponding captions, while the remaining data pairs only comprise one audio-caption pair. 5.1.2 Auto-ACD Benchmark. In addition to the Auto-ACD training set, we also randomly selected 2K data samples to form the validation set and 1K samples for the test set. We conduct a manual verification of the test set, by removing incorrect information from the language descriptions and rewriting inappropriate vocabulary expressions. This test set is used for evaluating both audio-language retrieval and automatic audio captioning tasks. 5.1.3 Metrics. In order to validate the rich and accurate information of our dataset, we compare the traditional metrics, Recall@k performance, on commonly used datasets, for example, AudioCaps and Clotho. We also adopt these metrics on the Auto-ACD test set, offering a comprehensive overview. 5.1.4 Training Details. We train our proposed Audio-Text Retrieval (ATR) model for 20 epochs, employing a batch size of 768, and utilizing the Adam optimizer with a warm-up phase, and an initial learning rate of 1e-4 with a cosine learning rate decay schedule. We use the same hyperparameters as those in the existing CLAP model configuration. The dimensions of both the audio encoder and text encoder output are 512. Additionally, we introduce 25% random masking on words in the sentences and randomly apply augmentations such as Noise and Gain to 50% of audio samples to enhance the model training. We further fine-tune the model on specific datasets, for example, Clotho and AudioCaps, with an initial learning rate of 2e-5 for 15 epochs. 5.1.5 Results. As shown in Table.2, we can draw the following key observations: (i) comparing with training on Laion-Audio-630K, training on our proposed Auto-ACDys dataset leads to a significant improvement in Recall@k metrics on AudioCaps and Auto-ACD benchmarks. (ii) training on Auto-ACD and fine-tuning on specific datasets, which is not applicable to Auto-ACD benchmark, results in a remarkable performance gain. This improvement is particularly evident when evaluating the model on the test set of AudioCaps, as AudioCaps is a subset of AudioSet and shares a similar data distribution with Auto-ACD. Such fine-tuning processes enable the model to acquire a more comprehensive understanding of both audio and text information, thus enhancing retrieval performance. (iii) on the Auto-ACD benchmark, characterized by a more diverse lexicon and abundant language description, training on Auto-ACD datasets significantly outperforms the model trained on Laion-Audio-630K. 5.2 Automatic Audio Captioning 5.2.1. Dataset. In addition to the datasets mentioned in Section 5.1, we also use the MACS dataset [36], which comprises 3.9K audio-text data pairs, with each audio accompanied by two to five captions and several audio tags. In total, we train the automatic audio captioning model utilizing a total of 58k data pairs from Clotho, AudioCaps and MACS, and evaluate on Clotho and Auto-ACD test set. --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. 5.2.2 Metrics. In addition to conventional captioning metrics, for example, Meteor [5], RougeL [30], Spider [34], we incorporate SentenceBERT [52] as additional evaluation metrics, that not solely rely on lexical alignment, but rather prioritize the semantic resemblance and accuracy of the captions’ content. 5.2.3 Training Details. We devise two mapping networks, MLP and transformer, and fine-tune the parameters of GPT during the training process. We set the number of prefixes to be 8, each with a dimension of 512. We train this audio captioning model on the MACS [36], Clotho and AudioCaps for 15 epochs with a batch size of 128 and an initial learning rate of 5e-4. In this task, we compare the audio encoder from our pre-trained audio-text retrieval model and the pre-trained CLAP [59], by only training the mapping network of both models on the benchmark datasets, namely, Clotho, and Auto-ACD. 5.2.4 Results. As shown in Table. 3, we can draw two observations: (i) the automatic audio captioning model, with the audio encoder initialised from our pre-trained audio-text retrieval model, shows improved performance across all evaluation metrics than baseline. (ii) there is a more pronounced outcome when evaluated on Auto-ACD: the baseline approach’s performance oversees a sharp decrease in the test set of Auto-ACD. We conjecture this is because the baseline features extracted from the CLAP model lack detailed descriptions of environmental information. While captioning model based on our pre-trained audio-text retrieval model shows a significant performance improvement, and is able to infer where the sound occurs precisely. This observation signifies that Auto-ACD showcases an extensive lexicon, enabling the portrayal of a given audio using various sentence structures. On the other hand, it illustrates that models trained on our dataset will deduce the context in which the sound emanates. Table 3: The automatic audio captioning results on Clotho and Auto-ACD test sets. “S-BERT” refers to SentenceBERT, “Env.” refers to whether the predicted captions contain environmental information. Eval Set Audio Encoder Meteor RougeL Spider S-BERT Env. Cloth CLAP 155 349 206 460 x one Ours 166 36.2 212 474 x CLAP 99 230 196 8&7 x Auto-ACD Ours 213 37.9 56.7 101 V 5.3. Zero-shot Classification 5.3.1 Dataset. Auto-ACD stands out for integrating its incorporation of environmental information within its text descriptions. Following the training on Auto-ACD, we conduct environmental classification in four distinct scenarios: (i) a collection of samples from the AudioSet evaluation set, annotated with child classes of "Acoustic Environment" within the AudioSet ontology, referred to as AudioSet Env. To prevent data leakage, here we exclusively utilize the model pre-trained on Auto-ACDys for this experiment; (ii) the urban acoustic scene dataset [19], known as DCASE 2020 Mobile, previously utilized in the DCASE 2020 challenge. (iii) the popular Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie urban sound event classification dataset, UrbanSound 8k [54]; (iv) the music genre classification dataset, GTZANGenres [57]. 5.3.2. Metrics. We approach zero-shot classification as an audiotext retrieval experiment, employing a conventional paraphrasing template: "The sound in [environment label] / of [label]." We utilize Recall@1 as the metric for evaluating the environment classification outcomes in this experiment. 5.3.3 Results. The experimental results, as illustrated in Table. 4, highlight the superior environmental recognition capability of ATR pre-trained on Auto-ACD in comparison to CLAP. Notably, on the AudioSet Env, our model significantly outperforms CLAP, even though we only utilize Auto-ACDys, for pre-training without any data leakage from AudioSet into our training dataset, further serving as a testament to the rich and accurate environmental information in Auto-ACD. The results on UrbanSound 8K and GTZANGenres shows that in addition to the audio events, the captions may also include more information, for example, diverse environment descriptions, fine-grained musical genres. wan Table 4: Zero-Shot Acoustic Environment Classification. refers to pre-training model on Auto-ACDys. “US-8K” refers to UrbanSound 8K. Model AudioSetEnv DCASE US-8K GTZANGenres CLAP 19.5 32.2 75.0 31.Ours 39.5" 36.5 76.2 45.6 CONCLUSION In this paper, we present an automatic pipeline for audio caption generation, accompanied by a large-scale and comprehensive audio captioning dataset comprising 1.5M data pairs. Furthermore, we evaluate the performance of various audio-language models on our dataset to authenticate the effectiveness, and provide a manually verified test set along with a benchmark for audio-language tasks. These experimental findings unveil the wealth of information and precise descriptions inherent in our data, facilitating the models to learn more robust audio-language representations. Owing to the fact that a portion of our dataset originates from VGGSound, procured through an automatic pipeline. The transformation from online videos to precise audio-language pairs has evolved into a thoroughly automated and replicable procedure. Consequently, the acquisition of an expanded corpus of audio-language datasets is now a straightforward endeavour. Furthermore, as opensource computer vision models and Large Language Models (LLMs) undergo continuous refinement and advancement, the capacity to extract more precise audio-visual indicators improves, subsequently enhancing the precision of inferences and the quality of paraphrasing the final audio captions. ACKNOWLEDGMENTS This work is supported by National Key R&D Program of China (No.2022ZD0161400). --- --Auto-ACD REFERENCES20.22, 23, Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovié, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. 2020. Self-supervised multimodal versatile networks. Advances in Neural Information Processing Systems 33 (2020), 25-37. Relja Arandjelovic and Andrew Zisserman. 2017. Look, listen and learn. In Proceedings of the IEEE International Conference on Computer Vision. 609-617. Elham Babaee, Nor Badrul Anuar, Ainuddin Wahid Abdul Wahab, Shahaboddin Shamshirband, and Anthony T Chronopoulos. 2017. An overview of audio event detection methods from feature extraction to classification. Applied Artificial Intelligence 31, 9-10 (2017), 661-714. Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman. 2023. WhisperX: Time-Accurate Speech Transcription of Long-Form Audio. Proceedings of the INTERSPEECH Conference (2023). Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. 65-72. Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. 2021. Localizing visual sounds the hard way. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16867-16876. Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. 2020. Vggsound: A large-scale audio-visual dataset. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 721-725. Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2022. HTS-AT: A hierarchical token-semantic audio transformer for sound classification and detection. In Proceedings of the IEEE International Con‘ference on Acoustics, Speech and Signal Processing. 646-650. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 248-255. Biyun Ding, Tao Zhang, Chao Wang, Ganjun Liu, Jinhua Liang, Ruimin Hu, Yulin Wu, and Difei Guo. 2023. Acoustic scene classification: a comprehensive survey. Expert Systems with Applications (2023), 121902. Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. 2020. Clotho: An audio captioning dataset. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 736-740. Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong Lu. 2024. Avsegformer: Audio-visual segmentation with transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 12155-12163. Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology and human-labeled dataset for audio events. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 776-780. Di Hu Guangyao li, Yixin Xu. 2023. Multi-scale attention for audio question answering. Proceedings of the INTERSPEECH Conference (2023). Andrey Guzhoy, Federico Raue, Jorn Hees, and Andreas Dengel. 2022. Audioclip: Extending clip to image, text and audio. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 916-980. Tengda Han, Max Bain, Arsha Nagrani, Giil Varol, Weidi Xie, and Andrew Zisserman. 2023. AutoAD II: The Sequel - Who, When, and What in Movie Audio Description. In Proceedings of the IEEE/CVF International Conference on Computer Vision. Tengda Han, Max Bain, Arsha Nagrani, Giil Varol, Weidi Xie, and Andrew Zisserman. 2023. AutoAD: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18930-18940. Tengda Han, Weidi Xie, and Andrew Zisserman. 2019. Video representation learning by dense predictive coding. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops. 1-10. Toni Heittola, Annamaria Mesaros, and Tuomas Virtanen. 2020. TAU Urban Acoustic Scenes 2020 Mobile, Development dataset. https://doi.org/10.5281/zenodo.Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. 2017. CNN architectures for large-scale audio classification. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 131-135. Xixi Hu, Ziyang Chen, and Andrew Owens. 2022. Mix and localize: Localizing sound sources in mixtures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10483-10492. Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. 2024. Synchformer: Efficient synchronization from sparse cues. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 5325-5329. Simon Jenni, Alexander Black, and John Collomosse. 2023. Audio-visual contrastive learning with temporal self-supervision. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 7996-8004.25, 26.30,32) 33,35, 36.38, 39)43,45,MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. 2019. Audiocaps: Generating captions for audios in the wild. In Proceedings of theConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 119-132. A Sophia Koepke, Andreea-Maria Oncescu, Joao F Henriques, Zeynep Akata, and Samuel Albanie. 2022. Audio retrieval with natural language queries: A benchmark study. IEEE Transactions on Multimedia 25 (2022), 2675-2685. Qiugiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley. 2020. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28 (2020), 2880-2894. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the International Conference on Machine Learning. 1973019742. Kang Li, Yan Song, Li-Rong Dai, lan McLoughlin, Xin Fang, and Lin Liu. 2023. Ast-sed: An effective sound event detection method based on audio spectrogram transformer. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 1-5. Kexin Li, Zongxin Yang, Lei Chen, Yi Yang, and Jun Xiao. 2023. Catr: Combinatorial-dependence audio-queried transformer for audio-visual video segmentation. In Proceedings of the 31st ACM International Conference on Multimedia. 1485-1494, Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics. 150-157. Samuel Lipping, Parthasarathy Sudarsanam, Konstantinos Drossos, and Tuomas Virtanen. 2022. Clotho-aqa: A crowdsourced dataset for audio question answering. In Proceedings of the 30th European Signal Processing Conference. 1140-1144. Jinxiang Liu, Yu Wang, Chen Ju, Chaofan Ma, Ya Zhang, and Weidi Xie. 2024. Annotation-free audio-visual segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 5604-5614. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. 2023. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303,05499 (2023). Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. 2017. Improved image captioning via policy gradient optimization of spider. In Proceedings of the IEEE International Conference on Computer Vision. 873-881. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.(2019). Irene Martin Morato and Annamaria Mesaros. 2021. Diversity and bias in audio captioning datasets. In Detection and Classication of Acoustic Scenes and Events. 90-94. Xinhao Mei, Xubo Liu, Mark D Plumbley, and Wenwu Wang. 2022. Automated audio captioning: an overview of recent progress and new challenges. EURASIP Journal on Audio, Speech, and Music Processing 2022, 1 (2022), 26. Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D Plumbley, Yuexian Zou, and Wenwu Wang. 2023. WavCaps: A chatGPTassisted weakly-labelled audio captioning dataset for audio-language multimodal research. arXiv preprint arXiv:2303.17395 (2023). Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. 2019. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2630-2640. Shentong Mo and Pedro Morgado. 2022. A closer look at weakly-supervised audiovisual source localization. Advances in Neural Information Processing Systems(2022), 37524-37536. Shentong Mo and Yapeng Tian. 2023. Audio-visual grouping network for sound localization from mixtures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10565-10574. Shentong Mo and Yapeng Tian. 2023. AV-SAM: Segment anything model meets audio-visual localization and segmentation. arXiv preprint arXiv:2305.(2023). Ron Mokady, Amir Hertz, and Amit H Bermano. 2021. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734 (2021). Michael Nigro and Sridhar Krishnan. 2023. SARdBScene: Dataset and Resnet Baseline for Audio Scene Source Counting and Analysis. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 1-5. Andreea-Maria Oncescu, A Koepke, Joao F Henriques, Zeynep Akata, and Samuel Albanie. 2021. Audio retrieval with natural language queries. arXiv preprint arXiv:2105,02192 (2021). Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia.50.52) 53)55,Andrew Owens and Alexei A Efros. 2018. Audio-visual scene analysis with self-supervised multisensory features. In Proceedings of the European Conference on Computer Vision. 631-648. Kamalesh Palanisamy, Dipika Singhania, and Angela Yao. 2020. Rethinking CNN models for audio classification. arXiv preprint arXiv:2007.11154 (2020). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, etal. 2021. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning. 8748-8763. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAlI blog 1,8 (2019), 9. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 1, 2 (2022), 3. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjérn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684-10695. Justin Salamon, Christopher Jacoby, and Juan Pablo Bello. 2014. A dataset and taxonomy for urban sound research. In Proceedings of the 22nd ACM International Conference on Multimedia. 1041-1044. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems 35 (2022), 25278-25294. Weixuan Sun, Jiayi Zhang, Jianyuan Wang, Zheyuan Liu, Yiran Zhong, Tianpeng Feng, Yandong Guo, Yanhao Zhang, and Nick Barnes. 2023. Learning audio-visual source localization via false negative aware contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6420-6429. 57, 58,60,62, 63,65, 66. Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie George Tzanetakis and Perry Cook. 2002. Musical genre classification of audio signals. IEEE Transactions on Speech and Audio Processing 10, 5 (2002), 293-302. Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. 2022. Wav2clip: Learning robust audio representations from clip. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 45634567. Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing. 1-5. Xuenan Xu, Mengyue Wu, and Kai Yu. 2022. A comprehensive survey of automated audio captioning. arXiv preprint arXiv:2205.05357 (2022). Xuenan Xu, Zhiling Zhang, Zelin Zhou, Pingyue Zhang, Zeyu Xie, Mengyue Wu, and Kenny Q Zhu. 2023. Blat: Bootstrapping language-audio pre-training based on audioset tag-guided synthetic data. In Proceedings of the 31st ACM International Conference on Multimedia. 2756-2764. Yanpeng Zhao, Jack Hessel, Youngjae Yu, Ximing Lu, Rowan Zellers, and Yejin Choi. 2022. Connecting the Dots between Audio and Text without Parallel Data through Visual Knowledge Transfer. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics. 4492-4507. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. 2017. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 6 (2017), 1452-1464. Jinxing Zhou, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, and Yiran Zhong. 2022. Audiovisual segmentation. In Proceedings of the European Conference on Computer Vision. 386-403. Xiao Zhou, Yujie Zhong, Zhen Cheng, Fan Liang, and Lin Ma. 2023. Adaptive sparse pairwise loss for object re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 19691-19701. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. 2024. Multimodal c4: An open, billion-scale corpus of images interleaved with text. Advances in Neural Information Processing Systems 36 (2024). --- --Auto-ACD 7 DATASET ANALYSIS In this section, we conduct a more thorough analysis of the proposed dataset, Auto-ACD. In Section 7.1 and Section 7.2, we compare Auto-ACD with existing audio-language datasets, and discuss the necessity for data filtering. In Section 7.3, we present the distribution of vocabulary. In Section 7.4 and Section 7.5, we compare the captions among Laion-Audio-630K, WavCaps and Auto-ACD, and manually check the quality of a subset of Auto-ACD. In Section 7.6, we present additional examples from Auto-ACD. 7.1 Dataset Statistics In total, we have collected 1.5 million audio samples, each with a duration of 10 seconds, accompanied by one detailed caption. As indicated in Table 5, in comparison to other datasets, AutoACD not only surpasses them significantly in terms of volume, but also contains a longer average sentence length. It stands as the only large-scale dataset that includes environmental information within its descriptions. Laion-Audio-630k may possess a higher vocabulary count, but the majority of its lexicon comprises useruploaded device information and timestamps, which are irrelevant to the audio content. Table 5: Comparation with other audio caption datasets. “Length” and “# Vocab.” refer to average length and vocabulary. “Env.” and “Auto.” refer to environmental information and automatic pipeline, respectively. Dataset Quantity Length # Vocab. Env. Auto. AudioCaps [24] 57K 8.8 5K x x Clotho [11] 30K 11.3, 4K ix LAION-Audio-630K [59] 630K 7.3 311K x v WavCaps [38] 400K 78 29K x v Auto-ACD (ours) 15M 18.1 22K dov 7.2 Dataset Filtering Our data collection procedure relies on strong audio-visual correspondence. However, many entries within AudioSet contain considerable noise, posing challenges to achieving such coherence, for instance, videos with background music, serene speeches, videos depicting gameplay or software tutorials. Such videos typically only encompass two types of audio events: speech and music. Consequently, the generated captions often contain sparse information and exhibit high error rates. We employ an analysis of audio-visual labels and synchronization model to filter these samples. The specific details of this filtering process are described in Section 3.3 of the main text. In Figure. 7, we present some examples of video frame sequences and the outcomes of audio ASR (Automatic Speech Recognition) by WhisperX [4] for the excluded data. The majority of discarded entries are due to the audio and video are not unrelated or not synchronized. 7.3 Dataset Corpus We visualize the captions for our dataset with word cloud. As depicted in Figure. 8, and the common audio tags, man speak and MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. music play still predominate in frequency within our data. It is noteworthy that terms describing settings, such as small room and music studio, also emerge with considerable frequency. These are a plethora of audio events, such as birds chirping, engine idling and water splashing, further demonstrating the diverse audio events in Auto-ACD. “Anyways, so I run pretty much full arms except for drums of war and one point in...” “Would be equal to 3 divided by 5. So now looking at the tangent of theta. There's two different ways we can do this now.”” [only background music] [only background music] Figure 7: Samples deleted in filter processing. The text on the right side represents transcriptions of speech from the audio in the video, processed using WhisperX. Br, Stn it: delivers’ 11 water splashes man man” Bateioretely epee cn fid 4,gaEs fiusic Creating to by < al oS. sesoFigure 8: Corpus in Auto-ACD. The higher the frequency of occurrence, the larger the font size of the respective word. 7.4 Dataset Comparison In Table. 6, we show example captions from LAION-Audio-630K, WavCaps, and Auto-ACD for the same audio sample. Since the original sounds of the three datasets overlap, we select the descriptions of the same audio in different datasets for comparison. Specifically, LAION-Audio-630K employs a keyword-to-caption model to transform the tag labels into captions. WavCaps utilizes ChatGPT to rephrase the tag labels into simple captions. The captions in LAIONAudio-630K and WavCaps tend to be concise, and contain minimal information beyond the audio tags. In particular, the captions of LAION-Audio-630K are short, and may include information deviate from common sense, for instance, “rapping a tree”. WavCaps, on the other hand, exhibits a simple sentence structure, such as “... sound can be heard”. The captions of these two datasets hardly present --- --MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Luoyi Sun, Xuenan Xu, Mengyue Wu, and Weidi Xie Table 6: Caption comparison with LAION-Audio-630K and WavCaps, “LA.”, “WavC.” and “ACD” refer to LAION-Audio-630K, ‘WavCaps and Auto-ACD, respectively. No. Dataset Generated Caption LA. A person is rapping a tree. 1. WavC. Music plays with a man rapping. ACD. A woman sings while hip-hop music plays in the background, creating a rapping audio event in a computer room. LA. a slushy water lily. 2. WavC. Stream noise, crowd and splashing sounds. ACD. A crowd of people yells and cheers as water sloshes in the background at a water park. LA. a truck with a siren and a fire engine in an emergency. 3. WavC. A fire engine siren is heard. ACD. An emergency vehicle siren blares loudly as a fire truck rushes through a residential neighbourhood. LA. a vehicle with a medium frequency of engine idling. 4. WavC. A medium engine sound can be heard. ACD. A medium-sized engine is idling and vibrating, while an adult male speaks in the background near a running vehicle. more information than audio tags. In contrast, Auto-ACD features longer sentences and provide more comprehensive descriptions of the audio scenes. 7.5 Dataset Quality Check To evaluate the quality of Auto-ACD, we conduct a manual check on randomly sampled 1000 audio-captions pairs. As shown in Table 7, (i) we assess the correspondence between our generated captions and original audio; (ii) we revise the incorrect words in the final captions and calculate the percentage of modified vocabulary; (iii) we calculate the ratio of captions that contain inaudible information, for example, colours. The results, high correspondence and low erroneous words percentage, indicate that our proposed approach enables high-quality, scalable caption generations, with minimal incorrect information or inaudible information. Table 7: Statistics of Manual Check on Auto-ACD. Correspondence Modification Inaudibility Statistics 0.924 0.053 0.In addition, we further conduct manual check on each of the steps during caption generation, i.e, the various tools used for generating visual clues. We conduct a manual check on randomly sampled 200 audio-captions pairs, to analyse the quality of clues from six different open-source tools and the generated captions. As shown in Table 8, we define a clue that contradicts the audio as incorrect, and we calculate the accuracy of each tool and caption and count the number of correct clues in each sample. These tools possess high accuracy, with a high average accuracy at 81.3% and the highest accuracy at 91.5%. we find that 94.0% of the samples contain at least four correct clues. The fact that 88.0% of generated captions align with the audio further demonstrates that the LLM is capable of removing incorrect information and producing coherent audio captions. 7.6 Dataset Visualization As shown in Table. 9, we show more generated captions for audios from VGGSound and AudioSet. Note that, we present the video sequences to demonstrate how visual information can assist the language description for audio. It can be observed that, the captions in Auto-ACD not only accurately depict sound events but also infer additional information based on visual priors, that can also be inferred from audios, for example, (i) environmental details, for instance, “a lively performance arena", “in a music studio" and “a peaceful zen garden’, (ii) sound attributes like “A civil defense siren blares loudly" and “music plays in the background’, (iii) sound variations, for example, “motorcycle engine revs up and down" and “a car speeds down a dirt track". Table 8: Accuracy of Manual Check on Open-source Tools. “Caption.” refers to AudioCaption model. BLIP-2 DINO CLIP Place365 Caption. PANNs Accuracy 0.915 0.755 0.805 0.725 0.770 0.--- --Auto-ACD MM °24, October 28-November 1, 2024, Melbourne, VIC, Australia. Table 9: Data visualization in Auto-ACD. In each sample, the top line showcases the video frame sequence, the bottom line presents the corresponding audio caption. The sound events in the caption are highlighted in bold text, and environmental information is indicated in italics text. No. Generated Caption Music plays as a crowd cheers and a band performs on stage with vibrant lights in a lively performance arena.