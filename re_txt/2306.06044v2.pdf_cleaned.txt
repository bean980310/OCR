--- --2306.06044v2 [cs.CV] 18 SeparXiv GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields BARBARA ROESSLE, Technical University of Munich, Germany NORMAN MULLER, Technical University of Munich, Germany and Meta Reality Labs Zurich, Switzerland LORENZO PORZI, Meta Reality Labs Zurich, Switzerland SAMUEL ROTA BULO, Meta Reality Labs Zurich, Switzerland PETER KONTSCHIEDER, Meta Reality Labs Zurich, Switzerland MATTHIAS NIESSNER, Technical University of Munich, Germany Nerfacto Ours Neural Radiance Fields (NeRF) have shown impressive novel view synthesis results; nonetheless, even thorough recordings yield imperfections in reconstructions, for instance due to poorly observed areas or minor lighting changes. Our goal is to mitigate these imperfections from various sources with a joint solution: we take advantage of the ability of generative adversarial networks (GANs) to produce realistic images and use them to enhance realism in 3D scene reconstruction with NeRFs. To this end, we learn the patch distribution of a scene using an adversarial discriminator, which provides feedback to the radiance field reconstruction, thus improving realism in a 3D-consistent fashion. Thereby, rendering artifacts are repaired directly in the underlying 3D representation by imposing multi-view path rendering constraints. In addition, we condition a generator with multi-resolution NeRF renderings which is adversarially trained to further improve rendering quality. We demonstrate that our approach significantly improves rendering quality, e.g., nearly halving LPIPS scores compared to Nerfacto while at the same time improving PSNR by 1.4dB on the advanced indoor scenes of Tanks and Temples. CCS Concepts: « Computing methodologies > Reconstruction. Additional Key Words and Phrases: Neural radiance fields, Novel view synthesis 1 INTRODUCTION Neural Radiance Fields (NeRFs) [Mildenhall et al. 2020] can achieve remarkable novel view synthesis (NVS) results, powering applications in the domains of virtual/mixed reality, robotics, computational Authors’ addresses: Barbara Roessle, Technical University of Munich, Germany; Norman Miiller, Technical University of Munich, Germany and Meta Reality Labs Zurich, Switzerland; Lorenzo Porzi, Meta Reality Labs Zurich, Switzerland; Samuel Rota Buld, Meta Reality Labs Zurich, Switzerland; Peter Kontschieder, Meta Reality Labs Zurich, Switzerland; Matthias NieSner, Technical University of Munich, Germany. Fig. 1. GANeRF proposes an adversarial formulation whose gradients provide feedback for a 3D-consistent radiance field representation. This introduces additional constraints that enable more realistic renderings, and lead to improved novel view synthesis compared to Nerfacto and other baselines. photography, and many others. Given a set of posed input images, NeRFs distill complex and viewpoint-dependent scene information, parameterized as 5D input vectors (3D coordinates + 2D viewing direction), into volumetric density and color fields modeled with a neural network. Volumetric rendering techniques are then applied to generate photorealistic 2D output images for novel camera views from these fields. While NeRFs are highly effective at providing compact scene representations that enable photorealistic NVS, their applicability is nonetheless still limited for in-the-wild use cases. That is, NeRFs are optimized to overfit to the training dataset’s appearance information, which makes them highly dependent on carefully collected input data and properly chosen regularization strategies. In fact, the shape-radiance ambiguity [Zhang et al. 2020] describes that a set of training images can be perfectly regenerated from a NeRF without respecting the underlying geometry, but by simply exploiting the view-dependent radiance to simulate the actual scene geometry. As a consequence, novel view synthesis quality for non-training camera views drastically degrades and the generated images exhibit well-known cloudy floater artifacts. Unfortunately, even with significantly increased capture efforts and when input data comprises dense coverage for a scene, the reconstruction problem can remain ambiguous in areas with changing lighting conditions or in reflective or low-textured regions. Finally, for making NeRFs more widely adopted and applicable, the image capture process needs to be simple and of low effort while yielding high-quality reconstruction results. In our work, we take advantage of generative adversarial networks (GANs) to improve the NeRF quality in challenging real-world --- --2 + Barbara Roessle, Norman Miiller, Lorenzo Porzi, Samuel Rota Buld, Peter Kontschieder, and Matthias NieBner scenarios. To this end, we introduce GANeRF - a novel approach for resolving imperfections directly in the NeRF reconstruction. Our key idea is to leverage an adversarial loss formulation in an end-to-end fashion to introduce additional rendering constraints from a per-scene 2D discriminator. In particular, in regions with limited observations, this enforces the radiance field representation to generate patches that follow the distribution of real-world image patches more closely. Consequently, GANeRF enables notable mitigation of quality degradation effects in NVS due to imperfect input data, independently of their root causes like limited coverage, image distortion, or illumination changes. We propose a joint adversarial training during the NeRF optimization, such that a 2D patch discriminator informs the NeRF about the degree of photorealism for rendered patches. Through gradient feedback into the 3D scene representation, we reduce typical imperfections in the radiance field reconstruction while inherently encouraging 3D-consistent, photorealistic NeRF renderings. We show how to further improve the output quality with a subsequent generator that operates on the 2D NeRF renderings at multiple scales, by refining them to provide closer matches to the real distribution of the scene’s images. We evaluate our method on challenging indoor scenes from the novel ScanNet++ [Yeshwanth et al. 2023] and the well-known Tanks and Temples [Knapitsch et al. 2017] datasets, and show that leveraging our adversarial formulation within NeRFs leads to significant image quality improvements over prior works. Across all test scenes, we obtain remarkable improvements over the best-performing baselines [Barron et al. 2022; Tancik et al. 2023] for perceptual metrics like LPIPS (reductions between 28-48%), while maintaining consistently better PSNR and SSIM scores. In summary, we provide the following contributions: e We introduce a novel adversarial formulation that imposes patch-based rendering constraints obtained from a 2D discriminator to optimize a 3D-consistent radiance field representation. e We propose a 2D generator that further refines the rendering output, demonstrating significant improvements over stateof-the-art methods in novel view synthesis on challenging, large-scale scenes. 2 RELATED WORK Neural Radiance Fields (NeRFs) model a 3D scene as a volumetric function, which can be rendered from arbitrary viewpoints to generate highly-realistic images. While the seminal paper of [Mildenhall et al. 2020] encoded this function as a Multi-Layer Perceptron (MLP), more recent works have proposed alternative representations based on spatial data structures such as voxel grids [Sara Fridovich-Keil and Alex Yu et al. 2022; Sun et al. 2022], plane-based factorizations [Chen et al. 2022], or multi-scale 3D hash grids [Miiller et al. 2022]. Mip-NeRF [Barron et al. 2021] addressed aliasing issues by introducing an alternative rendering formulation based on conical frustums instead of rays, which was further expanded in [Barron et al. 2022] to account for unbounded scenes, and adapted to hash grid-based representations in [Barron et al. 2023]. In our work, we follow the Nerfacto model of [Tancik et al. 2023], which combines the field architecture from Instant-NGP [Miiller et al. 2022] with the multi-stage proposals of MipNeRF-360 [Barron et al. 2022] to achieve a good trade-off between speed and quality. 2.1 NeRFs with Priors Although the approaches discussed in the previous section achieve impressive visual fidelity, they generally struggle to represent underconstrained scene regions. Most NeRFs incorporate one or more heuristics to combat common artifacts such as “floaters”, e.g., by adding losses that promote peaked density [Barron et al. 2021, 2022; Hedman et al. 2021], injecting noise into the model during training [Mildenhall et al. 2020], or promoting surface smoothness [Oechsle et al. 2021; Wang et al. 2021; Zhang et al. 2021]. When data is insufficient or ambiguous, however, more effective priors are necessary to regularize the model. In the few-shot setting, geometric priors have been shown to be particularly effective: pre-trained models can be used to supervise the NeRF with predicted depth [Deng et al. 2022; Roessle et al. 2022] or surface normals [Yu et al. 2022]. RegNeRF [Niemeyer et al. 2022] utilizes a combination of geometric (surface smoothness, sampling space annealing) and appearance (normalizing flow) regularizers to enable training a NeRF on as few as 3 images. Similarly, SinNeRF [Xu et al. 2022] proposes a semi-supervised learning approach using geometric and semantic pseudo labels to guide a NeRF reconstruction from a single view. In our work, we focus on scenes with denser coverage, where geometric priors have been observed to provide only marginal improvements [Niemeyer et al. 2022]. Other works focus on appearance-based priors, e.g., DiffusioNeRF [Wynn and Turmukhambetov 2023] uses a 2D denoising diffusion model trained on RGBD images to construct an unsupervised loss term, which encourages the NeRF to render plausible images from unobserved viewpoints. Finally, some approaches learn scenebased priors, by casting NeRF reconstruction as a generalization problem: MVSNeRF [Chen et al. 2021] constructs a cost volume that is encoded to a neural volume, allowing consistent renderings from only a few images. PixelNeRF [Yu et al. 2021] and GenVS [Chan et al. 2023] train an image encoder to lift few input views to 3D-aware neural representations that can be rendered from novel views. However, these methods are still limited by the availability of training data: approaches based on scene-based priors in particular tend to produce results that lack detail, or impose strong assumptions on scene content and rendering trajectory according to the types of scenes they were trained on. In contrast, our per-scene adversarial optimization approach avoids the need for any external data. 2.2. GANs for Image Refinement Generative Adversarial Networks (GANs) [Goodfellow et al. 2014; Karras et al. 2020; Mescheder et al. 2018] are trained to produce images from a given data distribution by optimizing an adversarial loss. Initially proposed for unconditional image generation, GANs have also been applied to image-to-image translation and refinement tasks [Isola et al. 2017; Park et al. 2019; Wang et al. 2018a], such as colorization [Anwar et al. 2020], super-resolution [Ledig et al. 2017], and in-painting [Elharrouss et al. 2020]. In a similar setting, we optimize a conditional adversarial formulation based on StyleGANz2 [Karras et al. 2020] to refine the images produced --- --Par sone o,e NeRF Feature © Grid £222 3° 9? ; ; ote an 7 ¢ Volume d r o Rendering Tt Downsize > GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields +Refined Patches MLP Ray Rendered Patches Sampling Multi- View Discriminator Ds RGB Input Real Patches Real Io Fake Fig. 2. GANeRF method overview: our method takes as input a set of posed images and optimizes for a 3D radiance field representation. Our core idea is to incorporate multi-view patch-based re-rendering constraints in an adversarial formulation that guides the NeRF reconstruction process, and to refine rendered images using a conditional generator network. Particularly in under-constrained regions this significantly improves the resulting rendering quality. by a NeRF to more closely match the data distribution of a given scene. Prior work has combined GANs with NeRFs for different settings, such as unkown camera poses [Meng et al. 2021], generative NeRFs of objects and compositions [Niemeyer and Geiger 2021; Schwarz et al. 2020], 3D-aware generators [Kwak et al. 2022; Skorokhodov et al. 2022] and scene generation [Son et al. 2023]. 4K-NeRF [Wang et al. 2022] combines a low resolution NeRF with a 3D-aware decoder for super-resolution. In our work, we found that direct backpropagation from a discriminator to a full resolution NeRF is essential for view-consistent novel view synthesis. A related idea is explored concurrently to our work in NeRFLiX [Zhou et al. 2023], which trains a network in a non-adversarial setting to invert NeRF artifacts. In contrast, the approach in [Zhou et al. 2023] requires training on multiple scenes and relies on a hand-crafted model to simulate NeRF noise. 3 METHOD Given a set of posed input images capturing a static 3D scene, we focus on the problem of synthesizing novel views of that scene. We build on top of recent Neural Radiance Fields (NeRF) [Mildenhall et al. 2020], specifically, the Nerfacto model of [Tancik et al. 2023]. However, our ideas can also be applied to a different NeRF architecture, as shown in the experiments (Sec. 4.5.2). To improve realism in novel views, our method (Fig. 2) leverages a 2D adversarial loss that directly updates the 3D scene representation. To this end, a discriminator learns the distribution of image patches in the training data. Through adversarial training, the 3D NeRF representation (Sec. 3.1) is updated towards rendering patches that match this distribution (Sec. 3.2). On top of that, a 2D generator considers NeRF renderings at multiple resolutions, refining them based on feedback from a second discriminator (Sec. 3.3). 3.1 NeRF Preliminaries NeRF models represent a scene by providing a density og(x) and an RGB color &(x, d) for each point x € R? in 3D space, the color depending additionally on the viewing direction d € R? to account for view-dependent effects. This radiance field representation (09, 9), which depends on learnable parameters 0, allows rendering a pixel of an image by casting a ray from the camera origin o € R? through the pixel in the direction d and by computing the expected observed color along the ray with a distribution depending on the density field [Mildenhall et al. 2020]. Formally, the rendering function for a radiance field (og, &) and ray ris given by 00 t Ro(r) = [ corde] f colts) oles dat. (1) Here, ry := 0 + td is the 3D point along the ray at time t. Rg(r) can be approximated with the quadrature rule given samples of density and color along the ray [Mildenhall et al. 2020]. The NeRF parameters 6 are typically optimized to minimize the expected, per-ray mean squared error, which penalizes differences between the rendered color and the ground truth color. Specifically, given ray/color pairs (r,c) distributed as p(r,c), we minimize LXqp(9) = Bp Ro(x) ~ elle . (2) The superscript N distinguishes losses that are used to train NeRF from the ones used to train the generator, which are defined later. 3.2 NeRF Optimization with Discriminator It is often the case that scenes are rich in repeated elements and surfaces often look similar even from different perspectives. Accordingly, patches from one view form a good prior for patches in other --- ---8x4x4 Bilinear Downsample al on 88 onBilinear Downsample Norm sl | Norm std | Norn std Tilgiheliieil lili t t t t t Noise Noise Noise Noise 128 xBarbara Roessle, Norman Miiller, Lorenzo Porzi, Samuel Rota Buld, Peter Kontschieder, and Matthias NieBner 256 x 256 Rendered Patch Bilinear Downsample Ee Refined Patch ia r Legend @ Adda . ® Concatenate Noise Fig. 3. Our conditional generator architecture consists of a feature extraction pyramid of six blocks operating at multiple resolutions: Starting from a 4 xdown-sampled patch, convolutional blocks extract level-specific features that are upsampled and added together with generative noise to the next feature extraction level. The final output resolution matches the input resolution of 256 x 256. views. To encode this prior in our training scheme, we complement LN with an adversarial objective LN ‘dy (9; ) that leverages an auxiliary neural network Dg (a.k.a. discriminator) parametrized by ¢. The objective is optimized such that the discriminator is pushed towards classifying whether a given image patch is real or produced by the NeRF parametrized by 6 (a.k.a. fake), while the NeRF is encouraged to fool the discriminator, thus rendering realistic patches. Patches P are assumed to be distributed according to q(P) and we denote by rp the set of rays and by cp the pixel colors corresponding to patch P. The adversarial objective should be minimized with respect to the NeRF parameters 6 as a proper loss, but maximized with respect to the discriminator parameters ¢. Following [Mescheder et al. 2018], we adopt an R; gradient penalty term on the discriminator balanced by a nonnegative scalar AY yielding the following form of the adversarial objective: Lh y(9.9) = Eq [f (Dg (Ro(tp))) + f(—Dg(ep)) AY IVDg(eryI3} - 3) Here, by setting f(x) := —log(1 + exp(—x)), we obtain a regularized version of the loss originally proposed in the seminal GAN paper [Goodfellow et al. 2014]. We further employ a perceptual loss based on VGG [Johnson et al. 2016], which has shown success in conjunction with 2D conditional GANs [Wang et al. 2018b]: Lhere(9) := Eq [[IPvec(Ro(rp)) - ®vec(ep) 3]. (4) where ®ygg is the vectorized concatenation of the first 5 feature layers before the max pooling operation of a VGG19 network [Simonyan and Zisserman 2015], each layer being normalized by the square root of the number of entries. Lhere(9) encourages similarity of real and fake patch features at different granularities. The final loss that we minimize to train the NeRF is given by Ln (8) = Lh (9) + ApereLpere(8) + AN max Lryy(9.4), 5) where ® denotes the set of possible discriminators and AN are nonnegative balancing factors for the different losses. This objective is optimized with stochastic gradients computed from batches that combine rays and patches sampled uniformly across all training images (i.e., from the p and q distributions, respectively). Moreover, parameter updates are alternated between the NeRF and the discriminator to cope with the saddle-point problem, as usually done in GAN training schemes. 3.3. Conditional Generator Up to this point, artifacts have been repaired directly in 3D through the discriminator guidance introduced in Sec. 3.2. However, the rendering quality can be further improved in 2D by postprocessing synthesized images. To this end, we design a conditional generator G.) parameterized by w, which takes as input a NeRF rendering and an auxiliary random vector z and produces a cleaned image, thus serving as a stochastic denoiser. The architecture is inspired by the StyleGAN2 generator [Karras et al. 2020], but we adapt it to a conditional generator and omit the mapping network (Fig. 3). We keep the additive noise depending on z, which is close in spirit to the way pix2pix [Isola et al. 2017] uses dropout as source of randomness in their conditional GAN setting. As shown in Fig. 3, the conditioning input patch is 6 times bilinearly downsampled by a factor of 2, thus enabling a multi-scale analysis. The downscaled patches are encoded with a convolutional layer into 32 feature channels, which are concatenated to the output of the generator layer of corresponding scale. As in StyleGAN2, we use leaky ReLU activations, input/output skip connections and normalization layers. Additive noise is injected after the normalization layers, and noise is additionally scaled by a learnable per-layer factor. Akin to the NeRF optimization, the generator is trained in a regularized adversarial setting. This involves a second discriminator Dy, parameterized by w, which differs from the previously introduced Dg. Indeed, the NeRF and the conditional generator introduce different types of errors in the output image that can be addressed more effectively by independent discriminators. The adversarial objective Loy used to train the generator takes the following form: L8,,(0, 19) = Egn [f(Dy(Go(R19(tP),2))) +f(-Dy(cp)) — AspllVDy(ep)II3} . (6) where the expectation is additionally taken with respect to z distributed as a standard normal distribution n(z). The adversarial --- --objective should be minimized with respect to the generator’s parameters «, but maximized with respect to the discriminator parameter y. Moreover, we stop the gradient through the NeRF parameters @, which is denoted by L@. The adversarial objective is complemented with the same perceptual loss used to train the NeRF, namely Lbere(l4) = Egan [Il Pvoc(Go(R19(rp).z)) - Pvec(ep)Il3] (7) and an L, color loss, as in the conditional GAN pix2pix [Isola et al. 2017], operating on patches, defined as: L3,p(019) = Eqn [IIGo(Rio(xP).2) ~ cpl] - (8) The overall generator loss is defined as Lo (@I6*) == Apere Lpere(@10*) + Ae, Leap (10) +max Lol Wl), (9) where ¥ denotes the set of possible discriminators, A°* are nonnegative factors balancing the loss components, and 6” is the parametrization of the NeRF obtained by optimizing Ly. We cope with the saddle-point problem by alternating updates of the generator and the discriminator akin to what we have described for Ly. 4 EXPERIMENTS 4.1 Datasets and Metrics 4.1.1. ScanNet++. From the ScanNet++ [Yeshwanth et al. 2023] dataset, we evaluate on five indoor scenes consisting of office, lab and apartment environments. Each scene contains an average of ~ 700 images, ~ 20 of which are defined as a test set. In contrast to other datasets commonly used in NeRF works (e.g., LLFF [Mildenhall et al. 2020] or the 360° scenes from [Barron et al. 2022]), the test views in ScanNet++ are selected to be spatially out-of-distribution compared to the training views, in order to explicitly evaluate a model’s generalization capabilities. We use the DSLR images with the provided camera poses from structure from motion [Schénberger and Frahm 2016], and undistort and resize them to 768 x 1152. 4.1.2. Tanks and Temples. As a benchmark for larger-scale reconstruction, we consider four scenes from the advanced set of scenes of the Tanks and Temples [Knapitsch et al. 2017] dataset: Auditorium, Ballroom, Courtroom, and Museum. These scenes depict large indoor environments, with detailed geometries and complex illumination. For each scene, we randomly select 10% of the available images as a test set, resulting in a split of ~ 270 training and ~test views on average. We use the original resolution of 1080 x 1920. 4.1.3. Evaluation Metrics. We evaluate our results in terms of three main visual quality metrics: PSNR, SSIM [Wang et al. 2004], and LPIPS [Zhang et al. 2018]. Following previous works in the image generation literature, we additionally report KID [Binkowski et al. 2018] scores, which measure how closely our model’s outputs match the visual distribution of the scene. GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields +4.2. Training and Implementation Details 4.2.1 NeRF. In each training iteration, we render 4096 random rays and a 256 x 256 patch from a single random input image. The perceptual loss processes the patch as a whole, whereas the adversarial loss subdivides it into 16 smaller patches of size 64 x 64 without overlap. Dg is a StyleGAN2 discriminator [Karras et al. 2020], which is adapted to process 64 X 64 patches with the number of convolutional channels reduced by half. For each generated “fake” patch the discriminator is also given a real patch. The radiance field and discriminator Dg are trained for 400k iterations, using the Adam optimizer [Kingma and Ba 2015] and RMSprop [Tieleman and Hinton. 2012] with learning rates 1 x 10~* and 1 x 107%, respectively. The loss weights are set to Wi = 0.0003, Mere = 0.0003 and AY =0.1. 4.2.2 Conditional Generator. The initial training patch size before downsampling is 256X256. At inference time, however, the generator is able to run fully convolutionally on high-resolution images to refine the NeRF renderings. The generator is trained using a batch size of 8. While the perceptual loss operates on the full training patches, the adversarial loss subdivides each patch into 4 smaller patches of size 128 x 128 without overlap, thus Dy is trained with batch size 32. The discriminator is inspired by StyleGAN2 [Karras et al. 2020], and adapted to process 128 x 128 patches. Both generator and discriminator Dy are trained for 3000 epochs using the Adam optimizer [Kingma and Ba 2015] with learning rate 2 x 1077. The A G = GL GL loss weights are set to Apere = 1.0, Jreb = 3.0 and Ag, = 5.0. 4.3 Baseline Comparisons We compare our approach to several baselines from recent literature. We pick Mip-NeRF 360 [Barron et al. 2022] and Instant NGP [Miiller et al. 2022] as representatives of the current state of the art in NeRF architectures optimized for visual quality and speed, respectively. Furthermore, we evaluate four variations of the Nerfacto [Tancik et al. 2023] model that our proposed approach is based upon: i Nerfacto: the baseline model with no modifications. ii Nerfacto + extra capacity: this higher-capacity variation doubles the number of hidden dimensions of the MLPs, doubles the grid resolution, and increases the hash table size by 4 compared to the default Nerfacto. This results in ~ 44M trainable parameters, ie., ~ 25% more than our model (NeRF + Generator) and ~ 3.4x the number of parameters of the default Nerfacto (~ 12.9M). Nerfacto + pix2pix: Nerfacto results are refined by a pix2pix [Isola et al. 2017] generator. From the official pix2pix repository, we found that the generator using 9 ResNet [He et al. 2016] blocks, trained with Wasserstein objective and gradient penalty [Gulrajani et al. 2017] performs best. We equally train this model with the VGG perceptual loss, which improves its performance. iv Nerfacto + ControlNet: Nerfacto results are refined by Stable Diffusion [Rombach et al. 2022] with ControlNet [Zhang and Agrawala 2023] conditioning on Nerfacto renderings and LoRA [Hu et al. 2022] fine-tuning using the implementation of [Hecong 2023]. We further compare to the recent work 4K-NeRF [Wang et al. 2022]. For a fair comparison, all baselines are trained until convergence. As shown in Tab. 1 and Tab. 2, our approach leads to noticeable iii --- --6 + Barbara Roessle, Norman Miiller, Lorenzo Porzi, Samuel Rota Buld, Peter Kontschieder, and Matthias NieBner Table 1. Quantitative results on five ScanNet++ scenes. Our method outperforms the baselines by a large margin in the perceptual metrics, like LPIPS and KID, while maintaining consistently better PSNR and SSIM scores. Table 3. Ablation study on Auditorium scene from Tanks and Temples. The decline in performance when removing individual parts of our method confirms our design choices. Method PSNRT SSIMT LPIPS| KID| Mip-NeRF 360 [Barron et al. 2022] 24.9 0.862 0.225 0.Instant NGP [Miiller et al. 2022] 25.3 0.844 0.269 0.4K-NeRF [Wang et al. 2022] 22.7 0.807 0.254 0.Nerfacto [Tancik et al. 2023] 25.6 0.848 0.245 0.Nerfacto + extra capacity 25.9 0.854 0.228 0.Nerfacto + pix2pix [Isola et al. 2017] 24.9 0.848 0.193 0.[Zhang and 23.1 0.827. --:0.174.-—:0.Nerfacto + ControlNet Agrawala 2023] Ours w/o discriminator 25.8 0.857 0.177 (0.Ours w/o generator 25.9 0.860 0.198 0.Ours 26.1 0.864 0.161 0.Table 2. Quantitative results on four Tanks and Temples scenes. Our method achieves particularly strong improvements on the perceptual metrics, thus improving visual details and sharpness of novel view renderings. Method PSNRT SSIMT LPIPS| KID| Mip-NeRF 360 [Barron et al. 2022] 18.5 0.709 0.327 0.Instant NGP [Miiller et al. 2022] 19.3 0.700 0.369 0.4K-NeRF [Wang et al. 2022] 19.4 0.656 0.356 0.Nerfacto [Tancik et al. 2023] 19.5 0.716 0.329 0.Nerfacto + extra capacity 19.6 0.733 0.291 0.Nerfacto + pix2pix [Isola et al. 2017] 20.6 0.739 0.242 0.[Zhang and 19.6 0.706 —0.213-——-0.Nerfacto + ControlNet 4s rawala 2023) Ours w/o discriminator 20.6 0.745 0.192 0.Ours w/o generator 19.9 0.739 0.251 0.Ours 20.9 0.776 0.169 0.improvements when compared to baselines. In particular, the two perceptual metrics (LPIPS, KID) demonstrate the largest relative improvements, suggesting that our approach is able to fix many of the small visual artifacts that are often poorly measured by color similarity metrics (PSNR, SSIM). This is also confirmed by the qualitative evaluation shown in Figs. 4 to 6. Nerfacto + ControlNet achieves high perceptual quality (LPIPS, KID), however, the results are less view-consistent (see Sec. 4.5.1), which also reflects in lower PSNR. 4.4 Ablation Experiments To verify the effectiveness of the added components, we perform an ablation study on the ScanNet++ and Tanks and Temples datasets. The quantitative results (Tab. 1 and Tab. 2), as well as the qualitative results in Fig. 7 show that the complete version of our method achieves the highest performance. 4.4.1 Without Discriminator. In this experiment, we investigate the impact of optimizing the radiance field solely with an RGB loss and applying our generator as a pure post-processing step on the renderings. We observe that the geometry and texture details of the scene cannot be recovered to the same extent as achieved with our full method (Fig. 7). The drop in all metrics (“w/o discriminator” in Tabs. 1 and 2) clearly highlights the importance of the patchbased supervision to inform the NeRF representation in 3D. This Method PSNRT SSIM? LPIPS| KID | Nerfacto [Tancik et al. 2023] 21.1 0.843 -0.304_-——:0.Ours 22.3 0.862 0.158 0.w/o discriminator 21.9 0.857 0.178 ~—-0.w/o generator 21.7 0.854 0.247 0.NeFF training w/o adv. loss 21.9 0.861 0.175 0.w/o perc. loss 21.7 0.857 0.187 0.Generator training w/o high-res. patches 22.1 0.851 0.179 0.w/o low-res. patches 22.1 0.862 0.159 0.w/o RGB encoding 22.1 0.860 0.167 0.w/o adv. loss 22.3 0.861 0.174 0.w/o perc. loss 21.7 0.834 0.184 0.w/o RGB loss 21.8 0.859 0.163 0.indicates that our method, which incorporates gradient backpropagation to the 3D representation, surpasses a pure 2D post-processing approach. 4.4.2. Without Generator. When omitting the generator, the results are less sharp (e.g., samples 4, 5, and 7 in Fig. 7), which leads to lower performance (“w/o generator” in Tabs. 1 and 2) compared to the full version of our method. This shows that the generator helps to achieve high-detail renderings. 4.4.3 NeRF Training. Tabs. 3 and 5 provide more detailed ablations on the NeRF optimization, which show that dropping the adversarial loss (“w/o adv. loss”) or dropping the perceptual loss (“w/o perc. loss”) have a negative impact on performance. 4.4.4 Generator Training. Detailed ablations on the generator training are listed in Tabs. 3 and 5. We investigate the impact of omitting specific input patch resolutions in our model. Removing the two highest resolutions out of the six resolutions results in a noticeable performance drop (“w/o high-res. patches”), highlighting the essential role of high-resolution input for the generator. Conversely, when the two lowest resolutions are removed (“w/o low-res. patches”), we observe a decline in quality that indicates that the generator also relies on the low-resolution input. We further show that the small RGB encoder, i.e., one convolutional layer on the multi-resolution patches (Fig. 3), benefits the generator (“w/o RGB encoding”). Furthermore, our ablation experiments examine the individual loss functions, namely the adversarial loss (“w/o adv. loss”), perceptual loss (“w/o perc. loss”), and L1 RGB loss (“w/o RGB loss”). The results reveal that each of these loss functions contributes significantly to the overall performance of the generator. 4.4.5 Reduced Number of Images. Tab. 4 lists results when reducing the number of images of the largest scene (800 images) to 400, 200, 100, 50 and 25 images, which is extremely sparse for room-scale scenes. It shows that our method consistently outperforms Nerfacto by a similar margin as observed in the more dense setting. --- --GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields +Mip-NeRF 360 Instant NGP Nerfacto Ours Ground truth Fig. 4. Comparison on Tanks and Temples. Our method recovers more detail than the baselines, such as thin structures or patterns on the floor. --- --Ground truth us Instant NGP Ne: Mip-NeRF‘ renderings compared to the baselines. +. Our method produces less foggy artifacts, which leads to sharper Scannet+ Fig. 5. Comparison on --- --4K-NeRF Nerfacto + pix2pix Nerfacto + ControlNet GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields +—— 25: i Ours Ground truth Fig. 6. Comparison on Tanks and Temples. Using backpropagation to the NeRF representation and effective generator conditioning, our novel views closely match the ground truth patterns compared to other approaches based on generative models. Table 4. Ablation study with reduced number of images on a large ScanNet++ scene. Our method consistently outperforms Nerfacto by a similar margin, regardless of the level of input sparsity. Table 5. Ablation study on a ScanNet++ scene. Removing individual parts of our methods leads to a decline in performance, indicating the importance of the different architectural choices and training strategies. Method #images PSNRT SSIMT LPIPS| KID | Nerfacto [Tancik et al. 2023 800 24.2 0.844 0.247 0.Ours 24.7 0.870 0.169 0.Nerfacto [Tancik et al. 2023 400 24.2 0.843 0.252 0.Ours 24.3 0.864 0.169 0.Nerfacto [Tancik et al. 2023 200 23.5 0.825 0.274 0.Ours 23.9 0.862 0.182 0.Nerfacto [Tancik et al. 2023 100 22.2 0.805 0.307 0.Ours 22.5 0.842 0.216 0.Nerfacto [Tancik et al. 2023 50 20.2 0.771 0.347 0.Ours 20.5 0.788 0.282 0.Nerfacto [Tancik et al. 2023 25 15.5 0.686 0.524 0.Ours 16.7 0.727 0.408 0.4.5 Evaluations 4.5.1 View Consistency. To obtain view-consistent results, it is essential to first backpropagate to the NeRF and optimize the underlying 3D representation. Following [Lai et al. 2018], Tab. 6 shows consistency of test views computed with optical flow [Teed and Deng 2020]. Comparing Nerfacto and our method without backpropagation to NeRF (ie., “w/o discriminator”), shows that our generator largely improves visual quality (i-e., KID), while adding Method PSNRT SSIMT LPIPS| KID | Nerfacto [Tancik et al. 2023] 24.2 0.844 0.247 0.Ours 24.7 0.870 0.169 0.w/o discriminator 24.4 0.859 0.188 0.w/o generator 24.6 0.865 0.201 0.NeFF training w/o adv. loss 24.4 0.867 0.170 0.w/o perc. loss 24.5 0.866 0.184 0.Generator training w/o high-res. patches 24.2 0.849 0.202 0.w/o low-res. patches 24.6 0.870 0.171 0.w/o RGB encoding 24.6 0.870 = 0.171 0.w/o adv. loss 24.7 0.869 0.175 0.w/o perc. loss 24.4 0.858 0.170 0.w/o RGB loss 24.6 0.869 0.170 0.marginal inconsistencies. However, our full method backpropagates to the 3D representation, thus reducing the added inconsistencies by over half. Our video results show that inconsistencies are barely noticeable, and qualitative improvements dominate. The alternative refinement with ControlNet (Nerfacto + ControlNet) achieves high visual quality, however, the refined views are highly inconsistent. Following [Wang et al. 2022], Fig. 8 visualizes view consistency by --- --10 + Barbara Roessle, Norman Miiller, Lorenzo Porzi, Samuel Rota Buld, Peter Kontschieder, and Matthias NieBner Nerfacto Ours w/o Ours w/o generator Ours Ground truth discriminator Fig. 7. Ablation experiments on Tanks and Temples. “Ours w/o discriminator” significantly struggles with patterns (samples 4 and 5) and misses thin structures (samples 3 and 6). “Ours w/o generator” better recovers the patterns but produces blurry results compared to the full method (samples 4, 5 and 7). --- --Table 6. View consistency evaluation on ScanNet++. Novel views from our method are both highly view-consistent and of high visual quality (i.e., KID). Best and second best results are highlighted. Method View Consistency MSE | KID | Nerfacto [Tancik et al. 2023] 0.0018 0.[Zhang and Nerfacto + ControlNet Agrawala 2023] 0.0039 0.Ours w/o discriminator 0.0023 0.Ours w/o generator 0.0018 0.Ours 0.0020 0.Table 7. Using different NeRF backbones on ScanNet++. Our method is flexible to use an alternative NeRF representation, e.g., Instant NGP. The default approach builds on Nerfacto. Method PSNRT SSIMT LPIPS| KID | Instant NGP [Miiller et al. 2022] 25.3 0.844 = 0.269 0.Ours w/ Instant NGP 25.6 0.857 0.173 0.Nerfacto [Tancik et al. 2023] 25.6 0.848 0.245 0.Ours 26.1 0.864 0.161 0.Table 8. Runtime comparison on ScanNet++. Our rendering time is competitive with Nerfacto; the training time is likely comparable to Mip-NeRFwhich was trained on 4 GPUs vs. our method was trained on a single GPU. Method Training Time | Rendering Time | per frame (768x1152) Mip-NeRF 360 [Barron et al. 2022] 18h 4x NVIDIAV100 24s 1x NVIDIA VInstant NGP [Miiller et al. 2022] 3.8h Soo 047s Soe Nerfacto [Tancik et al. 2023] 35h 48 6 S os7s #8 6Ours 2d10h 2% 0895 Zs tracking a column of pixels across a sequence of video frames: the inconsistency of Nerfacto + ControlNet is clearly visible as vertical stripe patterns. The full version of our method shows the best combination of view consistency and visual quality in Fig. 8. 4.5.2 Alternative NeRF Backbone. We build our method on the Nerfacto radiance field representation, however, it is flexible to backpropagate gradients to a different NeRF backbone. Using Instant NGP as alternative NeRF, leads to improvements of similar magnitude across all metrics compared to using Nerfacto (Tab. 7). With the Instant NGP backbone, we use the parameters as described in Sec. 4.2, except, due to higher memory consumption of the Instant NGP rendering, we reduce the patch size to 192 x 192, from which we crop four 64 x 64 patches for the discriminator. The loss weights are then set to N, = 0.001, Mere = 0.001 and AY =0.1. 4.5.3. Runtime. Tab. 8 provides a comparison of training and rendering times. The rendering time of our method is dominated by the rendering time of the underlying NeRF, since the generator forward pass is comparably fast, i.e., takes only 13ms. Hence, our method renders novel views at effectively the same speed as Nerfacto and much faster than Mip-NeRF 360. Using Instant NGP as alternative NeRF backbone (Sec. 4.5.2) speeds up rendering towards the Instant NGP rendering time. Our training time is slower than Nerfacto or GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields +Pixel column over time First Frame Last Frame from (irs! to last frame || Nerfacto Nerfacto + ControlNet w g fl a esiA]Ro B Q g° Ours w/o generator Fig. 8. View consistency visualization. From a sequence of video frames, we extract a column of pixels at the same position in each frame (left) and concatenate them horizontally to visualize view consistency (right). Instant NGP; however, our method significantly improves visual quality. All methods are trained until convergence. 4.5.4 Impact of View Coverage on Visual Quality. Fig. 9-left shows that PSNR of novel views increases with view coverage, and stagnates for areas that have been observed by > 75 training views. The improvement of our method over Nerfacto (Fig. 9-right) is twice as high in regions of low coverage, that are observed by < 25 training --- --12. + Barbara Roessle, Norman Miiller, Lorenzo Porzi, Samuel Rota Buld, Peter Kontschieder, and Matthias NieBner Improvement over Nerfacto g 22.PS. A PSNRObserved by # train views 100Observed by # train views Fig. 9. Impact of view coverage on PSNR (left)/ improvement over Nerfacto (right) on a ScanNet++ scene. View coverage is computed using ground truth depth and counting # train views that project to pixels in test views. ALPIPS: 0 EMM 0.Nerfacto Ours Ground truth overlayed w/ LPIPS improvement Fig. 10. Perceptual improvement over Nerfacto. Our method particularly improves structured areas with objects or floor patterns, compared to textureless surfaces. LPIPS is computed on patches. views, compared to areas of higher coverage. Fig. 10 further visualizes the improvement in terms of LPIPS by calculating LPIPS on small 96 x 96 patches of test views in a convolutional manner. It shows that improvements particularly happen in richly structured areas, such as table-top objects or repetitive floor patterns, rather than texture-less walls. 4.5.5 GAN Hallucination Effects. Using generative approaches for reconstruction comes with the danger of hallucinating content, which may not be view-consistent. Through backpropagation to the NeRF representation and the generator conditioning, our novel Ours Nearby training view Fig. 11. GAN hallucination effects. Reconstruction of inscriptions is difficult, which can cause hallucinated characters (left). views are highly view-consistent (Sec. 4.5.1) and closely match the ground truth, e.g., difficult patterns in Fig. 6. We, however, observe that reconstructing written text is very challenging, which can lead to hallucinated, incorrect characters as shown in Fig. 11. 4.5.6 Discriminator on Unseen Views. Rendering fake patches from sampled poses rather than the training views, has potential to provide additional discriminator feedback to the NeRF representation. Our attempt to sample poses from Gaussians around the training poses to render fake patches, however, left the average performance unchanged. Likely, a more sophisticated strategy is needed to sample patches that benefit from additional supervision. 4.6 Limitations Our results show that we can achieve significant improvements compared to state-of-the-art methods. At the same time, we believe there are still important limitations. For instance, at the moment our patch discriminator is trained per scene. However, it would be beneficial to train a more generic prior that has access to a larger scene corpus to improve its capability. While feasible, a naive generalizable discriminator would tend to collapse to a scene classifier; ie., it would primarily identify whether a patch belongs to the same scene or not. To overcome this issue, one possible solution is to train a NeRF representation for each training scene simultaneously, which would entail considerable computational expenses. Another limitation is that we currently focus only on static scenes; however, we believe it would be interesting to expand our approach to recent deformable and dynamic NeRF approaches such as [Isik et al. 2023; Kirschstein et al. 2023; Park et al. 2021; Tretschk et al. 2021]. 5 CONCLUSION We introduce GANeRF, a new approach for adversarial optimization of neural radiance fields. The main idea behind our approach is to impose patch-based rendering constraints into the radiance field reconstruction. By backpropagating gradients from a scene patch discriminator, we effectively address typical imperfections and rendering artifacts stemming from traditional NeRF methods. In particular, in regions with limited coverage, this significantly improves rendering quality, both qualitatively and quantitatively outperforming state-of-the-art methods. At the same time, our method is only a stepping stone for combining rendering priors with high-end novel view synthesis. For instance, we believe that generalizing across scenes will offer numerous opportunities for leveraging similar ideas as those proposed in this work. --- --ACKNOWLEDGMENTS This work was funded by a Meta SRA. Matthias NieBner was also supported by the ERC Starting Grant Scan2CAD (804724). We thank Angela Dai for the video voice-over. REFERENCES Saeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal Mian, Fahad Shahbaz Khan, and Abdul Wahab Muzaffar. 2020. Image colorization: A survey and dataset. arXiv preprint arXiv:2008.10774 (2020). Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, and Pratul P. Srinivasan. 2021. Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. ICCV (2021). Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. 2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. CVPR (2022). Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. 2023. Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields. arXiv preprint arXiv:2304.06706 (2023). Mikolaj Birikowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. 2018. Demystifying mmd gans. arXiv preprint arXiv:1801.01401 (2018). Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. 2023. GeNVS: Generative Novel View Synthesis with 3D-Aware Diffusion Models. In arXiv. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. 2022. TensoRF: Tensorial Radiance Fields. In European Conference on Computer Vision (ECCV). Anpei Chen, Zexiang Xu, Fugiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. 2021. Mvsnerf: Fast generalizable radiance field reconstruction from multiview stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 14124-14133. Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. 2022. Depth-supervised NeRF: Fewer Views and Faster Training for Free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Omar Elharrouss, Noor Almaadeed, Somaya Al-Maadeed, and Younes Akbari. 2020. Image inpainting: A review. Neural Processing Letters 51 (2020), 2007-2028. Tan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Advances in Neural Information Processing Systems, Vol. 27. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. 2017. Improved Training of Wasserstein GANs. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS’17). Curran Associates Inc., Red Hook, NY, USA, 5769-5779. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 770-778. https://doi.org/10.1109/CVPR.2016.Wu Hecong. 2023. ControlLoRA: A Light Neural Network To Control Stable Diffusion Spatial Information. https://github.com/HighCWu/ControlLoRA, Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. 2021. Baking neural radiance fields for real-time view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5875-5884. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. Mustafa Isik, Martin Riinz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, and Matthias NieSner. 2023. HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion. ACM Transactions on Graphics (TOG) 42,(2023), 1-12. https://doi.org/10.1145/Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-Image Translation with Conditional Adversarial Networks. CVPR (2017). Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual Losses for Real-Time Style Transfer and Super-Resolution. In Computer Vision - ECCV 2016, Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (Eds.). 694-711. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and Improving the Image Quality of StyleGAN. In Proc. CVPR. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. CORR abs/1412.6980 (2015). Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias Niefner. 2023. NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads. https://doi.org/10.48550/arXiv.2305.03027 arXiv:2305.03027 [cs.CV] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG) 36, 4 (2017), 1-13. Jeong-gi Kwak, Yuanming Li, Dongsik Yoon, Donghyeon Kim, David Han, and Hanseok Ko. 2022. Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis. In European Conference on Computer Vision. Springer, GANeRF: Leveraging Discriminators to Optimize Neural Radiance Fields +236-253. Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and MingHsuan Yang. 2018. Learning Blind Video Temporal Consistency. In European Conference on Computer Vision. Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. 2017. Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4681-4690. Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and Jingyi Yu. 2021. GNeRF: GAN-based Neural Radiance Field without Posed Camera. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. 2018. Which Training Methods for GANs do actually Converge?. In International Conference on Machine Learning (ICML). Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV. Thomas Miiller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. ACM Trans. Graph. 41, 4, Article 102 (July 2022), 15 pages. https://doi.org/10.1145/3528223.Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall, Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan. 2022. RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). Michael Niemeyer and Andreas Geiger. 2021. GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields. In Proc. IEEE Conf, on Computer Vision and Pattern Recognition (CVPR). Michael Oechsle, Songyou Peng, and Andreas Geiger. 2021. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5589-5599. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. 2021. HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. ACM Trans. Graph. 40, 6, Article 238 (dec 2021). Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. 2019. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2337-2346. Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan, and Matthias Niefner. 2022. Dense Depth Priors for Neural Radiance Fields from Sparse Input Views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjA§rn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. 2022. Plenoxels: Radiance Fields without Neural Networks. In CVPR. Johannes Lutz Schénberger and Jan-Michael Frahm. 2016. Structure-from-Motion. Revisited. In Conference on Computer Vision and Pattern Recognition (CVPR). Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. 2020. GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis. In Advances in Neural Information Processing Systems 33, Vol. 25. Curran Associates, Inc., Red Hook, NY, 20154-20166. _https://papers.nips.ce/paper_files/paper/2020/hash/ €92e1b476bb5262d793fd4093 1e0ed53- Abstract.html Karen Simonyan and Andrew Zisserman. 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1409.Ivan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. 2022. EpiGRAF: Rethinking training of 3D GANs. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=TTM7iEFOTzJ Minjung Son, Jeong Joon Park, Leonidas Guibas, and Gordon Wetzstein. 2023. SinGRAF: Learning a 3D Generative Radiance Field for a Single Scene. In CVPR. Cheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction. In CVPR. Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Justin Kerr, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, and Angjoo Kanazawa. 2023. Nerfstudio: A Modular Framework for Neural Radiance Field Development. arXiv preprint arXiv:2302.04264 (2023). Zachary Teed and Jia Deng. 2020. RAFT: Recurrent All-Pairs Field Transforms for Optical Flow. In European Conference on Computer Vision. --- --14 + Barbara Roessle, Norman Miiller, Lorenzo Porzi, Samuel Rota Buld, Peter Kontschieder, and Matthias NieBner T. Tieleman and G. Hinton. 2012. Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude. (2012). Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollhéfer, Christoph Lassner, and Christian Theobalt. 2021. Non-Rigid Neural Radiance Fields: Reconstruction and Novel View Synthesis of a Dynamic Scene From Monocular Video. In IEEE International Conference on Computer Vision (ICCV). IEEE. Chaoyue Wang, Chang Xu, Chaohui Wang, and Dacheng Tao. 2018b. Perceptual Adversarial Networks for Image-to-Image Transformation. IEEE Transactions on Image Processing 27, 8 (2018), 4066-4079. https://doi.org/10.1109/TIP.2018.Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. 2021. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689 (2021). Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018a. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4 (2004), 600-612. Zhongshu Wang, Lingzhi Li, Zhen Shen, Li Shen, and Liefeng Bo. 2022. 4K-NeRF: High Fidelity Neural Radiance Fields at Ultra High Resolutions. arXiv preprint arXiv:2212.04701 (2022). Jamie Wynn and Daniyar Turmukhambetov. 2023. DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models. In arxiv. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. 2022. Sinnerf: Training neural radiance fields on complex scenes from a single image. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXII. Springer, 736-753. Chandan Yeshwanth, Yueh-Cheng Liu, Matthias NieSner, and Angela Dai. 2023. ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes. In Proceedings of the International Conference on Computer Vision (ICCV). Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelNeRF: Neural Radiance Fields from One or Few Images. In CVPR. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. 2022. Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction. arXiv preprint arXiv:2206.00665 (2022). Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. NeRF++: Analyzing and Improving Neural Radiance Fields. arXiv:2010.07492 (2020). Lvmin Zhang and Maneesh Agrawala. 2023. Adding Conditional Control to Text-toImage Diffusion Models. arXiv:2302.05543 [es.CV] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition. 586-595. Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. 2021. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (TOG) 40, 6 (2021), 1-18. Kun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, and Jiangbo Lu. 2023. NeRFLiX: High-Quality Neural View Synthesis by Learning a DegradationDriven Inter-viewpoint MiXer. In arxiv.