arXiv:2306.04632v1 [cs.CV] 7 JunDesigning a Better Asymmetric VQGAN for StableDiffusion Zixin Zhu¹* Xuelu Feng¹* Dongdong Chen² Jianmin Bao² Le Wang Yinpeng Chen² Lu Yuan² Gang Hua ¹,Lincoln's Toe Truck Lincoln's Toe Truck Input ¹Xi'an Jiaotong University 2Microsoft 3 Wormpex AI Research {zhuzixin@stu., xueluF @stu., lewang@}xjtu.edu.cn {jianbao, yiche, luyuan}@microsoft.com, {cddlyf, ganghua}@gmail.com Lancia's The fran Toe Stook Lasbild's Fun Track K Baseline Lincoln's Lincoln's Toe Truck Toe Truck Lincoln's Toe Truck CBXPUOurs Lincoln's Toe Truck Toe Truck Lincoln's Toe Truck XPUGround Truth Figure 1: Comparing StableDiffusion-based inpainting (top) / editing [40] (bottom) results with default VQGAN and our proposed asymmetric VQGAN. The vanilla VQGAN in StableDiffusion will cause serious distortion even for the non-edited regions. In contrast, our proposed asymmetric VQGAN can preserve more details and deliver superior results. Abstract StableDiffusion is a revolutionary text-to-image generator that is causing a stir in the world of image generation and editing. Unlike traditional methods that learn a diffusion model in pixel space, StableDiffusion learns a diffusion model in the latent space via a VQGAN, ensuring both efficiency and quality. It not only supports image generation tasks, but also enables image editing for real images, such as image inpainting and local editing. However, we have observed that the vanilla VQGAN used in StableDiffusion leads to significant information loss, causing distortion artifacts even in non-edited image regions. To this end, we propose a new asymmetric VQGAN with two simple designs. Firstly, in addition to the input from the encoder, the decoder contains a conditional branch that in* Work done during internship at Microsoft corporates information from task-specific priors, such as the unmasked image region in inpainting. Secondly, the decoder is much heavier than the encoder, allowing for more detailed recovery while only slightly increasing the total inference cost. The training cost of our asymmetric VQGAN is cheap, and we only need to retrain a new asymmetric decoder while keeping the vanilla VQGAN encoder and StableDiffusion unchanged. Our asymmetric VQGAN can be widely used in StableDiffusion-based inpainting and local editing methods. Extensive experiments demonstrate that it can significantly improve the inpainting and editing performance, while maintaining the original text-to-image capability. The code is available at https://github.com/ buxiangzhiren/Asymmetric_VQGAN. 1. Introduction Diffusion models have emerged as the most popular generative models, achieving remarkable results in image synthesis. Early diffusion models required significant computational resources, as they performed the diffusion process in the high-dimensional pixel space of RGB images. To reduce the training cost while preserving the generation quality, laten diffusion model (LDM) [31] employs VQGAN [10] to move the diffusion step to a low-dimensional latent space. In a subsequent development, StableDiffusion has further scaled up LDM with a larger model and data scale, resulting in a highly powerful general text-to-image generator. Since its public release, it has drawn significant attention in the world of generative AI. StableDiffusion not only possesses text-to-image generation capabilities, but it also supports various editingrelated tasks, such as inpainting [31] and local editing tasks [5, 24, 40]. For these editing tasks, StableDiffusion can generate new content for selected regions based on user-supplied input condition while aiming to preserve other regions. However, we have observed that the results of StableDiffusion-based editing in all existing methods [31, 40] suffer from distortion artifacts in the non-edited image regions, especially for the regions with fine-grained structures (e.g., texts). For example, as depicted in Figure 1, despite our intention to only inpaint the black mask region or composite the object provided in the reference image into the mask region, we observe severe distortion in the non-mask/non-edited areas. After extensive analysis, we have found that these issues are caused by the quantization error present in the vanilla VQGAN utilized by StableDiffusion. Specifically, VQGAN utilizes an encoder to downsample images multiple times into a latent space, after which the downsampled image vectors are quantified based on a codebook. As a result, quantization errors are inevitable even for the non-edited regions if we only feed the output of the encoder into the decoder, as the vanilla VQGAN operates by default. Additionally, during the inference process, the convolutional layers utilized in VQGAN's encoder impact the feature vectors of non-masked regions due to the masked regions. To this end, we propose a new asymmetric VQGAN with two simple yet effective designs in the decoder part. Firstly, we reformulate the VQGAN decoder as a conditional decoder to better support local editing tasks. This is achieved by incorporating an extra branch that can integrate information from task-specific priors. For instance, for inpainting or local editing tasks, we feed the non-edited regions into this branch so that the decoder can use both the output of the encoder and the original non-edited regions as inputs. In contrast, the vanilla VQGAN decoder only takes the output of the VQGAN encoder as input. Secondly, we enhance the capability of the decoder by using deeper or wider decoders rather than similar complexity as the encoder. This stronger decoder can better preserve the non-edited regions and recover more details from the quantized output of the encoder. Considering that the most time-consuming part of StableDiffusion inference is the iterative diffusion process, this larger decoder only slightly increases the total inference cost. In addition to the inference cost, the training cost of our asymmetric VQGAN is still very cheap. We only need to retrain a new asymmetric decoder while keeping StableDiffusion and the original vanilla VQGAN encoder unchanged. Additionally, by alternatively feeding/not feeding the taskspecific priors into the decoder, our asymmetric VQGAN can naturally support both editing tasks that require the taskspecific priors and pure generation tasks such as text-toimage generation, which do not require such prior input. To demonstrate the effectiveness of our asymmetric VQGAN, we conducted experiments on three different tasks. In inpainting and local editing tasks with masks (paint-byexample [40]), our asymmetric VQGAN achieved state-ofthe-art performance (1.03 FID on the Place dataset [44] and 86.35% CLIP score on the COCOEE dataset [40]). In the pure text-to-image task, our model can also achieve comparable or even better results compared to the original StableDiffusion. Our contributions can be summarized in the below three-folds: • To the best of our knowledge, we are the first that explicitly point out and investigate the distortion problem in StableDiffusion-based editing methods. • We design a new asymmetric VQGAN to address the above distortion issues with two simple yet effective designs. Compared to the typical symmetric VQGAN design, this new design can better preserve the nonedited regions and recover details, while maintaining a low training and inference cost. • Our asymmetric VQGAN achieves state-of-the-art performance on two representative tasks: the inpainting task on the Place dataset [44] and the local editing task (i.e., paint by example [40]) on the COCOEE dataset. 2. Related Work 2.1. Diffusion Models Diffusion models are a powerful family of generative models that have recently evolved and drawn significant attention due to their impressive performance on various tasks. Recent works [8, 35] have demonstrated that diffusion model can achieve astonishing results in high-fidelity image generation, even outperforming generative adversarial networks. Diffusion models are naturally ideal for learning models from complex and diverse data, and many variants have been proposed recently. For instance, Denoising Diffusion Probabilistic Models (DDPMs) [13] are the most popular diffusion models that learn to perform a diffusion process on a Markov chain of latent variables. And Denoising Diffusion Implicit Models (DDIMs) [35] is further proposed to accelerate the denoising process. To improve the efficiency while preserving high generation quality, Latent Diffusion Models (LDM) [31] propose to learn a diffusion model within the latent space rather than the pixel space. Diffusion models have proven to be highly effective in a variety of applications, including image generation [29, 26, 14, 4], image-to-image translation [6, 37, 38], superresolution [31], and image editing [26, 1]. Particularly, recent advancements in diffusion models [34] have led to state-of-the-art image synthesis [8, 10, 9, 26, 32, 11, 2, 31] as well as generative models for other modalities such as text [19], audio [18], and video [15]. In this paper, we focus on designing a new VQGAN architecture that can improve the performance of StableDiffusion like diffusion models that operates in the latent space, for image generation and editing tasks. 2.2. VQGAN The Vector Quantized Variational Autoencoder (VQVAE) [27, 30] is a widely used method for learning discrete image representations. VQ-based techniques have been successfully applied to image generation and completion, leveraging a learned codebook in the feature domain. While [27] extended this approach to use a hierarchy of learned representations, these methods still rely on density estimation from convolutional operations, making it challenging to capture long-range interactions in high-resolution images. In order to address above issues, the Vector Quantized Generative Adversarial Network (VQGAN) [10] was proposed, which identified that a powerful latent space auto-encoder is critical to capture image details for the following generation stage. So VQGAN uses the quantization procedure of VQVAE and improves the richness of VQVAE's learned codebook. VQGAN uses the quantization procedure of VQVAE and improves the richness of its learned codebook. Specifically, it employs adversarial loss and perceptual loss to train a better autoencoder in the first stage for synthesizing greater image details. VQ-based generative models have been applied in many tasks, including text [19], audio [18], image inpainting [22], and video [15] generations. While VQGAN has numerous benefits, the quantization error it introduces can lead to losing image details and causing serious distortion. Motivated by this observation, unlike the conventional symmetric VQGAN design, we explore a new asymmetric VQGAN to retain more details, which can benefit both image generation and editing tasks. Masked Image Masked Image Conditional Branch 2351 7 95 15 823 49 87Encoder Larger Decoder Zm Quantized ZT Stable Diffusion Zo Encoder 23 1 251 7 95 15 8Decoder 23 49 87Zm Quantized ZT Stable Diffusion Zo Output Output Figure 2: Top: The inference process of our symmetric VQGAN. Bottom: The inference process of vanilla VQGAN. 3. Method VQGAN plays an important role in StableDiffusion to map the original high-dimensional pixel space to lowdimensional latent space. However, this mapping process can lead to information loss in image conditional tasks, causing a lack of detail that hurt the quality of the generated result. In this section, we will first discuss the issue of information loss in VQGAN and then introduce our solution, the asymmetric VQGAN, which serves to address this challenge. 3.1. Information loss in VQGAN VQGAN aims to compress the pixel space into discrete latent space. Suppose X Є RH×W×3 is the input image. VQGAN first utilizes a CNN-based Encoder to obtain its feature variable z Є Rhxwxnz, where h× w is the spatial resolution and nz is the channel of the latent vector. Then VQGAN aims to be able to represent it with discrete codebook {z}1, where each spatial code zij find its closest codebook entry Zk in the codebook, the process can be denoted as follows: k=1' Zij = q(ij) = arg min ||Zij – Zk|| kЄ1,2,.., K (1) where q is the quantization encoder that maps the vector to an index of the codebook, Based on the quantized codewords z, VQGAN then adopts the decoder to reconstruct the input image x. Suppose the reconstructed result is = Dec(q(Enc(x)). Then the model and codebook can be trained end-to-end via the loss function: LvQ(Enc, Dec, {zk}k=1) = Lpixel + percep + ||sg[Enc(x)] − z ||22 (2) + ß ||sg [z] – Enc(X) || where Lpixel = ||x-✩||² is pixel-level loss and Lpercep perceptual loss calculated with VGG16 [33] network. sg[] is Input Mask m = Encoder51 7 95 15 823 49 87or Conv MGB m Conv Conv m MGB > m 8 ա Conv Conv Output MGB or Elementwise Multiplication Elementwise Addition Mask Guided Blend Random Mask Full Mask Our asymmetric decoder Masked Image Figure 3: The training process of our symmetric VQGAN. We generate two kinds of masks, i.e., random mask and full mask. The quantized vector from the encoder is fed to our asymmetric decoder. At the same time, the masked image is sent to our decoder as the input of conditional branch. After the conditional and main branches blend, the decoder output final results. the stop-gradient operator, ẞ is a hyper-parameter for loss weight. To further improve the quality of the generated samples, a discriminator is employed to perform an adversarial training process with the encoder and decoder. Since the continuous pixel space is mapped into limited discrete space, the information loss phenomenon exists during this process. We also notice that in some versions of StableDiffusion, the compressing from pixel space to latent space is trained by a KL-reg. The KL-reg shares a similar purpose to VQGAN since they all try to avoid arbitrarily high-variance latent spaces, and both of them share a similar issue: information loss from pixel space to latent space. 3.2. Asymmetric VQGAN Due to the impressive performance of StableDiffusion on text2image generation, it has been widely applied to various conditional image generation tasks. One of the most important and typical of these conditions is the use of image input. However, according to our analysis, these image conditions must be mapped into latent space to satisfy the diffusion process of StableDiffusion. As a result, these conditional images may lose some of their original information during manipulation. Our focus in this paper is to preserve the information of the conditional image input while leaving the pre-trained weights of StableDiffusion unchanged. To this end, we propose the Asymmetric VQGAN, to preserve the information of conditional image input. Asymmetric VQGAN involves two core designs compared with the original VQGAN as shown in Figure 2. First, we introduce a conditional branch into the decoder of the VQGAN which aims to handle the conditional input for image manipulation tasks. Second, we design a larger decoder for VQGAN to better recover the losing details of the quantized codes. In the next section, we will introduce the detailed structure and training strategy of Asymmetric VQGAN. Conditional decoder. We design a conditional decoder that aims to preserve the details of the conditional input. As illustrated in Figure 3, Suppose the conditional image is a masked input Y with mask m, We propose to represent the conditional image input as multi-level feature maps, instead of compressing it into single-layer features. Concretely, we feed the conditional input Yinto a lightweight encoder E and extract the feature map at different layers as the conditional input representation. More formally, we can define (3) fE(Y) = {f}(Y),fE(Y),…fE(Y)}, where f½ (Y) represents the k-th level feature map from the encoder E, n is the number of feature levels. Then these features will be integrated into the decoder via a mask-guided blending (MGB) module. MGB aims to preserve the decoders' capability for decoding latent codes while making full use of the features from encoder E. It utilizes a mask to directly copy the masked region of the decoder feature while combining the unmasked region feature from the encoder E. Specifically, suppose the feature at k-th level of the decoder is f/ Dec (z). So the blending process can be formulated as: fec(z) = fec(z) > m + f / ( >m, (4) where is element-wise multiplication, m = 1 − m. With this designed mask-guided blending module, we do not require any modification to the decoder and just insert several MGB modules into the decoder network while keeping the structures unchanged. Larger decoder. To further enhance the capability of decoders for recovering details from given latent codes, we enlarge the decoder model size of the original VQGAN. Increasing the model size of VQGAN is efficient since during the inference stage of StableDiffusion for conditional image input tasks, the decoder only requires to be forwarded one 40-50% masked Method FID↓ LPIPS↓↓ FID↓ All samples LPIPS↓ LaMa [36] 12.0.2.0.CoModGAN [43] 10.0.1.0.RegionWise [23] 21.0.4.0.DeepFill v2 [41] 22.0.5.0.EdgeConnect [25] 30.0.8.0.StableDiffusion* [31] 8.0.2.Ours 6.80 0.0.1.03 0.Table 1: Comparison of inpainting performance on 30k crops of size 512 × 512 from test images of Places [44]. Since the high-resolution images of Places is not available, we resize the 256 × 256 images to 512 × 512. The column 40-50% reports metrics computed over hard examples where 40-50% of the image region have to be inpainted. * denotes that the results are reproduced by us. time while the StableDiffusion model needs to be forwarded many times with a significantly larger model size. Training strategies. During training, we use the same weights and codebook from the original VQGAN in the Asymmetric VQGAN and only train the new decoder. To avoid the decoder from developing a simple solution of recovering information only from the conditional masked input, we consider two scenarios: one where the mask is randomly generated, and one where the mask is completely filled with 1, meaning the decoder needs to rely on the latent codes to recover the image. We alternately use these two scenarios for 50% of the training process. For the training objectives, we use the pixel level loss, perceptual loss, and adversarial loss as described in Section 3.1 to only update the weights of the Decoder. Asymmetric VQGAN is lightweight and flexible, it keeps the encode and latent space diffusion process unchanged, in which we can leverage all the pre-trained weights of the Encoder and StableDiffusion. We just need to change the decoder part while enjoying the strong capability of StableDiffusion for a wide of tasks. 4. Experiments range In order to demonstrate the outstanding application potential of our model, we conduct sufficient experiments based on our base and large models in three different tasks. Implementation details. In accordance with the training setting used for VQGAN [10] in StableDiffusion [31], we train our asymmetric VQGAN on the ImageNet [7] dataset. During training, we preprocess the image resolution to 256 × 256, and train our base model for 12 epochs. This process took approximately 5 days on 8 NVIDIA VGPUs, with a batch size per GPU of 10 and a learning rate warmed up to 3.6e-4 in the first 5,000 iterations. The learning rate was then decayed with a cosine scheduler. As for our large model, we use 64 NVIDIA V100 GPUs, with a batch size per GPU of 5 and a learning rate warmed up to 7.2e-4 in the first 5,000 iterations. The training for the large model also took around 5 days, with the learning rate decayed using a cosine scheduler too. Evaluation benchmark. For our inpainting task, we evaluated our model using the same protocol as LaMa [36], a recent inpainting model, to generate image masks. Our experiments were conducted on two popular datasets: Places [44] and ImageNet [7]. Due to the unavailability of highresolution images in the Places dataset, we resized the 256 × 256 images to 512 × 512. For the ImageNet dataset, we try to randomly select 3 images from each class in the ImageNet validation dataset. However, since some categories do not have 3 images available, we end up selecting a total of 2,968 images for our experiments. For the paint-by-example task, we evaluate our model using the COCOEE [40] dataset. COCOEE is a COCO exemplar-based image editing benchmark that contains 3,500 source images from the MSCOCO [20] validation set. Each image contains only one bounding box, and the mask region is no more than half of the entire image. The corresponding reference image patch is chosen from the MSCOCO training set. In the text-to-image task, we evaluated our model using the MSCOCO [20] validation set. Following the widelyused "Karpathy" split [17], we used 5,000 images for validation, with each image having approximately 5 captions. Hence, we generated a total of 25,000 images according to the captions. All images were resized to 512 × 512. Evaluation metrics. In the inpainting task, we use FID [12] and LPIPS [42] as metrics to evaluate the quality of our model's predictions. Additionally, to showcase the model's ability to preserve the non-edited regions of the image, we report the mean squared error (MSE) between our predictions and ground truth for such non-edited regions. In the paint-by-example task, we measure the quality of our model's output using the mean squared error (MSE) of nonedited image regions and the CLIP score [28]. The CLIP score evaluates the similarity between the edited region and the reference image. Specifically, we resize the two images to 224 × 224, extract their features via CLIP image encoder, and calculate their cosine similarity. A higher CLIP score indicates that the edited region is more similar to the reference image. In the text-to-image task, we evaluate our model's performance using FID and IS [3] as metrics. 4.1. Evaluation on the Inpainting Task Comparison with state-of-the-art methods. Tableshows the comparison of our inpainting approach with other state-of-the-art methods. Our results demonstrate that applying our asymmetric VQGAN to StableDiffusion can imMethod FID↓ LPIPS↓ Pre_error↓ StableDiffusion [31] 9.0.1082.8e-Ours (base) w/ mask 7.604 0.5.7e-Ours (base) w/o mask 9.0.1078.6e-Ours (large) w/ mask Ours (large) w/o mask 7.0.2.6e-9.0.1047.7e-Table 2: Results of our different modules in inpainting task. StableDiffusion is our baseline. All results are reported in ImageNet validation dataset. Input Naïve Blend Baseline Ours Figure 4: The harmonization of our asymmetric VQGAN in inpainting task. “Naive Blend” denotes that adding the non-edited image regions of inputs and the edited regions of results. StableDiffusion [31] is our baseline. All results are reported on ImageNet validation dataset. prove FID by 1.24. Additionally, our approach outperforms other methods on hard examples where 40-50% of the image region needs to be inpainted, further highlighting the effectiveness of our conditional branch and larger decoder. Effectiveness of our modules. This ablation study aims to support the effectiveness of our conditional branch and larger decoder. Table 2 presents the inpainting results that compare our conditional decoder with the original decoder in StableDiffusion [31]. Specifically, we replace the original decoder with our conditional decoder to decode the quantized vectors obtained from StableDiffusion. It's worth noting that the sample results sent to different decoders are the same since the diffusion process generates various sample results. Both our base model and larger model show an improvement of approximately 2.00 on FID and 50% on LPIPS. Furthermore, the error of non-edited image regions is significantly reduced. These results indicate that with the help of the conditional branch, the ability of VQGAN to preserve details has been greatly improved. If we do not use the conditional branch, our base model can still perform the same function as the original decoder. And our larger model can further outperform the original decoder. These results demonstrate that our model is compatible with various applications, whether they have masks or not. Method FID↓ LPIPS Pre_error_↓ Ours (base) add. 7.0.5.7e-Ours (base) concat. 7.0.5.3e-7.0.2.6e-4.4e-Ours (large) add. Ours (large) concat. 7.56 0.Table 3: Ablation study of different blending ways. The "add." denotes addition and "concat." denotes concatenation. All results are reported in ImageNet validation dataset. Method Blended Diffusion [1] DCCF [39] CLIP score Pre_error_↓ 80.82.StableDiffusion [31] 75.Paint-by-Example [40] Paint-by-Example* [40] 84.85.Ours 86.588.0.Table 4: Comparison of paint-by-example [40] performance on 3500 images of size 512 × 512. * denotes that the results are reproduced by us. All results are reported on COCOEE. Addition or concatenation. Our model offers two different blending methods: mask-guided addition and mask-guided concatenation. This ablation study aims to explore how they improve VQGAN. Since the mask is a hard mask, meaning its value is either 0 or 255, our conditional branch does not use Partial Convolutional Layer [21] when the blending way is mask-guided addition. This means that the main branch is responsible for the edited regions, while the conditional branch handles the non-edited image regions. As a result, the Partial Convolutional Layer, which infers the information of the edited regions, cannot contribute to the conditional branch. The detailed results are shown in Table 3. The overall performance of the concatenation is similar to the addition. Finally, we choose the addition as our blending way. Visual Comparison. In order to preserve the details of non-edited image regions, a common approach is to postprocess by adding the non-edited regions of the inputs to the edited regions of the results. However, this naive soluGUINNESS GUINNESS ARAC INNEGUINNESS DRAG JINNE FOREIGN EXTRA GUINNESS BAGO DRAG INNE FOREIGN EXTRATO กล Toilets Toilets Cra The Cool Frappes Cool Frappes MINSEED AUS MUSEUM AND MUSEUM AND Masked Image Reference Painting by Example Ours Source Image Figure 5: The preserving ability of our asymmetric VQGAN in paint-by-example [40] task. All results are reported in COCOEE dataset. CLIP score Pre_error_↓ Method Paint-by-Example* 85.588.Ours (base) 86.1.Ours (base) w/o mask 86.478.Ours (large) 86.0.Ours (large) w/o mask 86.451.Method StableDiffusion [31] Ours (base) w/o mask Ours (large) w/o mask FID↓↓ IS↑ 19.88 37.19.92 37.19.75 37.Table 5: Results of our different modules in paint-byexample [40] task. Paint-by-Example is our baseline. All results are reported in COCOEE dataset. tion can cause non-harmonization issues, as shown in Figure 4 in the third column. In contrast, our asymmetric VQGAN generates images with the same level of harmonization as the baseline (original StableDiffusion), while preserving many details of the non-edited regions. This indicates that our method effectively preserves details without causing harm to the harmonization of the image. Overall, our results demonstrate that our model with the asymmetric decoder and mask-guided addition blending way can significantly improve the performance of inpainting tasks, while maintaining harmonization and preserving details of nonedited image regions. Table 6: Results of text-to-image task. StableDiffusion is our baseline. Even though there is no any mask, our asymmetric VQGAN is comparable or superior to the baseline, StableDiffusion. All results are reported on MSCOCO. 4.2. Paint by example Paint-by-example [40] is a novel image editing scenario that semantically alters image content based on an exemplar image. Their approach relies on StableDiffusion as a strong prior, making our model easily applicable to their method. Comparison with state-of-the-art methods. The comparison results are presented in Table 4, where the "CLIP score" represents the similarity between the edited regions of edited images and reference images, while Pre_error" denotes the MSE between the non-edited image regions of edited images and source images. Our model can achieve the best performance in both masked and non-edited image regions. It can be seen that our model can achieve the best Input Naïve Blend Baseline Ours Figure 6: The harmonization of our asymmetric VQGAN in paint-by-example [40] task. The inputs are masked source images and reference images. 'Naive Blend” denotes that adding the non-edited image regions of source images and the edited regions of results. Paint-by-example [40] is our baseline. All results are reported on COCOEE dataset. "A red and white bus traveling down a street." "A man standing on a tennis court holding a racquet." "A little locomotive in a museum all shined Input up. Stable Diffusion Ours Figure 7: The comparison between StableDiffusion and ours in text-to-image task. Without the help of our conditional branch, we will get comparable results. performance in both masked and non-edited image regions. Effectiveness of our modules. The purpose of this ablation study is to demonstrate the effectiveness of our conditional branch and larger decoder. The results are presented in Table 5. In contrast to the inpainting task, the paint-byexample task involves combining two different images, resulting in a more severe loss of details in non-edited image regions. The non-edited regions of an image can be influenced by the other image, further complicating the task. Our base model reduces the preserving error from 588.to 1.37, while our larger model further reduces it to 0.76. Surprisingly, we find that the conditional branch not only improves the preservation of non-edited image regions but also enhances the generation quality of edited regions, as measured by the "CLIP score" Visual results. We have provided a significant number of visualization results in Figures 5 and 6 to demonstrate the preserving ability and harmonization of our asymmetric VQGAN. These visualization results illustrate that our model can be effortlessly applied to various tasks on different datasets while consistently enhancing performance. We firmly believe that our model has immense potential for a wide range of applications. 4.3. Text-to-Image These experiments aim to demonstrate that our asymmetric VQGAN can handle tasks without masks or taskspecific priors, in addition to tasks with mask (task-specific priors). The results are presented in Table 6. We can observe that when our base model does not use the conditional branch, its performance is comparable to the baseline, which suggests that the training strategy of replacing some masks with a full mask has been successful. Moreover, when our large model does not use the conditional branch, its performance is comparable to the baseline, indicating that the larger decoder can restore more details even without the help of the conditional branch. Visual results. To support our claim that our asymmetric VQGAN can perform well even without the conditional branch, we present visualization results in Figure 7. In the first row, it is evident that our model can work effectively without masks and does not produce inferior results. In the second and third rows, we observe that our larger decoder can recover more intricate details to some extent. Effectiveness of our modules. The purpose of this ablation study is to demonstrate the effectiveness of our conditional branch and larger decoder. The results are presented in Table 5. In contrast to the inpainting task, the paint-byexample task involves combining two different images, resulting in a more severe loss of details in non-edited image regions. The non-edited regions of an image can be influenced by the other image, further complicating the task. Our base model reduces the preserving error from 588.to 1.37, while our larger model further reduces it to 0.76. Surprisingly, we find that the conditional branch not only improves the preservation of non-edited image regions but also enhances the generation quality of edited regions, as measured by the "CLIP score". Visual results. We have provided a significant number of visualization results in Figures 5 and 6 to demonstrate the preserving ability and harmonization of our asymmetric VQGAN. These visualization results illustrate that our model can be effortlessly applied to various tasks on different datasets while consistently enhancing performance. We firmly believe that our model has immense potential for a wide of applications. range 4.4. Text-to-Image These experiments aim to demonstrate that our asymmetric VQGAN can handle tasks without masks or taskspecific priors, in addition to tasks with mask (task-specific priors). The results are presented in Table 6. We can observe that when our base model does not use the conditional branch, its performance is comparable to the baseline, which suggests that the training strategy of replacing some masks with a full mask has been successful. Moreover, when our large model does not use the conditional branch, its performance is comparable to the baseline, indicating that the larger decoder can restore more details even without the help of the conditional branch. Visual results. To support our claim that our asymmetric VQGAN can perform well even without the conditional branch, we present visualization results in Figure 7. In the first row, it is evident that our model can work effectively without masks and does not produce inferior results. In the second and third rows, we observe that our larger decoder can recover more intricate details to some extent. 5. Conclusion In this paper, we present a novel asymmetric VQGAN for StableDiffusion with two new design features. Firstly, our decoder incorporates an additional conditional branch, allowing it to accept both the output of the VQGAN encoder and task-specific priors as input. Secondly, our decoder is designed to be more complex (e.g. deeper and wider) than the encoder, enabling it to better preserve local details of non-edited regions and recover details from the quantized output of the encoder. Our asymmetric VQGAN architecture is highly efficient for both training and inference, and can be used for local editing tasks and pure text-to-image generation tasks. Through extensive experimentation on two representative tasks, we demonstrate the effectiveness of our asymmetric VQGAN design. Moving forward, we plan to explore whether scaling up the decoder could further improve the quality of our results. References [1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In CVPR, pages 18208-18218, 2022. [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [3] Shane Barratt and Rishi Sharma. A note on the inception score. arXiv preprint arXiv:1801.01973, 2018. [4] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, and Christian Etmann. Conditional image generaarXiv preprint tion with score-based diffusion models. arXiv:2111.13606, 2021. [5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. [6] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938, 2021. [7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248–255. Ieee, 2009. [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780-8794, 2021. [9] Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. Advances in Neural Information Processing Systems, 34:3518– 3532, 2021. [10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 12873-12883, 2021. [11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In CVPR, pages 10696-10706, 2022. [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020. [14] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. J. Mach. Learn. Res., 23(47):1-33, 2022. [15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. [16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. [17] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39:664–676, 2017. [18] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. [19] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. arXiv preprint arXiv:2205.14217, 2022. [20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740-755, 2014. [21] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In ECCV, pages 85-100, 2018. [22] Qiankun Liu, Zhentao Tan, Dongdong Chen, Qi Chu, Xiyang Dai, Yinpeng Chen, Mengchen Liu, Lu Yuan, and Nenghai Yu. Reduce information loss in transformers for pluralistic image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11347-11357, 2022. [23] Yuqing Ma, Xianglong Liu, Shihao Bai, Lei Wang, Aishan Liu, Dacheng Tao, and Edwin R Hancock. Regionwise generative adversarial image inpainting for large missing areas. IEEE Transactions on Cybernetics, 2022. [24] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022. [25] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi, and Mehran Ebrahimi. Edgeconnect: Generative image inpainting with adversarial edge learning. arXiv preprint arXiv:1901.00212, 2019. [26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [27] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Generating diverse structure for image inpainting with hierarchical vq-vae. In CVPR, pages 10775-10784, 2021. [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, 2021. [29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [30] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. [32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. [33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256-2265. PMLR, 2015. [35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [36] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In WACV, pages 2149–2159, 2022. [37] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Semantic image synthesis via diffusion models. arXiv preprint arXiv:2207.00050, 2022. [38] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Sindiffusion: Learning a diffusion model from a single natural image. arXiv preprint arXiv:2211.12445, 2022. [39] Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang Zhao, and Xing Tang. Dccf: Deep comprehensible color filter learning framework for high-resolution image harmonization. In ECCV, pages 300-316, 2022. [40] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. arXiv preprint arXiv:2211.13227, 2022. [41] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Free-form image inpainting with gated convolution. In ICCV, pages 4471–4480, 2019. [42] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, pages 586-595, 2018. [43] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. arXiv preprint arXiv:2103.10428, 2021. [44] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452–1464, 2017. Appendices In the following section, we first introduce the loss objectives for Asymmetric VQGAN, then we present the other technique for compressing image space to latent space KLreg, which shares a similar spirit to VQGAN. Finally, we introduce the architecture of our models. A. Training Objectives For the training of Asymmetric VQGAN, we fix the weights of the encoder and the codebook and employ a reconstruction and adversarial loss to train the decoder. The reconstruction loss is the sum of pixel-loss and perceptual loss. Pixel-level loss is the MAE loss used between each pixel of the output image from the quantized vector z and input image x, which can be denoted by Spixel 3HW |× - ✰, where x = Dec(z) denotes the output image, H and w denote the high and width of the image, respectively. Moreover, the perceptual loss percep is calculated with VGG16 [33] network, and it can be formulated as:HkWk percep(x, x) = - fk (||øk (x) — øk (ŵ)||²),Lpercep = Σ percep (x, ✰) = k== (5) where k {0, 1, 2, 3, 4}, Hk and Wk denote the high and width of the image feature in k-th layer, respectively. f means a convolution operation with 1 × 1 kernel to reduce the channel to 1. And ok (.) is the k-th layer of pretrained VGG16 network. To sum up, the reconstruction loss can be generalized as follows: = LREC(Dec) Lpixel + Lpercep (6) Another important loss to improve the quality of the generated results is the GAN loss, and the equation is defined by: LGAN (Dec, D): = min max Ex [log(D(x))] Dec D + Ex[log(1 − D(✩)], (7) where D denotes a patch-based discriminator [16], Dec denotes the generator which is our asymmetric decoder. Therefore the overall objective for training the decoder model then reads L = LREC (Dec) + ALGAN (Dec, D), where we compute the adaptive weight > according to (8) The VGL] denotes the gradient of its input w.r.t. the last layer L of our decoder and 10-4 is used for numerical stability. B. KL-reg for Training VAEGAN Besides VQGAN, in some versions of StableDiffusion, the compressing from pixel space to latent space is trained by a KL-reg, which can be regarded as VAEGAN. The VAEGAN shares a similar purpose to VQGAN since they all try to avoid arbitrarily high-variance latent spaces. For VAEGAN, the encoder network outputs the mean and covariance of the latent vector, i.e., μ and €. Lkl is used to reduce the gap between the prior and the proposal distributions. Ски = (μμ+sum(exp(€) — € − 1)) . and the VAEGAN loss can be formulated as: LVAEGAN ({Enc, Dec}) = Lpixel + Lpercep + Lkl. (10) (11) We also include a GAN loss (Eqn. 9) to improve the quality. For the training of the asymmetric VAEGAN, the encoder part is also fixed, and we only apply the Lpixel + Lpercep + LGAN (Dec, D) loss functions to update the decoder. The Lk is omitted during the training process. C. Architecture of Our Models We further present the details of the architecture of our base model in Table 7, and the architecture of our large model is shown in Table 8. D. Larger Decoder In contrast to the conventional balanced size between encoder and decoder, we propose to design the decoder to be heavier than the encoder (in-balanced design). This is based on the important observation that the main computation bottleneck of StableDiffusion [31] lies in the diffusion process but not VQGAN. This design can not only improve the quality for both masked and unmasked regions for local editing, but benefit the pure text-to-image generation performance. As shown in Table 9 and Table 10, our method not only benefits the masked region generation quality in local editing (FID and LPIPS improvement for the whole edited image) but also improves the original text-to-image generation task quality upon StableDiffusion, whereas all the blending methods cannot achieve this goal. VGL [pixel] 入 = V GL [LGAN] +8° (9) Branch l-thLayer/kernel PConv2d(3, 3) Output Size 192 x 512 xPConv2d(3, 3) 384 × 512 ×ConditionPConv2d(4, 4) 768 x 256 ×PConv2d(4, 4) 768 × 128 ×PConv2d(4, 4) Out Concat Conv2d(1, 1) GroupNorm (1, 1) Conv2d/(3, 3) Concat Conv2d(1, 1) ResBlock xConcat 768 x 64 x385 × 512 x192 x 512 x192 × 512 ×3 x 512 x769 × 512 x384 × 512 xBranch l-th Layer/kernel Output SizeConditionPConv2d / (3, 3) PConv2d(3, 3) PConv2d(4, 4) PConv2d(4, 4) PConv2d(4, 4) 128 × 512 ×256 × 512 ×512 × 256 ×512 × 128 ×Conv2d(1, 1) ResBlock x192 x 512 x1537 x 256 ×768 x 256 ×384 × 256 ×Upsample Out MainConcat Conv2d(1, 1) GroupNorm (1, 1) Conv2d(3, 3) Concat Conv2d(1, 1) ResBlock xConcat Conv2d(1, 1) ResBlock xUpsample Concat Conv2d(1, 1) ResBlock xUpsample Concat Conv2d(1, 1) ResBlock xUpsample Conv2d/(3,3) 512 × 64 ×257 × 512 ×128 x 512 x128 x 512 x3 x 512 x513 x 512 x256 × 512 xMain Concat 384 x 512 x1537 × 128 ×128 x 512 x1025 x 256 ×Conv2d(1, 1) ResBlock xUpsample Concat Conv2d(1, 1) ResBlock xUpsample Conv2d / (3, 3) 768 x 128 ×768 x 128 ×768 x 256 ×1537 x 64 x768 × 64 ×768 × 64 ×768 x 128 ×512 × 256 xResBlock 256 x 256 ×768 x 64 xAttnBlock 256 × 512 xResBlock 1025 × 128 ×512 × 128 ×512 × 128 ×512 × 256 ×1025 × 64 ×512 × 64 ×512 × 64 ×512 × 128 xTable 8: Architecture of our large model. The way of mask guided blend is concatenation. "PConv" denotes Partial Convolutional Layer [21]. Method Base decoderResBlock AttnBlock ResBlock 512 x 64 xLargel decoder Large 2 decoder FID↓ LPIPS↓ 7.60 0.7.55 0.7.49 0.Pre_error↓ 5.7e-2.6e-2.1e-Table 7: Architecture of our base model. The way of mask guided blend is concatenation. “PConv” denotes Partial Convolutional Layer [21]. The gray font denotes the vanilla decoder. Table 9: Effectiveness of our larger decoder in inpainting task. "Large decoder” denotes we increase the width and depth of the decoder by 1.5 times. “Large×2 decoder" denotes we increase the width of the decoder by 2 times and the depth by 2.5 times. Method Base decoder w/o mask Large decoder w/o mask FID↓↓ IS↑ 19.92 37.19.75 37.Largex2 decoder w/o mask 19.68 37.Table 10: Effectiveness of our larger decoder on text-toimage task.