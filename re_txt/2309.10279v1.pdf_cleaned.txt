--- --2309.10279v1 [cs.CV] 19 SeparXiv 360° Reconstruction From a Single Image Using Space Carved Outpainting Nuri Ryu Minsu Gong Geonung Kim POSTECH POSTECH POSTECH South Korea South Korea South Korea ryunuri@postech.ac.kr gongms@postech.ac.kr k2woong92@postech.ac.kr Joo-Haeng Lee Sunghyun Cho Pebblous POSTECH South Korea South Korea joohaeng @pebblous.ai Pebblous South Korea s.cho@postech.ac.kr (a) Single RGB Input (b) NeuralLift-(c) RealFusion (d) Ours (e) Ours - Novel View Figure 1: Examples of a full 360° view 3D reconstruction from a single RGB image given in (a). The bottom-right image in (a) is the ground-truth mesh corresponding to the input image, while the other bottom-right images in (b)-(e) are reconstructed meshes by each method. The results in (b) and (c) show that the naive usage of the distillation loss and neural density fields leads to sub-optimal novel views and a low-fidelity surface [Melas-Kyriazi et al. 2023; Xu et al. 2023]. On the other hand, our framework successfully generates novel views that resemble the original input image and also reconstructs the 3D object’s surface with high fidelity, as we observe in (d) and (e). Image in (a): rendered from the data in the Objaverse dataset [Deitke et al. 2022] [OHorton, CC BY]. ABSTRACT We introduce POP3D, a novel framework that creates a full 360°view 3D model from a single image. POP3D resolves two prominent issues that limit the single-view reconstruction. Firstly, POP3D offers substantial generalizability to arbitrary categories, a trait that previous methods struggle to achieve. Secondly, POP3D further improves reconstruction fidelity and naturalness, a crucial aspect that concurrent works fall short of. Our approach marries the strengths of four primary components: (1) a monocular depth and normal predictor that serves to predict crucial geometric cues, (2) a space carving method capable of demarcating the potentially unseen portions of the target object, (3) a generative model pretrained on a large-scale image dataset that can complete unseen Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0315-7/23/12...$15.https://doi.org/10.1145/3610548.regions of the target, and (4) a neural implicit surface reconstruction method tailored in reconstructing objects using RGB images along with monocular geometric cues. The combination of these components enables POP3D to readily generalize across various in-the-wild images and generate state-of-the-art reconstructions, outperforming similar works by a significant margin. Project page: http://cg.postech.ac.kr/research/POP3D. CCS CONCEPTS + Computing methodologies — Reconstruction; Computer graphics; Artificial intelligence. KEYWORDS Single-View 3D Reconstruction, Shape and Appearance Reconstruction, Novel-View Synthesis, Space Carving, Outpainting ACM Reference Format: Nuri Ryu, Minsu Gong, Geonung Kim, Joo-Haeng Lee, and Sunghyun Cho. 2023. 360° Reconstruction From a Single Image Using Space Carved Outpainting. In SIGGRAPH Asia 2023 Conference Papers (SA Conference Papers °23), December 12-15, 2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3610548.--- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia 1 INTRODUCTION The ability to generate high-quality realistic 3D models from minimal input is an ongoing challenge for various applications in computer graphics, vision, virtual reality, and augmented reality.Despite the recent advances in the area of multi-view reconstruction through the differentiable rendering of neural representations [Mildenhall et al. 2020; Niemeyer et al. 2020; Sitzmann et al. 2019], such methods rely heavily on vast amounts of images paired with camera parameters. While this reliance may yield impressive results, it inhibits practicality and accessibility, particularly in scenarios where obtaining multiple views of an object is impractical or impossible. In real-world scenarios, a user might only have a single view of an object. For instance, the image may be a photo of an object that is not easily accessible or an output of a 2D generative model. Consequently, 3D model generation from a single image has immense practical significance, enabling a broader range of applications and making 3D modeling more accessible to a wider user base. Due to its practical significance, 3D reconstruction from a single image has been an active area of research. However, existing methods still suffer from two major problems: generalizability and reconstruction fidelity. Various methods have been proposed to learn from 3D data or object-centric videos for single-view reconstruction [Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Nichol et al. 2022; Saito et al. 2019; Wang et al. 2018; Wu et al. 2023a; Xie et al. 2019]. However, the acquisition of such data is often more challenging compared to collecting unstructured 2D data, thereby undermining the scalability and generalizability of these methods. While other techniques have also been proposed to circumvent the need for 3D data by relying on 2D image data, such methods are often bound to a specific category [Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021], thereby limiting their generalizability. Concurrent methods [Deng et al. 2023; Melas-Kyriazi et al. 2023; Xu et al. 2023] that leverage a large-scale image prior [Rombach et al. 2022] via a distillation loss [Poole et al. 2023] frequently fall short of faithfully reconstructing the input view. This discrepancy arises as the distillation loss interferes with the RGB reconstruction loss of the input view and their limited target resolution of the reconstruction further exacerbates this problem. Furthermore, their use of naive neural density fields often leads to low-fidelity surface reconstruction. This paper presents POP3D (Progressive OutPainting 3D), a novel framework designed to address the aforementioned issues of generalizability and reconstruction fidelity.To tackle the challenge of generalizability, our framework leverages the power of various priors pre-trained on large-scale datasets. This approach effectively mitigates the inherent ill-posedness of 3D reconstruction from a single RGB image across arbitrary categories.For a high-fidelity reconstruction covering the full 360° view of an object, we generate novel views that match the quality of the given view through a largescale generative model. These novel views, in conjunction with their monocular geometry predictions, form a pseudo-ground-truth dataset. By training on this dataset following a training strategy tailored to incorporate monocular geometry cues, we reconstruct a neural implicit surface and its corresponding appearance of the Ryu et al. given single image with high fidelity compared to concurrent works as we illustrate in Fig. 1. To elaborate, our framework begins by processing a single RGB input by using state-of-the-art monocular depth and normal predictors [Bhat et al. 2023; Eftekhar et al. 2021] to infer its geometric cues. The input RGB and its monocular geometry predictions constitute an initial dataset and are used to train an initial 3D model following a training strategy of MonoSDF [Yu et al. 2022]. After initializing the 3D model, we update our camera position following a camera schedule that encompasses the full 360° view of the target object. Then, our framework finds the visual hull [Laurentini 1994] of the object, thereby computing the target object’s seen area as well as the potentially unseen area. By removing the seen area, we obtain an outpainting mask, which is used as a guide for the generative model to produce a natural novel view of the object. Specifically, we use a conditional diffusion model [Rombach et al. 2022] trained ona large-scale dataset [Schuhmann et al. 2022] capable of outpainting the image given a mask and a text condition. After a process of extracting the monocular geometry information of the outpainted result, we expand our pseudo-ground-truth dataset with the processed data. This updated dataset is then used to retrain our 3D model and we repeat this gradual outpainting process until we create a dataset that covers a full 360° view of an object, ultimately leading to a high-fidelity 360° 3D reconstruction. Our framework provides some distinctive benefits. Firstly, thanks to the priors learned on large-scale datasets, our framework is not limited to a certain category of objects but can handle a wide range of objects from arbitrary categories. Secondly, our framework does not need any additional external training data such as multi-view images or 3D geometries, as we adopt priors already learned in off-the-shelf models. Thirdly, our progressive outpainting approach that builds a 360°-view dataset of the target object ensures the generation of novel-view images of high quality and the faithful reconstruction of the input image. Finally, by using the pseudoground-truth dataset to train a neural implicit surface representation, we can extract a well-defined high-quality surface. To summarize, our primary contributions are as follows: ¢ We introduce a novel framework to reconstruct a full 360° model from a single image. Our framework generalizes well to in-the-wild RGB images without any category-specific pre-training by leveraging off-the-shelf priors. ¢ We develop a progressive outpainting scheme to generate pseudo-ground-truth images for 3D model reconstruction. Our method ensures a faithful reconstruction with novelview images that naturally harmonize with the input image. Our model design accounts for both geometric and photometric consistency leading to high-fidelity shape and appearance reconstruction. ¢ We show that our framework can produce state-of-the-art 360° reconstruction results from single RGB images in terms of novel-view synthesis and geometry reconstruction. 2 RELATED WORKS 2.1 Few-View-to-3D Reconstruction NeRF [Mildenhall et al. 2020] and its variants [Barron et al. 2021, 2022; Verbin et al. 2022; Zhang et al. 2020] have shown remarkable --- --360° Reconstruction From a Single Image Using Space Carved Outpainting reconstruction performance of scenes and objects only given RGB images paired with camera poses. However, without dense camera views, training a neural radiance field becomes a severely underconstrained problem. When only given a few views, such models may overfit to each given view resulting in a broken geometry and blurry noise when rendering novel views [Jain et al. 2021]. Recently, a line of work has been introduced to reduce the number of required views for high-fidelity reconstruction [Jain et al. 2021; Roessle et al. 2022; Sajjadi et al. 2022; Verbin et al. 2022; Yu et al. 2021; Zhou and Tulsiani 2023]. Nevertheless, they still require more than a single view for proper reconstruction. 2.2 Single-View-to-3D Reconstruction Most of the early work that reconstruct 3D models from a single image rely on the visible information given in an image such as shading [Zhang et al. 1999], texture [Loh 2006], or defocus [Favaro and Soatto 2005]. Recent works use a more general prior in order to generate the invisible parts of an input image. For instance, some methods use 3D datasets to learn a 3D prior that can be used for reconstruction [Choy et al. 2016; Girdhar et al. 2016; Groueix et al. 2018; Saito et al. 2019; Wang et al. 2018; Xie et al. 2019]. However, a large-scale 3D dataset is needed for such models to generalize to inthe-wild images. Compared to large-scale 2D image datasets such as LAION-5B that offers 5.85 billion image-text pairs [Schuhmann et al. 2022], 3D datasets are often limited in variety and scale. On the other hand, our model does not require any 3D training data but can generalize to in-the-wild images by leveraging geometry and image priors [Bhat et al. 2023; Eftekhar et al. 2021; Rombach et al. 2022] trained on large-scale datasets. To overcome the issues arising from needing a 3D training dataset, methods that learn 3D structures from image collections have been introduced [Chan et al. 2023; Gu et al. 2023; Guo et al. 2022; Henzler et al. 2019; Jang and Agapito 2021; Kanazawa et al. 2018; Karnewar et al. 2023; Lin et al. 2023; Pavllo et al. 2023; Vasudev et al. 2022; Wu et al. 2023b; Ye et al. 2021]. However, they either need further annotations such as semantic key points and segmentation masks [Kanazawa et al. 2018] or multi-view images of the same scene with accurate camera parameters [Chan et al. 2023; Gu et al. 2023; Guo et al. 2022; Karnewar et al. 2023; Lin et al. 2023; Vasudev et al. 2022]. Other methods that train with single view per scene are category-specific [Henzler et al. 2019; Jang and Agapito 2021; Pavllo et al. 2023; Wu et al. 2023b; Ye et al. 2021]. In contrast, our model does not require any additional information apart from a single RGB image thanks to the off-the-shelf models that we incorporate. Also, we stress that our model can generalize to in-the-wild images regardless of the given view’s category. While 3D diffusion models [Shue et al. 2023; Wang et al. 2023b] are also gaining attention, concurrent works [Deng et al. 2023; SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia single view. This often disruy often leads to a poor reconst: ts the RGB loss and consequently ruction of the input view. While a very recent work [Tang et al. 2023] tries to bypass this problem by projecting the reference image on to the trained 3D representation, novel views far from the reference view tend to lack quality. In contrary, our framework buil composed of multi-view image: Is up a pseudo-ground-truth dataset 's that allow for the use of multi-view reconstruction strategies leading to high-fidelity reconstructions. Furthermore, these models 2023; Tang et al. 2023; Xu et Deng et al. 2023; Melas-Kyriazi et al. al. 2023] have other limitations as well. Firstly, they have a low target resolution, e.g., 96 x 96 or 128 x 128, while our method aims for a resolution of 384 xMelas-Kyriazi et al. 2023; Tang et al. to directly use a 2D diffusion model on a large-scale image-text dataset prior for single view reconstruction. from the reference view, they heavi 2023; Xu et al. 2023] attempt Rombach et al. 2022] trained Schuhmann et al. 2022] as a . To generate unseen regions ily rely on a distillation loss similar to the score distillation sampling loss introduced by Poole et al. [2023]. The problem is that the distillation loss is simultaneously applied to views that have overlap; ing regions from the given yielding results with higher quality and overall detail. While Tang et al. [2023] try to overcome the this problem through a two-stage training scheme, it shares the other problems described below as well. Secondly, these works use naive neural density functions as their geometry representations, which may produce noisy artifacts due to the lack of a well-defined surface threshold. In contrast, our method simply allows for high-fidelity geometry extraction from the zero-level set of the learned neural implicit surface. Lastly, these models only rely on the given single image and its augmentations to personalize the diffusion model using a method similar to Textual Inversion [Gal et al. 2023] in an attempt to generate unseen regions consistent with the input image. In contrast to these methods, our data generation framework allows the use of a state-of-the-art diffusion model personalization method, DreamBooth [Ruiz et al. 2023], that requires multiple views of the same object by using multi-view pseudo-ground-truth images, which allows for a better personalization quality. Raj et al. [2023] also showed that highquality personalized text-to-3D can be achieved using DreamBooth. However, their method requires multiple views of a target object whereas our method only requires a single view of an object thanks to our pseudo-ground-truth multi-view generation scheme. Single View to Point Cloud. Other recent works aim to reconstruct colorized point clouds based on a reference view. For instance, MCC [Wu et al. 2023a] takes an RGB-D image as input and reconstructs the lifted color points into a complete point cloud of the target object. Similarly, Point-E [Nichol et al. 2022] introduces a point-cloud diffusion model that uses a reference RGB image to generate a colorized point cloud that resembles the input image. Unlike such models, our framework reconstructs a high-fidelity neural implicit surface and an appearance of superior quality. 3D Photography. Another line of work utilizes monocular depth predictions and color inpainting to generate a 3D photo or scene from a single image [Han et al. 2022; Hdllein et al. 2023; Shih et al. 2020; Zhang et al. 2023]. However, such methods are only designed to inpaint both the foreground and background at the same time, and does not account for the backside of an object. Therefore, they are not directly applicable to 360° reconstruction of an object. Novel View Synthesis from a Single View. Some works [Liu et al. 2023; Watson et al. 2023] focus on generating a 3D novel view when given an input image and a relative pose. However, the outputs of such models are only approximately 3D consistent and therefore do not guarantee a high-fidelity shape reconstruction. --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia Ryu et al. Initialization => Camera Position Update Initialization Process < a é RGB Train ry Single Initial Initialized Updated RGB Dataset 3D Model Camera Outpainting “A photo of [V] [Class] in a white background, A / seen from [Dir]" Volume Rendering VATrained 3D Model Initial Novel View => Outpainting Mask Acquisition = Camera Position Update Previous Outpainting =» 3D Model Update Outpainting Mask Acquisition So Subtract Seen Area Visual Hull , Extraction Acquire Silhouette at Updated Camera Outpainting Mask Camera 3D Model Update Process J J intels Train LM —) é Z F Updated Pseudo-Ground-Truth Dataset Trained 3D Model Figure 2: Framework overview. POP3D operates in five interconnected steps. Initially, we process the single RGB input to create a preliminary pseudo-ground-truth dataset and use this data to initialize a 3D model. We then progress through a loop of steps aiming to cover the complete 360° view of the target object. This loop includes: updating the camera position according to a predetermined schedule; acquiring an outpainting mask by extracting the visual hull from the pseudo-ground-truth dataset and subtracting the seen area; generating a pseudo-ground-truth novel view using the initial novel view from the trained 3D model, outpainting mask, and a suitable text prompt; and training the 3D model using the updated pseudo-ground-truth dataset. This process continues until we encompass the 360° view of the object. Input image: rendered from the data in the Objaverse dataset [Deitke et al. 2022] [Olaboratorija, CC BY-NC-SA]. Figure 3: Visual hull extraction. We illustrate the acquisition of the visual hull from two camera views. We preserve the shaded region seen on the right which constitutes both seen and potentially unseen regions of a target object. Car image: from freesvg [Public Domain]. 3 METHOD We present an overview of POP3D in Fig. 2. Given a single image of an object, our framework reconstructs its 360° shape and appearance using a neural implicit surface representation. Our key idea is to progressively outpaint the unseen regions of the object by synthesizing their color and geometric information. To this end, our framework consists of five steps: initialization, camera position update, outpainting mask acquisition, outpainting, and 3D model update. In the initialization step, we estimate the depth and normal maps of the input image and lift it to a 3D view. Then, we update the camera position to a nearby viewpoint that has not been seen before, and obtain an outpainting mask that indicates the region to be outpainted using space carving [Kutulakos and Seitz 1999]. Next, we outpaint the masked region by generating its color and geometric information using a latent diffusion model (LDM) [Rombach et al. 2022]. Finally, we update the 3D model of the object using the outpainted information. We repeat these steps until we cover the entire 360° of the object. To represent the shape and appearance of a 3D object, we adopt VoISDF [Yariv et al. 2021], which represents a 3D object using a pair of neural networks. Specifically, to represent the geometry of an object, we use a neural network modeling a signed distance function (SDF) fg : x +» s, which maps a 3D point x € R° to its signed distance s € R to the surface. To account for the appearance, we use another neural network that models a radiance function Lo (x, i, 2) where fi is the spatial gradient of the SDF at point x. Z is the global geometry feature vector same as in Yariv et al. [2020]. Unlike VoISDF, we do not give the viewing direction as input to Lg and ignore view-dependent color changes as a single image does not provide view-dependent lighting information and conventional outpainting methods do not account for view dependency. As 3D model generation from a single image is an extremely ill-posed task, we impose a couple of assumptions to restrict the possible outcomes of the reconstruction results. First, we assume that the target object lies within a cube, which has its center at the origin, and edges of length 2 aligned with the coordinate axes, and initialize the object as a unit sphere following Atzmon and Lipman [2020]. We also assume a virtual camera looking at the target 3D object during our 3D reconstruction process. Specifically, we place the camera on a sphere of radius 3 to point at the origin and parameterize its position using spherical coordinate angles. The field of view (FoV) of the camera is set to 60° assuming that the camera parameters of the input image are not given. In the following, we describe each step of our framework in detail. --- --360° Reconstruction From a Single Image Using Space Carved Outpainting 3.1 Initialization The initialization step constructs an initial 3D model from an input image Lp of a target object. Specifically, given Lo, we first extract the foreground object by estimating a binary mask Mo using an off-the-shelf binary segmentation method [Lee et al. 2022]. We then estimate the depth map Dp and the normal map Np for the foreground object using off-the-shelf monocular depth and normal estimators [Bhat et al. 2023; Eftekhar et al. 2021]. Using the estimated depth and normal maps, and binary mask, we estimate an initial 3D model. Specifically, we first initialize the pseudo-groundtruth dataset P as P = {(Lo, Do, No, Mo, ¢o)} where ¢o is the initial camera position, and train the implicit representation of the initial 3D model (fo, Lg) using P. The initial camera position is set to ¢o = (90°, 0°) where the first and second angles are the polar and azimuthal angles, respectively, assuming that the initial image contains the frontal side of the target object. The pseudo-ground-truth dataset is iteratively updated in the following steps to progressively reconstruct the 3D model of a target object. For training the implicit representation, we adopt the approach of MonoSDF [Yu et al. 2022] with a slight modification to consider the mask Mo. Refer to the supplementary material for more details on the training. 3.2 Camera Position Update After the model has been initialized, we iteratively update the 3D model exploring the unseen regions of the target object by changing the camera position. To this end, we define a camera schedule S= [¢o. Pres ds], designed to cover the 360° view of the target object, and update the camera position at each iteration according to S. In theory, the camera schedule may be an arbitrary set provided that it encompasses the complete 360° view. However, we found that an excessively small or large interval may detrimentally affect the output. Hence, we use an interval of 45° degrees in our experiments, and discuss the adverse impacts of an overly granular or coarse camera schedule in Sec. 4.2.1. In the rest of the section, we will denote the camera positions that have been explored until the i-th camera position in S as So,; such that 0 <i <s. 3.3. Outpainting Mask Acquisition In order to generate the appearance and shape of unseen regions seamlessly, the areas designated for outpainting need to be appropriately chosen. To address this, we leverage the concept of the visual hull [Laurentini 1994]. The visual hull provides a rough approximation of the object’s shape derived from the object’s silhouettes from different viewpoints. Using our current dataset P, we can compute the visual hull to determine the maximum possible area that the object might occupy as illustrated in Fig. 3. By computing the silhouette of the visual hull seen from the updated camera view, we obtain an initial mask that comprises both previously observed and potentially unseen regions. To create our outpainting mask, we subtract the observed regions from this initial mask, leaving only the potentially new visible areas. Visual Hull Computation via Space Carving. For the computation of the visual hull [Laurentini 1994], we use a depth-based voxel carving method driven by a voting scheme [Kutulakos and Seitz 1999]. The process first voxelizes the object-bounding cube. Now SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia we assume that we have explored the camera positions of So.j-1. Then, for a voxel p from the voxelized bounding cube, we raise a vote if its projection to the j-th view is inside the foreground region where j € {0,...,i— 1}, and if its distance to the j-th view’s camera center is longer than the distance between the foreground region and the j-th view’s camera center. Mathematically, we raise a vote for p if KPjpeé My. and * (1) IIp - ojllz = IIp* — ojlla. where K is the camera intrinsic matrix. P; and 0; represent the projection to the camera space and the camera center of the j-th view in P, respectively. p* is the point of intersection between the zero-level set of fg and the ray cast from 0; towards p. Here, we only consider the first intersection where the ray penetrates the object from the exterior for the first time. If the total number of vote counts equals the size of P, or the number of views, the voxel is preserved. Otherwise, the voxel is carved out. Through this procedure, we collect the voxels comprising the visual hull of P. To add, when we only have a single image, this process can be thought of an extrusion of the trained 3D surface. By projecting the visual hull onto the i-th viewpoint, we obtain its silhouette My". Foreground Mask Computation via Warping Operation. Since Mya contains both seen and unseen regions, we should subtract out the seen region in order to obtain our outpainting mask Mj. This is achieved by using a warping operation to compute the foreground mask Mrs in the target view. The process involves rendering the depth from the previous viewpoints Sp.j-1, lifting the image points to the 3D space, and subsequently projecting the lifted points to the target view ¢;. To mitigate aliasing during the warping process, we scale up the image by a scaling factor of 8. We account for visibility and do not warp pixels not visible from the target viewpoint via back-point culling. Specifically, the warping process including the back-point culling is performed as follows. In the process of warping an image from the j-th view to the i-th view, we denote a lifted pixel from the j-th image in P as p/. Then, we render its normal Ni from fo. The pixel is only warped for the target camera center o if: (p’ - oi) -NI <0 (2) The warped coordinate p' is then computed as: p' =KP).;K"'p! (3) where P;—,; denotes a 4 x 4 transformation matrix that warps the camera from the source position to the target position. A binary mask ofc is computed from the collection of the warped pixels. Then, M; is calculated as Mj = Myt - Mrs. 3.4 Pseudo-Ground-Truth Generation In order to reconstruct the 360° shape and appearance of the target object, we generate pseudo-ground-truth images to fill in the unseen parts of the object. For this purpose, we use a pretrained state-ofthe-art generative model. Specifically, we use the Latent Diffusion Model (LDM) [Rombach et al. 2022] that takes an RGB image, a --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia 66.(a) Input RGB (b) Ours (c) RealFusion (d) NeuralLift-Figure 4: Qualitative comparison on the input image reconstruction. Given a single input image (a), our method successfully reconstructs the reference view as seen in (b). However, RealFusion [Melas-Kyriazi et al. 2023] and NeuralLift360 [Xu et al. 2023] do not faithfully reconstruct the input view even when it utilizes an RGB reconstruction loss. Input image in (a): rendered from the data in the Objaverse dataset [Deitke et al. 2022] [©laboratorija, CC BY-NC-SA]. mask condition, and a text condition as input and outputs an RGB image following the input conditions. However, naively using a pretrained diffusion model may result in outpainting results that do not resemble the reference image. To generate pseudo-ground-truth images that are coherent to the given single view, we adopt a personalization technique outlined in DreamBooth [Ruiz et al. 2023]. This technique allows us to learn a unique identifier token [V] of the object, which can be included as part of the text condition. Since our framework generates multiview pseudo-ground-truth images, we can trivially apply such a technique to generate well-harmonized results. For the details of the conditional diffusion model and its personalization, we refer the readers to the supplementary document. As the inputs to the personalized LDM, we use: e J; - the RGB image rendered from the trained model at the updated camera view, © M; - the outpainting mask at the updated camera view, as detailed in Section 3.3, and ¢ a text prompt designed to generate view-consistent results. For the text condition, we utilize a prompt structured as “A photo of [V] [Class] in a white background, seen from [Dir]” where [V] represents the personalized unique identifier of the specific object, [Class] refers to a simple class keyword such as ‘hamburger’ or ‘doll’, and [Dir] is a directional keyword such as ‘front’, ‘left’, ‘right’ and ‘behind’ used to guide the generation following the approach of Poole et al. [2023]. Upon obtaining the outpainted view, we apply 2x super-resolution [Wang et al. 2023a] for image enhancement. We then employ offthe-shelf monocular depth [Bhat et al. 2023] and normal [Eftekhar et al. 2021] estimators to extract geometric predictions Dj and Nj. Furthermore, we use a background remover [Lee et al. 2022] to obtain the foreground mask Mj. Finally, we update the pseudoground-truth dataset P as P — P U {(Lj, Dj, Ni, Mi, $i) } 3.5 3D Model Update Using the updated pseudo-ground-truth dataset P, we train the SDF fg and neural radiance field Lg following MonoSDF [Yu et al. 2022]. After retraining the target 3D model, we return to the camera position update step described in Sec. 3.2, and continue the loop until we go through the whole camera schedule S. Ryu et al. ; (c) Ag = 90° (a) Input RGB (b) Outpainting results with Ag = 15° Figure 5: Effect of camera intervals on outpainting results. For a single RGB input (a), both camera intervals excessively small (b) and overly large (c) have their drawbacks in their own ways as described in Sec. 4.2.1. Input RGB: generated using a diffusion model [Rombach et al. 2022]. SVE (b) Initial novel view ‘a) Previous Grow View (c) Outpainting result na DreamBooth (d) Outpainting result w/ DreamBooth Figure 6: Effect of diffusion model personalization. After updating the camera view from (a) to (b), we outpaint the target object with the same input to the LDM. However, naive usage of the LDM may result in an outpainting result that does not consider the images in the pseudo-ground-truth dataset such as a doll with a duplicate face in (c), rather than naturally outpainting the doll’s hat as shown in (d). Input image of this experiment: rendered from the data in the Objaverse dataset [Deitke et al. 2022] [Oshirava, CC BY]. 4 EXPERIMENTS This section presents experimental results to evaluate the reconstruction quality of our framework in terms of shape and appearance. For all the experiments, we employ a camera schedule S whose polar angles are 90° and the azimuthal angles are [o°, 45°, —45°, 90°, —90°, 135°, -135°, 180°| . Nevertheless, our framework is not limited to a particular camera schedule. We show more qualitative results using a customized camera trajectory in the supplementary document. For comparison, we utilize objects reconstructed using photogrammetry from Objaverse [Deitke et al. 2022]. Our selection of ten object categories provides a diverse range of shapes and appearances for testing. We compare our method with concurrent works [Melas-Kyriazi et al. 2023; Nichol et al. 2022; Wu et al. 2023a; Xu et al. 2023] that can reconstruct 360° appearance and shape from a single reference view, along with a single-imagebased 3D shape reconstruction method [Vasudev et al. 2022]. To ensure a fair comparison, we use the same off-the-shelf depth estimator [Bhat et al. 2023] for methods that require monocular depth guidance [Melas-Kyriazi et al. 2023; Wu et al. 2023a; Xu et al. 2023]. 4.1 Comparisons with Other Methods Input-View Reconstruction. Given a single RGB input, we expect the model to faithfully reconstruct the given image after the training process. Therefore, we compare our framework with methods that also use an RGB reconstruction loss during training to test the input-view reconstruction capability. To inspect the fidelity of the reference-view reconstruction, we use commonly used image --- --360° Reconstruction From a Single Image Using Space Carved Outpainting quality metrics: PSNR, SSIM [Wang et al. 2004], and LPIPS [Zhang et al. 2018]. Our method outperforms the others in all categories as shown in Tab. 1. As discussed in Sec. 1, concurrent works exhibit lower input-view reconstruction performance since the RGB loss is affected by the distillation loss in similar viewpoints. However, our framework that trains directly on a multi-view pseudo-groundtruth dataset does not face such a problem as we observe in Fig. 4. Novel-View Synthesis. For the evaluation of novel-view synthesis, we evaluate the results in terms of the similarity to the ground-truth views and the overall image quality of the output. CLIP similarity [Radford et al. 2021] is used to evaluate the similarity between the model outputs and their corresponding ground-truth views. The image qualities of the generated outputs are assessed via the NIQE score [Mittal et al. 2013]. We evenly sample views around a 360° trajectory, resulting in a total of 100 views for comparison. Tab. 2 presents a quantitative comparison. As shown in the table, our method consistently shows high CLIP scores and outperforms the others in NIQE scores by a large margin for all categories. This shows that our method can generate novel views that are semantically similar to the given single view while maintaining high quality. Qualitative comparisons of novel-view synthesis and their corresponding shapes are presented in Figs. 8 and 9, where it can be observed that our method generates natural-looking novel views throughout the entire 360° trajectory. In contrast, concurrent methods [Melas-Kyriazi et al. 2023; Xu et al. 2023] often produce results that hardly resemble the input RGB images since their diffusion model personalization only relies on the single input view and its augmentations. In contrast, we leverage diffusion model personalization using multiple generated views, leading to a more coherent output. Moreover, our framework’s utilization of neural implicit surface representation effectively reduces the introduction of foggy artifacts commonly seen in methods that adopt a more simplistic use of neural density fields. Compared to the methods that reconstruct colorized point clouds [Nichol et al. 2022; Wu et al. 2023a], our framework generates novel views with much finer details. 4.2 Ablations 4.2.1 Outpainting Errors and the Camera Schedule Interval. While we may use any camera schedule as long as it covers the entire 360° of a target object in theory, during the outpainting process, we identified two key factors that may precipitate failure scenarios. The first issue arises when the outpainting mask barely extends beyond the object’s boundary. In this instance, the input image dominates the input condition, making the outpainting process highly sensitive to any artifacts in the immediate vicinity of the outpainting region. By repeating the outpainting process, such boundary artifacts are accumulated, which often leads to failure cases. The second issue manifests when the outpainting mask is excessively large compared to the object region in the original image. In this case, the outpainting tends to generate an image that adheres largely to the text prompt, thus neglecting the input image. Consequently, excessively granular or large camera intervals may result in reconstruction failures as depicted in Fig. 5. To mitigate these issues, we use an interval size of 45° in our experiments. Empirically, this interval size effectively circumvents outpainting failures, thereby facilitating the reconstruction of high-fidelity 360° SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia Table 1: Quantitative comparison of the PSNR, SSIM and LPIPS scores. Our model shows a large margin in terms quantitative result of the reference view reconstruction. NL and RF stand for NeuralLift [Xu et al. 2023] and RealFusion [MelasKyriazi et al. 2023], respectively. PSNR T SSIMT LPIPS | NL RF Ours | NL RF Ours | NL RE Ours. Berry 17.47 27.85 32.30 | 0.88 0.94 0.99 | 0.21 0.12 0.Broccoli 18.28 14.73 37.66 | 0.85 0.83 0.99 | 0.24 0.28 0.Cactus 17.33 22.99 31.50 | 0.90 0.92 0.98 | 0.17 0.16 0.Cauliflower | 15.50 27.81 33.60 | 0.86 0.93 0.99 | 0.23 0.16 0.Croissant 12.60 29.68 36.60 | 0.82 0.96 0.99 | 0.29 0.10 0.Doll Statue | 14.68 13.89 39.49 | 0.85 0.87 0.99 | 0.21 0.23 0.Frog Statue | 14.62 20.27 36.70 | 0.90 0.91 0.99 | 0.20 0.19 0.Owl 16.41 27.75 36.01 | 0.84 0.91 0.99 | 0.23 0.17 0.Pear 10.26 15.86 40.85 | 0.67 0.88 0.99 | 0.44 0.17 0.Skull 1.79 24.99 36.30 | 0.14 0.94 0.99 | 0.80 0.13 0.Mean 13.89 22.58 36.10 | 0.77 0.91 0.99 | 0.30 0.17 0.Table 2: Quantitative comparison of the CLIP similarity and NIQE scores. Our model not only achieves the best embedding similarity but also achieves the best image quality score. NL, RF, MCC, and P-e stand for NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], and Point-E [Nichol et al. 2022], respectively. CLIPT NIQET NL RF MCC Pe Ours] NL RF MCC Pe Ours Berry, 0.77 0.81 0.76 0.84 0.82 | 22.81 9.56 23.35 19.33 5.Broccoli | 0.86 0.88 0.69 0.80 0.84 | 27.79 11.65 20.28 21.57 9.Cactus | 0.79 0.82 0.69 0.84 0.89 | 24.92 15.39 26.57 22.70 9.Cauliflower | 0.82 0.80 0.74 0.77 0.87 | 22.58 12.76 22.61 19.91 7.Croissant | 0.74 0.76 0.70 0.78 0.86 | 29.07 13.86 24.10 32.01 12.Doll Statue | 0.78 0.77 0.67 0.85 0.84 | 21.46 21.75 24.49 31.42 18.Frog Statue | 0.76 0.77 0.81 0.83 0.85 | 22.30 15.13 18.22 19.63 8.Owl 0.81 0.76 0.67 0.80 0.86 | 11.19 11.82 22.99 16.93 7.Pear 0.81 0.86 0.81 0.82 0.88 | 26.36 10.49 25.01 17.88 9.Skull 0.71 0.83 0.76 0.81 0.87 | 24.62 13.25 1985 16.18 7.Mean 0.78 0.80 0.73 0.81 0.86 | 23.31 13.57 22.75 21.76 10.views of a target object. Nevertheless, while the suggested interval size may serve as a good starting point, the best warping angle interval or the camera schedule itself may vary for various objects. We show examples of more customized intervals in the supplementary. 4.2.2 Outpainting Results Without LDM Personalization. As our model architecture generates multiple pseudo-ground-truth views of a target object throughout the reconstruction process, it allows for the personalization of the pre-trained LDM [Rombach et al. 2022] using the state-of-the-art technique, DreamBooth [Ruiz et al. 2023]. The benefit of this approach is evidenced in Fig. 6, where the application of the personalized LDM [Rombach et al. 2022] generates a natural-looking novel view that seamlessly integrates with the pseudo-ground-truth dataset. In contrast, naive reliance on the vanilla LDM may result in an outpainting outcome that does not reflect the previously seen views of the target object. 5 CONCLUSION AND FUTURE WORK In this study, we present POP3D, a novel framework that addresses two long-standing challenges in the domain of 360° reconstruction from a single RGB image: generalization and fidelity. POP3D fully leverages current state-of-the-art priors trained on large-scale datasets and successfully overcomes the problem of generalization --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia y (a) Input RGB (b) Novel Views (c) Complete Image Backside Figure 7: Limitations. Given an input RGB (a) on the left, our model generates plausible novel views (b) following a camera schedule. However, our model sometimes generates a subpar complete backside image when compared to the input single RGB, as we see in (c). Image in (a): rendered from the data in the Objaverse dataset [Deitke et al. 2022] [©ShekhirevaVictoria, CC BY]. to arbitrary images. Experimentally, we demonstrate that our framework not only faithfully reconstructs the provided single RGB input but also generates realistic novel views. These views collectively form a pseudo-ground-truth multi-view dataset, facilitating the direct application of multi-view reconstruction strategies. Compared to existing methodologies, our approach exhibits superior performance, reaffirming its potential as a robust solution for 3D reconstruction tasks. 5.1 Limitations Our approach does exhibit certain limitations. Since our framework is a composition of off-the-shelf priors each playing a significant role in the pipeline, a failure of one model may impact the final reconstruction result. For instance, if the monocular depth or normal predictors fail on challenging cases, e.g., thin structures, this may lead to artifacts in the reconstructed shape. Moreover, our framework occasionally generates the object’s complete backside with subpar quality when compared to the input view. We illustrate this problem in Fig. 7. This deficiency may be attributed to an accumulation of outpainting artifacts, which could compromise the performance of off-the-shelf priors and degrade the overall image quality in the long term. Since our approach incrementally increases the number of views through the generation of pseudo-ground-truth images, the computational time associated with 3D model training also escalates along the camera schedule. The current run time for the reconstruction of a single object takes around seven hours using a single 3090 RTX GPU. Nevertheless, our framework has a modular structure and it would be easy to replace the models used in each step. Especially, we may replace VolSDF [Yariv et al. 2021] with more advanced methods [Rosu and Behnke 2023; Wang et al. 2022] to accelerate the reconstruction process. Furthermore, we may adopt LoRA [Hu et al. 2022] for accelerating DreamBooth [Ruiz et al. 2023]. To address these issues, our future research will focus on exploring methods to minimize any artifacts and further refine the reconstruction process while improving the reconstruction time. ACKNOWLEDGMENTS This research was supported by IITP grants funded by the Korea government (MSIT) (IITP-2021-0-02068, IITP-2019-0-01906), and the Starting growth Technological R&D Program (TIPS Program, Ryu et al. (No. S3200141)) funded by the Ministry of SMEs and Startups (MSS, Korea) in 2021. REFERENCES Matan Atzmon and Yaron Lipman. 2020. SAL: Sign Agnostic Learning of Shapes From Raw Data. In Proc. of CVPR. Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, and Pratul P. Srinivasan. 2021. Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. In Proc. of ICCV. Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. 2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In Proc. of CVPR. Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Miller. 2023. ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth. arXiv:2302.12288 [es.CV] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. 2023. Generative Novel View Synthesis with 3D-Aware Diffusion Models. arXiv:2304.02602 [es.CV] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 2016. 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction. In Proc. of ECCV. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. 2022. Objaverse: A Universe of Annotated 3D Objects. arXiv:2212.08051 [cs.CV] Congyue Deng, Chiyu “Max” Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, and Dragomir Anguelov. 2023. NeRDi: Single-View NeRF Synthesis With Language-Guided Diffusion As General Image Priors. In Proc. of CVPR. 2063720647. Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. 2021. Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets From 3D Scans. In Proc. of ICCV. 10786-10796. P. Favaro and S. Soatto. 2005. A geometric approach to shape from defocus. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 27, 3 (2005), 406417. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. 2023. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In Proc. of ICLR. R. Girdhar, D.F. Fouhey, M. Rodriguez, and A. Gupta. 2016. Learning a Predictable and Generative Vector Representation for Objects. In Proc. of ECCV. Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan Russell, and Mathieu Aubry. 2018. AtlasNet: A Papier-Maché Approach to Learning 3D Surface Generation. In Proc. of CVPR. Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. 2023. NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion. In Proc. of ICML. Pengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel Ulbricht, Joshua M. Susskind, and Qi Shan. 2022. Fast and Explicit Neural View Synthesis. In Proc. of WACV. 3791-3800. Yuxuan Han, Ruicheng Wang, and Jiaolong Yang. 2022. Single-View View Synthesis in the Wild with Learned Adaptive Multiplane Images. In Proc. of ACM SIGGRAPH. Philipp Henzler, Niloy J Mitra, , and Tobias Ritschel. 2019. Escaping Plato’s Cave: 3D Shape From Adversarial Rendering. In Proc. of ICCV. Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In Proc. of ICLR. Lukas Héllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias NieSner. 2023. Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models. arXiv:2303.11989 [es.CV] Ajay Jain, Matthew Tancik, and Pieter Abbeel. 2021. Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis. In Proc. of ICCV. 5885-5894. Wonbong Jang and Lourdes Agapito. 2021. CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. In Proc. of ICCV. 12949-12958. Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and Jitendra Malik. 2018. Learning Category-Specific Mesh Reconstruction from Image Collections. In Proc. of ECCV. Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J. Mitra. 2023. HOLODIFFUSION: Training a 3D Diffusion Model Using 2D Images. In Proc. of CVPR. 18423-18433. KN. Kutulakos and $.M. Seitz. 1999. A theory of shape by space carving. In Proc. of ICCV. 307-314 vol.1. A. Laurentini. 1994. The Visual Hull Concept for Silhouette-Based Image Understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 16,(1994), 150-162. Min Seok Lee, WooSeok Shin, and Sung Won Han. 2022. TRACER: Extreme Attention Guided Salient Object Tracing Network. In Proc. of AAAI Conference on Artificial --- --360° Reconstruction From a Single Image Using Space Carved Outpainting Intelligence, Vol. 36. 12993-12994. Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. 2023. Vision Transformer for NeRF-Based View Synthesis From a Single Input Image. In Proc. of WACV. 806-815. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot One Image to 3D Object. arXiv:2303.11328 [cs.CV] Angeline Loh. 2006. The recovery of 3-D structure using visual texture patterns. Ph.D. Dissertation. Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. 2023. RealFusion: 360deg Reconstruction of Any Object From a Single Image. In Proc. of CVPR. 8446-8455. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In Proc. of ECCV. Anish Mittal, Rajiv Soundararajan, and Alan C. Bovik. 2013. Making a “Completely Blind” Image Quality Analyzer. IEEE Signal Processing Letters 20, 3 (2013), 209-212. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. 2022. Point-E: A System for Generating 3D Point Clouds from Complex Prompts. arXiv:2212.08751 [cs.CV] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and Andreas Geiger. 2020. Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision. In Proc. of CVPR. Dario Pavllo, David Joseph Tan, Marie-Julie Rakotosaona, and Federico Tombari. 2023. Shape, Pose, and Appearance from a Single Image via Bootstrapped Radiance Field Inversion. In Proc. of CVPR. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Text-to-3D using 2D Diffusion. In Proc. of ICLR. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proc. of ICML, Vol. 139. 8748-8763. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, and Varun Jampani. 2023. DreamBooth3D: Subject-Driven Text-to-3D Generation. arXiv:2303.13508 [cs.CV] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall, Pratul P. Srinivasan, and Matthias Niefner. 2022. Dense Depth Priors for Neural Radiance Fields from Sparse Input Views. In Proc. of CVPR. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In Proc. of. CVPR. 10684-10695. Radu Alexandru Rosu and Sven Behnke. 2023. PermutoSDF: Fast Multi-View Reconstruction With Implicit Surfaces Using Permutohedral Lattices. In Proc. of CVPR. 8466-8475. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. In Proc. of CVPR. 22500-22510. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. In Proc. of ICCV. Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Luéié, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas Funkhouser, and Andrea Tagliasacchi. 2022. Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. In Proc. of CVPR. 6229-6238. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. 2022. LAION-5B: An open large-scale dataset for training next generation image-text models. In Proc. of NeurIPS. Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 2020. 3D Photography using Context-aware Layered Depth Inpainting. In Proc. of CVPR. J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 2023. 3D Neural Field Generation Using Triplane Diffusion. In Proc. of CVPR. 20875-20886. Vincent Sitzmann, Michael Zollhéfer, and Gordon Wetzstein. 2019. Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations. In Proc. of NeurlPS. Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. 2023. Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior. arXiv:2303.14184 [cs.CV] Kalyan Alwala Vasudev, Abhinav Gupta, and Shubham Tulsiani. 2022. Pre-train, Self-train, Distill: A simple recipe for Supersizing 3D Reconstruction. In Proc. of CVPR. Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. 2022. Ref-NeRF: Structured View-Dependent Appearance for SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia Neural Radiance Fields. In Proc. of CVPR. Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C. K. Chan, and Chen Change Loy. 2023a. Exploiting Diffusion Prior for Real-World Image Super-Resolution. arXiv:2305.07015 [es.CV] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. 2018. Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. In Proc. of ECCV. Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, and Baining Guo. 2023b. RODIN: ‘A Generative Model for Sculpting 3D Digital Avatars Using Diffusion. In Proc. of CVPR. 4563-4573. Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. 2022. NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction. arXiv:2212.05231 [cs.CV] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing 13, 4 (2004), 600-612. Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. 2023. Novel View Synthesis with Diffusion Models. In Proc. of ICLR. Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. 2023a. Multiview Compressive Coding for 3D Reconstruction. In Proc. of CVPR. 9065-9075. Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. 2023b. MagicPony: Learning Articulated 3D Animals in the Wild. Proc. of CVPR. Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, and Shengping Zhang. 2019. Pix2Vox: Context-aware 3D Reconstruction from Single and Multi-view Images. In Proc. of ICV. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. 2023. NeuralLift-360: Lifting an In-the-Wild 2D Photo to a 3D Object With 360deg Views. In Proc. of CVPR. 4479-4489. Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural implicit surfaces. In Proc. of NeurlPS. Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. In Proc. of NeurIPS. Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. 2021. Shelf-Supervised Mesh Prediction in the Wild. In Proc. of CVPR. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. 2021. pixelNeRF: Neural Radiance Fields from One or Few Images. In Proc. of CVPR. Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sattler, and Andreas Geiger. 2022. MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction. In Proc. of NeurIPS. Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. 2023. Text2NeRF: TextDriven 3D Scene Generation with Neural Radiance Fields. arXiv:2305.11588 [cs.CV] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. NeRF++: Analyzing and Improving Neural Radiance Fields. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In Proc. of CVPR. Ruo Zhang, Ping-Sing Tsai, J.E. Cryer, and M. Shah. 1999. Shape-from-shading: a survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 21,(1999), 690-706. Zhizhuo Zhou and Shubham Tulsiani. 2023. SparseFusion: Distilling View-Conditioned Diffusion for 3D Reconstruction. In Proc. of CVPR. 12588-12597. --- --SA Conference Papers ’23, December 12-15, 2023, Sydney, NSW, Australia Ryu et al. P-e MCC NL RF Ours SS3D Figure 8: Qualitative comparison. We reconstruct the 360° shape and appearance of the single RGB image given on top with various models and compare them with our result. NL, RF, MCC, P-e, and SS3D stand for NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], Point-E [Nichol et al. 2022], and SS3D [Vasudev et al. 2022], respectively. Since SS3D [Vasudev et al. 2022] does not reconstruct the object’s appearance we only show its shape output. For better visualization, we use marching cubes for MCC [Wu et al. 2023a] to extract the surface with the same occupancy threshold that is used to sample the point cloud. For Point-E [Nichol et al. 2022], we use the point-cloud-to-mesh conversion provided by the authors. Input: rendered from the data in the Objaverse dataset [Deitke et al. 2022] [OHorton, CC BY]. --- --360° Reconstruction From a Single Image Using Space Carved Outpainting SA Conference Papers '23, December 12-15, 2023, Sydney, NSW, Australia Input : bidet? . fi 32% 2 ViEGLE 2 iit Figure 9: Another qualitative comparison. We reconstruct the 360° shape and appearance of the single RGB image given on top with various models and compare them with our result. NL, RF, MCC, P-e, and SS3D stand for NeuralLift [Xu et al. 2023], RealFusion [Melas-Kyriazi et al. 2023], MCC [Wu et al. 2023a], Point-E [Nichol et al. 2022], and SS3D [Vasudev et al. 2022], respectively. Since SS3D [Vasudev et al. 2022] does not reconstruct the object’s appearance we only show its shape output. For better visualization, we use marching cubes for MCC [Wu et al. 2023a] to extract the surface with the same occupancy threshold that is used to sample the point cloud. For Point-E [Nichol et al. 2022], we use the point-cloud-to-mesh conversion provided by the authors. Input: rendered from the data in the Objaverse dataset [Deitke et al. 2022] [Oshirava, CC BY].