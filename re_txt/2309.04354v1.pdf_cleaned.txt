--- --arXiv:2309.04354v1 [cs.CV] 8 SepMobile V-MoEs: Scaling Down Vision Transformers via Sparse Mixture-of-Experts Erik Daxberger* Floris Weers Bowen Zhang Tom Gunter Ruoming Pang Marcin Eichner Michael Emmersberger Yinfei Yang Alexander Toshev Xianzhi Du Apple Abstract 75Sparse Mixture-of-Experts models (MoEs) have recently FAgained popularity due to their ability to decouple model £size from inference efficiency by only activating a small sub-set of the model parameters for any given input token. As ge ] such, sparse MoEs have enabled unprecedented scalability, 3resulting in tremendous successes across domains such as < natural language processing and computer vision. In this 2 aa work, we instead explore the use of sparse MoEs to scale- 245down Vision Transformers (ViTs) to make them more attrac- a a0 | @ Mobile MoE (ours) tive for resource-constrained vision applications. To this iy le. @ Dense ViT end, we propose a simplified and mobile-friendly MoE de- 33-4 ox6t , 1 + sign where entire images rather than individual patches are 50M 100M cone 1B routed to the experts. We also propose a stable MoE training procedure that uses super-class information to guide the router. We empirically show that our sparse Mobile Vision MoEs (V-MoEs) can achieve a better trade-off between performance and efficiency than the corresponding dense ViTs. For example, for the ViT-Tiny model, our Mobile V-MoE outperforms its dense counterpart by 3.39% on ImageNet1k. For an even smaller ViT variant with only 54M FLOPs inference cost, our MoE achieves an improvement of 4.66%. 1. Introduction The trade-off between performance and efficiency of neural networks (NNs) remains a challenge, especially in settings where computational resources are limited. Recently, sparsely-gated Mixture-of-Experts models (sparse MoEs) have gained popularity as they provide a promising solution to this problem by enabling the decoupling of model size from inference efficiency [3]. MoEs are NNs that are partitioned into “experts”, which are trained jointly with a router to specialize on subsets of the data. In MoEs, each input is processed by only a small subset of model parameters (aka conditional computation). In contrast, traditional dense models activate all parameters for each input. “Correspondence to: Erik Daxberger, edaxberger@apple.com. Figure 1. Accuracy vs. FLOPs for ViTs of different sizes. Labels (e.g. 12x 192, which is ViT-Tiny) refer to the number of ViT layers (e.g. 12) and the hidden embedding dimension (e.g. 192). The sparse MoEs outperform their corresponding dense baselines across different model scales. Fig. 3a lists all numerical results. Sparse MoEs were popularized in deep learning by [16], which introduced sparse MoE-layers as drop-in replacements for standard NN layers. Most recent MoEs are based on the Transformer [19], which processes individual input tokens; in accordance, recent MoEs also route individual input tokens to experts, i.e., image patches in the case of Vision Transformers (ViTs) [2, 13] (see Fig. 2b). Conditional computation as implemented by sparse MoEs has enabled the training of Transformers of unprecedented size [4]. As a result, MoEs have achieved impressive successes across various domains including language [4, 10], vision [13], speech [20] and multi-modal learning [12], and currently hold state-of-the-art results on many benchmarks [21]. The ability to increase model capacity while keeping inference cost low is also appealing for resource-constrained vision problems. While Transformers are getting increasingly established as the de-facto standard architecture for large-scale visual modeling [2, 13], virtually all mobile --- --nse Layer Dense Layer eco ba(b) Regular V-MoE (a) Dense ViT MoE ViT Layer Dense ViT Layer (c) Mobile V-MoE (d) Layer types Figure 2. Model architectures. (a) The dense ViT baseline model uses dense ViT layers throughout. (b) Regular sparse V-MoE with layer-wise per-patch routers. (c) Our proposed sparse Mobile V-MoE design with a single per-image router. In both (b) and (c), dense ViT layers are followed by MoE-ViT layers (here, k = 1 out of E = 3 experts are activated per input). (d) In contrast to dense ViT layers [19], MoE-ViT layers have a separate MLP per expert (preceded by a router) while all other parts of the layer are shared across all experts [13]. friendly models still leverage convolutions due to their efficiency [1,5,6, 11, 15,18]. As such, conditional computation could potentially enable attention-based models to reduce the gap to convolutional models in the small-scale regime. However, Transformer-based MoEs have not yet been explored for resource-constrained settings; this might be due to two main weaknesses of recently-popularized MoEs [16]. Firstly, while per-token routing increases the flexibility to learn an optimal computation path through the model, it makes inference inefficient, as many (or even all) experts need to be loaded for a single input image. Secondly, recent MoEs train the routers jointly with the rest or the model in an end-to-end fashion. To avoid collapse to just a few experts while ignoring all others, one needs to use load balancing mechanisms [3] such as dedicated auxiliary losses [16]. However, the resulting complex optimization objectives often lead to training instabilities / divergence [4, 10, 12,21]. In this work, we investigate the potential of sparse MoEs to scale-down ViTs for resource-constrained vision applications via an MoE design and training procedure that addresses the aforementioned issues. Our contributions are: 1. We propose a simplified, mobile-friendly sparse MoE design in which a single router assigns entire images (rather than image patches) to the experts (see Fig. 2c). 2. We develop a simple yet robust training procedure in which expert imbalance is avoided by leveraging semantic super-classes to guide the router training. 3. We empirically show that our proposed sparse MoE approach allows us to scale-down ViT models by improving their performance vs. efficiency trade-off. 2. Scaling down ViTs via sparse MoEs 2.1. Conditional computation with sparse MoEs An MoE implements conditional computation by activating different subsets of a NN (so-called experts) for different inputs. We consider an MoE layer with EF experts as (yy MoE(x) = J 9(%).ei where x € R” is the input to the layer, e; : R? > R? is the function computed by expert 7, and g : R? > R® is the routing function which computes an input-dependent weight for each expert [16]. In a ViT-based MoE, each expert e; is parameterized by a separate multi-layer perceptron --- --(MLP) within the ViT layer, while the other parts are shared across experts (see Fig. 2d). We use the routing function g(x) = TOP; (softmax(Wx)), (2) where the operation TOP; (x) sets all elements of x to zero except those with the k largest values [13]. In a sparse MoE, we have k < E, s.t. we only need to load and compute the k experts with the largest routing weights. This allows us to scale-up the overall model capacity (determined by F) without increasing the inference cost (determined by k). 2.2. Efficient and robust MoEs for small-scale ViTs Per-image routing. Recent large-scale sparse MoEs use per-patch routing (i.e. the inputs x are individual image patches). This generally requires a larger number of experts to be activated for each image. For example, [13] show that in their MoE with per-patch routing, “most images use —on aggregate by pooling over all their patches— most of the experts” [13, Appendix E.3]. Thus, per-patch routing can increase the computational and memory overhead of the routing mechanism and reduce the overall model efficiency. We instead propose to use per-image routing (i.e., the inputs x are entire images) to reduce the number of activated experts per image, as also done in early works on MoEs [7,9]. Super-class-based routing. Previous works on sparse MoEs jointly train the router end-to-end together with the experts and the dense ViT backbone, to allow the model to learn the optimal assignment from inputs to experts based on the data [13]. While learning the optimal routing mechanism from scratch can result in improved performance, it often leads to training instabilities and expert collapse, where most inputs are routed to only a small subset of the experts, while all other experts get neglected during training [3]. Thus, an additional auxiliary loss is typically required to ensure load-balancing between the different experts, which can increase the complexity of the training process [3]. In contrast, we propose to group the classes of the dataset into super-classes and explictly train the router to make each expert specialize on one super-class. To this end, we add an additional cross-entropy loss £, between the router output g(x) in Eq. (2) and the ground truth super-class labels to the regular classification loss Lo to obtain the overall weighted loss L = Lo + AL, (we use \ = 0.3 in our experiments, which we found to work well). Such a super-class division is often readily provided with the dataset (e.g. for CIFAR-10/100 or MS-COCO). If a dataset does not come with a super-class division, we can easily obtain one as follows: 1) we first train a dense baseline model on the dataset; 2) we then compute the model’s confusion matrix over a held-out validation set; 3) we finally construct a confusion graph from the confusion matrix and apply a graph clustering algorithm to obtain the super-class division [8]. This approach encourages the super-classes to contain semantically ID Classes Super-class 0 boxer, pug, Rottweiler dogs 1 orangutan, weasel, panda _—_ other mammals 2 toucan, flamingo, ostrich birds 3 eel, scorpion, hammerhead other animals 4 minivan, ambulance, taxi land vehicles 5 submarine, canoe, pirate sea vehicles 6 guacamole, hotdog, banana foodbackpack, pyjama, kimono clothes monitor, iPod, photocopier tech devices xylophone, harp, trumpet instruments Table 1. Super-class division for E = 10. For each super-class, we list three randomly chosen class names (which turn out to be semantically related) together with a possible super-class name. similar images that the model often confuses. Intuitively, by allowing the different MoE experts to specialize on the different semantic data clusters, performance on the highlyconfused classes should be improved. We use this approach in our experiments on ImageNet-1k, computing the confusion matrix via a dense ViT-S/16 model. The resulting super-class division for E = 10 experts is shown in Tab. 1; the super-classes contain semantically related classes. 3. Experiments We now present empirical results on the standard ImageNet-1k classification benchmark [14]. We train all models from scratch on the ImageNet-1k training set of 1.28M images, and then evaluate their top-1 accuracy on the held-out validation set of 50K images. In Sec. 3.1, we first evaluate our proposed sparse Mobile V-MoE across a range of model scales and show that they achieve better performance vs. efficiency trade-offs than the respective dense ViT baselines. In Sec. 3.2, we then conduct several ablation studies to get a better understanding of the properties of our proposed sparse MoE model design and training procedure. 3.1. Accuracy vs. efficiency across ViT scales We consider ViT models (both MoEs and corresponding dense baselines) of different sizes by scaling the total number of layers (we use 12, 9 or 6) and the hidden embedding size (we use 384, 192, 96 or 64). The number of multi-head self-attention heads is (6, 3, 3, 2) for the different hidden embedding sizes. The embedding size of the MLP is 4x the hidden embedding size, as is common practice. We use E = 10 experts in total for the MoE, out of which k =is activated per input image. Our MoEs comprise of L =MoE-ViT layers preceded by (10, 7 or 4) dense ViT layers (see Fig. 2c). We use a patch size of 32 x 32 for all models. This is because the the patch size effectively controls --- --Model FLOPs Top-1 Accuracy E- Router MoE A k FLOPs Dense MoE A Dense MoE A 5 86.43 72.33 +1.64 1 2534M =: 70.79 73.44. -+2.12x 384 207M 71.88 74.23 42.35 7 87.43 73.13 +2.44 2 27609M_—s 71.70) 74.42) +2.10 87.12 73.52 +2.83 3 3005M_ = 73.58 «= 74.44 = +0.9x384 1752M = 69.94. 72.47 +2.6x384 0M 6321 66.91 43.70 15 84.16 73.10 +2.41 5 3476M = 74.87 74.32 ~~ -0.12192! 618M 59.5 62.90 +339 20 84.08 73.36 +2.67 10 4653M 75.10 74.37 -0.9x 192 478M = 56.50 59.52 +3.02 (b) Total number of experts EL. (d) Number of experts k per image. 6x192 338M) = 51.18 55.69 44.51 y 12x96 176M 53.79 55.39 -+1.60 L_ Router Mok A Routing Input = Ace. A 9x96 140M 51.27 552.99 41.72 1 90.17 72.14 +1.45 Dense N/A 71.6x96 103M = 46.54 50.28) -+3.74 2 87.12 73.52 +2.83 Super-class image 74.42 +2.12x64 88M 42.90 46.07 +3.17 4 82.12 71.67 +0.98 Rand. class image 69.22 -2.9x64 TIM 40.46 43.95 +3.49 6 77.70 70.07 -0.62 End-to-end image 73.57 +1.6x64 54M =. 36.64 41.30 +4.66 8 72.09 64.47 = -6.22 End-to-end token 74.85 +3.(a) Accuracy vs. efficiency across ViT scales. Figure 3. Empirical results. (e.g. 12x 192) refer to the number of layers (12) and the embedding size (192). (b-e) Ablation studies using DeiT-Ti/16 [17], with k = 1, E = 10, L = 2 by default. Best performance vs. efficiency trade-off is achieved with (b) E = 10 experts total, (c) L = 2 MoE layers (out of 12 layers total), (d) k = 1 or k = 2 experts activated per image, (e) our semantic super-class routing; the settings used in (a) are bolded. (c) Number of MoE layers L. (a) Our Mobile V-MoEs outperform the respective dense ViTs across model scales. the trade-off between FLOPs and number of model parameters: as we aim to optimize for FLOPs, a larger patch size (resulting in a fewer number of patches) is beneficial. We also tried using a smaller patch size of 16 x 16, where the result trends were basically the same (but where the number of FLOPs was higher relative to the model capacity and thus accuracy). For the ViTs with hidden sizes 384 and 192, we use the DeiT training recipe [17], while for hidden sizes 96 and 64, we use the standard ViT training recipe [2] to avoid underfitting. Figs. | and 3a compare top-1 validation accuracy vs. FLOPs. Our Mobile V-MoEs outperform the corresponding dense ViT baselines across all model sizes. 3.2. Ablation studies We train DeiT-Tiny [17] (12 layers total, 192 embedding size, 16 x 16 patch size) with k = 1 out of E = 10 experts per input, and with L = 2 MoE layers (unless noted otherwise); the dense ViT baseline achieves 70.79% accuracy. Total number of experts E. We consider different widths of the MoE, i.e., different numbers of experts E (and thus super-classes), ranging between EF = 5 and E = 20. We report both the accuracy of the entire MoE model (i.e., on the 1,000-way classification task), as well as the accuracy of the router (i.e., on the E-way super-classification task). Fig. 3b shows that overall performance improves until E = 10, from which point onwards it stagnates. The router accuracy also drops beyond EF = 10 due to the increased difficulty of the E-way super-classification problem. Number of MoE layers L. We consider different depths of the MoE, i.e., different numbers of MoE layers L, rang ' This corresponds to the ViT-Tiny model [17] with patch size 32 x 32. (e) Routing strategies. Model names ing between L = 1 and L = 8 (out of 12 ViT layers in total). We again report both the full MoE and router accuracies. Fig. 3c shows that overall performance peaks at L = 2, and rapidly decreases for larger L. This is due to the router accuracy, which declines with increasing L as the router gets less information (from the 12 — L ViT layers). Number of experts & per image. We vary the number of experts k activated per image. We compare against dense baselines that use an MLP with hidden dimension scaled by & to match the MoE’s inference FLOPs. Fig. 3d shows that k = 1 and k = 2 perform best (relative to the dense baseline), with decreasing performance delta for larger k. Routing strategies. We compare our proposed semantic super-class per-image routing vs. end-to-end-learned routing (both per-image and per-token) and a baseline with random super-classes (for k=2). Fig. 3e shows that our method (Fig. 2c) is better, except for learned per-token routing (as in the regular V-MoE [13], Fig. 2b), which however needs to activate many more experts and thus model parameters for each input image (up to 11.05M, vs. 6.31M for ours). 4. Conclusions and future work We showed that sparse MoEs can improve the performance vs. efficiency trade-off compared to dense ViTs, in an attempt to make ViTs more amenable to resourceconstrained applications. In the future, we aim to apply our MoE design to models that are more mobile-friendly than ViTs, e.g., light-weight CNNs such as MobileNets [5,6, 15] or ViT-CNN hybrids [1, 11, 18]. We also aim to consider other vision tasks, e.g., object detection. Finally, we aim to get actual on-device latency measurements for all models. --- --References (Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobileformer: Bridging mobilenet and transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5270-5279, 2022. 2,Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1,William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022. 1, 2,William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. J. Mach. Learn. Res, 23:1-40, 2021. 1,Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1314-1324, 2019. 2,Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv: 1704.04861, 2017. 2,Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79-87, 1991.Ruochun Jin, Yong Dou, Yueqing Wang, and Xin Niu. Confusion graph: Detecting confusion communities in large scale image classification. In JJCAI, pages 1980-1986, 2017.Michael I Jordan and Robert A Jacobs. tures of experts and the em algorithm. Neural computation, 6(2):181-214, 1994.Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006. 16668, 2020. 1,Sachin Mehta and Mohammad Rastegari. Mobilevit: lightweight, general-purpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178, 2021. 2,Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. arXiv preprint arXiv:2206.02770, 2022. 1,Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:8583-8595, 2021. 1, 2, 3,Hierarchical mix-Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211—252, 2015.Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510-4520, 2018. 2,Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. arXiv preprint arXiv:1701.06538, 2017. 1,Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Training data-efficient image transformers & distillation through attention. In Jnternational conference on machine learning, pages 10347-10357. PMLR, 2021.Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. An improved one millisecond mobile backbone. arXiv preprint arXiv:2206.04040, 2022. 2,Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 1,Zhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts. arXiv preprint arXiv:2105.03036, 2021.Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906, 2022. 1,