--- --arXiv:2307.11795v1 [eess.AS] 21 JulPrompting Large Language Models with Speech Recognition Abilities Yassir Fathullah!}/ Chunyang Wu! Egor Lakomkin! —Junteng Jia! Yuan Shangguan' KeLi! JinxiGuo! Wenhan Xiong! Jay Mahadeokar' Ozlem Kalinli' Christian Fuegen! Mike Seltzer! Meta AI!, University of Cambridge? yf286@cam.ac.uk, chunyang@meta.com Abstract Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to generate fewer embeddings. The results from these studies show that multilingual ASR is possible even when the LLM is frozen or when strides of almost 1 second are used in the audio encoder opening up the possibility for LLMs to operate on long-form audio. 1 Introduction Large language models (LLMs) have been proven to be highly flexible models able to solve a wide range of tasks. By being trained to predict the next token on a vast amount of unsupervised text data, these systems learn to encode world knowledge in the network parameters, useful in many downstream open-domain generative tasks such as abstractive summarization, question answering, knowledge retrieval, text generation and machine translation. However, interacting with LLMs purely through text can in many cases be limiting. There exists many other structured modalities which encode information that is difficult to capture through text. For example, audio can encode a wide range of emotions in a person’s speech and images can represent the geometry and location of objects that might be much harder to describe through text. Recently published work have extended LLMs with the ability to ingest other modalities. The multi-modal PaLM-E combined a large pretrained visual transformer with the PaLM LLM [7] and were able to achieve state-of-the-art performance on their robotics tasks. Similarly, the work of [24] utilize a pretrained visual model and the large language model Vicuna, a derivative of LLaMA [5] in creating an aligned model with the ability to reason with both visual and textual inputs. Furthermore [12| propose LTU, an extension of LLaMA with an aligned audio encoder trained on an audio question answering corpus, enabling it to reason with and understand sounds. However, LTU has limited speech understanding and recognition abilities. “Work done during internship at Meta AI. --- --Due to the immense number of parameters in these large language model oriented systems, it can often be computationally impractical and expensive to adapt the whole system to new tasks. The work of trained a single projection layer which adapts the outputs of the visual encoder to be aligned to the language model, representing a highly parameter efficient approach. However, this severely limits the adaptability and performance of the system on new tasks. On the contrary, the multi-modal PaLM-E investigated training the whole visual encoder and language model jointly. However, adapting the whole language model is extremely expensive and impractical. Alternative approaches include: inserting adapter layers (20) |13) or prefix embeddings which are trained on the new task. While these approaches are effective parameter efficient approaches they increase the inference costs. Low-rank Adaptation solves these issues by using low-rank matrices to modify some parameters of the system and has been shown to be highly promising. The approach is memory efficient during training and does not impact inference runtime. Contributions: In this paper we investigate equipping a large language model with speech recognition abilities by conditioning the LLM on a variable length sequence of audio embeddings. We show that a decoder-only large language model conditioned on the audio sequence is able to perform multilingual speech recognition, outperforming monolingual supervised trained baselines. Furthermore, this paper explores a range of factors that can enable better recognition performance such as the audio encoder model size and frame rate, low-rank adaptation of LLM parameters, text token masking and the type of large language model. Finally, by analysing the outputs of the audio encoder, we show that the audio embeddings are similar and aligned to the text tokens. 2 Methodology Our approach will be centered around the use of a large language model (LLM) to model sequences of embeddings irrespective of the modality of the embedding. Inspired by the work of which utilize a visual encoder to generate a fixed-length sequence of visual embeddings in the same space as text embeddings, we utilize a pretrained audio encoder to generate a variable-length sequence of audial embeddings. By conditioning on the audial embeddings, the large language model can be allowed to perform speech recognition and other speech based tasks. Therefore, the only marginal difference between a traditional LLM and the proposal is the mixing of embeddings of different modalities. 2.1 Audial Embeddings We use a conformer based audio encoder to produce a sequence of embeddings that will be used to condition the LLM similar to a prompt, however, in embeddings space. To ensure the audio encoder can extract useful embeddings it will initially be trained on a simple connectionist temporal classification (CTC) loss. Since the sequence output of this encoder can be very long, one can further reduce the length by stacking consecutive embeddings, resulting in larger but fewer embeddings, see Figure[I]for the encoder structure. In this work we investigate different levels of stacking, ranging up to embeddings that encode 960ms of audio which on average contains several tokens worth of information in a single vector. The stacked embeddings are then projected to the hidden dimension of the large language model to ensure they can be prepended to the text embeddings. 2.2 Large Language Model Most experiments will utilize the smallest LLaMA-7B model (23). The causal self-attention parameters of this system will be adapted using a parameter efficient Low-rank Adaptation (LoRA) (14), keeping all other parameters frozen. In an ablation we will investigate whether any LLM parameters need to be tuned at all to perform ASR. Furthermore, we investigate whether the choice of LLM is important by replacing LLaMA with various BLOOM models (21). The ASR-LLM problem can possibly be reinterpreted as a copying/translation task where the LLM needs to regurgitate the information in the audio sequence. If the audio encoder provides a sequence of embeddings aligned with the text embeddings the problem collapses to a repetition task which should not require the full capacity of an LLM. This interpretation will be investigated in Section[4] See Figure [2] for an overview of the system. --- --:10ms { Filterbank Features q ' 80ms Conformer Encoder 240ms e) Figure 1: Audio encoder architecture. The initial conformer is trained on a CTC loss. Thereafter the outputs are stacked and projected to the dimension of the LLM to ensure compatibility. This figure showcases a stacking factor of 3 resulting in 240ms embeddings. pp pop Pry TI ttt id Audio Encoder Text Embedding Matrix Large Language Model Pld} iid 7 love playing the guitar and piano! <eos> Figure 2: Model architecture. The embedding sequence generated from the audio encoder is directly prepended to the text embeddings sequence. This is directly fed into the decoder-only LLM, tasked with predicting the next token. The LLM can be frozen, adapted with parameter efficient approaches such as LoRA or fully finetuned. This work will investigate the former two. 3 Experimental Evaluation 3.1 Dataset The Multilingual LibriSpeech (MLS) is a 50k hour ASR corpus derived from read audiobooks of LibriVox [19]. Consisting of 8 languages: English (en), German (de), Dutch (nl), French (fr), Spanish (es), Italian (it), Portuguese (pt) and Polish (pl) the dataset is predominately in English with 44.5k hours. Some low-resource languages such as Portugese and Polish only have 161 and 103 hours respectively. To account for the imbalance in the dataset we follow the strategy outlined in (0 by oversampling from the lower resource languages. Each utterance is up to 20 seconds long. None of our reported word error rates include the use of the n-gram models provided by MLS. 3.2 Model Setup & Training Details Audio Encoder The audio encoder operates on 80-d filterbank features with 10ms frame rate. It consists of convolutional feature extractor with a coarse effective stride of 8 followed by linear layer to project the output to 512 dimensions and 18 layers of non-macaron Conformer blocks. The blocks have a hidden dimension of 512, a feed-forward net dimension of 2048, a convolutional kernel size --- --of 11 and 8 attention heads. A final linear layer is used to pretrain the audio encoder using a CTC loss with a SentencePiece vocabulary of size 1547. The final linear layer is discarded after pretraining. Note that the effectiveness of this relatively small audio encoder of 72 million parameters could be significantly improved by scaling the size up, reducing the level of striding and utilizing a range of unsupervised and semi-supervised learning approaches [9] |T} {22} 2} [3} {6 [8]. However, we restrict ourselves to a simpler setup and only use supervised learning to train our models. We focus our attention on showing that an LLM can be conditioned to perform speech recognition and investigate what factors improve its ability at performing this task. Audial Embeddings The output of the encoder is a sequence of 512-d vectors with a frame rate of 80ms. To reduce sequence length and memory consumption, every n consecutive frames are stacked to form 512n-dimensional frames which are projected to 4096-d embeddings to match the LLaMA-7B dimension, with a resulting frame rate of 80nms. We investigate producing embeddings up to a frame rate of 960ms, corresponding to stacking 12 consecutive frames. These embeddings are prepended to the text embeddings (as specified in Figure[2) and fed into the LLM, which is tasked with predicting the next text based token. Large Language Model Adaptation We use the Low-rank adaptation (LoRA) approach to adapt the key, query, value and output layers of the self-attention mechanism leaving feed-forward nets, embedding and final linear output layer unchanged. Unless specified otherwise, default LoRA hyperparameters are set to a rank of R = 8 and a = 16. We investigate the impact of R in an ablation study. Training The audio encoders were initially trained using the Adam optimizer with 31 = 0.9, 82 = 0.98 (i5]. The learning rate was linearly warmed up over 20k training steps up to a peak value of le-3 followed by a exponential decaying schedule. This was done on 16 NVIDIA A100 40GBs with 4 gradient accumulations using a per-gpu batch size of up to 500 seconds of audio. The checkpoint with the best validation loss was picked. The joint system with audio encoder and LLM was thereafter trained with a similar schedule of 5k warmup steps up to a peak learning rate of 5e-4 decaying down to Se-6 over 250k steps. Training was often stopped early withing 100k steps. This was performed on 64 NVIDIA A100 40GBs with 4 gradient accumulations steps using batch sizes of up to 80 seconds. The checkpoint with the lowest validation loss was picked for evaluation. Evaluation All reported word error rates (WER) exclude the use of external language models provided by (79). Decoding is done using greedy search with a maximum output token length of 200. 3.3 Baselines Our approach relies solely on supervised learning and so the most relevant baselines are the monolingual models provided by MLS [19]. Since we follow the same data sampling strategy and Table 1: Language specific and average WER performance on the MLS dataset. The first block monolingual models refers to training a separate model for each language. The second block multilingual model refers to training a single model on all languages concurrently. The last block refers to pretraining a model on all languages, followed by finetuning a pretrained checkpoint for each language separately. trainable . params de nl fr es it pt pl Avg supervised learning: monolingual models 36L Transformer CTC 0.3B 6.8 7.1 3.1 66 67 11.8 205 21.7 1.36L Transformer CTC [19] w/ LM 0.3B 59 65 12.0 56 61 105 195 20.4 0.supervised learning: multilingual model Decoder-only LLaMA-7B (960ms) 0.10B 76 74 19 7.0 61 114 186 19.1 1.Decoder-only LLaMA-7B (480ms) 0.09B 73°74 19 67 61 ILS 183 17.0 0.Decoder-only LLaMA-7B (240ms) 0.09B 70 7.2 14 64 60 %I15 175 16.7 0.Decoder-only LLaMA-7B (160ms) 0.08B 6.9 7.0 13 62 54 116 174 148 0.Decoder-only LLaMA-7B (80ms) 0.08B 62 67 113 5.5 5.2 10.8 16.2 15.9 | 9.self-supervised learning + monolingual finetuning w2v2 XLSR-53 w/ LM 0.3B - 70 10.8 7.6 63 104 14.7 17.2 0.--- --setup as in (9) we will also include the self-supervised XLSR-53 with monolingual finetuning as a baseline. There are many alternative and powerful audio encoders in literature that achieve highly competitive results on the MLS benchmark, while relevant these systems are often trained using self/semi-supervised approaches with significantly more compute and trainable parameters, representing orthogonal contributions to our aims. 3.4 Main Results Since we keep most parameters in the LLM frozen, and make use of a very small audio encoder, our approach has much fewer trainable parameters compared to baselines, see Table[I] As expected, the Decoder-only LLaMA with the highest frame rate (80ms) outperforms systems with lower frame rate, also outperforming the monolingual models by 18% and 10% on average word error rate. Reducing the frame rate degrades performance, however, even systems with large strides (480/960ms), reducing the original filterbank sequence by a factor of up to 96, are able to compete with the monolingual baselines. These high striding systems could also be one viable avenue for operating on long-form audio, by compressing the audio sequence length orders of magnitude. 3.5 Ablation Studies Larger Audio Encoders The level of audio encoder striding has a notable impact on the speech recognition ability of LLaMA. Therefore, we also investigate the number of layers in the audio encoder, scaling it from 72 up to 142 million parameters, see Table[2] The largest audio encoder with Table 2: Investigating the impact of number of layers of the audio encoder on the MLS dataset. trainable . | params de nl fr es it pt pl Avg 18L Conformer (240ms) 0.09B 70 #72 114 64 60 %I15 175 16.7 | 10.24L Conformer (240ms) 0.11B 66 66 108 59 54 I15 145 168 | 9.36L Conformer (240ms) 0.16B 61 63 110 55 49 I11 15.9 16.7 | 9.36 conformer layers and 240ms striding leads to an average WER of 9.7% matching the performance of the 18 layer audio encoder with 80ms striding. This shows the importance of the audio encoder in generating higher quality embeddings used in conditioning the LLM. Low-rank Adaptation All experiments have fixed the low-rank adaptation parameter to R =for adjusting the LLaMA self-attention parameters. We further investigate the impact of the LORA by adjusting R € [0,8, 16,32]; setting R = 0 is equivalent to completely freezing LLaMA. All experiments in Table [3] use 240ms striding. Each rank adds approximately | million trainable Table 3: Investigating the impact of rank R. Setting R = 0 is equivalent to freezing the LLM. trainable . params de nl fr es it pt pl Avg Decoder-only LLaMA-7B (240ms) R = 0 0.08B 75 7A 120 68 5.9 11.8 182 174 | 10.Decoder-only LLaMA-7B (240ms) R = 8 0.09B 70 #72 114 64 60 I15 175 16.7 | 10.Decoder-only LLaMA-7B (240ms) R = 16 0.10B 63 68 114 5.7 55 108 16.3 15.0 | 9.Decoder-only LLaMA-7B (240ms) R = 32 0.11B 60 65 Il 54 5.2 109 15.7 15.3 | 9.parameters. Interestingly, keeping LLaMA frozen and only training the audio encoder leads to reasonable results with an average WER of 10.9%. This would also maintain the original capabilities of the LLM; all other finetuning setups would negatively affect the ability of LLaMA in performing text based tasks (uj. Furthermore, increasing the rank of the trainable parameters significantly improves performance, where R = 32 is able to achieve an average WER of 9.5%, outperforming the best system in Table|1]which uses 80ms striding and R = 8. Based on these results, parameter tuning the whole LLM could lead to additional performance gains but is significantly more expensive to train. Masking Since the training task is based on causal next token prediction, but is conditioned on the audio sequence which contains the needed information, masking text tokens could be --- --useful in boosting performance [17]. The table below shows performance when a fraction F € (0.000, 0.125, 0.250, 0.375, 0.500] of the text tokens are randomly replaced with the <unk> token during training. The introduction of masked text tokens during training can lead to notable Table 4: Masking a fraction F' of text tokens during training. | trainable de nl fees it pt pl Avg params Decoder-only LLaMA-7B (240ms) F' = 0.000 0.09B 70 #72 114 64 60 %I15 175 16.7 | 10.Decoder-only LLaMA-7B (240ms) F’ = 0.125 0.09B 67 70 11.3 61 56 11.3 168 16.3 | 10.Decoder-only LLaMA-7B (240ms) F' = 0.250 0.09B 65 69 11.3 61 56 11.2 165 15.1 | 9.Decoder-only LLaMA-7B (240ms) F' = 0.375 0.09B 65 70 114 61 54 111.3 174 162 | 10.Decoder-only LLaMA-7B (240ms) F' = 0.500 0.09B 64 70 115 62 5.1. It 17.1 168 | 10.improvements in performance, with F' = 0.250 leading to a 5.7% average WER improvement compared to the baseline F' = 0.000. However, beyond this point, increasing the level of masking has a negative impact on the low resource languages Portuguese and Polish. It is possible to set different levels of masking depending on the amount of language specific data but we leave this investigation to future work. Large Language Model LLaMA was trained on predominantly English text with a small fraction covering other languages (23). BLOOM , on the other hand, was specifically designed to be multilingual and has support for an order of magnitude more languages. Therefore, we replace LLaMA-7B with a choice of {BLOOM-560M, BLOOM-1B7, BLOOM-7B1} to understand the impact of LLM and how performance changes with increasing LLM scale, see Table[5] Comparing Table 5: Replacing LLaMA-7B with various BLOOM language models. trainable . . params de nl fr es it pt pl | Avg Decoder-only LLaMA-7B (240ms) 0.09B 70 72 W4 64 60 11.5 175 16.7 | 10.Decoder-only BLOOM-560M (240ms) 0.07B 82 84 126 7.3 65 12.5 18.3 19.8 | 11.Decoder-only BLOOM-1B7 (240ms) 0.08B 75 83 12.2 67 58 12.2 166 19.0 | 11.Decoder-only BLOOM-7B1 (240ms) 0.08B 70 78 12.1 59 53 11.8 156 17.7 |LLaMA-7B and the similarly sized BLOOM-7B1 we observe no significant difference in average WER. Although BLOOM is multilingual it seems this ability is not as impactful once the system is trained on a multilingual speech dataset. However, there is a clear trend showing significantly better performance from scaling an LLM while keeping the conformer audio encoder fixed. 4 Analysing Audio Encoder Text Alignment As hypothesized in Section the speech recognition task can be interpreted as a regurgitation task—the language model is tasked with cleaning and repeating (in the same order) information that is present in the audio encoder output sequence. Since the audio encoder is trained to generate embeddings in the same semantic space as the text embeddings, this implies that the audio and text embeddings should be monotonically aligned for a properly trained system. We therefore, compute the cosine similarity between each possible pair of audio and text embedding for an English test set example. This is done for the LLaMA models in[I]to understand the impact of increased striding on the impact of alignment, see Figure[3| These alignment plots support the hypothesis that the encoder is attempting to align the audio embeddings to the text in a monotonic manner. As the striding is increase, the task of aligning audio to text becomes harder and harder. Furthermore, this begs the question whether or not the audio encoder can benefit from further supervision by training the output to be monotonically aligned to the text, instead of indirectly training it through next token prediction via the language model. --- --text tokens audio tokens (a) text tokons text tokons text tokens text tokens audio tokens, audio tokens audio tokens audio tokens (b) () (d) (e) Figure 3: The pairwise cosine similarity between every pair of audio and text embeddings for a given test example from the English set. The subfigures (a)-(e) represent the models in Table} [Twith stridings ranging from 80ms up to 960ms. 5 Conclusion Overall this work has shown a simple procedure for enabling multilingual speech recognition with a large language model. By prepending an audio embedding sequence, the large language model can be triggered to perform speech recognition in a decoder-only fashion. Furthermore, this work investigates a range of different factors that are key in enabling better recognition performance including analysing the audio encoder stride & size. The paper also investigates the importance of the LLM by comparing LLaMA against BLOOM, the importance of tuning the LLM with the use of low-rank adapters and finally how the LLM can perform better recognition by augmenting the input with masking. After joint training of the encoder and LLM it was shown that the audio embeddings are tending to be aligned with the text embeddings. Future work can make use of this observation by directly training the audio encoder to be aligned with the language model. --- --ReferencesArun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al. “XLS-R: Self-supervised cross-lingual speech representation learning at scale”. In: arXiv preprint arXiv:2111.09296 (2021). Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. “Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations”. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. 2020. Junwen Bai, Bo Li, Yu Zhang, Ankur Bapna, Nikhil Siddhartha, Khe Chai Sim, and Tara N. Sainath. “Joint Unsupervised and Supervised Training for Multilingual ASR”. In: 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. “Language Models are Few-Shot Learners”. In: Advances in Neural Information Processing Systems. 2020. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. “Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality”. In: (2023). Chung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. “Self-supervised learning with random-projection quantizer for speech recognition”. In: Proceedings of the 39th International Conference on Machine Learning. 2022. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. “Palm: Scaling language modeling with pathways”. In: arXiv preprint arXiv:2204.02311 (2022). Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. “w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training”. In: 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). 2021. Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. “Unsupervised Cross-lingual Representation Learning for Speech Recognition”. In: Interspeech. 2021. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. “Scaling vision transformers to 22 billion parameters”. In: arXiv preprint arXiv:2302.05442 (2023). Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. “Palm-e: An embodied multimodal language model”. In: arXiv preprint arXiv:2303.03378 (2023). Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass. “Listen, Think, and Understand”. In: arXiv preprint arXiv:2305.10790 (2023). Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. “Parameter-Efficient Transfer Learning for NLP”. In: Proceedings of the 36th International Conference on Machine Learning. Vol. 97. 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. “LoRA: Low-Rank Adaptation of Large Language Models”. In: International Conference on Learning Representations. 2022. Diederik P. Kingma and Jimmy Ba. “Adam: A Method for Stochastic Optimization”. In: International Conference on Learning Representations (ICLR). 2015. Taku Kudo and John Richardson. “SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing”. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2018. Ke Li, Jay Mahadeokar, Jinxi Guo, Yangyang Shi, Gil Keren, Ozlem Kalinli, Michael L. Seltzer, and Duc Le. “Improving fast-slow Encoder based Transducer with Streaming Deliberation”. In: International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2023. Xiang Lisa Li and Percy Liang. “Prefix-Tuning: Optimizing Continuous Prompts for Generation”. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. “MLS: A LargeScale Multilingual Dataset for Speech Research”. In: Interspeech. 2020. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. “Learning multiple visual domains with residual adapters”. In: Advances in Neural Information Processing Systems. Vol. 30. 2017. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili¢, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, Frangois Yvon, Matthias Gallé, et al. “Bloom: A 176b-parameter open-access multilingual language model”. In: arXiv preprint arXiv:2211.05100 (2022). --- --[22] [23] [24] Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. “wav2vec: Unsupervised Pretraining for Speech Recognition”. In: Interspeech. 2019. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Roziére, Naman Goyal, Eric Hambro, Faisal Azhar, et al. “Llama: Open and efficient foundation language models”. In: arXiv preprint arXiv:2302.13971 (2023). Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. “Minigpt-4: Enhancing visionlanguage understanding with advanced large language models”. In: arXiv preprint arXiv:2304.(2023).