--- --2305 .02783v4 [cs.SE] 23 May1V arX Automated Code generation for Information Technology Tasks in YAML through Large Language Models Saurabh Pujar*!, Luca Buratti*!, Xiaojie Guo*!, Nicolas Dupuis*!, Burn Lewis*!, Sahil Suneja*!, Atin Sood*!, Ganesh Nalawade*?, Matthew Jones”, Alessandro Morari!, and Ruchir Puri! 'IBM Research ?Red Hat Abstract—The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code generation models. In few-shot settings we asses the impact of training with Ansible, YAML data and compare with different baselines including Codex-Davinci002. We also show that after finetuning, our Ansible specific model (BLEU: 66.67) can outperform a much larger CodexDavinci-002 (BLEU: 50.4) model, which was evaluated in few shot settings. Index Terms—Generative Model, Ansible, Code Generation I. INTRODUCTION In the recent years, Large Language models (LLMs) have demonstrated considerable content-generation capabilities in multiple domains, including natural language, vision, video and audio processing [1]. More recently, LLMs have been applied to the Software Engineering field, with the objective of improving aspects such as programmer’s productivity [2] and software security [3]. In the space of general-purpose programming languages, a growing amount of research is exploiting the capabilities of large language models to perform code generation, clone detection, code repair and other tasks [4], [5]. This is fueling a new generation of coding assistants’ products, able to speed up developer work, leveraging the availability of open-source code bases to train the models. While the field of Al-assisted coding assistants is at its infancy, the potential impact on the field of Software Engineering cannot be underestimated. The application of these techniques to IT “Equal contribution. Correspondence: saurabh.pujar@ibm.com domain specific languages like YAML, however, has received less attention, despite their importance to a wide array of fields. In this work, we explore the application of LLMs to the important area of IT automation. IT automation replaces manual IT admin work with automated execution of domain specific scripts. This approach dramatically improves cloud infrastructure security, cost-efficiency, reliability and scalability. YAML files are often used to define and configure key aspects of IT infrastructure. Ansible is one of the most widely used applications for IT automation which uses YAMLbased configurations, and thousands of companies rely on this technology to manage their IT infrastructure. While easier to write than general purpose programming languages such as Java and C++, Ansible-YAML requires considerable expertise to be used proficiently. For many companies, speeding up Ansible adoption would mean faster digital transformation towards a safer, cost-efficient approach for IT infrastructure management. This paper investigates the use of LLMs to generate Ansible-YAML code, with the objective of building an AI assistant for Ansible-YAML users and improving their productivity. We propose the use of transformer-based models for the task of Ansible- YAML code generation given a natural language prompt. We start from training four versions of large domain-specific pre-trained decoder-based model, by learning from large amount of YAML and Ansible-YAML data in general. We then perform fine-tuning for the downstream Natural Language to Ansible- YAML generation task. This is the first work looking at LLMs for YAML in general and for Ansible-YAML in particular. The contributions of this work are the following: e We explore the implications of applying code generation to Ansible-YAML and provide a formal definition of the problem. e We build the YAML and an Anisible-YAML dataset for both pretraining and finetuning tasks in the code generation. e We theoretically re-formalize the Ansible- YAML generation problem into code completion with novel prompt, by utilizing the unique features of YAML data and --- --practically trained a series of transformer-based models, which show much superiority. « We propose two novel evaluation metrics specially designed for Ansible-YAML, compare our models against the latest LLMs, and highlight their limitations. II. RELATED WORK A. Pre-trained Language Models for Code Most recently, language models have fueled progress towards the longstanding challenge of source code synthesis [6], [7], which excel at downstream tasks such as code completion, code generation and code summarization. According to Xu et al. [8], pre-training methods for source code modeling fall into three categories: (i) The first category is based on left-to-right language models, namely, autoregressive decoder-based models. These models predict the probability of a token given the previous tokens. For example, CodeGPT [9], CodeParrot [10], Codex [6], AlphaCode [11] and CodeGen [12] all follow into this line, which are highly useful for code generation and completion tasks. (ii) The second category is based on masked language models, which can make use of the bi-directional information to learn whole sentence representations, such as CodeBERT [13] and CuBERT [14]. This line of pre-trained models perform well for the code classification and detection tasks. (iii) The third category of models is based on encoder-decoder models that incorporate pre-training objectives such as masked span prediction and denoising sequence reconstruction. CodeTS5 [15], PLBART [16], and PolyCoder [8] fall into the third category and perform well in sequence-to-sequence downstream tasks such as code commenting and code-to-code translation. Among these models, CodeGen has been trained on The Pile [17] and on data from Google BigQuery, hence it has been exposed to natural language, code and some YAML data. B. Code Generation Source code generation (or program synthesis) can be defined as the generation of a program or code snippet, starting from a natural language specification. Traditional methods use a probabilistic context free grammar (PCFG) to generate the abstract syntax tree (AST) of the source code [6], [18]. Yin et al. [18] proposed a neural model in combination with a transition system to generate abstract syntax trees. With the recent development of large scale language models, large scale Transformers have also been applied to this problem. Transformers typically treat code as text. Feng et al. [13] proposed the use of a masked language model with bi-modal text from the CodeSearchNet challenge [19]. Two works [6], [20] propose the use of a decoder language model trained on large amounts of source code and web data. Furthermore, Xu et al. [8] perform a systematic evaluation of these models, finding that the presence of natural language in the training corpus helps with general code-language modeling. Many transformer models have been developed for software engineering tasks which focus on specific programming languages like Python [14] and C [3]. Multi-lingual LLMs like PLBART [16], CODEGEN [12] and Codex [6] are trained on multiple programming languages but most of the data is comprised of commonly used languages like C, C++, Java, Python etc. Widely used domain specific languages like YAML have received far less attention. Tools such as Ansible, OpenShift and many others rely on YAML for managing their configuration files. State of the art models such as Codex [6] or CODEGEN [12], are primarily evaluated on general purpose programming languages. While Codex or CODEGEN could be used to generate YAML, including Ansible-YAML, due to their very large and heterogeneous training datasets, we did not find any work evaluating this capability. To the best of our knowledge, this is the first work addressing the problem of YAML code generation using a large language model. Ill. BACKGROUND The Red Hat Ansible Automation Platform is an opensource [21], [22] IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multinode orchestration. Ansible makes complex changes like zerodowntime rolling updates with load balancers simpler. A system running Ansible will have a control node and one or more managed nodes. The control node is where Ansible is executed, while the managed nodes are the devices being automated, for example, Linux and Windows server machines. An Ansible Playbook (or playbook) is a YAML file that describes a set of Ansible Tasks (or tasks) to be performed by Ansible on the managed node. The playbook defines the desired state of the managed nodes, and the tasks specify the steps to bring the nodes to that desired state. For example, a playbook might define a set of tasks to install and configure a particular application, or to set up a particular system configuration. Playbooks are organized into a series of plays, which are executed in order. Each play specifies a group of managed nodes and a set of tasks to be performed on those nodes. Playbooks can also include variables and conditional statements, which allow for more flexible and dynamic execution. This makes it easy to define complex configurations and deploy them consistently across a fleet of servers. Fig. 1 shows an example of an Ansible playbook. The playbook in Fig.consists of a single play that targets all managed nodes in the server group. The play includes two tasks: the task named “Install SSH server” uses the ansible.builtin.apt module to install the openssh-server package, and the task named “Start SSH server” uses the ansible. builtin.service module to start the ssh service. The “name” field of each task can be customized by users to describe the intention of the task. IV. METHODOLOGY A. Problem formulation We define the task Ansible-YAML Generation as follows: given a task description that includes both natural language (NL) as prompt X and Ansible YAML as the context script --- --- hosts: tasks: - name: Install SSH server ansible. builtin . apt: name: openssh- server state: present - name: Start SSH server ansible. builtin . service: name: ssh state: started servers Fig. 1: Example of an Ansible playbook Source File YAML Usage Count Type Galaxy 112K Ansible FT GitLab 64K Ansible PT GitHub + GBQ 11M Ansible PT GitHub + GBQ 2.2M. Generic PT TABLE I: Extracted file count per data source. The data is used for pre-training (PT) or fine-tuning (FT) of Wisdom models. C, generate an Ansible task or playbook snippet Y based on the intent of X and C. Both X and C are represented as a sequence of tokens. The snippet code Y is also formalized as an Ansible Language sequence (AL). We also define a Probabilistic generative model to model the distribution of an Ansible snippet Y given X and C as p(Y|X,C). The bestpossible Ansible task snippet is then given by g = arg max p(Y|X,C). (1) B. Dataset Construction We curated a YAML dataset from multiple data sources, including GitHub, Google BigQuery”, GitLab and Ansible Galaxy [23]. We use data extraction logic specific to each data source, while querying their respective API endpoints to extract YAML files and relevant associated metadata. For Google BigQuery, we downloaded every file with a valid YAML extension (‘.yml’, ‘.yaml’). For GitHub and GitLab, we considered every repository containing “Ansible” either in the name or the description. We de-duplicated the dataset using a simple exact match criterion. In addition to generic YAML, our dataset contains ansible-specific YAML, appropriately tagged so as to preserve the interplay between Ansible roles, collections, tasks and playbooks. Our curated dataset contains ~1.1M Ansible task and playbook YAMLs, and ~2.2M other generic YAML files. Table I summarizes our data sources, file count, type of YAML, and whether we use the data for pre-training (PT) of fine-tuning (FT). A publicly available dataset https://cloud.google.com/bigquery published by Google, C. Pre-training Our pre-trained models are implemented with the same architecture as CODEGEN, a decoder-based model released by SalesForce [12]. CODEGEN has been pre-trained on several datasets: (1) the Pile [24], around 350 billion tokens of natural language and 31 billion tokens of code; (2) BigQuery, around 119 billion tokens of code in 6 programming languages; (3) BigPython, around 71 billion tokens of Python code. CODEGEN has seen a large amount of natural language, but only a limited number of Ansible-YAML. For example, the Pile only includes around 25K Ansible-YAML and 600K generic YAML files. To improve the pre-trained model understanding of the semantics and syntax of YAML, we build WISDOMANSIBLE-MULTI and WISDOM-YAML-MULTI, which are trained from the CODEGEN checkpoint with a dataset that contains only Ansible-YAML files and a dataset that contains Ansible-YAML and generic YAML files, respectively (see table I for detail). The Ansible-YAML and generic YAML files account for about 1.1 billion training tokens in total. In addition, to exploring the effectiveness of using CODEGEN checkpoint as initialization, we propose WISDOM-ANSIBLE and WISDOM-YAML, which are trained from scratch with the above mentioned two datasets. Wisdom is designed to assist Ansible programmers in realtime and latency is therefore a critical parameter to consider. Which is why we choose a reasonably-sized model with a high token-per-second throughput rather than a very large model with a low throughput. We tested the architecture of CODEGEN 350M and CODEGEN 2.7B. We benchmarked the generation throughput on single GPU for both models and found that the 350M model was ~ 1.9x faster than than the 2.7B. Our training code is based on the Huggingface Transformers library [25] that provided the CODEGEN checkpoints and tokenizers. We trained the model using our YAML dataset for 9 epochs using 16 A100 GPUs with 80 GB of memory. To speed up the training we used bf16 data type. Effective batch size was 32 and learning rate was 5 x 10~> with a linear decreasing schedule. During pre-training, YAML files were packed to fill up a context window of 1024, and we used a special separator token to separate the files. D. Fine-tuning 1) Dataset: We used the Ansible Galaxy data to fine-tune the pretrained models mentioned above on the Ansible-YAML generation downstream tasks, as this dataset is a collection of good quality files created and vetted by the Ansible community. Galaxy contains many type of Ansible files, but we extracted only playbooks containing tasks, and lists of tasks from roles. We checked for valid YAML and correct playbook or task syntax using PyYAML (https://pyyaml.org), and standardized the formatting to match the style recommended by the Ansible team. The Galaxy data files were randomly split into train (80%), validation (10%) and test (10%) sets. Exact match deduplication is performed at both the file and sample level across all splits. --- --2) Generation Types: As described in Section IV-A, the goal is to generate two kinds of Ansible output, either a full playbook (PB) or a task (T) given the natural languages (NL) requirement. The task (T) can be either part of a playbook, or part of a role. Thus, we can have 4 types of input-output combinations in the fine-tuning dataset. « NL->PB: The context is empty, so the only input is the natural language prompt. We have limited the expected output playbooks to examples containing only 1 or 2 tasks. This forms the vast majority of playbooks. Playbooks containing more than 2 tasks are used to generate the next type of samples. « PB+NL->T: The model is expected to predict the next task in a Playbook, and the context is a playbook with at least 1 task. « NL->T: The context is empty and the model is expected to generated only 1 task, which is the first task of a role. « T+NL-T: The model is expected to predict the next task in a role based on the natural language prompt, where the context is the previous tasks. 3) Input Prompt Formulation: A helpful feature of the Ansible language is that each Playbook or Task frequently contains a “name” field, whose value is the natural language description of the goal of playbook or task, as shown in Fig. 1. Thus the target output can be represented as Y = {Ynz, Yar}, where Yy rp, refers to the “name” line in the Ansible script, and Yar refers to the remainder of the script. In addition, Yyz, is exactly the same as the NL sequence X in the original problem formulation in Section IV-A. Thus, to take advantage of this feature and to make it accommodate best the pre-trained decoder-based model, we reformalize the text-to-code generation problem in Section IV-A into a code completion problem. Specifically, Eq.1 can be formalized as g =arg max p(Ywr, Yar|X,C). = p(Ynr|X, C)p(Yar|X, C) = p(X|X,C)p(YarlYnz,C) = p(Yar|Ynz,C) (2) considering that Y4; and Yy,, are conditionally independent given X and C and X = Yw_y. Thus, we can use the value of the “name” line Yy, as the prompt. When the output is a playbook, we combine the values of “name” fields of the playbook and its tasks to create the prompt. We have experimentally validated that this re-formalization can largely improve the overall performance regarding all kinds of metrics. The results are provided in the following section. 4) Training: We fine-tuned pre-trained models using our Galaxy dataset for 8 epochs. The effective batch size wasand the learning rate was 5 x 10~° with a cosine decreasing schedule. We used the BLEU score on the validation set to determine the best checkpoint. E. Demo/Plugin We expose a GRPC and REST API based interface to model predictions so that inference can be called out using GRPC and REST clients. We wrote a custom Visual Studio Code plugin that is enabled for ansible files and gets triggered when the user hits a binding key. This triggers a call to the API to carry out the prediction which is then formatted and pasted back on to the editor. In our current setup, when a user writes the prompt for the task, example “- name: install nginx on RHEL”, and hits enter, we invoke the API to carry out the prediction and then take the results and paste it back on the editor. The user can either hit tab and accept the suggestion, or escape key to reject the suggestion. In future implementations, we plan to improve user experience in terms of quality of recommendation by leveraging additional information in workspace of the editor, as well as improving latency by using techniques like caching. V. EXPERIMENTS A. Evaluation Metrics Since the generated ansible task or a small playbook always has high dependency on external resources, it is not practical to evaluate the correctness of a task by executing it. For example it would be impractical to evaluate a task that installs a package on a number of remote hosts by executing it with Ansible and checking that the result is as expected. Thus, our evaluation metrics are based on the similarity between the generated ansible tasks or playbooks and the ground-truth. For the experiments described in this paper, 4 comparison metrics are used: Exact Match, BLEU [26], [27]", Ansible Aware, and Schema Correct. Among these, Ansible Aware and Schema Correct are two novel metrics designed specially for the Ansible tasks or playbooks. Ansible Aware: Ideally a metric should reflect the user’s view of the result, e.g. how many changes must be made to correct it. The purpose of the Ansible-aware metric is to use knowledge of the Ansible YAML syntax to compare the modules, keywords and parameters that comprise an Ansible task or playbook. Since an Ansible task or playbook is a mapping (dictionary) the order of the key-value pairs is not significant — the usual key order for a task is: name, module, keyword(s). The “name” is optional, its value is a natural language description of the task. The module key identifies the operation to be performed while its value is a dict holding the module’s parameters. The optional keywords define conditions that influence the execution of the task, e.g. environment, elevated privileges, remote userid, error handling, conditionals, loops. The keyword values may be scalars, lists, or dicts. The score of a task is computed from the average of the scores of the toplevel key-value pairs found in the target and predicted YAMLs. “Since the sequences of tokens in an Ansible YAML file are important, while some reordering is permitted, the BLEU score’s basis on n-gram coverage suggests it could be a useful metric. --- --1 2 # Generating a task from NL prompt (L18)3. # using a playbook as context (L1- L17)4 # model expected output in (L19- L20)5 - name: Network Setup Playbook6 connection: ansible .netcommon.network_cli7 gather_facts : false8 all10 - name: Get config for VyOS devices ; 11 vyos.vyos. vyos_facts :12 gather_subset : all "13 - name: Update the hostname14 vyos.vyos. vyos_config:15 backup: yes16 lines :17 - set system host- name vyos- changed18 - name: Get changed config for VyOS devices19 vyos.vyos. vyos_facts : 20 gather_subset : all (a) PB+NL—+T 1 -- 2 # Generating a task from NL prompt (L9)3 # using task(s) as context (L1- L8)4 # model expected output in (L10- L12)5 - name: Ensure apache is at the latest version6 ansible . builtin .yum:7 name: httpd8 state: latest9 - name: Write the apache config file10 ansible . builtin . template: 11 sre: /srv/httpd .j12 dest: /etc/httpd .conf (c) T+NL3T # Generating a playbook from NL prompt (L5) # without context # model expected output in (L6- L17) - name: Network Setup Playbook connection: ansible .netcommon.network_cli gather_facts : false hosts: all tasks : - name: Get config for VyOS devices vyos.vyos. vyos_facts : gather_subset : all - name: Update the hostname vyos.vyos. vyos_config: backup: yes lines : - set system host- name vyos- changed (b) NL-+PB # Generating a task from NL prompt (L5) # without context # model expected output in (L6- L8) - name: Ensure apache is at the latest version ansible . builtin . yum: name: httpd state: latest (d) NL>T Fig. 2: Ansible generation types defined in IV-D2. Each snippet of code includes a comment (in red) highlighting the NL prompt, the context provided to the model as well as the expected output. The comments are used here only for illustration purpose and are not provided to the model during training or inference. Similarly for playbooks the scores of its top-level key-value pairs are averaged, where the score of each of its tasks is computed as above. The “name” key and its value can be ignored as they has no effect on the execution of the task. The score for each key-value pair is the average of the key and value scores. Currently keys missing from the prediction are given a score of 0, while keys inserted in the prediction are ignored. If a key’s value is a list or dict, its score is recursively computed by averaging the scores of each dict entries or list items. When comparing the module names they are first replaced by their fully qualified collection name (FQCN) if necessary, e.g. copy is changed to ansible. builtin.copy. Another normalization that is applied is to convert the old ky = v1, ka = ve syntax for module parameters into a dict. There are some modules that are almost equivalent, e.g. command / shell, copy / template, package / apt, dnf, yum. Since they accept many of the same arguments and in some cases can be exchanged, such module differences are given a partial key score which is averaged with the score of their arguments. Our motivation for ignoring insertions is that they are less costly than deletions as they can be easily removed, but we plan to investigate the impact of including an insertion penalty. Schema Correct: this metrics is designed to measure the correctness of the result, i.e. whether or not it satisfies the Ansible schema. It does not reflect the accuracy of the model, as it applies just to the predictions. The Ansible playbook and tasks schema used by the Ansible linter are quite strict and do not accept some historical forms which are still allowed by Ansible itself. Hence a low score does not necessarily mean that the results would be rejected by Ansible. Since we did not filter our training data with these schema a sample with a perfect Exact Match score may have a Schema Correct score of 0. B. Results 1) Pre-training: Pre-trained Models for Comparison All the models are implemented with the same architecture as CODEGEN, but are pre-trained on different datasets. Table IL introduces the names of the models and the datasets they were pre-trained on. The first three models correspond to the original CODEGEN checkpoints released by Salesforce [12]: --- --Model Dataset The Pile BigQuery BigPython Ansible YAML | Generic YAML’ CODEGEN-NL v CODEGEN-MULTI v v CODEGEN-MONO v v v WISDOM-ANSIBLE v WISDOM- YAML v v WISDOM-ANSIBLE-MULTI v v v WISDOM-YAML-MULTI v v v v TABLE II: Model names and their associated pre-training datasets. The Pile, BigQuery and BigPython were used by Salesforce [12], while Ansible YAML and Generic YAML are introduced in this work. CODEGEN-NL, CODEGEN-MULTI, and CODEGEN-MONO. The last four rows are the domain-specific pre-trained WISDOM models proposed in this paper for YAML data. WISDOM-ANSIBLE was pre-trained only on Ansible YAML while WISDOM-YAML was pre-trained on both Ansible YAML and Generic YAML. WISDOM-ANSIBLE-MULTI was initialized with the weights of CODEGEN-MULTI and we extended the pre-training using Ansible YAML. WISDOM- YAML-MULTI was also initialized with the weights of CODEGEN-MULTI and we extended the pre-training using both Ansible YAML, and Generic YAML. Experiment Settings We first evaluate the models in few-shot setting on our Ansible test set which includes a distribution of the four generation tasks described previously: PB+NL-—T, NL-—PB, T+NL-T, and NLT. Our main goal here is to understand how much adding Ansible and generic YAMLs to our pre-training improves the performances of the models. Table III presents the results for all CODEGEN and WIsDOM models as well as OpenAI Codex. For each row, we indicate the size of the model (i.e. number of parameters), as well as the size of the inference context window. When the input to the model {Yz,C} (see IV-D3) is larger than the context window, it is left-truncated. For the tasks that do not include any context (NL—PB and NL-+T), we found that adding the string “Ansible\n” prior to the prompt improved the performances of CODEGEN models as well as Codex. For the WISDOM models, we did not observe any significant change and therefore left the context empty. All the models were evaluated using the four metrics described in Section V-A: Schema Correct, Exact Match (EM), BLEU, and Ansible Aware. In order to correctly evaluate against these metrics, in the case of Ansible task generations, we truncated the models output predictions to keep only the first generated task. For playbook generation (NL-—>PB), we did not apply any truncation. Finally, all results presented thereafter were obtained using greedy decoding. We would expect some improvement by using random sampling or beam search decoding. CODEGEN Comparison on Ansible Generation The first three rows refer to the CODEGEN models as released by Salesforce. As shown in Table II], CODEGEN-MULTI trained on The Pile and BigQuery performs the best among the three CODEGEN models. Specifically, CODEGEN-NL 350M performs the worse across all metrics with a BLEU of 24.and an Ansible Aware score of 6.24. The Schema Correct is 71.26. This rather high value shows that the small subset of YAMLs present in the Pile is already enough for the model to have a good understanding of the YAML syntax. However, note that this metric does not compare against any target, and only indicates that CODEGEN-NL can generate correct Ansible YAML, ~71% of the time. CODEGEN-MULTI 350M scores are higher, especially the Ansible Aware score that improves by ~28 points. The improvement is mainly attributed to the very large amount of code present in the BigQuery (~120B training tokens). The additional code samples help the model to have a better understanding of structures and syntax (e.g. indentation) as seen by the 12 point boost of Schema Correct. The results of CODEGEN-MONO are similar to CODEGEN-MULTI, showing that the addition of more PYTHON code does not help our Ansible generation tasks. To measure the effect of the size of the model, we additionally compared CODEGEN-MULTI 350M, 2.7B, and 6B, as shown in Table III. The larger models do perform slightly better, but the improvement is not striking. Comparing with the 350M baseline, the 2.7B model improves the Ansible Aware score by ~1.8 points and the 6B model by ~4.9 points. Codex for Ansible Generation We also evaluated Codex (Codex-Davinci-002) on the Ansible generation tasks, as shown in Table II. The Schema Correct and BLEU scores of Codex are in the same order of magnitude as CODEGENMULTI 350M but the Ansible Aware is significantly higher (48.78). Also note that the exact match is the highest of all models tested, which indicates that Codex likely saw large portions of our Galaxy dataset. WISDOM models for Ansible Generation As shown in the last four rows in Table II. The WISDOM models are notably better than CODEGEN and Codex baselines, showing that our YAML pre-training provided a boost in performance. The last two rows show the two WISDOM models pre-trained with YAMLs only. Both models reach Ansible Aware score similar to Codex, and BLEU score comparable to CODEGEN-MULTI 6B. WISDOM-ANSIBLE-MULTI 350M has the highest Ansible Aware score, ~6 points higher than Codex and ~15 points higher than CODEGEN-MULTI 6B. The BLEU score is also ~6 points better than CODEGEN-MULTI 6B. These results show that adding a large collection of YAMLs to pre-train or extend the pre-training of an existing model offer a large boost in performance for the Ansible task generations. Further, the WISDOM models outperform CODEGEN and Codex with --- --less parameters, which is advantageous in this application that requires fast inference. 2) Finetuning: We fine-tuned and evaluated CODEGEN and WISDOM models on the Galaxy dataset described in IV-D1. As shown in Table IV, fine-tuning on specified Ansible generation task is necessary and largely boost the performance compared to the few-shot results in Table III. For example, comparing CODEGEN-MULTI with 2048 context window in few shot vs. fine-tuned CODEGEN-MULTI with the same context window, both BLEU Ansible aware scores increase by ~30 points. To better understand how different experimental factors influence the Fine-tuned models, we conducted ablation studies regarding the format of prompt, pre-trained models, model size, context window size, and the dataset size. Effectiveness of Prompt Formulation As mentioned in Section IV-D3, to take advantage of the feature of AnsibleYAML data, we re-formalized the code generation problem into a code completion process by utilizing the natural language prompt as a part of “name” field. To validate its effectiveness, we compare it with the typical prefix-based CODEGEN model (named as CODEGEN-prefix) which contains the prefix term‘context code” before the context information and “prompt” before the natural language part. According to the results shown in Table IV, under the window size 1024, CODEGEN with the proposed prompt format largely outperforms CODEGEN-prefix, for example, 10% percent higher on BLEU, 26% percent higher on SCHEMA CORRECT, and 16% percent higher on EM. Analysis on Different Pre-trained Models The pre-trained models play an important role in the performance of fine-tuned models. By comparing the fine-tuned CODEGEN-MULTI and WISDOM-ANSIBLE-MULTI (both with window sizeand model size 350M) in Table IV, it shows pre-training on Ansible data can help the large language model better understand the syntax and structure of Ansible. Specifically, WISDOM-ANSIBLE-MULTI has gained 1% increase regarding Schema Correct, EM and Ansible Aware. Analysis on Context Window Size We first compare fine-tuned CODEGEN-MULTI models with different context window sizes. More context improves the model predictions at inference time, but it also requires more compute resources for training. In addition, in the case of our Ansible-specific generation tasks, this is not obvious whether a very large context improves the model outputs. In table V, the first three rows present results for context window sizes of 512, 1024, and 2048, respectively. The 512 context window has a 61.BLEU and a 64.84 Ansible Aware score. When doubling the size of the context to 1024, the BLEU goes up to 66 and the Ansible Aware score to 69.77. However, we do not observe improvement when going beyond 1024, as seen with theresults. Note that this observation is based on our current dataset, and this is possible that other test sets would benefit further from larger contexts. Nonetheless, in our current setup, we conclude that a 1024 context window is adequate. Analysis on Number of Training Data To investigate the influence of the training data, we use varying amount of data (10%, 20% and 50% of training data) for finetuning. The comparison results are shown in the last three lines in Table IV. With the increment of training data, the performance improved accordingly, for example, BLUE from 61.68% to 66.67%. However, the speed of improvement decreases, from 1.7% per 10% of data to 1.2% per 50% of data. This shows the current training data size has almost converged and the current finetuning data size is selected with the high performance cost ratio. It is interesting to note that by finetuning with even a little bit of data, the Wisdom model performance on AnsibleYAML Generation task becomes much better than CodexDavinci-002 (in few-shot settings) on all metrics. As can be seen in Tables III and IV, the best performing Wisdom model, WISDOM-ANSIBLE-MULTI, trained on 100% finetuning data is better than Codex-Davinci-002 in fewshot settings by about 15 BLEU points and about 16 EM points. Analysis on Generation Types As discussed in Section IV-D2, there are 4 types of generation problems for Ansible- YAML Generation. To validate how the proposed model deal with these 4 types, we evaluate on them sepecreatly, as shown in Table V. Due to the dominant number of T+NL—>T in the training data, the proposed model performs the best in this task. For PB+NL->T type, even there are onlyfine-tuning samples, the performance is comparable to that of T+NL-—T. This may because T+NL->T and PB+NL-T are both used for generating a task given the natural languages and context ansible data and thus can benefit each other while fine-tuning. The proposed model has difficulty in generating a playbooks, as shown in the second line of the table, which is because of the limited number (i.e., 550 counts) of training data for NLT. In addition, by comparing the performance between PB+NL—T and NL->T, the necessities of utilizing the context information for generation is validated. Though NL-T has more training data than that of PB+NL-—>T, the performance on NL->T decrease dramatically compared to that of PB+NL-—>T, for example, 33% decrement in BLEU VI. CONCLUSION This work describes the application of transformer-based models to the generation of Ansible-YAML, starting from a user-provided natural language prompt. The objective of this model is to build an AI assistant for Ansible users and improve their productivity. We provide a formal definition of the problem and we start from an existing pre-trained decoder-based model. We built a new training dataset with Ansible data for code generation that will be shared with the community. We extend the training of the base model with our dataset and evaluate the results. Our results show that with our approach, Wisdom performs Ansible generation equally or better compared to state-of-the-art models for code generation. --- --Model Size Context Schema EM BLEU Ansible Window Correct Aware CODEGEN-NL 350M 2048) 71.26 1.69 24.95 6.CODEGEN-MONO 350M 2048 82.40 6.37 34.24 34.CODEGEN-MULTI 350M 2048 83.65 6.92 34.26 34.CODEGEN-MULTI 2.7B 2048 78.00 1.74 37.27 36.CODEGEN-MULTI 6B 2048 85.80 7.98 39.67 39.Codex-Davinci-002 175B 2048) 88.82 13.66 50.40 55.WISDOM-ANSIBLE-MULTI 350M 1024 96.56 7.35 46.58 54.WISDOM-YAML-MULTI 350M 1024 95.97 7.16 45.52 53.WISDOM-ANSIBLE 350M 1024 95.10 4.63 39.49 48.WISDOM-YAML 350M 1024 94.63 4.19 40.13 47.TABLE III: Evaluation results for CODEGEN, Codex, and WISDOM models in few-shot setting. The first section refers to the CODEGEN models released by Salesforce, the second one to OpenAI Codex and the third one to WISDOM models. Model Size Context Schema EM BLEU Ansible Window Correct Aware CODEGEN-MULTI 350M 512 97.77 61.75 64.CODEGEN-MULTI 350M 1024 98.06 66.03 69.CODEGEN-MULTI 350M 2048 98.02 66.12 69.CODEGEN-MULTI 2.7B 1024 98.36 65.25 69.CODEGEN- MULT:WISDOM-ANSIBLE-MULTIWISDOM- YAML-MULTIWISDOM-ANSIBLEWISDOM- YAMLWISDOM-ANSIBLE-MULTI-WISDOM-ANSIBLE-MULTI 350M 1024 98.08 25.00 63.37 67.-WISDOM-ANSIBLE-MULTI 350M 1024 98.08 22.62 61.68 66.-TABLE IV: Evaluation results of the fine-tuned models. The first section shows results of CODEGEN-MULTI fined-tuned on Galaxy, varying the size of the context window and the number of parameters. The second section shows the WISDOM models fine-tuned on Galaxy, for a fixed 1024 context window. The last section corresponds to WISDOM-ANSIBLE-MULTI fine-tuned on Galaxy and varying the amount of data (10%, 20%, and 50% of the dataset). Generation Types Count Schema EM BLEU Ansible Correct Aware ALL 50580 98.06 28.64 66.03 69.NL-—PB 550 93.09 0.0 22.76 23.NLT 6961 96.51 5.17 45.46 49.PB+NL—>T 3441 98.75 46.00 79.66 82.T+NL—+T 39628 98.35 31.65 69.41 72.TABLE V: Breakdown of the evaluation metrics per generation type (see [V-D2) for CODEGEN-MULTI fine-tuned on Galaxy. “ALL” refers to all generation types combined together. LIMITATIONS A lot of Ansible development happens on playbooks. However, playbooks are not well represented in our fine-tuning dataset since we found very few acceptable playbook samples in Ansible Galaxy. And most of the ones that are included are small with two or less tasks. Ansible Blocks, which are logical groups of tasks are also something we have not specifically trained and tested on. This is something we hope to expand to in the future. We also hope to do more analysis on the models sensitivity to prompts and robustness to changes in indentation, quotes and letter case. Currently we focus on the Natural Language to Ansible generation task. This can be expanded to a more general completion task where a user can prompt the model at any stage of code development. ETHICS STATEMENT A. Legal Implications Wisdom is trained on code repositories that are publicly available and with an open-source license. Training of ML algorithms on public repositories, such as those in GitHub, has been regarded has fair use [28]. Once trained, even if it is a rare occurrence, Wisdom could potentially generate code that is identical to a training set sample. When this happens, the generated code is most likely a very common pattern in Ansible, rather than the result of a copy. Furthermore, while Wisdom provides a recommendation, it is the user’s choice to accept it and use it in the codebase. --- --B. Offensive Language Large amounts of public repositories could contain language that is offensive or discriminatory to multiple groups in the form of comments or code. While this is primarily a research work, not intended for product use, a product ready version of the model would undergo a major data cleaning and normalization process to avoid the generation of unwanted expressions. C. Security and Safety Risks Wisdom is trained on good quality data, however there is a significant risk of generating Ansible that contains security vulnerabilities or could damage a system. The model it is not trained to generate Ansible that is secure or safe, but only to optimize metrics with respect to our test set. While security vulnerabilities cannot be completely eliminated, it is possible to reduce this event by explicitly improving the security and safety of training data, and also by performing basic postprocessing analysis to avoid the most common vulnerabilities. Both approaches would be considered in a product-ready version of the model. D. Economic and Labor Market Impact The topic of economic and labor market disruption by AI algorithms has been the subject of a wide range of arguments. Specifically, an AI coding assistant could potentially be seen as a threat to software development labor. A deeper look at how these coding assistants should and are being used [6] will clearly highlight how the presence of an human expert cannot be replaced. Indeed, current ML models cannot provide the deep semantic comprehension needed to understand and integrate the recommendation into the codebase. REFERENCES OpenAI, “Gpt-4 technical report,” 2023. 2] N. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Rajamani, and R. Sharma, “Jigsaw: Large language models meet program synthesis,” in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 1219-1231. 3] L. Buratti, S. Pujar, M. Bornea, S. McCarley, Y. Zheng, G. Rossiello, A. Morari, J. Laredo, V. Thost, Y. Zhuang et al., “Exploring software naturalness through neural language models,” arXiv preprint arXiv:2006.12641, 2020. 4] R. Puri, D. S. Kung, G. Janssen, W. Zhang, G. Domeniconi, V. Zolotov, J. Dolby, J. Chen, M. Choudhury, L. Decker, V. Thost, L. Buratti, S. Pujar, S. Ramji, U. Finkler, S. Malaika, and F. Reiss, “Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks,” 2021. 5] S. Greengard, “Ai rewrites coding,” Commun. ACM, vol. 66, no. 4, p. 12-14, mar 2023. [Online]. Available: https://doi.org/10.1145/6] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman ef al., “Evaluating large language models trained on code,’ arXiv preprint arXiv:2107.03374, 2021. 7) H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziére, N. Goyal, E. Hambro, F. Azhar et al., “Llama: Open and efficient foundation language models,” arXiv preprint arXiv:2302.13971, 2023. 8] FF Xu, U. Alon, G. Neubig, and V. J. Hellendoorn, “A systematic evaluation of large language models of code,” in Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, 2022, pp. 1-10.10.[26] 227) [28] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang et al., “Codexglue: A machine learning benchmark dataset for code understanding and generation,” in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. L. Tunstall, L. von Werra, and T. Wolf, Natural language processing with transformers. " O’Reilly Media, Inc.", 2022. Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago er al., “Competitionlevel code generation with alphacode,” Science, vol. 378, no. 6624, pp. 1092-1097, 2022. E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, “A conversational paradigm for program synthesis,” arXiv preprint arXiv:2203.13474, 2022. Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang et al., “Codebert: A pre-trained model for programming and natural languages,” in Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 1536-1547. A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, “Learning and evaluating contextual embedding of source code,’ in International Conference on Machine Learning. PMLR, 2020, pp. 5110-5121. Y. Wang, W. Wang, S. Joty, and S. C. Hoi, “Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation,” in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 8696-8708. W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, “Unified pretraining for program understanding and generation,” in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 2655-2668. L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al., “The pile: An 800gb dataset of diverse text for language modeling,” arXiv preprint arXiv:2101.00027, 2020. P. Yin and G. Neubig, “A syntactic neural model for general-purpose code generation,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume I: Long Papers), 2017, pp. 440-450. H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt, “Codesearchnet challenge: Evaluating the state of semantic code search,” arXiv preprint arXiv:1909.09436, 2019. J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le ef al., “Program synthesis with large language models,” arXiv preprint arXiv:2108.07732, 2021. R. H. Ansible, “Red Hat Ansible, automation for https://www.ansible.com/. A. Github, “Ansible Github Project,” https://github.com/ansible/ansible. Ansible, Inc, “Ansible Galaxy,” https://galaxy.ansible.com/. S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy, K. McDonell, J. Phang et al., “Gpt-neox-20b: An opensource autoregressive language model,” in Proceedings of BigScience Episode 5—Workshop on Challenges & Perspectives in Creating Large Language Models, 2022, pp. 95-136. T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush, “Transformers: State-of-the-art natural language processing,” in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 3845. [Online]. Available: https://aclanthology.org/2020.emnlp-demos.K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,’ IBM Research Report RC22176 (W0109-022), 2001. C.-Y. Lin and F. J. Och, “Orange: a method for evaluating automatic evaluation metrics for machine translation,” in COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics, 2004, pp. 501-507. C. O’Keefe, D. Lansky, J. Clark, and C. Payne, “Comment regarding request for comments on intellectual property protection for artificial intelligence innovation. before the united states patent and trademark office department of commerce,” 2019, https://perma.cc/ZS7G-2QWF. everyone,”