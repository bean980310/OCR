arXiv:2306.09327v1 [cs.CV] 15 JunLanguage-Guided Music Recommendation for Video via Prompt Analogies Daniel McKee¹* Justin Salamon² Josef Sivic 2,3 Bryan Russell² ¹University of Illinois at Urbana-Champaign 2 Adobe Research 3Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical University dbmckee2@illinois.edu salamon@adobe.com josef.sivic@cvut.cz brussell@adobe.com https://www.danielbmckee.com/language-guided-music-for-video Abstract We propose a method to recommend music for an input video while allowing a user to guide music selection with free-form natural language. A key challenge of this problem setting is that existing music video datasets provide the needed (video, music) training pairs, but lack text descriptions of the music. This work addresses this challenge with the following three contributions. First, we propose a textsynthesis approach that relies on an analogy-based prompting procedure to generate natural language music descriptions from a large-scale language model (BLOOM-176B) given pre-trained music tagger outputs and a small number of human text descriptions. Second, we use these synthesized music descriptions to train a new trimodal model, which fuses text and video input representations to query music samples. For training, we introduce a text dropout regularization mechanism which we show is critical to model performance. Our model design allows for the retrieved music audio to agree with the two input modalities by matching visual style depicted in the video and musical genre, mood, or instrumentation described in the natural language query. Third, to evaluate our approach, we collect a testing dataset for our problem by annotating a subset of 4k clips from the YT8M-Music Video dataset with natural language music descriptions which we make publicly available. We show that our approach can match or exceed the performance of prior methods on video-to-music retrieval while significantly improving retrieval accuracy when using text guidance. Language-Guided Music Retrieval Retrieved Music Video+Language Query Folk music with guitar Upbeat pop Upbeat pop User input includes video and a text description of target music. ViML A somber folk ballad featuring female vocalist, guitar, and tambourine ViML ViML Pop featuring a female vocalist with energetic synth melodies Lighthearted pop song with a male vocalist backed by guitar, drums Descriptions of the retrieved music tracks provided by the authors for illustration only. Figure 1. Language-guided music retrieval. Our ViML model takes a video and text prompt as input to retrieve a suitable music track from a database. The model learns to fuse video and language representations in order to guide retrieval. Notice how our approach retrieves audio matching both the video and language content. For the same video query (top two rows), we can change the music style to match the language query, and for the same Upbeat pop query (bottom two rows) we can change the vocalist to match the video content. To fully appreciate our results, please view and listen to the companion video on our website. 1. Introduction A key part of the video editing process for creators is choosing a musical soundtrack. Especially given the rise of short-form videos on social media platforms, automated music recommendation systems have become an increas*Work done as an intern with Adobe Research ingly common and important part of video editing applications. While these systems can be helpful for finding relevant music, they often provide limited capability for user control over the types of music recommended. In previous work, music is retrieved based solely on the visual content and style from a video [31,36]. However, music itself can convey critical information about how a video should be perceived. Music selection alone can transform a visual scene into one that is perceived as happy, scary, or sad¹. As a result, the lack of user input capability to describe a target music for an inputted video is a key limitation on the utility of current music recommendation methods. In this work, we propose a more flexible music-for-video recommendation approach that allows a user to guide recommendations towards specific musical attributes including mood, genre, or instrumentation, illustrated in Figure 1. To maximize flexibility and user convenience, we propose to take user musical attribute descriptions in the form of free-form natural language (e.g., "Folk music with guitar” in Figure 1). There are two key challenges in learning a model for language-guided music recommendation for video. First, while there are datasets which include music+text [5,7,19,29] or music+video [1], there are no available datasets which include music, video, and text together. Further, the existing datasets that do include text and music focus on a limited vocabulary of tags rather than free-form text. Second, previous works have explored jointly learning visual, audio, and text embeddings [2-4, 33, 46], and without careful regularization, a network can overfit and possibly learn to ignore one of the input modalities. We seek to train a model that keeps the information flow through the network and does not ignore one of the modalities. In order to meet the challenges outlined above, our work makes the following contributions: (1) We propose a new approach to automatically generate natural language descriptions for a music video dataset. This approach combines a pre-trained music tagger with a large-scale language model to output natural language descriptions for any music clip, illustrated in Figure 2 (left). First, the tagger predicts tags from a pre-defined vocabulary describing musical genre, mood, or instrumentation. Second, these predicted tags, together with their probabilities, are converted into a rich natural language description for the music video using a carefully designed large-scale language model prompting procedure based on analogies with a small number of human-provided text descriptions (i.e., A (tags): A' (description) :: B (tags): B' (description)), where A and B are music tags automatically provided by the tagger, A' is a human-provided text description, and B' is the natural language description output by the large-scale language model. (2) We propose a Transformer-based model architecture with a video-text fusion module. Our model, which we call Video to Music with Language (ViML), is able to retrieve music that matches both the visual content/style of the input video and described musical genre, mood, and instrumentation in the natural language query. Similar to prior work [16,27,38], we find that training with text dropout as a regularization mechanism is critical to achieve music rehttps://www.youtube.com/watch?v=iSkJFs7myntrieval performance improvements from added text inputs. (3) We release a dataset of 4000 high quality text annotations for clips from a subset of the YT8M-Music Video dataset [1] to evaluate language-guided music recommendation. We show that our method can achieve substantial improvements over prior works on music retrieval when incorporating text inputs. Moreover, our model can match or even exceed performance of baseline music-for-video recommendation models when the text input is ignored. 2. Related Work Music and language. There are numerous music tagging datasets which contain tags specifying attributes like mood, genre, or instrumentation [5, 7, 19, 29], and several works have studied training automated music taggers from such datasets [10,20,21, 30, 42, 43]. Beyond these methods constrained to limited tag vocabularies, some works also have studied jointly embedding music and free-form natural language [9, 14, 26, 43]. However, none of these approaches incorporate the video modality. Music recommendation for video. Others have investigated automatic recommendation of music based on style and content of an input video [13, 23, 31, 36, 48]. Prétet et al. [31] build on previous self-supervised methods [13] by incorporating learned audio features instead of handcrafted features. More recently, Surís et al. [36] propose the MVPt model which employs a self-supervised contrastive loss and Transformer [37] architecture to greatly improve the long-range temporal context modeling in order to retrieve suitable music for a given input video. However, none of these approaches incorporate the natural language modality which we focus on in this work. Video, audio, & language. While a wide variety of works have explored audio-visual or vision-language topics, a smaller number focus on jointly embedding video, audio, and language [2, 4, 12, 33, 44, 46]. Specifically, Alayrac et al. [3] investigate how best to combine audio and video with text representations. The VATT model [2] is a fully endto-end tri-modal model capable of using a single shared Transformer backbone across modalities. Lastly, two recent methods [12, 44] extend CLIP [32] to jointly embed audio. While relevant, all of these approaches share a common focus on "environmental” or “everyday” sounds rather than music, and they lack the long-range temporal context modeling critical for music recommendation as a result. In addition, none of these works address a downstream problem of using two modalities in combination (video, text) to query results from another (music). Few-shot language model prompting. Recent large language models have shown significant success at a wide variety of few-shot or zero-shot tasks from those related to read→ Music Tagger Music Tagger I. prompt2text Synthesis PROMPT INPUT TO BLOOM-176B: [Input]: GENRES: electronic (82.6%), dance (63.9%); MOODS: dynamic (46.8%), dramatic A (33.3%); INSTRUMENTS: synthesizer keyboard (81.9%), electronic drumset (77.6%), synth bass (68.4%) [Output]: Electronic party track with high A' energy synth lines and autotuned female vocals. ➡B [Input]: GENRES: country (47.2%), rock (30.8%); MOODS: happy (40.2%), relaxing (31.7%), nostalgic (30.9%); INSTRUMENTS: drumset (63.9%), electric guitar (50.5%), male vocals (49.7%), electric bass (49.1%), acoustic guitar (33.7%) [Output]: OUTPUT: Country rock track with a nostalgic feel. The B' song features acoustic guitar, electric guitar, electric bass, drums, and male vocals. … GENRES: country, rock II. data2text Synthesis Music Tagger MOODS: happy, relaxing, nostalgic INSTRUMENTS: drumset, male vocals, acoustic guitar Input Tags into Template Sentences This is country and rock music. The music gives a happy, relaxing and nostalgic vibe. The soundtrack has drumset, male vocals, and acoustic guitar. Zero-shot D2T Pipeline: Ordering, Aggregation, and Compression This is country and rock music. The soundtrack has acoustic guitar, drumset, and male vocals giving a happy, relaxing, and nostalgic vibe. Music Tagger III. tags Synthesis acoustic guitar, country, happy, drumset, relaxing, electric bass, male vocals, rock, nostalgic, electric guitar Figure 2. Overview of three text synthesis approaches explored in our work. All rely on tag predictions from a pretrained music tagger model. We highlight output text from each method in green, inputs from the tagger in blue font, and inputs from a human annotator in red font. Left: We introduce the prompt2tags approach for generating natural language descriptions given automatically predicted music tags and a small set of human descriptions. We ask a large language model (BLOOM-176B) to complete an analogy task (A : A' :: B : B') between music tags (A, B) and descriptions (A', B'). Top right: The data2text pipeline inserts sampled tags into randomly selected template sentences corresponding to each tag category. The Zero-shot D2T model [17] then orders, aggregates, and compresses these templates into a final output description. Bottom right: The tags approach involves direct concatenation of high confidence tags to form the text description of the music. ing comprehension and QA [8,40], to reasoning [18,41], or even data augmentation [11,39,45]. A few works have extended this success to multimodal applications. For example, Zeng et al. [47] show that language models can solve video understanding or image captioning tasks by reformulating these problems as reading comprehension or QA tasks with inputs from large visual or audio models. Other works have used language models to help with generating or retrieving text annotations for multimodal tasks [24] or in robotic planning [15,34]. In this work, we propose a completely new application of few-shot language query modeling: generating free-form musical text descriptions from music tags. 3. Approach Our goal is to train a pair of feature encoders fut and ƒm which are capable of predicting the similarity s(fut, fm) between an input pair of video and musical text description (v, t) and a music clip m, as illustrated in Figure 3. To train such a model in a supervised manner, it is necessary to have a dataset of corresponding triplets (v, m, t). While largescale datasets of videos with paired music are available, it is difficult to find datasets which also contain high-quality natural language descriptions of the paired music tracks. As a result, we investigate a synthesis approach based on a model G which generates text descriptions from available structured data in the form of music tags for each music track. In the following sections, we first discuss the musical description synthesis approach G before describing an approach to train a language-guided video-to-music recommendation model. 3.1. Synthesizing Text Descriptions for Music Suppose that we are given a set of video and music audio pairs (vi, mi) and that we also have access to structured data di ETD which describe the music mi. In our case, this structured data consists of musical tags with confidences. Each music track m¿ may be described by a free-form human text description t¿ Є TT. However, it can be prohibitively expensive to obtain high-quality human descriptions on a large scale. Instead, we propose to synthesize such text descriptions using a generator G : TD → TT which maps structured data describing an audio track to the space of natural human descriptions. = The goals of such a mapping function G are that: (i) a predicted output ti = G(d) should preserve the semantic meaning contained within the structured data d¿ corresponding to a specific musical track and (ii) the distribution of predicted outputs t; should follow the distribution of ground truth human text annotations t₁ = TT. Training a fully supervised model to be the generator function G would require a large quantity of human text descriptions. Instead, we explore zero-shot or few-shot approaches to obtain a generator function G. In particular, we describe three approaches that all use the automatically predicted music tags: a prompt2text approach which relies entirely on careful few-shot prompting of a pretrained language model, a zero-shot data2text approach which rephrases templated sentences using pretrained language models, and a zero-shot tags baseline that represents the music track description directly via the set of automatically obtained tags. Details are given next. ~ I. Few-shot prompt2text approach. We first explore whether the full mapping function G can be encompassed by a single large language model through careful few-shot prompting. This approach relies on a small set of example human-provided descriptions to, ..., ty where t; ~ TT. We assume that for each example ti, we also have a paired structured data output d¿, provided by the automatic music tagger, which describes the same audio track. Unlike prior prompt-based data augmentation works [11, 39, 45] which aim for an unconditional generator G, we aim to generate text data ti G(di) conditioned on structured data d¿ such that it follows the distribution of human sentences t¿ Є TT. As shown in Figure 2 (left), the structured data output di is converted to text form via a template, and a set of pairs (do, to), ..., (dk, tk) are used to form k input/output components in the prompt. The final segment of the prompt is the structured data d; corresponding to a new music track. Given di, the model will attempt to output a description ti following the mapping TD → TT suggested by the example inputs. For text generation in this setting, we use the BLOOM-176B [6] model which is trained on a highly diverse 1.5TB text corpus. Given that the prompt2text allows for the greatest freedom in generation, the model can more easily generate a diverse set of text resembling the target distribution Ț. The prompt2text approach is also very flexible as large language models like BLOOM can handle a variety of different structured data inputs such as both tags and their confidence predictions. However, the model may also be less likely to preserve semantic meaning from structured data. II. Zero-shot data2text approach. The second setting that we propose involves a data-to-text generation process which is illustrated in Figure 2 (top right). At a high level, the goal of this method is to insert structured tag data into predefined template sentences and rephrase these template sentences using a language model while preserving original semantic meaning. We begin with the tags predicted for each music track and grouped into genre, mood, and instrument categories. We define a set of category-specific templates in the form of short sentences with placeholders for tags. We randomly sample a template sentence for each category, and fill the template with the high-confidence predicted tags for that category. To form these sentences into more natural free-form descriptions, we make use of pretrained large language models. Specifically, we follow the Zero-shot D2T approach [17], which consists of ordering, aggregation, and compression modules built on pretrained RoBERTa [25] and BART [22] language models. The pipeline components first set the order of the individual filled template sentences and assign which sentences should be combined into a single sentence. Next, the compression module uses a generative text model to rewrite the input sentences based on the ordering and aggregation specifications. The module aims to rephrase the information while preserving semantic meaning. Because this D2T pipeline makes use of models that are pre-trained on large, general text corpuses, we find these modules to perform well at generating music descriptions in a zero-shot manner. III. tags approach. The final setting we use involves a simple concatenation of predicted tags. We take the set of top filtered predicted tags for each music track (this set typically numbers around 10-15 tags total). We then randomly shuffle these tags to prevent model dependence on ordering and concatenate all of the tags into a comma-separated list of musical descriptions (e.g., “synthesizer keyboard, electronic drumset, pop, dance, synth bass, electronic, happy, electric guitar, frantic, dynamic”). While this approach strongly preserves the semantic meaning, it fails to generate text with diverse vocabulary and form which would well represent the human annotation distribution TT. 3.2. Text Dropout for Music Retrieval Training The objective here is to retrieve music track m matching a query video v and a natural language query t describing the target music track. This is a challenging task as the model has to fuse together information from both the input video and the input language query to then find a semantically appropriate music track. Moreover, the difference in granularity between audio/video and text can significantly hinder training. We design a tri-modal approach, dubbed ViML, for this task and introduce text dropout to address the granularity issue. In a similar manner to the way dropout prevents overfitting by reducing co-adaptation between individual neurons [35], text dropout serves to avoid overfitting to the text inputs and prevent co-adaptations between the video and text encoders. The approach is illustrated in Figure 3 and details of model architecture, loss, and text dropout are given next. Model architecture. Our model is trained on a set of (video, music, text) pairings, (v, m, t), corresponding to a CLIP Image Encoder g³ Visual Transformer Input Video CLIP Text Encoder xt Text Transformer Fusion Module fvt Input Text 8 gt Text Dropout Input Music DeepSim Music Encoder gm xm Music Transformer fm Feature similarity s(yut, ym) Figure 3. Our proposed ViML model embeds inputs from three modalities (video, text, and audio) into an embedding space. We extract base features using DeepSim [20] for music input and from CLIP [32] for video frames and text descriptions. These base features are inputted to Transformer encoders for each modality. The video and text features are combined with a fusion module to enable querying of music in a shared embedding space. Finally, we employ text dropout to address the difference in granularity between the three modalities. Since video is a more complex input modality, text dropout forces an improved video representation by preventing co-adaptation of the video and text representations. music video clip v, which has been labeled with a generated text description t of its music track m, as outlined in Section 3.1. We transform these inputs into base features xv g" (v) for visual video features, xm = gm (m) for music features, and xt = gt (t) for text features using pretrained large-scale encoders gº, gm, and gt which are frozen during training. = Each base feature representation x is of dimension n x d, where n is the length of the temporal sequence of base features representing a video clip and d is the dimension of the base feature. We note that while our model is capable of handling a sequence of temporal text descriptions similar to music or video, we obtain only a track-level text description in practice meaning that n = 1 for text. Our tri-modal model consists of three separate modules corresponding to each modality fʊ, fm, ft, and a fourth fusion module fut to combine video and text representations. The modules take respective base features and output embeddings y = f (x²), ym = fm (xm), yt = ft (x²). The fusion model outputs a fused embedding from the video and text embeddings yʊt = fvt (yʊ, y²). Fusion loss. For training, we use an InfoNCE loss [28] between music and fused video-text embeddings: Lvt→m iЄD exp(s(yot, ym)/T) ΣjED exp (s(yt, ym)/T) (1) where s is a similarity function, D is a batch of data, and T is a temperature hyperparameter we set as 7 = 0.03. For our similarity metric, we use the cosine similarity defined as s(x, y) = x²y/(||x|| ||y||). We note that the loss Lut→m is not symmetric as negatives are sampled from music embeddings only. So that our loss is symmetric, we instead train with the summed loss Lm,vt = Lvt→m + Lm→vt. Text dropout. To address difficulties posed by the difference in granularity between audio/video and text, we introduce text dropout as a regularization mechanism. With probability p, we set the input text embedding xt to a specific value x NULL. In practice, we assign this x NULL input as the embedding produced by the pretrained gt model for an empty string. However, using a zero vector as x NULL works similarly. In addition to improving the performance of music retrieval from text and video together, training with text dropout yields a model which also performs well at retrieval from video alone by removing dependence on text inputs. 3.3. Implementation Details Music tag generation. The key first step to our text generation process is obtaining the structured data describing each musical track. To do this, we use a music tagger trained on a dataset of music tracks manually annotated with a fixed pre-defined vocabulary of tags [20]. Specifically, the tagger predicts confidences for 41 instrument tags, 20 genre tags, and 28 mood tags. We aggregate these predictions at the clip or track level, and we filter the subsequent set based on confidence, keeping only those above a particular threshold (0.3 in our experiments). ViML Model. Following MVPt [36], we employ the Transformer architecture [37] for our music and video encoders fm and fv. Transformers play a key role in improving model performance by encoding long-term context from video and music clips. We also use a similar two-layer Transformer architecture for our text encoder ft and the video-text fusion layer fut. However, we find that other fusion module architectures such as a single linear layer yield similar results. Please see our supplemental for study of fusion module architectures. For base features, we use CLIP [32] to encode representations for video frames and text inputs, and DeepSim [20] to encode music. Following communication with the authors of [36], we split the video into 10-second segments and compute a feature for each segment by averaging CLIP embedding features computed at 6 frames per second. We compute 512 dimensional CLIP embedding features using OpenAI's CLIP ViT-B/32 model. We encode all input base features into embeddings of size d 256 using a linear projection layer for each modality. We also select d =as the output dimension for encoded video, text, music, and fused video-text representations from our model. = A faint, simple acoustic piece of singing by a female vocalist with an acoustic guitar with a fast-paced strumming pattern in a closed room recorded live. great for singing along. score. Hip-hop track with a dark Instrumental track featuring A cover version of Elvis synth pad with male aggres- an ambient pad and bell-like sive rapping along with a sounds. Seems to be a film chipmunk voice. Punjabi track with multiple Presley's pop song featuring vocals and other instruments male and female vocals and that's great for traditional piano layers. Sounds like a marriage celebrations. confession in love. Figure 4. Example annotations from our collected YouTube8M-MusicTextClips dataset. Each example shows a frame from the 10sec source video clip from which audio was extracted for annotation. Note that annotators were only provided audio from the music video, so the annotation describes the music, but not the corresponding video. Each example in the figure contains a hyperlink to the corresponding YT8M source video with timestamp at the start of the 10sec target clip. Hover over the video frame image and click to follow the link. 4. Experiments In this section, we report our experimental settings and results. First, we describe our datasets and the evaluation protocol in Sec. 4.1. Next, we investigate tag-based video-to-music retrieval, comparing against state-of-the-art video-to-music retrieval methods in Sec. 4.2. In Sec. 4.3, we evaluate performance of video-to-music retrieval guided by free-form text annotations. Finally, we perform ablation studies to measure the influence of text dropout in Sec. 4.4. 4.1. Datasets and Evaluation Protocol YT8M-Music Video. In all of our experiments, we train models using the YT8M-Music Video dataset which includes around 100k videos with the “music video" tag from the much larger YouTube8M dataset [1]. We synthesize tags and a natural language text describing the music track of each video for the full dataset using the approaches described in Sec. 3.1. We also use the test split of YT8MMusic Video to evaluate tag-based retrieval in Sec. 4.2. YT8M-MusicTextClips. In addition to the full YT8MMusic Video dataset, we also annotate a 4,000 sample subset of clips from YT8M-Music Video with human-provided text descriptions of the music track accompanying each video. To create these annotations, we sample 10 second audio clips from the middle of each music video, and we ask human annotators to describe the music they hear after listening to the audio clip. Thus, an annotation describes only the music from a YT8M sample, and the annotators do not see the corresponding video. Example annotations are shown in Figure 4 with links to the starting timestamp of the 10sec clips in corresponding YouTube videos. This annotated set is meant mainly for evaluation. As a result, the annotations are split into a larger set of 3,000 samples from the test set of YT8M-Music Video and a smaller set of 1,000 samples from the train set of YT8M-Music Video which we use as examples in the few-shot prompt2text synthesis process. We make the annotated text descriptions publicly available at our companion website². Evaluation Set-up and Metrics. We evaluate music retrieval performance consistently with previous works [31, 36]. However, in our case, a query can be either a video alone or a video and corresponding text annotation together. For each query, we compute feature similarity between the query and a pool of N music tracks (we set N=2000 in the track-level setting and N=500 for evaluation on clips). The pool contains a single ground truth music track corresponding to the input query (the positive example) with the remaining music tracks in the pool being non-matching (i.e., negative examples). We rank the music tracks in a query's pool by feature similarity, and find the rank of the query's ground truth matching music track (the positive example). We then compute Recall@K (shortened to R@K) for K=1,5,10 and Median Rank, calculating the average of each of these metrics across the full set of test queries. 4.2. Tag-Based Retrieval For our first set of experiments, we explore the setting of tag-based retrieval. Here the goal is to retrieve a music track given a query video together with a set of tags from a predefined vocabulary, such as "happy", "piano" and "jazz”. This setting could be practically interesting in some applications, e.g., tag-based search. To address this setting, we train our model on text synthesized with the tags approach. In these experiments, we train a track-level model and perform retrieval on a track-level in a manner consistent with prior work [31,36]. To directly compare results with prior work, we perform retrieval on the full YT8M test set consisting of around 10K samples. As shown in Table 1, we include three baselines: the model proposed by Pretét et al. [31], the MVPt model [36], and an improved version of MVPt that we call MVPt+, where we tune the temperature 2https://www.danielbmckee.com/language-guidedmusic-for-video/index.html Method Train Text Query Text Input Median Rank↓↓ R@1↑ R@5 ↑ R@ 10 ↑ a. Pretét et al. [31]0.3.5.b. MVPt [36]6.24.41.c. MVPt+ [36]27.50.60.d. ViML (ours) e. ViML (ours) tags tags29.62.75.tags49.81.89.f. Chance0.0.0.Table 1. Tag-based music retrieval on full YouTube8M-Music Video test set. We compare ViML against prior methods on video to music retrieval without tag queries (row d.). We also evaluate ViML on video+text to music retrieval using (synthetic) tags at test time (row e.). The text descriptions for both training and evaluation are generated with the tags approach for these experiments. Method a. MVPt+ Train Text MR↓ R@1↑ R@5 ↑ R@10 ↑12.29.40.b. ViML tags11.30.42.data2text13.61 33.prompt2text0.14.09 35.1.46.47.2.c. ViML d. ViML Chance Table 2. Music retrieval with free-form natural language on YT8M-MusicTextClips test set. All methods which take text input are evaluated on the human text annotations as queries. Since the MVPt+ model does not take text inputs, it is evaluated on music retrieval from video alone for the same set of 3k video clips. MR is median rank. parameter 7 in the InfoNCE loss to 0.03. This change leads to further significant improvement in performance. Next, we introduce our model (ViML) trained on data generated from the tags approach. We evaluate our ViML model in two settings. First, we evaluate without input texts at test time (an empty text input is used instead). Second, we evaluate with text inputs at test time. As we do not have track-level human-provided music tag annotations for the full YT8M-Music Video split, we evaluate the track-level model on synthetically generated tags using our tags approach. While a model trained on the tags synthesized data may not generalize to out-of-domain free-form text inputs, the tag-based prompting can be a convenient way to guide music retrieval with key desired attributes (for example “female vocalist, guitar, happy”). The tag-based retrieval we report can serve as an upper bound for this type of user tag-guided retrieval since the tag-based text for testing comes from the same music tagger model we used to synthesize training data. Evaluating our model with synthetic tags leads to a very substantial performance increase over MVPt+ of 2030 points in each recall metric. Interestingly, our ViML model evaluated without text at test time not only matches the video-to-music retrieval performance of MVT+ but substantially improves over MVPt+, especially in Recall@and Recall @10. This performance increase is not simply a result of added parameters in the fusion layer, as a fusion module consisting of only a single linear layer yields similar results (see our supplemental for further details). This result suggests that training jointly with the text domain can lead to improvements in the video and audio representations. We hypothesize that the joint training with language helps to disentangle the video-audio space into semantically meaningful dimensions corresponding to the provided tags as well as helps to suppress non-relevant dimensions, e.g., corresponding to presence/absence of some non-relevant objects. 4.3. Free-Form Natural Language Retrieval For the next experiments, we turn to retrieval with freeform natural language inputs. The goal is, given an input video and a query free-form natural language description, to retrieve a relevant music track. For this setting, we evaluate on testing videos from the YT8M-MusicTextClips dataset which contains free-form human text annotations describing the music corresponding to each video in the dataset. In these experiments, we use a similar protocol to the "segment-level" setting reported by Surís et al. [36], but our input video includes only a 30sec clip surrounding the 10sec of audio labeled by a human annotator. In contrast, a model had access to a large context spanning the full source video in the previous segment-level setting reported by Surís et al. [36]. We note that retrieval in this setting is significantly more difficult than the segment-level setting in [36] or the track-level setting reported in 4.2 due to the limited context. However, such retrieval is of particular interest given the rise of short-form video in social media and entertainment. Results are summarized in Table 2. Our baseline is an MVPt+ model which has been trained on 30sec segments (training MVPt+ on full videos and testing on 30sec clips causes a much more severe drop in performance). We next report music retrieval using video and free-form human text descriptions as input queries to our ViML model. In Table 2, we report three variants trained on YT8M music videos with text synthesized by each of the three approaches described in Sec. 3.1. The model trained with our first tags synMethod Train Text Dropout Text Inputs Median Rank↓ R@1↑ R@5↑ R@ 10 ↑ a. MVPt+12.20 29.40.b. ViML prompt 2text9.26.37.c. ViML prompt2text human11.45 30.42.d. ViML prompt2text12.27 30.41.e. ViML prompt2text human14.35.47.Table 3. Influence of training with text dropout on retrieval performance. Evaluated on the YT8M-MusicTextClips test set. thesis baseline (b.) provides substantial improvement over retrieval with MVPt+ using only video (a.). Next, we evaluate the data2text approach (c.) which generates more natural phrases while strictly preserving tag semantics. This approach provides a consistent improvement over the ViML tags variant (b.). Finally, our prompt2text approach (d.) leads to the best performance showing that large language models prove to be strong annotators on this task with careful few-shot prompting. Qualitative results. In Figure 5, we provide qualitative retrieval results for examples in YouTube8MMusic TextClips. In the first example, both models retrieve tracks that match the style and beat of the input video well. However, only the ViML can match the correct musical style by using the input text. In the second example, only the ViML result correctly matches the desired music genre and the mood of the video. 4.4. Analysis of Text Dropout In Table 3, we compare the performance of our ViML model trained on prompt2text descriptions with and without text dropout. We evaluate this model on music retrieval in two settings: (i) using only video (inputting empty text, rows b. and d.) as a query and (ii) using both video and human text descriptions together as a query (rows c. and e.). As expected, adding text dropout during training (d.) improves the performance of retrieval using only video (b.). However, interestingly, text dropout also substantially improves performance when the query includes natural language (e. vs. c.), suggesting that text dropout is a very useful regularization technique in the multimodal setting. We find that without text dropout, training begins to plateau early as the model starts overfitting to the training text inputs. Since video is a much richer and more complex modality, forcing more attention to this modality during training improves learning. We find that the dropout technique is most effective at high rates of dropout in the range 0.8-0.95, and we use a dropout rate of 0.8 in all of our experiments. 5. Conclusion In this work, we introduced an approach to allow language-guided music recommendation for video. We proVideo+Language Query Retrieved Music MVPt+ ViML Electronic dance pop track that has male vocals and a pulsating rhythm. Best for nightclubs. Male rap song in a foreign language with hip-hop beats and electronic music that gives a freestyle vibe to it. Percussive Latin music with a male vocalist, synth line, and Cumbia beat aligned with video. Upbeat jazzy melody played on a saxophone over rhythmic electronic bass beats. Energetic electronic party song with male vocals over rising synth layers and claps aligned with video. French rap song with processed male vocals, thumping drum beats, and synth pad effects. Figure 5. Qualitative results on YouTube8M-MusicTextClips test set. We compare music retrieval quality for two examples using the MVPt+ model and our ViML model. The column on the left includes a frame from the input video and the input text description describing the target music. The MVPt+ model takes only the video as an input while the ViML model takes both video and corresponding text. The two columns on the right contain retrieved music for MVPt+ and ViML respectively. Please see results in the companion video on our website. posed a model, ViML, which fuses text and video inputs to find music matching both domains and introduced the text dropout technique to improve training. To obtain data for training, we proposed a free-form music description synthesis approach using a large language model (BLOOM176B) and outputs from a pretrained music tagger. Our results show that large language models provide a powerful tool for training data synthesis in domains where text data is limited but other structured data is available. To evaluate our method, we also introduced a new dataset, YouTube8MMusic TextClips, which includes high quality free-form human descriptions of the music in YT8M videos. There are many exciting directions to build upon this work including allowing more fine-grained control over specific music attributes or language-guided audio-video generation. References [1] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A largescale video classification benchmark. arXiv preprint arXiv:1609.08675, 2016. 2,[2] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in Neural Information Processing Systems, 34:24206-24221, 2021.[3] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelović, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Selfsupervised multimodal versatile networks. Advances in Neural Information Processing Systems, 33:25-37, 2020.[4] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. See, hear, and read: Deep aligned representations. arXiv preprint arXiv:1706.00932, 2017.[5] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The million song dataset. In Proceedings of the 12th International Society for Music Information Retrieval Conference, 2011.[6] BigScience. Bigscience large open-science open-access multilingual language model, 2022. "https:// huggingface.co/bigscience/bloom".[7] Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, and Xavier Serra. The mtg-jamendo dataset for automatic music tagging. In Machine Learning for Music Discovery Workshop, International Conference on Machine Learning (ICML 2019), Long Beach, CA, United States, 2019.[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.[9] Jeong Choi, Jongpil Lee, Jiyoung Park, and Juhan Nam. Zero-shot learning for audio-based music classification and tagging. In International Society for Music Information Retrieval Conference, 2019.[10] Keunwoo Choi, George Fazekas, and Mark Sandler. Automatic tagging using deep convolutional neural networks. In Proceedings of the 12th International Society for Music Information Retrieval Conference, 2016.[11] Bosheng Ding, Linlin Liu, Lidong Bing, Canasai Kruengkrai, Thien Hai Nguyen, Shafiq Joty, Luo Si, and Chunyan Miao. DAGA: Data augmentation with a generation approach for low-resource tagging tasks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6045-6057, Online, Nov. 2020. Association for Computational Linguistics. 3,[12] Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas Dengel. Audioclip: Extending clip to image, text and audio. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 976-980. IEEE, 2022.[13] Sungeun Hong, Woobin Im, and Hyun Seung Yang. Cbvmr: Content-based video-music retrieval using soft intra-modal structure constraint. Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval, 2017.[14] Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel P. W. Ellis. Mulan: A joint embedding of music audio and natural language. In International Society for Music Information Retrieval Conference, 2022.[15] Wenlong Huang, F. Xia, Ted Xiao, Harris Chan, Jacky Liang, Peter R. Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In Conference on Robot Learning, 2022.[16] Ahmed Hussen Abdelaziz, Barry-John Theobald, Paul Dixon, Reinhard Knothe, Nicholas Apostoloff, and Sachin Kajareker. Modality dropout for improved performancedriven talking faces. In Proceedings of the 2020 International Conference on Multimodal Interaction, pages 378386, 2020.[17] Zdeněk Kasner and Ondřej Dušek. Neural pipeline for zeroshot data-to-text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3914–3932, 2022. 3,[18] Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, 2022.[19] Edith Law, Kris West, Michael I Mandel, Mert Bay, and J Stephen Downie. Evaluation of algorithms using games: The case of music tagging. In ISMIR, pages 387–392, 2009.[20] Jongpil Lee, Nicholas J. Bryan, Justin Salamon, Zeyu Jin, and Juhan Nam. Disentangled multidimensional metric learning for music similarity. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020. 2,[21] Jongpil Lee, Jiyoung Park, Keunhyoung Kim, and Juhan Nam. Sample-level deep convolutional neural networks for music auto-tagging using raw waveforms. In Proceedings of the 14th Sound and Music Computing Conference, 2017.[22] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdel rahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. In Annual Meeting of the Association for Computational Linguistics, 2019.[23] Bochen Li and Aparna Kumar. Query by video: Cross-modal music retrieval. In ISMIR, pages 604–611, 2019.[24] Xudong Lin, Fabio Petroni, Gedas Bertasius, Marcus Rohrbach, Shih-Fu Chang, and Lorenzo Torresani. Learning to recognize procedural activities with distant supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13853–13863, 2022.[25] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.[26] Ilaria Manco, Emmanouil Benetos, Elio Quinton, and György Fazekas. Contrastive audio-language learning for music. In International Society for Music Information Retrieval Conference, 2022.[27] Natalia Neverova, Christian Wolf, Graham Taylor, and Florian Nebout. Moddrop: adaptive multi-modal gesture recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(8):1692-1706, 2015.[28] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.[29] Sergio Oramas, Oriol Nieto, Francesco Barbieri, and Xavier Serra. Multi-label music genre classification from audio, text and images using deep features. In International Society for Music Information Retrieval Conference, 2017.[30] Jordi Pons and Xavier Serra. musicnn: Pre-trained convolutional neural networks for music audio tagging. arXiv preprint arXiv: 1909.06654, 2019.[31] Laure Prétet, Gael Richard, and Geoffroy Peeters. Crossmodal music-video recommendation: A study of design choices. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1–9. IEEE, 2021. 1, 2, 6,[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 2,[33] Andrew Rouditchenko, Angie Boggust, David F. Harwath, Dhiraj Joshi, Samuel Thomas, Kartik Audhkhasi, Rogério Schmidt Feris, Brian Kingsbury, Michael Picheny, Antonio Torralba, and James R. Glass. Avlnet: Learning audio-visual language representations from instructional videos. In Interspeech, 2020.[34] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating situated robot task plans using large language models. In International Conference on Robotics and Automation (ICRA), 2023.[35] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929–1958, 2014.[36] Dídac Surís, Carl Vondrick, Bryan Russell, and Justin Salamon. It's time for artistic correspondence in music and video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10564-10574, 2022. 1, 2, 5, 6,[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2,[38] Weiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classification networks hard? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12695-12705, 2020.[39] Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, and Daxin Jiang. PromDA: Promptbased data augmentation for low-resource NLU tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4242-4255, Dublin, Ireland, May 2022. Association for Computational Linguistics. 3,[40] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022.[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2482424837. Curran Associates, Inc., 2022.[42] Minz Won, Keunwoo Choi, and Xavier Serra. Semisupervised music tagging transformer. In Proc. of International Society for Music Information Retrieval, 2021.[43] Minz Won, Andres Ferraro, Dmitry Bogdanov, and Xavier Serra. Evaluation of cnn-based automatic music tagging models. In Proc. of 17th Sound and Music Computing, 2020.[44] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip: Learning robust audio representations from clip. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4563-4567. IEEE, 2022.[45] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. Generative data augmentation for commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1008-1025, Online, Nov. 2020. Association for Computational Linguistics. 3,[46] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16375–16387, 2022.[47] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv: 2204.00598, 2022.[48] Donghuo Zeng, Yi Yu, and Keizo Oyama. Audio-visual embedding for cross-modal music video retrieval through supervised deep cca. In 2018 IEEE International Symposium on Multimedia (ISM), pages 143–150. IEEE, 2018.A. Fusion Module Architecture Study Appendix We evaluate different architectures for the fusion module that is responsible for combining encoded visual and text inputs. In Table 4, we present five architecture variants. First, we benchmark fusion by direct addition of the visual and text Transformer encoder outputs (a.) which removes learned parameters from the fusion module entirely. Next, we evaluate three different learned fusion module architectures which involve passing the concatenated visual and text features as input to: (b.) a single linear layer (c.) a two-layer MLP, (d.) a 1-layer Transformer network, and (e.) a 2-layer Transformer network. We find that the size of the fusion module does not significantly change perforImance. We use the 2-layer Transformer fusion architecture in our main results given the slightly higher performance in recall metrics, but similar performance can be achieved with the other fusion architectures including the “addition" fusion module which does not include learned parameters. B. Training without Video We also experimented with models trained only on music and text but found these models to significantly underperform other baselines at music retrieval on the YT8MMusic TextClips test set. This is not surprising as the input video contains a great deal more information than the short human text descriptions in the dataset. Performance of our music+text model trained on prompt2text data and evaluated on human texts in the YT8M-Music TextClips test set was Recall @1/5/10=2.52/9.27/15.52 and MR=56 (compare to Table 2 results from the main paper). We report the results of our track-level music+text model trained with tag inputs as MT in Table 5 (a.) (compare to Table 1 results from the main paper). This model also performed substantially below MVPt+ or ViML. C. Ensembled Models In addition to the baselines reported in Table 1 of the main text, we also investigated forming a stronger baseline by combining MVPt+ and the music+text model from Appendix B into an ensemble. More specifically, for a music track m and a corresponding video, text pair (v, t), we compute the total similarity score as a weighted sum (1 − a) · s(y³‚ym)+a·s(zt, zm) where y", ym are the video and music embeddings generated by MVPt+, zt, 2m are the text and music embeddings generated by our music+text model, and a is a coefficient which we tuned. As shown in Table 5 (d.), we found this music+text model and MVPt+ ensemble to reach strong performance, exceeding Recall@1 performance of ViML and achieving similar Recall @5/10 ViML performance. However, we found that such ensembling could be used to improve the performance of ViML as well. In particular, computing scores for music retrieval as a weighted sum of similarity scores from ViML and MVPt+ led to substantial improvements over ViML performance as shown in Table 5 (e.). An ensemble of ViML and our music+text model led to the highest performance in Table 5 (f.). D. Music Matching the Pace of Videos In our qualitative results, we did not observe many examples where the music beats per minute (BPM) does not match the video pace. We hypothesize that a given music genre lives in a limited tempo range. Therefore, being able to match effectively the music genre may return a wellmatching tempo for free. Note that we do not have finegrained tempo alignment, e.g., depicted dance motions may not be perfectly in sync with the music. One possible future direction could be to refine the alignment between the music and depicted action in the video. E. Text Synthesis Examples Outputs In Figures 6 and 7, we present generated outputs from our text synthesis approaches along with real human annotations for randomly selected examples from the YouTube8M-Music Video dataset. The text synthesis approaches show different tradeoffs between tag accuracy and diversity of form/language. The prompt2text setting is the most free-form text synthesis approach but will sometimes generate outputs which are not true to the original tag predictions for a track. In general, the prompt2text descriptions tend to be shorter and often omit information in the input tags. The language model used in the prompt2text approach can also sometimes hallucinate information which is completely wrong (e.g. "female vocal and a piano section" in Fig. 6 ex. 3). However, the diversity of vocabulary and structure in the outputs produced by prompt2text makes this approach most similar to real human annotations. Method a. Addition # Parameters Median Rank↓ R@1↑ R@5 ↑ R@ 10 ↑13.34.46.b. Linear 131K13.33.45.c. MLP 1.6M13.32.44.d. Transformer (1 layer) 1.4M13.34.46.e. Transformer (2 layer) 2.8M14.09 35.47.Table 4. Study of fusion layer architecture. All models are trained on the synthesized prompt2text data, and we report results on the YT8M-Music TextClips 3k test set. Method Train Text Query Text Input Median Rank↓↓ R@1↑ R@5 ↑ R@10↑ a. MT tags tags11.30.42.b. MVPt+ [37]27.93 50.60.c. ViML (ours) tags tags49.81.89.d. MT & MVPt+ Ens. tags tags55.81.88.e. ViML & MVPt+ Ens. tags tags59.85.91.f. ViML & MT Ens. tags tags63.91.96.g. Chance0.0.0.Table 5. Tag-based music retrieval on full YouTube8M-Music Video test set for music+text model and model ensembles. For convenient comparison, we also report the MVPt+ and ViML results from Table 2 in the main text. We denote the music+text model described in Sec. B as MT in the table. 1) 2) 3) 4) https://youtu.be/xcZIwXABBKA?t=TAGS: rock, lead vocals, electric bass, frantic, powerful, vocals, electric guitar, angry, slow, male vocals, acoustic drumset DATA2TEXT: The soundtrack is rock with a mood of powerful, slow, and frantic. Acoustic drums, male vocals, vocals, lead vocals, and electric guitar are playing in this song. PROMPT2TEXT: Rock song with electric guitars, electric bass, drums, and male vocals. The track has a powerful and angry vibe. HUMAN: Alternative hard rock song with grungy male vocals, heavy drumming, guitar riffs and bass conveying high energy. https://youtu.be/A_j0jhJnHmA?t=TAGS: dance, synthesizer keyboard, hip-hop, male vocals, sad, electronic, electronic drumset, vocals, synth bass DATA2TEXT: This music is hip-hop, electronic, and dance with vocals, synthesizer keyboard, electronic drumset, and male vocals. PROMPT2TEXT: Hip-hop instrumental with a male vocal. Great for rap music. HUMAN: Rebellious hip-hop track with electronic music and impactful male vocals. https://youtu.be/ZUyZP9AS01c?t=TAGS: pop, hip-hop, synth bass, happy, dance, vocals, electronic, sad, male vocals, synthesizer keyboard, lead vocals, electronic drumset, dreamy DATA2TEXT: This song is electronic and hip-hop, with synthesizer keyboard, electronic drumset, male vocals, synth bass, vocals, and lead vocals. The feeling is happy, dreamy, and sad. PROMPT2TEXT: Hip-hop track with a female vocal and a piano section. Great for a sad scene. HUMAN: A jivey male led track with electronic music and groovy trap beats making the song a dance retreat. https://youtu.be/Z1UhPHPjE10?t=TAGS: vocals, synthesizer keyboard, male vocals, synth bass, happy, lead vocals, sad, hip-hop, angry, electronic drumset DATA2TEXT: This song sounds sad, but is a song with electronic drumset, male vocals, synthesizer keyboard, and synth bass. PROMPT2TEXT: Hip-hop track with a slow beat, male vocals, and a synth bass. The track has a sad and melancholic feel. HUMAN: Foreign heavy rap with low bass effects and trap synth line. Groovy jamming with friends feeling. https://youtu.be/kXPxUMv-S9I?t=TAGS: electric guitar, vocals, slow, rock, dance, lead vocals, electric bass, blues, frantic, electronic, acoustic drumset, male vocals, quirky, angry, 5) powerful 6) 7) 8) 9) DATA2TEXT: This is blues and electronic music with the sound of vocals, male vocals, acoustic drumset, lead vocals, and electric bass giving an angry, powerful, and frantic feel. PROMPT2TEXT: A rock song with a bluesy feel, with electric guitar and drums. HUMAN: Grunge rock rebel kind of music like Greenday. A male vocal with distorted guitars, bass and drums. https://youtu.be/enHWyaXrcfl?t=TAGS: hip-hop, electronic drumset, electronic, sad, synthesizer keyboard, male vocals, lead vocals, dance, synth bass, vocals DATA2TEXT: The music sounds like hip-hop, electronic, and dance, and sad, including synthesizer keyboard, electronic drumset, male vocals, synth bass, and lead vocals. PROMPT2TEXT: Hip-hop track with male vocal, electronic drumset, synthesizer keyboard and bass. HUMAN: Trap song with male rapper, very piercing hi-hat beats and bass line that sounds indulgent and addictive. https://youtu.be/kE2wuQT4J14?t=TAGS: electric bass, blues, dynamic, acoustic drumset, happy, vocals, male vocals, frantic, electric guitar, rock DATA2TEXT: This is blues and rock music featuring acoustic drums, electric bass, vocals, male vocals, and electric guitar with a dynamic, happy, and frantic feeling. PROMPT2TEXT: Blues rock track with a male vocalist. The song is happy and has a strong electric guitar and electric bass. HUMAN: Soulful rock track with male vocals backed by synth layers, over driven electric guitars and drums. The song has a relaxing note. https://youtu.be/2dFMqtk1ieM?t=TAGS: synthesizer keyboard, hip-hop, vocals, pop, electronic drumset, happy, dreamy, dance, electronic, male vocals, lead vocals DATA2TEXT: This song is electronic, dance, and hip-hop, sounding dreamy. The instrumentation includes male vocals and an electronic drumset. PROMPT2TEXT: A dreamy pop track with a strong bass line and a catchy melody. Best for commercials, advertisements, and videos. HUMAN: Mid tempo low sounding hip hop track with rap male vocal and drum groove. https://youtu.be/PKJ7p_8tj1c?t=TAGS: synth bass, synthesizer keyboard, electronic, electronic drumset, vocals, happy, female vocals, slow, powerful, frantic, male vocals, dance, lead vocals DATA2TEXT: The music gives a slow, happy, and powerful feel with synthesizer keyboards, electronic drumset, vocals, male vocals, and female vocals playing. PROMPT2TEXT: A dance track with a strong beat and a lot of synth sound effects. Ideal for a party. HUMAN: Electropop track with strong bass and impactful electronic beats. Great for disco. Figure 6. Synthesized text examples using our approach. We randomly select examples from the YouTube8M-MusicTextClips test set. We show the output from our tags, data2text, and prompt2text approaches for each example video. We also show real human annotations collected for each example. The text synthesis approaches show tradeoffs between preserving tag accuracy and increasing diversity of vocabulary and phrase structure. The prompt2text approach generates outputs with the highest diversity and most closely resembling human annotations. Additional examples shown in Figure 7. 10) 11)12)13) 14) 15) 16) 17) 18) https://youtu.be/QJIw6v5uuYc?t=TAGS: vocals, synth bass, electronic, female vocals, frantic, lead vocals, pop, synthesizer keyboard, dance, happy, electronic drumset, male vocals DATA2TEXT: This sounds like dance and electronic music with a happy sound. The instrumentation includes male vocals, lead vocals, electronic drumset, vocals, female vocals, synthesizer keyboard, and synth bass. PROMPT2TEXT: EDM Pop track with a synth pad layer, heavy bass drops and beats. Sounds like a party track. HUMAN: Melodious pop song with rhythm guitars, drums and bass. The track adds a happy vibe to the setting. https://youtu.be/w3RzK9PEуHs?t=TAGS: electronic, synth bass, vocals, hip-hop, male vocals, lead vocals, dance, happy, synthesizer keyboard, electronic drumset DATA2TEXT: This song is happy. Electronic drumset, lead vocals, synth bass, vocals, male vocals, and synthesizer keyboard are playing in this song. PROMPT2TEXT: Hip-hop music with electronic drums, bass, and synthesizer. HUMAN: Impactful freestyle male rap with backing prominent synth lines, guitars and drums. https://youtu.be/Jkb-MeVp4cY?t=TAGS: electric guitar, dreamy, dance, synth bass, hip-hop, lead vocals, electronic, pop, synthesizer keyboard, electronic drumset, vocals, happy DATA2TEXT: This is some electronic, dance, and pop music with a happy sound. The soundtrack has synth bass, vocals, synthesizer keyboard, electric guitar, electronic drumset, and lead vocals. PROMPT2TEXT: A very upbeat track with a lot of energy. Great for a party or a nightclub. Has a lot of synth bass and a lot of synths. HUMAN: A classical coordinated track with acoustic guitar, wind chimes, soft rock drum beats, and a male artist in a foreign language. Very nostalgic and elegant piece. https://youtu.be/Jvjgq9LpDZA?t=TAGS: electronic drumset, sad, lead vocals, hip-hop, happy, synthesizer keyboard, vocals, synth bass, male vocals DATA2TEXT: This sounds like hip-hop music and sad, including male vocals, synthesizer keyboard, vocals, lead vocals, and electronic drumset. PROMPT2TEXT: Hip-hop song with male vocals, synth, drum, bass and an impactful drop. HUMAN: Dark rap song with male vocals and electronically produced layers. https://youtu.be/7SzppacIY1M?t=TAGS: dance, hip-hop, synthesizer keyboard, electronic drumset, sad, vocals, electronic, synth bass, lead vocals DATA2TEXT: This is some hip-hop and electronic music featuring an electronic drumset and synth bass. The feeling is sad. PROMPT2TEXT: Hip-hop track with male lead, bass, drums and synths. The song is a stress buster. HUMAN: Hip-hop track with with syncopated beats and synth sound effects that sounds like a Jay-Z song. https://youtu.be/y2GXHr7P3D0?t=TAGS: synth bass, electronic, novelty, angry, synthesizer keyboard, dynamic, electronic drumset, dance DATA2TEXT: This is a song with synth bass and electronic drumset. The mood is dynamic, dance, novelty, and electronic. PROMPT2TEXT: A dance track with a robotic female vocal and a synth bass line. The track is perfect for a club. HUMAN: Death-metal track with chaotic music that sounds like guns being fired. https://youtu.be/CzVcyff_gc4?t=TAGS: vocals, dance, synth bass, electronic drumset, male vocals, electronic, happy, pop, synthesizer keyboard, lead vocals DATA2TEXT: The genre of this music is electronic, sounds happy, electronic drumset, synthesizer keyboard, lead vocals, synth bass, and male vocals playing in this song. PROMPT2TEXT: A happy and upbeat dance track with a catchy melody and a strong beat. The vocals are sung in an unfamiliar language. HUMAN: Lounge chill hip hop music with rap male vocals, drum and bass. https://youtu.be/u6KD0-jBfx4?t=TAGS: dance, female vocals, lead vocals, happy, synth bass, electronic drumset, vocals, pop, synthesizer keyboard, electronic, dynamic DATA2TEXT: The music is dynamic and happy with a pop, electronic, and dance feel, featuring synthesizer keyboard, lead vocals, synth bass, and female vocals. PROMPT2TEXT: A dance track with a strong beat, a catchy melody and a lot of energy. HUMAN: Very passionate love ballad track with sensual female vocals, keys, guitar, drum and bass. https://youtu.be/qwCWz4BFNuk?t=TAGS: relaxing, male vocals, lead vocals, dreamy, nostalgic, acoustic guitar, sad, vocals, piano, acoustic DATA2TEXT: This music sounds dreamy and nostalgic, including acoustic guitar, lead vocals, male vocals, and piano. PROMPT2TEXT: Acoustic ballad with female vocals and piano. HUMAN: A sorrowful pop song with a melancholic melody. The passionate female vocals add a feeling of separation and longingness. Figure 7. Synthesized text examples using our approach. Continued from Figure 6.