--- --arXiv:2305.11337v1 [cs.CV] 18 MayRoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture Liangchen Song?”, Liangliang Cao!, Hongyu Xu Kai Kang!, Feng Tang!, Junsong Yuan’, Yang Zhao! ! Apple Inc. Input: Scanned Room : Dreamed Room ? University at Buffalo lsong8@buffalo.edu llcao@apple.com Figure 1. Our method aims at jointly improving geometry and generating texture for an input indoor mesh. The upper figure shows the true room with a panoramic view and a depth map. Then, given a text prompt (in the middle), our model can synthesize new rooms with different styles (in the bottom rows). Note that input mesh is often of low quality, and our method can polish both the texture and geometry. Abstract The techniques for 3D indoor scene capturing are widely used, but the meshes produced leave much to be desired. In this paper, we propose “RoomDreamer”, which leverages powerful natural language to synthesize a new room with a different style. Unlike existing image synthesis methods, our work addresses the challenge of synthesizing both geometry and texture aligned to the input scene structure and prompt simultaneously. The key insight is that a scene should be treated as a whole, taking into account both scene texture and geometry. The proposed framework consists of two significant components: Geometry Guided Diffusion and Mesh Optimization. Geometry Guided Diffusion for 3D Scene guarantees the consistency of the scene style by applying the 2D prior to the entire scene simultaneously. Mesh Optimization improves the geometry and texture jointly and eliminates the artifacts in the scanned scene. To validate the proposed method, real indoor scenes scanned with smartPhones are used for extensive experiments, through which the effectiveness of our method is demonstrated. 1. Introduction Commercial depth sensors [42] and LiDAR sensors [9] on mobile devices have opened a new era in 3D scene capturing for millions of users in their everyday lives. However, the quality of the meshes acquired by these sensors leaves much to be desired, often exhibiting issues such as holes, distorted objects, and blurred textures. In addition, users typically find their surroundings lack variation and may want to further edit and stylize the scene. To solve Video results: https://youtu.be/p4xgwj4QJcQ. --- --these problems, this paper demonstrates how to build a 3D scene from text prompts that matches the geometry of a lowquality 3D mesh but differs in style. Our method is motivated by recent advances in 2D content generation, especially the diffusion models [16, 29, 32, 33,40]. One benefit of diffusion models is to allow userprovided text prompts to guide the image synthesis process, and hence is versatile to generate different styles. One straightforward way of extending 2D content generation to a higher dimensional space is treating the 3D scene as a collection of multiview images, and synthesizing the images in a frame-by-frame outpainting manner. However, this approach will suffer from artifacts, and the generated images may not match the geometry of the captured scenes. Given a 3D scene and a text prompt like *modern style”, our work can generate a new scene aligned to the text with coherent geometry and texture. Our approach involves first generating the 3D scene’s texture, followed by the joint optimization of the mesh texture and geometry. We ensure that the generated texture is consistent with the scene’s style by starting with a cubemap (a 360° image) at the center of the mesh and then updating the unexplored areas. For the joint optimization of mesh texture and geometry, we propose to identify smooth areas within the generated 2D images and update the mesh geometry accordingly. Fig. 1 shows the results of our approach. Our method differs from previous work in the creation of 3D objects from text [20, 25,36] and the generation of 3D content based on 2D images [10, 14,22, 25] in two key aspects. Firstly, we consider a novel and practical setting, as it assumes the presence of a scanned scene, which is common yet largely unexplored. In our distinct setting, we aim to refine existing geometry, as opposed to the previous techniques, which primarily focus on generating new geometry. Secondly, our approach is motivated by a different insight into 2D diffusion models. Our motivation is on the good underlying geometry behind each generated 2D image, whereas previous methods are motivated by generating a set of multi-view 2D images iteratively through diffusion. Note, we can easily project a mesh-based representation to 2D images, while it is much harder to refine the mesh geometry from 2D inputs. Extensive experiments demonstrate that our approach is accurate and flexible to use in many real applications. To sum up, the contributions of this work are threefolded: ¢ We introduce a novel framework that employs 2D diffusion models to edit a given mesh. Our framework facilitates the editing and stylization of both geometry and texture based on textual prompts. ¢ We design a 2D diffusion scheme for controlling the diffusion models, leading to the production of a scene consistent and structurally aligned texture for the input mesh. * We conduct extensive experiments using real indoor meshes scanned with smartphones, which verify the effectiveness and reliability of our framework. 2. Related Works The domain of 3D content creation [16, 32, 33] has significantly improved in recent years. We consider research in this field into two categories. Firstly, using 3D ground truth content for supervision to direct content generation process [5, 13, 23], which is limited due to the availability of high-quality 3D ground truth. The second research category focuses on using the power of existing 2D image generators [29] for 3D content creation. Poole et al. [25] proposed Score Distillation Sampling (SDS) to use the structure of the diffusion model, providing supervisory signals to a 3D neural field. Concurrently, Wang et al. [36] proposed Score Jacobian Chaining to lift pretrained 2D diffusion models for 3D generation. Lin et al. [20] presented Magic3D, which represents 3D content first through neural fields and then meshes to improve the quality and efficiency of 2D diffusion-guided generation. Fantasia3D [7] proposed to decompose the 3D asset generation as geometry and texture generation problems. SDS has also been applied to convert existing 2D images into 3D models [38,43]. Liu et al. [21] proposed adapting existing 2D diffusion models to be camera pose-aware, enabling the direct generation of multi-view images. Chan et al. [6] proposes synthesizing a novel view from a single input by incorporating geometry priors with stable diffusion backbones. Another recent work, Text2Room [17], uses 2D diffusion models and depth estimation models to generate a textured room mesh from text prompts. The biggest difference between our method and the above works is that our method is guided by a scanned mesh, therefore the newly generated 3D scenes will be accurately aligned with the input scene but with different styles. For editing existing 3D assets, InstructN2N [14] proposed a methodology to update 2D multiview images iteratively. This was based on a 2D image editing model known as InstructP2P [4]. InstructN2N performed the editing on 2D images, which meant that the ability to dream entirely new scenes may have been restricted by image-based editing. On the other hand, our approach relies on a geometrycontrolled 2D diffusion generation, which implies that it is not hindered by the texture of the original scene. One big challenge of 3D data collection lies in the imperfect scene scanning results. Because the Lidar on mobile devices is of limited power and resolution, some parts of the scenes are often missed in the point clouds. There have been quite a few researches to improve 3D reconstructions with generative priors, such as self-supervised genera --- --tion [8], retrieval-based generation [30], style transfer [18]. Besides the reconstruction problem, some prior research has treated 3D indoor scene generation as an object layout prediction problem [24, 28,37]. After predicting the layout, objects are retrieved from a 3D furniture dataset such as 3D-FRONT [12] and placed within the scene. Other approaches, such as Plan2Scene [35], use the floorplan and image observations of an indoor scene to predict a textured mesh for the entire scene. Meanwhile, GSN [11], GAUDI [3], and CC3D [1] focus on generating images of indoor scenes through the use of neural radiance fields. These research works are inspiring to our work. In practice, we focus on refining indoor scene meshes, especially the smoothness of 3D geometry and the match of geometry and visual textures. We will explain more details in later sections. 3. Method Our approach’s input includes a 3D mesh with both geometric and texture information, as well as user-provided text prompts. Our method is composed of two steps: First, we render the 3D scene to 2D images, and synthesize new styles using a new diffusion process; Then we reconstruct a new 3D mesh with the new textures and polished geometry. An overview of our method is shown in Fig. 2. 3.1. Geometry Guided Diffusion for 3D Scene Synthesizing a new 3D scene is more challenging than synthesizing a 2D image because standard diffusion models [33] can easily create inconsistency across different views. A straightforward approach for generating scene texture using 2D image diffusion models begins with a randomly positioned camera and iteratively samples neighboring cameras to outpaint [31,39] the unobserved area, as depicted in Fig. 3. However, we have observed that this baseline method produces noticeable artifacts (Fig. 6), which can be attributed in part to the limited outpainting capability of 2D diffusion models. To avoid the artifacts brought by the view-by-view outpainting generation process, we propose to first generate a 360° image with a central view of the scene. A panorama image can be generated with 2D diffusion models by simply extending the diffusion process to cubemap patches [2,41]. Unlike the classic diffusion model which is conditioned on text prompts, our method is conditioned on both a text prompt Ctext and a depth map D, thus the diffusion step is Xt-1 = f (Xz, Ctext, D). Following the previous work, we denote the diffusion model as a mapping function denoted as f : (REXW x3, C) > RExWx3 where R#*W x3 represent the spaces for images with size H x W, and C represent the spaces of conditional prior including both prompt and image depth. Further, we denote Xo = f7°(Ctext, D) as the whole diffusion process from random noise and conditioned on the text and depth. However, directly using depth map for controlling the generation of cubemap may lead to inconsistency, since the depth value is correlated with camera poses. Different camera poses lead to inconsistency of the depth value in cubemap faces. Fig. 5(a) illustrates the inconsistency of depth map. The depth map associated with each camera is represented in terms of the distance between the camera and the planes. Consequently, the depth values can largely differ from one view to another for the same plane, and lead to artifacts. To further reduce the inconsistency, we consider distance map D which represents the geometric distance between points and the camera origin. Let a point with world coordinate p and its associated screen coordinate be (u,v), then the (u,v) pixel on the distance map D is ||p — oll, where o is the world coordinate of the camera origin. A comparison between the depth map and the distance map is shown in Fig. 4. Distance map D and depth map D have different properties in terms of controlling the diffusion process. Structures are well aligned with RGB images in D, but distorted in D. For example, image Laplacian on the planes will be zero in D, but not in D. However, the border in cubemap will with a smoother change with D, which could be observed in Fig. 2(b). To achieve both realistic geometric alignment and border consistency, we propose a blending scheme. For an image patch p at the intersection of cube maps J, and Jy, let ra and r, be the ratios of pixels from J, and J, in the patch, respectively. We then define \ = |rq — |. Each step of the denoising process is calculated using the equation: Xp1 = AF (Xt, Ctexts Dp) + (1—A) Ff (Xt, Ctexts Dp), qd) where D, and Dy are the depth and distance maps respectively for the patch p being denoised. After generating a cubemap, the mesh texture is subsequently updated using a differentiable renderer [19]. We then randomly sample cameras in the scene, and the areas not captured by the 360° image are updated through masked generation (i.e., outpainting) with the diffusion model. For judging areas captured or not by the cubemap, we can simply project the vertices to the previous cameras and see which vertices are occluded. 3.2. Mesh Optimization Both the input and output of RoomDreamer are 3D meshes. We denote a mesh as (V, F, V..), where V, F,, V.. are the vertices, faces, and the color of vertices, respectively. For a specific camera 7, we can render a depth map D and RGB image X at this view: X =Rx(V,FVe,7) Q) D =R0(V, Fn) 3) --- --j input geometry. ba » A se. \ Peer ‘* = \s = sample|cubemap-camera sample random}camera eometr eeuided y + prompt "cozy wooden cabin..." ————— Liexture input texture input geometry texture Lgcometry monocular depth estimator [iti] —_> ———~x~ pseudo depth Geometry Guided Diffusion for 3D Scene Mesh Optimization Figure 2. The overall framework of our method. Firstly, in the Geometry Guided Diffusion stage for 3D scenes, we create a cubemap representing the scene, followed by outpainting the uncovered areas of the cubemap, as detailed in Sec. 3.1. Subsequently, we optimize the mesh texture and geometry. For the geometry optimization, we utilize monocular depth prediction as pseudo supervision and align the smooth areas of the scene, as elaborated in Sec. 3.2. viewmasked diffuse sample viewdiffuse sample (a) View-by-view outpainting sample a cubemap at center diffuse —— optimize — (b) Cubemap based texture Figure 3. Methods for generating scene texture. The step “diffuse” means generating a 2D image with diffusion models. The “optimize” means updating the mesh texture with the 2D generated images (cf., Eq. (5)). (a) A straightforward baseline based on outpainting with 2D diffusion models. Outpainting is achieved by masked diffusion and the gray area means the masked area remains unchanged through the diffusion. (b) Generating a cubemap for the scene, then optimizing the mesh texture. far surface depth depthmap “" far camera origin distance distance map Enear Figure 4. Illustration of the depth map and the distance map. Depth map measures the length between the object plane to the screen plane, while distance map measures the length between points to the camera origin. where F denotes the rendering function, In our implementation, we use a differentiable render [19], with which we can back-propagate the gradients to 3D to generate a mesh from synthesized images. Let {7} represent a set of cameras, we can generate a sequence of images {X¢} using the method in 3.1. Then we can define --- --cubemap face A: cubemap face B Ba x a. (ee (a) diffusion with depth map cubemap face A ! cubemap face B project onto the wall ee ANS oA (b) diffusion with distance map Figure 5. Different controlling effects of the depth map and the distance map. The depth map exhibits rapid change at the joint boundary of the two faces of the cube map. Conversely, the distance map changes smoothly. Generating consistent cube maps with depth control becomes challenging, whereas the employment of distance map results in more consistent texture. However, the distance map results in artifacts such as the window on the wall, as the diffusion model is conditioned on the depth map during training. a texture-based loss: Leexture =). |[Rx(V,F,Ve,t*) — X$IP, 4) k Then we get a baseline method of reconstructing the mesh texture by gradient descent: OLtexture Ve Ve ay OLtexture ORx(V, F, Ve, r*) =V- , RRR Var) OV. (5) Note that Eq. (5) can only update V.. but not the geometry V, F. This is because the differentiable render [19] cannot compute the gradient for geometry, i.e., oRx = 0,28 x 0. A further step is to optimize jointly V, F with V.. Because the input mesh often exhibits low-quality geometry (e.g., with holes), we hope the geometry V, F can be adjusted to match the image sequences {X/*}. An essential observation is that when a synthesized scene contains a smooth region, such as a planar shape, the reconstructed geometry should also be planar. To model this observation, we first reconstruct the depth map D?*" from {X}} using an off-the-shelf monocular depth estimator E (e.g., MiDaS [27]). Then we define the condition for planar regions JADE" (u, v)| <7 (6) where A is the Laplacian of the depth map D,, and 7 is a threshold for determining smooth areas. Then, we denote the smooth area as P = {(u, v)| ADE" (u,v)| < 7}. We expect on P, reconstructed depth map D is as smooth as possible, i.e., with Laplacian close to zero. Thus, we get another loss function: Lgcometry = >) > |ADg(u,)I, k (wvyeP (7) where D, = Rp(V, Fr") Besides the geometry loss, the generated images are used to update the texture of the scene with the following loss. The overall progress of updating the scene mesh can be represented as follows, OLtexture Ve Ve = ov. Ve vy ~ 7Seasometny (8) OL FeF- geometry ~*~ I~ oF An algorithm overview is shown in Algorithm 1. 4. Experiments Our problem setting assumes inputting with a scanned room mesh, which has not been extensively explored in the existing literature. We compare the result of our method with two groups of works: (1) Ablation studies of our own baselines, but without submodules such as geometryguided diffusion, distance map, smooth region regularization, etc. (2) NeRF-based methods, including Score Distillation Sampling (SDS) [25] and InstructN2N [14]. Since we can project our input mesh to generate multi-view images, which will be used to reconstruct NeRF. Note the reconstruction of NeRF is more computationally expensive, and the reconstructed geometry of NeRF is not always accurate, so we also use the ground truth depth from the mesh input to boost the reconstruction performance of InstructN2N [14] to get a fair comparison. 4.1. Dataset and Implementation Details We conducted experiments on the ARKitScenes dataset [9], which comprises real indoor scenes captured by an --- --Figure 6. Qualitative comparisons of the output mesh. Outpainting is the baseline method in that textures are outpainted sequentially, while we treat the scene as a whole. Strip shape artifacts can be observed in the outpainting baseline. iPhone. Our method is evaluated qualitatively and quantitatively on the validation set of ARKitScenes, which covers a diverse range of room types and floor plans. To generate the cubemap, we set the camera origin at the center of the mesh. For generating depth-based images, we utilize ControlNet [40] and maintain the default hyperparameters, such as the guidance scale and the number of diffusion steps T’. To cover the regions not included in the cubemap, we randomly select K = 100 cameras around the center and use the masked generation mode of the diffusion model. We predict the monocular depth using MiDaS [27]. During the optimization process, we use the Adam optimizer with a learning rate of 0.001 for optimizing both the geometry (V, F) and the vertices color V.. The optimization is run for a total of N = 1000 steps. A scene takes around 15 mins to process with one A100 GPU. --- --"space cabin" Inputs Ours SDS [25] InstructN2N [4, 14] Figure 7. Comparisons with other 2D diffusion based 3D editing methods. SDS [25] can improve the geometry, but the generated texture is kind of blurry. InstructN2N [14] is limited by its backbone InstructP2P [4], which is a purely image-based editing method, and thus may be misled by the presented input image. Our scheme can well handle geometry and texture generation. Algorithm 1 Overall pipeline of our method Input: System requirements: 2D diffusion model f, monocular depth estimator From user: mesh (V, F, V.), text prompt Ctext Output: updated mesh (V*, F*, V.*) with the new style Step 1: Geometry Guided Diffusion for 3D Scene 1: set cubemap cameras at scene center 2: acquire cubemap depth D and distance D : generate cubemap Xgube from f, Ctext, D, D > use Eq. (1) : sample K random cameras {7} b> for uncovered area for k = 1 to Kk do if 7, sees areas not covered by Xgube then acquire depth D, from 7, generate image xk from f, Ctext, Dk end if 10: end for Step 2: Mesh Optimizing 11: for X¥ in all generated images do 12; get D&*™ = E(X®) using monocular depth estimation 13: end for 14: for n = 1 to N do 15: Update the 3D mesh using Eq. (8). 16: end for w 4.2. Comparing with Baselines We first conduct a qualitative assessment of our approach in contrast to the outpainting baseline. The results of this analysis are depicted in Fig. 6, wherein we compare our cubemap based scene texture generation with the outpainting baseline. Evidently, the scene generated by outpainting exhibits strip-like artifacts that arise from the flawed outcome of the masked generation mode of the diffusion models. Examples of strip-like artifacts can be observed on the walls of the scene. Conversely, our technique consistently produces images with a high-quality and uniform style that is retained throughout the entire scene. This superiority can be attributed to the employment of our cubemap texture generation scheme. Furthermore, we delve into the impact of blending the distance map (cf., Eq. (1)) during cubemap diffusion in Fig. 8. Notably, a crucial difference can be observed in the region demarcated by the orange box, which represents a patch of the joint area of two cubemap faces. Upon assessing the input scene, we establish that the wall in this area is distorted, implying that the depth signal for diffusion is distorted as well. Specifically, the second row of the figure shows that this area has been treated as a turning corner of two walls instead of a single plane, which indeed is. This generated corner is a result of the distortion in the depth controlling signal. Upon incorporating distance map controlled denoising during the diffusion as shown in the third row, the patch is correctly considered as a single wall plane. Apart from the joint area, we also observe other benefits such as the alignment of areas as indicated by the red arrows. In Fig. 9, we present a comparison between the original input scene’s geometry and the updated scene’s geometry. The text prompt used for the generation is “a royal castle”. It can be observed that after using our method, the mesh is smoother than before. Additionally, our method successfully fills in some holes in the mesh. --- --input scene cubemap face A cubemap face Bcubemap face C cubemap face D 1 cubemap face A Figure 8. Cubemap generation with and without the distance map blending step (cf., Eq. (1)). Without distance map blending, the 2D diffusion tends to generate two planes on the border area of the cubemap (i.e., the orange box area). With distance map blending, the border area is treated as one plane. original geometry updated geometry Figure 9. Visualization of the geometry editing. 4.3. Comparing with NeRF based Approaches In this subsection, we compare our method to two prominent 2D diffusion based 3D generation methods, namely Score Distillation Sampling (SDS) [25] and InstructN2N [14]. We used the open-source version [34] of SDS with Stable Diffusion as the 2D diffusion model. Next, we present a comparative analysis of the performance of our approach versus recently proposed 2D-based 3D editing methods, as depicted in Fig. 7. To ensure a fair comparison, we employ the depth-guided diffusion model (i.e., the same as ours) for our SDS experiments. Upon inspection of the results, we observe that SDS can result in blurred effects, which is similar to the phenomenon recently reported in a study on 2D image generation utilizing SDS [15]. For InstructN2N [4, 14], we note that its performance is primarily influenced by the efficacy of its backbone, i.e., InstructP2P [4]. It is important to mention that InstructP2P solely relies on the input image and is not conditioned on geometry like depth. Consequently, in InstructN2N, we observe instances where the model is misled by empty regions (i.e., white areas) in the input image, such as the white space on the sofa in the first row. Moreover, employing a purely image-based editing approach may restrict the diversity of the generated images, as demonstrated by the wooden floor in the second row. The comparisons presented in this figure illustrate that our proposed scheme is capable of successfully generating texture with a suboptimal quality input mesh. 4.4. Quantitative Evaluation The quality of a stylized indoor scene is usually subjective, but we have adopted the evaluation approach from [4, 14] for quantitative analysis of the generated results. This evaluation process is based on the embedding provided by CLIP [26]. There are two metrics used for evaluation. The first metric, called “Text-Image Similarity,’ computes the inner product between the given text prompt and the generated image. Higher values indicate higher similarity between the text and image vectors, which implies a smaller angle between them. The second metric, referred to as “Direction Consistency,” assesses the consistency of the generated scenes across different views. The score is computed as follows: Given two CLIP embeddings of the original input views, denoted as o, and o,, and two CLIP embeddings of the generated views, denoted as g, and gy, the score is --- --Text-Image Direction Similarity Consistency InstructN2N [4, 14] 0.2022 0.SDS [25] 0.1532 0.Ours 0.2543 0.Table 1. Quantitative comparisons with other editing methods. For the two metrics (cf., Eq. (9)), a higher value indicates better performance. Text-Image View Similarity Consistency w/o Cubemap 0.2123 0.w/o Distance 0.2274 0.w/o Geo Optimize 0.2017 0.Full Model 0.2543 0.Table 2. Quantitative ablation. “w/o Cubemap” is the outpainting baseline. “w/o Distance” means removing the distance map based blending scheme. “w/o Geo Edit” means we do not update the geometry with the pseudo depth supervision (i.e., no Lgeometry). calculated as (Ea = 0a) * (8b = 0») la — Oa||gv — ov] A lower score implies that the direction of generation is better aligned across different views, indicating greater consistency in the scene generation. To evaluate the performance, we select a total of 80 meshes from the validation set and create 15 textual prompts. For each mesh, we randomly select 4 different views to test. For calculating Direction Consistency, we use the sampled 4 views and one remaining view, resulting in 4 original-generated pairs. Thus, we obtain a total of 4800 pairs of image-text and originalgenerated pairs which are used to calculate the evaluation metrics. We first present a comparison of our method with SDS and InstructN2N in Table 1. Our approach outperforms both SDS and InstructN2N with higher similarity and consistency scores. The similarity score of SDS appears to be relatively low, which may be attributed to the blurring effect. The high direction consistency score of SDS reflects the consistent blurring effect. InstructN2N achieves a lower text-image similarity score due to the restriction of its purely image-based editing backbone (InstructP2P). However, the direction consistency score of InstructN2N is good, indicating the effectiveness of its dataset updating scheme. We then carry out some ablation studies, as shown in Table 2. Our outpainting model exhibits good direction consistency but a low text-image similarity. Removing the distance map blending scheme adversely impacts the text (9) image similarity score. Furthermore, we find that removing the geometry optimization has a noticeable negative impact on the text-image similarity score. This indicates the necessity of optimizing the geometry when seeking to customize and stylize a scanned mesh. 5. Conclusion In this paper, we tackle the problem of synthesizing a 3D interior scene from text prompts based on a scanned indoor mesh input. We propose a solution that capitalizes on the capabilities of 2D diffusion text-to-image generative models. The primary challenge lies in generating coherent 3D geometry and textural information from the 2D generative priors. To ensure the consistent visual appearance of the whole scene, we first develop a geometry-guided 3D scene texture generation technique. Our key idea is to generate a cubemap of the space, thus achieving a consistent style throughout the different views. We then jointly optimize mesh geometry and texture, based on the pseudo-depth estimated by a monocular depth estimator. Our claimed contributions are validated via experiments on real scanned meshes. Acknowledgements We are grateful to our colleagues at Apple Inc. for their valuable support in enhancing the quality of this work. References [1] Sherwin Bahmani, Jeong Joon Park, Despoina Paschalidou, Xingguang Yan, Gordon Wetzstein, Leonidas Guibas, and Andrea Tagliasacchi. Cc3d: Layout-conditioned generation of compositional 3d scenes. arXiv preprint arXiv:2303.12074, 2023.[2] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113, 2023.[3] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, et al. Gaudi: A neural architect for immersive 3d scene generation. Advances in Neural Information Processing Systems, 35:25102-25116, 2022.[4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 2,7, 8,[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16123-16133, 2022.[6] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini --- ---De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view synthesis with 3d-aware diffusion models. arXiv preprint arXiv:2304.02602, 2023.Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023.Angela Dai, Christian Diller, and Matthias NieSner. Sg-nn: Sparse generative neural networks for self-supervised scene completion of rgb-d scans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 849-858, 2020.Afshin Dehghan, Gilad Baruch, Zhuoyuan Chen, Yuri Feigin, Peter Fu, Thomas Gebauer, Daniel Kurz, Tal Dimry, Brandon Joffe, Arik Schwartz, and Elad Shulman. Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile RGB-D data. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. 1,Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov, et al. Nerdi: Single-view nerf synthesis with languageguided diffusion as general image priors. arXiv preprint arXiv:2212.03267, 2022.Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W Taylor, and Joshua M Susskind. Unconstrained scene generation with locally conditioned radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14304-14313, 2021.Huan Fu, Bowen Cai, Lin Gao, Lingxiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, and Hao Zhang. 3d-front: 3d furnished rooms with layouts and semantics. In 202] IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 10913-10922. IEEE, 2021.Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerfguided distillation from 3d-aware diffusion. arXiv preprint arXiv:2302.10109, 2023.Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. 2023. 2, 5,7, 8,Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090, 2023.Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.Lukas Héllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias NieBner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989, 2023.Lukas Hdllein, Justin Johnson, and Matthias NieBner. Stylemesh: Style transfer for indoor 3d scene reconstruc-tions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6198-6208, 2022.Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics, 39(6), 2020. 3, 4,Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming- Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In JEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023.Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Realfusion: 360° reconstruction of any object from a single image. arXiv e-prints, pages arXiv—2302, 2023.Norman Miiller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Buld, Peter Kontschieder, and Matthias NieBner. _Diffrf: Rendering-guided 3d radiance field diffusion. arXiv preprint arXiv:2212.01206, 2022.Despoina Paschalidou, Amlan Kar, Maria Shugrina, Karsten Kreis, Andreas Geiger, and Sanja Fidler. Atiss: Autoregressive transformers for indoor scene synthesis. Advances in Neural Information Processing Systems, 34:12013—12026, 2021.Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022. 2,5, 7, 8,Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In JCML, 2021.René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022. 5,Daniel Ritchie, Kai Wang, and Yu-an Lin. Fast and flexible indoor scene synthesis via deep convolutional generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6182— 6190, 2019.Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695, 2022.Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan, Matthias NieBner, and Angela Dai. Retrievalfuse: Neural 3d scene reconstruction with a database. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12568-12577, 2021.Josef Sivic, Biliana Kaneva, Antonio Torralba, Shai Avidan, and William T Freeman. Creating and exploring a large --- ---photorealistic virtual space. In 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 1-8. IEEE, 2008.Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 2,Jiaxiang Tang. Stable-dreamfusion: Text-to-3d with stable-diffusion, 2022. https://github.com/ashawkey/stabledreamfusion.Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel X Chang, and Manolis Savva. Plan2scene: Converting floorplans to 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10733-10742, 2021.Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. arXiv preprint arXiv:2212.00774, 2022.Kai Wang, Manolis Savva, Angel X Chang, and Daniel Ritchie. Deep convolutional priors for indoor scene synthesis. ACM Transactions on Graphics (TOG), 37(4):1-14, 2018.Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360° views. 2022.Zongxin Yang, Jian Dong, Ping Liu, Yi Yang, and Shuicheng Yan. Very long natural scenery image prediction by outpainting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10561-10570, 2019.Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 2,Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming yu Liu. Diffcollage: Parallel generation of large content with diffusion models. In CVPR, 2023.Zhengyou Zhang. Microsoft kinect sensor and its effect. IEEE MultiMedia, 19(02):4-10, 2012.Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d reconstruction. In CVPR, 2023.