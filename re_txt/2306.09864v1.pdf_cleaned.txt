--- --2306.09864v1 [cs.CV] 16 JunarXiv AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu™, Xun Cao Nanjing University Nanjing, China casual personal photos of (W] clothes [V] in [W] clothes casual personal photos of character [V] [V] in armor [V] wearing doctor suit Figure 1: We present AvatarBooth, a framework for generating 3D avatars from text prompts or certain images. Our method can generate 3D human avatars in prompt generative mode (red), appearance customized mode (blue), or hybrid mode (green). ABSTRACT We introduce AvatarBooth, a novel method for generating highquality 3D avatars using text prompts or specific images. Unlike previous approaches that can only synthesize avatars based on simple text descriptions, our method enables the creation of personalized avatars from casually captured face or body images, while still supporting text-based model generation and editing. Our key contribution is the precise avatar generation control by using dual fine-tuned diffusion models separately for the human face and body. This enables us to capture intricate details of facial appearance, clothing, and accessories, resulting in highly realistic avatar generations. Furthermore, we introduce pose-consistent constraint to the optimization process to enhance the multi-view consistency of synthesized head images from the diffusion model and thus eliminate interference from uncontrolled human poses. In addition, we present a multi-resolution rendering strategy that facilitates coarseto-fine supervision of 3D avatar generation, thereby enhancing the performance of the proposed system. The resulting avatar model can be further edited using additional text descriptions and driven by motion sequences. Experiments show that AvatarBooth outperforms previous text-to-3D methods in terms of rendering and geometric quality from either text prompts or specific images. KEYWORDS Avatar creation, diffusion model, neural implicit field, model finetuning 1 INTRODUCTION Creating 3D human avatars from texts or images is a longstanding challenging task in both computer vision and computer graphics, which is key to a broad range of downstream applications including the digital human, film industry, and virtual reality. Previous approaches have relied on expensive and complex acquisition equipment to reconstruct high-fidelity avatar models [Alexander et al. 2010; Guo et al. 2017; Xiao et al. 2022]. However, these methods require multi-view images or depth maps that are unaffordable for consumer-level applications. Alternatively, other methods leverage a neural network to predict plausible avatar models from a single image input [Saito et al. 2019; Xiu et al. 2022; Zheng et al. 2021]. Nonetheless, these approaches are limited by the availability of suitable images and are non-editable once a reference image is provided. Recently, 3D content generation based on large-scale pre-trained vision-language models has shown promising performance [Lin et al. 2022; Poole et al. 2023; Raj et al. 2023]. Specifically, these --- --methods leverage the general 2D image priors learned from largescale pre-trained models to guide the optimization of an implicit 3D representation. In early attempts, the contrastive language-image pre-training (CLIP) [Radford et al. 2021b] is leveraged to synthesize the appearance of the avatar given a text prompt [Hong et al. 2022; Youwang et al. 2022]. Then, the Score Distillation Sampling (SDS) [Poole et al. 2023] is further proposed to boost the performance by distilling the 2D knowledge from a pre-trained diffusion model [Ho et al. 2020; Rombach et al. 2022; Saharia et al. 2022] to 3D content generation via differentiable rendering. Although significant progress has been made, current methods are still unable to synthesize high-quality shapes and appearances of the human object, which contains complex poses and detailed 3D structures like cloth wrinkles and facial shapes. On the other hand, generating a customized avatar of arbitrary identity that corresponds to input images remains a challenging problem. Though DreamBooth3D [Raj et al. 2023] provided a solution for generating personalized 3D assets, it struggles to reproduce the high-fidelity human face with the exact identity shown in images. A novel fine-tuning strategy is required to support both detailed appearance synthesis and multimodal-driven customization of the avatar. In this paper, we propose a novel method, named AvatarBooth, for generating high-quality and customizable avatars from text prompts or image sets. Our method aims to generate identity-customized 3D avatars that accurately reflect the visual and textual features of a specific individual. To this end, a neural implicit surface [Wang et al. 2021] is learned to represent the shape and appearance of the human avatar, which is supervised with dual pre-trained or finetuned latent diffusion models for the face and body respectively. Meanwhile, the pose-consistent constraint is introduced to enhance the fine-tuning of the diffusion models in the task of appearancecustomized generation, which provides more accurate multi-view supervision with a consistent appearance in a canonical pose space. Furthermore, a multi-resolution SDS scheme is introduced to predict the fine structure and appearance of the avatar in a coarse-to-fine manner. By leveraging a few pictures of a person, the model can synthesize 3D avatars that not only possess the individual’s unique appearance but also match the abstract features specified in the input text prompt. These abstract features include attributes such as “wearing glasses or hats in a certain style’, which are user-friendly in editing and modifying the avatar’s overall visual identity. Our approach is designed to leverage priors in both large languagevision models as well as concrete input images, resulting in avatars that are faithful to the input appearance while also being editable through text prompts. The contribution of this paper can be summarized as: e We propose a 3D human avatar generation framework that supports both text prompts and arbitrary images as input. Dual latent diffusion models are introduced to supervise the face and body generation separately, yielding detailed facial appearance, clothes, and wearings. ¢ Pose-consistent constraint is introduced to customize the large pre-trained diffusion models given photos of a specific person. We use ControlNet [Zhang and Agrawala 2023] to enhance the multi-view consistency of the synthesized images, so as to eliminate the interference of uncontrolled human poses and lead to high-quality appearance and geometry. e We present a multi-resolution score distillation sampling strategy that supervises the generation of the 3D avatar ina coarse-to-fine manner. Experiments show that this strategy not only enhances the rendering quality but also improves the robustness of generation. 2 RELATED WORKS Text-guided 2D&3D Generation. In recent years, diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; Song et al. 2020] have rapidly developed due to their remarkable performance in synthesizing high-quality images. A core structure of diffusion models consists of forward diffusion steps that add noise according to a scheduler and backward generative steps that denoise the noise. In addition to unconditional generation from Gaussian noise only, the diffusion model can generate high-quality images from text prompts or images as input. Among the various diffusion models, the latent diffusion model [Rombach et al. 2022] has emerged as a promising text-to-image model that strikes a good balance between image quality and memory usage. When it comes to 3D content generation, existing methods [Chen et al. 2023; Lin et al. 2022; Poole et al. 2023] leveraged pre-trained text-to-image diffusion models to supervise coordinate-based networks with score distillation loss (SDS) [Poole et al. 2023]. Other methods [Wang et al. 2023; Wu et al. 2023; Zhang et al. 2023] leverage the 3D parametric face model [Yang et al. 2020; Zhu et al. 2021a], large pre-trained language-vision models [Radford et al. 2021a; Rombach et al. 2022], or large-scale synthetic data [Wood et al. 2021] to achieve the textguided generation of high-fidelity 3D human faces. Moreover, some approaches [Metzer et al. 2022; Seo et al. 2023] adopt a neural radiance field to represent the latent space of Stable Diffusion, which enables the synthesis of novel views from text descriptions. Except for using SDS for guidance, other methods [Hong et al. 2023a; Wang et al. 2022] also use Score Jacobian Chaining to generate 3D assets with text, which takes into consideration the gradient of diffusion models. While these methods have successfully generated viewconsistent 3D models, generating 3D avatars remains a challenging task due to the complexities involved in articulated 3D shape and appearance diversity. Finetuning of Diffusion Models. In recent years, with the growing interest in the text-to-image domain, pioneer researchers have explored personalizing text-to-image models using photos of specific subjects. One representative work is DreamBooth [Ruiz et al. 2022], which leverages a rare token to represent a particular subject or style while preventing overfit with a prior preservation loss. Starting from another strategy, textual inversion [Gal et al. 2022] creates a new embedding for the input concept and optimizes this embedding vector with a few photos to achieve subject-driven image generation. LoRA [Hu et al. 2022] proposes to fine-tune large language models, which freezes the pre-trained model weights and meanwhile injects learnable rank decomposition matrices into the layers of the Transformer network [Vaswani et al. 2017]. LoRA significantly improves the ease of diffusion model fine-tuning by --- --| want to generate a high-quality 3D avatar of [V] in [W] clothes. Multi-Types Renderings (Texture-less / Normal / Color) Avatar Modeling (Neural Implicit Surface) A Volume Rendering Training-Supervised Diffusion Models (Selected by different modes) ~ Pre-Trained Latent Diffusion mode III mode I Pre-Trained Latent Diffusion MS-SDS Pre-Trained Latent Diffusion Head Fine-Tuned DreamBooth Latent Diffusion Pose-Consistent Constraint Pre-Trained Latent Diffusion DreamBooth Figure 2: Overall pipeline. Our method represents a human avatar with NewS, which is initialized with an SMPL shape. Through the volume renderer, the avatar model is transformed into textureless, color, and normal renderings, which are used for SDS training with the supervision of diffusion models. By adopting a selection or combination of pre-trained and fine-tuned diffusion models, our approach can be performed in three modes: (I) prompt generative mode; (II) appearance customized mode; and (III) hybrid mode. The fine-tuning strategies for the face and full body are shown in (a) and (b) respectively. reducing the number of trainable parameters for downstream tasks. DreamBooth3D [Raj et al. 2023] proposes a 3-stage iterative approach to produce 3D models using the fine-tuned personalized diffusion model with DreamBooth. However, DreamBooth3D fails to recover an identity-consistent and detailed human avatar, meanwhile, the generated personalized content is not editable based on text prompts. Therefore, we explore addressing these limitations and enhancing the flexibility of avatar generation based on both image sets and text prompts. Avatar Generation Models. Traditional methods for generating 3D avatars often rely on training on 3D datasets, which can be difficult to collect and scale up. To overcome this challenge, recent methods have utilized cheaper 2D data to train a neural field, such as EG3D [Chan et al. 2022], GNARF [Bergman et al. 2022], EVA3D [Hong et al. 2023b], HumanGen [Jiang et al. 2023a], ENARFGAN. Additionally, some explicit methods [Alldieck et al. 2019; Han et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et al. 2022; Zheng et al. 2021] have been developed to generate human avatars conditioned on a single input image. These methods have limitations in generating avatars with unseen styles, wearings, and appearances that are not present during training, let alone personalizing avatars with a certain identity via word descriptions. AvatarCLIP [Hong et al. 2022] was the first to generate and animate 3D avatars in a zero-shot text-driven manner, while CLIP-Actor learns a displacement map from CLIP for mesh deformation and vertex coloring. More recently, following works like AvatarCraft [Jiang et al. 2023b] and DreamAvatar [Cao et al. 2023] have utilized diffusion models to produce high-quality 3D avatars. However, AvatarCraft is limited in producing large deformations to the original template model, and their geometry has some artifacts in places like the chest and back. DreamAvatar generates high-quality clothing geometry and texture, but it is unable to extract a fine-grained mesh due to the inherent feature of LatentNeRF, resulting in non-articulated results after the training process is finished. Our method manages to eliminate such issues and can synthesize a customized avatar from free-view images, which can be edited through text prompts. 3 METHOD 3.1 Preliminaries Neural Implicit Surfaces (NeuS) [Wang et al. 2021]. NeuS is a neural implicit representation that represents a 3D surface as the zero-level set of a signed distance function (SDF) [Park et al. 2019]. --- --Given a coordinate (x, y, z) and viewing location/direction (0, d), two MLPs are used for predicting the SDF value and the RGB value respectively. Then, pixel colors can be calculated using the volume rendering equation: n C(o,d) = ¥" (w(te(p(t),d)) ) peR where p(t) is a sampled point, and R contains n sampled points along the ray 0 +t - d, and w(t) is formulated as: _ __ bs(fPM)) w(t) ~ RO Re iy $s(fo@)))du where @s is the logistic density distribution, f is an SDF network. Compared to NeRF [Mildenhall et al. 2020] and its variances, the formulation of NeuS eliminates bias in the first order of approximation, leading to more detailed surface reconstruction. Score Distillation Sampling (SDS) [Poole et al. 2023]. SDS is a strategy to generate subjects from a diffusion model by optimizing a loss function, which can be used to optimize an avatar represented by a 3D field. Specifically, the gradient of this score function indicates a higher density region for rendered images. The detailed formula is introduced in Section 3.2. In order to achieve personalized generation under the SDS strategy, DreamBooth3D proposes to supervise the learning of the subject’s appearance with a diffusion model fine-tuned by DreamBooth [Ruiz et al. 2022]. In our method, we follow the SDS strategy cooperated with fine-tuned diffusion models, then made improvements to further improve the customizable capability and the accuracy of the synthesized appearance. (2) 3.2 Pipeline Overview Our method takes a set of images or text prompts as input and synthesizes a 3D detailed avatar represented by NeuS. As shown in Fig. 2, the entire generation pipeline consists of three modules. In the avatar modeling module, a bare rendering of SMPL model [Loper et al. 2015; Pavlakos et al. 2019] is trained into a neural implicit field [Wang et al. 2021] that consists of an SDF network f(x; @) and a color network c(x; 0), following prior works [Hong et al. 2022; Jiang et al. 2023) In the rendering module, three types of renderings are obtained from pre-defined virtual cameras located around the avatar space. Empirically, we rendered normal maps J; in addition to color and texture-less renderings {Z., Ty}, and experiments demonstrate that the introduction of normal maps enhances the geometric details such as facial contours and cloth wrinkles. Then, we leverage the SDS Loss to guide the NeuS to converge, which can be formulated as: w(t) (é (efi, t) - é) ort where, ¢ represents the parameters of the diffusion model, I is the image used for supervision including {Ig, Tn, Ic}, and z! is the corresponding latent code of the image J. The function e() represents the noise predicted by the diffusion model, while y and t denote the input prompt and timestep, respectively. To optimize the face and human body simultaneously, we adopted two sets of VyLsps(9. 2) = E (3) rendering parameters centered on the face and the whole human body respectively, which will be detailed in Section 3.3. In the SDS training modules, pre-trained and fine-tuned latent diffusion models are selected or combined to supervise the training of NeuS via the renderings. A multi-resolution training of SDS is implemented to model the avatar in a coarse-to-fine manner, which will be detailed in Sec. 3.5. In the fine-tuning of the latent diffusion models, we propose to introduce the pose-consistent constraint, which will be detailed in Sec. 3.4. According to how the pre-trained diffusion models are used in SDS training, our framework may work in the following three modes: I) Prompt generative mode. Similar to AvatarCLIP [Hong et al. 2022] and AvatarCraft [Jiang et al. 2023b], we use only text prompts as input to generate avatars that conform to the description without fine-tuning the pre-trained diffusion models. Since text prompts can only describe general or well-known appearances, this mode only works for synthesizing avatars with roughly matched appearances or celebrities. (II) Appearance customized mode. We propose to customize the diffusion models as well as the learned avatars to match the appearance given a set of images. These images can be full-body or facial images taken freely from any viewpoint. Details of the appearance and clothing are passed on to generate the avatar model, even if the input picture contains only an incomplete or slightly contradictory appearance. (II) Hybrid mode. The above two modes can be performed simultaneously in a single model generation, that is, the hybrid mode. This mode can achieve the more complex conditional generation of an avatar, such as modifying the subject’s clothes, hairstyle, age, beard, etc., through text prompts on the premise of synthesizing appearance according to input images. 3.3. Dual Model Fine-tuning We propose to leverage two diffusion models to supervise the training of the whole body and head, and the two models are also finetuned respectively. Though the previous works [Hong et al. 2022; Jiang et al. 2023b] augment the rendering samples around the face to improve the facial details, they do not exploit the potential of the fine-tuned vision-language models, so their attempts cannot enhance the performance of personalized avatar generation. We initially uses only one diffusion model to supervise the training of the full body. We observe that a single SDS loss with the fine-tuning strategy of DreamBooth3D fails to strike a balance between the modeling of the facial appearance and the body clothes. Specifically, in early training steps, the appearance of clothes on the body is learned but the facial appearance is still unclear. If more training steps are made, the facial appearance will turn clear, but the global features like clothes style may be overfitted to the input images, which means it is hard to edit the body via text prompts in the hybrid mode. Besides, we also observed that Img2Img stage of DreamBooth3D can’t produce accurate character identity faithful to the input images. We believe that this is due to the large difference in the scale between facial appearance and body appearance, which leads to the inconsistent convergence rate in the SDS training. --- --Pose-Consistent Constraint Multi-view Facial Images Generation 900-Steps DreamBooth 500-Steps DreamBooth Figure 3: Pose-Consistent Constraint. We first train a initial DreamBooth model Djnit using input images J,.,; for plenty of steps. Guided by pose constraint rendered from Openpose, we can produce multi-view facial images J, that share the same identity with the person in the input images. Then, we combine the multiview images J, along with the input personal images J,.q) to fine-tune the final DreamBooth Drinal for more steps. To address this issue, we propose the dual model fine-tuning strategy. When running in appearance customized mode or hybrid mode, the input images are divided into full body shots and headshots, which are used for fine-tuning two pre-trained models respectively. In the SDS training phase, we randomly sample cameras around the face and the whole body, then employ different diffusion models to guide the generation of the face and body, using head-oriented rendering and full-body rendering respectively. In the fine-tuning of the head shots model, we also introduce the pose-consistent constraint, which will be detailed in Sec. 3.4. 3.4 Pose-Consistent Constraint To enhance the facial details of avatars generated from fine-tuned diffusion models, we propose a pose-consistent fine-tuning method by introducing ControlNet [Zhang and Agrawala 2023]. Previous approaches[Raj et al. 2023] proved that directly utilizing DreamBooth with SDS-based methods will result in unsatisfactory outcomes, as the DreamBooth model tends to overfit the camera views used during fine-tuning. In this work, we propose a two-stage strategy that utilizes ControlNet to incorporate more facial prior to the training process. Specifically, we first train an initial DreamBooth model Dinit using the input images J,¢q). Then, we employ a keypoint ControlNet to produce multiview facial images Jy guided by a skeleton constraint generated by OpenPose [Cao et al. 2021, 2017], which are rendered in surround views. These synthetic images Jy are then combined with real images J,¢q; to further fine-tune a new diffusion model Drinal by the DreamBooth method, thereby augmenting the facial details of the 3D model. Unlike previous methods[Raj et al. 2023] that attempt to solve this issue during the training process of NeRF, our approach leverages ControlNet to address this problem before training a Neural Surface Field. As a result, we can use the same DreamBooth model Dyinat to generate different avatars with the same identity without re-training from scratch. Experiments show that the use of ControlNet to guide the generation of multiview facial images in combination with the DreamBooth model leads to more accurate and realistic avatars. 3.5 Multi-Resolution SDS As directly rendering high-resolution images from neural implicit filed is very computationally expensive, a common solution is to render a low-resolution image, then up-sample it to a higher resolution for SDS training [Chen et al. 2023; Lin et al. 2022]. The up-sampled images are then encoded to the latent space and used to supervise the training of a neural implicit field. However, we observed that increasing the upsampled resolution directly can lead to training collapse or inconsistent appearance. To address these issues, we propose a multi-resolution optimization strategy, which gradually improves the up-sampling resolution for more stable SDS training. Starting from H x W images {Iy, In, Ic} rendered from NeuS, we initialize the network by training an up-sampled resolution of 512 x 512 for a few steps, then gradually improve the supervision resolution to 640 x 640 and 768 X 768. The lower resolution in the early training steps provides a coarse but robust starting point for the training process, while the higher resolution in the latter steps helps learn detailed geometry and high-quality appearance. Through experiments, we demonstrate that this simple strategy efficiently improves the stability in the early training stage and augments the appearance quality, ultimately yielding a more accurate and visually plausible avatar. 4 EXPERIMENTS In this section, we verify the effectiveness of the proposed method through experiments and compare and discuss it with previous methods. 4.1 Implementation details To model the neural implicit surface, we use a 6-layer MLP for the SDF network and a 4-layer MLP for the color network. To generate an avatar, we train the network for 8000 steps, including 2000 steps under an interpolation resolution of 512 x 512, 2000 steps under an interpolation resolution of 640 x 640, and 4000 steps under an interpolation resolution of 768 x 768. In the diffusion model fine-tuning phase, we train the first DreamBooth model for 900 iterations to produce multi-view images, then use the generated images combined with the personal images to train a second DreamBooth model for 500 steps. We randomly sample the virtual camera locations for rendering, which contains 25% centering on the face and the other 75% centering on the overall body. The normal maps, shadow images, and color images are randomly rendered at a ratio of 1:1:8. These hyper-parameters are the same in all three modes. Adam optimizer [Kingma and Ba 2015] is used to train our model and the learning rate is set to 0.005. On an NVIDIA RTX3090 GPU, it takes about 90 minutes to synthesize the avatar model and another 30 minutes to complete the fine-tuning of the diffusion model if appearance-customized generation is required. 4.2 Qualitative Evaluation Our results of prompt generative mode are shown in Fig. 4. We can see that our method synthesizes plausible human avatars with --- --Hilary Clinton Bill Gates Superman Deadpool Stormtrooper Luffy Figure 4: Results in prompt-generative mode. Our method recovers fine geometric shapes and textures, and the resulting human avatars closely match the text prompts. --- --Elsa in Frozen ii AvatarCLIP CLIP-Actor A TEXTure Ours Figure 5: Qualitative comparisons-I. We visualize the avatars generated by ours and previous works by rendering the model into color images and normal maps. The models generated by our method contain better geometric details, such as clothing lines and facial features, and have better rendering quality. detailed geometry and fine appearance, which closely match the input text prompts. Our results of appearance customized mode and hybrid mode are shown in Fig. 10. In these experiments, we can see that the human appearance from input image sets is transferred to the generated avatar, even if the images are captured in free conditions. The customized avatar can be further modified according to the additional text prompts. For example, by using simple prompts like “’a [V] man with yellow hair’, the yellow hair will appear on the result avatar’s head accordingly. Moreover, we demonstrated that even more abstract prompts like ’[V] wearing like a wizard’ and ’[V] in his fifties’ were effective, meanwhile, the customized appearance from the input image set is maintained if working in the hybrid mode. By employing text prompts, we were able to produce a diverse range of avatars with different appearances and styles, providing users with an efficient and personalized way of creating their desired avatars. Our result models can be easily rigged for animation. We recommend watching the video for more results and surrounding-view rendering. We also compare our results qualitatively with prior works. Considering that previous approaches do not support customized avatar generation based on image sets, we only compare our methods in prompt generative mode with previous text-to-avatar methods, as shown in Fig. 5. As there are no official implementations for AvatarCraft [Jiang et al. 2023b] and DreamAvatar [Cao et al. 2023], we compare the performance with the same setting in their experiments, as shown in Fig. 6. Our model achieves significant improvements over existing approaches in terms of both geometry and AvatarCraft DreamAvatar Ours Figure 6: Qualitative comparison-III. We compare our method with AvatarCraft and DreamAvatar using the rendered results provided by the authors. appearance. Specifically, our method can generate high-quality geometry and textures while preserving the character’s identity. It is worth noting that, our method enables the generation of avatars with loose garments and accessories, which cannot be achieved by CLIP-Actor [Youwang et al. 2022], AvatarCLIP [Hong et al. 2022], TEXTure [Richardson et al. 2023], AvatarCraft [Jiang et al. 2023b]. We believe that this is due to the fact that previous methods rely on the constraints from SMPL [Loper et al. 2015; Pavlakos et al. 2019] --- --A aX ar Fintuned Images Model produced by Ours Model produced by DreamBooth3D Figure 7: Qualitative comparison-II. We compare our method with DreamBooth3D in appearance-customized mode and hybrid mode. to maintain the avatar generation, while our method efficiently utilizes the vision-language prior and can generate a more generalized avatar model without heavy constraints from SMPL shape. DreamAvatar [Cao et al. 2023] is also free of the SMPL constraints, however, the appearance and geometry of its resulting model are less detailed than that of our results. We trained NeuS following the strategy of DreamBooth3D with the images above, and we find that DreamBooth3D can’t supervise Neural Surface Field to produce 3D assets consistent with the input images. This can be attributed to the inaccuracy of facial details generated from the Img2Img stage in DreamBooth3D. Simply changing the prompt. characters will share different identities in DreamBooth3D results. Compare with DreamBooth3D, our method produces results more faithful to the fine-tuned images. Besides, our results will keep the same identity under all kinds of textual descriptions. 4.3 Quantitative Evaluation User Study. To quantify the quality of the synthesized avatars, we conduct a user study to compare our results with generated ones from other state-of-the-art methods, i.e. CLIP-Actor, AvatarCLIP and TEXTure. We generate 10 avatars from randomly selected text prompts for each method and recruit 30 volunteers to evaluate the results w.r.t four different aspects: correspondence with the text prompts, appearance quality, geometry quality, and face fidelity. The volunteers are required to score from 1 (worst) to 5 (best) for each term. The results are shown in Fig. 8. Our method achieves the highest score over all four aspects, which proves that we are capable of generating avatars with more detailed appearance and geometry. Text-to-image metric. To the best of our knowledge, there are no metrics that can directly and quantitatively evaluate text-to3D generative models, therefore, we render the generated avatar models to images and then use a text-to-image metric for evaluation. Specifically, the models generated by our method and previous works are first rendered to 2000 images from 25 different viewpoints, then the avatar quality is compared by PickScore [Kirstain et al. 2023], which is a text-to-image metric that gauges the fidelity of generated content based on learned human preferences. As reported in Fig. 9, PickScores show that our method outperforms CLIP-Actor, AvatarCLIP, and TEXTure by a large margin, indicating that our results own better subjective quality. = Correspondence with Text ® Appearance Quality ™ Geometry Quality Face Fidelity 5.4.3.1.0.CLIP-Actor AvatarCLIP TEXTure Ours Figure 8: User Study. We investigated the user’s evaluation of our method and previous works w.r.t. correspondence with the text prompts, appearance quality, geometry quality, and face fidelity. Our method achieves optimal evaluation in all four metrics. vin = Tie [MM Lose Figure 9: Evaluations by PickScore. The results of PickScore demonstrate that our method (prompt generative mode) outperforms CLIP-Actor, AvatarCLIP, and TEXTure in visual quality. 4.4 Ablation Study To verify the effectiveness of each part of our proposed methods, we conduct ablation experiments to remove certain modules from the complete pipeline, then compare the performance for each setting. The description for each setting is shown below: A. Complete Pipeline. B. w/o Normal Map (Sec. 3.2): The normal maps are not rendered in the training. C. w/o Multi-Resolution SDS (Sec. 3.4): The multi-resolution SDS in the complete pipeline is replaced with the strategy of fixed high-resolution SDS. D. w/o Facial Supervision (Sec. 3.3): Only one diffusion model is used in SDS training targeting the full body. E. w/o Pose-Consistent Constraint (Sec. 3.4): The pose-consistent constraint is removed from the complete pipeline. Comparing A and B in Fig. 11, we observe that the removal of normal map supervision guidance in setting B leads to a significant loss of geometric details in the generated avatars, while the complete method with normal maps supervision in setting A produces high-quality avatars with improved details, including facial features and clothing wrinkles. These findings support the effectiveness of --- --ells Images of clothes [Y] ws é ;5. Images of person [X] et Elsa wearing [Y], in Pixar Style Taylor Swift wearing yellow [Y] [U] dressed up like [X] dressed in the police suit Captain Marvel ty [X] with yellow hair and in yellow suit F LX] in his fifties [U] wearing [Y] Obama wearing [V] Figure 10: Results in appearance customized / hybrid mode. We can see that the human appearance from input image sets is transferred to the generated avatar, even if the images are captured in free conditions. The customized avatar can be further modified according to the additional text prompts. our proposed normal map guidance strategy in generating realistic geometry from text descriptions. Comparing A and C in Fig. 11, we can see that the avatar generated by a fixed high-resolution upsampling setting contains unclear textures, wrong textures, or unreasonable geometric details, while these issues are eliminated after the multi-resolution SDS is implemented. We think the reason is that the multi-resolution SDS scheme learns multi-scale information in a coarse-to-fine manner, which helps generate a more detailed and clear 3D avatar. Comparing A and D in Fig. 11, it can be seen that the generated face supervised with a single model contains obvious artifacts on the head, and the facial geometry and appearance are less detailed. Besides, the removal of SDS loss for the head leads to an imbalance head-body ratio. By contrast, the avatar face generated under the supervision of dual models contains clear texture and detailed geometry. B.wioNormal C. w/o Multi-Resolution C. w/o Multi-Resolution _D. w/o Dual Model SDS (512 « 512) SDS (768 » 768) Finetuning ‘A.Complete Pipeline Map Figure 11: Ablation Study-I. The generated avatars under setting A-D are shown to verify the effectiveness of each module. E. w/o Pose-Consistent Constraint A. Complete Pipeline Figure 12: Ablation Study-II. The generated avatars under setting A and E are shown to verify the effectiveness of poseconsistent constraint. By comparing A and E in Fig. 12, we find that the surface of the avatar face tends to converge into a flat or even concave surface if the pose-consistent constraint is removed. We believe that this is because the head poses synthesized by the diffusion model with no pose-consistent constraint are conflicting, which leads to wrong geometry regression under multi-view photometric supervision. In contrast, the introduction of the pose-consistent constraint yields 3D avatars with more plausible geometry. 5 CONCLUSION In this paper, we propose a method for generating avatar models based on text prompts or free-captured image sets, or both. The human avatar to be synthesized is represented by a neural implicit surface and large pre-trained vision-language models are leveraged for the training of the model via score distillation sampling loss. The pose-consistent constraint is introduced to improve the accuracy of the avatar’s geometry and appearance. Dual model fine-tuning and multi-resolution SDS further boost the avatar quality and fidelity in text prompt mode or appearance customized mode. --- --There are still some limitations to our approach. The accuracy of the generated models still has the potential to be improved, and the speed of the fine-tuning and training phases still needs to be enhanced. In addition, we do not leverage 3D human datasets for training. Though the data amount of existing 3D human datasets is relatively limited, these priors are still expected to significantly improve the quality of avatar generation. Acknowledgement. This work was supported by the NSFC grant 62001213, 62025108, a gift funding from Huawei, and Tencent RhinoBird Research Program. REFERENCES Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan Chiang, Wan-Chun Ma, Chuan-Chang Wang, and Paul Debevec. 2010. The digital emily project: Achieving a photorealistic digital actor. IEEE Computer Graphics and Applications 30, 4 (2010), 20-31. Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, and Marcus Magnor. 2019. Tex2shape: Detailed full human body geometry from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2293-2303. Alexander Bergman, Petr Kellnhofer, Wang Yifan, Eric Chan, David Lindell, and Gordon Wetzstein. 2022. Generative neural articulated radiance fields. Advances in Neural Information Processing Systems 35 (2022), 19900-19916. Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee K Wong. 2023. DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models. arXiv preprint arXiv:2304.00916 (2023). Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2021. OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields. IEEE Transactions on Pattern Analysis & Machine Intelligence 43, 01 (2021), 172-186. Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition. 7291-7299. Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. 2022. Efficient geometry-aware 3D generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1612316133. Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. arXiv preprint arXiv:2303, 13873 (2023). Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems 34 (2021), 8780-8794. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022). Kaiwen Guo, Feng Xu, Tao Yu, Xiaoyang Liu, Qionghai Dai, and Yebin Liu. 2017. Realtime geometry, albedo, and motion reconstruction using a single rgb-d camera. ACM Transactions on Graphics (ToG) 36, 4 (2017), 1. Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang, Young-Jae Park, and HaeGon Jeon. 2023. High-fidelity 3D Human Digitization from Single 2K Resolution Images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. 2021. Arch++: Animation-ready clothed human reconstruction revisited. In Proceedings of the IEEE/CVF international conference on computer vision. 11046-11056. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems 33 (2020), 6840-6851. Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu. 2023b. Eva3d: Compositional 3d human generation from 2d image collections. In International Conference on Learning Representations. Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. 2022. AvatarCLIP: zero-shot text-driven generation and animation of 3D avatars. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1-19. Susung Hong, Donghoon Ahn, and Seungryong Kim. 2023a. Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation. arXiv preprint arXiv:2303, 15413 (2023). Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. 2020. Arch: Animatable reconstruction of clothed humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3093-3102. Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. 2023b. AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control. arXiv preprint arXiv:2303.(2023). Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wenzheng Chen, and Lan Xu. 2023a. HumanGen: Generating Human Radiance Fields with Explicit Priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of International Conference on Learning Representations (ICLR). Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. 2023. Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation. arXiv preprint arXiv:2305.01569 (2023). Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2022. Magic3D: HighResolution Text-to-3D Content Creation. arXiv preprint arXiv:2211.10440 (2022). Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. 2015. SMPL: A Skinned Multi-Person Linear Model. ACM Trans. Graphics (Proc. SIGGRAPH Asia) 34, 6 (Oct. 2015), 248:1-248:16. Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2022. Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. arXiv preprint arXiv:2211.07600 (2022). B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ramamoorthi, and R Ng. 2020. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference on computer vision. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. 2019. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 165-174. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. 2019. Expressive Body Capture: 3D Hands, Face, and Body from a Single Image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR). 10975-10985. Sida Peng, Yuanging Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. 2021. Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9054-9063. Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2023. Dreamfusion: Text-to-3d using 2d diffusion. In Proceedings of the International Conference on Learning Representations (ICLR). Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021a. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 8748-8763. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021b. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the International Conference on Machine Learning (ICML). 8748-8763. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. 2023. DreamBooth3D: Subject-Driven Text-to-3D Generation. arXiv preprint arXiv:2303.(2023). Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023. TEXTure: Text-Guided Texturing of 3D Shapes. ACM Trans. Graphics (Proc. SIGGRAPH) (2023). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjérn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1068410695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2022. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. 2022. Photorealistic Textto-Image Diffusion Models with Deep Language Understanding, In Advances in Neural Information Processing Systems, Vol. 35. 36479-36494. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. 2019. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF international conference on computer vision. 2304-2314. Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. 2020. Pifuhd: Multilevel pixel-aligned implicit function for high-resolution 3d human digitization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 84-93. Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young Chun. 2023. DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model. arXiv preprint arXiv:2304.02827 (2023). --- --Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising Diffusion Implicit Models. In International Conference on Learning Representations. Feitong Tan, Hao Zhu, Zhaopeng Cui, Siyu Zhu, Marc Pollefeys, and Ping Tan. 2020. Self-supervised human depth estimation from monocular videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 650-659. Gul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin Yumer, Ivan Laptev, and Cordelia Schmid. 2018. Bodynet: Volumetric inference of 3d human body shapes. In Proceedings of the European conference on computer vision (ECCV). 20-36. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. 2022. Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation. arXiv preprint arXiv:2212.00774 (2022). Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. 2021. NeuS: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction. Advances in Neural Information Processing Systems(2021), 27171-27183. Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. 2023. Rodin: A generative model for sculpting 3d digital avatars using diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4563-4573. Erroll Wood, Tadas Baltrugaitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie Shotton. 2021. Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the IEEE/CVF international conference on computer vision. 3681-3691. Menghua Wu, Hao Zhu, Linjia Huang, Yiyu Zhuang, Yuanxun Lu, and Xun Cao. 2023. High-Fidelity 3D Face Generation From Natural Language Descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4521-4530. Yunze Xiao, Hao Zhu, Haotian Yang, Zhengyu Diao, Xiangju Lu, and Xun Cao. 2022. Detailed facial geometry recovery from multi-view images by learning an implicit function. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 2839-2847. Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J Black. 2023. ECON: Explicit Clothed humans Optimized via Normal integration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J Black. 2022. ICON: implicit clothed humans obtained from normals. In Proceedings of the IEEE/CVF international conference on computer vision. 13286-13296. Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and Xun Cao. 2020. Facescape: a large-scale high quality 3d face dataset and detailed riggable 3d face prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 601-610. Kim Youwang, Kim Ji-Yeon, and Tae-Hyun Oh. 2022. CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. In Proceedings of the European conference on computer vision (ECCV). 173-191. Lvmin Zhang and Maneesh Agrawala. 2023. Adding conditional control to text-toimage diffusion models. arXiv preprint arXiv:2302.05543 (2023). Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and Jingyi Yu. 2023. DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance. arXiv preprint arXiv:2304.03117 (2023). Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. 2021. Pamir: Parametric modelconditioned implicit representation for image-based human reconstruction. IEEE transactions on pattern analysis and machine intelligence 44, 6 (2021), 3170-3184. Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. 2019. Deephuman: 3d human reconstruction from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 7739-7749. Hao Zhu, Yebin Liu, Jingtao Fan, Qionghai Dai, and Xun Cao. 2016. Video-based outdoor human reconstruction. IEEE Transactions on Circuits and Systems for Video Technology 27, 4 (2016), 760-770. Hao Zhu, Hao Su, Peng Wang, Xun Cao, and Ruigang Yang. 2018. View extrapolation of human body from a single image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 4450-4459. Hao Zhu, Haotian Yang, Longwei Guo, Yidi Zhang, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang Yang, and Xun Cao. 2021a. FacesCape: 3D facial dataset and benchmark for single-view 3D face reconstruction. arXiv preprint arXiv:2111.(2021). Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang Yang. 2019. Detailed human shape estimation from a single image by hierarchical mesh deformation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4491-4500. Hao Zhu, Xinxin Zuo, Haotian Yang, Sen Wang, Xun Cao, and Ruigang Yang. 2021b. Detailed avatar recovery from single image. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 11 (2021), 7363-7379.