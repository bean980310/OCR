arXiv:2305.06456v3 [cs.CV] 11 SepPerpetual Humanoid Control for Real-time Simulated Avatars Zhengyi Luo¹,Kris Kitani¹,Jinkun Cao² Alexander Winkler¹ ¹Reality Labs Research, Meta; 2Carnegie Mellon University https://zhengyiluo.github.io/PHC/ Weipeng Xu¹ A person waves the person walked forward and is picking up his toolbox. A person kicks with their left leg Figure 1: We propose a motion imitator that can naturally recover from falls and walk to far-away reference motion, perpetually controlling simulated avatars without requiring reset. Left: real-time avatars from video, where the blue humanoid recovers from a fall. Right: Imitating 3 disjoint clips of motion generated from language, where our controller fills in the blank. The color gradient indicates the passage of time. Abstract We present a physics-based humanoid controller that achieves high-fidelity motion imitation and fault-tolerant behavior in the presence of noisy input (e.g. pose estimates from video or generated from language) and unexpected falls. Our controller scales up to learning ten thousand motion clips without using any external stabilizing forces and learns to naturally recover from fail-state. Given reference motion, our controller can perpetually control simulated avatars without requiring resets. At its core, we propose the progressive multiplicative control policy (PMCP), which dynamically allocates new network capacity to learn harder and harder motion sequences. PMCP allows efficient scaling for learning from large-scale motion databases and adding new tasks, such as fail-state recovery, without catastrophic forgetting. We demonstrate the effectiveness of our controller by using it to imitate noisy poses from videobased pose estimators and language-based motion generators in a live and real-time multi-person avatar use case. 1. Introduction Physics-based motion imitation has captured the imagination of vision and graphics communities due to its potential for creating realistic human motion, enabling plausible environmental interactions, and advancing virtual avatar technologies of the future. However, controlling high-degree-of-freedom (DOF) humanoids in simulation presents significant challenges, as they can fall, trip, or deviate from their reference motions, and struggle to recover. For example, controlling simulated humanoids using poses estimated from noisy video observations can often lead humanoids to fall to the ground[50, 51, 22, 24]. These limitations prevent the widespread adoption of physics-based methods, as current control policies cannot handle noisy observations such as video or language. In order to apply physically simulated humanoids for avatars, the first major challenge is learning a motion imitator (controller) that can faithfully reproduce human-like motion with a high success rate. While reinforcement learning (RL)-based imitation policies have shown promising results, successfully imitating motion from a large dataset, such as AMASS (ten thousand clips, 40 hours of motion), with a single policy has yet to be achieved. Attempts to use larger or a mixture of expert policies have been met with some success [45, 47], although they have not yet scaled to the largest dataset. Therefore, researchers have resorted to using external forces to help stabilize the humanoid. Residual force control (RFC) [52] has helped to create motion imitators that can mimic up to 97% of the AMASS dataset [22], and has seen successful applications in human pose estimation from video[54, 23, 12] and language-based motion generation [53]. However, the external force compromises physical realism by acting as a “hand of God” that puppets the humanoid, leading to artifacts such as flying and floating. One might argue that, with RFC, the realism of simulation is compromised, as the model can freely apply a non-physical force on the humanoid. Another important aspect of controlling simulated humanoids is how to handle noisy input and failure cases. In this work, we consider human poses estimated from video or language input. Especially with respect to video input, artifacts such as floating [53], foot sliding [57], and physically impossible poses are prevalent in popular pose estimation methods due to occlusion, challenging view point and lighting, fast motions etc. To handle these cases, most physics-based methods resort to resetting the humanoid when a failure condition is triggered [24, 22, 51]. However, resetting successfully requires a high-quality reference pose, which is often difficult to obtain due to the noisy nature of the pose estimates, leading to a vicious cycle of falling and resetting to unreliable poses. Thus, it is important to have a controller that can gracefully handle unexpected falls and noisy input, naturally recover from failstate, and resume imitation. In this work, our aim is to create a humanoid controller specifically designed to control real-time virtual avatars, where video observations of a human user are used to control the avatar. We design the Perpetual Humanoid Controller (PHC), a single policy that achieves a high success rate on motion imitation and can recover from fail-state naturally. We propose a progressive multiplicative control policy (PMCP) to learn from motion sequences in the entire AMASS dataset without suffering catastrophic forgetting. By treating harder and harder motion sequences as a different "task" and gradually allocating new network capacity to learn, PMCP retains its ability to imitate easier motion clips when learning harder ones. PMCP also allows the controller to learn fail-state recovery tasks without compromising its motion imitation capabilities. Additionally, we adopt Adversarial Motion Prior (AMP)[35] throughout our pipeline and ensure natural and human-like behavior during fail-state recovery. Furthermore, while most motion imitation methods require both estimates of link position and rotation as input, we show that we can design controllers that require only the link positions. This input can be generated more easily by vision-based 3D keypoint estimators or 3D pose estimates from VR controllers. To summarize, our contributions are as follows: (1) we propose a Perpetual Humanoid Controller that can successfully imitate 98.9% of the AMASS dataset without applying any external forces; (2) we propose the progressive multiplicative control policy to learn from a large motion dataset without catastrophic forgetting and unlock additional capabilities such as fail-state recovery; (3) our controller is taskagnostic and is compatible with off-the-shelf video-based pose estimators as a drop-in solution. We demonstrate the capabilities of our controller by evaluating on both Motion Capture (MoCap) and estimated motion from videos. We also show a live (30 fps) demo of driving perpetually simulated avatars using a webcam video as input. 2. Related Works Physics-based Motion Imitation. Governed by the laws of physics, simulated characters [32, 31, 33, 35, 34, 7, 45, 52, 28, 13, 2, 11, 46, 12] have the distinct advantage of creating natural human motion, human-to-human interaction [20, 48], and human-object interactions [28, 34]. Since most modern physics simulators are not differentiable, training these simulated agents requires RL, which is time-consuming & costly. As a result, most of the work focuses on small-scale use cases such as interactive control based on user input [45, 2, 35, 34], playing sports [48, 20, 28], or other modular tasks (reaching goals [49], dribbling [35], moving around [32], etc.). On the other hand, imitating large-scale motion datasets is a challenging yet fundamental task, as an agent that can imitate reference motion can be easily paired with a motion generator to achieve different tasks. From learning to imitate a single clip [31] to datasets [47, 45, 7, 44], motion imitators have demonstrated their impressive ability to imitate reference motion, but are often limited to imitating high-quality MoCap data. Among them, ScaDiver [47] uses a mixture of expert policy to scale up to the CMU MoCap dataset and achieves a success rate of around 80% measured by time to failure. Unicon[45] shows qualitative results in imitation and transfer, but does not quantify the imitator's ability to imitate clips from datasets. MoCapAct[44] first learns single-clip experts on the CMU MoCap dataset, and distills them into a single that achieves around 80% of the experts' performance. The effort closest to ours is UHC [22], which successfully imitates 97% of the AMASS dataset. However, UHC uses residual force control [51], which applies a non-physical force at the root of the humanoid to help balance. Although effective in preventing the humanoid from falling, RFC reduces physical realism and creates artifacts such as floating and swinging, especially when motion sequences become challenging [22, 23]. Compared to UHC, our controller does not utilize any external force. Fail-state Recovery for Simulated Characters. As simulated characters can easily fall when losing balance, many approaches [39, 51, 34, 42, 7] have been proposed to help recovery. PhysCap [39] uses a floating-base humanoid that does not require balancing. This compromises physical realism, as the humanoid is no longer properly simulated. Egopose [51] designs a fail-safe mechanism to reset the humanoid to the kinematic pose when it is about to fall, leading to potential teleport behavior in which the humanoid keeps resetting to unreliable kinematic poses. NeruoMoCon [14] utilizes sampling-based control and reruns the sampling process if the humanoid falls. Although effective, this approach does not guarantee success and prohibits real-time use cases. Another natural approach is to use an additional recovery policy [7] when the humanoid has deviated from the reference motion. However, since such a recovery policy no longer has access to the reference motion, it produces unnatural behavior, such as high-frequency jitters. To combat this, ASE [34] demonstrates the ability to rise naturally from the ground for a sword-swinging policy. While impressive, in motion imitation the policy not only needs to get up from the ground, but also goes back to tracking the reference motion. In this work, we propose a comprehensive solution to the fail-state recovery problem in motion imitation: our PHC can rise from fallen state and naturally walks back to the reference motion and resume imitation. Progressive Reinforcement Learning. When learning from data containing diverse patterns, catastrophic forgetting [9, 27] is observed when attempting to perform multitask or transfer learning by fine-tuning. Various approaches [8, 16, 18] have been proposed to combat this phenomenon, such as regularizing the weights of the network [18], learning multiple experts [16], or increasing the capacity using a mixture of experts [56, 38, 47] or multiplicative control [33]. A paradigm has been studied in transfer learning and domain adaption as progressive learning [6, 4] or curriculum learning [1]. Recently, progressive reinforcement learning [3] has been proposed to distill skills from multiple expert policies. It aims to find a policy that best matches the action distribution of experts instead of finding an optimal mix of experts. Progressive Neural Networks (PNN) [36] proposes to avoid catastrophic forgetting by freezing the weights of the previously learned subnetworks and initializing additional subnetworks to learn new tasks. The experiences from previous subnetworks are forwarded through lateral connections. PNN requires manually choosing which subnetwork to use based on the task, preventing it from being used in motion imitation since reference motion does not have the concept of task labels. 3. Method We define the reference pose as ât = (Ôt, pt), consisting of 3D joint rotation Ô₁ = RJ×6 and position pt Є RJ×³ of all J links on the humanoid (we use the 6 DoF rotation representation [55]). From reference poses 1:T, one can compute the reference velocities 91.7 through finite difference, 1:T Δ where ât ± (ŵt, vt) consist of angular ŵ₁ € R³×³ and linear velocities ût Є RJ×³. We differentiate rotation-based and keypoint-based motion imitation by input: rotationbased imitation relies on reference poses 1:T (both rotation and keypoints), while keypoint-based imitation only requires 3D keypoints 1:T. As a notation convention, we use ~ to represent kinematic quantities (without physics simulation) from pose estimator/keypoint detectors, to denote ground truth quantities from Motion Capture (MoCap), and normal symbols without accents for values from the physics simulation. We use "imitate", "track”, and “mimic" reference motion interchangeably. In Sec.3.1, we first set up the preliminary of our main framework. Sec.3.2 describes our progressive multiplicative control policy to learn to imitate a large dataset of human motion and recover from fail-states. Finally, in Sec.3.3, we briefly describe how we connect our task-agnostic controller to off-the-shelf video pose estimators and generators for real-time use cases. 3.1. Goal Conditioned Motion Imitation with Adversarial Motion Prior = = Our controller follows the general framework of goalconditioned RL (Fig.3), where a goal-conditioned policy TPHC is tasked to imitate reference motion ĝ1:t or keypoints Î₁:T. Similar to prior work [22, 31], we formulate the task as a Markov Decision Process (MDP) defined by the tuple M (S, A, T,R,y) of states, actions, transition dynamics, reward function, and discount factor. The physics simulation determines state s₁ = S and transition dynamics T while our policy TPHC computes per-step action at € A. Based on the simulation state st and reference motion ật, the reward function R computes a reward rt R(St, ât) as the learning signal for our policy. The policy's goal is to maximize the discounted reward E Σ -1 -1 rt, and we use the proximal policy gradient (PPO) [37] to learn TPHC· State. The simulation state st (8,8%) consists of humanoid proprioception so and the goal state so. Proprioceptions (qt, ġt, 3) contains the 3D body pose qt, velocity qt, and (optionally) body shapes ẞ. When trained with different body shapes, ß contains information about the length of the limb of each body link [24]. For rotation-based motion imitation, the goal state so is defined as the difference between the next time step reference quantitives and their simulated counterpart: sg-rot± (Ôt+1¤¤¤‚ÂÎt+1¯Pt, Ût+1−Vt, ŵt where ‚ Pt+1) calculates the rotation difference. For keypointonly imtiation, the goal state becomes ¸²-kp ± (ît+1 − Pt, Ût+St Vt, Pt+1). All of the above quantities in sε and so are normalized with respect to the humanoid's current facing direction and root position [49, 22]. Reference Motion Simulated Motion Full dataset Primitive: P(1) Moderate difficulty ART. Hard Mining Primitive: p(2) ART ▶: Lateral connection or weight sharing Hard Mining: Hard negative mining by evaluation Fail recovery Hardest M Full dataset THE Hard Mining Primitive: p(K) Primitive: P(F) Composer: C TM Thaal -Training ProgressFigure 2: Our progressive training procedure to train primitives P(1), P(2), ..., P(K) by gradually learning harder and harder sequences. Fail recovery P (F) is trained in the end on simple locomotion data; a composer is then trained to combine these frozen primitives. Policy: TPHC Pretrained Primitives Action Composer: C p()p(2) pik) p(F) -a₁→ Weights: w ‚1:(K+1), Gating Function Motion Data: State: St Discriminator D Reference State Physics Simulation (Isaac Gym) Discriminator Reward amp Inference Training Goal Reward Learning (PPO) Figure 3: Goal-conditioned RL framework with Adversarial Motion Prior. Each primitive P(k) and composer C is trained using the same procedure, and here we visualize the final product πPHC. Reward. Unlike prior motion tracking policies that only use a motion imitation reward, we use the recently proposed Adversarial Motion Prior [35] and include a discriminator reward term throughout our framework. Including the discriminator term helps our controller produce stable and natural motion and is especially crucial in learning natural failstate recovery behaviors. Specifically, our reward is defined as the sum of a task reward r¾, a style reward r additional energy penalty r energy [31]: amp r = 0.5 +0.5t + rt . energy ampand an (1) For the discriminator, we use the same observations, loss formulation, and gradient penalty as AMP [35]. The energy penalty is expressed as -0.0005 Σje joints | Mjwj|² where μ; and w; correspond to the joint torque and the joint angular velocity, respectively. The energy penalty [10] regulates the policy and prevents high-frequency jitter of the foot that can manifest in a policy trained without external force (see Sec.4.1). The task reward is defined based on the current training objective, which can be chosen by switching the reward function for motion imitation Rimitation and fail-state recovery Recover. For motion tracking, we use: g-imitation р = Rimitation -10||âtqt|| + Wire (8t, âu) Wjpe + Wive -100 || pt-pt || (2) -0.1||t-vt || -0.1||ŵt-wt|| + Wjwe where we measure the difference between the translation, rotation, linear velocity, and angular velocity of the rigid body for all links in the humanoid. For fail-state recovery, we define the reward in Eq.3. rt g-recover = Action. We use a proportional derivative (PD) controller at each DoF of the humanoid and the action at specifies the PD target. With the target joint set as q = at, the torque applied at each joint is ₁ = k³ (at — qt) - kdo ġt. Notice that this is different from the residual action representation [52, 22, 30] used in prior motion imitation methods, where the action is added to the reference pose: qt=ât at to speed up training. As our PHC needs to remain robust to noisy and ill-posed reference motion, we remove such a dependency on reference motion in our action space. We do not use any external forces [52] or meta-PD control[54]. Control Policy and Discriminator. Our control policy TPHC(@t]8t) N(μ(st), σ) represents a Gaussian distribution with fixed diagonal covariance. The AMP discriminator D(S 10:t) computes a real and fake value based on the current prioproception of the humanoid. All of our networks (discriminator, primitive, value function, and discriminator) are two-layer multilayer perceptrons (MLP) with dimensions [1024, 512]. Humanoid. Our humanoid controller can support any human kinematic structure, and we use the SMPL [21] kinematic structure following prior arts [54, 22, 23]. The SMPL body containsrigid bodies, of which 23 are actuated, resulting in an action space of at Є R23×3. The body proportion can vary based on a body shape parameter ẞ = R10. Initialization and Relaxed Early Termination. We use reference state initialization (RSI) [31] during training and randomly select a starting point for a motion clip for imitation. For early termination, we follow UHC [22] and terminate the episode when the joints are more than 0.5 meters globally on average from the reference motion. Unlike UHC, we remove the ankle and toe joints from the termination condition. As observed by RFC [52], there exists a dynamics mismatch between simulated humanoids and real humans, especially since the real human foot is multisegment [29]. Thus, it is not possible for the simulated humanoid to have the exact same foot movement as MoCap, and blindly following the reference foot movement may lead to the humanoid losing balance. Thus, we propose Relaxed Early Termination (RET), which allows the humanoid's ankle and toes to slightly deviate from the MoCap motion to remain balanced. Notice that the humanoid still receives imitation and discriminator rewards for these body parts, which prevents these joints from moving in a nonhuman manner. We show that though this is a small detail, it is conducive to achieving a good motion imitation success rate. Hard Negative Mining. When learning from a large motion dataset, it is essential to train on harder sequences in the later stages of training to gather more informative experiences. We use a similar hard negative mining procedure as in UHC [22] and define hard sequences by whether or not our controller can successfully imitate this sequence. From a motion dataset Q, we find hard sequences hard ≤ Q by evaluating our model over the entire dataset and choosing sequences that our policy fails to imitate. 3.2. Progressive Multiplicative Control Policy As training continues, we notice that the performance of the model plateaus as it forgets older sequences when learning new ones. Hard negative mining alleviates the problem to a certain extent, yet suffers from the same issue. Introducing new tasks, such as fail-state recovery, may further degrade imitation performance due to catastrophic forgetting. These effects are more concretely categorized in the Appendix (App. C). Thus, we propose a progressive multiplicative control policy (PMCP), which allocates new subnetworks (primitives P) to learn harder sequences. Progressive Neural Networks (PNN). A PNN [36] starts with a single primitive network P (1) trained on the full dataset Q. Once P (1) is trained to convergence on the entire motion dataset Q using the imitation task, we create a subset of hard motions by evaluating P(¹) on Q. We define convergence as the success rate on Q() no longer increases. The sequences that P (1) fails on is formed as Q. We then freeze the parameters of P(1) and create a new primitive P(2) (randomly initialized) along with lateral connections that connect each layer of P(¹) to P (2). For more information about PNN, please refer to our supplementary material. hard (1) hard hard During training, we construct each Q (K)d by selecting the failed sequences from the previous step Q resulting in a smaller (k) (k-1) hard " (k+1) and smaller hard subset: Qhard Chard ). In this way, we ensure that each newly initiated primitive P(k) is responsible for learning a new and harder subset of motion sequences, as can be seen in Fig.2. Notice that this is different from hard-negative mining in UHC [22], as we initialize a new primitive P to train. Since the original PNN is proposed to solve completely new tasks (such as different Atari games), a lateral connection mechanism is proposed to allow later tasks to choose between reuse, modify, or discard prior experiences. However, mimicking human motion is highly correlated, where fitting to harder sequences Q ( can (k) effectively draw experiences from previous motor control experiences. Thus, we also consider a variant of PNN where there are no lateral connections, but the new primitives are initialized from the weights of the prior layer. This weight sharing scheme is similar to fine-tuning on the harder motion sequences using a new primitive P(+1) and preserve P(k)'s ability to imitate learned sequences. hard Algo 1: Learn Progressive Multiplicative Control Policy 1 Function TrainPPO (π, Q (k), D, V, R):while not converged do Minitialize sampling memory; while M not full doP (k)1:T fort sample motion from Q; 1...T do St← (8), s); at π(at St) ; St+1T(8+1 St, at) // simulation%;B rt R(st, ĝt+1) ; store (st, at, rt, st+1) into memory M ; VPPO update using experiences collected in M ; D← Discriminator update using experiences collected in M return; 15 Input: Ground truth motion dataset Q; 16 D, V, Q← Q (1) hard function, and 17 for k 1... K do // Initialize discriminator, value dataset ; Initialize P(k) Lateral connection/weight sharing%;B (k) ← TrainPPO (P(k), Q (k+1), D, V, Rimitation); hard (k) );(k+1) ←eval(P(k), P(k) <freeze P(k) loco D, V, Recover)22 P(F) TrainPPO (P // Fail-state Recovery%;B P(F) 23 TPHC Á {P(¹)... p(K) ‚c} ; 24 TPHC – TrainPPO(TPHC, Q, D, V, {R // Train Composer; imitation Rrecovery }) Fail-state Recovery. In addition to learning harder sequences, we also learn new tasks, such as recovering from fail-state. We define three types of fail-state: 1) fallen on the ground; 2) faraway from the reference motion (> 0.5m); 3) their combination: fallen and faraway. In these situations, the humanoid should get up from the ground, approach the reference motion in a natural way, and resume motion imitation. For this new task, we initialize a primitive P (F) at the end of the primitive stack. P(F) shares the same input and output space as P(1) ...P(k), but since the reference motion does not provide useful information about failstate recovery (the humanoid should not attempt to imitate the reference motion when lying on the ground), we modify the state space during fail-state recovery to remove all information about the reference motion except the root. For the reference joint rotation ột = [ột, ỗ½, · · · ] where corresponds to the ¿th joint, we construct = [0, 0, 0] where all joint rotations except the root are replaced with simulated values (without). This amounts to setting the non-root joint goals to be identity when computing s the goal states: sg-Fail = (-0, pt-pt,ốt ớt, cất tốt, pi). 88-Fail thus collapse from an imitation objective to a point-goal [49] objective where the only information provided is the relative position and orientation of the target root. When the reference root is 5x (pt-pt) too far (> 5m), we normalize pt – pt as to clamp the ||pt-pt ||goal position. Once the humanoid is close enough (e.g. < 0.5m ), the goal will switch back to full-motion imitation: = g-Fail -Fail || P||2 ≤0.otherwise. (3) To create fallen states, we follow ASE [34] and randomly drop the humanoid on the ground at the beginning of the episode. The (a) MoCap Motion Imitation (b) Fail-state Recovery (b) Nosiy Motion Imitation (Video: H36M dataset, pose estimation from HybriK) (c) Nosiy Motion Imitation (Language: prompt "a person runs and turns') (d) Nosiy Motion Imitation (Video: real-time & live, pose estimation from MeTRAbs) Figure 4: (a) Imitating high-quality MoCap – spin and kick. (b) Recover from fallen state and go back to reference motion (indicated by red dots). (b) Imitating noisy motion estimated from video. (c) Imitating motion generated from language. (d) Using poses estimated from a webcam stream for a real-time simulated avatar. faraway state can be created by initializing the humanoid 2 ~meters from the reference motion. The reward for fail-state recovery consists of the AMP reward r, point-goal reward 7-8-point , and energy penality energy, calculated by the reward function Rrecover: amp t t As each P(k) is an independent Gaussian, the action distribution: Ci (St) kN Σ C₁₂ (st) Ci (St) µ³¹² (St), σ³ (St) = σ(St) (st) (Σ of (81) (6) g-recover recover πt = R (St,Ô4t) = g-point 0.5r-P +0.50.1r energy " = (4) g-point The point-goal reward is formulated as r (dt-1 - dt) where dt is the distance between the root reference and simulated root at the time step t [49]. For training P (F), we use a handpicked subset of the AMASS dataset named Q loco where it contains mainly walking and running sequences. Learning using only Qloco coaxes the discriminator D and the AMP reward r‍ª amp to bias toward simple locomotion such as walking and running. We do not initialize a new value function and discriminator while training the primitives and continuously fine-tune the existing ones. Multiplicative Control. Once each primitive has been learned, we obtain {P (1).P(K), P(F)}, with each primitive capable of imitating a subset of the dataset Q. In Progressive Networks [36], task switching is performed manually. In motion imitation, however, the boundary between hard and easy sequences is blurred. Thus, we utilize Multiplicative Control Policy (MCP) [33] and train an additional composer C to dynamically combine the learned primitives. Essentially, we use the pretrained primitives as a informed search space for the composer C, and C only needs to select which primitives to activate for imitation. Specifically, our composer C (w1:K+1 st) consumes the same input as the primitives and outputs a weight vector wi ЄR+1 to activate the primitives. Combining our composer and primitives, we have the PHC's output distribution: 1:K+kTPHC(@t | 8t) (i) = 1) (a (1) | St) C (st), C(st) ≥ 0. (5) II C(St)where μ(st) corresponds to the P()'s jth action dimension. Unlike a Mixture of Expert policies that only activates one at a time (top-1 MOE), MCP combines the actors' distribution and activates all actors at the same (similar to top-inf MOE). Unlike MCP, we progressively train our primitives and make the composer and actor share the same input space. Since primitives are independently trained for different harder sequences, we observe that the composite policy sees a significant boost in performance. During composer training, we interleave fail-state recovery training. The training process is described in Alg.1 and Fig.2. 3.3. Connecting with Motion Estimators Our PHC is task-agnostic as it only requires the next timestep reference pose Ĩt or the keypoint pt for motion tracking. Thus, we can use any off-the-shelf video-based human pose estimator or generator compatible with the SMPL kinematic structure. For driving simulated avatars from videos, we employ HybrIK [19] and MeTRAbs [41, 40], both of which estimate in the metric space with the important distinction that HybrIK outputs joint rotation + while MeTRAbs only outputs 3D keypoints pt. For language-based motion generation, we use the Motion Diffusion Model (MDM) [43]. MDM generates disjoint motion sequences based on prompts, and we use our controller's recovery ability to achieve in-betweening. 4. Experiments We evaluate and ablate our humanoid controller's ability to imitate high-quality MoCap sequences and noisy motion sequences estimated from videos in Sec.4.1. In Sec.4.2, we test our controller's ability to recovery from fail-state. As motion is best in Table 1: Quantitative results on imitating MoCap motion sequences (* indicates removing sequences containing human-object interaction). AMASS-Train*, AMASS-Test*, and H36M-Motion* contains 11313, 140, and 140 high-quality MoCap sequences, respectively. AMASS-Train* AMASS-Test* H36M-Motion* Method RFC Succ↑ Eg-mpjpe↓ Empipe↓ Eacc↓ Evel↓ Succ ↑ Eg-mpipe↓ Empjpe↓ EaccEvel↓ Succ↑ Eg-mpipe Empipe↓ Eacc↓ Evel↓ UHC ✓ 97.0 % 36.25.4.5.96.4 % 50.31.9.7 12.1 87.0% 59.35.4.7.UHC Χ 84.5 % 62.39.10.10.62.6% 58.98.Ours Ours-kp 98.9 % 37.26.3.X 98.7% 40.32.3.4.9 96.4% 5.5 97.1% 47.30.6.53.39.7.22.8 21.9.10.23.6% 92.9% 95.7% 133.67.14.17.50.33.3.5.49.39.3.5.Table 2: Motion imitation on noisy motion. We use HybrIK[19] to estimate the joint rotations ết and uses MeTRAbs [41] for global 3D keypoints pt. HybrIK + MeTRAbs (root): using joint rotations + from HybrIK and root position po from MeTRAbs. MeTRAbs (all keypoints): using all keypoints pt from MeTRAbs, only applicabile to our keypoint-based controller. H36M-Test-Video* Method RFC UHC ✓ Pose Estimate HybrIK + MeTRAbs (root) Succ↑ Eg-mpjpe↓ Empipe↓ 58.1% 75.49.UHC Ours Ours-kp Ours-kp X HybrIK + MeTRAbs (root) 18.1% 126.67.× HybrIK + MeTRAbs (root) 88.7% 55.34.X HybrIK + MeTRAbs (root) MeTRAbs (all keypoints) 90.0% 55.41.91.9% 55.41.videos, we provide extensive qualitative results in the supplementary materials. All experiments are run three times and averaged. Baselines. We compare with the SOTA motion imitator UHC [22] and use the official implementation. We compare against UHC both with and without residual force control. Implementation Details. We uses four primitives (including failstate recovery) for all our evaluations. PHC can be trained on a single NVIDIA A100 GPU; it takes around a week to train all primitives and the composer. Once trained, the composite policy runs at > 30 FPS. Physics simulation is carried out in NVIDIA's Isaac Gym [26]. The control policy is run at 30 Hz, while simulation runs at 60 Hz. For evaluation, we do not consider body shape variation and use the mean SMPL body shape. Datasets. PHC is trained on the training split of the AMASS [25] dataset. We follow UHC [22] and remove sequences that are noisy or involve interactions of human objects, resulting in 11313 high-quality training sequences and 140 test sequences. To evaluate our policy's ability to handle unseen MoCap sequences and noisy pose estimate from pose estimation methods, we use the popular H36M dataset [15]. From H36M, we derive two subsets H36M-Motion* and H36M-Test-Video*. H36M-Motion* contains 140 high-quality MoCap sequences from the entire H36M dataset. H36M-Test-Video* contains 160 sequences of noisy poses estimated from videos in the H36M test split (since SOTA pose estimation methods are trained on H36M's training split). * indicates the removal of sequences containing human-chair interaction. Metrics. We use a series of pose-based and physics-based metrics to evaluate our motion imitation performance. We report the success rate (Succ) as in UHC [22], deeming imitation unsuccessful when, at any point during imitation, the body joints are on average > 0.5m from the reference motion. Succ measures whether the humanoid can track the reference motion without losing balance or significantly lags behind. We also report the root-relative mean per-joint position error (MPJPE) Empipe and the global MPJPE Eg-mpjpe (in mm), measuring our imitator's ability to imitate the reference motion both locally (root-relative) and globally. To show physical realism, we also compare acceleration Eace (mm/frame²) and velocity Evel (mm/frame) difference between simulated and MoCap motion. All the baseline and our methods are physically simulated, so we do not report any foot sliding or penetration. 4.1. Motion Imitation Motion Imitation on High-quality MoCap. Tablel reports our motion imitation result on the AMASS train, test, and H36MMotion* dataset. Comparing with the baseline with RFC, our method outperforms it on almost all metrics across training and test datasets. On the training dataset, PHC has a better success rate while achieving better or similar MPJPE, showcasing its ability to better imitate sequences from the training split. On testing, PHC shows a high success rate on unseen MoCap sequences from both the AMASS and H36M data. Unseen motion poses additional challenges, as can be seen in the larger per-joint error. UHC trained without residual force performs poorly on the test set, showing that it lacks the ability to imitate unseen reference motion. Noticeably, it also has a much larger acceleration error because it uses high-frequency jitter to stay balanced. Compared to UHC, our controller has a low acceleration error even when facing unseen motion sequences, benefiting from the energy penalty and motion prior. Surprisingly, our keypoint-based controller is on par and sometimes outperforms the rotation-based one. This validates that the keypoint-based motion imitator can be a simple and strong alternative to the rotation-based ones. Motion Imitation on Noisy Input from Video. We use off-theshelf pose estimators HybrIK [19] and MeTRAbs [41] to extract joint rotation (HybrIK) and keypoints (MeTRAbs) using images from the H36M test set. As a post-processing step, we apply a Gaussian filter to the extracted pose and keypoints. Both HyBrIK and MeTRAbs are per-frame models that do not use any temporal information. Due to depth ambiguity, monocular global pose estimation is highly noisy [41] and suffers from severe depth-wise jitter, posing significant challenge to motion imitators. We find that MeTRAbs outputs better global root estimation Ã, so we use its p combined with HybrIK's estimated joint rotation ỗt (HybrIK + Metrabs (root)). In Table2, we report our controller and baseline's performance on imitating these noisy sequences. Similar to results on MoCap Imitation, PHC outperforms the baselines Table 3: Ablation on components of our pipeline, performed using noisy pose estimate from HybrIK + Metrabs (root) on the H36M-Test-Video* data. RET: relaxed early termination. MCP: multiplicative control policy. PNN: progressive neural networks. Table 4: We measure whether our controller can recover from the fail-states by generating these scenarios (dropping the humanoid on the ground & far from the reference motion) and measuring the time it takes to resume tracking. H36M-Test-Video* RET MCP PNN Rotation Fail-Recover Succ↑ Eg-mpipe↓ Empjpe↓ Method Ours Fallen-State Succ-5s ↑ 95.0% Far-State 51.2% 56.34.✓ X X 59.4% 60.37.Ours-kp 92.5% 98.8% 94.6% Succ-5s ↑ Succ-10s ↑ 83.7% 95.1% Succ-10s ↑ 99.5% 96.0% Fallen Far-State Succ-5s ↑ Succ-10s ↑ 93.4% 98.8% 79.4% 93.2% 66.2% 59.38.✓ ✓ ✓ ✓ 86.9% 53.33.✓ 88.7% 55.34.✓ 90.0% 55.41.by a large margin and achieves a high success rate (~90%). This validates our hypothesis that PHC is robust to noisy motion and can be used to drive simulated avatars directly from videos. Similarly, we see that keypoint-based controller (ours-kp) outperforms rotation-based, which can be explained by 1) estimating 3D keypoint directly from images is an easier task than estimating joint rotations, so keypoints from MeTRABS are of higher quality than joint rotations from HybrIK; 2) our keypoint-based controller is more robust to noisy input as it has the freedom to use any joint configuration to try to match the keypoints. Ablations. Table3 shows our controller trained with various components disabled. We perform ablation on the noisy input from H36M-Test-Image* to better showcase the controller's ability to imitate noisy data. First, we study the performance of our controller before training to recover from fail-state. Comparing row(R1) and R2, we can see that relaxed early termination (RET) allows our policy to better use the ankle and toes for balance. R2 vs R3 shows that using MCP directly without our progressive training process boosts the network performance due to its enlarged network capacity. However, using the PMCP pipeline significantly boosts robustness and imitation performance (R3 vs. R4). Comparing R4 and R5 shows that PMCP is effective in adding fail-state recovery capability without compromising motion imitation. Finally, R5 vs. R6 shows that our keypoint-based imitator can be on-par with rotation-based ones, offering a simpler formulation where only keypoints is needed. For additional ablation on MOE vs. MCP, number of primitives, please refer to the supplement. Real-time Simulated Avatars. We demonstrate our controller's ability to imitate pose estimates streamed in real-time from videos. Fig.4 shows a qualitative result on a live demonstration of using poses estimated from an office environment. To achieve this, we use our keypoint-based controller and MeTRAbs-estimated keypoints in a streaming fashion. The actor performs a series of motions, such as posing and jumping, and our controller can remain stable. Fig.4 also shows our controller's ability to imitate reference motion generated directly from a motion language model MDM [43]. We provide extensive qualitative results in our supplementary materials for our real-time use cases. 4.2. Fail-state Recovery To evaluate our controller's ability to recover from fail-state, we measure whether our controller can successfully reach the reference motion within a certain time frame. We consider three scenarios: 1) fallen on the ground, 2) far away from reference motion, and 3) fallen and far from reference. We use a single clip of standing-still reference motion during this evaluation. We generate fallen-states by dropping the humanoid on the ground and applying random joint torques for 150 time steps. We create the far-state by initializing the humanoid 3 meters from the reference motion. Experiments are run randomly 1000 trials. From Tab.4 we can see that both of our keypoint-based and rotation-based controllers can recover from fall state with high success rate (>90%) even in the challenging scenario when the humanoid is both fallen and far away from the reference motion. For a more visual analysis of fail-state recovery, see our supplementary videos. 5. Discussions Limitations. While our purposed PHC can imitate human motion from MoCap and noisy input faithfully, it does not achieve a 100% success rate on the training set. Upon inspection, we find that highly dynamic motions such as high jumping and back flipping are still challenging. Although we can train single-clip controller to overfit on these sequences (see the supplement), our full controller often fails to learn these sequences. We hypothesize that learning such highly dynamic clips (together with simpler motion) requires more planning and intent (e.g. running up to a high jump), which is not conveyed in the single-frame pose target t+1 for our controller. The training time is also long due to our progressive training procedure. Furthermore, to achieve better downstream tasks, the current disjoint process (where the video estimator pose is unaware of the physics simulation) may be insufficient; tighter integration with pose estimation [54, 23] and language-based motion generation [53] is needed. Conclusion and Future Work. We introduce Perpetual Humanoid Controller, a general purpose physics-based motion imitator that achieves high quality motion imitation while being able to recover from fail-states. Our controller is robust to noisy estimated motion from video and can be used to perpetually simulate a real-time avatar without requiring reset. Future directions include 1) improving imitation capability and learning to imitate 100% of the motion sequences of the training set; 2) incorporating terrain and scene awareness to enable human-object interaction; 3) tighter integration with downstream tasks such as pose estimation and motion generation, etc. Acknowledgements. We thank Zihui Lin for her help in making the plots in this paper. Zhengyi Luo is supported by the Meta AI Mentorship (AIM) program. References [1] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, pages 41-48, New York, NY, USA, June 2009. Association for Computing Machinery. [2] Kevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. DReCon: Data-driven responsive control of physics-based characters. ACM Trans. Graph., 38(6):11, 2019. [3] Glen Berseth, Cheng Xie, Paul Cernek, and Michiel Van de Panne. Progressive reinforcement learning with distillation for multi-skilled motion control. Feb. 2018. [4] Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, and Yu-Wing Tai. Cross-domain adaptation for animal pose estimation. Aug. 2019. [5] Jinkun Cao, Xinshuo Weng, Rawal Khirodkar, Jiangmiao Pang, and Kris Kitani. Observation-centric SORT: Rethinking SORT for robust multi-object tracking. Mar. 2022. [6] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou Huang. Progressive feature alignment for unsupervised domain adaptation. Nov. 2018. [7] Nuttapong Chentanez, Matthias Müller, Miles Macklin, Viktor Makoviychuk, and Stefan Jeschke. Physics-based motion capture imitation with deep reinforcement learning. Proceedings - MIG 2018: ACM SIGGRAPH Conference on Motion, Interaction, and Games, 2018. [8] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE Trans. Pattern Anal. Mach. Intell., 44(7):3366–3385, July 2022. [9] Robert M French and Nick Chater. Using noise to compute error surfaces in connectionist networks: a novel means of reducing catastrophic forgetting. Neural Comput., 14(7):1755-1769, July 2002. [10] Zipeng Fu, Xuxin Cheng, and Deepak Pathak. Deep wholebody control: Learning a unified policy for manipulation and locomotion. Oct. 2022. [11] Levi Fussell, Kevin Bergamin, and Daniel Holden. SuperTrack: motion tracking for physically simulated characters using supervised learning. ACM Trans. Graph., 40(6):1–13, Dec. 2021. [12] Kehong Gong, Bingbing Li, Jianfeng Zhang, Tao Wang, Jing Huang, Michael Bi Mi, Jiashi Feng, and Xinchao Wang. PoseTriplet: Co-evolving 3D human pose estimation, imitation, and hallucination under self-supervision. CVPR, Mar. 2022. [13] Leonard Hasenclever, Fabio Pardo, Raia Hadsell, Nicolas Heess, and Josh Merel. CoMic: Complementary task learning & mimicry for reusable skills. http://proceedings.mlr.press/v119/ hasenclever20a/hasenclever20a.pdf. cessed: 2023-2-13. Ac[14] Buzhen Huang, Liang Pan, Yuan Yang, Jingyi Ju, and Yangang Wang. Neural MoCon: Neural motion control for physically plausible human motion capture. Mar. 2022. [15] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments. IEEE Trans. Pattern Anal. Mach. Intell., 36(7):1325-1339, 2014. [16] Zhiwei Jia, Xuanlin Li, Zhan Ling, Shuang Liu, Yiran Wu, and Hao Su. Improving policy optimization with generalistspecialist learning. June 2022. [17] Jocher, Glenn and Chaurasia, Ayush and Qiu, Jing. Yolov8. [18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka GrabskaBarwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proc. Natl. Acad. Sci. U. S. A., 114(13):3521-3526, 2017. [19] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. HybrIK: A hybrid analytical-neural inverse kinematics solution for 3D human pose and shape estimation. Nov. 2020. [20] Siqi Liu, Guy Lever, Zhe Wang, Josh Merel, S M Ali Eslami, Daniel Hennes, Wojciech M Czarnecki, Yuval Tassa, Shayegan Omidshafiei, Abbas Abdolmaleki, Noah Y Siegel, Leonard Hasenclever, Luke Marris, Saran Tunyasuvunakool, H Francis Song, Markus Wulfmeier, Paul Muller, Tuomas Haarnoja, Brendan D Tracey, Karl Tuyls, Thore Graepel, and Nicolas Heess. From motor control to team play in simulated humanoid football. May 2021. [21] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. SMPL: A skinned multiperson linear model. ACM Trans. Graph., 34(6), 2015. [22] Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani. Dynamics-regulated kinematic policy for egocentric pose estimation. NeurIPS, 34:25019-25032, 2021. [23] Zhengyi Luo, Shun Iwase, Ye Yuan, and Kris Kitani. Embodied scene-aware human pose estimation. NeurIPS, June 2022. [24] Zhengyi Luo, Ye Yuan, and Kris M Kitani. From universal humanoid control to automatic physically valid character creation. June 2022. [25] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. AMASS: Archive of motion capture as surface shapes. Proceedings of the IEEE International Conference on Computer Vision, 2019Octob:5441-5450, 2019. [26] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance GPU-based physics simulation for robot learning. Aug. 2021. [27] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Gordon H Bower, editor, Psychology of Learning and Motivation, volume 24, pages 109–165. Academic Press, Jan. 1989. [28] Josh Merel, Saran Tunyasuvunakool, Arun Ahuja, Yuval Tassa, Leonard Hasenclever, Vu Pham, Tom Erez, Greg Wayne, and Nicolas Heess. Catch and carry: Reusable neural controllers for vision-guided whole-body tasks. ACM Trans. Graph., 39(4), 2020. [29] Hwangpil Park, Ri Yu, and Jehee Lee. Multi-segment foot modeling for human animation. In Proceedings of the 11th Annual International Conference on Motion, Interaction, and Games, number Article 16 in MIG '18, pages 1-10, New York, NY, USA, Nov. 2018. Association for Computing Machinery. [30] Soohwan Park, Hoseok Ryu, Seyoung Lee, Sunmin Lee, and Jehee Lee. Learning predict-and-simulate policies from unorganized human motion data. ACM Trans. Graph., 38(6):1– 11, Nov. 2019. [31] Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. DeepMimic. ACM Trans. Graph., 37(4):1– 14, 2018. [32] Xue Bin Peng, Glen Berseth, Kangkang Yin, and Michiel Van De Panne. DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Trans. Graph., 36(4):1-13, July 2017. [33] Xue Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine. MCP: Learning composable hierarchical control with multiplicative compositional policies. May 2019. [34] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. ASE: Large-scale reusable adversarial skill embeddings for physically simulated characters. May 2022. [35] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. AMP: Adversarial motion priors for stylized physics-based character control. ACM Trans. Graph., (4):1-20, Apr. 2021. [36] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv [cs.LG], June 2016. [37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. Technical report, 2017. [38] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. arXiv [cs.LG], Jan. 2017. [39] Soshi Shimada, Vladislav Golyanik, Weipeng Xu, and Christian Theobalt. PhysCap: Physically plausible monocular 3D motion capture in real time. (1), Aug. 2020. [40] István Sárándi, Alexander Hermans, and Bastian Leibe. Learning 3D human pose estimation from dozens of datasets using a geometry-aware autoencoder to bridge between skeleton formats. Dec. 2022. [41] István Sárándi, Timm Linder, Kai O Arras, and Bastian Leibe. MeTRAbs: Metric-scale truncation-robust heatmaps for absolute 3D human pose estimation. arXiv, pages 1-14, 2020. [42] Tianxin Tao, Matthew Wilson, Ruiyu Gou, and Michiel van de Panne. Learning to get up. arXiv [cs.GR], Apr. 2022. [43] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano, and Daniel Cohen-Or. Human motion diffusion model. arXiv [cs.CV], Sept. 2022. [44] Nolan Wagener, Andrey Kolobov, Felipe Vieira Frujeri, Ricky Loynd, Ching-An Cheng, and Matthew Hausknecht. MoCapAct: A multi-task dataset for simulated humanoid control. Aug. 2022. [45] Tingwu Wang, Yunrong Guo, Maria Shugrina, and Sanja Fidler. UniCon: Universal neural controller for physics-based character motion. arXiv, 2020. [46] Alexander Winkler, Jungdam Won, and Yuting Ye. QuestSim: Human motion tracking from sparse sensors with simulated avatars. Sept. 2022. [47] Jungdam Won, Deepak Gopinath, and Jessica Hodgins. A scalable approach to control diverse behaviors for physically simulated characters. ACM Trans. Graph., 39(4), 2020. [48] Jungdam Won, Deepak Gopinath, and Jessica Hodgins. Control strategies for physically simulated characters performing two-player competitive sports. ACM Trans. Graph., 40(4):1– 11, July 2021. [49] Jungdam Won, Deepak Gopinath, and Jessica Hodgins. Physics-based character controllers using conditional VAES. ACM Trans. Graph., 41(4):1–12, July 2022. [50] Ye Yuan and Kris Kitani. 3D ego-pose estimation via imitation learning. In Computer Vision - ECCV 2018, volume 11220 LNCS, pages 763-778. Springer International Publishing, 2018. [51] Ye Yuan and Kris Kitani. Ego-pose estimation and forecasting as real-time PD control. Proceedings of the IEEE International Conference on Computer Vision, 2019Octob:10081-10091, 2019. [52] Ye Yuan and Kris Kitani. Residual force control for agile human behavior imitation and extended motion synthesis. (NeurIPS), June 2020. [53] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. PhysDiff: Physics-guided human motion diffusion model. arXiv [cs.CV], Dec. 2022. [54] Ye Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason Saragih. SimPoE: Simulated character control for 3D human pose estimation. CVPR, Apr. 2021. [55] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2019June: 5738-5746, 2019. [56] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing. Feb. 2022. [57] Yuliang Zou, Jimei Yang, Duygu Ceylan, Jianming Zhang, Federico Perazzi, and Jia-Bin Huang. Reducing footskate in human motion reconstruction with ground contact constraints. In 2020 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, Mar. 2020. Appendix A Introduction B. Implementation Details B.1. Training Details .B.2. Real-time Use CasesB.3. Progressive Neural Network (PNN) DetailsC. Supplementary ResultsC.1. Categorizing the Forgetting ProblemC.2. Additional AblationsFigure 5: Our framework can support body shape and gender variations. Here we showcase humanoids of different gender and body proportion holding a standing pose. We construct two kinda of humanoids: capsule-based (top) and mesh-based (bottom). Red: female, Blue: male. Color gradient indicates weight. D. Extended Limitation and Discussions Appendices A. Introduction In this document, we include additional details and results that are not included in the paper due to the page limit. In Sec.B, we include additional details for training, avatar use cases, and progressive neural networks (PNN) [36]. In Sec.C, we include additional ablation results. Finally, in Sec.D, we provide an extended discussion of limitations, failure cases, and future work. Extensive qualitative results are provided on the project page. We highly encourage our readers to view them to better understand the capabilities of our method. Specifically, we show our method's ability to imitate high-quality MoCap data (both train and test) and noisy motion estimated from video. We also demonstrate real-time video-based (single- and multi-person) and language-based avatar (single- and multiple-clips) use cases. Lastly, we showcase our fail-state recovery ability. B. Implementation Details B.1. Training Details Humanoid Construction. Our humanoid can be constructed from any kinematic structure, and we use the SMPL humanoid structure as it has native support for different body shapes and is widely adopted in the pose estimation literature. Fig.5 shows our humanoid constructed based on randomly selected gender and body shape from the AMASS dataset. The simulation result can then be exported and rendered as the SMPL mesh. We showcase two types of constructed humanoid: capsule-based and mesh-based. The capsule-based humanoid is constructed by treating body parts as simple geometric shapes (spheres, boxes, and capsules). The mesh-based humanoid is constructed following a procedure similar to SimPoE[54], where each body part is created by finding the convex hull of all vertices assigned to each bone. The capsule humanoid is easier to simulate and design, whereas the mesh humanoid provides a better approximation of the body shape to simulate more complex human-object interactions. We find that mesh-based and capsule-based humanoids do not have significant performance differences (see Sec.C) and conduct all experiments using the capsule-based humanoid. For a fair comparison with the baselines, we use the mean body shape of the SMPL with neutral gender for all evaluations and show qualitative results for shape variation. For both types of humanoids, we scale the density of geometric shapes so that the body has the correct weight (on average 70 kg). All inter-joint collisions are enabled for all joint pairs except for between parent and child joints. Collision between humanoids can be enabled and disabled at will (for multi-person use cases). Training Process. During training, we randomly sample motion from the current training set Q (k) and normalize it with respect to the simulated body shape by performing forward kinematics using Ô1:T. Similar to UHC [22], we adjust the height of the root translation to make sure that each of the humanoid's feet touches the ground at the beginning of the episode. We use parallelly simulate 1536 humanoids for training all of our primitives and composers. Training takes around 7 days to collect approximatelybillion samples. When training with different body shapes, we randomly sample valid human body shapes from the AMASS dataset and construct humanoids from them. Hyperparamters used during training can be found in Table.Data Preparation. We follow similar procedure to UHC [22] to filter out AMASS sequences containing human object interactions. We remove all sequences that sits on chairs, move on treadmills, leans on tables, steps on stairs, floating in the air etc., resulting in 11313 high-quality motion sequences for training and 140 sequences for testing. We use a heuristic-based filtering process based on i.e. identifying the body joint configurations corresponding to the sitting motion or counting number of consecutive airborne frames. Runtime. Once trained, our PHC can run in real time (~32FPS) together with simulation and rendering, and around (~ 50FPS) when run without rendering. Table.6 shows the runtime of our method with respect to the number of primitives, architecture, and humanoid type used. Model Size. The final model size (with four primitives) is 28.MB, comparable to the model size of UHC (30.4 MB). B.2. Real-time Use Cases To Real-time Physics-based Virtual Avatars from Video. achieve real-time physics-based avatars driven by video, we first use Yolov8[17] for person detection. For pose estimation, we use MeTRABS [41] and HybrIK [19] to provide 3D keypoints pt and rotation t. MeTRAbs is a 3D keypoint estimator that computes Table 5: Hyperparameters for PHC. σ: fixed variance for policy. : discount factor. e: clip range for PPO State: St Batch Size Learning Rate Primitive: P(1) Primitive: P(2) Primitive: p(K) Primitive: p(F) σ Υ Є Composer: C h(D) h(2) h(K) h Value5 × 2-0.05 0.0.h) h(c) h MLP h(2) h(K) MLP MLP h(F) MLP Value Wjp 0.Wjr Wjv Wjw MLP 0.0.0.3D joint positions pt in the absolute global space (rather than in the relative root space). HybrIK is a recent method for human mesh recovery and computes joint angles ỗt and root position p for the SMPL human body. One can recover the 3D keypoints pt from joint angles ết and root position p using forward kinematics. Both of these methods are causal, do not use any temporal information, and can run in real-time (~ 30FPS). Estimating 3D keypoint location from image pixels is an easier task than regressing joint angles, as 3D keypoints can be better associated with features learned from pixels. Thus, both HybrIK and MeTRAbs estimate 3D keypoints pt, with HybrIK containing an additional step of performing learned inverse kinematics to recover joint angles Ot. We show results using both of these off-the-shelf pose estimation methods, using MeTRAbs with our keypoint-based controller and HybrIK with our rotation-based controller. Empirically, we find that MeTRAbs estimates more stable and accurate 3D keypoints, potentially due to its keypoint-only formulation. We also present a real-time multi-person physics-based human-to-human interaction use case, where we drive multiple avatars and enable inter-humanoid collision. To support multi-person pose estimation, we use OCSort [5] to track individual tracklets and associate poses with each person. Notice that real-time use cases pose additional challenges than offline processing: detection, pose/keypoint estimation, and simulation all need to run at real-time at around 30 FPS, and small fluctuations in framerate could lead to unstable imitation and simulation. To smooth out noisy depth estimates, we use a Gaussian filter to smooth out estimates from t-120 to t, and use the "mirror" setting for padding at boundary. Virtual Avatars from Language. For language-based motion generation, we adopt MDM [43] as our text-to-motion model. We use the official implementation, which generates 3D keypoints pt by default and connects it to our keypoint-based imitator. MDM generates fixed-length motion clips, so additional blending is needed to combine multiple clips of generated motion. However, since PHC can naturally go to far-away reference motion and handles disjoint between motion clips, we can naively chain together multiple clips of motion generated by MDM and create coherent and physically valid motion from multiple text prompts. This enables us to create a simulated avatar that can be driven by a continuous stream of text prompts. B.3. Progressive Neural Network (PNN) Details A PNN [36] starts with a single primitive network P(¹) trained on the full dataset Q. Once P (1) is trained to convergence on the entire motion dataset Q using the imitation task, we create a subset of hard motions by evaluating P(¹) on Q. Sequences that P(1) fails forms (¹) We then freeze the parameters of P(1) hard⚫ Composer: C h(C) MLP 1:(K+1) -Weights: w Gating Function Action: a₁ State: St Primitive: P(1) Primitive: P(2) Primitive: p(K) Primitive: p(F) h(1) h(2) h(K)h(2) h(K) h hQF) MLP MLP MLP MLP : MLP layer -Weights: w ,1:(K+1) Gating Function : Weight sharing : Lateral connection Action: at Figure 6: Progressive neural network architecture. Top: PNN with lateral connection. Bottom: PNN with weight sharing. h() indicates hidden activation of jth primitive's ¿th layer. and create a new primitive P(2) (randomly initialized) along with lateral connections that connect each layer of P(¹) to P (2). Given the layer weight W,(k), activation function f, and the learnable lateral connection weights U(j:k), we have the hidden activation h) of the ith layer of kth primitive as: (k) h(k) 1(k) h ( 1 + ΣU (jk) h (² = (P+R). Συ j<k (7) Fig.6 visualizes the PNN with the lateral connection architecture. Essentially, except for the first layer, each subsequent layer receives the activation of the previous layer processed by the learnable connection matrices U(jk). We do not use any adapter layer as in the original paper. As an alternative to lateral connection, we explore weight sharing and warm-starts the primitive with the weights from the previous one (as opposed to randomly initialized). We find both methods equally effective (see Sec.C) when trained with the same hard-negative mining procedure, as each newly learned primitive adds new sequences that PHC can imitate. The weight sharing strategy significantly decreases training time as the policy starts learning harder sequences with basic motor skills. We use weight sharing in all our main experiments. Epoch2000022500-27500300000 25 50 75 100 125 150 175 200 225 250 275 300 325 350 375 400 425 450 475 500Motion Index Figure 7: Here we plot the motion indexes that the policy fails on over training time; we only plot the 529 sequences that the policy has failed on over these training epoches. A white pixel denotes that sequence is can be successfully imitated at the given epoch, and a black pixel denotes an unsuccessful imitation. We can see that while there are 30 sequences that the policy consistently fails on, the remaining can be learned and then forgotten as training progresses. The staircase pattern indicates that the policy fails on new sequences each time it learns new ones. C. Supplementary Results C.1. Categorizing the Forgetting Problem As mentioned in the main paper, one of the main issues in learning to mimic a large motion dataset is the forgetting problem. The policy will learn new sequences while forgetting the ones already learned. In Fig.7, we visualize the sequences that the policy fails to imitate during training. Starting from the 12.5k epoch, each evaluation shows that some sequences are learned, but the policy will fail on some already learned sequences. The staircase pattern indicates that when learning sequences failed previously, the policy forgets already learned sequences. Numerically, each evaluation has around 30% overlap of failed sequences (right end side). The 30% overlap contains the backflips, cartwheeling, and acrobatics; motions that the policy consistently fails to learn when trained together with other sequences. We hypothesize that these remaining sequences (around 40) may require additional sequence-level information for the policy to learn properly together with other sequences. Fail-state recovery Learning the fail-state recovery task can also lead to forgetting previously learned imitation skills. To verify this, we evaluate P(F) on the H36M-Test-Video dataset, which leads to a performance of Succ: 42.5%, Eg-mpjpe: 87.3, and Empipe: 55.9, which is much lower than the single primitive P(¹) performance of Succ: 59.4%, Eg-mpjpe: 60.2, and Empipe: 34.4. Thus, learning the fail-state recovery task may lead to severe forgetting of the imitation task, motivating our PMCP framework to learn separate primitives for imitation and fail-state recovery. C.2. Additional Ablations In this section, we provide additional ablations of the components of our framework. Specifically, we study the effect of MOE vs. MCP, lateral connection vs. weight sharing, and the number of primitives used. We also report the inference speed (counting network inference and simulation time). All experiments are carried out with the rotation-based imitator and incorporate the fail state recovery primitive P(F) as the last primitive. PNN Lateral Connection vs. Weight Sharing. As can be seen in Table 6, comparing Row 1 (R1) and R7, we can see that PNN with lateral connection and weight sharing produce similar perTable 6: Supplmentary ablation on components of our pipeline, performed using noisy pose estimate from HybrIK + Metrabs (root) on the H36M-Test-Video* data. MOE: top-1 mixture of experts. MCP: multiplicative control policy. PNN: progressive neural networks. Type: between Cap (capsule) and mesh-based humanoids. All models are trained with the same procedure. PNN-Lateral FPS H36M-Test-Video* PNN-Weight MOE MCP Туре # Prim SuccEg-mpjpe↓ Empipe↓ ✓ X x x Cap87.5% 55.36.2X ✓ ✓ Cap87.5% 56.34.3X ✓ X ✓ Mesh 4 86.9% 62.39.5X X x x Cap59.4% 60.37.2✓ Cap65.6% 58.37.3X ✓ x x Cap80.9% 56.36.1✓ Cap 4 88.7% 55.34.Cap 5 87.5% 57.36.X formance, both in terms of motion imitation and inference speed. This shows that in our setup, the weight sharing scheme is an effective alternative to lateral connections. This can be explained by the fact that in our case, each “task” on which the primitives are trained is similar and does not require lateral connection to choose whether to utilize prior experiences or not. MOE vs. MCP. The difference between the top-1 mixture of experts (MOE) and multiplicative control (MCP) is discussed in detail in the MCP paper [33]: top-1 MOE only activates one expert at a time, while MCP can activate all primitives at the same time. Comparing R2 and R7, as expected, we can see that top-MOE is slightly inferior to MCP. Since all of our primitives are pretrained and frozen, theoretically a perfect composer should be able to choose the best primitive based on input for both MCP and MOE. MCP, compared to MOE, can activate all primitives at once and search a large action space where multiple primitives can be combined. Thus, MCP provides better performance, while MOE is not far behind. This is also observed by CoMic[13], where they observe similar performance between mixture and product distributions when used to combine subnetworks. Note that top-inf MOE is similar to MCP where all primitives can be activated. Capsule vs. Mesh Humanoid. Comparing R3 and R7, we can see that mesh-based humanoid yield similar performance to capsulebased ones. It does slow down simulation by a small amount (FPS vs. 32 FPS), as simulating mesh is more compute-intensive than simulating simple geometries like capsules. Number of primitives. Comparing R4, R5, R6, R7, and R8, we can see that the performance increases as the number of primitives increases. Since the last primitive P (F) is for fail-state recovery and does not provide motion imitation improvement, Ris similar to the performance of models trained without PMCP (R4). As the number of primitives grows from 2 to 3, we can see that the model performance grows quickly, showing that MCP is effective in combining pretrained primitives to achieve motion imitation. Since we are using relatively small networks, the inference speed does not change significantly with the number of primitives used. We notice that as the number of primitives grows, Q(k) becomes more and more challenging. For instance, Q (4) contains mainly highly dynamic motions such as high-jumping, back flipping, and cartwheeling, which are increasingly difficult to learn together. We show that (see supplementary webpage) we can overfit these sequences by training on them only, yet it is significantly more challenging to learn them together. Motions that are highly dynamic require very specific steps to perform (such as moving while airborne to prepare for landing). Thus, the experiences collected when learning these sequences together may contradict each other: for example, a high jump may require a high speed running up, while a cartwheel may require a different setup of foot-movement. A per-frame policy that does not have sequence-level information may find it difficult to learn these sequences together. Thus, sequence-level or information about the future may be required to learn these high dynamic motions together. In general, we find that using 4 primitives is most effective in terms of training time and performance, so for our main evaluation and visualizations, we use 4-primitive models. D. Extended Limitation and Discussions Limitation and Failure Cases. As discussed in the main paper, PHC has yet to achieve 100% success rate on the AMASS training set. With a 98.9% success rate, PHC can imitate most of our daily motion without losing balance, but can still struggle to perform more dynamic motions, such as backflipping. For our real-time avatar use cases, we can see a noticeable degradation in performance from the offline counterparts. This is due to the following: • Discontinuity and noise in reference motion. The inherent ambiguity in monocular depth estimation can result in noisy and jittery 3D keypoints, particularly in the depth dimension. These small errors, though sometimes imperceptible to the human eye, may provide PHC with incorrect movement signals, leaving insufficient time for appropriate reactions. Velocity estimation is also especially challenging in real-time use cases, and PHC relies on stable velocity estimation to infer movement cues. • Mismatched framerate. Since our PHC assumes 30 FPS motion input, it is essential for pose estimates from video to match for a more stable imitation. However, few pose estimators are designed to perform real-time pose estimation (> 30 FPS), and the estimation framerate can fluctuate due to external reasons, such as the load balance on computers. • For multi-person use case, tracking and identity switch can still happen, leading to a jarring experience where the humanoids need to switch places. A deeper integration between the pose estimator and our controller is needed to further improve our real-time use cases. As we do not explicitly account for camera pose, we assume that the webcam is level with the ground and does not contain any pitch or roll. Camera height is manually adjusted at the beginning of the session. The pose of the camera can be taken into account in the pose estimation stage. Another area of improvement is naturalness during fail-state recovery. While our controller can recover from fail-state in a human-like fashion and walks back to resume imitation, the speed and naturalness could be further improved. Walking gait, speed, and tempo during fail-state recovery exhibits noticeable artifacts, such as asymmetric motion, a known artifact in AMP [35]. During the transition between fail-state recovery and motion imitation, the humanoid can suddenly jolt and snap into motion imitation. Further investigation (e.g. better reward than the point-goal formulation, additional observation about trajectory) is needed. Discussion and Future Work. We propose the perpetual humanoid controller, a humanoid motion imitator capable of imitating large corpus of motion with high fidelity. Paired with its ability to recover from fail-state and go back to motion imitation, PHC is ideal for simulated avatar use cases where we no longer require reset during unexpected events. We pair PHC with a real-time pose estimator to show that it can be used in a video-based avatar use case, where the simulated avatar imitates motion performed by the actors perpetually without requiring reset. This can empower future virtual telepresence and remote work, where we can enable physically realistic human-to-human interactions. We also connect PHC to a language-based motion generator to demonstrate its ability to mimic generated motion from text. PHC can imitate multiple clips by performing motion inbetweening. Equipped with this ability, future work in embodied agents can be paired with a natural language processor to perform complex tasks. Our proposed PMCP can be used as a general framework to enable progressive RL and multi-task learning. In addition, we show that one can use only 3D keypoint as motion input for imitation, alleviating the requirement of estimating joint rotations. Essentially, we use PHC to perform inverse kinematics based on the input 3D keypoints and leverages the laws of physics to regulate its output. We believe that PHC can also be used in other areas such as embodied agents and grounding, where it can serve as a low-level controller for high-level reasoning functions.