--- --2308 .02510v2 [eess.IV] 16 AugarXiv Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals Yu-Ting Lan'*, Kan Ren”, Yansen Wang’, Wei-Long Zheng’, Dongsheng Li’, Bao-Liang Lu’, Lili Qiu? 'Shanghai Jiao Tong University, "Microsoft Research {kanren, yansenwang } @ microsoft.com, weilong @sjtu.edu.cn Abstract Seeing is believing, however, the underlying mechanism of how human visual perceptions are intertwined with our cognitions is still a mystery. Thanks to the recent advances in both neuroscience and artificial intelligence, we have been able to record the visually evoked brain activities and mimic the visual perception ability through computational approaches. In this paper, we pay attention to visual stimuli reconstruction by reconstructing the observed images based on portably accessible brain signals, i.e., electroencephalography (EEG) data. Since EEG signals are dynamic in the time-series format and are notorious to be noisy, processing and extracting useful information requires more dedicated efforts; In this paper, we propose a comprehensive pipeline, named NEUROIMAGEN, for reconstructing visual stimuli images from EEG signals. Specifically, we incorporate a novel multi-level perceptual information decoding to draw multigrained outputs from the given EEG data. A latent diffusion model will then leverage the extracted information to reconstruct the high-resolution visual stimuli images. The experimental results have illustrated the effectiveness of image reconstruction and superior quantitative performance of our proposed method. Introduction Understanding cortical responses to human visual perception has emerged a research hotspot, which can significantly motivate the development of computational cognitive system with the knowledge of neuroscience (Palazzo et al. 2020). Along with the rapid development of physiological techniques such as functional magnetic resonance imaging (f{MRJ or electroencephalograph (EEG), it becomes possible to record the visually-evoked human brain activities for further analysis. Thus, the research community put the attention onto these complicated brain signal data and try to reconstruct the stimuli contents used for evoking human subjects in the experiments, for understanding and simulating the human visual perception. One of the mainstream attempts to study the human visual perception is to reconstruct the seen contents such as images (Takagi and Nishimoto 2023) or videos (Chen, Qing, and Zhou 2023) used to evoke the human subjective in the “Work done during Yuting’s intern at Microsoft Research Asia, correspondence to Kan Ren, Yansen Wang and Weilong Zheng. stimuli experiments, via computational approaches such as leep neural networks. These works are mainly based on fMRI data (Allen et al. 2022), while collecting these imaging lata requires expensive devices and lacks of convenience for practical usage. In contrast, EEG has provided a more expeient solution to record and analyze brain signals, yet few works are learning visual perception upon these brain signal lata. EEG data are commonly time-series electrophysiological signals recorded via electrodes placed upon the human scalp, while the subjects are watching some stimuli contents such as images which have also been temporally aligned to the recorded signals in the data. Though more convenient, reconstruction of visual stimuli from EEG signals are more challenging than that from fMRI data. First, EEG signals are in time-series data format, which is temporal sequence and quite different to the static 2D/3D images, leading to the challenge of the matching stimuli to the corresponding brain signal pieces. Second, the effects of electrode misplacement or body motion result in severe artifacts in the data with quite low signal-to-noise ratio (SNR), which have largely influenced the modeling and understanding of the brain activities. Simply mapping the EEG input to the pixel domain to recover the visual stimuli is of low quality. The existing works tackling image reconstruction from EEG either traditional generative models from scratch (Kavasidis et al. 2017) and fine-tuning large generative models (Bai et al. 2023), which are less efficient; or just retrieving similar images from the data pool (Ye et al. 2022). They fail to capture semantic information or reconstruct high-resolution outputs. In this work, we propose a comprehensive pipeline for Neural Image generation, namely NEUROIMAGEN, from human brain signals. To tackle with aforementioned challenges in this task, we incorporate a multi-level semantics extraction module which decodes different semantic information from the input signal with various granularity. Specifically, the extracted information contains sample-level semantics which is easy to decode, and pixel-level semantics such as the saliency map of silhouette information that tends to more decoding difficulties. The multi-level outputs are further fed into the pretrained diffusion models with the control of the generation semantics. Through this way, our method can flexibly handle the semantic information extraction and decoding problem at different granularity and diffi --- --culties, which can subsequently facilitate the generation via effectively controlling the fixed downstream diffusion model at different levels. We evaluate our methods with the traditional image reconstruction solutions on EEG data. The results demonstrate the superiority of our NEUROIMAGEN over the compared methods in both quantitative and qualitative results on the EEG-image dataset. The proposed multi-level semantics extraction at different granularity can largely increase the structure similarity and semantic accuracy of reconstructed images with the observed visual stimuli. Related Work Diffusion Models Recently, diffusion models have emerged as state-of-theart approaches in the field of generative models for several tasks, including image synthesis, video generation, and molecule design (Yang et al. 2022; Song, Meng, and Ermon 2020; Dhariwal and Nichol 2021). A denoising diffusion probabilistic model (DDPM) (Ho, Jain, and Abbeel 2020; Sohl-Dickstein et al. 2015) is a parameterized bi-directional Markov chain using variational inference to produce sample matching after a finite time. The forward diffusion process is typically designed with the goal to transform any data distribution into a simple prior distribution (e.g., an isotropic Gaussian), and the reverse denoising diffusion process reverses the former by learning transition kernels parameterized by deep neural networks, such as U-Net (Ronneberger, Fischer, and Brox 2015). However, DDPM operates and undergoes evaluation and optimization in pixel space, leading to slower inference speeds and higher training costs. To address these limitations, Rombach et al.(2022) introduced the concept of latent diffusion models (LDMs). In LDMs, diffusion models are applied within the latent space of the powerful pretrained autoencoders. This approach proves to be an effective generative model, accompanied by a separate compression stage that selectively eliminates imperceptible details. By operating in the latent space, LDMs overcome the drawbacks of pixel space evaluation, enabling faster inference and reducing training costs. Image Decoding from {MRI The most recent works reconstructing the stimuli contents from the brain activities are mainly focused on fMRI data. fMRI, as the measurement of the brain’s blood-oxygenlevel-dependent (BOLD) signals, has seen substantial advancements in brain signal decoding. The conventional visual decoding methods in fMRI usually rely on training deep generative neural networks, such as generative adversarial networks (GAN) and variational autoencoders (VAE) with paired brain-image data (Shen et al. 2019; Beliy et al. 2019). However, the decoding performance of these conventional methods is usually limited, and they struggle to produce visual contents with high resolution, because training a deep generative model from scratch is in general challenging and the dataset of brain signals is relatively small and noisy. Thus, recent research attempts to directly map brain signals into carefully pretrained latent spaces and finetune large-scale pretrained models to generate diverse and highresolution images. Takagi and Nishimoto map the brain activities to latent space and convert them to natural images using LDM. MinD-Vis (Chen et al. 2023) integrates mask brain modelings and LDM to generate more plausible images with preserved semantic information. Zeng et al. integrate silhouette information from brain signals with a controllable diffusion model to reconstruct high-quality images consistent with original visual stimuli. These methods generate more plausible and semantically meaningful images. Image Decoding from EEG Signals EEG is more portable but has relatively lower spatial resolution and suffers from larger noise, compared to fMRI, which makes decoding visual experience from brain signals a challenging problem. Brain2Image (Kavasidis et al. 2017) implements long short-term memory (LSTM) stacked with GAN and VAE techniques to generate seen images of ImageNet (Krizhevsky, Sutskever, and Hinton 2012) from EEG signals (Kavasidis et al. 2017). Neurovison (Khare et al. 2022) proposes conditional progressive growing of GAN (CProGAN) to develop perceived images and showed a higher inception score. Ye et al. focuses on cross-modal alignment and retrieves images at the instance level, ensuring distinguishable model output for EEG signals. We also note that there is a parallel work DreamDiffusion (Bai et al. 2023) to ours, which finetunes the diffusion model with an auxiliary task for aligning the EEG data and Image CLIP embeddings. However, the end-to-end training framework of DreamDiffusion struggles to effectively decode and utilize multi-level semantic information from EEG signals, which limits its ability to handle inherent noise characteristics. In addition, DreamDiffusion requires fine-tuning the diffusion models, which poses practical challenges and limitations in terms of scalability and efficiency. Methodology In this section, we design our method, NEUROIMAGEN, to extract multi-level semantics from EEG signals and subsequently integrate them into a pretrained diffusion model to reconstruct the observed visual stimuli from EEG signals. We briefly introduce the intuition of multi-level semantics extraction in our NEUROIMAGEN. EEG signals are nonstationary time-series signals and are easy to disturb by artifacts like body motion, resulting in the low SNR of the signals. To tackle this challenge, we decode different semantic information with various granularity. Specifically, the pixellevel semantics such as the saliency map of silhouette information preserve fine-grained color, position, and shape details of the observed stimuli. The sample-level semantics provides a coarse-grained description of the visual stimuli, such the concept or category of the visual content. These designs exhibit the capacity to effectively manage the challenges posed by noisy time-series EEG data, consequently facilitating the reconstruction of high-quality visual stimuli. In the following, we first formulate the problem and give an overview of NEUROIMAGEN. Then, we describe the multi-level semantics extractions of the NEUROIMA --- --Multi-level Information i Pixel Level i EEG Signals M, FI sagen acatinartmmna z f — 3 teed z 1 ; — 2 iyo :aption / Sample Level CLIP Embedding Decoding From EEG Signals Supervision Figure 1: Overview of our NEUROIMAGEN. AIL Fine-grained Control Latent Diffusion Model Diffusion Proc Reconstructed Image Denoising Process Coarse-grained Control the modules with dotted lines, i.e. pixel-level supervision and sample-level supervision, are only used during the training phase. and would be removed during the inference phase. GEN, including pixel-level semantics and sample-level semantics with the corresponding training details of the decoding procedure. Finally, we detail the image reconstruction procedure of NEUROIMAGEN, which integrates the coarsegrained and fine-grained semantics with a pretrained latent diffusion model to reconstruct the observed visual stimuli from EEG signals. Problem Statement In this section, we formulate the problem and give an overview of NEUROIMAGEN. Let the paired {(EEG, image)} dataset as Q = {(2x;,yi)}j_1, where yi € RYXW*S ig the visual stimuli image to evoke the brain activities and 2; € R°*™ represents the recorded corresponding EEG signals. Here, C’ is the channel number of EEG sensors and T is the temporal length of a sequence associated with the observed image. The general objective of this research is to reconstruct an image y using the corresponding EEG signals x, with a focus on achieving a high degree of similarity to the observed visual stimuli. Multi-level Semantics Extraction Framework Figure 1 illustrates the architecture of NEUROIMAGEN. In our approach, we extract multi-level semantics, represented as {Mj (x), Mo(x),--+ , Mn(x)}, which capture various granularity ranges from coarse-grained to fine-grained information from EEG signals corresponding to visual stimuli. The coarse-grained semantics serves as a high-level overview, facilitating a quick understanding of primary attributes and categories of the visual stimuli. On the other hand, fine-grained semantics offers more detailed information, such as localized features, subtle variations, and smallscale patterns. The multi-level semantics are then fed into a high-quality image reconstructing module F to reconstruct the visual stimuli g¢ = F[Mj(x),Mo(x),---,Mn(2)]. Specifically, we give two-level semantics as follows. Let M, and M, be the pixel-level semantic extractor and samplelevel semantic extractor, respectively. Pixel-level semantics is defined as the saliency map of silhouette information M,(x) € R4»*W*3. This step enables us to analyze the EEG signals in the pixel space and provide the rough structure information. Subsequently, we define the sample-level semantics as M,(x) € REX: to provide coarse-grained information such as image category or text caption. To fully utilize the two-level semantics, the high-quality image reconstructing module F' is a latent diffusion model. It begins with the saliency map M,(a) as the initial image and utilizes the sample-level semantics M(x) to polish the saliency map and finally reconstruct 7 = F(Mp(), Ma(2)). Pixel-level Semantics Extraction In this section, we describe how we decode the pixel-level semantics, i.e. the saliency map of silhouette information. The intuition of this pixel-level semantics extraction is to capture the color, position, and shape information of the observed visual stimuli, which is fine-grained and extremely difficult to reconstruct from the noisy EEG signal. However, as is shown in Figure 3, despite the low image resolution and limited semantic accuracy, such a saliency map successfully captures the rough structure information of visual stimuli from the noisy EEG signals. Specifically, our pixel-level semantics extractor M, consists of two components: (1) contrastive feature learning to obtain discriminative features of EEG signals and (2) the estimation of the saliency map of silhouette information based on the learned EEG features. Contrastive Feature Learning We use contrastive learning techniques to bring together the embeddings of EEG signals when people get similar visual stimulus, i.e. seeing images of the same class. The triplet loss (Schroff, Kalenichenko, and Philbin 2015) is utilized as Leipler = max(0, 6+||fo(x") — fo(a?)||—||fo(e*) — fo(a")|]3), where fg is the feature extraction function (Kavasidis et al. 2017) that maps EEG signals to a feature space. 7%, 2”, x” () --- --are the sampled anchor, positive, and negative EEG signal segments, respectively. The objective of eq. (1) is to minimize the distance between x* and x? with the same labels (the class of viewed visual stimuli) while maximizing the distance between x® and x” with different labels. To avoid the compression of data representations into a small cluster by the feature extraction network, a margin term 3 is incorporated into the triplet loss. Estimation of Saliency Map After we obtain the feature of EEG signal f(x), we can now generate the saliency map of silhouette information from it and a random sampled latent z ~ N(0, 1), ie., M,(x) = G(z, fo(x)). G denotes for the saliency map generator. In this paper, we use the generator from the Generative Adversarial Network(GAN) (Goodfellow et al. 2020) framework to generate the saliency map and the adversarial loss is defined as follows: Lgy = max(0, 1 — D(A(y), fo(x)))+ max(0,1 + D(A(Mp(2))), fo(z))), — @) Lay = — D(A(Mp(2)), fa(2))- (3) In GAN, besides the generator G, a discriminator D is introduced to distinguish between images from the generator and ground truth images zx. It is optimized by minimizing the hinge loss (Lim and Ye 2017) defined in Equation (2). A is the differentiable augmentation function (Zhao et al. 2020). To stabilize the adversarial training process and alleviate the problem of mode collapse, we add the mode seeking regularization (Mao et al. 2019): Ln = — (“ (G (21, fe(x)) CO i) . dz (2, 22) (4) where d,.(-) denotes the distance metric in image space a or latent space z and 21, z2 ~ N(0,1) are two different sampled latent vectors. To enforce the accuracy of the generated saliency map from the visual stimuli, we use the observed image as supervision and incorporate the Structural Similarity Index Measure (SSIM) as well: (2HoPat,(e) + C1) (2¢e0m, (a) + C2) (12 t Hu, («) t C1) (02 t or, (0) t C2) (5) where lz, LM,(x)> Tx, aNd Oy4,(x) Tepresent the mean and standard values of the ground truth images and reconstructed saliency maps of the generator. C; and C2 are constants to stabilize the calculation. The final loss for the generator is the weighted sum of the losses: Lssiu =Le =a + LG, +02 + Lins + 03 > Lssim, (6) and a¢{1,2,3} are hyperparameters to balance the loss terms. Sample-level Semantics Extraction As aforementioned, the EEG signals are notorious for their inherent noise, making it challenging to extract both precise and fine-grained information. Therefore, besides finegrained pixel-level semantics, we also involve sample-level semantic extraction methods to derive some coarse-grained information such as the category of the main objects of the image content. These features have a relatively lower rank and are easier to be aligned. Despite being less detailed, these features can still provide accurate coarse-grained information, which is meaningful to reconstruct the observed visual stimuli. Specifically, the process M, will try to align the information decoded from the input EEG signals to some generated image captions, which are generated by some other additional annotation model such as Contrastive LanguageImage Pretraining (CLIP) model (Radford et al. 2021). Below we detail the processes of image caption ground-truth generation and semantic decoding with alignment. GT images Label captions 1 An image of An elephant | african elephant standing next to a ' large rocka ee An image of A person flying ! parachute a parachute in the air with a banner Ared and white flower with yellow center Aman riding a mountain bike down a trail in the woods Figure 2: Examples of ground-truth images, label captions, and BLIP captions, respectively. Generation of Image Captions We propose two methods to generate the caption for each image to help supervise the decoding procedure of semantic information from EEG signals. Since the observed images are from ImageNet dataset containing the class of the image, we define a straightforward and heuristic method of label caption, which utilizes the class name of each image as the caption, as illustrated in the middle column of Figure 2. The second method is that we use an image caption model BLIP (Li et al. 2023), which is a generic and computation-efficient vision-language pretraining (VLP) model utilizing the pretrained vision model --- --and large language models. We opt for the default parameter configuration of the BLIP model to caption our images. The examples are demonstrated in the right column of Figure 2. As can be seen, the label captions tend to focus predominantly on class-level information, and the BLIP-derived captions introduce further details on a per-image level. Predict the Text CLIP Embedding After the generation of the image caption ground-truth, the goal of the semantic decoding is to extract the information from the EEG signals to align the caption information. Note that, this procedure is conducted in the latent space, where the latent embeddings have been processed from the CLIP model from the above generated captions. Specifically, We extracted the CLIP embeddings heip, from the generated captions and align the output hip of EEG sample-level encoder with the loss function as . Leip = ||Retip — hetip,|I35 (7) where * € {B, L} denotes the BLIP caption embedding or label caption embedding. Combining Multi-level EEG Semantics with Diffusion Model In this section, we present a comprehensive explanation of how multi-level semantics can be effectively integrated into a diffusion model for visual stimulus reconstruction. We utilize both pixel-level semantics, denoted as M,(x) (obtained using G(z, fo(x))), and sample-level semantics, represented as M(x) (obtained using Netip), to exert various granularity control over the image reconstruction process. The reconstructed visual stimuli are defined as y = F(M,(«), Ms(2)) = F(G(fa(@), hetip)) Specifically, we used the latent diffusion model to perform image-to-image reconstructing with the guidance of conditional text prompt embeddings: (1) First, we reconstruct the pixel-level semantics G(f9(a)) from EEG signals and resize it to the resolution of observed visual stimuli (2) G(fo(x)) is then processed by the encoder Eyam of autoencoder from the latent diffusion model and added noise through the diffusion process. (3) Then, we integrate the sample-level semantics hip as the cross-attention input of the U-Net to guide the denoising process. (4) We project the output of the denoising process to image space with Diam and finally reconstruct the high-quality image 4. Experiments Dataset The effectiveness of our proposed methodology is validated using the EEG-image dataset (Spampinato et al. 2017). This dataset is publicly accessible and consists of EEG data gathered from six subjects. The data was collected by presenting visual stimuli to the subjects, incorporating 50 images from 40 distinct categories within the ImageNet dataset (Krizhevsky, Sutskever, and Hinton 2012). Each set of stimuli was displayed in 25-second intervals, separated by a 10second blackout period intended to reset the visual pathway. This process resulted in totally 2000 images, with each experiment lasting 1,400 seconds (approximately 23 minutes and 20 seconds). The EEG-image dataset encompasses a diverse range of image classes, including animals (such as pandas), and objects (such as airlines). Following the common data split strategy (Kavasidis et al. 2017), we divide the pre-processed raw EEG signals and their corresponding images into training, validation, and testing sets, with corresponding proportions of 80% (1,images), 10% (200 images), and 10% (200 images) and build one model for all subjects. The dataset is split by images, ensuring the EEG signals of all subjects in response to a single image are not spread over splits. Evaluation Metrics N-way Top-k Classification Accuracy (ACC) Following (Chen et al. 2023), we evaluate the semantic correctness of our reconstructed images by employing the N-way top-k classification accuracy. Specifically, the ground truth image y and reconstructed image y are fed into a pretrained ImageNet1k classifier (Dosovitskiy et al. 2020), which determines whether y and % belong to the same class. Then we check for the reconstructed image if the top-k classification in N selected classes matches the class of ground-truth image. Importantly, this evaluation metric eliminates the need for pre-defined labels for the images and serves as an indicator of the semantic consistency between the ground truth and reconstructed images. In this paper, we select 50-way top-1 accuracy as the evaluation metric. Inception Score (IS) IS, introduced by (Salimans et al. 2016), is commonly employed to evaluate the quality and diversity of reconstructed images in generative models. To compute the IS, a pretrained Inception-v3 classifier (Szegedy et al. 2016) is utilized to calculate the class probabilities for the reconstructed images. We use IS to give a quantitative comparison between our method and baselines. Structural Similarity Index Measure (SSIM) SSIM offers a comprehensive and perceptually relevant metric for image quality evaluation. SSIM is computed over multiple windows of the ground truth image and the corresponding reconstructed image in luminance, contrast, and structure components, respectively. Results Experiment Results on the ImageNet Dataset The main results are illustrated in Figure 3. The images positioned on the left with red boxes represent the ground truth images. The second images from the left represent the saliency map reconstructed from EEG signals. The three images on the right exhibit the three sampling results for the given pixel-level saliency map with the guidance of samplelevel semantics of EEG signals. Upon comparison with the ground truth images and the reconstructed saliency maps, we validate that our pixel-level semantics extraction from EEG signals successfully captures the color, positional, and shape information of viewed images, despite limited semantic accuracy. Comparing the GT images and three reconstructed samples, it is demonstrated that the latent diffusion model successfully polishes the decoded saliency map --- --GT images Saliency Map Sample! Sample2 SampleSampleSampleFigure 3: The main results of our NEUROIMAGEN. The images positioned on the left with red boxes represent the ground truth images. The second images from the left represent the pixel-level saliency map reconstructed from EEG signals. The three images on the right exhibit the three sampling results for the given saliency map under the guidance of sample-level semantics. with coarse-grained but accurate guidance of sample-level semantics from EEG signals. The high-quality reconstructed images purely from brain signals are perceptually and semantically similar to the viewed images. Model ACC (%) IS SSIM Brain2Image ~ 5.01 ~ Neuro Vision ~~ 5.23 ~~ NEUROIMAGEN 85.6 33.50 | 0.Table 1: The quantitative results of our NEUROIMAGEN, Brain2Image (Kavasidis et al. 2017) and NeuroVision (Khare et al. 2022) on EEG-image dataset. Comparison with Baselines The quantitative results of NEUROIMAGEN and baselines are listed in Table 1. We have introduced the IS reported in the relevant literature, to exemplify the quality of the reconstructed images. The IS is calculated by encompassing all images reconstructed across all subjects and all classes within the test set. As is demonstrated in Table 1, the IS of our NEUROIMAGEN is significantly higher than Brain2Image and NeuroVision. Furthermore, inspired by (Bai et al. 2023), we provide a qualitative comparison with the baselines in Figure 4. As can be seen, the quality of the images reconstructed by our NEUROIMAGEN is markedly higher than those reconstructed by the Brain2Image. This observed enhancement serves to validate the effectiveness and superiority of our proposed methodology. Subject | ACC(%) | IS | SSIM subj0l | 83.84 | 32.64] 0.subj 02 | 84.26 | 32.33 | 0.subj 03 | 86.66 | 32.93 | 0.subj04 | 86.48 | 32.40 | 0.subj 05 | 87.62 | 32.97 | 0.subj 06 | 85.25. | 31.76 | 0.Table 2: The quantitative results of different subjects. Generation Consistency in Different Subjects Since EEG signals are subject-specific cognitive processes that differ significantly in different subjects. In this section, we validate the robustness and feasibility of NEUROIMAGEN across different individuals. As is illustrated in Figure 5. The quantitative metric of different subjects are stable, which proves the generalization ability of NEUROIMAGEN. The qualitative results are shown in Figure 5. It can be seen the sampling from different subjects are semantically similar to the ground truth images. Ablation Study We further conduct experiments on the EEG-image dataset to analyze the effectiveness of each module of our NEUROIMAGEN. We define B and L as the sample-level semantics from EEG signals using BLIP caption as supervision or label caption as supervision. We define J as the pixel semantics from EEG signals. The effectiveness of different methods is verified by employing ACC, IS, and SSIM. --- --Brain2Image Airliner Jack-o’-Lantern Figure 4: Comparison baseline Brain2Image (Kavasidis et al. 2017) and our proposed NEUROIMAGEN in three classes, namely ’ Airliner’, ’ Panda’, and ’Jack-o’-Lantern’. The first and second row depicts the results of Brain2Image and our NEUROIMAGEN, respectively. Model | B | L | I | ACC(%) IS SSIM 1 [X[X|V] 45° | 1631 | 0.2 [X|VW [xX] 3859 | 34.12 | 0.3 [VW |X |X] 741 | 29.87 | 0.4 [V [XV] 65.3 | 25.86 | 0.5 [X|V [Vv] 85.6 | 33.50 | 0.Table 3: Quantitative results of ablation studies. B and L represent the semantic decoding using BLIP caption and label caption from EEG signals, respectively. J represents the perceptual information decoding from EEG signals. Pixel-level Semantics To demonstrate the effectiveness of the pixel-level semantics from EEG signals, we conduct validation on models 2, 3, 4, and 5. By comparing 2 with 5 and 3 with 4, we find that using the pixel-level semantics, i.e. the saliency map, can significantly increase the structure similarity of the reconstructed images and ground truth images. Sample-level Semantics We investigate the module of sample-level semantics decoding from EEG signals on guid ing the denoising process. Models 1, 4, and 5 represent the experimental results only using the saliency, both the saliency map and sample-level semantics with the supervision of BLIP caption, and both the saliency map and samplelevel semantics with the supervision of label caption, respectively. By comparing 1 with 4 and 1 with 5, the experimental results demonstrate that the use of sample-level semantics significantly increases the semantic accuracy of reconstructed images. BLIP Captions vs Label Captions We also compare the two caption supervision methods with models 2 with 3 and 4 with 5. The experimental results of the label caption in all metrics are superior to using BLIP caption. We impute these results to that the EEG signals may only capture the classlevel information. So the prediction of BLIP latent is inaccurate, which decreases the performance of diffusion models. Conclusion In this paper, we explore to understand the visually-evoked brain activity. Specifically, We proposed a framework, named NEUROIMAGEN, to reconstruct images of visual perceptions from EEG signals. The NEUROIMAGEN first --- --GT images Electric guitar Pizza Anemone fish Canoe Subj 05 SubjFigure 5: Comparison of reconstructed images on different subjects. The images on the left with red boxes represent the ground truth images. The other six images represent the reconstructed images of different subjects. The shown classes include fish, pizza, guitar, and canoe. generates multi-level semantic information, i.e., pixel-level saliency maps and sample-level textual descriptions from EEG signals, then use the diffusion model to combine the extracted semantics and obtain the high-resolution images. Both qualitative and quantitative experiments reveals the strong ability of the NEUROIMAGEN. As a preliminary work in this area, we demonstrate the possibility of linking human visual perceptions with complicated EEG signals. We expect the findings can further motivate the field of artificial intelligence, cognitive science, and neuroscience to work together and reveal the mystery of our brains to proceed visual perception information. References Allen, E. J.; St-Yves, G.; Wu, Y.; Breedlove, J. L.; Prince, J. S.; Dowdle, L. T.; Nau, M.; Caron, B.; Pestilli, F; Charest, I; et al. 2022. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nature Neuroscience, 25(1): 116-126. Bai, Y.; Wang, X.; Cao, Y.; Ge, Y.; Yuan, C.; and Shan, Y. 2023. DreamDiffusion: Generating High-Quality Images from Brain EEG Signals. arXiv preprint arXiv:2306.16934. Beliy, R.; Gaziv, G.; Hoogi, A.; Strappini, F.; Golan, T.; and Irani, M. 2019. From voxels to pixels and back: Selfsupervision in natural-image reconstruction from fMRI. Advances in Neural Information Processing Systems, 32. Chen, Z.; Qing, J.; Xiang, T.; Yue, W. L.; and Zhou, J. H. 2023. Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 22710-22720. Chen, Z.; Qing, J.; and Zhou, J. H. 2023. Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity. arXiv preprint arXiv:2305.11675. Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34: 8780-8794. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16xwords: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2020. Generative adversarial networks. Communications of the ACM, 63(11): 139-144. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33: 6840-6851. Kavasidis, I.; Palazzo, S.; Spampinato, C.; Giordano, D.; and Shah, M. 2017. Brain2image: Converting brain signals into --- --images. In Proceedings of the 25th ACM international conference on Multimedia, 1809-1817. Khare, S.; Choubey, R. N.; Amar, L.; and Udutalapalli, V. 2022. NeuroVision: perceived image regeneration using cProGAN. Neural Computing and Applications, 34(8): 5979-5991. Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems, 25. Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597. Lim, J. H.; and Ye, J. C. 2017. Geometric gan. arXiv preprint arXiv:1705.02894. Mao, Q.; Lee, H.-Y.; Tseng, H.-Y.; Ma, S.; and Yang, M.H. 2019. Mode seeking generative adversarial networks for diverse image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1429-1437. Palazzo, S.; Spampinato, C.; Kavasidis, I.; Giordano, D.; Schmidt, J.; and Shah, M. 2020. Decoding brain representations by multimodal learning of neural activity and visual features. [EEE Transactions on Pattern Analysis and Machine Intelligence, 43(11): 3833-3849. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 8748-8763. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 10684— 10695. Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, 234-241. Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Radford, A.; and Chen, X. 2016. Improved techniques for training gans. Advances in Neural Information Processing Systems, 29. Schroff, F.; Kalenichenko, D.; and Philbin, J. 2015. Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 815-823. Shen, G.; Dwivedi, K.; Majima, K.; Horikawa, T.; and Kamitani, Y. 2019. End-to-end deep image reconstruction from human brain activity. Frontiers in Computational Neuroscience, 13:21. Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and Ganguli, S. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, 2256-2265. Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Spampinato, C.; Palazzo, S.; Kavasidis, I.; Giordano, D.; Souly, N.; and Shah, M. 2017. Deep learning human mind for automated visual classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6809-6817. Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2818-2826. Takagi, Y.; and Nishimoto, S. 2023. High-resolution image reconstruction with latent diffusion models from human brain activity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14453-14463. Yang, L.; Zhang, Z.; Song, Y.; Hong, S.; Xu, R.; Zhao, Y.; Shao, Y.; Zhang, W.; Cui, B.; and Yang, M.-H. 2022. Diffusion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796. Ye, Z.; Yao, L.; Zhang, Y.; and Gustin, S. 2022. See what you see: Self-supervised cross-modal retrieval of visual stimuli from brain activity. arXiv preprint arXiv:2208.03666. Zeng, B.; Li, S.; Liu, X.; Gao, S.; Jiang, X.; Tang, X.; Hu, Y.; Liu, J.; and Zhang, B. 2023. Controllable Mind Visual Diffusion Model. arXiv preprint arXiv:2305.10135. Zhao, S.; Liu, Z.; Lin, J.; Zhu, J.-Y.; and Han, S. 2020. Differentiable augmentation for data-efficient gan training. Advances in Neural Information Processing Systems, 33: 7559-71570.