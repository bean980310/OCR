--- --arXiv:2305.07677v2 [cs.SD] 24 MayMasked Audio Text Encoders are Effective Multi-Modal Rescorers Jinglun Cai, Monica Sunkara, Xilai Li, Anshu Bhatia, Xiao Pan, Sravan Bodapati AWS AI Labs {cjinglun, sunkaral, xilaili, anshubha, panxx, sravanb}@amazon.com Abstract Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on indomain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours) MATE achieves a WER reduction of 8%-23% over the first-pass baseline. 1 Introduction Performance of Automatic Speech Recognition (ASR) systems has been traditionally improved during inference time via either editing/refinement (Leng et al., 2021; Chi et al., 2021; Cai et al., 2023) or second-pass rescoring (Xia et al., 2017; Sainath et al., 2019; Hu et al., 2020) using language models . In recent studies, Transformerbased pre-trained Large Language Models (LLMs) have shown promising results when used as secondpass rescorers. Previous works (Xu et al., 2022; Salazar et al., 2020; Udagawa et al., 2022) have shown that deep bidirectional Transformers (Devlin et al., 2019) perform better than their unidirectional counterparts such as GPT-2 (Radford et al., 2019b). While LLMs are trained on giant text corpora, they may not be representative of the specific domain of interest, in this case, speech transcriptions. This may result in limited generalization ability without domain-specific fine-tuning. Further, ASR applications warrant robustness to noise and other distortions, which text-only LLMs are incapable of handling on their own at rescoring time. A potential solution to mitigate these limitations is to incorporate the speech input into LLM rescorers. Recent studies have demonstrated the effectiveness of leveraging audio information during second-pass rescoring (Sainath et al., 2019; Gandhe and Rastrow, 2020; Hu et al., 2020, 2022) to improve performance. However, a tight integration of rescorer, attending to a shared speech encoder used in the first-pass, relies on ASR architecture, training mechanism and internal features, limiting the flexibility of being applied to other ASR systems. Inspired by recent multi-modal LLM works (Tsimpoukelli et al., 2021; Gao et al., 2022; Bapna et al., 2021; Chen et al., 2022), we propose MATE, a multi-modal MLM rescorer, which is compatible with encapsulated ASR systems: our method by design can work with any first-pass ASR models (Hybrid / CTC / Transducer). The rescorer is agnostic to ASR architecture, training mechanism and internal features, leading to better generalization capability. To the best of our knowledge, this is the first work to integrate a pre-trained self-supervised learning (SSL) speech representation model (Baevski et al., 2019, 2020; Hsu et al., 2021; Chen et al., 2021) into the second-pass rescoring. One key challenge of incorporating acoustic information into LLMs is to transform the speech into a form that can be accepted by the language model. We overcome this by using a cross-modal adaptation module consisting of Convolutional Neural Network (CNN) (LeCun et al., 1989) and adapter network (Houlsby et al., 2019). We experiment with different auxiliary alignment losses for audio-text alignment, to effectively learn shared representations across the two modalities, and adopt contrastive learning which significantly improves the model performance. Empirically, we show that MATE transfers well to new domains --- --in zero-shot and few-shot settings, outperforming text-only baselines. Lum 4 L=Luim+ oLerr Masked Language Model (BERT) =--P/ @c0c@ e@ Lexical Embedding Speech Encoder (WavLM) It is eighty uh [MASK] two degrees “It is eighty uh eighty two degrees” Figure 1: MATE is trained with two losses: (1) The MLM which takes concatenated cross-modal representation as input and computes Cum on masked text tokens. (2)Lcrr to align the audio and text latent representations. 2 Approach MATE consists of a pre-trained masked language model BERT, an self-supervised learning (SSL) based speech encoder WavLM (Chen et al., 2021) and a modality matching module (CNN and adapter network), as illustrated in Figure 1. 2.1 System Architecture Masked Language Model We use BERT, a pretrained bidirectional MLM, as the primary component of our rescorer. In this work, we extend BERT to incorporate speech data along with text. The pre-trained embedding layers of BERT serve as the text embedding module, while the intermediate encoder layers take both acoustic and lexical representations as input. Pre-trained Speech Encoder To extract the acoustic representation, we use WavLM model, pretrained on masked speech prediction and speech denoising tasks, achieving state-of-the-art performance on various speech processing tasks and outperforming other models like Wav2Vec2(Baevski et al., 2020) and HuBERT(Hsu et al., 2021) on SUPERB (Yang et al., 2021) benchmark. Cross-modal Adaptation To align the acoustic and lexical representations in the same feature space, we design a cross-modal adaptation module. It is composed of two sub-modules: (i) Convolutional Neural Network (CNN) based subsampling component, to balance the sequence length between the modalities, and (ii) A bottleneck adapter network to project the acoustic representations to the BERT encoder input space. The outputs from the adapter network a and lexical embedding / are concatenated! horizontally a~1, and passed through the BERT encoder layers to fuse the information from the two modalities. 2.2 Alignment Loss Pre-trained Masked Language Models are trained on text corpora (Devlin et al., 2019). To explicitly align audio and text modalities, we propose introducing an explicit alignment loss function, thereby further enhancing the quality of cross-modal learning. We adopt a contrastive loss function to enforce the mapping of acoustic representations a and lexical representations / to a shared feature space. We conduct average pooling at utterance level, and denote the pooled vectors by (@;,1;), from the acoustic or lexical representation a; and 1; respectively. Given acoustic-lexical representations (Gi, lii<i<n where JN is the batch size, we use the paired vectors (;, 1;) as positive samples and the unpaired vectors (4, 1; )izj in the same mini-batch as negative samples. The training objective is to minimize the following contrastive loss £crr with Negative Log-Likelihood (NLL) function: exp(sim(G;, l; Lcrtr Shoes W ) () Dj=1 exp(sim(G:, i;)) where sim(-,-) is a similarity metric, imple mented as dot product in our experiments. Contrastive loss promotes a higher level of similarity between paired acoustic and lexical representations, as compared to unpaired representations, thus enhancing the alignment between the two modalities. 2.3. Training and Inference Training MATE is trained jointly on the MLM objective Cyy_m, Similar to that employed in the pretraining of BERT, and the contrastive loss Cerr. L=Luim +a: Lcerr (2) Following BERT pre-training, a portion of tokens in the text sequence are randomly selected for prediction, and are replaced by the [MASK] token, a 'We have also experimented with a cross-attention based merging mechanism, which leads to inferior performance. --- --random token or left unchanged. In order to optimize the model’s performance, the model is trained end-to-end and all the parameters are updated during the training process. Inference We use pseudo-log-likelihood (PLL) scoring (Wang and Cho, 2019; Salazar et al., 2020) to compute sequence level scores. Given an acoustic sequence a = (s1,...,5R) and a lexical sequence | = (ti,...,t7), let \_p = (ti, sy tp—1, [MASK], tp41,---5 tr), PLL score is computed by summing conditional log probabilities log Pum (Ii, !\;) of each masked lexical token: T PLL(1) = S> logPuim (lila, 4) (3) i=l The final score of an utterance is computed as a linear interpolation of the first-pass ASR confidence score and second-pass PLL score, leveraging the complementary information to improve performance while allowing a trade-off between them. 3 Experiments 3.1 Datasets Training Set The training corpora consist of 10K+ hours of paired audio-text data, sampled from both public and in-house datasets. This data regime is representative of a variety of ASR systems used for various speech applications, with a mix of accents, speakers, sampling rates, and background noise. Less than 5% of the data are synthetic audios generated using AWS Polly Text-to-Speech (TTS) ? neural backend. Evaluation Set We evaluate the proposed MATE approach on both synthetic and real datasets from various domains: MTDialogue (movie-twitter), LibriSpeech (LS) (Panayotov et al., 2015) and VoxPopuli (Wang et al., 2021) are in-domain sets, as the training set includes their corresponding train data splits. Wall Street Journal (WSJ) (Garofolo et al., 1993), ConvAI (in-house), SLURP (Bastianelli et al., 2020) datasets are out-of-domain (OOD) datasets for zero-shot evaluation. MTDialogue (movie-twitter) is based on a public lexical dialogue corpus * which consists of movie subtitles and twitter user interactions. The 7https://aws.amazon.com/polly/ 3https://github.com/Phylliida/Dialogue-Datasets audios are generated from TTS system. MTDialogue dataset is a seen dataset for open-book evaluation; i.e., all its data samples are covered in training data. An subset of 1.2 hour is sampled for evaluation. LibriSpeech(LS) (Panayotov et al., 2015) is a read English speech corpus based on Libri Vox audiobooks. We consider the two official evaluation sets: test-clean and test-other, each with 5.hours of test audios. VoxPopuli (Wang et al., 2021) consists of public political speech, sampled from 2009-2020 European Parliament event recordings. For our evaluation purpose, we utilize a 5-hour subset of VoxPopuli English data. We also evaluate MATE on OOD evaluation sets: ConvAI, WSJ, and SLURP. The Wall Street Journal (WSJ) (Garofolo et al., 1993) corpus contains conventional and spontaneous dictation by journalists. The test_eval93 split of 0.4 hour is selected for our evaluation. ConvAI is based on in-house user utterances of a task-oriented conversational AI system. The typical usage scenarios include booking flights, ordering food and querying health insurance information, etc. The 2.0 hours of audios are generated from TTS system. SLURP (Bastianelli et al., 2020) is a public dataset for smart home virtual assistant development. Top usage scenarios include checking calendar, playing music, and asking about time, etc. We utilized the 10 hr test set for evaluation. Ethical Considerations: We have reviewed all licenses of public datasets, which allow the usage for research and paper publication. The in-house dataset ConvAI is internally approved for research purposes. All datasets are sets are de-identified to ensure anonymity. We also make sure the datasets cover various English accents, speakers and backgrounds. 3.2 Evaluation Metrics We use word error rate (WER) and content word error rate (CWER) as the evaluation metrics. CWER is computed on content words only (e.g., “pizza”, “parliament”, “airline”), where we apply rule based method to filter out a predefined block-list of function words. Furthermore, we evaluate Spoken Language Understanding (SLU) performance on SLURP dataset using standard SLU metrics (accuracy and FI score); SLU predictions (scenario, action and entity) are generated by a bi-directional Long Short-Term Memory (BiLSTM) NLU module (Appendix A). --- --In-domain Out-of-domain MTDialogue LS test-clean LS test-other — Voxpopuli WSJ ConvAI SLURP WER CWER WER CWER WER CWER WER CWER|WER CWER WER CWER WER CWER @) No rescoring 947 1463 6.75 8.07 ‘11.98 15.61 11.06 1033 | 816 875 5.89 9.00 2491 29.Q) GPT2-text 932 1437 645 7.78 11.70 “15.11 10-72 9.94 | 7.64 405.76 8.66 24.91 29.@) BERT-text 9.05 13.88 5.50 7.20 10.70 1445 10.33 9.96 | 646 820 5.38 8.37 2448 29.() LAStescoring [9.27 14.15 67 7.99 11.97 15.59 11.02 10.21 | 801 855° 5.81 884 2491 29.(5) Multi-modal-GPT2] 9.24 14.17 6.35 7.69 11.54 14.93 10.56 9.83 | 7.55 8.20 5.69 8.59 24.89 29.(6) MATE-NA 9.05 13.90 5.55 7.29 10.75 “14.51 1034 9.92 | 649 810 540° 836 2446 29.(7) MATE-MSE 749 11.41 5.22 695 10.31 13.97 10.10 9.62 | 610 7.65 5.07 7.92 23.84 28.(8) MATE (ours) 7.64 11.70 5.16 6.84 10.30 13.81 9.91 9.47 | 6.01 7.46 5.10 7.91 23.77 28.Parameter-Efficient Tuning 0) Frozen-ME 921 1422557 7.34 10.82 14.65 1037 9.80 [655 815° 542 834 2439 29.(10) WavLM-adapter | 9.15 14.02 5.58 741 10.81 14.69 10.23 9.86 | 6.52 8.05 5.47 8.39 24.56 29.(e85) ME-adapter 9.19 14.12 5.56 743 10.79 14.63 10.09 9.60 | 643 820 542 8.35 24.34 29.Table 1: Performance measured by WER | and CWER |. All models except (2-3) are multi-modal. (2) GPT2-text (Radford et al., 2019a): Full fine-tuning of GPT2 on training corpora transcriptions. (3) BERT-text (Salazar et al., 2020; Devlin et al., 2019): Full fine-tuning of BERT on training corpora transcriptions, also denoted as "text-only baseline". (4) LAS rescoring (Sainath et al., 2019): A multi-modal baseline with LAS head rescoring (attention based LSTM decoder) accepting acoustic information from WavLM. (5) Multi-modal-GPT2: A multi-modal uni-directional baseline with GPT2, accepting acoustic information from WavLM. (6) MATE-NA: MATE without additional alignment loss; (7) MATE-MSE: MATE trained with MSE loss instead of contrastive loss. (9) Frozen-ME (Masked Encoder): Fine-tune all parameters in multi-modal system except masked encoder (BERT) layers with only MLM objective. (10) WavLM-adapter: add bottleneck adapter to speech encoder (WavLM) and do adapter-tuning on WavLM, all other parameters are frozen. (11) ME (Masked Encoder)-adapter: do adapter-tuning on masked encoder (BERT), all other parameters are frozen. 4 Results and Analysis We summarize the observations and analysis of the results from our experiments * as follows: MATE excels at both in-domain and out-ofdomain generalization: Table | summarizes the performance of the proposed MATE and multiple baseline models, under various settings, across in-domain and OOD datasets. Overall, we observe that our proposed approach (row 8) significantly outperforms text-only baseline (row 3) on in-domain datasets indicating that audio information helps even when we have sufficient target domain corpus for fine-tuning. Furthermore, results on OOD datasets indicate that MATE generalizes much better to new domains in the complete absence of domain data (zero-shot setting), when compared to the text-only baseline, by utilizing the rich information from audio. MLMs are more effective multi-modal rescorers than autoregressive LMs: Rows 2-5 indicate a significant performance gap between BERT and autoregressive rescorers (LAS/GPT-2). BERT-Text, which is a text-only baseline, outperforms even the multi-modal GPT2 indicating the root cause of the gap is the lack of bi-directional (left and right) context in GPT2 which is necessary for reliable 4 Appendix B contains experimental setup details, including hyperparameters and infrastructure setting. and effective LLM scoring, hence validating the choice of MLM in MATE. Alignment loss gives significant performance boost: To study the effect of alignment loss, we train the multi-modal rescorer with two loss functions: Mean squared error (MSE) loss and contrastive loss. Significant performance gains (rowvs. row 7-8) in Table | indicate that explicit alignment techniques greatly improve learning of multimodal representations. Specifically, contrastive loss not only aligns relevant pairs like MSE loss, but also promotes distancing irrelevant samples, leading to improved generalization on OOD sets. Parameter-efficient fine-tuning results in limited gains: Rows 9-11 study the performance of a multi-modal rescorer under different parameter efficient fine-tuning settings. We observe that performance degrades as we move from full finetuning to adapter-tuning and freezing the full BERT encoder layers, indicating that fine-tuning BERT encoder is the most beneficial in terms of performance improvement. As expected, in comparison to model with full fine-tuning (row 6), rows 9-11 exhibit lower performance. This suggests that frozen or parameter-efficient training methods may lack the model capacity to fully leverage the acoustic information present in the multi-modal data. --- --5.6% 13.0%, gg 2908 23g 9g 1.6% 200% — 49.79¢——20.3%—20 5% —20 5520.9.2% 9.2% 8.0% 299 — 8.98e0— 65% 7 The-—6.7%——6.9% 0.0 0.28 0.83 2.78 8.— —BERT-text, Voxpopuli_ © — —MATE, Voxpopuli — —BERT-text, WS! — MATE, WS) Figure 2: Relative WER reduction (over first-pass) versus domain specific training data size. Scenario Action Entity No rescoring 78.01 72.53 53.GPT2-text 78.01 72.53 53.Multi-modal-GPT2 78.07 72.65 53.BERT-text 77.72 72.45 53.MATE 78.76 73.70 54.Table 2: Zero-shot evaluation on SLURP SLU task: Accuracy for Scenario/Action, and F1 score for Entity. MATE is the most effective few-shot learner: To study the effect of few-shot learning, we plot the relative WER reduction (WERR) on Voxpopuli and WSJ datasets across different resource conditions as shown in Figure 2. We observe that MATE transfers well to the new domains in the zero-shot setting with no training or domain data at all. Fewshot performance clearly improves with more examples and goes a reasonable way towards closing the gap from zero-shot performance to full finetuning performance. We also observe that MATE consistently has superior performance to text-only baseline across both datasets, confirming the ability to rapidly adapt to new domains by leveraging additional information from the audio modality. MATE achieves best zero-shot performance improvement on downstream SLU tasks: To evaluate the effectiveness of the proposed approach on the end goals in a dialog system, we compare it with other baselines using metrics such as scenario/action accuracy and entity Fl score in a zeroshot setting on SLURP dataset. From results in Table 2, we observe that MATE consistently outperforms? the other baselines on end-to-end goals indicating that the improvements are mainly on >The SLURP is a challenging corpus, which mimics the noisy use cases of smart home assistants. Hence, by improving rescoring method alone, we achieve less than 2% absolute improvement in WER and SLU metrics. recognition of content words and slot entities. © 5 Conclusions We propose a novel multi-modal rescorer, MATE, which achieves significant WER, CWER reduction on in-domain and OOD datasets. In zero-shot and few-shot settings, MATE performs well on unseen domains and adapts rapidly with limited data. The domain generalization capability of MATE makes it an effective choice as a second-pass rescorer for scaling ASR systems to new domains. 6 Limitations One limitation of our approach is that incorporating acoustic features from an SSL speech encoder, in our case WavLM, introduces extra latency overhead, as we use a standalone ASR model for firstpass. Therefore, our approach may not be appropriate for certain applications that have exceptionally low latency constraints. Another limitation is that while multi-modal LLMs have the potential to improve ASR performance, they can be more complex and harder to interpret than text-only LLMs. This makes it more challenging to understand the model’s decision making process or debug any potential errors. Qualitative examples are presented in Appendix E. --- --References Alexei Baevski, Steffen Schneider, and Michael Auli. 2019. vq-wav2vec: Self-supervised learning of discrete speech representations. ArXiv, abs/1910.05453. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems, volume 33, pages 12449-12460. Curran Associates, Inc. Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philémon Brakel, and Yoshua Bengio. 2016. End-toend attention-based large vocabulary speech recognition. In 20/6 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4945-4949, Ankur Bapna, Yu-an Chung, Nan Wu, Anmol Gulati, Ye Jia, Jonathan H Clark, Melvin Johnson, Jason Riesa, Alexis Conneau, and Yu Zhang. 2021. Slam: A unified encoder for speech and language modeling via speech-text joint pre-training. arXiv preprint arXiv:2110,10329. Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. 2020. SLURP: A spoken language understanding resource package. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7252-7262, Online. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc. Jinglun Cai, Mingda Li, Ziyan Jiang, Eunah Cho, Zheng Chen, Yang Liu, Xing Fan, and Chenlei Guo. 2023. Kg-eco: Knowledge graph enhanced entity correction for query rewriting. In JCASSP 2023 -IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4960-4964. Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Micheal Zeng, and Furu Wei. 2021. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. JEEE Journal of Selected Topics in Signal Processing, 16:1505-1518. Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ramabhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen. 2022. MAESTRO: Matched Speech Text Representations through Modality Matching. In Proc. Interspeech 2022, pages 4093-4097. Ethan A. Chi, Julian Salazar, and Katrin Kirchhoff. 2021. Align-refine: Non-autoregressive speech recognition via iterative realignment. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1920-1927, Online. Association for Computational Linguistics. Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. End-to-end continuous speech recognition using attention-based recurrent nn: First results. In NJPS 2014 Workshop on Deep Learning, December 2014. Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models. In NAACL 2022: the 2022 Conference of the North American chapter of the Association for Computational Linguistics: human language technologies, pages 16931706. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics. Ankur Gandhe and Ariya Rastrow. 2020. Audioattention discriminative language model for asr rescoring. In ICASSP 2020. Heting Gao, Junrui Ni, Kaizhi Qian, Yang Zhang, Shiyu Chang, and Mark Hasegawa-Johnson. 2022. WavPrompt: Towards Few-Shot Spoken Language Understanding with Frozen Language Models. In Proc. Interspeech 2022, pages 2738-2742. John S. Garofolo, David Graff, Doug Paul, and David Pallett. 1993. CSR-I (WSJO) Complete LDC93S6A. Linguistic Data Consortium. Alex Graves. 2012. Sequence transduction with recurrent neural networks. ArXiv, abs/1211.3711. --- --Alex Graves and Navdeep Jaitly. 2014. Towards endto-end speech recognition with recurrent neural networks. In Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1764— 1772. PMLR. Anmol Gulati, James Qin, Chung Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020. Conformer: Convolution-augmented transformer for speech recognition. In Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH. Demi Guo, Alexander Rush, and Yoon Kim. 2021. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 48844896, Online. Association for Computational Linguistics. Awni Y. Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, and Andrew Y. Ng. 2014. Deep speech: Scaling up endto-end speech recognition. CoRR, abs/1412.5567. Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. 2012. Deep neural networks for acoustic modeling in speech recognition. Signal Processing Magazine. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In ICML, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451-3460. Ke Hu, Ruoming Pang, Tara N. Sainath, and Trevor Strohman. 2021. Transformer based deliberation for two-pass speech recognition. In 202] IEEE Spoken Language Technology Workshop (SLT), pages 68-74. Ke Hu, Tara N Sainath, Yanzhang He, Rohit Prabhavalkar, Trevor Strohman, Sepand Mavandadi, and Weiran Wang. 2022. Improving deliberation by textonly and semi-supervised training. arXiv preprint arXiv:2206.14716. Ke Hu, Tara N Sainath, Ruoming Pang, and Rohit Prabhavalkar. 2020. Deliberation model based two-pass end-to-end speech recognition. In JCASSP 20202020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7799— 7803. IEEE. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980. Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551. Yichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu, Tao Qin, Xiangyang Li, Edward Lin, and Tie-Yan Liu. 2021. Fastcorrect: Fast error correction with edit alignment for automatic speech recognition. In Advances in Neural Information Processing Systems, volume 34, pages 21708-21719. Curran Associates, Inc. Erik McDermott, Hasim Sak, and Ehsan Variani. 2019. A density ratio approach to language model fusion in end-to-end automatic speech recognition. InIEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 434-441. Yajie Miao, Mohammad Abdelaziz Gowayyed, and Florian Metze. 2015. Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding. 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 167-174. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). A. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019a. Language models are unsupervised multitask learners. Alec Radford and Karthik Narasimhan. 2018. Improving language understanding by generative pretraining. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019b. Language models are unsupervised multitask learners. OpenAl blog, 1(8):9. Navid Rekabsaz, Simone Kopeinik, and Markus Schedl. 2021. Societal biases in retrieved contents: Measurement framework and adversarial mitigation of bert rankers. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’21, page 306-316, New York, NY, USA. Association for Computing Machinery. Tara N. Sainath, Ruoming Pang, David Rybach, Yanzhang He, Rohit Prabhavalkar, Wei Li, Mirké Visontai, Qiao Liang, Trevor Strohman, Yonghui Wu, Jan McGraw, and Chung-Cheng Chiu. 2019. TwoPass End-to-End Speech Recognition. In Proc. Interspeech 2019, pages 2773-2777. --- --Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. 2020. Masked language model scoring. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699-2712, Online. Association for Computational Linguistics. R. Schwartz and Steve Austin. 1991. A comparison of several approximate algorithms for finding multiple (n-best) sentence hypotheses. JCASSP 9]: 199] International Conference on Acoustics, Speech, and Signal Processing, pages 701-704 vol. 1. Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam Coates. 2018. Cold Fusion: Training Seq2Seq Models Together with Language Models. In Proc. Interspeech 2018, pages 387-391. Tzu-Wei Sung, Jun-You Liu, Hung-yi Lee, and Linshan Lee. 2019. Towards end-to-end speech-to-text translation with two-pass decoding. In ICASSP- 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7175— 7179. Edmondo Trentin and Marco Gori. 2001. A survey of hybrid ann/hmm models for automatic speech recognition. Neurocomputing, 37(1):91-126. Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, S. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot learning with frozen language models. In Advances in Neural Information Processing Systems, volume 34, pages 200-212. Curran Associates, Inc. Takuma Udagawa, Masayuki Suzuki, Gakuto Kurata, Nobuyasu Itoh, and George Saon. 2022. Effect and Analysis of Large-scale Language Model Rescoring on Competitive ASR Systems. In Proc. Interspeech 2022, pages 3919-3923. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, £ ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Alex Wang and Kyunghyun Cho. 2019. BERT has a mouth, and it must speak: BERT as a Markov random field language model. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, pages 30-36, Minneapolis, Minnesota. Association for Computational Linguistics. Changhan Wang, Morgane Riviére, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021. VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Yingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin, Nenghai Yu, and Tie-Yan Liu. 2017. Deliberation networks: Sequence generation beyond one-pass decoding. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Liyan Xu, Yile Gu, Jari Kolehmainen, Haidar Khan, Ankur Gandhe, Ariya Rastrow, Andreas Stolcke, and Ivan Bulyko. 2022. Rescorebert: Discriminative speech recognition rescoring with bert. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6117-6121. Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Ko tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, ShangWen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung yi Lee. 2021. SUPERB: Speech Processing Universal PERformance Benchmark. In Proc. Interspeech 2021, pages 1194-1198. Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and Shankar Kumar. 2020. Transformer transducer: A streamable speech recognition model with transformer encoders and mn-t loss. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7829-7833. Ding Zhao, Tara N. Sainath, David Rybach, Pat Rondon, Deepti Bhatia, Bo Li, and Ruoming Pang. 2019. Shallow-Fusion End-to-End Contextual Biasing. In Proc. Interspeech 2019, pages 1418-1422. Guolin Zheng, Yubei Xiao, Ke Gong, Pan Zhou, Xiaodan Liang, and Liang Lin. 2021. Wav-BERT: Cooperative acoustic and linguistic representation learning for low-resource speech recognition. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2765-2777, Punta Cana, Dominican Republic. Association for Computational Linguistics. --- --Appendix A SLURP SLU semantics and NLU module SLURP dataset consists of user interactions with smart home virtual assistants. The semantics are annotated with three levels of semantics: Scenario, Action and Entity. For example, ASR transcript “how do I make a turkey” is the annotated with semantics “scenario: cooking | action: recipe | entities: [(type: food | filler: turkey)]”. The SLU semantics spans over 18 different scenarios,defined actions and 55 different entity types (Bastianelli et al., 2020). In the NLU module, we treat semantics prediction as a sequence-to-sequence problem. Specifically, given an ASR transcript after rescoring “how do I make a turkey”, the goal is to predict: “‘scenario: cooking | action: recipe | entities: [(type: food | filler: turkey)]”. The NLU module has an encoder-decoder structure based on bi-directional Long Short-Term Memory (Bi-LSTM). Both the encoder and the decoder have hidden dimention 256. The encoder has 2 layers while the decoder has 3 layers. We use Negative Log-Likelihood (NLL) loss for as training objective for sequence prediction. We train the model on ground truth <transcript, NLU semantics> paris from SLURP training dataset. The learning rate is set to 3e-4 and the training is conducted for 20 epochs with batch size 16. B_ Experimental Setup MATE has 217M parameters in total. For both masked language model and speech encoder, we utilize base size models for efficiency (BERT-Base 110M and WavLM-Base+ 95M respectively). The convolutional network contains 3 layers withchannels with strides (2,1,2) and kernel widths (3, 1, 1). The bottleneck adapter layer has compression factor 0.5. The training experiment for MATE is conducted end-to-end: we train all modules simultaneously. We use Adam optimizer (Kingma and Ba, 2014) with linear decay of learning rate. We set initial learning rate to 5e — 5 and batch size to 32. We searched the hyperparameter a in Eq.2 with (1.0, 3.0, 10.0), and the final value is set to 1.0. The training was conducted for 88/¢ steps. All the experiments are performed with NVIDIA Tesla V100 GPUs ina single run. The training for MATE takes 39.7 hours on a Tesla V100 8-GPU machine. Our first-pass ASR model has a conformerCTC (Gulati et al., 2020) architecture. which is trained on 50K+ hours audio-transcript paired data. The conformer encoder consists of 20 layers of conformer blocks with hidden dimension 2048; while the shallow decoder is a single Transformer-based layer with the same hidden dimension of 2048. The conformer-CTC model has approximately 140M parameters, We use SCTK’ package for WER and CWER evaluation. CWER has the same logic as WER computation except that we filter out function words. We use SLURP toolkit® for SLU semantics evaluation. C Attention Visualization We visualize the learned self-attention plots extracted from the proposed MATE model in Figure 3. The model has 12 Transformer layers and withheads in each multi-head self-attention. We selected 6 representative plot from the 144 total attention plots with a sample utterance from wsj_evaltest set. The input utterance has 33 tokens andframes for the acoustic feature, the acoustic features are appended to the lexical embedding before fed into the BERT model. Our observations are listed as follows: * (a) (b) (c) and (d) The plots highlight the border of the text input and audio input (the vertical straight line on position 32). We can conclude that even without feeding any modality border information to MATE, it can learn the border of two modalities itself. (a), (d), (e) and (f) The monotonic audio-totext position alignment is clearly shown in the plots. This indicates that the acoustic and lexical representations are successfully mapped to one unified feature space. Interestingly, plots (a), (e) and (f) show that text-to-audio position alignment can also be learned by MATE. D_ Risks The proposed system, MATE, incorporates both pre-trained language model (BERT) and speech model (WavLM) into its design. Such pre-trained models can contain biases and stereotypes against Thttps://github.com/chinshr/sctk Shttps://github.com/pswietojanski/slurp --- --(a) Layer 4 head 3 (b) Layer 6 heado 0 oO 202 40 © © (0) Layer 7 head 5 (d) Layer 8 heado 2 0 © > 0 © © (e) Layer 11 head 1 (f) Layer 11 headFigure 3: Selected attention plots from the self-attention layers of the 12-layer BERT encoder The sample utterance (from wsj_eval93) contains 110 total frames: the first 33 frames are lexical embedding, followed byacoustic embedding frames. The utterance is: "last year new hampshire enacted legislation enabling banks from outside the state to acquire new hampshire banks but restrictions in the bill discouraged potential buyers" certain religion, race and gender groups (Rekabsaz et al., 2021; Delobelle et al., 2022). E Qualitative Examples To further understand why the proposed approach, MATE yields more accurate prediction, we selected several representative cases from the evaluation sets. Table 3 clearly shows that MATE tends to correct more vocabulary or grammar errors present in the n-best list. We observe MATE is able to correct many ASR errors which are not resolvable by text information alone. In the example from SLURP, both “who in the hallway” and “hoover the hallway” are plausible utterances in an informal style of daily speech. With the aid of acoustic information, MATE is able to assign higher score to the correct utterance “hoover the hallway”. --- --Dataset Utterance SLURP Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE remove tuesday alarm of nine am move to alarm of nine am remove tuesday alarm at nine a m Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE hoover the hallway who in the hallway hoover the hallway Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE cancel business meeting on wednesday council business meeting on wednesday cancel business meeting on wednesday Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE can you let delta know i am never using them again can you let doctor know i am never using them again can you let delta know i am never using them again Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE i want to play fifa seventeen i want to leave for seventeen i want to play fifa seventeen Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE what do you know about fringe in edinburgh next year what do you know about french in edinburgh next year what do you know about fringe in edinburgh next year Voxpopuli Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE for example the report talks about the rule of law and corruption for example the report talks about the rule of law on corruption for example the report talks about the rule of law and corruption Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE i have met them they are young capable and visionary i have met them they are young capable and missionary i have met them they are young capable and visionary MTDialogue Ground Trut Rescored 1-best Rescored 1-best by BERT-text by MATE it’s muffled it’s muff it’s muffled Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE how much she got to pay how much he got to pay how much she got to pay ConvAl Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE why did the noodle box in greensborough fail its health inspection why did the noodle box in greensboro fail its health inspection why did the noodle box in greensborough fail its health inspection Ground Trutl Rescored 1-best Rescored 1-best by BERT-text by MATE tell me about duty free shopping tell me about duty free shop tell me about duty free shopping Table 3: Qualitative examples: We contrast the 1-best outputs of BERT-text model and MATE in reference to ground truth. We can observe that MATE improves recognition of content words and slot entities.