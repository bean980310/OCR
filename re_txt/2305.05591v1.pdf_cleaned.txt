--- --2305.05591v1 [cs.SD] 9 MayarXiv AUDIOSLOTS: A SLOT-CENTRIC GENERATIVE MODEL FOR AUDIO SEPARATION Pradyumna Reddy* University College London ABSTRACT In a range of recent works, object-centric architectures have been shown to be suitable for unsupervised scene decomposition in the vision domain. Inspired by these methods we present AudioSlots, a slot-centric generative model for blind source separation in the audio domain. AudioSlots is built using permutation-equivariant encoder and decoder networks. The encoder network based on the Transformer architecture learns to map a mixed audio spectrogram to an unordered set of independent source embeddings. The spatial broadcast decoder network learns to generate the source spectrograms from the source embeddings. We train the model in an end-to-end manner using a permutation invariant loss function. Our results on Libri2Mix speech separation constitute a proof of concept that this approach shows promise. We discuss the results and limitations of our approach in detail, and further outline potential ways to overcome the limitations and directions for future work. Index Terms— Speech separation, object-centric representation 1. INTRODUCTION Recently there has been a lot of research into neural network based architectures that operate on set-structured data and architectures that learn to map from unstructured inputs to set-structured output spaces. In particular, in the vision domain, slot-centric or objectcentric architectures underpin recent advances in object detection and unsupervised object discovery These object-centric architectures have an inbuilt inductive bias of permutation equivariance, making them a natural fit for the task of audio separation. In this paper we apply the core ideas from these architectures to the problem of sound separation: the task of separating audio sources from mixed audio signals without access to privileged knowledge about the sources or the mixing process. Sound separation is inherently a set-based problem, as the ordering of the sources is arbitrary. We frame sound separation as a permutation-invariant conditional generative modeling problem: we learn a mapping from a mixed audio spectrogram to an unordered set of independent source spectrograms. Our method, AudioSlots, separates audio into individual latent variables per source, which are then decoded into individual source spectrograms. It is built using permutation-equivariant encoder and decoder functions based on the Transformer architecture (4) and thus invariant to the ordering of the source latent variables (“slots”). To evaluate the promise of such an architecture, we train AudioSlots using a matching-based loss to generate separate sources from the mixed audio-signal. We demonstrate our method on a simple two-speaker speech separation task from Libri2Mix [5]. *Work done while interning at Google. Scott Wisdom, Klaus Greff, John R. Hershey, Thomas Kipf Google Research While our results primarily constitute a proof of concept for this idea, we find that sound separation with slot-centric generative models shows promise, but comes with certain challenges: the presented version of our model struggles to generate high-frequency details, relies on heuristics for stitching independently predicted audio chunks, and still requires ground-truth reference audio sources for training. We are optimistic that these challenges can be overcome in future work, for which we outline possible directions in this paper. 2. RELATED WORK Our work explores a novel set-based generative modeling approach for sound separation. In the following, we provide a brief overview on recent prior learning-based approaches for sound separation as well as on set-based (or slot-centric) neural network architectures used in other domains. Sound separation. A variety of neural network methods have been proposed for supervised sound separation, differing in terms of their sound reconstruction mechanisms and their overall architectures. Mask-based methods reconstruct sound by predicting separation masks that are applied to an analysis/synthesis basis representation of the input audio signal (such as STFT or learned basis), Alternatively, direct reconstruction gnals or their spectra without explic itly estimating mas! Many generic architectures have been proposed for sound separation, including recurrent networks [16], convolutional networks , U-nets attention networks , and their combinations. These address the arbitrary permutation of output sources using a permutation-invariant loss during training (7|[8}. Some methods have gone further to address permutation invariance at the architecture level by producing un-ordered representations corresponding to each source. Deep clustering and deep attractor networks (6][18][7], employ a permutation-equivariant architecture, which operates in an attention-like way over embeddings of each time-frequency bin. Our approach produces an embedding for each source, using a slot-based attention mechanism, and decodes it to directly estimate the source spectrogram, using a NeRF-like method. This differs from mask-based methods in using direct prediction, and the method of direct prediction using NeRF architecture is novel. The slot-based attention mechanism is different from previous attention networks for sound separation, and is more closely related to deep clustering. However our attention method works on higher level spectrogram regions rather than individual time-frequency bins, and uses generalpurpose attention mechanisms instead of simple affinity-based methods. Recent unsupervised approaches, such as mixture invariant training (MixIT) us only audio mixtures for training. While we only explore supervised sound separation using ground-truth isolated reference sources in our work, a training setup like in MixIT --- --Encoder Decoder Pre-processing Matching Spatial Decoder ¥ Transformer Fig. 1: Architecture overview. The input waveform is first cropped and transformed into a spectrogram. Then the neural network encodes the spectrogram to a set of permutation invariant source embeddings s}...,,, these embeddings are decoded to generate a set of individual source spectrograms. The whole pipeline is supervise function. is orthogonal to our approach and would be interesting to explore in future work. Slot-centric neural networks. Neural networks that operate on unordered sets of features have been studied for some time {23}. Most closely related to our work are approaches that generate an unordered set of outputs conditioned on some input data [24] and methods that use a set of latent variables (“slots”) to model permutation-invariant aspects of the input , such as objects in visual scenes. We refer to the latter as slot-centric neural networks. In the context of vision, this class of models forms the basis for many modern scene understanding approaches, including object-detection anoptic segmentation [26], and unsupervised object discovery In our work, we demonstrate that slotcentric generative models similarly hold promise for compositional, permutation-invariant tasks in audio processing, specifically for separating individual audio sources from a mixture. 3. METHOD We present a generalized permutation invariant training framework for supervised sound separation. Unlike previous methods, we approach the source-separation task from a generative perspective. The main objective of our method is to project the input audio into a set of embeddings each representing a different source in the input. These embeddings are then used to generate the magnitude spectrograms of individual sources in a permutation invariant manner. The whole pipeline is supervised with the ground-truth source spectrograms using a permutation-invariant loss. In rest of this section we elaborate on different steps in our training pipeline. Preprocessing: Given a mixture waveform during training we first randomly crop a 0.5-second audio clip. Then following we transform the clip into a spectrogram using a short-time Fourier transform with window size 512 and hop size 125. Then the absolute values of the complex spectrograms are non-linearly scaled by exponentiating them by power of 0.3 to emphasize low values. These scaled absolute values are passed as the input to the next step. Encoding: To infer source embeddings, the input spectrogram is first encoded using a ResNet-34 network [29] to a 32 x 8 grid of encoded “image” features z. We use a reduced stride in the ResNet root block to retain a higher spatial resolution. Next a transformer with 4 layers maps z to source embeddings s1..., where n is the number of sources and s; € R*. Unlike the original formulation in each transformer layer we first perform a self-attention operation between query vectors q and then perform cross-attention between the outputs of the self-attention step and z. We use same variables as both the key and value in self-attention and cross-attention steps. with the groundtruth source spectrograms using a matching based permutation invariant loss The initial queries q of dimensionality 4096 are learned via backpropagation, similar to DETR [I]. Decoding: We use a spatial broadcast decoder [30] to generate individual source spectrograms from sj...,. First each embedding 8; are copied across a 2D grid to create tensor g; of shape F’ x T x dwhere F, T are the frequency bins and timesteps of the output spectrogram. Then positional embeddings with fourier features are appended to g; making its shape F x T’ x (d + e) where e is the size of each fourier feature embeddings. Subsequently a fully connected network with shared parameters is applied across all the vectors in g; to arrive at a set of spectrograms each with shape F’ x T. Note that this decoder is similar to a NeRF model [19], which similarly takes positional codes as input and learns a fully-connected network to produce coordinate-dependent outputs. In our case, we directly produce spectrogram values, arranged on a 2D image grid, as outputs and further condition the network on the respective latent source embeddding s; for each generated spectrogram. Objective: The predicted spectrograms are generated in no predetermined ordering as the source separation problem is permutation invariant. Therefore we need to match estimated spectrograms with ground-truth spectrograms to calculate the loss of the network. Among all the possible matches between ground-truth and estimated spectrograms we seek to find the optimal assignment with minimum reconstruction error. We suggest to use the Hungarian matching algorithm to solve this assignment problem because of its speed, accuracy and ability to handle large numbers of items efficiently (although we only use datasets with a small number of sources in our experiments). It works by assigning costs or weights to each possible pair and then selects pairs which minimize the total cost or weight associated with them. Finally the mean squared error between matched ground-truth and estimated spectrograms is minimized as the training objective to optimize the network parameters. Source Separation: During testing, given an input mixture waveform we first break it into multiple non-overlapping waveforms of length 0.5-seconds. We add zero-padding at the end in case the input waveform length is not evenly divisible into chunks of 0.5s. These waveforms are preprocessed as mentioned above and are passed as inputs to the network, trained using the above pipeline to estimate absolute values of the spectrograms of individual sources. The estimated spectrograms are first rescaled by exponentiating them by power of 1/0.3 to invert the scaling done during preprocessing. These rescaled estimates are used to calculate the masks in the oracle method to create complex spectrogram estimates of individual sources from the input. Given input spectrogram J the output source spectrograms are computed as m; * I where m, is the mask corresponding to the i‘ source estimated using the spectrograms --- --Table 1: Results. We use the estimated absolute spectrograms as masks over the input complex spectrogram to produce complex spectrograms of the individual sources In the table below we present SI-SNR[dB] and SI-SNRi[dB] values with IBM and Wienerlike masking. Higher is better. Notice that there is only a minimal drop in performance between Autoencoder and AudioSlots showing that our pipeline is able to learn to separate speech well. SI-SNR_ SI-SNRi_ SI-SNR__ SI-SNRi Masking Type IBM IBM Wiener Wiener Oracle 12.07 12.07 12.32 12.Autoencoder 10.19 10.20 10.15 10.AudioSlots 09.50 09.50 09.96 09.Oracle (1-sec) 13.15 13.16 13.46 13.AudioSlots (1-sec) 09.66 09.67 10.20 10.predicted by the neural network. These complex spectrograms are inverted to waveforms using an inverse short-time Fourier transform (STFT) and then stitched together, resolving matching using the best match with the ground-truth signal for simplicity. Training: We train using Adam for 300k steps with a batch size of 64, a learning rate of 2e-4, 2500 warmup steps and a cosine decay schedule. 4, EXPERIMENTS We evaluate the performance of our method on speech separation using the Libri2Mix dataset. We use the anechoic version of the dataset. Each instance in the dataset is sampled at 16kHz and 10 seconds long. Libri2Mix contains contains utterances from both male and female speakers drawn from LibriSpeech . The train360-clean split of the dataset contains 364 hours of mixtures and the sources are drawn without replacement. As mentioned above we use the masking to estimate the complex spectrograms of the individual sources using the input spectrogram and the network predictions. There are various masking functions that can be used [33]. In our experiments we use the ideal binary mask (IBM) and Wiener filter like mask as mask functions which are defined as: IBM: m; = 1 . 0 otherwise 7 = argmax(m) — _ (mi)? ~ DL, (mi)? Metrics: We measure the separation performance using scaleinvariant signal-to-noise ratio (SI-SNR) and SI-SNR improvement (SI-SNRi). Let y denote the target and y denote the estimate obtained by our method. Then SI-SNR measures the fidelity between y and ¥ within an arbitrary scale by rescaling the target: “Wiener like”: m; - low? SI-SNR(y, 9) = 10log,, ey — awhere a = argmin,||ay — §||? = y7 G/||y||?. The SI-SNRi is the difference between the SI-SNR of each source estimate after processing and the SI-SNR obtained using the input mixture as the estimate for each source. During evaluation we first match the targets and estimates to maximize SI-SNR and then average the resulting SI-SNR and SI-SNRi scores. Results: In Table[I] we compare our performance with an autoencoder variant of our method (which receives ground-truth reference sources as input) and the performance of separation obtained using the (preprocessed) ground-truth signals. The metrics computed using the ground-truth signals represent the maximum values that can be obtained with our (lossy) preprocessing. In the autoencoder variant we train the network to reconstruct the individual source signals with n = 1. We then use the individual estimates as the masks over the complex spectrogram of the mixture. Since the spectrograms contain high-frequency features, this would help us understand the ability of our architecture to faithfully represent these features. We also present an ablation by increasing the crop length in the preprocessing step to 1 second. The difference in performance between the autoencoder variant and the separation model is only 0.18 + 0.01. This indicates that our method, AudioSlots, is able learn speech separation well, closely approaching the performance of the baseline which receives fullyseparated sources as input. Still, there is stantial headroom for improvement, both in terms of our model as well as our overall pipeline: prior maskingbased approaches {13} already solve Libri2Mix speaker separation to an impressive degree, achieving significantly higher SISNR values than we report here. Very recently, diffusion-based approaches have also shown competitive performance . This gap is in part due to our lossy preprocessing pipeline: for example, computing STFTs on pre-chunked audio (done here for simplicity) introduces border artifacts which even reduces our ground-truth SI-SNR scores below what other models can achieve. We further zero-pad all audio signals to the same length for simplicity. Also, since we create masks from the generated spectrograms, we are also bound y the limitations of mask-based methods, e.g. that masked spectral content cannot be regenerated. Limitations: Our experimental comparison highlights the main imitation of our method, which is reconstruction fidelity: both the autoencoder baseline and our AudioSlots model encode the signal using a latent bottleneck representation and tend to discard certain igh-frequency details (among others). This is also qualitatively visible in Figure | The comparison further shows that the crop length affects the separation performance: while this can be partially explained by boundary artifacts due to our simplistic cropping approach, it also hints towards sensitivity of AudioSlot’s ability to oth separate and reconstruct audio spectrograms on chunk length. Discussion: Our results show promise for addressing audio separation using slot-centric generative models that represent the audio using a set of source-specific latent variables. This is a significant departure from earlier methods that directly operate on the input audio using e.g. a masking-based approach [20]. Learning sourcespecific latent variables further has the benefit that these decomposed latent variables can likely be used not just for generation, but also for recognition t similar to how slots in object-centric computer vision models serve as a basis for object detection and segmentation. We are optimistic that the current limitations of our approach can be overcome in future work: * To address the issue of reconstruction fidelity (blurry reconstructions for high-frequency features), it is likely that moving away from a deterministic feedforward decoder to e.g. an autoregressive decoding approach, as in AudioLM , or an iterative diffusionbased decoder, as in [39], can bridge the gap to high-fidelty generation. + Atpresent, AudioSlots assumes supervision in the form of groundtruth sources. An extension to fully-unsupervised training on raw, --- --Groundtruth Autoencoder Ours & && ag && aFig. 2: Comparison between absolute value of the individual source spectrograms of Groundtruth, Autoencoder estimates and AudioSlots (Ours) estimates. The input spectrogram (top) is a mixture and rest of the rows show the spectrograms of the individual sources. The input and groundtruth spectrograms are preprocessed using the steps mentioned in Sec] Notice that our method is able to reconstruct harmonics fairly well, however struggles with estimating the high-frequency features (see highlighted example regions). mixed audio would be desirable. To this end, we explored replacing the Transformer in AudioSlots with a Slot Attention [3] module which has an inductive bias towards decomposition that allows it to be trained unsupervised in the context of visual scene decomposition. In initial experiments we found, however, that this inductive bias might not suffice for decomposing audio spectrograms in a fully-unsupervised fashion. A supervised version of AudioSlots with a Slot Attention module, however, performed similar to the version with a Transformer module in initial experiments, highlighting that further exploration is still promising for future work. ¢ We think that the limitation of processing individual chunks in isolation, which requires post-hoc stitching, can be overcome by using a sequential extension of the model, where slots of the past time step are used as initialization for the next time step as in Slot Attention for Video We leave this for future work. 5. CONCLUSION We present AudioSlots, a slot-centric generative architecture for audio spectrograms. We demonstrate a proof of concept that AudioSlots holds promise for addressing the task of audio source separation using structured generative models. While our current implementation of AudioSlots has several limitations, including low reconstruction fidelity for high-frequency features and requiring separated audio sources as supervision, we are optimistic that these can be overcome and outline several possible directions for future work. 6. REFERENCES [1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko, “End-toend object detection with transformers,” in ECCV, 2020. [2] Klaus Greff, Raphaél Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner, “Multi-object representation learning with iterative variational inference,” in ICML, 2019. Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf, “Object-centric learning with slot attention,’ NeurIPS, 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, “Attention is all you need,” NeurIPS, 2017. Joris Cosentino, Manuel Pariente, Samuele Cornell, Antoine Deleforge, and Emmanuel Vincent, “Librimix: An opensource dataset for generalizable speech separation,” arXiv preprint arXiv:2005.11262, 2020. John R Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in JEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016. Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, and John R Hershey, “Single-channel multi-speaker separation using deep clustering,” arXiv preprint arXiv:1607.02173, 2016. Morten Kolbek, Dong Yu, Zheng-Hua Tan, and Jesper Jensen, “Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2017. Yi Luo and Nima Mesgarani, “Conv-tasnet: Surpassing ideal time-frequency magnitude masking for speech separation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2019. Ethan Manilow, Gordon Wichern, Prem Seetharaman, and Jonathan Le Roux, “Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity,” in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2019. --- ---Yi Luo, Zhuo Chen, and Takuya Yoshioka, “Dual-path mn: efficient long sequence modeling for time-domain singlechannel speech separation,” in JEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Cem Subakan, Mirco Ravanelli, Samuele Cornell, Mirko Bronzi, and Jianyuan Zhong, “Attention is all you need in speech separation,’ in JEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021. Kai Li, Runxuan Yang, and Xiaolin Hu, “An efficient encoderdecoder architecture with top-down attention for speech separation,” arXiv preprint arXiv:2209.15200, 2022. Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas, and Dominik Roblek, “SEANet: A multi-modal speech enhancement network,” in Proc. Interspeech, 2020. Zhong-Qiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeong-Yeol Kim, and Shinji Watanabe, “Tf-gridnet: Making time-frequency domain models great again for monaural speaker separation,’ arXiv preprint arXiv:2209.03952, 2022. Felix Weninger, John R Hershey, Jonathan Le Roux, and Bjorn Schuller, “Discriminatively trained recurrent neural networks for single-channel speech separation,” in 2014 IEEE global conference on signal and information processing (GlobalSIP). IEEE, 2014, pp. 577-581. Yuma Koizumi, Shigeki Karita, Scott Wisdom, Hakan Erdogan, John R Hershey, Llion Jones, and Michiel Bacchiani, “Df-conformer: Integrated architecture of conv-tasnet and conformer using linear complexity self-attention for speech enhancement,” in 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). YEEE, 2021, pp. 161-165. Zhuo Chen, Yi Luo, and Nima Mesgarani, “Deep attractor network for single-microphone speaker separation,” in JEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). YEEE, 2017. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” in ECCV, 2020. Scott Wisdom, Efthymios Tzinis, Hakan Erdogan, Ron Weiss, Kevin Wilson, and John Hershey, “Unsupervised sound separation using mixture invariant training,” NeurIPS, 2020. Oriol Vinyals, Samy Bengio, and Manjunath Kudlur, “Order matters: Sequence to sequence for sets,” arXiv preprint arXiv:1511.06391, 2015. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola, “Deep sets,” NeurIPS, 2017. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh, “Set transformer: A framework for attention-based permutation-invariant neural networks,” in ICML, 2019. Yan Zhang, Jonathon Hare, and Adam Prugel-Bennett, “Deep set prediction networks,” NeurIPS, 2019. Adam R Kosiorek, Hyunjik Kim, and Danilo J Rezende, “Conditional set generation with transformers,’ arXiv preprint arXiv:2006. 16841, 2020.Yi Zhou, Hui Zhang, Hana Lee, Shuyang Sun, Pingjun Li, Yangguang Zhu, ByungIn Yoo, Xiaojuan Qi, and Jae-Joon Han, “Slot-vps: Object-centric representation learning for video panoptic segmentation,” in CVPR, 2022. Pradyumna Reddy, Paul Guerrero, and Niloy J Mitra, “Search for concepts: Discovering visual concepts using direct optimization,” arXiv preprint arXiv:2210.14808, 2022. Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojeiech Galuba, Florian Metze, Christoph Feichtenhofer, et al., “Masked autoencoders that listen,’ arXiv preprint arXiv:2207.06405, 2022. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep residual learning for image recognition,” in CVPR, 2016. Nicholas Watters, Loic Matthey, Christopher P Burgess, and Alexander Lerchner, “Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes,” arXiv preprint arXiv:1901.07017, 2019. Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng, “Fourier features let networks learn high frequency functions in low dimensional domains,” NeurIPS, 2020. Diederik P Kingma and Jimmy Ba, “Adam: A method for stochastic optimization,’ arXiv preprint arXiv: 1412.6980, 2014. Hakan Erdogan, John R. Hershey, Shinji Watanabe, and Jonathan Le Roux, “Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in JEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. Yipeng Li and DeLiang Wang, “On the optimality of ideal binary time-frequency masks,” Speech Communication, 2009. Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R Hershey, “Sdr—half-baked or well done?,’ in JEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019. Shahar Lutati, Eliya Nachmani, and Lior Wolf, “Separate and diffuse: Using a pretrained diffusion model for improving source separation,” arXiv preprint arXiv:2301.10752, 2023. Zalén Borsos, Raphaél Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour, “Audiolm: a language modeling approach to audio generation,” arXiv preprint arXiv:2209.03143, 2022. Curtis Hawthorne, Ian Simon, Adam Roberts, Neil Zeghidour, Josh Gardner, Ethan Manilow, and Jesse Engel, “Multiinstrument music synthesis with spectrogram diffusion,” arXiv preprint arXiv:2206.05408, 2022. Thomas Kipf, Gamaleldin F. Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff, “Conditional Object-Centric Learning from Video,” in JCLR, 2022.