arXiv:2306.03504v2 [cs.CV] 2 AugAda-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis * #Zhenhui Ye Ziyue Jiang *#12 Yi Ren *#Jinglin Liu Chen Zhang* Xiang Yin Zejun Ma² Zhou ZhaoAbstract We are interested in a novel task, namely lowresource text-to-talking avatar. Given only a fewminute-long talking person video with the audio track as the training data and arbitrary texts as the driving input, we aim to synthesize high-quality talking portrait videos corresponding to the input text. This task has broad application prospects in the digital human industry but has not been technically achieved yet due to two challenges: (1) It is challenging to mimic the timbre from outof-domain audio for a traditional multi-speaker Text-to-Speech system. (2) It is hard to render high-fidelity and lip-synchronized talking avatars with limited training data. In this paper, we introduce Adaptive Text-to-Talking Avatar (AdaTTA), which (1) designs a generic zero-shot multispeaker TTS model that well disentangles the text content, timbre, and prosody; and (2) embraces recent advances in neural rendering to achieve realistic audio-driven talking face video generation. With these designs, our method overcomes the aforementioned two challenges and achieves to generate identity-preserving speech and realistic talking person video. Experiments demonstrate that our method could synthesize realistic, identity-preserving, and audio-visual synchronized talking avatar videos. 1. Introduction Recent years have witnessed an emergence of generative artificial intelligence in various domains, for instance, with a large language model (LLM)-based chatbot (Adamopoulou & Moussiades, 2020), we can obtain high-quality, natural, and realistic dialogue text content. Using an advanced textto-speech (TTS) system (Kim et al., 2021; Ren et al., 2021; Wang et al., 2023; Ye et al., 2023b), we can synthesize *Equal contribution #Interns at ByteDance 'Zhejiang University ByteDance. Correspondence to: Zhou Zhao <zhaozhou@zju.edu.cn>. Proceedings of the 40th International Conference on Machine Learning, 2023. Copyright 2023 by the author(s). personalized speech given reference audio and plain texts. The development of neural rendering techniques also makes it possible to achieve high-fidelity and realistic talking face video generation (TFG) (Prajwal et al., 2020; Guo et al., 2021; Ye et al., 2023c) with only a few training samples. It is natural to think of combining the TTS model and TFG method so that the joint system could allow users to create a talking video with only text input. This joint system has broad potential applications such as news broadcasting, virtual lectures, and talking chatbots considering the recent advance of ChatGPT (Ouyang et al., 2022). However, previous TTS and TFG system typically requires a large amount of identity-specific data to produce satisfying personalized results(Ren et al., 2020; Suwajanakorn et al., 2017), which raises challenges to real scenarios in which typically only a few-minute-long video of a target person is available. Motivated by this observation, we are interested in a novel task named low-resource text-to-talking avatar (TTA). Given only a few-minute-long talking person video with transcribed audio track as the training data, we aim to synthesize identity-preserving and audio-lip synchronized talking portrait videos given the driving input text. We first consider the challenges in TTS and TFG respectively. As for the TTS stage, the main challenge is how to properly preserve the timbre identity of the input audio (Kharitonov et al., 2023). A naive solution is to finetune a pre-trained TTS model on the given text-audio pairs. However, fine-tuning induces a large latency, and due to the limited amount of data, generalizability, the performance is not guaranteed; Another solution is to extract a speaker embedding of the input audio with an off-the-shelf toolkit (Jia et al., 2018), which is known to introduce information loss and the identity-preserving quality is not satisfying. As for the TFG stage, the main challenge is to achieve high-fidelity and audio-visual synchronization given the limited amount of audio-video pairs. Some zero-shot methods (Prajwal et al., 2020; Zhou et al., 2020; 2021) achieve good lip synchronization by training the model on big data, but the video quality is not as good. Some recent neural rendering-based methods (Guo et al., 2021; Tang et al., 2022; Liu et al., 2022) achieve the goal of high fidelity, yet lip synchronization is poor due to the small amount of audio-lip training pairs. Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis In this paper, we propose Ada-TTA to handle the aforementioned problems. Ada-TTA is a joint system of TTS and TFG, which takes advantage of the most recent advances in each sub-task. To improve the identity-preserving power of the TTS model, we introduce a well-designed zero-shot multi-speaker TTS model trained on a 20,000hour-long TTS dataset, which can synthesize high-quality personalized speech with only one short recording of an unseen speaker. To achieve high-fidelity and lip-synchronized talking face generation, we utilize the recently proposed GeneFace++ (Ye et al., 2023a) as the TFG system, since it improves the lip-synchronization and system efficiency of the previous neural rendering-based methods while maintaining high fidelity. Combining the advantages of these two advanced systems in TTS and TFG, Ada-TTA achieves lowresource but high-quality text-to-talking avatar synthesis. The experiment shows good performance of our Ada-TTA in terms of synthesized speech and video. It also shows Ada-TTA outperforms the baseline from the perspective of objective and subjective metrics. 2. Related works Our work is majorly related to a low-resource personalized text-to-speech and talking face generation. We discuss related works from these two fields respectively. Previous personalized speech generation approaches, also known as zero-shot multi-speaker TTS, can be categorized into speaker adaptation and speaker encoding methods. Traditional works (Ren et al., 2019; Casanova et al., 2022; Ye et al., 2022) are typically trained on small reading-style datasets and cannot generalize well for unseen speakers. Some recent works trained on large-scale multi-domain datasets demonstrate the effectiveness in zero-shot scenarios. Among them, some works (Wang et al., 2023; Shen et al., 2023) utilize the neural audio codec models to convert audio waveform into latent and consider it as the intermediate representation for speech generation, which ignores the intrinsic properties of speech attributes and may lead to inferior or uncontrollable results (e.g, degraded timbre identity similarity and uncontrollable prosody). By contrast, the proposed zero-shot multi-speaker TTS system disentangles the speech into different attributes and models each of them using architectures with appropriate inductive biases, which improves the identity-preserving power and prosody naturalness. Traditional works in zero-shot/low-resource TFG (Prajwal et al., 2020; Zhou et al., 2021; 2020) typically adopt a GAN-based renderer given a reference image of the identity, which fails to generate realistic and identity-preserving video. Recent works have embraced the neural radiance field (NeRF) (Mildenhall et al., 2020; Guo et al., 2021) as it enables realistic 3D pose control and could achieve good video A short video for training Audio Track Video Frames Input Text "Hello everyone..." Reference Timbre Prosody Zero-shot Multi-Speaker TTS Synthesized Speech Train the renderer Appearance Geometry Talking Face Generation Synthesized Video Figure 1: The overall pipeline of Ada-TTA. The dotted line denotes that the process is only executed during the training phase. quality with limited data of an identity. Then Some works explore sample-efficient and time-efficient training (Liu et al., 2022; Shen et al., 2022). Recently, GeneFace++ (Ye et al., 2023a), a recently proposed method improves lip synchronization to OOD audio and achieves real-time inference, which promotes the NeRF-based TFG applicable to real-world scenarios. 3. Proposed method The proposed system consists of two main modules: a zeroshot multi-speaker TTS module and a TFG module. First, the TTS model obtains a reference audio clip of the target identity and extracts the timbre and prosody pattern from it in an in-context-learning manner, then transforms the input text to the speech audio. Subsequently, the TFG module synthesizes the talking person portrait video synchronized with the input speech. The overall pipeline of our system is shown in Figure 1. 3.1. Zero-Shot Multi-Speaker TTS We use an internal zero-shot multi-speaker TTS model. As shown in Figure 2, it is a VQGAN-based (Esser et al., 2021) TTS model which comprises a timbre encoder, a text encoder, a vector quantization (VQ) prosody encoder, and a mel decoder to disentangles the mel-spectrogram into speech attributes (e.g., prosody and timbre). We disentangle the mel-spectrogram with a carefully designed information bottleneck: 1) we use the text encoder to encode the phoneme sequence into the content representation; 2) we feed the reference mel-spectrogram sampled from a different speech of the same speaker to disentangle the timbre and content information and temporally average the output of the timbre encoder to get a one-dimensional global timbre Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis infer P-LLM O Mel Decoder train VQ-Prosody Timbre Encoder Encoder Text Encoder Reference Mel "hello everyone..." Text Figure 2: The overall structure of our internal zero-shot multi-speaker TTS model. vector. 3) we feed the first 20 bins in each mel-spectrogram frame into the prosody encoder, as it contains almost complete prosody and less timbre/content information. We also introduce a carefully-tuned vector quantization (VQ) layer and a phoneme-level downsampling layer to the prosody encoder to constrain the information flow. The correctlydesigned bottleneck will learn to remove the content information and the global timbre information from the output of the prosody encoder, which ensures the performance of disentanglement. During training the GT mel-spectrogram is used to extract the prosody sequence. And during the inference phase, we need to predict the prosody sequence given the input text. To this end, leveraging the powerful in-context learning capability of LLMs, we learn a prosody large language model (P-LLM) that fits the distribution of prosody in an auto-regressive manner. To be specific, the P-LLM is a decoder-only transformer-based architecture that uses prosody codes from the reference speech as the prompt to generate the prosody codes for the target speech. Once the P-LLM is trained, during inference, we propose to use the content from the given text sequence, the timbre extracted from the prompt speech, and the prosody predicted by our P-LLM to generate the target speech for zero-shot personalized speech synthesis. 3.2. Talking Face Generation We adopt GeneFace++ (Ye et al., 2023a) as the talking face generation model, which is a low-resource TFG method that could render lip-synchronized and high-fidelity talking face video in real-time. It compromises an audio-to-motion module to transform the input speech into facial landmarks, and a motion-to-video render to synthesize video frames given the landmark. We extract pitch contours and HUBERT features from the raw waveform as the audio representation, and select 68 facial landmarks from the reconstructed 3DMM mesh as the motion representation. The audio-tomotion module consists of a generic HuBERT-conditioned VAE and an identity-specific postnet to produce the audiosynchronized and identity-preserving facial landmark. The motion-to-video renderer is a landmark-conditioned NeRF that can freely control the facial expression and head pose by adjusting the facial landmark and the camera pose. To improve the system efficiency, the audio-to-motion module is built with a non-autoregressive network structure that processes the whole audio sequence in one forward, and the motion-to-video renderer adopts recent advances in Gridbased NeRF, which replaces previous computationally expensive dense MLP query with simple bi-linear indexing in a learnable grid. 3.3. Training and Inference The training procedure of the zero-shot multi-speaker TTS model has two stages. In the first stage, we train the VQGAN-based TTS model to reconstruct the melspectrogram given the input text, GT prosody code, and the reference mel-spectrogram. The training loss of the VQGAN-based TTS is as follows: L = ||y − ŷ||² + LvQ + L Adv (1) where yŷ is the L2 loss on mel-spectrogram, LvQ is the VQVAE loss function (Van Den Oord et al., 2017; Esser et al., 2021) and Adv is the LSGAN-styled adversarial loss (Mao et al., 2017). Then in the second stage, we train the P-LLM to predict the prosody code given the input text and previous prosody code sequence. The P-LLM is trained in a teacher-forcing mode in the training stage via the crossentropy loss. In training our talking face generation model, we use the pretrained audio-to-motion module in GeneFace++ and train the NeRF-based renderer from scratch. To further improve the image quality, we adopt VGG perceptual loss on the lip part of the predicted image. During inference, the zero-shot multi-speaker TTS model synthesizes identity-preserving speech given the input text, then the talking face generation model takes the synthesized audio as input and generates the talking portrait images. Finally, the synthesized audio and video frames are integrated into the final video. 4. Experiments Training Details. As for the TTS model, to improve the timbre and prosody generalizability, we scale up the TTS model to 222.5M parameters and adopt GigaSpeech (Chen et al., 2021), a 10,000-hour-long English TTS dataset for training the model. We train this large-scale TTS model on 8 NVIDIA A100 GPUs with a batch size of 30 sentences on each GPU. It takes 420k steps for convergence and no furYourTTS+ Wav2Lip Ada-TTA Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis Table 1: Objective evaluation of the TTA systems. Spk-Sim denotes cosine speaker similarity. Method Spk-Sim↑ FID↓ YourTTS + Wav2Lip 0.9392 55.Ada-TTA (Ours) 0.9854 28.everyone together university <silence> Figure 3: The video frames generated by the TTA systems. ther fine-tuning is needed. As for the talking face generation, we use the pre-trained audio-to-motion module provided by GeneFace++ (Ye et al., 2023a) and train the NeRF-based renderer for each specific identity for 320k steps, which takes about 6 hours on 1 NVIDIA A100 GPU. The talking person videos used to train the renderer are about 3 minutes in length. Comparison baseline. Since there is no publicly available low-resource text-to-avatar system, we construct a baseline by combining YourTTS (Casanova et al., 2022), a recently proposed zero-shot multi-speaker TTS, and Wav2Lip (Prajwal et al., 2020), a state-of-the-art few-shot talking face generation method from the perspective of lip synchronization. We denote the baseline as YourTTS+Wav2Lip. Note that during inference Wav2Lip takes the whole training video as input and only regenerates the lip part. By contrast, our model renders the whole frame. Evaluation Metrics. We conduct the CMOS (comparative mean opinion score) test to evaluate the performance of the text-to-avatar systems. We analyze the CMOS in three aspects: CMOS-A (Only analyze the audio, including speaker similarity, prosody, and audio quality), CMOS-V (Only analyze the video, including image fidelity, 3D realness, and identity preserving), CMOS (the overall opinion score of the synthesized video). We tell the tester to focus on one corresponding aspect and ignore the other aspect when scoring. As for the subjective evaluation, we use the WavLM (Chen et al., 2022) model fine-tuned for speaker verification to compute the cosine similarity score between the ground-truth speech and synthesized speech; we use FID to measure the image quality of the synthesized video. Experiment Results. As shown in Table 1, our method achieves a higher speaker similarity than YourTTS, proving the effectiveness of our zero-shot multi-speaker TTS model to preserve the identity of the input reference audio. We also notice our method achieves better FID than Wav2Lip, which denotes a better image quality of the rendered video frames. We further perform the CMOS test to evaluate the performance in terms of human perception. The results are Table 2: CMOS Results of the TTA systems. The error bar is standard deviation. Y+W denotes YourTTS+Wav2Lip. Method CMOS-A CMOS-V Y+W 0.0.Ada-TTA +0.84 0.50 +0.76±0.CMOS 0.+0.74 ± 0.listed in Table 2. We find that prefer the synthesized videos by our method in terms of audio quality (CMOS-A), video quality (CMOS-V), and overall quality (CMOS). To make a better qualitative comparison, we recommend the reader to watch a demo video ¹. We also provide some keyframes in Figure 3 to compare the performance of our Ada-TTA and the baseline. We can observe that compared with the baseline, Ada-TTA produces (1) better speech in terms of high timbre similarity and good audio quality; and (2) better video with high lip-synchronization, good image fidelity, and free head pose control. 5. Conclusions In this paper, we present Ada-TTA, an adaptive high-quality text-to-talking avatar synthesis system. Given only a fewminute-long talking person video as training data, with AdaTTA we can synthesize identity-preserving speech given arbitrary input text, and generate the lip-synchronized video. We describe a zero-shot multi-speaker TTS model and a high-quality talking face generation method used to construct the Ada-TTA system. Our experiments demonstrate our system's ability to synthesize realistic speech and video at a limited data scale. References Adamopoulou, E. and Moussiades, L. Chatbots: History, technology, and applications. Machine Learning with Applications, 2:100006, 2020. Casanova, E., Weber, J., Shulby, C. D., Junior, A. C., Gölge, E., and Ponti, M. A. Yourtts: Towards zero-shot multispeaker tts and zero-shot voice conversion for everyone. 'The link for demo video is https:// genefaceplusplus.github.io/GeneFace++/ada_ tta.mpAda-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis In International Conference on Machine Learning, pp. 2709-2720. PMLR, 2022. Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. Wavlm: Largescale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6): 1505–1518, 2022. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873-12883, 2021. Guo, Y., Chen, K., Liang, S., Liu, Y.-J., Bao, H., and Zhang, J. Ad-nerf: Audio driven neural radiance fields for talking head synthesis. In ICCV, pp. 5784-5794, 2021. Jia, Y., Zhang, Y., Weiss, R., Wang, Q., Shen, J., Ren, F., Nguyen, P., Pang, R., Lopez Moreno, I., Wu, Y., et al. Transfer learning from speaker verification to multispeaker text-to-speech synthesis. Advances in neural information processing systems, 31, 2018. Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., and Zeghidour, N. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision. arXiv preprint arXiv:2302.03540, 2023. Kim, J., Kong, J., and Son, J. Conditional variational autoencoder with adversarial learning for end-to-end textto-speech. In ICML. PMLR, 2021. Liu, X., Xu, Y., Wu, Q., Zhou, H., Wu, W., and Zhou, B. Semantic-aware implicit neural audio-driven video portrait generation. arXiv preprint arXiv:2201.07786, 2022. Mao, X., Li, Q., Xie, H., Lau, R. Y., Wang, Z., and Paul Smolley, S. Least squares generative adversarial networks. In ICCV, 2017. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, pp. 405-421. Springer, 2020. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. NeurIPS, 2022. Prajwal, K., Mukhopadhyay, R., Namboodiri, V. P., and Jawahar, C. A lip sync expert is all you need for speech to lip generation in the wild. In ACM MM, pp. 484–492, 2020. Ren, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. Fastspeech: Fast, robust and controllable text to speech. Advances in neural information processing systems, 32, 2019. Ren, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. Fastspeech 2: Fast and high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020. Ren, Y., Liu, J., and Zhao, Z. Portaspeech: Portable and high-quality generative text-to-speech. NIPS, 34:1396313974, 2021. Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., and Bian, J. Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers. arXiv preprint arXiv:2304.09116, 2023. Shen, S., Li, W., Zhu, Z., Duan, Y., Zhou, J., and Lu, J. Learning dynamic facial radiance fields for few-shot talking head synthesis. In ECCV, 2022. Suwajanakorn, S., Seitz, S. M., and KemelmacherShlizerman, I. Synthesizing obama: learning lip sync from audio. ACM Transactions on Graphics (ToG),(4):1-13, 2017. Tang, J., Wang, K., Zhou, H., Chen, X., He, D., Hu, T., Liu, J., Zeng, G., and Wang, J. Real-time neural radiance talking portrait synthesis via audio-spatial decomposition. arXiv preprint arXiv:2211.12368, 2022. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. Syntaspeech: Ye, Z., Zhao, Z., Ren, Y., and Wu, F. syntax-aware generative adversarial text-to-speech. arXiv preprint arXiv:2204.11792, 2022. Ye, Z., He, J., Jiang, Z., Huang, R., Huang, J., Liu, J., Ren, Y., Yin, X., Ma, Z., and Zhao, Z. Geneface++: Generalized and stable real-time audio-driven 3d talking face generation. arXiv preprint arXiv:2305.00787, 2023a. Ye, Z., Huang, R., Ren, Y., Jiang, Z., Liu, J., He, J., Yin, X., and Zhao, Z. Clapspeech: Learning prosody from text context with contrastive language-audio pre-training. arXiv preprint arXiv:2305.10763, 2023b. Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis Ye, Z., Jiang, Z., Ren, Y., Liu, J., He, J., and Zhao, Z. Geneface: Generalized and high-fidelity audio-driven 3d talking face synthesis. In ICLR, 2023c. Zhou, H., Sun, Y., Wu, W., Loy, C. C., Wang, X., and Liu, Z. Pose-controllable talking face generation by implicitly modularized audio-visual representation. In CVPR, pp. 4176-4186, 2021. Zhou, Y., Han, X., Shechtman, E., Echevarria, J., Kalogerakis, E., and Li, D. Makelttalk: speaker-aware talkinghead animation. ACM Transactions on Graphics (TOG), 39(6):1-15, 2020.