--- --arXiv:2307.10373v3 [cs.CV] 20 NovTOKENFLOW: CONSISTENT DIFFUSION FEATURES FOR CONSISTENT VIDEO EDITING Michal Geyer* Omer Bar-Tal* Shai Bagon Tali Dekel Weizmann Institute of Science *Indicates equal contribution. Project webpage: [nttps ://diffusion-tokenflow.github.io “A Van Gogh portrait” “A wolf in Machu Pichu” sl’ Figure 1: TokenFlow enables consistent, high-quality semantic edits of real-world videos. Given an input video (top row), our method edits it according to a target text prompt (middle and bottom rows), while preserving the semantic layout and motion in the original scene. ABSTRACT The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and motion of the input video. Our method is based on a key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-toimage editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos. 1 INTRODUCTION The evolution of text-to-image models has recently facilitated advances in image editing and content creation, allowing users to control various proprieties of both generated and real images. Nevertheless, expanding this exciting progress to video is still lagging behind. A surge of large-scale text-to-video generative models has emerged, demonstrating impressive results in generating clips solely from textual descriptions. However, despite the progress made in this area, existing video models are still in their infancy, being limited in resolution, video length, or the complexity of video dynamics they can represent. In this paper, we harness the power of a state-of-the-art pre-trained text-to-image model for the task of text-driven editing of natural videos. Specifically, our goal is to generate high-quality videos that adhere to the target edit expressed by an input text prompt, while preserving the spatial layout and motion of the original video. The main challenge in leveraging an image diffusion model for video editing is to ensure that the edited content is consistent across all video frames — ideally, each physical point in the 3D world undergoes coherent modifications across time. Existing and concurrent video editing methods that are based on image diffusion models have demonstrated that global appearance coherency across the edited frames can be achieved by extend ing the self-attention module to include multiple frames 2022}{Khachatryan et al.}/2023b --- --Ceylan et al.| {2023 (2023). Nevertheless, this approach is insufficient for achieving the desired level 0: temporal consistency, as motion in the video is only implicitly preserved through the attention module. Consequently, professionals or semi-professionals users often resort to elaborate video editing pipelines that entail additional manual work. In this work, we propose a framework to tackle this challenge by explicitly enforcing the original inter-frame correspondences on the edit. Intuitively, natural videos contain redundant information across frames, e.g., depict similar appearance and shared visual elements. Our key observation is that the internal representation of the video in the diffusion model exhibits similar properties. That is, the level of redundancy and temporal consistency of the frames in the RGB space and in the diffusion feature space are tightly correlated. Based on this observation, the pillar of our approach is to achieve consistent edit by ensuring that the features of the edited video are consistent across frames. Specifically, we enforce that the edited features convey the same inter-frame correspondences and redundancy as the original video features. To do so, we leverage the original inter-frame feature correspondences, which are readily available by the model. This leads to an effective method that directly propagates the edited diffusion features based on the original video dynamics. This approach allows us to harness the generative prior of state-of-the-art image diffusion model without additional training or fine-tuning, and can work in onjunction with an off-the-shelf diffusion-based image editing method (e.g., (2022); Hertz et al. (2022); Zhang & Agrawala (2023); Tumanyan et al. (2023). ‘0 summarize, we make the following key contributions: ¢ A technique, dubbed TokenFlow, that enforces semantic correspondences of diffusion fea tures across frames, allowing to significantly increase temporal consistency in videos generated by a text-to-image diffusion model. ¢ Novel empirical analysis studying the proprieties of diffusion features across a video. ¢ State-of-the-art editing results on diverse videos, depicting complex motions. 2 RELATED WORK Text-driven image & video synthesis Seminal works designed GAN architectures to synthesize images conditioned on text embeddings (Reed et al} 2016} Zhang et al.||2016). With the evergrowing scale of vision-language datasets and pretraining strategies (Radford et al.||faa a 202), there has been a remarkable progress in text-driven image generation capabilities. Users can sytnesize high-quality visual content using simple text prompts. Much of this progress Dhariwal| as state is also attributed to diffusion models (Sohl-Dickstein et al. 2015} |Croitoru et al.| 2022] & Nichol 2021} Ho et al.| 2020} ) which have been establishe of-the-art text-to-image generators ( Rombach et al} 3023] Sheynin et al. : . Such models have been extended or text-to-video generation, by extending 2D architectures to the temporal dimension (e.g., using temporal attention (2022b)) and performing large-scale training on video datasets et al.| {2022a} [Blattmann et al. Singer et al.|/2022). Recently, Gen-1 2023) taiored a diffusion model architecture for the task of video editing, by conditioning the network on structure/appearance representations. Nevertheless, due to their extensive computation and memory requirements, existing video diffusion models are still in infancy and are largely restricted to short clips, or exhibit lower visual quality compared to image models. On the other side of the spectrum, a promising recent trend of works leverage a pre-trained image diffusion model for video synthesis tasks, without additional training 2023). Our work falls into this category, employing a pretrained text-to-image diffusion model for e task of video editing, without any training or finetuning. Consistent video stylization A common approach for video stylization involves applying image editing techniques (e.g., style transfer) on a frame-by-frame basis, followed by a post-processin: stage to address temporal inconsistencies in the edited video [Us tapars), Although these methods effectively reduce high-frequency temporal flickering, they are not designed to handle frames that exhibit substantial variations in content, which often occur when applying text-based image editing techniques (Qi et af] 2023). propose to decompose a video into a set of 2D atlases, each provides a unified representation of the background or of a foreground object throughout the video. Edits applied to the 2D atlases are automaticall: mapped back to the video, thus achieving temporal consistency with minimal effort. (2022); [Lee et al.| (2023b) leverage this representation to perform text-driven editing. However, atlas representation is limited to videos with simple motion and requires long training, limiting the applicability of this technique and of the methods built upon it. Our work is also related to classical works that demonstrated that small patches in a natural video extensively repeat across frames 2005) (Shahar et al.||2011}|Cheung et al. , and thus consistent editing can by simplified by editing --- --Sample Frames Per-frame . xX xX xX Figure 3: Diffusion features across time. Left: Given an input video (top row), we apply DDIM inversion on each frame and extract features from the highest resolution decoder layer in €g. We apply PCA on the features (i.e., output tokens from the self-attention module) extracted from all frames and visualize the first three components (second row). We further visualize an x-t slice (marked in red on the original frame) for both RGB and features (bottom row). The feature representation is consistent across time — corresponding regions are encoded with similar features across the video. Middle: Frames and feature visualization for an edited video obtained by applying an image editing method ) on each frame; inconsistent patterns in RGB are also evident in the feature space (e.g., on the dog’s body). Right: Our method enforces the edited video to convey the same level of feature consistency as the original video, which translates into a coherent and high-quality edit in RGB space. Features (PCA) x-t slice a subset of keyframes and propagating the edit across the video by establishing patch correspondences using handcrafted features and oj 2020) flow (Ruder et al. | 2016} |Jamriska et al.| [Jamri8ka et al.| et al. 2019) or by training a patch-based GAN (Texler et al|] evertheless, such propagation methods struggle to handle videos with Wate ae cl | 020) or ¢ with complex dynamics. Importantly, they rely on a user provided consistent edit of the keyframes, which remains a labor-intensive task yet to be automated. combines keyframe editing with a propagation method by (2019). They edit keyframes using a text-to-image diffusion model while enforcing optical flow constraints on the edited keyframes. However, since optical flow estimation between distant frames is not reliable, their method fails to consistently edit keyframes that are far apart (as seen in our Supplementary Material - SM), and as a result, fails to consistently edit most videos. Our work shares a similar motivation as this approach that benefits from the temporal redundan cies in natural videos. We show that such redun- @) ©) , © an dancies are also present in the feature space of a Reconstructed Warped —_Nearest-Neighbour Target Source» Target Field text-to-image diffusion model, and leverage this property to achieve consistency. Target I Controlled generation via diffusion features manipulation Recently, a surge of works demonstrated how text-to-image diffusion models can be readily adapted to various editing and generation tasks, by performing simple operations on the intermediate feature rej eo of the diffusion network (Chefer et al. Ma et al [2023] [Tumanyan| | 2023) [Hertz et al:| (2022; [Patashnik et al. 2023 Cao _et_al J (2023). (2023); to reconstruct nearby frames. This is done by: (a) -| 2023) demonstrated semantic ap- swapping each feature in the target by its nearest feapearance swapping using diffusion feature corre- ture in the source, in all layers and all generation time spondences. Here ral (2028 observed that by steps, and (b) simple warping in RGB space, using manipulating the cross-attention layers, it is pos- a nearest neighbour field (c), computed between the sible to control the relation between the spatial source and target features extracted from the highest layout of the image to each word in the rales, soo) g- resolution decoder layer. The target is faithfully re and-Play Diffusion (PnP,/Tumanyan et al.| constructed, demonstrating the high level of spatial analyzed the spatial features and the se 1 (2025) granularity and shared content between the features. Target II Figure 2: Fine-grained feature correspondences. Features (i.e., output tokens from the self-attention modules) extracted from of a source frame are used --- --Compute NN field Y Extract tokens inversion Noisy Sampled keyframes ee oe ier & painting” a Figure 4: TokenFlow pipeline. Top: Given an input video Z, we DDIM invert each frame, extract its tokens, i.e., output features from the self-attention modules, from each timestep and layer, and compute inter-frame features correspondences using a nearest-neighbor (NN) search. Bottom: The edited video is generated as follows: at each denoising step ¢, (I) we sample keyframes from the noisy video J; and jointly edit them using an extended-attention block; the set of resulting edited tokens is Tyase. (II) We propagate the edited tokens across the video according to the pre-computed correspondences of the original video features. To denoise J, we feed each frame to the network, and replace the generated tokens with the tokens obtained from the propagation step (II). painting” observed that by extending the self-attention module to operate on more than a single frame, it is possible to generate frames that share a common global appearance. |Qi et al|| 2025}; property to achieve globally-coherent video edits. Nevertheless, as demonstrated in Sec. [5] inflating the self-attention module is insufficient for achieving fine-grained temporal consistency. Prior and concurrent works either compromise visual quality, or exhibit limited temporal consistency. In this work, we also perform video editing via simple operations in the feature space of a pre-trained text-to-image model, we explicitly encourage the features of the model to be temporally consistent through TokenFlow. maps and found that they capture semantic information at high spatial granularity. Tune-A-Video 3 PRELIMINARIES Diffusion Models Diffusion probabalistic models (DPM) (Sohl-Dickstein et al.| fetal] 2022} [Dhariwal & Nichol] 2021} [Mo et al] [2020} [Nichol & Dharivall 2021) are a classgenerative models that aim to approximate a data distribution q through a progressive denosing process. Starting from a Gaussian 1.i.d noisy image a7 ~ N(0, I), the diffusion model eg, gradually denoises it, until reaching a clean image a) drawn from the target distribution g. DPM can learn a conditional distribution by incorporating additional guiding signals, such as text conditioning. derived DDIM, a deterministic sampling algorithm given an initial noise a. By applying this algorithm in the reverse order (a.k.a. DDIM inversion) starting from the clean ao, it allows to obtain the intermediate noisy images {2x;}7_, used to generate it. Stable Diffusion Stable Diffusion (SD) (Rombach et al.|/2022) is a prominent text-to-image diffusion model that operates in a latent image space. A pretrained encoder maps RGB images to this space, and a decoder decodes latents back to high-resolution images. In more detail, SD is based on a U-Net architecture (Ronneberger ea] 2015), which comprises of residual, self-attention, and cross-attention blocks. The residual block convolves the activations from a previous layer, while cross-attention manipulates features according to the text prompt. In the self-attention block, features are projected into queries Q, keys K, and values V. The Attention operation computes the affinities between the d-dimensional projections Q, K to yield the output of the layer: T A-V where A = Attention(Q;K) and Attention(Q; K) = softmax (2%) (1) --- --Input video Input video Bl “A Van Gogh portrait” “Ice oa of oma car” “A marble sculpture” ‘Sand sculpture a car on the beach” Input video “A robotic wolf” “Maui from Moana Movie” “A colourful polygonal illustration” “A Pixar animation” Figure 5: Results. Sample results ‘ our =! We refer the reader to our webpage and SM for more examples and full-video results. 4 METHOD Given an input video Z = [J', ..., J"], and a text prompt P describing the target edit, our goal is to generate an edited video J = [J',..., J”] that adheres to the text P, while preserving the original motion and semantic layout of Z. To achieve this, our framework leverages a pretrained and fixed text-to-image diffusion model eg. Naively leveraging €g for video editing, by applying an image editing method on each frame inde dently (e.g.,|Hertz et al. (2022); Tumanyan et al. (2023); Meng et al. (2022); Zhang & Agrawala! (2033), middle ), results in content inconsistencies across frames (e.g., Fig. column). Our key nding is that these inconsistencies can be alleviated by enforcing consistency among the internal diffusion features across frames, during the editing process. Natural videos typically depict coherent and shared content across time. We observe that the internal representation of natural videos in €g has similar properties. This is illustrated in Fig. visualize the features extracted from a given video (first column). As seen, the features depict a shared and consistent representation across frames, i.e., corresponding regions exhibit similar representation. We further observe that the original video features provide fine-grained correspondences between frames, using a simple nearest neighbour search (Fig |2). Moreover, we show that these corresponding features are interchangeable for the diffusion model — we can ne (raat).one frame by swapping its features by their corresponding ones in a nearby frame (Fig|2[a)). Nevertheless, when an edit is applied to each frame individually, the consistency of the features breaks (Fig. B]middle column). This implies that the level of consistency of in RGB space is correlated with the consistency of the internal features of the frames. Hence, our key idea is to manipulate the features of the edited video to preserve the level of consistency and inter-frame correspondences of the original video features. As illustrated in Fig. [4] our framework, dubbed TokenFlow, alternates at each generation timestep between two main components: (i) sampling a set of keyframes and jointly editing them according to P;; this stage results in shared global appearance across the keyframes, and (ii) propagating the features from the keyframes to all of the frames based on the correspondences provided by the original --- ---Input a “A shiny metal sculpture” “An origami of a stork” TAV PNP Gen-Ours Figure 6: Comparison. We compare our method against Tune-A-Video cTaV, [Wu et all 2022)), PnPDiffusion (Tumanyan et al.|/2023) applied per frame, Gen-1 (Esser et al.|/2023), Text2 Video-Zero (Khachatryan| and Fate-Zero (Oi et al PP ero ( 2023). We refer the reader to our supplementary material for full-video parisons. video features; this stage explicitly preserves the consistency and fine-grained shared representation of the original video features. Both stages are done in combination with an image editing technique & (e.g, Hanan otal) 3023), Intuitively, the benefit of alternating between keyframe editing and propagation is twofold: first, sampling random keyframes at each generation step increases the robustness to a particular selection. Second, since each generation step results in more consistent features, the sampled keyframes in the next step will be edited more consistently. Pre-processing: extracting diffusion features. Given an input video Z, we apply DDIM inversion (see Sec [3) on each frame I’, which yields a sequence of latents [a/,...,a',]. For each generation timestep t, we feed the latent } of each frame i € [n] to the model and extract the tokens («;) from the self-attention module of every layer in the network €g (fig. |4} top). We will later use these tokens to establish inter-frame correspondences between diffusion features. 4.1 KEYFRAME SAMPLING AND JOINT EDITING Our observations imply that given the features of a single edited frame, we can generate the next frames by propagating its features to their corresponding locations. Most videos, however, can not be represented by a single keyframe. To account for that, we consider multiple keyframes, from which we obtain a set of features (tokens), Thase, that will later be propagated to the entire video. Specifically, at each generation step, we randomly sample a set of keyframes {J‘}ic, in fixed frame intervals (see SM for details). We _joinly edit the keyframes by extending the selfattention block to simultaneously process them (Wie al) 2023 , thus encouraging them to share a global appearance. In more detail, the input to the modified block are the self-attention features from all keyframes {Q' }icn, {K' Sick, {V' }icx where Q’, K', V" are the queries, keys, and values of frame i € K,&K = {i1,...i,}. The keys of all frames are concatenated, and the extended-attention is: -K"] ) @Q) vd i [Ke extattn(Q'[K",...K"*]) soft --- --The output of the block for frame i is given by: o(J') = A-[V",...V*] where A= extattn(Q'; [K™ K"*}) 6) Intuitively, each keyframe queries all other keyframes, and aggregates information from them. This results in a roughly unified appearance in the edited frames (Wu et al.| |2022} |Khachatryan et al} 2023b} |Ceylan et al.|{2023 2023). We define Tyase = {6(J") }icx, for each layer in the ig. [4 mid networ! bottom ale). 4.2 EDIT PROPAGATION VIA TOKENFLOW Given Tyase, We propagate it across the video based on the token correspondences extracted from the original video. At each generation step t, we compute the nearest neighbor (NN) of each original frame’s tokens, b(xi), and its two adjacent keyframess’ tokens, o(2'*), o(2'~) where i+ is the index of the closest future keyframe, and i— the index of the closest past keyframe. Denote the resulting NN fields ‘+, y'~: * [pl = argmin D (o(a')[p], o(@"*)[a]) (4) Where p,q are spatial locations in the token feature map, and D is cosine distance. For simplicity, we omit the generation timestep t; our method is applied in all time-steps and self-attention layers. Once we obtain 7*, we use it to propagate the edited frames’ tokens Ty,s¢ to the rest of the video, by linearly combining the tokens in Ty,se corresponding to each spatial location p and frame 7: Fy (Leases 4?) = wi OS Yb [pl] + (= wi) (Tl bl] (5) Where o(J'*) € Tyase and w; € (0,1) is a scalar proportional to the distance between frame i and its adjacent keyframes (see SM), ensuring a smooth transition. Note that F also modifies the tokens of the sampled keyframes. That is, we modify the self-attention blocks to output a linear combination of the tokens in T,,,. for all frames, including the keyframes, according to the original video token correspondences. Overall algorithm We summarize our video editing algorithm in Alg. We first perform DDIM inversion on the in- Tput: Algorithm 1 TokenFlow editing put video Z and extract the sequence T=lr,...,1" > Input Video of noisy latents {v%}7_, for all frames P > Target text prompt i € [n] (fig | top). We then denoise wv > Diffusion-based image editing technique the video, alternating between keyframes editing and TokenFlow propagation: At each generation step t, we randomize k < n keyframe indices, and denoise K = fi,.. them using an image editing technique Vi € [n], t € [T] , i} < sample keyframe indices Fy << '* Vi €[n] compute NN field {Ti_ihiex + Gl{Ij }jex; Extattn] . Trase + O({J{_,}iex) extract keyframes’ tokens q-[3] Fig. pve (1). We then denoise the entire video Sia €o[I1; TokenFlow(F, (Toase))] , by combining the image-editing tech- Output: 7 = [Jo,..., Jo nique with TokenFlow (Eq. [5] Fig. |4]()) at every self-attention block in every layer of the network. Note that each layer includes a residual connection between the input and output of the self-attention block, thus performing TokenFlow at each layer is necessary. 5 RESULTS We evaluate our method on DAVIS videos (Pont-Tuset et al.|/2017) and on Internet videos depicting animals, food, humans, and various objects in motion. The spatial resolution of the videos is 384 x 672 or 512 x 512 pixels, and they consist of 40 to 200 frames. We use various text prompts on each video to obtain diverse editing results. Our evaluation dataset comprises of 61 text-video pairs. We utilize PnP-Diffusion (Tumanyan et al.|/2023) as the frame editing method, and we use the same hyper-parameters for all our results. PnP-Diffusion may fail to accurately preserve the structure of each frame due to inaccurate DDIM inversion (see Fig. |3} middle column, right frame: the dog’s head is distorted). Our method improves robustness to this, as multiple frames contribute to the generation of each frame in the video. Our framework can be combined with any diffusion-based image editing technique that accurately preserves the structure of the images; results with different --- --image editing techniques (e.g. [Meng et al| 2022}; (2023)) are available in the SM. Fig Bland{|show sample frames from the edited videos. Our edits are temporally consistent and adhere to the edit prompt. The man’s head is changed to Van-Gogh or marble (top left); importantly, the man’s identity and the scene’s background are consistent throughout the video. The patterns of the polygonal wolf (bottom left) are the same across time: the body is consistently orange while the chest is blue. We refer the reader to the SM for implementation details and video results. Baselines. We compare our method to state-of-the-art, and concurrent works: (i) Fate-Zero and (ii) Text2Video-Zero , that utilize a text-to-image mode or video editing using self-attention inflation. (iii) Re-render a Video (Yang et al.||2023) that edits keyframes by adding optical flow optimization to self-attention inflation of an image model, and then propagates the edit from the keyframes to the rest of the video using an off-the-shelf propagation method. (iv) Tune-a-Video (Wu et al.|/2022) that fine-tunes the text-to-image model on the given test video. (v) Gen-1 (2023), a video diffusion model that was trained on a large-scale image and video dataset. (vi) Per-frame diffusion-based image editing baseline, PnP-Diffusion (Tumanyan| . We additionally consider the two following baselines: (i) Text2LIVE (Bar-Tal et al] (2022) which utilize a layered video representation (NLA) (Kasten et al.||2021) and perform test-time training using CLIP losses. Note that NLA requires foreground/background separation masks and takes ~ 10 hours to train. (ii) Applying PnP-Diffusion on a single keyframe and propagating the edit to the entire video using (2019). 5.1 QUALITATIVE EVALUATION Fig.|6|provides a qualitative comparison of our method to prominent baselines; please refer to SM for the full videos. Our method (bottom row) outputs videos that better adhere to the edit prompt while maintaining temporal consistency of the resulting edited video, while other methods struggle to meet both these goals. Tune-A-Video (second row) inflates the 2D image model into a video model, and fine-tunes it to overfit the motion of the video; thus, it is suitable for short clips. For long videos it struggles to capture the motion resulting with meaningless edits, e.g., the shiny metal sculpture. Applying PnP for each frame independently (third row) results in exquisite edits adhering to the edit prompt but, as expected, lack any temporal consistency. The results of Gen-1 (fourth row) also suffer from some temporal inconsistencies (the beak of the origami stork changes color). Moreover, their frame quality is significantly worse than that of a text-to-image diffusion model. The edits of Text2Video-Zero and Fate-Zero (fifth and sixth row) suffer from severe jittering as these methods rely heavily on the extended attention mechanism to implicitly encourage consistency. The results of Rerender-a-Video exhibit notable long-range inconsistencies and artifacts arising primarily from their reliance on optical flow estimation for distant frames (e.g. keyframes), which is known to be sub-optimal (See our video results in the SM; when the wolf turns its head, the nose color changes). We provide qualitative comparison to Text2LIVE and to a RGB propagation baseline in the SM. 5.2. QUANTITATIVE EVALUATION We evaluate our method in terms of: Table 1: We evaluate our method in temporal consistency (i) edit fidelity measured by comput- by computing warp-error and conducting a user study, and ing the average similarity between jn fidelity to the target text prompt using CLIP similarity. the CLIP embedding (Radford et al.| See Sec. [5|for more details. of each edited frame and the target text prompt; (ii) temporal con- Warp-err | User preference| CLIP sistency: rrollowing (x10~*) | of our method [score t (2023); (20T8a), tempo- LDM recon. 2.0 = 0.ral consistency is measured by (a) PnP-Diffusion ~~ [| Ins "fF 94% YF 0.computing the optical flow of the Text2Video-Zero 12.5 78% 0.original video using Tune-a-Video 30.0 82% 0., warping the edited frames Fate-Zero 6.9 71% 0.according to it, and measuring the Genl = 70% 0.warping error, and (b) a user study; Rerender-a-Video 1.8 71% 0.We adopt a Two-alternative Forced yrs wjointattention | ~ 5.9 ~~~ 90% ~~ 7 0.Choice (2AFC) Bors}: | suggested Qurg w/o rand keyframes 3.7 - 0.in [Kolkin et al.] (2019); [Park et al.) Ours 3.0 _ 0., where participants are shown e input video, ours and a baseline result, and are asked to determine which video is more tem porally consistent and better preserves the motion of the original video. The survey consists of 2000-3000 judgments per baseline obtained using Amazon mechanical turk. We note that warpingerror could not be measured for Gen! since their product platform does not output the same number of input frames. Table[I]compares our method to baselines. Our method achieves the highest CLIP --- --F = =_— —E q oe So, =F ee | | Figure 7: Limitations. Our method edits the video according to the feature correspondences of the original video, hence it cannot handle edits that requires structure deviations. score, showing a good fit between the edited video and the input guidance prompt. Furthermore, our method has a low warping error, indicating temporally consistent results. We note that Re-rendera-Video optimizes for the warping error and uses optical flow to propagate the edit, and hence has the lowest warping error; However, this reliance on optical flow often creates artifacts and longrange inconsistencies which are not reflected in the warping error. Nonetheless, they are apparent in the user study, that shows users significantly favoured our method over all baselines in terms of temporal consistency. Additionally, we consider the reference baseline of passing the original video through the LDM auto-encoder without performing editing (LDM recon.). This baseline provides an upper bound on the temporal consistency achievable by LDM auto-encoder. As expected, the CLIP similarity of this baseline is poor as it does not involve any editing. However, this baseline does not achieve zero warp error either due to the imperfect reconstruction of the LDM auto-encoder, which hallucinates high-frequency information. We further evaluate our correspondences and video representation by measuring the accuracy of video reconstruction using TokenFlow. Specifically, we reconstruct the video using the same pipeline of our editing method, only removing the keyframes editing part. Table[2]reports the PSNR and LPIPS distance of this reconstruction, compared to vanilla DDIM reconstruction. As seen, TokenFlow reconstruction slightly improves DDIM inversion, demonstrating robust frame representation. This improvement can be attributed to the keyframe randomization; It increases robustness to challenging frames since each frame is reconstructed from multiple other frames during the generation. Notably, our evaluation focuses on accurate correspondences within the feature space during generation, rather than RGB frame correspondences evaluation, which is not essential to our method. 5.3 ABLATION STUDY First, we ablate the use of TokenFlow, Sec.[4.2| for en- Table 2: We reconstruct the video using forcing temporal consistency. In this experiment, we the TokenFlow pipeline, excluding keyframe replace TokenFlow with extended attention (Eq. [3) and editing. We evaluate the TokenFlow represencompute it between each frames of the edited video and tation with PSNR and LPIPS metrics. Our the keyframes (w joint attention). Second, we ablate the _Teconstruction improves vanilla DDIM inverrandomizing of the keyframe selection at each genera- Sin, highlighting the robusteness of Tokention step (w/o random keyframes). In this experiment, we _F!w representation. use the same keyframe indices (evenly spaced in time) PSNR + | LPIPS| across the generation. Table|1|(bottom) shows the quan-_ [DM tecon. titative results of our ablations, the resulting videos can - EH FM inversion | be found in the SM. As seen, TokenFlow ensures higher - gq 7.----- degree of temporal consistency, indicating that solely relying on the extension of self-attention to multiple frames is insufficient for achieving fine-grained temporal consistency. Additionally, fixing the keyframes creates an artificial partition of the video into short clips between the fixed keyframes, which reflects poorly on the consistency of the result. 6 DISCUSSION We presented a new framework for text-driven video editing using an image diffusion model. We study the internal representation of a video in the diffusion feature space, and demonstrate that consistent video editing can be achieved via consistent diffusion feature representation during the generation. Our method outperforms existing baselines, demonstrating a significant improvement in temporal consistency. As for limitations, our method is tailored to preserve the motion of the original video, and as such, it cannot handle edits that require structural changes (Fig i) Moreover, our method is built upon a diffusion-based image editing technique to allow the structure preservation of the original frames. When the image-editing technique fails to preserve the structure, our method enforces correspondences that are meaningless in the edited frames, resulting in visual artifacts. Lastly, the LDM decoder introduces some high frequency flickering (Biman tal 2025) A ossible solution for this would be to combine our framework with an improved decoder (e.g., Blattmann et al. (2023), Zhu et al.|(2023)). We note that this minor level of flickering can be easily eliminated with exiting post-process deflickering (see SM). Our work shed new light on the internal representation of natural videos in the space of diffusion models (e.g., temporal redundancies), and how they can be leveraged for enhancing video synthesis. We believe it can inspire future research in harnessing image models for video tasks, and for the design of text-to-video models. --- --7 ACKNOWLEDGEMENT We thank Narek Tumanyan for his valuable comments and discussion. We thank Hila Chefer for proofreading the paper. We thank the authors of Gen-1 and of Fate-Zero for their help in running their comparisons. This project received funding from the Israeli Science Foundation (grant 2303/20), the Carolito Stiftung, and the NVIDIA Applied Research Accelerator Program. Dr. Bagon is a Robin Chemers Neustein AI Fellow. We thank GEN-1 authors and Fate-Zero authors for their help in conducting comparisons. REFERENCES Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In European Conference on Computer Vision, pp. 707-723. Springer, 2022. Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113, 2023. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In JEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yingiang Zheng. Masactrl: Tuningfree mutual self-attention control for consistent image synthesis and editing, 2023. Duygu Ceylan, Chun-Hao Paul Huang, and Niloy Jyoti Mitra. Pix2video: Video editing using image diffusion. ArXiv, abs/2303.12688, 2023. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. arXiv preprint arXiv:2301.13826, 2023. V. Cheung, B.J. Frey, and N. Jojic. Video epitomes. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05), 2005. Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. arXiv preprint arXiv:2209.04747, 2022. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 2021. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023. Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel. Scenescape: Text-driven consistent scene generation. arXiv preprint arXiv:2302.01133, 2023. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruigi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv:2204.03458, 2022b. Susung Hong, Gyuseong Lee, Wooseok Jang, and Seungryong Kim. Improving sample quality of diffusion models using self-attention guidance. arXiv preprint arXiv:2210.00939, 2022. Ondfyej Jamri8ka, Sarka Sochorova, Ondej Texler, Michal Lukaé, Jakub Fier, Jingwan Lu, Eli Shechtman, and Daniel S¥kora. Stylizing video by example. ACM Transactions on Graphics, 2019. Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 2021. Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. ArXiv, abs/2303.13439, 2023a. Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023b.--- --Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Style transfer by relaxed optimal transport and self-similarity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10051-10060, 2019. Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In European Conference on Computer Vision, 2018a. Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman, Ersin Yumer, and Ming-Hsuan Yang. Learning blind video temporal consistency. In Proceedings of the European conference on computer vision (ECCV), pp. 170-185, 2018b. Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, and Jia-Bin Huang. Shape-aware textdriven layered video editing. arXiv preprint arXiv:2301.13173, 2023a. Yao-Chih Lee, Ji-Ze Genevieve Jang Jang, Yi-Ting Chen, Elizabeth Qiu, and Jia-Bin Huang. Shape-aware text-driven layered video editing demo. arXiv preprint arXiv:2301.13173, 2023b. Chenyang Lei, Yazhou Xing, and Qifeng Chen. Blind video temporal consistency via deep video prior. In Advances in Neural Information Processing Systems, 2020. Chenyang Lei, Xuanchi Ren, Zhaoxiang Zhang, and Qifeng Chen. Blind video deflickering by neural filtering with a flawed atlas. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. Shaoteng Liu, Yuecheng Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with crossattention control. ArXiv, abs/2303.04761, 2023. Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. arXiv, 2023. Wan-Duo Kurt Ma, JP Lewis, W Bastiaan Kleijn, and Thomas Leung. Directed diffusion: Direct control of object placement through attention guidance. arXiv preprint arXiv:2302.13153, 2023. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 8162-8171. PMLR, 2021. Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei A. Efros, and Richard Zhang. Swapping autoencoder for deep image manipulation. In Advances in Neural Information Processing Systems, 2020. Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. arXiv preprint arXiv:2303.11306, 2023. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeldez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv: 1704.00675, 2017. Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv:2303.09535, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In International conference on machine learning, pp. 1060-1069. PMLR, 2016. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjérm Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022.--- --Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, 2015. Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox. Artistic style transfer for videos. In Pattern Recognition - 38th German Conference (GCPR), 2016. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-toimage diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarezyk, and Jenia Jitsev. Laion-5b: An open largescale dataset for training next generation image-text models. ArXiv, abs/2210.08402, 2022. Oded Shahar, Alon Faktor, and Michal Irani. Space-time super-resolution from a single video. In CVPR 2011, 2011. Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knn-diffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022. Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang gil Lee, and Sung-Hoon Yoon. Edit-a-video: Single video editing with object-aware consistency. ArXiv, abs/2303.07945, 2023. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265. PMLR, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2020. Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision— ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16. Springer, 2020. Ondyej Texler, David Futschik, Michal Kuéera, Ondfej JamriSka, Sarka Sochorova, Menglei Chai, Sergey Tulyakov, and Daniel Sykora. Interactive video stylization using few-shot patch-based training. ACM Transactions on Graphics, 2020. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-tovideo translation, 2023. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N. Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. 2017 IEEE International Conference on Computer Vision (ICCV), pp. 5908-5916, 2016. Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and MingHsuan Yang. A tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. arXiv preprint arxiv:2305.15347, 2023. Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, and Gang Hua. Designing a better asymmetric vqgan for stablediffusion, 2023.--- --Table 3: We report average runtime in seconds, of running ours and competing methods on a video of 40 frames. TAV | Text2video-zero | Rerender-a-video | fatezero | PnP | ours (preprocess) | ours (sampling) ours (total) 2684 198 285 349 208 50We provide additional implementation details below. We refer the reader to the HTML file attached to our Supplementary Material for video results. A IMPLEMENTATION DETAILS StableDiffusion. We use Stable Diffusion as our pre-trained text-to-image model; we use the StableDiffusion-v-2-1 checkpoint provided via official HuggingFace webpage DDIM inversion. In all of our experiments, we use DDIM deterministic sampling with 50 steps. For inverting the video, we follow (2023) and use DDIM inversion with classifierfree guidance scale of 1 and 1000 forward steps; and extract the self-attention input tokens from this process similarly to{Qi et al.](2023p. Runtime. Since we don’t compute the attention module on most video frames (i.e., we only compute the self-attention output on the keyframes) our method is efficient in run-time, and the sampling of the video reduces the time of per-frame editing by 20%. The inversion process with 1000 steps is the main bottleneck of our method in terms of run-time, and in many cases a significantly smaller amount of steps is suffieicent (e.g. 50). Table|3]reports runtime comparisons using 50 steps in all methods. Notably, our sampling time is indeed faster than that of per-frame editing (PnP). Hyper-parameters. In equation|[5]we set w; to be: w; = o(d_/(dy +d_)) where dx = ||i—i*||,d_ = || -— a7 || (6) where o is a sigmoid function, i+ and i~ are the future and past neighboring keyframes of i, respectively. For sampling the edited video we set the classifier-free guidance scale to 7.5. At each timestep, we sample random keyframes in frame intervals of 8. Baselines. For running the baseline of Tune-a-video (Wt et al.| we used their official repository. For Gen-1 (Esser et al.| we used their platform on Runaway website. This platform outputs a video that is not in the same length and frame-rate as the input video; therefore, we could not compute the warping error on their results. For text-to-video-zero (2023b) we used their official repository, with their depth conditioning configuration. For Fate-Zero (Qi et al.| with used their official repository, and verified the run configurations with the authors.