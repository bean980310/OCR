--- --arXiv:2309.04663v2 [cs.CL] 12 SepFIAT: FUSING LEARNING PARADIGMS WITH INSTRUCTION- ACCELERATED TUNING Xinyi Wang, John Wieting, Jonathan H. Clark Google DeepMind {xinyiwang, jwieting, jhclark}@google.com ABSTRACT Learning paradigms for large language models (LLMs) currently tend to fall within either in-context learning (ICL) or full fine-tuning. Each of these comes with their own trade-offs based on available data, model size, compute cost, ease-of-use, and final quality with neither solution performing well across-the-board. In this article, we first describe ICL and fine-tuning paradigms in a way that highlights their natural connections. Based on these connections, we propose a new learning paradigm called Frat] that fuseq?] the best of these paradigms together, enabling promptengineered instructions and chain-of-thought reasoning with the very largest models while also using similar methods to perform parameter updates on a modestly-sized LLM with parameter-efficient tuning. We evaluate FIAT’s effectiveness on a variety of multilingual task?}| and observe that FIAT performs better than both ICL and fine-tuning at scales ranging from 100—10,000 training examples. We hope that FIAT provides a practical way of harnessing the full potential of LLMs without needing to make a hard choice between learning paradigms. 1 INTRODUCTION Large language models (LLMs) show impressive generalization ability to new tasks and languages. Some of their most exciting capabilities, such as producing logical reasoning to solve a problem, are found to emerge only when the model size is over a certain threshold, often hundreds of billions of parameters (Wei et al.||2022bja). The impressive capabilities of these models to produce high-quality responses without any task-specific tuning along with the very high cost of further tuning such models has led much recent work to focus on the paradigm of In-Context Learning (ICL)—placing a few task-specific examples and instructions into the model’s input (Brown et al.||2020}[Chowdhery et al.| 2022} Google et al.}/2023} OpenAl| 2023). Although prior work has seen that fine-tuning a model on task data can often lead to superior performance on the downstream task compared to ICL [2021] 2020ajb| 2023), there are significantly fewer recent efforts on fine-tuning models for tasks with limited data, perhaps because the time and compute costs associated with tuning a very large model drives practitioners toward smaller models, abandoning the ability to take advantage of emergent model capabilities. ICL and model fine-tuning each come with their own trade-offs. ICL does not incur any training cost and it allows one to utilize the most capable LLMs (Schick & Schiitze| |2020b}/OpenAl| 2023). However, while ICL can achieve competitive performance on many tasks with a handful of annotated examplars, it often requires very large models to work well and it cannot take advantage of additional training examples if they do not fit into the context window. For many tasks, this leads to ignoring a substantial amount of potentially-useful training examples. Fine-tuning, on the other hand, is not constrained by the need to fit training examples into the model’s input, and it can be quite effective 'We derive the name FIAT from Fusing Learning Paradigms with Instruction Accelerated Tuning. >FIAT fuses not only the learning paradigms but the models themselves. 3We say that these tasks are naturally low-data because no additional data is available for such languages and it’s non-trivial to obtain more; we contrast this with artificially low-data scenarios where large data exists, but is ignored. --- --Updated by humans Updated by gradient by prompt engineering descent on task data Instruction-tuned LLM LLM | a y] } + + Input Sequence Few-shot In-Context Learning Full Fine-tuning CoT Reason Instruction-tuned LLM. Small parameter updated by gradient descent on task data | _IE:Input Sequence | Updated by humans |_| Dp Xx by prompt engineering Input Sequence FIAT using both Paradigms Figure 1: Overall flow of FIAT and how it compares to ICL and fine-tuning. The colored components are updated while building and learning a task-specific instance of FIAT, while other components are fixed.@g is the parameters of the larger LLM and J, are the instructions used to induce reasoning; 6, are the parameters of a moderately-sized LLM to be tuned and I, is its instructions, which helps the model predict the correct final answer. even with smaller language models. These trade-offs tend to lead practitioners to arbitrarily pick a paradigm or run costly experiments on these disparate methods in order to choose the best approach. We instead take the view that these two model learning paradigms are in fact complementary. To this end, we propose FIAT—Fusing Learning Paradigms with Instruction-Accelerated Tuning (FIAT), which utilizes both ICL on very large models and parameter tuning on moderately-sized LLM while fusing the common techniques associated with each paradigm. FIAT uses hand-engineering instruction prompts that elicit chain-of-thought reasoning from a very large model, while also using the generated reasoning and instruction prompts to tune a moderately-size LLM with parameter-efficient tuning. [Figure I|shows the workflow of FIAT and how it compares to ICL and fine-tuning. In the remainder of this article, we formally describe the connections between ICL and fine-tuning, along with the various techniques that have developed within each paradigm ($2); we propose FIAT, which fuses the best of these together and avoids many of the pitfalls of each of the individuals ($2.3); we present experiments demonstrating how FIAT improves over both learning paradigms in data scenarios ranging from 100—10,000 examples along with ablations detailing where these gains come from ($3). 2 LEARNING PARADIGMS FOR LLMS In this section, we review two popular learning paradigms for LLMs (ICL in §2.1]and parameter tuning in §2.2) while considering their strengths and weaknesses, which directly lead to FLAT (§2.3). 2.1 IN-CONTEXT LEARNING Instructed ICL keeps the parameters of the LLM fixed, but it instead selects an instruction prompt (often through manual optimization) to improve the accuracy of the downstream task. Formally, a model prediction is made by sampling“|a very large pre-trained LLM parameterized by fixed 6 anda textual instruction J: P(ya;0,D) (1) Typically, the sampling is a simple argmax with temperature 0, though this isn’t always the case as in techniques such as majority voting. --- --While the instructions J are prefixed onto the model input x in practice, we intentionally notate them as an argument of the model, which we argue better reflects how they are conceptualized; we will build on this later. Chain-of-thought reasoning _ pushes instructed ICL a step further by crafting J to induce stepby-step reasoning in the output of the model that improves the model’s ability to arrive at a correct prediction (Wei et al.||2022b). This allows auto-regressive inference to output observations about the input or solve sub-problems of the overall task that future decoding steps can leverage when predicting the final answer; it may also elicit textual patterns that the model saw during pre-training, that would otherwise be difficult to access in the model’s latent feature space (e.g. via fine-tuning). Few-shot ICL Few-shot ICL differs from instructed ICL in that its instructions J are composed of a small number of examplars selected among training examples D that have been formatted as a textual input to the model via instructions. Instruction-tuned Base Models _ Instruction-tuned models such as FLAN and TO (Sanh et al.|/Chung et al} 20224 Longpre et al} 2023) often provide significant improvements on ICL compared to using a pre-trained model. This is because instruction-tuning is essentially a second stage pretraining using a set of multitask data whose distribution is closer to the downstream task. The ICL paradigm achieves competitive results on various tasks with no or only a handful of annotated examples. While it does not incur any additional model tuning cost, ICL often has high inference cost because it requires LLMs over a certain size to work well, especially when using techniques such as chain-of-thought. It also cannot take advantage of additional task data beyond what fits into the context window of the model. 2.2 PARAMETER TUNING Full-Parameter Fine-tuning Given pre-trained parameters 0 of a LLM to tunef)| standard finetuning simply optimizes all parameters of the model on task-specific supervised training data D according to: P(y|z; 8) (2) The optimization of 6 is similar in purpose to the process of human prompt engineering of J in ICL. Since model fine-tuning does not have to fit training data into the context window of the model, it is more effective when there are slightly more training examples available. Fine-tuning also works well on smaller language models with enough training examples, leading to faster inference. However, fine-tuning incurs additional training cost and requires access to model parameters, while some of the most capable LLMs are available for inference-only API access. The model could also easily overfit to the training examples due to catastrophic forgetting (Goodfellow et al.|{2013), especially for tasks with limited data. Parameter-efficient Fine Tuning (PEFT) improves the tuning procedure by using a learning parameterization 0°E*T where |@PE"T| < |6|. Besides reducing the danger of overfitting, this learning technique also avoids forgetting features that may be useful for generalization beyond the training set. Similarly, ICL avoids catastrophic forgetting by only modifying the input to the model while keeping the parameters fixed. 2.3. FUSING LEARNING PARADIGMS WITH FIAT In this section, we construct FIAT, motivating the purpose of each design choice in terms of modeling capabilities. ICL and fine-tuning each have compelling strengths along with pitfalls, which we summarize in[Table T] At a high level, we observe that these properties are largely complementary. *In practice, |6| tends to be much smaller for fine-tuning than for ICL. --- --ICL Fine-tuning Strengths Works well with small model No Yes Supports large training data No Yes Supports chain-of-thought reasoning Yes No Usage of instruction prompts Yes No Challenges No parameter updates Yes No Avoids catastrophic forgetting Yes No Table 1: Comparison of the ICL and fine-tuning learning paradigms, according to common usage patterns. Reflecting on these abilities of ICL and fine-tuning, we seek an approach that is capable of: ¢ Instruction following: follows human-engineered instructions to achieve high quality predictions; * Chain-of-thought reasoning: produces intermediate text that helps the model toward correct predictions; ¢ Parameter tuning: refines its internal representation to align with a moderate to large number of supervised training examples; and ¢ Data scaling: provides high quality models with data scales from 100 to 1000’s of examples. Model stacking via CoT-augmented Tuning We begin with the observation that chain-of-thought prompting is typically not supervised, but rather induced via carefully-written instructions. Motivated by this, we fuse two models for learning and inference: a big model ( with all the most powerful emergent capabilities of LLMs, and a tunable model tT whose size can be flexibly chosen depending on the capacity needs of the task of interest. We assign the responsibility of chain-of-thought inference to 6 and then provide its textual predictions #g to the tunable model; it can then learn how to best use these inputs (e.g. chain-of-thought explanations) based on how useful they are with regard to predicting the supervised outputs. The parameters 6 remain fixed as we do not have nor require any directly supervised data for its sub-task. Instruction-augmented Tuning Crafting a good instruction prompt is known to be essential to high-quality ICL performance, and so we naturally include instructions J to generate reasoning and explanations as a first step. Although instructions are typically not used for smaller tunable model I, we observe that instructions have the potential to benefit tuning as well. We speculate that instructions help better align a task’s inputs with the distribution seen during pre-training, allowing the model to not only converge faster but also make fewer parameter updates. This, in turn, avoids the risk of catastrophic forgetting associated with excessive parameter updates. Therefore, FIAT also provides separate instructions I, for the tunable model Pervasive Instruction-tuned Models Already, instruction-tuned models have become the standard for ICL; we use such models as @g in all of our experiments. However, given FIAT’s use of Instructionaugmented Tuning, we also depart from the common practice of fine-tuning starting from models pre-trained primarily on span corruption objectives and instead initialize with instruction-tuned checkpoint (Longpre et al.| |2 This makes optimization easier since the model is already expecting instructions; this can be especially beneficial in limited training data scenarios. Parameter-efficient Tuning So far, we have added chain-of-thought reasoning, instruction following in tuning, and instruction-tuned initialization to FIAT’s design, all of which move the pre-tuning model and the task definition toward each other in terms of increasing the probability of the desired output. We hypothesize that parameter-efficient tuning is a particularly good fit for optimizing 0, in FIAT over the training data, because large changes to the model parameters 6, should not be ®TIn FIAT, instructions can be viewed as serving purpose analogous to a Bayesian prior in earlier statistical learning methods: They allow encoding human knowledge into the learning procedure alongside supervised data that empirically estimates parameters. However, textual instructions are a far more natural way of doing this than the hyperparameters of a Dirichlet. --- --Algorithm 1: Model building with FIAT Algorithm 2: Inference with FIAT : 7Input: 03, 0,-,D Input: x, Ig, I, 0g, 0, Output: 61, I3, I, Output: // Write reasoning instructions & select exemplars. // Generate expansions, explanations, reasoning. Ig = PROMPTENGINEERING(D, 0g) 9a = argmax, P(y|x; 0, 13) /] Write tuning instructions, based on large model. // infer final output using tuned model. = ; ; : : ay I, = PROMPTENGINEERING(D, 63) y = argmax, P(y|a, 9a; 07, 17) // Initialize parameter-efficient tuning. PEFT OPEFT < IniT(6,) // Tterate over examples or batches of data. for x,y € Ddo // Generate expansions, explanations, reasoning. §o = arg max, P(y|x; 0g, Is) // Optimize using parameter-efficient update. _ fae \PEFT Gr = VewerP(y\2, 9930, Of", Ly) \PEFT \PEFT OLPFT <— Uppate(Or', g,) end /I Apply PEFT updates to final tuned model. \PEFT 0 + 0, BOF Figure 2: Model building and inference with FIAT. Left: Model building with FIAT begins with interactive prompt engineering of the instructions I. I specifies how to perform reasoning using few-shot exemplars on @3—i.e. behaviors for which we have no large-scale annotations, while I, specifies guidance to the tuned model 0, for using the generated reasoning and input to produce a final output. Both g and 6, are instruction-tuned models and only @; is updated during training via parameter-efficient tuning. Right: Inference with FIAT is very simple, requiring only: (1) a call to the large generative model using the fixed pre-trained parameters 0, and the reasoning instructions Ig; and (2) a call to the tuned model 6, along with the associated task instructions [,. necessary given a good initialization[!|Formalizing all the above modifications, we arrive at the final formulation of FIAT used for fine-tuning and inference in[Alg. I]and 3. EXPERIMENTS Datasets One of our primary objectives in selecting datasets that naturally cover a broad variety of training data sizes. We consider tasks ranging from classification to exercising a model’s ability to generate short answers, and we include a large number and variety of languages to evaluate the generality of the method. First, we use XOR-ATTRIQA (Muller et al.|{2023), a classification task where model is asked to predict whether the provided answer to the question is supported by the given passage context, which includes 5 languages with 262 examples total. We refer to this as the (100) data scenario. We also study FIAT’s behavior on the Cross-lingual QA task of XTREME-UP (Ruder et al] 2023). This data is an expansion of the XOR QaB]dataset (Asai et al.| , a cross-lingual variant of the TyDi QA dataset. This task asks a model to predict the correct English answer span given a non-English question and an English answer passage; this task also includes the possibility that the passage does not contain a correct answer, making it more challenging. Cross-lingual QA is a particularly important task for languages that have very little answer content as it enables providing answers to questions that would otherwise be unanswerable using only in-language content. We provide results on two focus sets. First, we use the subset of 20 Indic languages in XTREME-UP Cross-lingual QA where each language has about 300 examples, to allow for studying a scenario with In FIAT, we use LORA (Hu et al.| 2021) to parameterize the tuning procedure because it does not induce ‘uture work shou: additional inference cost. Fi id consider other methods such as soft prompt tuning po Ip. OR QA stands for cross-lingual open-retrieval question answering; note the difference between XOR QA and XOR-ATTRIQA. --- --XTREME-UP XTREME-UP XOR-ATTRIQA Cross-lingual QA (Indic) Cross-lingual QA (Full) (100) ©1000) (10000) 6, 03 Method Acc / AUC-PR Fl Fl — L Ici 78.6 | —t 68.9 69.xs > Fine-tune 90.5 / 52.1 63.5 75.L FIAT 94.0/78.1 73.6S — Fine-tune 90.6 / 54.5 67.1L FIAT 93.9/77.5 771.3 79.Gain over best baseline +3.5 / +26.0 (vs S fine-tune) +8.4 (vs ICL) +1.5 (vs S fine-tune) Table 2: Overall results of FIAT and typical baselines. While we provide improvements with regard to the best baseline, we also point out that the best baseline often differs between ICL and fine-tuning, especially at smaller model sizes; this leaves practitioners to empirically determine the best course of action. ‘AUC-PR is not computed for the ICL because outputs are text-only. moderate data; we refer to this as the O(1000) data scenario. We also study the full XTREME-UP Cross-lingual QA task which has 22,500 examples across 27 languages where the 5 high-resource languages have more than 2500 examples each; we refer to this as the O(10,000) data scenario] Together, these tasks allow us to test our methods on three different data size scenarios from small 100’s to over training 20,000 examples. Details of the languages and the dataset size can be found in Models We use PaLM-2 (Google et al.| {2023) as our base model, and we experiment with instruction-tuned models using the FLAN mixture (Chung et al.|/2022). We use PaLM-2 L as Mg and we use PaLM-2 XS and S for M,. Baselines We compare to both ICL and fine-tuning baselines. For ICL, we use PaLM-2 L with chainof-thought reasoning (Wei et al.|{2022b). We include 4 few-shot exemplars with hand-written chainof- thought explanations in English for each of the 5 languages in the XOR-ATTRIQA Attribution '0/for a total of 20 exemplars. However, for XTREME-UP cross-lingual QA, it was not feasible to hand-engineer prompts for each of the 27 languages. Therefore, we hand-write 4 chain-of-thought explanations based on Bengali exemplars|""Jand use the same ICL examples for all 20 languages. 3.1 RESULTS We present the performance of the baselines (ICL and fine-tuning) and our FIAT framework for all three data settings in We show the average scores across all languages in each dataset for simplicity, and we provide the result for each language in[App. A.2| Looking at the baselines, we find that few-shot ICL using PaLM-2 L model is quite competitive without any additional model tuning, but still lags behind PaLM-2 S fine-tuned on a relatively small amount of task data. However, we find that the best baseline differs between ICL and fine-tuning PaLM-2 XS across different tasks and data size settings. If one were choosing between just ICL or fine-tuning, this inconsistency makes it difficult to determine the best course of action without empirical comparisons. On the other hand, FIAT offers the best performance by combining the strengths of both ICL and fine-tuning. 4 ABLATIONS AND ANALYSIS In this section, we study the effect of individual design decisions within FIAT and present the results in|Table 3} and drawing conclusions from them below. In the end, we find that while certain design °We report the average result on the under-represented languages, following the recommendations of the XTREME-UP benchmark. ‘During manual prompt engineering, we used Google Translate to assist with explanation annotation. "Note that while the exemplars have Bengali questions, we instruct the model to carry out its reasoning in English. --- --XOR-ATTRIQA XTREME-UP XTREME-UP Cross-lingual QA: Indics — Cross-lingual QA: Full (100) (1000) (10000) 6, 68 Method Acc / AUC-PR Fl Fl — L Few-shot ICL 78.6 / — 68.9 69.L FIAT 94.0/78.1 73.6— __ w/o CoT-augmentated tuning 94.0 / 80.3 70.7 76.XS = — _ w/o Instruction-augmented tuning 93.5 /72.4 69.8 76.— __ w/o Parameter-efficient tuning 93.7 / 69.8 67.8 75.— __ w/o Instruction-tuned base model 90.5 /52.1 63.5 75.L FIAT 93.9/77.5 71.3 79.— __ w/o CoT-augmentated tuning 94.7 / 80.7 76.7 79.Ss — _ w/o Instruction-augmented tuning 94.1/71.6 75.3 79.— __ w/o Parameter-efficient tuning 94.7 /76.2 72.3 78.— __ w/o Instruction-tuned base model 90.6 / 54.5 67.1Table 3: Ablations showing the contribution of each modification within the FIAT recipe; each removal is cumulative with the one above. We observe that each modification tends to make a substantial positive impact on at least one scenario. The bottom line in each block is equivalent to traditional fine-tuning. choices tend to have a larger effect on some settings than others, each tends to have substantial contributions in some area, and together the overall modeling recipe is very effective as a whole. Instructed-tuned base models improve final quality of fine-tuned models. The instruction-tuned Flan XS model improves over the base model on all datasets, especially on XOR-ATTRIQA and XTREME-UP Cross-lingual QA Indic, where the total amount of task data is around O(100) to O(1000). This indicates that instruction-tuned models are not only beneficial for ICL, but can also be beneficial for fine-tuning on limited data However, the advantage of instruction-tuned model on XTREME-UP Cross-lingual QA decreases from the Indic (O(1000) training examples) to Full (O(10000) training examples), indicating that instruction-tuned model is less helpful when the fine-tuning dataset is large. Instruction-augmented Tuning generally leads to significant improvements. Adding an appropriate prompted format to the task data is generally beneficial for all tasks. This result indicates that prompt engineering is not only helpful for direct few-shot ICL, but also has a positive impact on model fine-tuning. Prompted tuning is especially helpful for XOR-ATTRIQA and XTREME-UP Crosslingual QA Indic, where the amount of task data is very limited. This is because the prompt format aligns the distribution of downstream task closer to the model pretraining distribution, which allows the pretrained model to generalize to the downstream task with a small amount of task examples. CoT-augmentated Tuning is helpful for most tasks. Our CoT-augmented Tuning can lead to large improvement for XTREME-UP Cross-lingual QA Indic task. Surprisingly, it does not help XORATTRIQA, which is contradictory to findings from prior works which show that explanations can be especially helpful for classification tasks (Hsieh et al. 2023} Zhou et al.| 2023). We hypothesize that this is because the model already performs quite well on XOR-ATTRIQA without having access to the explanations (over 90 percent accuracy) and this task may be reaching its saturation point. CoT-augmented Tuning is even more helpful for tasks and languages with lower performance. We analyze the relationship between the gains brought by CoT-augmentated Tuning on the XTREMEUp Cross-lingual QA tasks. [Figure 3]shows the improvement in F1 score of different languages versus a baseline model’s F1 score that lacks CoT-augmented Tuning. We can see that there is an inverse relationship between the benefit of CoT-augmented Tuning and the baseline model score, indicating that CoT is more beneficial for harder tasks or languages where the model could not perform well without the help of the CoT augmentation. This means that while we see meaningful gains in aggregate, for individual languages (or, more generally, individual tasks and use cases), CoT can have an out-sized impact on quality. --- --= rc) Method Fl — Gains Baseline 70.7 — Distilled CoT 72.5 +Our CoT-augmented Tuning 73.6 +A F1 Score with CoT Figure 4: Performance on XTREME-UP Cross-lingual QA Indic compared to the baseline without CoT. Our CoT-augmented Tuning method significantly outperforms previous methods on distilling CoT. 60F1 Score Figure 3: Gains in Fl on XTREME-UP Crosslingual QA with CoT-augmented Tuning. The lower performing languages tend to benefit more from CoT augmentation. 70 XOR QA Indic Attribution 60 Mmm FLAN mE40 —— Base —— Prompted0 500Step . AUC-PR Figure 5: The validation Fl score through- Figure 6: Improvement with Instruction out training on XTREME-UP Cross-lingual QA for methods with and without Instructionaugmented Tuning. Instruction-augmented Tuning out-performs baseline and it has much better performance at step 0, before any augmented Tuning for the model with and without instruction-tuning. Instructionaugmented Tuning is generally helpful for both types of models, and it tends to be more beneficial for instruction-tuned models model optimization. CoT-augmented Tuning leads to better quality than CoT distillation. Recent work proposed distilled CoT, which uses the explanation as a multitask output target, so that the model does not need to generate additional explanations at test time [2023). Here we compare the performance of these two different ways of using the CoT explanations and list the performance on cross-lingual QA tasks 1 Despite incurring higher inference cost, our CoT augmentation method further out-performs tilled CoT by a large margin on the harder XTREME-UP Cross-lingual QA Indic task. In general, we view distillation as an orthogonal technique to FIAT, which is aimed at efficiency over quality. Adding instructions to tuning helps from beginning to end. In [Figure 5] we plot the training curves of Flan PaLM-2 S model with and without Instruction-augmented Tuning. We can see that adding instructions to tuning leads to much better performance at step 0, before any model optimization. This indicates that adding the instructions to the task data during fine-tuni can significantly improve the zero-shot performance of the model, probably because it makes the task "Note we use the term instruction-augmented tuning to differentiate from the separate concepts of instruction-tuned base models, which creates base models that are better able to follow instructions of specific tasks later, and prompt tuning, which learns soft prompt embeddings. --- --data more similar to the data used in the instruction tuning stage. Importantly, this also implies that the model parameters don’t need to move as far away from their starting point in order to achieve the same level of quality, reducing the risk of catastrophic forgetting. However, the model does not only reach the same level of quality with less steps, but also manages to exceed the quality of a model without instructions. Instruction-augmented Tuning helps more with an instruction-tuned base model. We compare the effect of prompted tuning on models with and without instruction tuning. [Figure 6] shows that prompted tuning generally brings improvements for both the base model without instruction tuning and the Flan model with instruction tuning, while the gains on the instruction-tuned Flan model tend to be slightly larger and more consistent. This is likely because the data format we used for prompted tuning (task instructions followed by the input) is more similar to the Flan data mixture used for instruction tuning. 5 RELATED WORK Instruction Tuning —Instruction-tuned models (Wei et al.|[2021|/Longpre et al.|[2023) often have better performance for few-shot ICL tasks than base language models since they are already primed to following instructions due to being fine-tuned on a diverse set of tasks. Using instruction-tuned models is a key component of FIAT. In-Context Learning In in-context learning, the parameters of the LLM remain fixed and a prompt containing a few examples along with reasoning steps is used to prime the model for solving similar tasks (Nye et al} 2021} |Wei et al} 2022b). In-context learning works best for large language models. FIAT uses this capability of large language models, along with fine-tuning, to power small language models in the low-data regime. Knowledge Transfer from Larger to Smaller LLMs A popular prior method for transferring knowledge from large models to smaller ones is model distillation (Hinton et al. (2015), where the outputs of a larger model are used as a training signal for a smaller one. Other approaches include using the larger language model to generate data and then using this data to train smaller models. More recently, the latter has approach has been extended to generate reasoning steps which are provided as fine-tuning data for the smaller language model (Magister et al. Huang et al. 2022} Li et al. 2022} Ho et al.| 2023} Hsieh et al.| 2023} Fu et al.|/2023}|Zhu et al. Li et al.|/2023). Under-represented Languages Most work that trains large language model and uses them for downstream tasks focus on English or the collection of 100 or so languages where there are large, easily available corpora (ImaniGooghari et al. 2023p. Tail languages have often been ignored by language technologies due to lack of available corpora Ca aria aansatar oun has focused on tail languages outside of these head languages (Bapna et al.|/2022}/Ruder et al.|/2023). In this work, we make the low-data regime the focus of our efforts, which is especially useful for tail languages. Fine-tuning smaller LLMs _ While fine-tuning with prompts has been studied for encoders pretrained with masked language modeling objectives (Scao & Rush| (2021p, we show that it is also important to fine-tuning generative language models. For example, some works show that fine-tuning a smaller language model is a more competitive and efficient method for practical low-data learning problems than few-shot ICL (Asai et al |2023) Ruder eta, ]2023). [Agrawal et al|(2022) propose t synthetic QA data generated from very large LLM to improve the performance of a smaller model. 6 CONCLUSION We have presented FIAT, a method that fuses the ICL and fine-tuning learning paradigms and leads to improved model predictions across a variety of data scenarios, ranging from 100—10,000 training examples. We hope FIAT provides a practical way of harnessing the full potential of LLMs without needing to make a hard choice between learning paradigms. --- --REFERENCES Priyanka Agrawal, Chris Alberti, Fantine Huot, Joshua Maynez, Ji Ma, Sebastian Ruder, Kuzman Ganchev, Dipanjan Das, and Mirella Lapata. Qameleon: Multilingual qa with only 5 examples. arXiv preprint arXiv:2211.08264, 2022. Akari Asai, Jungo Kasai, Jonathan H Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. Xor qa: Cross-lingual open-retrieval question answering. arXiv preprint arXiv:2010.11856, 2020. Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023. Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, et al. Building machine translation systems for the next thousand languages. arXiv preprint arXiv:2205.03983, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022. Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages. Transactions of the Association for Computational Linguistics, 8: 454-470, 2020. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023. Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv: 1312.6211, 2013. Google, Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305. 10403, 2023. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv: 1503.02531, 2015. Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14852-14882, Toronto, Canada, July 2023. Association for Computational Linguistics. Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301, 2023. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.--- --Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André FT Martins, Frangois Yvon, et al. Glot500: Scaling multilingual corpora and language models to 500 languages. arXiv preprint arXiv:2305.12182, 2023. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Symbolic chain-of-thought distillation: Small models can also" think" step-by-step. arXiv preprint arXiv:2306.14050, 2023. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726, 2022. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688, 2023. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason. arXiv preprint arXiv:2212.08410, 2022. Benjamin Muller, John Wieting, Jonathan H Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Baldini Soares, Roee Aharoni, Jonathan Herzig, and Xinyi Wang. Evaluating and modeling attribution for cross-lingual question answering. arXiv preprint arXiv:2305.14332, 2023. Ravindra Nayak and Raviraj Joshi. L3Cube-HingCorpus and HingBERT: A code mixed HindiEnglish dataset and BERT language models. In Proceedings of the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference, pp. 7-12, Marseille, France, June 2022. European Language Resources Association. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. OpenAlI. Gpt-4 technical report, 2023. Sebastian Ruder, Jonathan H Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel A Sarr, Xinyi Wang, et al. Xtreme-up: A user-centric scarce-data benchmark for under-represented languages. arXiv preprint arXiv:2305.11938, 2023. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021. Teven Le Scao and Alexander M Rush. How many data points is a prompt worth? NAACL, 2021. Timo Schick and Hinrich Schiitze. Exploiting cloze questions for few shot text classification and natural language inference. arXiv preprint arXiv:2001.07676, 2020a. Timo Schick and Hinrich Schiitze. It’s not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118, 2020b. Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022a.--- --Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824—24837, 2022b. Yangqiaoyu Zhou, Yiming Zhang, and Chenhao Tan. Flame: Few-shot learning from natural language explanations. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics, 2023. Xuekai Zhu, Biging Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou. Pad: Program-aided distillation specializes large models in reasoning. arXiv preprint arXiv:2305.13888, 2023.--- --Split bn fi ja ru te Train 40 66 20 84Validation 218 150 578 136Test 2822 1318 1908 1268Table 4: Dataset size for XOR-ATTRIQA. bho _ brx ebm gom gu hi bne kn mai ml omni mr mur or paps sata our abn ja iO 326 326 326 326 326 326 326 326 326 326 326 326 326 326 326 326 326 326 326 3159 377 2467 2026 3327 2560358 357 365 365 371 519 372 373 369 373 380 385 386 386 385 384 385 384 387 O41 618 978 727 861 731631 633 634 629 630 1049 629 631 635 629 628 633 632 632 624 633 630 630 634 582 397 606 471 S48Table 5: Dataset size for XTREME-UP Cross-lingual QA. A APPENDIX A.l We provide the number of training, validation, and test examples for each task ii LIST OF LANGUAGES FOR EACH TASK A.2 LANGUAGE-WISE BREAKDOWN OF THE RESULTS We provide the performance for each language in|Table 6 Table 7| and|Tablebn fi ja ru te M, Mg Method Acc / AUC-PR — L_ Few-shot ICL 85.9/ 78.5/ 85.4/— 845/— 58.9/ L Fiat 92.6/81.1 91.0/85.3 96.3/66.5 94.8/84.9 95.3/72.—- w/o CoT-Augmented Tuning 92.5/84.7 918/858 96.2/70.3 94.6/84.1 95.0/76.XS —- w/o Instruction-Augmented Tuning 91.7/74.1 91.2/81.4 95.9/53.5 93.8/77.4 94.8/75.—- w/o Parameter-efficient Tuning 92.6/73.9 92.0/76.7 95.0/55.8 94.2/74.1 94.7/ 68.—- w/o Instruction-tuned base model 89.4/65.6 88.9/65.9 94.3/42.1 90.1/58.6 89.7/28.L Fiat 92.3/81.3 92.1/84.0 96.2/62.4 94.6/84.9 94.0/93.—- w/o CoT-Augmented Tuning 93.0/84.3 94.4/81.2 955/588 988/874 95.3/78.S — w/o Instruction-Augmented Tuning 93.1/75.6 92.7/82.9 95.0/51.3 94.6/78.1 95.2/70.—- w/o Parameter-efficient Tuning 92.7/76.2 93.2/83.6 96.3/59.0 95.1/83.3 96.5/78.—- w/o Instruction-tuned base model 90.9/66.3 88.6/67.7 93.2/41.0 89.7/57.5 90.3/40.Table 6: Results on each language for XOR-ATTRIQA.--- --tho bx gbm gom gu hne kn mall aml ar mw or paps Method Fl Fev shot ICL S618 430603 708 815 708 79 TS TD S71 TS OS Tid TAT 106 726 715 OA Fiat 759 739 47277 71 793 63 166 135 163 O11 754 733 760 756 166 T14 73d TAS WoCoT-Augmented Tuning 732-730 407 688 Tel 790 D3 M0 Ta 167 BS BI 23 Ne Me 2 749 130Wl lnstruction-Augmented Tuning 732 715 39.1 678 BI 185 703 140 712 147 501 739 714 109 722 BS 18 145w/o Parameter-efficient Tuning 107 695 49.2 65.7 805 674 699 697 709 516 70.0 678 668 695 69.7 687 70.9 698Wo nstruction-tuned base model 65.6 647 493 60.3 626 657 769 632 652 637 654 528 642 635 68 658 643 ORT O54 OTat 802 778 522 712 183 806 922 795 197 TSS TS GS TA 774 TA 807 80.0 B04 798=. wlo CoT-augmented Tuning 19.1 784 503 756 187 799 846 718 792 783 792 624 118 777 796 192 788 799 801= Wotnstrction-Augmented Tuning 188 776 47.7 75:1 161 701 $28 163 184 780 784 580 781 760 193 781 7710 782 780w/o Parameter-efficient Tuning 743 712 506 717 727 746 818 727 751 741 749 619 739 721 758 755 735 726 736Wo nstruction-tuned base model 688 G82 461 665 675 69.0 794 68 GOA G3 OIA SHS GRA 671 602 GRA 694 673 700Table 7: Results on each language for XTREME-UP Cross-lingual QA Indic. My Metiod FL Tr Resto BSG ROWS EE We AS WS EY BS BEA AS OST BT WS EE AS OT ROBE GS GS AE T_T Far Wi -Wd 326 T10 R9 WT GT MS HOE TO WS GO Ts RI WS Bi WI WS Ws RI BT we Bs OT ws we wd + Mcoraugmened tuning WS HS OL TY tes Bi M2 MS BO Tel BO S82 18 BY BSR? M1 BI ss Mi 5 a0 OI os ad MS st XS —_ Wolninatonaygnendtuning 188 128 #2 128 70 77 $39 108 ALI ToL god 383 187 o2 TE TO Joe M1 Da Wa MES Ste BLS SO 870 SLT= MePmmecratenTnng = 183 He $34 17 HO WO M9 tes BO Ta MS od TS TS m9 Ma Meo Teo WS TS M3 as a9 G2 Rl M0 OLS = _Molnictontunelbacmaiel 109 Tet 306 TS M2 108 84t TA 119 1S TBI 628 TS MS 149 M3 Mes 19 MS TS 824 862 S12 828 aRl StS Far SOS SIS TES HOS WEEN BIS WEL SIS TW] WOE 6 WIN BLS WEES OS SES wd WOT ATT WS we Flo Co augmented Tning Hs $9 To $00 se Go Md wat as M7 U9 S14 $02 820 HO SO sad 0 MD soe a9 O24 Bl S89 MLS ae Mis tnstaton amend rani 300 412 TS 4 420 47 $03 82 M3 S18 Gls S10 D7 82 MOS $09 808 Sle M4 2 HT 83 MUS Set St BLO = Wiener seen Toning Ms G13 M3 3 $01 AS BO M9 MO MS GS WO BE Ws MA MI WI MS BS aad GL sel eo 92 S87 eee = _Molnimctontnelbacmaiel 193 TA 554 he 11 9 88 TS 807 183 M3 684 OS THA TS Mo TT TS 807 THT 819 S88 SO 850 AS 919 Sat Table 8: Results on each language for XTREME-UP Cross-lingual QA All.