--- --arXiv:2307.13974v1 [cs.CV] 26 JulTracking Anything in High Quality Jiawen Zhu', Zhenyu Chen!, Zeqi Hao!, Shijie Chang!, Lu Zhang', Dong Wang!, Huchuan Lu!, Bin Luo’, Jun-Yan He?, Jin-Peng Lan’, Hanyuan Chen”, Chenyang Li? ‘Dalian University of Technology, China 2DAMO Academy, Alibaba Group {jiawen, dlutczy,hzq,csj}@mail.dlut.edu.cn, {luzhangdut, junyanhe1989, lanjinpengl015}@gmail.com {wdice, lnchuan}@dlut.edu.cn, {luwu.lb,hanyuan.chy, lee. lcy}@alibaba-inc.com Abstract Visual object tracking is a fundamental video task in computer vision. Recently, the notably increasing power of perception algorithms allows the unification of single/multiobject and box/mask-based tracking. Among them, the Segment Anything Model (SAM) attracts much attention. In this report, we propose HQTrack, a framework for High Quality Tracking anything in videos. HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). Given the object to be tracked in the initial frame of a video, VMOS propagates the object masks to the current frame. The mask results at this stage are not accurate enough since VMOS is trained on several closeset video object segmentation (VOS) datasets, which has limited ability to generalize to complex and corner scenes. To further improve the quality of tracking masks, a pretrained MR model is employed to refine the tracking results. As a compelling testament to the effectiveness of our paradigm, without employing any tricks such as test-time data augmentations and model ensemble, HQTrack ranks the 2"4 place in the Visual Object Tracking and Segmentation (VOTS2023) challenge. Code and models are available at https://github.com/jiawen-zhu/HQTrack. 1. Introduction As a fundamental video task in computer vision, visual object tracking has become the cornerstone of many related areas, such as robot vision and autonomous driving. The task aims at consistently locating the specified object in a video sequence. As one of the most influential challenges in the tracking field, Visual Object Tracking (VOT) challenge [16, 18] attracted a lot of attention, and many SOTA algorithms participate to show their cutting-edge performance. The Visual Object Tracking and Segmentation challenge (VOTS2023') relaxes the constraints enforced in 'https://www.votchallenge.net/votspast VOT challenges for considering general object tracking in a broader context. Therefore, VOTS2023 merges shortterm and long-term, single-target and multiple-target tracking with segmentation as the only target location specification. This poses more challenges, e.g., inter-object relationship understanding, multi-target trajectory tracking, accurate mask estimation, etc. Visual object tracking has made great strides with deep learning techniques [10, 13, 19]. Previous methods can be grouped into either online-update trackers [3,9] and Siamese trackers [2,31]. Recently, Transformer [29] sweeps in computer vision, the dominant tracking methods are Transformer-based trackers [5, 8,35, 39]. TransT [5] proposes transformer-based ECA and CFA modules to replace the long-used correlation calculation. Benefiting from Transformer’s superior long-range modeling capability, TransT outperforms the previous correlation modules which are a capable class of linear modeling. More recently, some trackers [8,39] introduce pure transformer architecture, and the feature extracting and template-search region interaction is completed in a single backbone, tracking performance is pushed to new records. These trackers mainly focus on single object tracking and output the bounding box for performance evaluation. Hence, merely employing SOT trackers is not well-suited to the VOTS2023 challenge. Video object segmentation aims to segment out the specific objects of interest in a video sequence. Similar to VOT, semi-supervised video object segmentation also manually provides the annotated in the first frame. The main difference is that the VOS task provides a more fine-grained mask annotation. Early VOS methods propagate object masks over video frames via motion clues [6, 28] or adopt online learning strategies [4,20]. Recently, Space-Temporal Memory (STM) network [24, 32] extracts the spatio-temporal context from a memory bank to handle the appearance changes and occlusions, offering a promising solution for semi-supervised video object segmentation. For multiobject segmentation, these methods segment the objects one by one, the final results are merged masks by post ensemble. --- --AOT [37] proposes an identification mechanism that can encode, match, and segment multiple objects at the same time. Based on AOT [37], DeAOT [38] decouples the hierarchical propagation of object-agnostic and object-specific embeddings from previous frames to the current frame, further improving the VOS accuracy. Although the above VOS methods can handle tracking task with multi-object and mask output, challenges in VOTS2023 benchmark remain. (i) VOTS videos contain a large number of long-term sequences, the longest of which exceeds 10,000 frames, which requires the tracker to be able to discriminate the drastic changes in object appearance and adapt to variations in the environment. At the same time, long-term video sequences also make some memory-based methods face memory bank space challenges. (ii) In VOTS videos, targets will leave the field of view and then returns. Trackers require additional design to accommodate the disappearance and appearance of targets. (iii) A series of challenges such as fast motion, frequent occlusion, distractors, and tiny objects also make this task more difficult. In this work, we propose Tracking Anything in High Quality (termed HQTrack), which mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). VMOS is an improved variant of DeAOT [38], we cascade a 1/8 scale gated propagation module (GPM) for perceiving small objects in complex scenarios. Besides, Intern-T [33] is employed as our feature extractor to enhance object discrimination capabilities. To save memory usage, a fixed length of long-term memory is used in VMOS, excluding the initial frame, the memory of early frames will be discarded. On the other hand, it should be beneficial to apply a large segmentation model to refine our tracking masks. SAM [15] is prone to failure when predicting objects with complex structures [14], and these difficult cases are common in VOTS chanllenge. To further improve the quality of tracking masks, a pre-trained HQ-SAM [14] model is employed to refine the tracking masks. We calculate the outer enclosing boxes of the predicted masks from VMOS as box prompts and feed them into HQ-SAM together with the original images to gain the refined masks, the final tracking results are selected from VMOS and MR. Finally, HQTrack obtains an impressive 0.615 quality score on the VOTS2023 test set, achieving runner-up at the VOTS2023 challenge. 2. Method In this section, we present our HQTrack in detail. We first showcase the pipeline of our method. Subsequently, we introduce each component in our framework. Finally, we describe the training and inference details. 2.1. Pipeline The pipeline of the proposed HQTrack is depicted in Figure |. Given a video and the first frame reference (mask annotated), HQTrack first segments the target objects for each frame via VMOS. The segmentation results of the current frame are from the propagation of the first frame along the temporal dimension, utilizing the modeling of appearance/identification information and long/short-term memory. VMOS is a variant of DeAOT [38] so that it can accomplish the modeling of multiple objects in a scene within a single propagation process. Furthermore, we employ HQSAM [14] as our MR to refine the segmentation masks of VMOS. HQ-SAM is a variant of SAM [15], it can handle objects with more complex structures than SAM. We first perform bounding box extraction on the target masks predicted by VMOS, and they are fed into the HQ-SAM model as box prompts. Last, we design a mask selector to select the final results from VMOS and MR. 2.2. Video Multi-object Segmenter (VMOS)) VMOS is a variant of DeAOT [38], thereby in this subsection, we first provide a brief revisiting of DeAOT which is the baseline of our VMOS, then we delve into the design of our VMOS. DeAOT. AOT [37] proposes to incorporate an identification mechanism to associate multiple objects in a unified embedding space which enables it to handle multiple objects in a single propagation. DeAOT is a video object segmentation model with a AOT-like hierarchical propagation. To alleviate the loss of object-agnostic visual information in deep propagation layers, DeAOT proposes to decouple the propagation of visual and identification embeddings into a dual-branch gated propagation module (GPM). GPM is an efficient module with single-head attention for constructing hierarchical propagation. VMOS. The video multiple object segmenter (VMOS) in HQtTrack is a variant of DeAOT. As shown in the left of Figure 1, to improve the segmentation performance, especially perceiving tiny objects, we cascade a GPM with 8x scale and expand the propagation process to multiple scales. The original DeAOT only performs propagating operation on the visual and identification features of 16x scale. At this scale, lots of detailed object clues are lost, especially for tiny objects, 16x scale features are insufficient for accurate video object segmentation. In our VMOS, considering the memory usage and model efficiency, we only use upsampling and linear projection to upscale the propagation features to 4x scale. Multi-scale propagation features will be fed into the decoder along with multi-scale encoder features for mask prediction. Decoder is a simple FPN [21]. In addition, as a new large-scale CNN-based foundation model, Internimage [33] employs deformable convolution as the core operator, showing impressive performance on --- --| oneuee ID Emb onneee ID Emb Figure 1. Overview of HQTrack. It mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). various representative tasks e.g., object detection and segmentation. In VMOS, Intern-T is employed as our encoder to enhance object discrimination capabilities. 2.3. Mask Refiner (MR) MR is a pre-trained HQ-SAM [14], in this section, we first revisit the HQ-SAM method which is a variant of SAM [15], then we provide the usage of HQ-SAM. SAM and HQ-SAM. Segment anything model (SAM) has recently attracted high-heat attention in the field of image segmentation, and researchers have utilized SAM to secondary a series of work (including but not limited to segmentation) with many stunning results. SAM scales up segmentation models by training with a high-quality annotated dataset containing 1.1 billion masks. In addition to the powerful zero-shot capabilities brought by large-scale training, SAM also involves flexible human interaction mechanisms achieved by different prompt formats. However, when the processed image contains objects with intricate structures, SAM’s prediction masks tend to fall short. To tackle such an issue as well as maintain SAM’s original promptable design, efficiency, and zero-shot generalizability, Ke et al. propose HQ-SAM [14]. HQ-SAM introduces a few additional parameters to the pre-trained SAM model. High-quality mask is obtained by injecting a learning output token into SAMs mask decoder. MR. HQTrack employs the above HQ-SAM as our mask refiner. As shown in the right of Figure 1, we take the prediction mask from VMOS as the input of MR. Since the VMOS model is trained on scale-limited close-set datasets, the first stage mask from VMOS probably with insufficient quality especially handling some complex scenarios. Hence, employing a large-scale trained segmentation algorithm to refine the primary segmentation results will bring considerable performance improvement. Specifically, we calculate the outer enclosing boxes of the predicted mask from VMOS as the box prompts and feed them into HQ-SAM together with the original image to obtain the refined masks. HQ-SAM here is a version with a ViT-H backbone. Finally, the output mask of HQTrack is selected from the mask results from VMOS and HQ-SAM. Specifically, we find that for the same target object, the mask refined by HQ-SAM is sometimes completely different from the predicted mask of VMOS (very low IoU score) which instead harms the segmentation performance. This may be a result of the different understanding and definition of object between HQ-SAM and reference annotation. Hence, we set an IoU threshold r (between masks from VMOS and HQ-SAM) to determine which mask will be used as the final output. In our case, when the IoU score is higher than 7, we choose the refined mask. This process constrains HQ-SAM to focus on refining the current object mask rather than re-predicting another target object. 3. Implementation Details In VMOS of HQTrack, InternImage-T [33] is employed as the backbone for the image encoder for the trade-off between accuracy and efficiency. The layers number of the GMP for 16x and 8x scale is set to 3 and 1. The 4x scale --- --Method AUC A R NRE| DRE| ADQ G= AUC A R NRE| DRE! ADQ MS_AOT (Separate) MS_AOT (Joint) 0.552 0.625 0.831 0.063 0.106 0.0.566 0.645 0.782 0.097 0.121 0.Table 1. Ablation study of separate tracking v.s. joint tracking paradigm on VOTS2023 validation set. The metrics marked with { indicate that smaller is better and vice versa. NRE: Not-Reported Error. DRE: Drift-Rate Error. ADQ: Absence-Detection Quality. We refer readers to [17] for more details about evaluation metrics. # |Method AUC A R NRE! DRE| ADQ 1 |Baseline 0.576 0.675 0.77 0.122 0.108 0.2 |w/ InternImage-T 0.611 0.656 0.809 0.137 0.054 0.VMOS 0.650 0.681 0.886 0.059 0.055 0.Table 2. Ablation study of components of VMOS on VOTSvalidation set. We train a DeAOT [38] as the baseline method. propagation features are up-sampled and projection features from 8x scale. The long and short-term memory is used in our segmenter to deal with object appearance changes in long-term video sequences. To save memory usage, we use a fixed length of long-term memory of 8, excluding the initial frame, the early memory will be discarded. Model Training. The training process comprises two stages, following previous methods [37,38]. In the first phase, we pre-train VMOS on synthetic video sequences generated from static image datasets [7, 11, 12, 22,27]. In the second stage, VMOS uses multi-object segmentation datasets for training for a better understanding of the relationship between multiple objects. The training splits of DAVIS [25], YoutubeVOS [34], VIPSeg [23], BURST [1], MOTS [30], and OVIS [26] are chosen for training our VMOS, in which OVIS is employed to improve the robustness of the tracker in handling occluded objects. We useNVIDIA Tesla A100 GPUs with a global batch size ofto train our VMOS. The pre-training stage uses an initial learning rate of 4 x 10~4 for 100,000 steps. The second stage uses an initial learning rate of 2 x 10~4 for 150,steps. Learning rates gradually decay to 1 x 10~° ina polynomial manner [36]. Inference. The inference process is as described in our pipeline. We do not use any test time augmentation (TTA) such as flipping, multi-scale testing, and model ensemble. 4. Experiment 4.1. Ablation Study Separate tracking v.s. Joint tracking. We conduct ablation studies on different tracking paradigms. Separate tracking means initializing a separate tracker for each target object, and running multiple times of inference for multiple object tracking. Joint tracking means joint tracking all tar 10 0.610 0.668 0.807 0.110 0.083 0.20 0.607 0.65 0.806 0.12 0.074 0.30 0.626 0.689 0.813 0.127 0.060 0.40 0.650 0.681 0.886 0.059 0.055 0.50 0.669 0.692 0.885 0.057 0.058 0.60 0.653 0.669 0.889 0.059 0.052 0.70 0.656 0.688 0.865 0.052 0.082 0.Table 3. Ablation study of long-term memory gap (G) on VOTS2023 validation set. get objects with a single tracker. We choose MS_AOT [16] (removing Mixformer [8]) as the baseline. The results on VOTS2023 validation set are shown in Tabled 1. We can see that joint tracking shows better performance than separate tracking. It may be that when joint tracking, the tracker will have a better understanding of the relationship between the target objects which makes the tracker obtain better robustness to distractor interference. Component-Wise Analysis on VMOS. Table 2 shows the component-wise study results on VMOS. #1 is a trained baseline method DeAOT [38]. In #2, we replace the original ResNet50 [13] backbone with InternImage-T [33], and the AUC score increases to 0.611. Then, as reported in #3, we add the multi-scale propagation mechanism as described in Section 2.2, the performance boosts to 0.650 in terms of AUC score, with a remarkable improvement of 3.9%, which demonstrates the effectiveness. Long-term Memory Gap. Since the VOTS video sequences tend to be long (the longest exceeds 10,frames), the original long-term memory gap parameter on test time for the VOS benchmark is less suitable. Therefore, we do an ablution study on long-term memory gap (G) parameter as shown in Table 3. We find that a memory gap of 50 shows the best performance. Analysis on Mask Refiner (MR). As we discuss in Section 2.3, directly refining all the segmentation masks is not optimal. We provide a comparison between VMOS and VMOS + SAM in Figure 3. In VMOS + SAM case, a SAM-h [15] is employed to refine all the object masks from VMOS. We can see that refining by SAM can bring significant improvement. However, for these masks with low quality (with low IoU score on ground truth), SAM harms the performance instead. Therefore, we propose to select mask results from VMOS and SAM. We calculate the IloU score between the masks from VMOS and SAM. When the IoU score is higher than 7, we choose the refined mask as the final output. We evaluate the influence of threshold 7 in MR on the VOTS2023 validation set, the results are shown in Table 4. t = 0.1 yields the most promising results and we choose this setting in HQTrack. --- --Figure 2. Qualitative results of HQTrack on videos from VOTS2023 test set. t= AUC A R NREL DRE| ADQ 0 0.702 0.756 0.866 0.072 0.062 0.0.1 0.708 0.753 0.878 0.072 0.050 0.0.2 0.707 0.753. 0.878 = 0.072 Ss «0.050 ~—(0.0.3 0.704 0.750 0.878 0.072 0.050 0.04 0.701 0.745 0.878 0.072 0.050 0.0.5 0.695 0.739 0.878 0.072 0.050 0.Table 4. Tracking performance with different threshold 7 on VOTS2023 validation set. Mask refiner (MR) is a SAM_H model. 4.2. Challenge Results The results on VOTS2023 test set are shown in Table After replacing the VMOS encoder from ResNet50 [13] to InternImage-T [33], the AUC score increased by 3.2%. When using SAM_H to refine the masks of VMOS, the Method AUC A R- NRE| DRE| ADQ VMOS (Res50) 0.564 0.693 0.759 0.155 0.086 0.VMOS 0.596 0.724 0.765 0.159 0.075 0.VMOS +SAM_H_ 0.610 0.751 0.757 0.159 0.084 0.HQTrack 0.615 0.752 0.766 0.155 0.079 0.Table 5. Performance on VOTS2023 test set. performance in terms of AUC increased by 1.4%. After employing HQ-SAM_H as our mask refine module, the AUC score boosts to 0.615, which outperforms VMOS by 0.9%. Figure 4 provides the quality plot comparison between VMOS and HQtrack. As we can see and compare with Figure 3, selectively taking the processed results of the MR can effectively avoid performance degradation from low IoU objects. Finally, HQTrack ranks 2nd place~ in the‘VOTS2023 benchmark is open for allowing post-challenge submissions. --- --Quality plot - Plot VMOS+SAM, 7=0.0 oz os 06 08Threshold Figure 3. VMOS v.s. VMOS + SAM on VOST2023 validation set. SAM is employed to refine all the masks from VMOS. Visual Object Tracking and Segmentation Challenge. 4.3. Visualization Figure 2 provides some representative visual results on challenging video sequences. As shown, HQTrack demonstrates strong tracking capabilities. It can stably handle long-term object tracking scenarios, tracking multiple objects at the same time, and capturing target objects accurately even if there are a lot of distractors. With the help of HQ-SAM, accurate masks can also be segmented when facing challenges such as object appearance changes, fast motion, and scale changes. 5. Conclusion In this report, we propose Tracking Anything in High Quality (HQTrack). HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask refiner (MR). VMOS is responsible for propagating multiple targets in video frames, and MR is a large-scale pre-trained segmentation model in charge of refining the segmentation masks. HQTrack demonstrates powerful object tracking and segmentation capabilities. Finally, HQTrack achieves the 2nd place in the Visual Object Tracking and Segmentation (VOTS2023) challenge. References [1] Ali Athar, Jonathon Luiten, Paul Voigtlaender, Tarasha Khurana, Achal Dave, Bastian Leibe, and Deva Ramanan. Burst: A benchmark for unifying object recognition, segmentation and tracking in video. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1674-1683, 2023.[2] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip H S Torr. Fully-convolutional siamese networks for object tracking. In ECCVW, 2016. | [3] Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning discriminative model prediction for tracking. In JCCV, 2019.[Quality plot - Plot Lo Success 00 02 oa 06 osThreshold Figure 4. VMOS v.s. HQTrack on VOST2023 test set. Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixé, Daniel Cremers, and Luc Van Gool. Oneshot video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 221-230, 2017.Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. In CVPR, 2021.Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and MingHsuan Yang. Segflow: Joint learning for video object segmentation and optical flow. In Proceedings of the IEEE international conference on computer vision, pages 686-695, 2017.Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu. Global contrast based salient region detection. IEEE transactions on pattern analysis and machine intelligence, 37(3):569-582, 2014.Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu. Mixformer: End-to-end tracking with iterative mixed attention. In CVPR, pages 13608-13618, 2022. 1,Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. ECO: Efficient convolution operators for tracking. In CVPR, 2017.Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88:303-338, 2010.Bharath Hariharan, Pablo Arbeléez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In 2011 international conference on computer vision, pages 991-998. IEEE, 2011.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 1,4,Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. arXiv preprint arXiv:2306.01567, 2023. 2,--- ---Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 2, 3,Matej Kristan, Ales Leonardis, Jifi Matas, Michael Felsberg, Roman Pflugfelder, Joni-Kristian Kamirainen, Hyung Jin Chang, Martin Danelljan, Luka Cehovin Zajc, Alan LukeZié, Ondrej Drbohlav, et al. The tenth visual object tracking vot2022 challenge results, 2022. 1,Matej Kristan, Jir1 Matas, Martin Danelljan, Luka Cehovin Zajc, and Alan Lukezic. The vots2023 challenge performance measures.Matej Kristan, Jiff Matas, Ale’ Leonardis, Michael Felsberg, Roman Pflugfelder, Joni-Kristian Kamirainen, Hyung Jin Chang, Martin Danelljan, Luka Cehovin, Alan LukeZié, et al. The ninth visual object tracking vot2021 challenge results. In ICCVW, pages 2711-2738, 2021.Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.Xiaoxiao Li and Chen Change Loy. Video object segmentation with joint re-identification and attention-aware mask propagation. In Proceedings of the European conference on computer vision (ECCV), pages 90-105, 2018.Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, pages 2117-2125, 2017.Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision—ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740-755. Springer, 2014.Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video panoptic segmentation in the wild: A benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21033-21043, 2022.Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9226-9235, 2019.Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeldez, Alex Sorkine-Hornung, and Luc Van Gool. Thedavis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017.Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang Bai, Serge Belongie, Alan Yuille, Philip HS Torr, and Song Bai. Occluded video instance segmentation: A benchmark. International Journal of Computer Vision, 130(8):2022-2039, 2022.Jianping Shi, Qiong Yan, Li Xu, and Jiaya Jia. chical image saliency detection on extended cssd. TPAMI, 38(4):717-729, 2015.Yi-Hsuan Tsai, Ming-Hsuan Yang, and Michael J Black. Video segmentation via object flow. In Proceedings of the Hierar-IEEE conference on computer vision and pattern recognition, pages 3899-3908, 2016.Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NJPS, 2017.Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, and Bastian Leibe. Mots: Multi-object tracking and segmentation. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 7942-7951, 2019.Paul Voigtlaender, Jonathon Luiten, Philip H. S. Torr, and Bastian Leibe. Siam R-CNN: Visual tracking by redetection. In CVPR, 2020.Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient objects with image-level supervision. In CVPR, pages 136-145, 2017.Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14408-14419, 2023. 2, 3,4,Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: A large-scale video object segmentation benchmark. arXiv preprint arXiv: 1809.03327, 2018.Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer for visual tracking. In JCCV, 2021.Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative video object segmentation by foreground-background integration. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part V, pages 332-348. Springer, 2020.Zongxin Yang, Yunchao Wei, and Yi Yang. Associating objects with transformers for video object segmentation. Advances in Neural Information Processing Systems, 34:2491— 2502, 2021. 2,Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. Advances in Neural Information Processing Systems, 35:36324—36336, 2022. 2,Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Joint feature learning and relation modeling for tracking: A one-stream framework. In ECCV, pages 341357. Springer, 2022.