--- --2305.07804v4 [cs.CL] 1 AugarXiv Improving Small Language Models on PubMedQA via Generative Data Augmentation Zhen Guo!*, Yanwei Wang!, Peiqi Wang! and Shangdi Yu! 'MIT Department of Electrical Engineering and Computer Science, 77 Massachusetts Ave, Cambridge, MAAbstract Large Language Models (LLMs) have made remarkable advancements in the field of natural language processing. However, their increasing size poses challenges in terms of computational cost. On the other hand, Small Language Models (SLMs) are known for their efficiency, but they often struggle with limited capacity and training data, especially in specific domains. In this paper, we introduce a novel method aimed at improving SLMs in the medical domain using LLM-based generative data augmentation. The objective of our approach is to develop more efficient and capable models that are specifically tailored for specialized applications. Through experiments conducted on the PubMedQA dataset, we demonstrate the effectiveness of LLMs in refining and diversifying existing question-answer pairs. This refinement process leads to improved performance in a significantly smaller model after fine-tuning. Notably, our best SLM, with under 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA dataset. Our code and generated data are publicly available to facilitate further explorations [1]. Keywords large language models, small language models, medical question-answering, data augmentation 1. Introduction In recent years, Large Language Models (LLMs) have transformed the field of natural language processing, demonstrating exceptional performance in a wide range of tasks. These models, powered by massive amounts of data and extensive pre-training [2], have advanced the state-of-the-art in various applications such as machine translation, program synthesis, and questionanswering [3, 4, 5]. Although LLMs have impressive capabilities, their growing size presents challenges in terms of computational efficiency, particularly for real-world applications and domain-specific tasks [6, 7]. Problems such as medical question-answering or legal document analysis require specialized knowledge that may not be fully captured by general-purpose LLMs [8, 9]. Small Language Models (SLMs), on the other hand, offer a more computationally efficient alternative to LLMs. However, SLMs often struggle in domain-specific tasks due to their limited capacity and training data. This limitation requires the development of new strategies to enhance the performance of SLMs in specialized tasks while maintaining their computational efficiency [10, 11]. In this paper, we introduce a novel method that im LLM4AI‘23: Workshop on Foundations and Applications in Large-scale AI Models -Pre-training, Fine-tuning, and Prompt-based Learning, colocated with the 29TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD), August 6-10, 2023, Long Beach, CA, USA *Corresponding author. Q zgu00525@mit.edu (Z. Guo); yanwei@mit.edu (Y. Wang); wpq@mit.edu (P. Wang); shangdiy@mit.edu (S. Yu) SDI) © 222 Copyright for this paper by is authors, Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). CEUR Workshop Proceedings (CEUR-WS.org) proves SLMs in the medical domain through LLM-based generative data augmentation. The objective is to develop more efficient and capable models tailored for specialized medical applications without using billions of parameters. Our results in the PubMedQA dataset [12] demonstrate the effectiveness of LLM in the refinement and diversification of question-answer pairs, leading to improved performance of a significantly smaller model after fine-tuning. The best SLM, with less than 1.6 billion parameters, outperforms the few-shot GPT-4 on PubMedQA, as shown in Table 1. In general, our method holds promise for enhancing SLMs for medical tasks, bridging the gap between computational efficiency and model performance in specialized domains. TableChatGPT vs. BioGPT with fine-tuning on PubMedQA Model Accuracy Macro-FGPT-3.5-turbo (175B) 0.372 0.GPT-4 (0-shot) [13] 0.752 NA GPT-4 (5-shot) [13] 0.744 NA Best BioGPT (1.6B) 0.754 0.Human Performance [12] 0.780 0.2. Technical background 2.1. Efficient fine-tuning Fine-tuning LLMs for specific tasks poses computational and time-related challenges [14, 15]. To address these issues, researchers have developed efficient fine-tuning --- --techniques, such as Prefix Tuning and Low-rank Adaptation, as alternatives to traditional fine-tuning methods that update the model’s weights entirely. Prefix tuning [16] adapts the behavior of a language model to specific tasks without modifying its pre-trained weights. While the low-rank adaptation [17] allows the model to capture the essential characteristics of the data and adapt to domain-specific tasks effectively by decomposing the weight matrices into smaller matrices. 2.2. Data Augmentation using LLMs LLMs serve as powerful tools to generate realistic text samples based on existing data. For NLP tasks, generating data with LLM can involve paraphrasing text, creating alternative question-answer pairs, or generating new sentences [18]. Producing diverse representations of input data enables models to learn various ways to express the same underlying concepts, increasing their adaptability to real-world data variations. For our preliminary study on the PubMedQA dataset, we used GPT-3.5 Turbo and GPT-4 to either rewrite existing medical question-answering pairs or generate new pairs from the training dataset (with a size of 450) with zero-shot prompting. This approach helped improve the diversity and coverage of the training data, ultimately improving the performance of the medical question-answering model trained on the augmented dataset. 3. Experimental settings We performed experiments on the MIT Supercloud [19], using PyTorch 1.12 and Python 3.8 with eight NVIDIA V100 GPUs and Intel Xeon Gold 6248 processors. We investigated the effectiveness of prefix tuning and lowrank adaptation on BioGPT-Large [20], LLaMA-7b [21], and Alpaca-7b [22] for medical question-answering tasks. The evaluation was carried out on the PubMedQA dataset [12], splitting it into 450 training, 50 validation, and 500 test samples. Accuracy and F1 score were calculated based on a hard match between predicted and ground truth answers. For prefix tuning, we follow the original implementation [16] and explored a token range of 16 to 512, while low-rank adaptation varied alpha from 16 to 512 with a fixed rank of 4. Fine-tuning employed a learning rate of 5e-5, AdamW optimizer [23], linear warm-up scheduler [24], gradient accumulation of 32 steps [25], and a batch size of 1024 tokens. During inference, we applied techniques including Repetition Penalty Logits Processor (penalty factor of 2.0), Temperature Logits Warper (temperature of 0.8), and beam search decoding with a beam size of 5 to ensure output quality. 4. Results 4.1. Low-rank Adaptation outperforms Prefix Tuning accuracy fl score 9 low-rank adaptation e prefix tuning é Fry 380 30 “400fine-tuning hyperparameter Figure 1: Comparison between two fine-tuning techniques for BioGPT-Large. We compared the performance of two techniques, Low-rank Adaptation and Prefix Tuning, for BioGPTLarge (Figure 1). We observed that Low-rank Adaptation demonstrated stability across with different hyperparameters (16 to 512), while Prefix Tuning showed sensitivity to the virtual token range. This finding suggests that Low-rank adaptation is more robust and less sensitive to hyperparameter selection, providing consistent and reliable performance for efficient fine-tuning. For all the results below, Low-rank adaptation is the default finetuning technique. 4.2. Instruction-tuning constrains domain adaptability of language models In Table 2, we present a comparison of BioGPT-Large, LLaMA-7b, and Alpaca-7b, all fine-tuned on the original PubMedQA dataset without data augmentation as the baseline. Alpaca-7b, a derivative of LLaMA-7b, is an instruction-tuned LLM designed to improve task-specific performance by following instructions. However, this approach restricts its adaptability to other domain-specific tasks compared to a naive pre-trained model. In our experiments, LLaMA-7b shows superior generalizability compared to BioGPT-Large by exhibiting a higher Fscore, when fine-tuned only on the original PubMedQA dataset. The reported accuracy for BioGPT-Large is lower than the numbers reported by Luo et al. [20] because we have different fine-tuning settings. While Luo et al. [20] inserted the virtual tokens right before the answer token, we inserted the virtual tokens before the question token to avoid the risk of overfitting. --- --TableComparison of BioGPT-Large, LLaMA-7b and Alpaca-7b finetuned on the original PubMedQA dataset. Model Accuracy Macro-FBioGPT-Large 0.630 0.LLaMA-7b 0.594 0.Alpaca-7b 0.380 0.TableComparison of LLaMA-7b and BioGPT-Large fine-tuned on augmented PubMedQA (Acc. stands for accuracy, GPT-3.refers to GPT-3.5-turbo, BioGPT represents BioGPT-Large, and F1 denotes marco F1 Score). SLM LLM Augment. Acc. (best) FI (best) none 0.594 0.rewriteQA 0.642 0.CPT-3.5 newQA 0.552 0.LLaMA combinedQA 0.582 0.rewriteQA 0.540 0.GPT-4 newQA 0.576 0.combinedQA 0.506 0.none 0.630 0.rewriteQA 0.720 0.CPT-3.5 newQA 0.718 0.BioGPT combinedQA 0.714 0.rewriteQA 0.654 0.GPT-4 newQA 0.754 0.combinedQA 0.708 0.4.3. Comparison between generative data augmentation approaches In Table 3, we provide a comparison of LLaMA-7b and BioGPT-Large fine-tuned on the augmented PubMedQA dataset. Our experiments demonstrate the efficacy of utilizing LLMs such as ChatGPT for refining and expanding question-answer pairs to enhance domain-specific QA datasets, even when the LLM exhibits near-random performance in generating answers (as for the case for gpt-3.5-turbo). The resulting alternative representations of questions and answers facilitated the construction of more diverse and robust training datasets suitable for SLMs. However, we found that instructing an LLM (gpt-3.5turbo) lacking domain knowledge to generate entirely new question-answer pairs did not lead to an improvement and resulted in a degradation of the downstream task performance for the fine-tuned SLM. This observation suggests that while LLMs are effective in refining and diversifying existing question-answer pairs, their ability to create novel, high-quality pairs for domain-specific tasks remains limited. On the other hand, recent advances in LLMs such as GPT-4, which have domain-specific knowledge and question-answering capacity for PubMedQA, can gen erate useful new training data. By incorporating new question-answer pairs from GPT-4 into the training process, we can significantly improve the performance of the fine-tuned smaller models. This finding highlight the importance of LLMs with domain-specific knowledge in enhancing domain-specific QA datasets and improving the performance of downstream tasks. Finally, not surprisingly, when BioGPT is fine-tuned on an augmented dataset, it outperforms LLaMA-7B. This is consistent with the previous finding [20], and highlights the effectiveness of pretraining with domain-specific data, enabling BioGPT to better understand and excel in domain-specific tasks. Leveraging domain-specific knowledge during fine-tuning improves the model’s accuracy and contextual relevance, resulting in superior performance for domain-specific questions or tasks. 5. Future works A promising direction for future work is to investigate the application of knowledge distillation, a popular technique that trains a smaller language model to mimic the behavior of a larger language model on medical questionanswering tasks. Another potential approach is through contrastive learning. By training an SLM using contrastive learning on medical question-answering data, contrastive loss can help the model learn to identify similarities and differences between different instances of data and improve its ability to generalize to new and unseen data. 6. Conclusion Our research highlights the effectiveness of LLM-based generative data augmentation in enhancing domainspecific question answering datasets. However, instructing LLMs without domain knowledge, such as GPT-3.5turbo, to generate new question-answer pairs resulted in decreased performance for fine-tuned smaller models. Conversely, leveraging LLMs with domain-specific knowledge, like GPT-4, significantly improved the performance of fine-tuned models by generating valuable new training data. These findings underscore the importance of incorporating domain-specific knowledge when applying generative data augmentation techniques. Acknowledgments We thank Prof. Yoon Kim at MIT CSAIL for his guidance and feedback. The authors acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing computing resources. We would also like to acknowledge OpenAI for providing access to their API. --- --References[Z. Guo, P. Wang, Y. Wang, S. Yu, Dr. LLaMA: Improving small language models in domain-specific qa via generative data augmentation, 2023. URL: https://github.com/zguo0525/Dr.LLaMA. J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, et al., Training compute-optimal large language models, arXiv preprint arXiv:2203.15556 (2022). W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al, A survey of large language models, arXiv preprint arXiv:2303.18223 (2023). J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large language models, arXiv preprint arXiv:2108.(2021). M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al., Evaluating large language models trained on code, arXiv preprint arXiv:2107.(2021). E. Strubell, A. Ganesh, A. McCallum, Energy and policy considerations for deep learning in nlp, arXiv preprint arXiv:1906.02243 (2019). Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, H. Poon, Domainspecific language model pretraining for biomedical natural language processing, ACM Transactions on Computing for Healthcare (HEALTH) 3 (2021) 1-23. O. Ram, Y. Kirstain, J. Berant, A. Globerson, O. Levy, Few-shot question answering by pretraining span selection, arXiv preprint arXiv:2101.00438 (2021). Z. Sun, A short survey of viewing large language models in legal aspect, arXiv preprint arXiv:2303.09136 (2023). N. Poerner, U. Waltinger, H. Schiitze, Inexpensive domain adaptation of pretrained language models: Case studies on biomedical ner and covid-19 qa, arXiv preprint arXiv:2004.03354 (2020). F.N. Iandola, A. E. Shaw, R. Krishna, K. W. Keutzer, Squeezebert: What can computer vision teach nlp about efficient neural networks?, arXiv preprint arXiv:2006.11316 (2020). Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, X. Lu, Pubmedga: A dataset for biomedical research question answering, arXiv preprint arXiv:1909.06146 (2019). H. Nori, N. King, S. M. McKinney, D. Carignan, E. Horvitz, Capabilities of gpt-4 on medical challenge problems, arXiv preprint arXiv:2303.(2023).15,20.H. Liu, D. Tam, M. Mugeeth, J. Mohta, T. Huang, M. Bansal, C. A. Raffel, Few-shot parameterefficient fine-tuning is better and cheaper than incontext learning, Advances in Neural Information Processing Systems 35 (2022) 1950-1965. D. Vos, T. Déhmen, S. Schelter, Towards parameterefficient automation of data wrangling tasks with prefix-tuning, in: NeurIPS 2022 First Table Representation Workshop, 2022. X. L. Li, P. Liang, Prefix-tuning: Optimizing continuous prompts for generation, arXiv preprint arXiv:2101.00190 (2021). E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685 (2021). A. Edwards, A. Ushio, J. Camacho-Collados, H. de Ribaupierre, A. Preece, Guiding generative language models for data augmentation in few-shot text classification, arXiv preprint arXiv:2111.(2021). A. Reuther, J. Kepner, C. Byun, S. Samsi, W. Arcand, D. Bestor, B. Bergeron, V. Gadepally, M. Houle, M. Hubbell, M. Jones, A. Klein, L. Milechin, J. Mullen, A. Prout, A. Rosa, C. Yee, P. Michaleas, Interactive supercomputing on 40,000 cores for machine learning and data analysis, in: 2018 IEEE High Performance Extreme Computing Conference (HPEC), IEEE, 2018, pp. 1-6. R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, T.-Y. Liu, Biogpt: generative pre-trained transformer for biomedical text generation and mining, Briefings in Bioinformatics 23 (2022). H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziére, N. Goyal, E. Hambro, F. Azhar, et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971 (2023). R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, T. B. Hashimoto, Stanford alpaca: An instruction-following llama model, https: //github.com/tatsu-lab/stanford_alpaca, 2023. I. Loshchilov, F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.(2017). A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, EL. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information processing systems 30 (2017). Y. Lin, S. Han, H. Mao, Y. Wang, W. J. Dally, Deep gradient compression: Reducing the communication bandwidth for distributed training, arXiv preprint arXiv:1712.01887 (2017).