arXiv:2307.02499v1 [cs.CL] 4 JulmPLUG-DocOwl : Modularized Multimodal Large Language Model for Document Understanding Jiabo Ye*, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang DAMO Academy, Alibaba Group {yejiabo.yjb, huanwen.haw, shuofeng.xhy, yeqinghao.yqh, ym119608}@alibaba-inc.com Abstract Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models' capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl. 1 Introduction Large language models (LLMs) like ChatGPT [OpenAI, 2022], BLOOM [Scao et al., 2022], and LLAMA [Touvron et al., 2023] have undergone rapid development to enable the realization of general artificial intelligence, boasting impressive zero-shot capabilities across diverse linguistic applications. With the LLM as the language decoder, Multimodal large language models (MLLMs) such as MiniGPT-4 [Zhu et al., 2023], LLAVA [Liu et al., 2023a], and mPLUG-Owl [Ye et al., 2023] have demonstrated remarkable zero-shot performance in various open-ended vision-and-language tasks. These models are trained to align text and images during the pre-training phase, and then to promote diverse abilities during the instruction tuning phase. Interestingly, these MLLMs exhibit superficial OCR-free text recognition abilities without explicit training on visual text understanding datasets [Ye et al., 2023, Liu et al., 2023b]. Nevertheless, due to lacking specific training, these models still face the challenge of comprehending intricate relationships between visual text and objects in diverse types of images, such as charts, documents and webpages. *Equal contribution *Corresponding author Preprint. Under review. By performing unified instruction tuning for Document Understanding upon the mPLUG-Owl [Ye et al., 2023], we further propose a modularized MLLM [Li et al., 2022, Xu et al., 2023b], namely mPLUG-DocOwl. Our approach utilizes a modularized framework similar to mPLUG-Owl [Ye et al., 2023], which incorporates a visual abstractor module to link a pre-trained LLM with a visual knowledge module, achieving the alignment of text and images. To enhance diverse document understanding capabilities, we reorganize various downstream document understanding tasks in the same form of instructions. To maintain general uni/multi-modal abilities, we also include language-only and general vision-and-language instruction datasets used by mPLUG-Owl to train the mPLUG-DocOwl. During training, both the visual knowledge module and LLM decoder are frozen, only the visual abstractor and the Low-Rank Adaption (LoRA) [Hu et al., 2022] in LLM are fine-tuned. mPLUG-DocOwl achieves ocr-free state-of-the-art performance on multiple commonly used document understanding datasets. Furthermore, our experiments on a carefully-built document instruction understanding evaluation set LLMDoc shows that mPLUG-DocOwl achieves significantly better visual text understanding performance on various domains than existing MLMMS. Our main contributions can be highlighted as follows: • We propose a modularized MLLM, mPLUG-DocOwl, which is the first one to balance language-only, general vision-and-language, and document understanding based on unified instruction tuning. •⚫ We carefully construct an instruction understanding test set with human evaluation, dubbed LLMDoc, to assess diverse document understanding capabilities. • Empirical results demonstrate that our mPLUG-DocOwl surpasses existing methods on ocr-free document understanding, including multiple standard benchmarks and LLMDoc. 2 Related Work 2.Visual Text Understanding There are two types of models for understanding images that contain rich textual information. The first kind of approaches [Xu et al., 2020, Huang et al., 2022, Hu et al., 2021, Tang et al., 2023, Yang et al., 2021] utilize off-the-shelf OCR models or APIs to recognize text from images, and then design pretraining tasks to facilitate cross-modality alignment between visual and textual inputs. On the other hand, end-to-end approaches [Davis et al., 2022, Kim et al., 2022, Lee et al., 2022] utilize a high-resolution image encoder to learn text recognition during the pretraining stage. Both two types of models rely on specific finetuning on different downstream datasets and can't achieve open-domain instruction understanding performance like Multimodal Large Language Models. 2.2 Multimodal Large Language Model Large Language Models (LLMs) have demonstrated impressive zero-shot abilities across various open-ended tasks. Recent research has also explored the application of LLMs for multi-modal generation, utilizing two different paradigms: systematic collaboration and end-to-end trained models. Systematic collaboration approaches, such as Visual ChatGPT [Wu et al., 2023] and MM-REACT [Yang et al., 2023], leverage various vision experts or tools to express visual information with text descriptions. Subsequently, LLMs, such as ChatGPT [OpenAI, 2022], can act as agents and select appropriate experts and tools for visual understanding. Finally, LLMs would summarize the output of these experts to answer user queries. On the other hand, some approaches, such as MiniGPT-[Zhu et al., 2023], LLAVA [Liu et al., 2023a], and mPLUG-Owl [Ye et al., 2023], leverage LLMs to build unified models for multi-modality with limited connected parameters. These methods show superficial OCR-free text recognition abilities under the zero-shot setting. However, for complicated document understanding, due to lacking in-domain training, they encounter challenges in handling diverse image types, recognizing rich texts and comprehending relationships between visual semantic and text information. In this work, through unified instruction tuning, mPLUG-DocOwl achieves much better document understanding performance and maintains general uni/multi-modal abilities.Natural Image Document Chart CALEA DOROBANTILOR Human: What was the busiest control point in Hong Kong in 2019? Al: Lo Wu Control Point Human: what is the value mentioned in the CID#: AI: 736 00Human Create a compact narrative representing the image presented. AI: Sign that says Calea Dorobantilor in blue and white Multimodal Pretraining mPLUG-Owl Unified Instruction Tuning Chart Document Table Natural Image WebPage General V&L Language-only mPLUG-DocOwl Table Human: juan román riquelme have the same score on july 11,november 17 2007, Yes or No? Al: Yes WebPage Human: What will the presentation aim to encourage a discussion about AI: It will aim to encourage a discussion about open data ownership rights and the differing social Language-only Human: Could you offer me an elaborated response that demands complex reasoning and requires breaking down the problem into multiple steps AI: Yes, I can certainly fulfill your request. Please provide me with your specific question or problem that you would like me to elaborate on. Figure 1: The summary of the instruction tuning paradigm of our mPLUG-DocOwl. 3 mPLUG-DocOwl 3.1 Architecture The architecture of mPLUG-DocOwl is based on a popular multi-modal language model, mPLUGOwl [Ye et al., 2023], which comprises a pre-trained visual foundation model, a visual abstractor, and a language foundation model. The visual foundation model is responsible for extracting visual features from the input images, and the visual abstractor distills these features using a set of learnable tokens. The resulting visual features are then concatenated with the word embeddings of the input sentence and fed into the language model to generate the response. This powerful architecture allows for accurate and efficient multi-modal language processing. The mPLUG-Owl [Ye et al., 2023] exhibits superficial OCR ability when presented with images containing salient text. Inspired by this, we propose to further fine-tune the model with document instruction tuning data for better document understanding performance, covering document, table, chart and natural image and webpage. During fine-tuning, we freeze the visual encoder and the language model and train the visual abstractor. We also adopt the low-rank adaptation approach (LORA) [Hu et al., 2022] to enhance the language model's ability. 3.2 Instruction Tuning Data This section introduces the composition of our instruction tuning data in detail. To ensure the versatility of mPLUG-DocOwl, we collect diverse document understanding datasets with different task formats, including Visual Question Answering (VQA) [Antol et al., 2015], Information Extraction (IE), Natural Language Inference (NLI) [Bowman et al., 2015], and Image Captioning (IC). mPLUG-Owl [Ye et al., 2023] performs instruction tuning with a unified format as "<image>Human: {question} AI:{answer}". In this work, we convert different document understanding tasks to the same format as mPLUG-Owl [Ye et al., 2023] by replacing the {question} and {answer} placeholders as follows. Visual Question Answering We simply use the raw question and answer as the {question} and {answer} placeholders. We collect VQA datasets on diverse domains, including ChartQA [Masry et al., 2022], DocVQA [Mathew et al., 2021], InfographicsVQA (InfoVQA) [Mathew et al., 2022],Document Table DocVQA WTQ Chart ChartQA General V&L LLAVA TabFact InfoVQA Language-only Natural Image Alpaca DeepForm WebPage TextVQA Vicuna KLC VisualMRC TextCaps Baize Figure 2: Different types of datasets used to train mPLUG-DocOwl. WikiTableQuestions (WTQ) [Pasupat and Liang, 2015], TextVQA [Singh et al., 2019] and VisualMRC [Tanaka et al., 2021]. Information Extraction requires the model to extract key-value pairs from the input image. The 'keys' (or 'categories') are always a stationary set. To convert this task to the instruction tuning format, we treat the value as the {answer} and construct the {question} as 'What is the value for the {key}?'. When the key does not exist in the image, the {answer} is set to 'None'. We collect Information Extraction data from DeepForm [Svetlichnaya, 2020], and Kleister Charity (KLC) [Stanislawek et al., 2021]. Natural Language Inference is a binary classification task with labels ‘Entailed' and 'Refuted'. Given a statement, we construct the {question} as ‘{statement}, Yes or No?'. The {answer} is 'Yes' or ‘No' and refers to ‘Entailed' or ‘Refuted', respectively. TabFact [Chen et al., 2020], a natural language inference dataset about tables, is chosen for instruction tuning. Image Captioning aims to briefly describe an image with fluent language. We treat the caption as the {answer} and randomly choose a prompt as the {question} like LLaVa [Liu et al., 2023a]. TextCaps [Sidorov et al., 2020] is an appropriate captioning dataset on natural images with texts. Language-only and General Vision-and-language Instruction Tuning. To enhance the model's ability of language comprehension and multi-modal open-ended conversation, we follow mPLUGOwl [Ye et al., 2023] to introduce language-only and general vision-and-language instruction tuning data [Taori et al., 2023, Vicuna, 2023, Xu et al., 2023a, Liu et al., 2023a]. Figure 2 shows the composition of our instruction tuning data grouped by the dataset type. We use training sets of these datasets as instruction tuning data and evaluate models on test sets. 3.3 Training Details We adopt a two-stage training paradigm, where the Vision Transformer and Language model are kept frozen. In the first stage, both the visual abstractor and LORA [Hu et al., 2022] in the language model are fine-tuned. The first stage only uses the document understanding data and takes 10 epochs. In the second stage, we further freeze the visual abstractor and only train the LORA. Besides document understanding data, the language-only and general vision-and-language instruction tuning data are further introduced at this stage and up-sampled 6 times. The second stage takes 3 epochs. Other training hyper-parameters are the same as mPLUG-Owl [Ye et al., 2023]. 4 Experiment 4.1 LLMDoc Existing benchmarks are hard to evaluate the open-ended instruction understanding results given by MLMMs. For better compare the instruction understanding performance in the document domain, we further construct a test set with human evaluation, namely LLMDoc.Table 1: Comparison with ocr-free methods on DUE-Benchmark. Model DocVQA Info VQA DeepForm KLC WTQ TabFact Dessurt 63.Donut 67.11.61.30.0 18.54.Pix2Structbase 72.38.mPLUG-DocOwl | 62.38.42.30.3 26.60.Table 2: Comparison with ocr-free methods on chart, natural image and webpage understanding. ChartQA TextVQA TextCaps Model VisualMRC Donut Pix2Structbase 41.43.74.93.56.88.mPLUG-DocOwl 57.52.111.188.Data Collection To comprehensively evaluate the model's abilities, we consider five scenarios to construct our evaluation dataset, including table (TabFact [Chen et al., 2020]), chart (ChartQA [Masry et al., 2022]), document (DocVQA [Mathew et al., 2021]), natural image (TextVQA [Singh et al., 2019]) and webpage (VisualMRC [Tanaka et al., 2021]). Specifically, for each dataset, we sample 20 images from the test split. For 10 of these images, we adopt a raw question as the instruction. While for the other 10, we ask annotators to write instructions requiring stronger capabilities like summarization, inference, and calculation. In total, we obtain 100 test samples. 100%Human Evaluation Following the rating criteria proposed in Self-Instruct [Wang et al., 2022], we perform the human evaluation to score the model's responses, where A > B >C>D and A represents 'correct and satisfying response', B means ‘acceptable response with minor imperfections', C refers to 'response to the instruction but has significant errors' and D means ‘irrelevant or invalid response'. We compare mPLUG-DocOwl with other pop■A ■B ➡C➡D ular mult-modal large language models, including mPLUG-Owl [Ye et al., 2023] and MiniGPT4 [Zhu et al., 2023], on LLMDoc. As shown in Figure 3, mPLUG-DocOwl achieves significantly better performance, with 37 responses being scored as "A", demonstrating the stronger understanding ability of mPLUGDocOwl in diverse document scenarios. Besides, it's worth noting that all models have some responses scored as "C" or "D", showing that instruction understanding performance in the document domain is still far from promising and needs more endeavor. 4.2 Benchmark Evaluation 50%0% mPLUG-DocOwl mPLUG-OwlMiniGPT-Figure 3: Human evaluation of mPLUG-DocOwl, mPLUG-Owl and MiniGPT-4 on LLMDoc. Besides human evaluation, we also compare our mPLUG-DocOwl with ocr-free state-ofthe-art document understanding models on public datasets. Table 1 shows the comparison with Dessurt [Davis et al., 2022], Donut [Kim et al., 2022] and Pix2Struct [Lee et al., 2022] on DUE-Benchmark [Borchmann et al., 2021], which mainly requires the text recognition and layout understanding abilities on documents and tables. Besides, Table 2 presents the evaluation on the chart, natural image and webpage datasets, which ask stronger ability to relate visual semantics and text information. Without finetuning on each dataset, our mPLUG-DocOwl achieves comparable or even better performance.At least two commercial publishers of software manuals have told me they are interested in using this license. GNU Operating System Why publishers should use the GNU F . LEFT ROOM RIGHT ROM RIGHT ROOM RIGHT COMER Toys yous 240 LytoHuman: How many commercial publishers have shown interest in using the license? mPLUG-DocOwl: At least two commercial publishers have shown interest in using the license. Human: what is the second room on the list? mPLUG-DocOwl: right room (a) (b) Opponent Week DateNo internet accessSeptember 6,September 13,September 20,September 27,October 4,17% 15% 19%October 11,at Los Angeles Rams at Cleveland Browns Miami Dolphins at New York Jets Cincinnati Bengals Seattle Seahawks Result W 27-W 9-Attendance 63,79,L 16-L 33-47,50.W 17-44.W 35-42,16% 13%October 18,at New England Patriots L 38-60,October 26,at Pittsburgh Steelers L 26-52,Never2018November 1, 1981. November 8,November 15,November 22,November 29,at Cincinnati Bengals Oakland Raiders L 34-54,W 17-45,December 3,December 13,December 20,at Kansas City Chiefs New Orleans Saints Atlanta Falcons Cleveland Browns at San Francisco 49ers Pittsburgh Steelers L 23-73,L 27-49,L 31-40.W 17-44.L 28-55,W 21-41,Human: Indicate a few pertinent items in this graphic. mPLUG-DocOwl: In 2019, 13% of the population in the United States reported that they never used the internet. Human: on which date did they score the most points? mPLUG-DocOwl: October 11,COP AGREEMENT (c) VIRTUAL VIOLENCE Core-Mark Mr. Rob Allen R. J. Reynolds Tobacco Co.Mr. Rick Sanders 91 % of parents are present at the time of game purchase or rental 06.0% of children recieve parental permission 1 91% 9% Human: Name the companies involved in 'CRP agreement? mPLUG-DocOwl: Cone-Mark & R.J. Reynolds Tobacco Co. Human: What % of parents are not present at the time of game purchase or rental? mPLUG-DocOwl: 9% (e) (f) Figure 4: Qualitative results of mPLUG-DocOwl. The crucial regions and corresponding words are annotated with the same colors for clearer visualization. Wrong answers are colored red. 4.3 Qualitative Analysis Benchmark Results. Qualitative results on different types of images are shown in Figure 4. Crucial regions and corresponding responses are annotated with the same colors. Case (a) shows that mPLUG-DocOwl can accurately find the answer from a webpage screenshot with complex contents. Case (b) shows that mPLUG-DocOwl is even able to understand hand-drawn tables and correctly recognize handwritten fonts. In case (c), mPLUG-DocOwl can summarize key points from a chart. It successfully understands that the table is about internet usage and infers that “Never” means “Never used internet”. However, it also generates illusory outputs, such as "in the United States". The question in case (d) requires the model to understand the "Result" column, compare the points and return the date with the best results. Case (e) demonstrates that our model is capable of processing scannedHuman: Convert each row of the table into json format. mPLUG-DocOwl: {"Game":1,"Date":1,"Location":"Redland Field","Time": 1:42,"Attendance":30,511,"Ga me":2,"Date":2,"Location":"Redland Field","Time": 1:42,"Attendance":29,698,"Ga me":3,"Date":3,"Location":"Comiskey Park Game Date Summary Location Time AttendanceOctober 1 Redland Field 1:30,OctoberRedland Field 1:29,OctoberComiskey Park (1) 1:29,OctoberComiskey Park (1) 1:OctoberComiskey Park (1) 1:OctoberRedland FieldOctoberRedland Field 2:1:34,34,32,13,(I)","Time": 1:30,"Attendance":34,363,"GameOctober 9 Comiskey Park (1) 2:27 32,":4,"Date":4,"Location":"Comiskey Park (I)","Time": 1:37,"Attendance":34,379,"Game ":5,"Date":6,"Location":"Redland Field","Time": 1:45,"Attendance":32,006,"Ga me":6,"Date":7,"Location":"Redland Field","Time":2:06,"Attendance": 13,923,"Ga me":7,"Date":8,"Location":"Redland Field","Time": 1:47,"Attendance":32,930} (a) Score: B Mini-GPT4: |Date Time Score: D Dickies MEDICAL UNIFORMS the SCRUB SHOP Human: What's the name of the mPLUG-DocOwl: The shop is called the Scrub Shop, and it sells medical uniforms. Score: A shop? What does the shop sell? Mini-GPT4: The name of the shop is Scrub Shop. The shop sells various types of clothing such as dresses, tops, pants, and accessories. Score: B (b) Figure 5: Qualitative comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Part one. documents and distinguishing company and person names. Case (f) shows that mPLUG-DocOwl can not only recognize small and blurry text but also perform simple calculations following the user intent. LLMDoc Results Figure 5 and Figure 6 present the comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Figure 5 (a) requires models to convert a table into JSON format. Our mPLUG-DocOwl correctly understands the instruction and return a string in JSON format, but misses the last row. Mini-GPT4 fails to comprehend the instruction and doesn't understand the content within the table. In Figure 5 (b), both mPLUG-DocOwl and Mini-GPT4 correctly recognize the name of the shop. However, Mini-GPT4 overlooks a smaller sign indicating clothes in this shop are medical uniforms. As for chart understanding in Figure 6 (c), Mini-GPT4 gives a wrong answer and redundant response, while our mPLUG-DocOwl gives a concise and correct response. In Figure 6 (d), Bernadette's actual purpose is to confirm with Suzy if she would like to have the copy sent overnight. This not only requires the model to accurately recognize the text, but also to understand the relationships between involved persons. mPLUG-DocOwl recognizes the phrase "request a copy of chapter," but misunderstands the subject and object. Mini-GPT4 only comprehends that this image is a mail scenario and provides a vague and hallucinatory response. In Figure 6 (e), mPLUG-DocOwl gives a correct summary of the two latest news but Mini-GPT4 generates news irrelevant to the webpage screenshot. Change in stroke death rates by age, Belarus,Death rates from stroke by age group, measured as the number of deaths per 100,000 individuals. Age-standardization assumes a constant population age & structure to allow for comparisons between countries and with time without the effects of a changing age distribution within a population (e.g. aging). 70+ years old 50-69 years old old _ 201:All ages 190Age-standardized 5-14 years old 0.119.1,289.1,Source: IHME, Global Burden of Disease (GBD) CC BY Re: Suzy: New copy of chapter from Merck Manual Human: Which age group of people needs to pay most attention to stroke? mPLUG-DocOwl: 70+ years old Score: A Mini-GPT4:The age group that needs the most attention for stroke is 65 and older. ... and the older population is at a higher risk of experiencing a stroke than younger people. Score: C Naima forwarded a message indicating you needed clean copy of this chapter. She requested that I fax it to you. Please let me know if you would like to have the copy sent overnight to you. Thanks, Bernie Janas Designetto Important Fix Message Please Delive Immediately Number of pages in cover sheet. 11:05 AM EST Nimfadedamage indicating you needed of this chapte DIVRITE DIN Human: What is Bernadette's intention in sending this message? mPLUG-DocOwl: Mini-GPT4: Bernadette's intention in sending the email is to request the recipient's assistance To request a copy of chapter from Merck Score: B with completing an important task. The message says, "I need your help with this," indicating that Bernadette needs the recipient's assistance to complete a specific task. Score: C Created: 10 FebruaryLATEST NEWS Europeana Education in the Republic of Moldova Collections Education teacher training anGourself moldova As part of European Heritage Days in October 2019, the National Library of the Republic of Moldova organised a series of educational sessions with the theme 10 things to discover with Europeana - the cultural friend of the student and the teacher'. In this post we explore the activities in the Republic of Moldova and the Europeana resources which support students and teachers throughout Europe. Read more Creed: FebruaryBringing Europeana to new audiences with Unsplash for Education Collections Education parmers At Europeana, we want to bring the richness of digital cultural heritage to both students and educators, and we work with partners to ensure that these groups can access Europeana's collections and resources. In this post, we explore how Europeana is working with the platform Unsplash to reach new educational audiences. Read more © Human: What are the two latest news? The two latest mPLUG-DocOwl: Mini-GPT4: The two latest news in the image are'Amazon to become a member of the US Chamber of Commerce' and 'The news are the European Union in the Republic of Moldova and the European Union resources for education. US Department of Agriculture has proposed to restrict trade with China due to alleged intellectual property violations.' SEE MORE Score: A Score: C Figure 6: Qualitative comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Part two.Vodafone NL 18:Chats (1) Lucy Lives! Guido, Janneke, Paul, Ti... Guido De Vries: 66% Info 90.117.84 117.84 117.84 117.Presente! 17:Timo Bestwerk: Check. 17:Janneke: Ik ben er half4 17:200817:Dan is alles al op ! Timo Bestwerk:Nee, aanmaakblokjes zijn er dan nog vast wel Janneke: Heus nie 17:Timo Bestwerk: Als jij het zegt... 17:17:130.131.127.04 127.117.84 117.84 117.84 117.84 117.84 117.Stuur Human: How many different speakers appear in the picture? mPLUG-DocOwl: There are three different speakers in the picture. (a) 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2018 2019Additional Information ©StatistaShow source Human: What was the average ticket price betweenand 2011? mPLUG-DocOwl: 117.(b) Figure 7: Failure cases on LLMDoc. Part one. " The LLMDoc contains many challenging instruction understanding cases in the document domain. Figure 7 and Figure 8 show some wrong responses given by mPLUG-DocOwl. In Figure 7 (a), mPLUG-DocOwl only takes note of the three names in the picture, but ignores the fact that the user itself is also a speaker. In Figure 7 (b), mPLUG-DocOwl fails to perform multi-step calculations on multiple elements in the image. In Figure 8 (c), the model can understand the scene and the text in it, but fantasizes about non-existent characters. In Figure 8 (d), mPLUG-DocOwl fails to understand the instruction for writing news and only read the texts in the tablet. 5 Conclusion In this work, we infuse diverse ocr-free document understanding capabilities into mPLUG-Owl by incorporating document understanding data into instruction finetuning. Experiment results demonstrate that our mPLUG-DocOwl achieves comparable or even better performance than existing OCRfree methods. Besides, benefiting from language-only and general vision-and-language instruction tuning, mPLUG-DocOwl can better comprehend user instructions and intentions, enabling more complex interactions. Moreover, human evaluation on LLMDoc reveals that mPLUG-DocOwl still struggles with document-related commonsense reasoning, mathematical calculations, and creative generation. This provides valuable insights about developing stronger document understanding abilities with the LLM in the future. SHRED Khed SPOOSHIN Medal table8221231222Rank Nation Gold Silver Bronze TotalRussiaUnited StatesUkraineJapanFranceItalyHungary China 220OOOOO GermanyPolandCanadaCzech RepublicEstoniaUnited KingdomHuman: What kind of scene do the words in the picture convey? mPLUG-DocOwl: The words in the picture convey a comic book or cartoon-like scene, with a character saying "K-Rash" and "Sploosh!" The words are accompanied by a drawing of a person, possibly a character from a comic book or cartoon, and the words are in a speech bubble. The scene appears to be a humorous or lighthearted moment, as the character is likely expressing an exaggerated or unexpected reaction to something. (c) Human: Write a news. mPLUG-DocOwl: Medal table Rank Nation Gold | Silver | Bronze | Total Russia 13 9 8 30 Ukraine 6 4 8 21 Japan 338 France 3 1 26 Italy 3205 Hungary 2 3 0 5 China25 Germany 0 2 3 5 Poland 021 4 Canada 0 1 13 Czech Republic 1011 Estonia 0 1 0 1 United Kingdom(d) Figure 8: Failure cases on LLMDoc. Part two.References S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015. L. Borchmann, M. Pietruszka, T. Stanislawek, D. Jurkiewicz, M. Turski, K. Szyndler, and F. Gralinski. DUE: end-to-end document understanding benchmark. In NeurIPS Datasets and Benchmarks, 2021. S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326, 2015. W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang. Tabfact: A large-scale dataset for table-based fact verification. In International Conference on Learning Representations (ICLR), Addis Ababa, Ethiopia, April 2020. B. L. Davis, B. S. Morse, B. L. Price, C. Tensmeyer, C. Wigington, and V. I. Morariu. End-to-end document recognition and understanding with dessurt. In ECCV Workshops (4), volume 13804 of Lecture Notes in Computer Science, pages 280–296. Springer, 2022. A. Hu, S. Chen, and Q. Jin. Question-controlled text-aware image captioning. In ACM Multimedia, pages 3097-3105. ACM, 2021. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Lowrank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Y. Huang, T. Lv, L. Cui, Y. Lu, and F. Wei. Layoutlmv3: Pre-training for document AI with unified text and image masking. In ACM Multimedia, pages 4083-4091. ACM, 2022. G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park. Ocr-free document understanding transformer. In ECCV (28), volume 13688 of Lecture Notes in Computer Science, pages 498–517. Springer, 2022. K. Lee, M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal, P. Shaw, M. Chang, and K. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. CORR, abs/2210.03347, 2022. C. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao, J. Zhang, S. Huang, F. Huang, J. Zhou, and L. Si. mplug: Effective and efficient vision-language learning by crossmodal skip-connections. In EMNLP, pages 7241–7259. Association for Computational Linguistics, 2022. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. CORR, abs/2304.08485, 2023a. Y. Liu, Z. Li, H. Li, W. Yu, M. Huang, D. Peng, M. Liu, M. Chen, C. Li, L. Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023b. A. Masry, D. X. Long, J. Q. Tan, S. R. Joty, and E. Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. In ACL (Findings), pages 2263–2279. Association for Computational Linguistics, 2022. M. Mathew, D. Karatzas, and C. V. Jawahar. Docvqa: A dataset for VQA on document images. In WACV, pages 2199–2208. IEEE, 2021. M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar. Infographicvqa. In WACV, pages 2582-2591. IEEE, 2022. OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022. P. Pasupat and P. Liang. Compositional semantic parsing on semi-structured tables. In ACL (1), pages 1470–1480. The Association for Computer Linguistics, 2015.T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurençon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. O. Sidorov, R. Hu, M. Rohrbach, and A. Singh. Textcaps: A dataset for image captioning with reading comprehension. In ECCV (2), volume 12347 of Lecture Notes in Computer Science, pages 742-758. Springer, 2020. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, pages 8317–8326. Computer Vision Foundation / IEEE, 2019. T. Stanislawek, F. Gralinski, A. Wróblewska, D. Lipinski, A. Kaliska, P. Rosalska, B. Topolski, and P. Biecek. Kleister: Key information extraction datasets involving long documents with complex layouts. In ICDAR (1), volume 12821 of Lecture Notes in Computer Science, pages 564–579. Springer, 2021. S. Svetlichnaya. Deepform: Understand structured documents at scale, 2020. R. Tanaka, K. Nishida, and S. Yoshida. Visualmrc: Machine reading comprehension on document images. In AAAI, pages 13878–13888. AAAI Press, 2021. Z. Tang, Z. Yang, G. Wang, Y. Fang, Y. Liu, C. Zhu, M. Zeng, C. Zhang, and M. Bansal. Unifying vision, text, and layout for universal document processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19254–19264, 2023. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/ stanford_alpaca, 2023. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. Vicuna. Vicuna: An open chatbot impressing gpt-4. https://github.com/1m-sys/FastChat, 2023. Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. CoRR, abs/2212.10560, 2022. doi: 10.48550/arxiv.2212.10560. URL https://doi.org/10.48550/arxiv. 2212.10560. C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. CORR, abs/2303.04671, 2023. C. Xu, D. Guo, N. Duan, and J. J. McAuley. Baize: An open-source chat model with parameterefficient tuning on self-chat data. CORR, abs/2304.01196, 2023a. H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, G. Xu, J. Zhang, S. Huang, F. Huang, and J. Zhou. mplug-2: A modularized multi-modal foundation model across text, image and video. CoRR, abs/2302.00402, 2023b. Y. Xu, M. Li, L. Cui, S. Huang, F. Wei, and M. Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In R. Gupta, Y. Liu, J. Tang, and B. A. Prakash, editors, KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 1192–1200. ACM, 2020. doi: 10.1145/3394486.3403172. URL https://doi.org/10.1145/3394486.3403172.Z. Yang, Y. Lu, J. Wang, X. Yin, D. Florêncio, L. Wang, C. Zhang, L. Zhang, and J. Luo. TAP: textaware pre-training for text-vqa and text-caption. In CVPR, pages 8751-8761. Computer Vision Foundation/IEEE, 2021. Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, and L. Wang. MM-REACT: prompting chatgpt for multimodal reasoning and action. CoRR, abs/2303.11381, 2023. Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and F. Huang. mplug-owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178, 2023. D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023.