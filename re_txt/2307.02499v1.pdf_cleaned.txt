--- --arX1v:2307.02499v1 [cs.CL] 4 JulmPLUG-DocOwl &: Modularized Multimodal Large Language Model for Document Understanding Jiabo Ye*, Anwen Hu; Haiyang Xu! Qinghao Ye, Ming Yan} Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang DAMO Academy, Alibaba Group {yejiabo.yjb, huanwen.haw, shuofeng.xhy, yeqinghao.ygh, ym119608}@alibaba-inc.com Abstract Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models’ capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOw! generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl. 1 Introduction Large language models (LLMs) like ChatGPT [OpenAI, 2022], BLOOM [Scao et al., 2022], and LLaMA [Touvron et al., 2023] have undergone rapid development to enable the realization of general artificial intelligence, boasting impressive zero-shot capabilities across diverse linguistic applications. With the LLM as the language decoder, Multimodal large language models (MLLMs) such as MiniGPT-4 [Zhu et al., 2023], LLaVA [Liu et al., 2023a], and mPLUG-Owl [Ye et al., 2023] have demonstrated remarkable zero-shot performance in various open-ended vision-and-language tasks. These models are trained to align text and images during the pre-training phase, and then to promote diverse abilities during the instruction tuning phase. Interestingly, these MLLMs exhibit superficial OCR-free text recognition abilities without explicit training on visual text understanding datasets [ Ye et al., 2023, Liu et al., 2023b]. Nevertheless, due to lacking specific training, these models still face the challenge of comprehending intricate relationships between visual text and objects in diverse types of images, such as charts, documents and webpages. “Equal contribution ¥ Corresponding author Preprint. Under review. --- --By performing unified instruction tuning for Document Understanding upon the mPLUG-Owl [Ye et al., 2023], we further propose a modularized MLLM [Li et al., 2022, Xu et al., 2023b], namely mPLUG-DocOwl. Our approach utilizes a modularized framework similar to mPLUG-Owl [Ye et al., 2023], which incorporates a visual abstractor module to link a pre-trained LLM with a visual knowledge module, achieving the alignment of text and images. To enhance diverse document understanding capabilities, we reorganize various downstream document understanding tasks in the same form of instructions. To maintain general uni/multi-modal abilities, we also include language-only and general vision-and-language instruction datasets used by mPLUG-Ow! to train the mPLUG-DocOwl. During training, both the visual knowledge module and LLM decoder are frozen, only the visual abstractor and the Low-Rank Adaption (LoRA) [Hu et al., 2022] in LLM are fine-tuned. mPLUG-DocOw! achieves ocr-free state-of-the-art performance on multiple commonly used document understanding datasets. Furthermore, our experiments on a carefully-built document instruction understanding evaluation set LLMDoc shows that mPLUG-DocOwl achieves significantly better visual text understanding performance on various domains than existing MLMMs. Our main contributions can be highlighted as follows: ¢ We propose a modularized MLLM, mPLUG-DocOwl, which is the first one to balance language-only, general vision-and-language, and document understanding based on unified instruction tuning. ¢ We carefully construct an instruction understanding test set with human evaluation, dubbed LLMDoc, to assess diverse document understanding capabilities. ¢ Empirical results demonstrate that our mPLUG-DocOwl surpasses existing methods on ocr-free document understanding, including multiple standard benchmarks and LLMDoc. 2 Related Work 2.1 Visual Text Understanding There are two types of models for understanding images that contain rich textual information. The first kind of approaches [Xu et al., 2020, Huang et al., 2022, Hu et al., 2021, Tang et al., 2023, Yang et al., 2021] utilize off-the-shelf OCR models or APIs to recognize text from images, and then design pretraining tasks to facilitate cross-modality alignment between visual and textual inputs. On the other hand, end-to-end approaches [Davis et al., 2022, Kim et al., 2022, Lee et al., 2022] utilize a high-resolution image encoder to learn text recognition during the pretraining stage. Both two types of models rely on specific finetuning on different downstream datasets and can’t achieve open-domain instruction understanding performance like Multimodal Large Language Models. 2.2 Multimodal Large Language Model Large Language Models (LLMs) have demonstrated impressive zero-shot abilities across various open-ended tasks. Recent research has also explored the application of LLMs for multi-modal generation, utilizing two different paradigms: systematic collaboration and end-to-end trained models. Systematic collaboration approaches, such as Visual ChatGPT [Wu et al., 2023] and MM-REACT [Yang et al., 2023], leverage various vision experts or tools to express visual information with text descriptions. Subsequently, LLMs, such as ChatGPT [OpenAI, 2022], can act as agents and select appropriate experts and tools for visual understanding. Finally, LLMs would summarize the output of these experts to answer user queries. On the other hand, some approaches, such as MiniGPT-[Zhu et al., 2023], LLaVA [Liu et al., 2023a], and mPLUG-Ow] [Ye et al., 2023], leverage LLMs to build unified models for multi-modality with limited connected parameters. These methods show superficial OCR-free text recognition abilities under the zero-shot setting. However, for complicated document understanding, due to lacking in-domain training, they encounter challenges in handling diverse image types, recognizing rich texts and comprehending relationships between visual semantic and text information. In this work, through unified instruction tuning, mPLUG-DocOwl achieves much better document understanding performance and maintains general uni/multi-modal abilities. --- --Chart Document Natural Image Human: What was the busiest control Human: what is the value Human : Create a compact narrative representing the point in Hong Kong in 2019? mentioned in the CID#: image presented. AL: Lo Wu Control Point AI: 736 00.00 AL: Sign that says Calea Dorobantilor in blue and white momecnenc nnn nnn i Unified Instruction Tuning Multimodal » Chart Document Table retral mPLUG-Owl Natural Image WebPage General V&L Language-only mPLUG-DocOwl aa —_—™ Table wtoccw TP =cT_ WebPage Language-only : : ig Human: What will the presentation Human: Could you offer me an elaborated aim to encourage a discussion about —_ response that demands complex reasoning Ak: It will aim to encourage a and requires breaking down the problem : = discussion about open data_into multiple steps Human: juan romén riquelme have ownership rights and the differing Al: Yes, I can certainly fulfill your request. the same score on july 11 , 2007 social Please provide me with your specific november 17 2007, Yes or No? = question or problem that you would like Al: Yes me to elaborate on. Figure 1: The summary of the instruction tuning paradigm of our mPLUG-DocOwl. 3 mPLUG-DocOwl 3.1 Architecture The architecture of mPLUG-DocOw/l is based on a popular multi-modal language model, mPLUGOwl [Ye et al., 2023], which comprises a pre-trained visual foundation model, a visual abstractor, and a language foundation model. The visual foundation model is responsible for extracting visual features from the input images, and the visual abstractor distills these features using a set of learnable tokens. The resulting visual features are then concatenated with the word embeddings of the input sentence and fed into the language model to generate the response. This powerful architecture allows for accurate and efficient multi-modal language processing. The mPLUG-Ow! [Ye et al., 2023] exhibits superficial OCR ability when presented with images containing salient text. Inspired by this, we propose to further fine-tune the model with document instruction tuning data for better document understanding performance, covering document, table, chart and natural image and webpage. During fine-tuning, we freeze the visual encoder and the language model and train the visual abstractor. We also adopt the low-rank adaptation approach (LoRA) [Hu et al., 2022] to enhance the language model’s ability. 3.2 Instruction Tuning Data This section introduces the composition of our instruction tuning data in detail. To ensure the versatility of mPLUG-DocOwl, we collect diverse document understanding datasets with different task formats, including Visual Question Answering (VQA) [Antol et al., 2015], Information Extraction (IE), Natural Language Inference (NLI) [Bowman et al., 2015], and Image Captioning (IC). mPLUG-Owl [Ye et al., 2023] performs instruction tuning with a unified format as "<image>Human:{ question} AI:{answer}". In this work, we convert different document understanding tasks to the same format as mPLUG-OwI [Ye et al., 2023] by replacing the {question} and {answer} placeholders as follows. Visual Question Answering We simply use the raw question and answer as the {question} and {answer} placeholders. We collect VQA datasets on diverse domains, including ChartQA [Masry et al., 2022], DocVQA [Mathew et al., 2021], Infographics VQA (InfoVQA) [Mathew et al., 2022], --- --General V&L ChartQA LLaVA Language-only WTQ WebPage DocVQA InfoVQA Natural Image Alpaca DeepForm TextVQA Vicuna KLC TextCaps VisualMRC Baize Figure 2: Different types of datasets used to train mPLUG-DocOwl. WikiTableQuestions (WTQ) [Pasupat and Liang, 2015], TextVQA [Singh et al., 2019] and VisualMRC [Tanaka et al., 2021]. Information Extraction requires the model to extract key-value pairs from the input image. The ‘keys’ (or ‘categories’) are always a stationary set. To convert this task to the instruction tuning format, we treat the value as the {answer} and construct the {question} as ‘What is the value for the {key}?’. When the key does not exist in the image, the {answer} is set to ‘None’. We collect Information Extraction data from DeepForm [Svetlichnaya, 2020], and Kleister Charity (KLC) [Stanislawek et al., 2021]. Natural Language Inference is a binary classification task with labels ‘Entailed’ and ‘Refuted’. Given a statement, we construct the {question} as ‘{statement}, Yes or No?’. The {answer} is ‘Yes’ or ‘No’ and refers to ‘Entailed’ or ‘Refuted’, respectively. TabFact [Chen et al., 2020], a natural language inference dataset about tables, is chosen for instruction tuning. Image Captioning aims to briefly describe an image with fluent language. We treat the caption as the {answer} and randomly choose a prompt as the {question} like LLaVa [Liu et al., 2023a]. TextCaps [Sidorov et al., 2020] is an appropriate captioning dataset on natural images with texts. Language-only and General Vision-and-language Instruction Tuning. To enhance the model’s ability of language comprehension and multi-modal open-ended conversation, we follow mPLUGOwl [Ye et al., 2023] to introduce language-only and general vision-and-language instruction tuning data [Taori et al., 2023, Vicuna, 2023, Xu et al., 2023a, Liu et al., 2023a]. Figure 2 shows the composition of our instruction tuning data grouped by the dataset type. We use training sets of these datasets as instruction tuning data and evaluate models on test sets. 3.3 Training Details We adopt a two-stage training paradigm, where the Vision Transformer and Language model are kept frozen. In the first stage, both the visual abstractor and LoRA [Hu et al., 2022] in the language model are fine-tuned. The first stage only uses the document understanding data and takes 10 epochs. In the second stage, we further freeze the visual abstractor and only train the LoRA. Besides document understanding data, the language-only and general vision-and-language instruction tuning data are further introduced at this stage and up-sampled 6 times. The second stage takes 3 epochs. Other training hyper-parameters are the same as mPLUG-Owl [Ye et al., 2023]. 4 Experiment 4.1 LLMDoc Existing benchmarks are hard to evaluate the open-ended instruction understanding results given by MLMMs. For better compare the instruction understanding performance in the document domain, we further construct a test set with human evaluation, namely LLMDoc. --- --Table 1: Comparison with ocr-free methods on DUE-Benchmark. Model | DocVQA_ InfoVQA DeepForm KLC WTQ_ TabFact Dessurt 63.2 - - - - Donut 67.5 11.6 61.6 30.0 18.8 54.Pix2Structyase 72.1 38.2 - - - mPLUG-DocOwl | 62.2 38.2 42.6 30.3 26.9 60.Table 2: Comparison with ocr-free methods on chart, natural image and webpage understanding. Model | ChartQA TextVQA_ TextCaps VisualIMRC Donut 41.8 43.5 74.4 93.Pix2Structyase 56.0 - 88.0 mPLUG-DocOwl | 57.4 52.6 111.9 188.Data Collection To comprehensively evaluate the model’s abilities, we consider five scenarios to construct our evaluation dataset, including table (TabFact [Chen et al., 2020]), chart (ChartQA [Masry et al., 2022]), document (DocVQA [Mathew et al., 2021]), natural image (TextVQA [Singh et al., 2019]) and webpage (VisualMRC [Tanaka et al., 2021]). Specifically, for each dataset, we sample 20 images from the test split. For 10 of these images, we adopt a raw question as the instruction. While for the other 10, we ask annotators to write instructions requiring stronger capabilities like summarization, inference, and calculation. In total, we obtain 100 test samples. Human Evaluation — Following the rating criteria proposed in Self-Instruct [Wang et al., 2022], we perform the human evaluation to score the model’s responses, where A > B > C > D and A represents “correct and satisfying response’, B means ‘acceptable response with minor imperfections’, C refers to ‘response to the instruction but has significant errors’ and D means ‘irrelevant or invalid response’. We compare mPLUG-DocOw] with other pop- =A —b —c mm D ular mult-modal large language models, includ ing mPLUG-Owl [Ye et al., 2023] and Mini- 100% GPT4 [Zhu et al., 2023], on LLMDoc. As shown in Figure 3, mPLUG-DocOwl achieves significantly better performance, with 37 responses being scored as “A”, demonstrating the stronger understanding ability of mPLUGDocOwl in diverse document scenarios. Besides, it’s worth noting that all models have some responses scored as “C” or “D”, showing that instruction understanding performance in the document domain is still far from promising and needs more endeavor. 4.2. Benchmark Evaluation Besides human evaluation, we also compare 0% aPLUG-DocOw! mPLUG-Ow! MiniGPT-our mPLUG-DocOwl with ocr-free state-of- __ . the-art document understanding models on Figure 3: Human evaluation of mPLUG-DocOwl, public datasets. Table 1 shows the comparison ™PLUG-Owl and MiniGPT-4 on LLMDoc. with Dessurt [Davis et al., 2022], Donut [Kim et al., 2022] and Pix2Struct [Lee et al., 2022] on DUE-Benchmark [Borchmann et al., 2021], which mainly requires the text recognition and layout understanding abilities on documents and tables. Besides, Table 2 presents the evaluation on the chart, natural image and webpage datasets, which ask stronger ability to relate visual semantics and text information. Without finetuning on each dataset, our mPLUG-DocOwl achieves comparable or even better performance. --- --[Al least two commercial publishers of software manuals have fold me they are intersted in using this Hcense Human: How many commercial publishers have shown Human: what is the second room on the list? interest in using the license ? mPLUG-DocOwl: right room mPLUG-DocOwl: At least two commercial publishers have shown interest in using the license. (a) (b) Week Date Opponent Result | Attendance 1 [September 6, 1981 at Los Angeles Rams |W 27-20, 63,2 | September 13, 1981 at Cleveland Browns W9-3_| 79,3 September 20, 1981 Miami Dolphins Li6-10 | 47.4 | September 27, 1981 at New York Jets 133-17 | 50,5 October 4,1981__ Cincinnati Bengals |, W117-10, 44,19% 6 | October 11, 1981 _| Seattle Seahawks w35-17| 42.17% 1555 16K 136 UCtaber TE, TIBI — | at New England Patriots C3810 60,8 [October 26,1981 at Pittsburgh Steelers 126-13 52,9 November 1, 1981 at Cincinnati Bengals 134-21 54,10 | November 8, 1981 Oakland Raiders wi7-16/ 45,11 | November 15, 1981 at Kansas City Chiefs 23-10 73,12 November 22, 1981 New Orleans Saints 27-24 49,@ 2018 © 2019 13 November 29, 1981 Atlanta Falcons L31-27 | 40,1d [December 3,1981 Cleveland Browns W17-13, 44,- 15 [December 13, 1981 at San Francisco 49ers 128-6 55,16 | December 20, 1981 | Pittsburgh Steelers | W21-20/ 41,Human: Indicate a few pertinent items in this graphic. Human: on which date did they score the most points? mPLUG-DocOwl: In 2019, 13% of the population in the mPLUG-DocOwl: October 11,United States reported that they never used the internet. (c) (d) Core-Mark Mr. Rob Allen RJ. Reynolds Tobacco Co. Mr. Rick Sanders 1-91% =9% Human: Name the companies involved in ‘CRP agreement’? | Human: What % of parents are not present at the time of mPLUG-DocOwl: Cone-Mark & R.J. Reynolds Tobacco Co.] game purchase or rental? mPLUG-DocOwl: 9% (e) (f) Figure 4: Qualitative results of mPLUG-DocOwl. The crucial regions and corresponding words are annotated with the same colors for clearer visualization. Wrong answers are colored red. 4.3 Qualitative Analysis Benchmark Results. Qualitative results on different types of images are shown in Figure 4. Crucial regions and corresponding responses are annotated with the same colors. Case (a) shows that mPLUG-DocOwl can accurately find the answer from a webpage screenshot with complex contents. Case (b) shows that mPLUG-DocOw] is even able to understand hand-drawn tables and correctly recognize handwritten fonts. In case (c), mPLUG-DocOwl can summarize key points from a chart. It successfully understands that the table is about internet usage and infers that “Never” means “‘Never used internet”. However, it also generates illusory outputs, such as "in the United States". The question in case (d) requires the model to understand the “Result” column, compare the points and return the date with the best results. Case (e) demonstrates that our model is capable of processing scanned --- --Human: Convert each row of the table into json format. mPLUG-DocOw!: Mini-GPT4: Summary {"Game":1,"Date":1,"Location":"Redland | Date | Time | Game Date Location Time | Attendance _ Field",""Time":1:42,"Attendance":30,511,"Ga 1 October 1. Redland Field 1:42 | 30,511 me":2,"Date":2,"Location":"Redland 2 October 2. Redland Field 1:42 | 29,698 Field","Time":1:42,"Attendance":29,698,"Ga 3 October 3 Comiskey Park (I) 1:30 | 29,126 me":3,"Date":3,"Location":"Comiskey Park 4 October 4 Comiskey Park (I) 1:37 | 34,363 (D","Time":1:30,"Attendance":34,363,"Game 5 October 6 Comiskey Park (I) 1:45 | 34,379 ":4,"Date":4,"Location":"Comiskey Park 6 October 7 Redland Field 2:06 | 32,006 (1)","Time":1:37,"Attendance":34,379,"Game 7 October 8 Redland Field 1:47 | 13,923 ":5,"Date":6,"Location":"RedlandField","Time":1:45,"Attendance":32,006,"Ga 'Location":"Redland 06," Attendance": 13,923,"Ga me":7,"Date":8,"Location":"Redland Field","Time":1:47,"Attendance":32,930} Score: B Score: D October 9 Comiskey Park (I) 2:27 | 32,(a) Human: What's the name of the shop? What does the shop sell? mPLUG-DocOwl: The shop is Mini-GPT4: The name of the called the Scrub Shop, and it shop is Scrub Shop. The shop sells medical uniforms. sells various types of clothing such as dresses, tops, pants, and accessories. Score: A Score: B (b) Figure 5: Qualitative comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Part one. documents and distinguishing company and person names. Case (f) shows that mPLUG-DocOwl can not only recognize small and blurry text but also perform simple calculations following the user intent. LLMDoc Results Figure 5 and Figure 6 present the comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Figure 5 (a) requires models to convert a table into JSON format. Our mPLUG-DocOwl correctly understands the instruction and return a string in JSON format, but misses the last row. Mini-GPT4 fails to comprehend the instruction and doesn’t understand the content within the table. In Figure 5 (b), both mPLUG-DocOwl and Mini-GPT4 correctly recognize the name of the shop. However, Mini-GPT4 overlooks a smaller sign indicating clothes in this shop are medical uniforms. As for chart understanding in Figure 6 (c), Mini-GPT4 gives a wrong answer and redundant response, while our mPLUG-DocOw! gives a concise and correct response. In Figure 6 (d), Bernadette’s actual purpose is to confirm with Suzy if she would like to have the copy sent overnight. This not only requires the model to accurately recognize the text, but also to understand the relationships between involved persons. mPLUG-DocOwl recognizes the phrase "request a copy of chapter," but misunderstands the subject and object. Mini-GPT4 only comprehends that this image is a mail scenario and provides a vague and hallucinatory response. In Figure 6 (e), mPLUG-DocOw!l gives a correct summary of the two latest news but Mini-GPT4 generates news irrelevant to the webpage screenshot. --- --oe es Human: Which age group of people needs to pay most attention oven oe = to stroke? mPLUG-DocOwl: Mini-GPT4:The age group that needs the 70+ years old most attention for stroke is 65 and older. ... and the older population is at a higher risk of experiencing a stroke than younger people. ets Score: A Score: C (©) Re: New copy of chapter from Merck Manual Suzy: Naima forwarded a message indicating you needed a clean copy of this chapter. ‘She requested that | fax ito you. Please let me know if you would like to have the copy sent overnight to you. Thanks, Bemie Janas Human: What is Bernadette's intention in sending this message? mPLUG-DocOwl: Mini-GPT4: Bernadette's intention in sending To request acopy _ the email is to request the recipient's assistance of chapter from with completing an important task. The Merck message says, "I need your help with this," indicating that Bernadette needs the recipient's assistance to complete a specific task. Score: B Score: © (d) = ~~ Human: What are the two latest news? LATEST NEWS ais mPLUG-DocOwl: Mini-GPT4: The two latest news The two latest in the image are'Amazon to news are the become a member of the US European Union — Chamber of Commerce' and 'The in the Republic of _US Department of Agriculture Moldova andthe __ has proposed to restrict trade European Union —_ with China due to alleged 5 Tesources for intellectual property violations.’ education. Score: A Score: C (e) Figure 6: Qualitative comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Part two. --- --117.84 117.84 117.84 117.90.Lucy Lives! Janneke, Paul, Ti Human: How many different speakers appear in the Human: What was the average ticket price betweenpicture? and 2011? mPLUG-DocOwl: There are three different speakers mPLUG-DocOw! : 117.in the picture. (a) (b) Figure 7: Failure cases on LLMDoc. Part one. The LLMDoc contains many challenging instruction understanding cases in the document domain. Figure 7 and Figure 8 show some wrong responses given by mPLUG-DocOwl. In Figure 7 (a), mPLUG-DocOw! only takes note of the three names in the picture, but ignores the fact that the user itself is also a speaker. In Figure 7 (b), mPLUG-DocOw/ fails to perform multi-step calculations on multiple elements in the image. In Figure 8 (c), the model can understand the scene and the text in it, but fantasizes about non-existent characters. In Figure 8 (d), mPLUG-DocOw/] fails to understand the instruction for writing news and only read the texts in the tablet. 5 Conclusion In this work, we infuse diverse ocr-free document understanding capabilities into mPLUG-Owl by incorporating document understanding data into instruction finetuning. Experiment results demonstrate that our mPLUG-DocOwl achieves comparable or even better performance than existing OCRfree methods. Besides, benefiting from language-only and general vision-and-language instruction tuning, mPLUG-DocOwl can better comprehend user instructions and intentions, enabling more complex interactions. Moreover, human evaluation on LLMDoc reveals that mPLUG-DocOwI still struggles with document-related commonsense reasoning, mathematical calculations, and creative generation. This provides valuable insights about developing stronger document understanding abilities with the LLM in the future. --- --Medal table Rank Nation Gold Silver Bronze Total Russia 13 9 8United States 4Ukraine 6 ay JapanFrance 3) ItalyHungaryChinaGermanyPolandCanadaCzech Republic |Estonia te) United Kingdom |Human: What kind of scene do the words in the picture convey? mPLUG-DocOw!: The words in the picture convey a comic book or cartoon-like scene, with a character saying "K-Rash" and "Sploosh!" The words are olololelsula alalalulalwinie ORIEN NIN BR WIN|R INN) & FlololRiniwnNiolo|niwiuio Blele)wlalu uu ulalo accompanied by a drawing of a person, possibly a Human: Write a news. character from a comic book or cartoon, and the mPLUG-DocOwl: Medal table Rank Nation Gold | Silver | words are in a speech bubble. The scene appears to Bronze | Total Russia 13 9 8 30 Ukraine 6 4 8 21 Japan 3be a humorous or lighthearted moment, as the 3 8 France 3 | 2 6 Italy 3 2 0 5 Hungary 2 3 05 Chinacharacter is likely expressing an exaggerated or 25 Germany 0 23 5 Poland 0 2 1 4 Canada 0 1 1 3 Czech unexpected reaction to something. Republic 1 0 1 1 Estonia 0 1 0 1 United Kingdom(c) (d) Figure 8: Failure cases on LLMDoc. Part two.--- --References S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433, 2015. L. Borchmann, M. Pietruszka, T. Stanislawek, D. Jurkiewicz, M. Turski, K. Szyndler, and F. Gralinski. DUE: end-to-end document understanding benchmark. In NeurIPS Datasets and Benchmarks, 2021. S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning. A large annotated corpus for learning natural language inference. arXiv preprint arXiv: 1508.05326, 2015. W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang. Tabfact : A large-scale dataset for table-based fact verification. In International Conference on Learning Representations (ICLR), Addis Ababa, Ethiopia, April 2020. B. L. Davis, B. S. Morse, B. L. Price, C. Tensmeyer, C. Wigington, and V. I. Morariu. End-to-end document recognition and understanding with dessurt. In ECCV Workshops (4), volume 13804 of Lecture Notes in Computer Science, pages 280-296. Springer, 2022. A. Hu, S. Chen, and Q. Jin. Question-controlled text-aware image captioning. In ACM Multimedia, pages 3097-3105. ACM, 2021. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Lowrank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https: //openreview.net/forum?id=nZeVKeeFYf9. Y. Huang, T. Lv, L. Cui, Y. Lu, and F. Wei. Layoutlmv3: Pre-training for document AI with unified text and image masking. In ACM Multimedia, pages 4083-4091. ACM, 2022. G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park. Ocr-free document understanding transformer. In ECCV (28), volume 13688 of Lecture Notes in Computer Science, pages 498-517. Springer, 2022. K. Lee, M. Joshi, IL. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal, P. Shaw, M. Chang, and K. Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. CoRR, abs/2210.03347, 2022. C. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao, J. Zhang, S. Huang, F. Huang, J. Zhou, and L. Si. mplug: Effective and efficient vision-language learning by crossmodal skip-connections. In EMNLP, pages 7241-7259. Association for Computational Linguistics, 2022. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. CoRR, abs/2304.08485, 2023a. Y. Liu, Z. Li, H. Li, W. Yu, M. Huang, D. Peng, M. Liu, M. Chen, C. Li, L. Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023b. A. Masry, D. X. Long, J. Q. Tan, S. R. Joty, and E. Hoque. Chartqa: A benchmark for question answering about charts with visual and logical reasoning. In ACL (Findings), pages 2263-2279. Association for Computational Linguistics, 2022. M. Mathew, D. Karatzas, and C. V. Jawahar. Docvqa: A dataset for VQA on document images. In WACY, pages 2199-2208. IEEE, 2021. M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar. Infographicvqa. In WACY, pages 2582-2591. IEEE, 2022. OpenAI. Introducing chatgpt. https: //openai.com/blog/chatgpt, 2022. P. Pasupat and P. Liang. Compositional semantic parsing on semi-structured tables. In ACL (1), pages 1470-1480. The Association for Computer Linguistics, 2015.--- --T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurengon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. 1. Adelani, and et al. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. O. Sidorov, R. Hu, M. Rohrbach, and A. Singh. Textcaps: A dataset for image captioning with reading comprehension. In ECCV (2), volume 12347 of Lecture Notes in Computer Science, pages 742-758. Springer, 2020. A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards VQA models that can read. In CVPR, pages 8317-8326. Computer Vision Foundation / IEEE, 2019. T. Stanislawek, F. Gralinski, A. Wréblewska, D. Lipinski, A. Kaliska, P. Rosalska, B. Topolski, and P. Biecek. Kleister: Key information extraction datasets involving long documents with complex layouts. In JCDAR (1), volume 12821 of Lecture Notes in Computer Science, pages 564-579. Springer, 2021. S. Svetlichnaya. Deepform: Understand structured documents at scale, 2020. R. Tanaka, K. Nishida, and S. Yoshida. Visualmrc: Machine reading comprehension on document images. In AAAI, pages 13878-13888. AAAI Press, 2021. Z. Tang, Z. Yang, G. Wang, Y. Fang, Y. Liu, C. Zhu, M. Zeng, C. Zhang, and M. Bansal. Unifying vision, text, and layout for universal document processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19254-19264, 2023. R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An instruction-following Ilama model. https://github.com/tatsu-lab/ stanford_alpaca, 2023. H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Roziére, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. Vicuna. Vicuna: An open chatbot impressing gpt-4. https: //github.com/lm-sys/FastChat, 2023. Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language model with self generated instructions. CoRR, abs/2212.10560, 2022. doi: 10.48550/arXiv.2212.10560. URL https: //doi.org/10.48550/arXiv.2212.10560. C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. CoRR, abs/2303.04671, 2023. C. Xu, D. Guo, N. Duan, and J. J. McAuley. Baize: An open-source chat model with parameterefficient tuning on self-chat data. CoRR, abs/2304.01196, 2023a. H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, G. Xu, J. Zhang, S. Huang, F. Huang, and J. Zhou. mplug-2: A modularized multi-modal foundation model across text, image and video. CoRR, abs/2302.00402, 2023b. Y. Xu, M. Li, L. Cui, S. Huang, F. Wei, and M. Zhou. Layoutlm: Pre-training of text and layout for document image understanding. In R. Gupta, Y. Liu, J. Tang, and B. A. Prakash, editors, KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 1192-1200. ACM, 2020. doi: 10.1145/3394486.3403172. URL https://doi.org/10.1145/3394486 . 3403172.--- --Z. Yang, Y. Lu, J. Wang, X. Yin, D. Floréncio, L. Wang, C. Zhang, L. Zhang, and J. Luo. TAP: textaware pre-training for text-vqa and text-caption. In CVPR, pages 8751-8761. Computer Vision Foundation / IEEE, 2021. Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu, M. Zeng, and L. Wang. MM-REACT: prompting chatgpt for multimodal reasoning and action. CoRR, abs/2303.11381, 2023. Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and F. Huang. mplug-owl: Modularization empowers large language models with multimodality. CoRR, abs/2304.14178, 2023. D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023.