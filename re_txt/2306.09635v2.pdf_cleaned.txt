--- --2306.09635v2 [cs.SD] 23 JularXiv CLIPSONIC: TEXT-TO-AUDIO SYNTHESIS WITH UNLABELED VIDEOS AND PRETRAINED LANGUAGE-VISION MODELS Hao-Wen Dong'?* Xiaoyu Liu! Santiago Pascual’ Joan Serra+ ! Dolby Laboratories ABSTRACT Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with highquality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that the pretrained diffusion prior can reduce the modality transfer gap. While we focus on text-to-audio synthesis, the proposed model can also generate audio from image queries, and it shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test. This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-vision models. Index Terms— Sound synthesis, audio generation, multimodal learning, diffusion models, neural networks, machine learning 1. INTRODUCTION With the advance of generative modeling [1—3] and language-audio contrastive learning [4—6], various deep learning-based text-to-audio synthesis systems have recently emerged [7-12]. However, these systems typically require a large amount of paired text-audio data for training. Despite extensive human annotation efforts, the current largest public text-audio dataset contains around 630k text-audio pairs [4]. Given the relative scarcity of text-audio data on the web as compared to text-image data, it remains unclear whether we can scale up text-audio datasets to a size comparable with large scale text-image datasets, e.g., the LAION-5B dataset [13], which contains 5.85 billion text-image pairs. In this work, we approach text-to-audio synthesis without text-audio pairs through leveraging the naturally-occurring audio-visual correspondence in videos and the multimodal representation learned by pretrained language-vision models (see Figure 1). The proposed CLIPSonic model is based on a conditional diffusion model [15], a constrastive language-image pretraining (CLIP) *Work done during an internship at Dolby. Hao-Wen thanks Taiwan Ministry of Education for supporting his PhD study. Contact: hwdong@ucsd.edu Jordi Pons* Taylor Berg-Kirkpatrick? Gautam Bhattacharya* Julian McAuley” ? University of California San Diego Text ; Pretrained a photo of | vision-language Naturally-occurring train whistling models audio-visual : correspondence + in videos Video frames Hebei ov Desired audio-text correspondence Audio Figure 1: We learn the text-audio correspondence by leveraging the audio-visual correspondences in videos and the multimodal representation learned by pretrained language-vision models. model [16], and a pretrained diffusion prior model [17], as illustrated in Figure 2. Given a video, CLIPSonic is trained to synthesize the mel spectrogram of the audio given a CLIP-encoded frame, randomly selected from the video. Since CLIP embeds images and texts into a cross-modal semantic space, CLIPSonic learns to map the CLIP embedding space to audio. At test time, we first explore performing a zero-shot modality transfer and conditioning the diffusion model directly with a CLIP-encoded text query. However, we observe in practice a noticeable performance drop with respect to image queries. To close this gap, we adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. We note that our proposed system requires only 1) unlabeled videos, for training the conditional diffusion model, and 2) image-text pairs, for pre-training the language-vision models. Through a subjective listening test and an objective evaluation, our experimental results demonstrate the effectiveness of the proposed method. Audio samples are available on our demo website.! Our study differs from prior work in several ways. Existing textto-audio models rely on large amounts of text-audio training pairs [7— 12], whereas CLIPSonic learns text-queried audio synthesis without text-audio pairs. Prior work studied image-to-audio synthesis [18— 20], but they do not examine the zero-shot modality transfer between texts and images. CLIPSep [21] and CLIPSynth [22] propose to learn text-queried source separation and audio synthesis from unlabeled videos, respectively, but they do not address the issue of the zeroshot modality transfer gap. DALL-E 2 [17] proposes the diffusion prior model to address the zero-shot modality transfer gap in CLIPbased text-to-image synthesis, and we explore leveraging a pretrained diffusion prior model to transfer the knowledge learned from videos for text-to-audio synthesis. Other related works are AudioLDM [9] and MusicLM [12], which rely on language-audio models [4, 5] to perform a zero-shot audio-to-text modality transfer, but such language-audio models are trained on audio-text pairs. ‘https: //salu133445. github. io/clipsonic/ --- --7 ; \ CLIP-image (pretrained; frozen) img a photo of Xx, CLIP-text ~ (pretrained; frozen) Qrexe A = { < U-Net Peal =T,..,1) yy Input video Xp = \ (0) Inference — CLIPSonie-ZS (zero-shot transfer) Diffusion model (for ¢ (— = >) CLIP-text Diffusion prior Xt (pretrained; frozen) (pretrained; frozen) train ‘illu 4] Bf} | *La ae text Gimg \ (a) Training - CLIPSonic _) \ © Inference — CLIPSonic-PD (pretrained diffusion prior) Diffusion model (for t = 7, ..., 1) ) Figure 2: Proposed CLIPSonic model. During training, CLIPSonic learns to synthesize the audio track of a video given the image in a video frame. At inference time, we feed a text query in the form of “a photo of [label]” to approach text-to-audio synthesis or use a pretrained diffusion prior model to close the gap between the text queries (used for noisy spectrogram at diffusion step t. The generated mel spectrogram Xo 2. CLIPSONIC In this section, we introduce the proposed CLIPSonic model for learning text-to-audio synthesis from unlabeled videos. As illustrated in Figure 2(a), CLIPSonic uses a mel spectrogram-based diffusion model for audio synthesis. We adopt the diffusion framework for its strong performance in audio synthesis [9, 23,24]. Given a video, CLIPSonic is trained to synthesize the mel spectrogram of the audio from the image in a randomly extracted video frame. Specifically, we first use a pretrained CLIP image encoder to encode the image into a query vector img. Then, this query vector is used as a conditional signal to guide the diffusion model to generate a mel spectrogram Xo. We adopt a denoising diffusion probabilistic model [15] and classifier-free guidance [25], which allows us to control the degree of conditioning signal through the guidance level variable w during inference.” The generated mel spectrograms are inverted back to waveforms using a separately-trained BigVGAN [14]. We choose to perform diffusion on the mel spectrogram domain for its lower dimensionality than waveforms, and because BigVGAN shows good quality when synthesizing general audio from mel spectrograms. CLIPSonic-ZS (zero-shot modality transfer). At inference time, we aim to leverage the language-vision embedding space learned by CLIP to achieve text-to-audio synthesis. CLIPSonic-ZS explores swapping the CLIP image embeddings for the CLIP text embeddings, as a way to use text queries in a zero-shot modality transfer setting. As illustrated in Figure 2(b), we use the CLIP text encoder to encode the input text query into a query vector Qtex:, which is fed as a condition to the diffusion model. We refer to this model as CLIPSonic-ZS, where “ZS” stands for zero-shot modality transfer. CLIPSonic-PD (pretrained diffusion prior). As to be shown in Section 4, we observe a modality gap between CLIP’s text and image embedding spaces. Following DALL-E 2 [17], we explore relying on a diffusion prior model to bridge this gap. As illustrated in Figure 2(c), we first encode the input text query into a CLIP text embedding vector qier: and then generate a CLIP image embedding vector Gimg from Qtert using the pretrained diffusion prior model. The generated query vector Qim, is then passed as the conditioning signal to the diffusion model. We refer to this 2We use the formulation: Vx log pw(x|q) = (1 — w)Vx log p(x) + wVx log p(x | q). A larger w leads to a stronger conditioning signal, and w = 1 corresponds to a conditional model without classifier-free guidance. inference) and the image queries (used for training). x; represents a is inverted back to waveform by a pretrained BigVGAN model [14]. model as CLIPSonic-PD (pretrained diffusion prior). Note that both CLIPSonic-ZS and CLIPSonic-PD require no text-audio pairs for training. Further, both the CLIP and diffusion prior models can be pretrained using only text-image pairs, hence suppressing the need for paired audio-text data. CLIPSonic-IQ and CLIPSonic-SD. While here we focus on textto-audio, CLIPSonic can also be used as an image-to-audio synthesis model by using qim, queries. We will refer to this variant as CLIPSonic-IQ (image-queried). Moreover, we find that it is possible to train the diffusion prior model from scratch on domain-specific datasets, and hence we also consider a variant called CLIPSonicSD (supervised diffusion prior), where we train the diffusion prior model from scratch using text-image pairs in our datasets. As will be specified in Section 3, since the text data used to train the diffusion prior in CLIPSonic-SD comes from audio labels in this work, CLIPSonic-SD serves as an oracle model against CLIPSonic-PD. By comparing CLIPSonic-PD to CLIPSonic-SD, we intend to study the effectiveness of using a diffusion prior model pretrained on a massive amount of data against one trained on the target dataset. 3. EXPERIMENTAL SETUP Data. We consider two datasets: VGGSound [26] and MUSIC [27]. The VGGSound dataset consists of 171,899 10-sec YouTube videos, covering 310 classes of sounds in the wild, and we follow the traintest split provided with the dataset. The MUSIC dataset cons of 1,055 full-length YouTube videos of people playing a musical instrument, with 21 instrument types in total. We randomly split the dataset into a 9:1 train-test split. VGGSound represents a large, diverse dataset captured from unstructured sources in the wild, whereas MUSIC represents a small, curated dataset of a specific domain of interest. As both datasets come with only class labels, we convert such labels into pseudo text in the form of “a photo of [label]”. Baseline models. We compare CLIPSonic models against the fol lowing text-to-audio (TTA) and reconstruction models. * CLIP-TTA is the supervised version of CLIPSonic where we use text-audio pairs for training. The pretrained CLIP-text embedding is used as conditioning. * CLAP-TTA is the same as CLIP-TTA but uses pretrained CLAPtext embeddings [4], where we use a prompt in the form of “the sound of [label]”. Unlike CLIP-text embeddings, CLAP-text --- --(a) FAD! (VGGSound) (b) CLAP Score t (VGGSound) (c) FAD! (MUSIC) (d) CLAP Score t (MUSIC) 6 ant! . . 35 eng * Zz 032 Perr ee ot ne Zz 031 a ae ener B65 0.30 eo g 30 080] he tg eee eg 5 g . 5 g ¢ % ga So} + oN 3g 83 | ol ¥. <“ ne er 1S og = 0.3s & 0.26 is a ee Be) %3 5 o24 7, a oe] 215 332 “z on « 210 0.B1 3 ° z Bs1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4Guidance level (w) Guidance level (w) Guidance level (w) Guidance level (w) —e— CLIPSonic-ZS (zero-shot modality transfer) -"- CLIPSonic-1Q (image-queried) @ CLIPTTA =~ BigVGAN reconstruction —e— CLIPSonic-PD (pretrained diffusion prior) -¢ CLIPSonic-SD (supervised diffusion prior) -@- CLAP-TTA Figure 3: Objective evaluation results on VGGSound and MUSIC. embeddings are expected to encode audio-grounded features rather than visually-grounded features. + BigVGAN mel spectrogram reconstruction are waveforms reconstructed from the ground-truth mel spectrograms by the BigVGAN model. This serves as an upper bound of spectrogram-based synthesis systems that use BigVGAN as the inversion model. Implementation details. For mel spectrogram computation, we use a sampling rate of 16 kHz, a hop size of 512, an FFT filter size of 2048, and 64 mel bands. During training, we use mel spectrograms of size 6464, which corresponds to two seconds of audio. For the diffusion model, we follow the network architecture proposed in [15] and use the open-source code in [28]. We use a cosine noise schedule with 4000 diffusion steps during training and 1000 steps at inference time. We use AdamW with a learning rate of 0.0001, a batch size of 32, and a dropout rate of 0.1 in classifier-free guidance. All diffusion models are trained for 200k steps on MUSIC and 500k steps on VGGSound using two NVIDIA RTX 2080 Ti GPUs, which takes a day on MUSIC and two days on VGGSound. For the pretrained CLIP model, we use the “ViT-L/14” version trained on 400 million imagetext pairs [29]. We use a pretrained transformer-based diffusion prior model trained on 2 billion image-text pairs using the same backbone CLIP model [30]. For training the diffusion prior model CLIPSonic-SD from scratch, we follow the same architecture as in CLIPSonic-PD and use the code in [31]. We use AdamW with a learning rate of 0.0001 and a batch size of 32. The diffusion prior models are trained on MUSIC and VGGSound, respectively, until convergence at around 200k steps, which takes a day on a NVIDIA RTX 2080 Ti GPU. For the CLAP model, we use the ““630k-audiosetfusion” version released in [32]. For the BigVGAN model, we pretrain it on VGGSound for 500k steps using the code in [33] and use this pretrained version in all of our experiments. Evaluation metrics. To compare the performance of our method against the baselines, we sample 512 audio samples from each model and compute the Fréchet audio distance (FAD) [34] and the CLAP score [4, 10]. The FAD measures how close the generated audio samples are to the reference audio in terms of quality and diversity.* We adopt the open-source implementation provided in [35] and use the VGGish [36] as the backbone model for FAD. The CLAP score measures the relevance between the generated audio and the input query text, and it is formally defined as the cosine similarity between the CLAP embedding of the audio and that of the input text query. Subjective test. We conduct a listening test to study the fidelity of the generated audio and their relevance to textual (text-to-audio) and Following [7,9] we also computed the Fréchet inception distance (FID) of the generated spectrograms, and found that the trend of FID aligned well with that of FAD. For brevity, we only report and discuss the FAD results. visual (image-to-audio) prompts. We ask 21 expert listeners to rate the generated audio samples on a 1-5 scale in terms of fidelity and relevance. Fidelity experiments study the quality of the generated audio (without evaluating its semantic grounding) while relevance experiments study the semantic correspondence with respect to the prompt (without evaluating its audio quality). The audio samples used for this test are available on our demo website. 4. RESULTS 4.1. Objective Evaluation Results Guidance. Figure 3 shows the results of the studied models as a function of the classifier-free guidance scale w. As noted in [25] using conceptually-similar measures, the different curves between FAD and CLAP scores imply a trade-off between quality/diversity (represented by FAD) and query-sample relevance (represented by CLAP score). Noticeably, in terms of CLAP score, all models (except CLIPSonic-PD on MUSIC) outperform the Big VGAN reconstruction on both datasets (see Figure 3(b) and (d)). We attribute the higher CLAP scores to the classifier-free guidance as it is shown to improve adherence to the conditioning [25] but at the cost of diversity—note the increasing FAD in Figure 3(a) and (c) as w increases. As such, practitioners can choose w based on their specific requirements. We use w = 1.5 as it offers a good balance between quality/diversity and relevance, and we report the results in Table 1. Models without text-audio pairs. First, we discuss CLIPSonic models in Table | that do not use text-audio pairs during training. CLIPSonic-IQ (image-queried) achieves a strong performance on both datasets. Yet, when we switch to using text queries in a zeroshot setting with CLIPSonic-ZS, we observe a performance drop in terms of FAD on both datasets. This performance drop suggests a modality gap between CLIP’s image (used during training) and text (used during inference) embedding spaces. In contrast, with the pretrained diffusion prior model, CLIPSonic-PD achieves a lower FAD than CLIPSonic-ZS across different w values (see also Figure 3). To further investigate this, we report in Table 2 the average cosine similarity between the query embedding (diet OF Qimg) and the ground truth CLIP-image embedding qimg. We note that CLIPSonicZS leads to a low cosine similarity, which supports our hypothesis that there is a modality gap in CLIP’s embedding space. In contrast, CLIPSonic-PD achieves a significantly higher cosine similarity, showing that the pretrained diffusion prior model can effectively bridge the modality gap. Moreover, while we observe a lower CLAP score for CLIPSonic-PD on MUSIC, we observe little difference in the relevance criterion in the listening test to be discussed in Section 4.2 (see Table 3), suggesting that all these models have passed a --- --Table 1: Evaluation results on VGGSound and MUSIC datasets, evaluated at w = 1.5. Without Query modality VGGSound MUSIC Model . we text-audio pairs Training Inference FAD | CLAP scoret FAD {| CLAP score t CLIPSonic-IQ (image-queried) - Image Image 2.97 - 4.71 CLIPSonic-ZS (zero-shot modality transfer) v Image Text 3.43 0.258 19.30 0.CLIPSonic-PD (pretrained diffusion prior) ov Image Text 3.04 0.265 13.51 0.CLIPSonic-SD (supervised diffusion prior) x Image Text 2.37 0.234 12.13 0.CLIP-TTA x Text Text 2.26 0.292 9.39 0.CLAP-TTA x Text Text 2.58 0.296 10.92 0.BigVGAN mel spectrogram reconstruction - - - 0.60 0.204 6.21 0.Table 2: Cosine similarities between various query embeddings. Table 4: Listening test results for image-to-audio synthesis (MOS). Model Similarity computed VGGSound MUSIC CLIPSonic-ZS sim(Qtext, Gimg) 0.205 0.CLIPSonic-PD sim (img, img) 0.647 0.CLIPSonic-SD sim (img, img) 0.711 0.Model Fidelity Relevance CLIPSonic-IQ (image-queried) 3.29+0.16 3.80+0.SpecVQGAN [19] 215+40.17 2.54+ 0.IM2Wav [20] 219+0.15 3.90 + 0.Table 3: Listening test results for text-to-audio synthesis (MOS). VGGSound MUSIC Fidelity Fidelity CLIPSonic-ZS 2.55 + 0.22 2.01 + 0.27 2.98 + 0.23 3.87 + 0.CLIPSonic-PD 3.04 + 0.20 2.86 + 0.25 3.67 + 0.18 3.91 + 0.CLIPSonic-SD 2.96 + 0.21 3.49 + 0.28 3.36 + 0.20 4.07 + 0.3.78 £0.19 3.54 + 0.29 3.90 + 0.17 4.34 + 0.Model Relevance Relevance Ground truth reasonable level of audio-text relevance. Models using text-audio pairs. We now compare the baseline models that do use text-audio pairs for training against the previous CLIPSonic variants. First, we see that CLIPSonic-SD, with a diffusion prior trained directly on the target dataset, achieves a lower FAD than CLIPSonic-PD, which uses the pretrained diffusion prior. This is possibly due to the distribution mismatch between the target datasets and the LAION-2B dataset used to train the pretrained prior.* From Table 2, we can also see that CLIPSonic-SD can generate a CLIP-image embedding closer to the ground truth embedding on the target datasets than CLIPSonic-PD. Yet, in our subjective evaluation below we will see that CLIPSonic-PD still exhibits a favorable degree of generalization to downstream datasets since it consistently outperforms CLIPSonic-ZS. Moreover, we observe a gap between the performance of CLIPSonic-PD and that of CLIP-TTA and CLAP-TTA. However, we note that this is an unfair comparison as CLIP-TTA and CLAP-TTA are trained on audio-text pairs, while CLIPSonic-PD does not use audio-text pairs in training. 4.2. Subjective Listening Test Results Text-to-audio synthesis. We conduct an ablation study to compare CLIPSonic-ZS, -PD and -SD variants on MUSIC and VGGSound. As shown in Table 3, CLIPSonic-ZS consistently underperforms, arguably because of the aforementioned mismatch between text and 4We note that there is also a mismatch in the semantics of the textual queries, where the target datasets contain audio-specific labels while LAION2B contains visually-grounded labels. However, the similar performance of CLIP-TTA and CLAP-TTA suggests that this is a minor effect. image embeddings. The two contributed variants, i.e., CLIPSonicPD and -SD, consistently achieve higher MOS than CLIPSonicZS, both in terms of relevance and fidelity. Notably, the ground truth scores are relatively low (an MOS between 3 to 4), especially noticeable for VGGSound as it is noisier than the MUSIC dataset. Image-to-audio synthesis. While our focus is to study text-toaudio synthesis, CLIPSonic-IQ can also generate audio from image queries. We compare it against SpecVQGAN [19], a representative image-to-audio model, and IM2WaAv [20], a state-of-the-art model for image-to-audio synthesis. All three models are trained on VGGSound and tested on out-of-distribution samples from IMAGEHEAR [20]. The selected samples conform a challenging benchmark for us because they are 1) selected from IM2WAv’s demo website and 2) out-of-distribution. As shown in Table 4, CLIPSonic-IQ outperforms the state-of-the-art in fidelity, while remaining competitive in terms of relevance. The improved fidelity can possibly be attributed to the fact that we use a continuous representation (mel spectrogram) with a state-of-the-art inversion model (BigVGAN), as compared to the discrete VQ-VAE representation used in IM2 WAV. 5. CONCLUSION We explored approaching text-to-audio synthesis without text-audio pairs by using unlabeled videos and pretrained language-vision models. Through both objective and subjective evaluations, we showed that the proposed models can effectively learn text-to-audio synthesis without text-audio pairs, and the pretrained diffusion prior can reduce the modality transfer gap caused by the mismatch between CLIP’s image (used for training) and text (used for inference) embedding spaces. Moreover, in a subjective listening test, the image-to-audio synthesis model that we base our modality transfer upon achieves competitive performance against a state-of-the-art image-to-audio synthesis model. Finally, we argue that images provide rich conditioning signals for audio synthesis, and leveraging such rich signals to improve text-to-audio synthesis is a promising research direction. Along this direction, CLIPSonic represents an example using videos and pretrained language-vision models. For future work, we intend to scale up the proposed method to a larger amount of videos, and explore using tri-modal audio-vision-language models [6, 37, 38]. --- ---(15] [16] 6. REFERENCES A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and IL. Sutskever, “Language Models are Unsupervised Multitask Learners,” Technical Report of OpenAl, 2019. J. Ho, A. Jain, and P. Abbeel, “Denoising Diffusion Probabilistic Models,” in Proc. NeurIPS, 2020. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in Proc. CVPR, 2022, pp. 10 684-10 695. Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, “Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation,” in Proc. ICASSP, 2023. Q. Huang, A. Jansen, J. Lee, R. Ganti, J. Y. Li, and D. P. W. Ellis, “MuLan: A Joint Embedding of Music Audio and Natural Language,” in Proc. ISMIR, 2022. A. Guzhov, F. Raue, J. Hees, and A. Dengel, “AudioCLIP: Extending CLIP to Image, Text and Audio,” in Proc. ICASSP, 2022, pp. 976-980. D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu, “Diffsound: Discrete Diffusion Model for Text-to-sound Generation,” arXiv preprint arXiv:2207.09983, 2022. F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi, “AudioGen: Textually Guided Audio Generation,” in Proc. ICLR, 2023. H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley, ““AudioLDM: Text-to-Audio Generation with Latent Diffusion Models,” Proc. ICML, 2023. R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao, “Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models,” in Proc. ICML, 2023. Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank, J. Engel, Q. V. Le, W. Chan, Z. Chen, and W. Han, “Noise2Music: Textconditioned Music Generation with Diffusion Models,” arXiv preprint arXiv:2302.03917, 2023. A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, M. Sharifi, N. Zeghidour, and C. Frank, “MusicLM: Generating Music From Text,” arXiv preprint arXiv:2302.03917, 2023. C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, P. Schramowski, S. Kundurthy, K. Crowson, L. Schmidt, R. Kaczmarezyk, and J. Jitsev, “LAION-5B: An open largescale dataset for training next generation image-text models,” in NeurIPS 2022 Datasets and Benchmarks, 2022. S. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon, “BigVGAN: A Universal Neural Vocoder with Large-Scale Training,” in Proc. ICLR, 2023. A. Nichol and P. Dhariwal, “Improved Denoising Diffusion Probabilistic Models,” in Proc. ICML, 2019. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning Transferable Visual Models From Natural Language Supervision,” in Proc. ICML, 2021, pp. 8748-8763.22)A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical Text-Conditional Image Generation with CLIP Latents,” arXiv preprint arXiv:2204.06125, 2022. A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adelson, and W. T. Freeman, “Visually Indicated Sounds,” in Proc. CVPR, 2016, pp. 2405-2413. V. Iashin and E. Rahtu, “Taming Visually Guided Sound Generation,” in Proc. BMVC, 2021. R. Sheffer and Y. Adi, “I Hear Your True Colors: Image Guided Audio Generation,” in Proc. ICASSP, 2023. H.-W. Dong, N. Takahashi, Y. Mitsufuji, J. McAuley, and T. Berg-Kirkpatrick, “CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos,” in Proc. ICLR, 2023. H.-W. Dong, G. Sigurdsson, C. Tao, J.-Y. Kao, Y.-H. Lin, A. Narayan-Chen, A. Gupta, T. Chung, J. Huang, N. Peng, and W. Zhao, “CLIPSynth: Learning Text-to-audio Synthesis from Videos Using CLIP and Diffusion Models,” in CVPR Workshop on Sight and Sound, 2023. Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, “DiffWave: A Versatile Diffusion Model for Audio Synthesis,” in Proc. ICLR, 2021. S. Pascual, G. Bhattacharya, C. Yeh, J. Pons, and J. Serra, “Fullband General Audio Synthesis with Score-based Diffusion,” in Proc. ICASSP, 2023. T. S. Jonathan Ho, “Classifier-Free Diffusion Guidance,” in NeurIPS Workshop on Deep Generative Models and Downstream Applications, 2021. H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, “VGGSound: A Large-scale Audio-Visual Dataset,” in Proc. ICASSP, 2020, pp. 721-725. H. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. McDermott, and A. Torralba, “The Sound of Pixels,” in Proc. ECCV, 2018. https://github.com/openai/improved- diffusion. https://github.com/openai/CLIP. https://huggingface.co/laion/DALLE2-PyTorch. https://github.com/lucidrains/DALLE2-pytorch. https://github.com/LAION- AI/CLAP. https://github.com/N VIDIA/Big VGAN. K. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi, “Fréchet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms,” in Proc. INTERSPEECH, 2019, pp. 2350-2354. https://github.com/gudgud96/frechet-audio- distance. S. Hershey, S. Chaudhuri, D. P. Ellis, J. F Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, et al., “CNN Architectures for Large-scale Audio Classification,” in Proc. ICASSP, 2017, pp. 131-135. H.-H. Wu, P. Seetharaman, K. Kumar, and J. P. Bello, “Wav2CLIP: Learning Robust Audio Representations From CLIP,” in Proc. ICASSP, 2022, pp. 4563-4567. A. Rouditchenko, A. Boggust, D. Harwath, B. Chen, D. Joshi, S. Thomas, K. Audhkhasi, H. Kuehne, R. Panda, R. Feris, B. Kingsbury, M. Picheny, A. Torralba, and J. Glass, “AVLnet: Learning Audio-Visual Language Representations from Instructional Videos,” in Proc. INTERSPEECH, 2021, pp. 1584— 1588. --- --A. IMPLEMENTATION DETAILS OF THE DIFFUSION PRIOR MODELS The diffusion prior models used in this paper are based on the opensource implementation of DALL-E 2 in [31]. Specifically, the input to the models is a sequence formed in the order of the encoded CLIP text tokens, the CLIP text embedding, the diffusion step embedding, the noised CLIP image embedding, and a learnable final input embedding. This sequence is fed to a 12-layer transformer consisting of causal multi-head self-attention and feed-forward networks. The last layer’s final output vector corresponding to the final input embedding serves as the prediction of the target CLIP image embedding. For the diffusion prior model used in CLIPSonic-SD, we use a cosine noise schedule with 1000 diffusion steps during training, and 64 steps at inference time. At each diffusion step during training, we minimize the mean squared error between the predicted and the target CLIP image embeddings. Based on DALL-E 2 [17], we also explore the classifier-free guidance for training the diffusion prior models by randomly replacing the encoded text tokens and the CLIP text embedding with learnable placeholders 10% of the time. However, at inference time, we empirically find that using no guidance yields the best results. At inference time, for each CLIP text embedding, we generate two CLIP image embeddings from the diffusion prior model, and select the one with a higher cosine similarity to the CLIP text embedding. To train the model, we use the AdamW optimizer with a learning rate of 0.0001, a batch size of 32, a weight decay of 0.06, and we apply an exponential moving average on the model parameters with a decay factor of 0.9999. The diffusion prior models in CLIPSonic-SD are trained on MUSIC and VGGSound independently until convergence at around 200k steps. B. CLAP SCORES FOR BIGVGAN RECONSTRUCTIONS In Figure 3, we observe that the CLAP scores of the BigVGAN reconstruction using the ground truth mel spectrogram, in many cases, are lower than those of the proposed systems, which indicates lower relevance between the ground truth audio and the text query. In order to adhere to the length of the test data, the BigVGAN CLAP scores are obtained based on the entire 10-sec audio samples. However, empirical listening finds that some segments within the 10-sec samples correspond poorly to the text queries. To further investigate the correspondence, We also compute the BigVGAN CLAP scores using a 4-sec sliding window (consistent with the synthesized sample length) with a hop size of 0.5 sec, and report the maximum, mean, and the minimum scores over all 4-sec segments within a 10-sec sample as the overall score of that sample. As shown in Table 5, the maximum scores on both datasets are higher than the rest, which supports our observation by listening. On VGGSound, the maximum CLAP score also exceeds those of CLIPSonic-ZS, CLIPSonic-PD, and CLIPSonic-SD (see Table 1). On MUSIC, there is a smaller gap between the maximum CLAP score and that obtained using the entire 10-sec audio, indicating a more uniform relevance level within a sample. However, the studied models trained on MUSIC still outperform the BigVGAN reconstruction in terms of the maximum CLAP score (except for CLIPSonic-PD, and CLIPSonic-ZS without using the classifier free guidance, see Figure 3). In addition to the contribution of the classifier free guidance (Section 4.1), the remaining reason requires further investigation. Possible directions include manually inspecting and removing samples with poor audio-text correspondence, and also finetuning CLAP on MUSIC. Table 5: CLAP scores computed on Big VGAN reconstructions using a sliding window. Window size Mode VGGSound MUSIC 4 sec Max 0.273 0.4 sec Mean 0.195 0.4 sec Min 0.111 0.10 sec - 0.204 0.C. LIMITATIONS We observe some limitations of the proposed method. First, as CLIPSonic is conditioned on the CLIP embedding of a single video frame, it is not readily applicable to handle more complex text queries that involve sequences of events or dynamic interactions between objects. A more powerful language-vision model that can understand videos is required to apply our proposed method to leverage the rich temporal information in videos. Second, since the conditioning signals are extracted from videos, CLIPSonic cannot learn audio concepts that have little meaning in the visual domain, such as pitch, prosody, genre, and tempo. This represents one of the fundamental limitations of approaches that use the visual domain as a bridge to learn the text-audio correspondence. Finally, CLIPSonic offers limited controllability in generating semantically complex audio, such as speech or music given specific words or scores, respectively. However, the proposed method may serve as a pretraining approach for training language-audio models, where we can first pretrain a language-audio model on a large dataset with only unlabeled videos and later finetune the model on a small dataset with audio-text pairs.