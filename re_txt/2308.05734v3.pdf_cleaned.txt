arXiv:2308.05734v3 [cs.SD] 11 MayAudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, Mark D. Plumbley Abstract Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called “language of audio" (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform selfsupervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, textto-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 framework against previous approaches. Our code, pretrained model, and demo are available at https://audioldm.github.io/audioldm2. Index Terms-audio generation, diffusion model, selfsupervised learning, speech synthesis, AIGC A I. INTRODUCTION RTIFICIAL intelligence generated content (AIGC) refers to any digital content such as images, videos, text, or audio that has been fully or partially created by an AI system without human involvement in the creative process [1]. Of particular interest is the ability of AI to produce audio content based on text, phonemes, or images [2]–[4]. AI-based audio generation has a wide potential in applications including synthesizing human or artificial voices for digital assistants [5], generating sound effects and background music for movies, and games [6], and automating the production of podcasts and audiobooks [7]. AI-based audio generation is often undertaken in separate sub-domains, such as the generation of speech [2], music [8], sound effects [4], and specific types of sounds such as footsteps and violin sounds [9], [10]. To address the specific Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Wenwu Wang, and Mark D. Plumbley are with the Centre for Vision, Speech and Signal Processing (CVSSP), University of Surrey, Guilford, UK. Email: {haohe.liu, yi.yuan, xubo.liu, x.mei, w.wang, m.plumbley} @surrey.ac.uk. Qiuqiang Kong is with the Department of Electronic Engineering, Chinese University of Hong Kong, Hong Kong, China. Email: qqkong@ee.cuhk.edu.hk Qiao Tian, Yuping Wang and Yuxuan Wang: are with the Speech, Audio & Music Intelligence (SAMI) Group, ByteDance Inc. Email: {tianqiao.wave, kongqiuqiang, wangyuping, wangyuxuan.11} @bytedance.com. challenges in each sub-domain, most previous works design task-specific inductive biases, which are predefined constraints that guide the learning process to a specific problem space. For example, pitch and duration predictors are often used in speech synthesis to model the prosody of speech [2], [11], while MIDI representation [12] and domain-specific pre-trained modules are often used in music generation [8], [13]. Despite significant progress being made in developing specialized models for specific sub-domains of audio generation, the limitations of such specialization restrict the broader application of audio-generation models in complex auditory scenarios. Although there are models that can generate various types of audio, such as AudioLDM [4], the speech they generate is still not intelligible. Whether a unified approach can be developed to generate various types of audio signals, including intelligible speech, remains unanswered. Different types of sound can occur simultaneously in real-world cases, such as in movie scenes, requiring a more general approach to modelling audio generation. While there are works that address audio generation in a general domain, they mostly focus on generating audio with correct audio events with limited attention to detail. For example, previous text-to-audio generation research tends to generate unintelligible speech [4], [14], [15]. Moreover, while inductive biases have been useful in addressing the challenges of specific sub-domains, conclusions about a specific design drawn from one domain may not necessarily transfer to another. Recent advancements in addressing problems from a unified perspective have yielded substantial progress [16]–[19]. This trend highlights the potential of constructing a unified audio generation framework. This paper presents a novel and versatile framework, called AudioLDM 2, that can generate audio with flexible conditions, without the need for domain-specific inductive bias. The core idea is to introduce a sequence of vectors that represent the semantic information of an audio clip, which we will refer to as the "language of audio” (LOA). This approach allows us to translate human-understandable information into LOA and synthesize audio representation conditioned on LOA. This idea is similar to the use of onomatopoeia in [20] to describe environmental sounds. However, although onomatopoeia can effectively mimic certain sounds like animal noises or simple actions (e.g., "splash" for water), it can not encompass the full range of audio nuances. In theory, the "language of audio" should be able to represent both fine-grained acoustic information (e.g., "what does the speaker say") and coarsegrained semantic information (e.g., "what is that sound"). Considering these requirements, we propose to leverage thefeatures extracted by an audio masked autoencoder (AudioMAE) [21], an audio-generative self-supervised pretraining framework. An AudioMAE is pre-trained on diverse audio content, and its dual generative and reconstructive pre-training approach makes it potentially a strong option for representing audio in generative tasks. Specifically, we utilize a GPT-2 language model [22] to translate conditioning information into the AudioMAE features. We then use a latent diffusion model [23] to synthesize audio based on the AudioMAE features. The latent diffusion model can be optimized in a self-supervised manner, allowing for pre-training with large-scale unlabelled audio data. Our language-modelling approach with GPT-2 enables us to leverage recent advancements in language models [24], while alleviating challenges such as high inference computation costs and error accumulation that appeared in previous audio autoregressive models [8], [25]. The improvement is largely attributed to the shorter length of the LOA sequence. The continuous nature of LOA also potentially provides a richer representation power than the discrete tokens used in previous models [8], [13], [26]. Our experimental results demonstrate that AudioLDM 2 achieves competitive performance on text-to-audio (TTA), and text-to-music (TTM) generation tasks, when evaluated on AudioCaps [27] and MusicCaps [8], respectively. On text-to-speech (TTS) generation tasks, AudioLDM 2 achieves performance comparable with the SoTA by significantly outperforming a strong baseline FastSpeech2 [11]. In comparison to the original AudioLDM [4], AudioLDM 2 contains a latent diffusion model that can be pretrained in a self-supervised manner, and enjoy the benefit of auto-regressive modeling of LOA with GPT-2 model. Besides, while retaining the same ability, AudioLDM 2 shows substantial advancements over AudioLDM in quality, versatility, and capacity to generate speech with intelligible content. Overall, our contributions are as follows: • We propose a novel and versatile audio generation model that is capable of performing conditional generation of audio, music, and intelligible speech. The proposed method is based on a universal representation of audio, which enables large-scale self-supervised pretraining of the core latent diffusion model without audio annotation and helps to combine the advantages of both the auto-regressive and the latent diffusion model. • Our experiments shows three variants of AudioLDM 2 achieves performance that match current state-of-theart (SOTA) in text-to-audio, text-to-music, and text-tospeech generation on AudioCaps [27], MusicCaps [8], and LJSpeech [28] evaluation set, respectively. II. RELATED WORK A. Conditional Audio Generation Audio generation is an emerging topic that focuses on modelling the generation of general audio, including recent models such as AudioGen [3], AudioLDM [4], and Make-anAudio [15]. AudioGen treats audio generation as a conditional language modelling task, while the other two works approach this task by latent diffusion. Studies on image-toaudio and video-to-audio generation, such as Im2Wav [29] and SpecVQGAN [30], are also areas of interest to researchers. Additionally, there are audio generation approaches that do not rely on conditioning, such as AudioLM [26], which performs audio language modelling based on a neural codec. Even though audio generation usually includes the topic of speech generation, previous works on text-to-audio generation tend to generate unintelligible speech [3], [4], [14], [15]. The field of audio generation encompasses sub-domains such as text-to-speech (TTS) and text-to-music (TTM). The former focuses on generating speech signals from transcriptions, while the latter involves creating a music clip from a textual description. Cutting-edge TTS models like FastSpeech2 [11], GradTTS [31], and NaturalSpeech [2] have made significant strides, producing speech of such high quality that it is nearly indistinguishable from human speech. Various techniques have been introduced to address speech generation in TTS, such as the monotonic alignment algorithm [32], which aligns phoneme features with spectrogram features, and a prosody predictor [11], used to guide model training and enhance expressiveness. Recent advances in TTM are evident in models like MusicLM [8], Noise2Music [33], MusicGen [34], and MeLoDy [13]. Similar to AudioLDM, the MusicLM model aligns music and language embeddings through contrastive pretraining modules, which enables textfree model optimization, alleviating the scarcity of musictext pairs. MusicLM also includes a semantic modeling stage based on w2v-BERT [35] to enhance the model performance. MusicGen uses a language modeling approach for music generation, enhanced with a mechanism for conditioning the model with melodic features for improved controllability. Meanwhile, MeLoDy, a diffusion model guided by language modeling, achieves significant computational reduction in music generation compared to MusicLM. In this paper, we propose a unified framework for audio generation, which encompasses a breadth of topics including, but not limited to, speech, sound effects, and music generation. B. Diffusion Models Diffusion models [36], [37] have demonstrated high sample quality in a variety of tasks including image generation [38][40], image restoration [41], speech generation [42]–[44], and video generation [45], [46]. In the realm of speech or audio synthesis, these models have been explored for both mel-spectrogram generation [31], [47] and waveform generation [48]-[50]. However, the iterative nature of generation in a high-dimensional data space often results in slow training and inference speeds. One solution involves the use of diffusion models in a more restricted latent space, a strategy exemplified in image generation [23]. This idea has been adopted in various audio generation works, including AudioLDM [4], Make-AnAudio [15], and TANGO [51]. These works utilize latent diffusion models trained on a continuous latent space. On the other hand, there are also studies that explore diffusion in the discrete latent space. For instance, DiffSound [14] employs a discrete autoencoder to mitigate redundancy [52], [53] in the Language of Audio Calculation / Prediction A: x Y or M: CY Language of Audio (LOA) YX or Ŷ Self-supervised Pretrained Generation Model GYx or G: Yxx **** Ꮖ Reshaped Patches Pooled I ConvBlock Patches ε~N(1,0) •Z₁=√α+Zq+(1−α+)ε A=Language of Audio (LOA) Universal Vocoder AudioMAE Freq➡ K V Q Transformer Block Encoder Υλ Pgt xn Time VAE Decoder Ŷ A(): Audio to LOA Encoder Ppred Train Q Zo Transformer Block Infer GPT-☐ ☐ ☐ ☐ ☐ VAE Encoder E Linear Projection Heads Prob. Switcher AudioMAE Features | CLAP IPhonemes Infer 2-1-G(Ź₁, ε) Train L(E,E) FLAN-T5 || Mel FilterBank STFT Encoder Transformer-UNetAudio Text Transcription M() Any Modality to LOA Translator *+*+ x G(): LOA to Audio Generator Fig. 1. The overview of the AudioLDM 2 architecture. The AudioMAE feature is a proxy that bridges the conditioning information to LOA translation stage (modelled by GPT-2) and the LOA to audio generation stage (modelled by the latent diffusion model). The probabilistic switcher controls the probability of the latent diffusion model using the ground truth AudioMAE (Pgt) and the GPT-2 generated AudioMAE feature (P pred) as the condition. Both the AudioMAE and latent diffusion models are self-supervised pre-trained with audio data. audio waveform and create a compressed representation of mel-spectrograms. DiffSound utilizes text-conditional discrete diffusion models to generate discrete tokens. A. Overview III. AUDIOLDMLet x = R¹s represent an audio signal, where L¸ is the length of the audio samples in x. An audio generation process can be denoted as H : C ⇒ x, where C is the conditioning information and H is the conditional audio generation system. The direct generation of x from C is usually challenging [54]. Motivated by regeneration learning [55], we propose to utilize an intermediate feature Y, as an abstraction of x, to bridge the gap between C and x, as introduced in Section III-B1. We call the feature Y the language of audio (LOA). The LOA feature is calculated by Y = A(x) in which A performs audio to LOA encoding with a self-supervised representation learning module such as AudioMAE [21], [55]. The LOA feature should be a representation that is potentially easier to model compared with x and contain meaningful semantic information about x. As illustrated in Figure 1, with the intermediate representation Y, the overall audio generation process can be denoted as Ho= GoM:CHYH2, (1) where Ŷ is the estimation of the ground truth LOA. As denoted in (1), the audio generation process of AudioLDM 2 includes the following two steps: : (i) Conditioning information to LOA translation: The function M C Ŷ aims to produce the LOA Y based on C, which could be the conditional information from other modalities, such as audio and text. As a potentially better representation of C in terms of audio generation, the generated Ŷ will be used in later stages as the conditioning information for audio generation. We implement the function M with autoregressive modelling, which is introduced in Section III-C. (ii) LOA to audio generation: Followed by M, function G accepts an LOA estimation Ŷ as input condition and estimates the audio data x. During the training process, when the training data x is available, the ground truth Y will be also available using A(), allowing the optimization of G in a self-supervised M(C), we condition the generation of a based on the Y manner. Specifically, instead of using the LOA estimation Ŷ : A(x), which can be formulated as H₁ = Go Ax ⇒ Y ⇒ î. (2) We introduce the detail of A(.) in Section III-B. Since the process H₁ only involves x as the training data, Equation (2) means model G can be optimized in a self-supervised manner without any audio annotation. This self-supervised scheme can alleviate the scarcity of the audio data labels [4] and provide a robust backbone for the overall generation system. Note that the self-supervised learning here does not refer to the entire AudioLDM 2, for example, the function M still needs paired data to optimize. We implement the function G with the latent diffusion model, which is introduced in Section III-D. The following sections provide a detailed introduction to AudioLDM 2. In Section III-B, we discuss the audio representations employed in AudioLDM 2, including the AudioMAE and VAE features. These features also serve as the generation targets for the two stages within AudioLDM 2. Section III-C introduces the auto-regressive modeling of the AudioMAE feature with GPT-2. In Section III-D, we elucidate the process of generating audio waveforms via the latent diffusion model, which applies a VAE for feature compression and generates audio conditioned on the LOA. The LOA here can be basedon either ground truth or GPT-2-generated data, which corresponds to self-supervised training and joint training with GPT2 (Section III-D3), respectively. B. Audio Representation Learning Motivated by MusicLM [8] and AudioLM [26], which perform semantic and acoustic modelling on two types of discrete representations [25], [56], we adopt a similar two-stage modelling approach. However, our work differs in that we work on the continuous semantic and acoustic representation, which can potentially provide richer information compared with discrete representations used by previous studies [25], [35]. In our work, we adopt AudioMAE [21] and variational autoencoder (VAE) [57] as the semantic and acoustic representation learning modules, respectively. Despite serving similar purposes, AudioMAE and VAE differ in architecture and objectives, yielding distinct representations. Further details on representation learning are provided below. 1) Semantic Representation Learning with the AudioMAE: To accurately represent diverse types of audio, encompassing speech, music, and sound effects, the LOA Y should effectively capture both the semantic and the acoustic details of audio signals. Therefore, we propose to use a self-supervised pretrained AudioMAE [21] as the representation extraction module for function A for its generality and high accuracy on the downstream audio classification task [21]. The audio masked autoencoder (AudioMAE) is an audio self-supervised pre-training model, which learns representations from unlabeled audio data without relying on manually labeled annotations. An AudioMAE consists of an encoder and a decoder, both realized with an architecture similar to the vision transformer (ViT) [58]. During self-supervised pretraining, input patches to the encoder, which are usually mel spectrograms, are randomly masked and the decoder learns to reconstruct the masked patches [21]. Compared with other audio self-supervised pretraining models, AudioMAE has the following two advantages: (i) The AudioMAE has been verified to work well in the general audio domain. For example, an AudioMAE can be effectively pre-trained on AudioSet [59], with state-of-the-art performance on the downstream audio classification tasks. In comparison, typical audio self-supervised models focus on a specific domain, such as the MERT [60] on music and the HUBERT [61] on speech. (ii) AudioMAE features are potentially better for generative tasks than other discriminative pre-training methods. Building upon the contrastive loss or next token prediction classification loss as the learning objective, previous systems such as wav2vec [62] and BYOL-A [63] utilize a discriminative approach during pre-training. In comparison, AudioMAE focuses on a generative process by learning the reconstruction of the masked patches. For an input audio signal x, AudioMAE first calculates the log mel spectrogram X = RTXF, where T represents the time steps of the mel spectrogram, and F denotes the mel bins. The mel spectrogram X is then treated as an image and split into patches each of size P × P, serving as the inputGroundTruth Reconstruction λ =1.54.7.1.4.7.Time Time Reconstruction λ =Reconstruction λ =4.7.1.4.7.Time Time Fig. 2. The influence of λ on audio reconstruction from LOA YX with the latent diffusion model. The reconstruction closely resembles the ground truth when = 1, suggesting that YX=1 retains sufficient audio details. However, with 2 or 4, the reconstruction diverges slightly from the original audio, indicating that while the post-processed AudioMAE feature may not include all details, it nonetheless accurately preserves semantic content. for the AudioMAE encoder. The patch size P is typically designed to be a common factor of T and F. Patch splitting and embedding are performed using a convolutional neural network with a kernel size of P, a stride of P, and D output channels. This yields an output shape of T' × F' × D, where D is the AudioMAE embedding dimension, T' = T/P, and F' F/P. The resulting output feature of the AudioMAE encoder, E = RT' F'XD, has the same shape as the input and is usually treated as the feature for downstream tasks after pretraining [21]. = 2) AudioMAE Feature Post Processing: As shown in Figure 1, once the AudioMAE features E are computed, we introduce an additional pooling step to aggregate E into YX, where I+ represents a hyper-parameter used in the postprocessing pooling step. This pooling step aims to reduce the sequence length, facilitating easier estimation in the function M. Specifically, we perform a two-dimensional average-max pooling [52] on the first two dimensions of E = RT' F³×D, in which the pooling kernel size and stride have the same value A Є I+. The two-dimensional pooling operation can help to preserve the time-frequency relationship in the output. The final output after pooling, YX, is reshaped into a embedding sequence with shape Lx × D, in which L T'F' /X². To facilitate implementation, \ is chosen so that L、 is always a positive integer. We demonstrate the effect of different choices of \ in Figure 2. In the remaining sections of this paper, if \ is not specified, we'll refer to Y simply as Y. 3) Acoustic Representation Learning with VAE: We use a VAE for feature compression and for learning an audio representation 2, which has a significantly smaller dimension thanx [4]. The VAE we used in this work is a convolutional architecture that consists of encoders with down-sampling and decoders with up-sampling following the architecture described in [4]. The forward pass of the VAE can be formulated as V: XZ X, where X is the mel-spectrogram of x and ✰ is the reconstruction of x. The reconstruction ✰ can be converted to the audio waveform ✰ using a pretrained HiFiGAN vocoder [64]. Following AudioLDM [4], we calculate a reconstruction loss and a discriminative loss based on X and Ŷ to optimize the parameters of the VAE. We also calculate the KL divergence between z and a standard Gaussian (µ = 0, σ² (µ = 0, σ² = 1) as a 1) as a loss function to limit theVAE latent space visualized AudioMAE latent space visualized Brushing teeth Clapping Engine Hand saw Pouring water Sea waves Sheep Snoring Toilet flush Train Brushing teeth Clapping Engine Hand saw Pouring water Sea waves Sheep Snoring Toilet flush Train Fig. 3. Visualization of the latent space based on tSNE and ten randomly selected classes in the ESC50 [65] dataset. Each point in the figure represents an audio clip. The AudioMAE feature space tends to group similar audio clips together, indicating more semantic structure than in the VAE feature. variance of the VAE latent space. 4) Comparison between AudioMAE and VAE: Since both AudioMAE and VAE are based on autoencoders for representation learning, one might wonder why we use a VAE for representation learning instead of directly modeling the AudioMAE latent space. Part of the reason is that AudioMAE does not primarily focus on reconstruction quality, and its latent space compression ratio is not as high as that of the VAE. On the other hand, the VAE exhibits good reconstruction ability and a higher compression level than AudioMAE, making VAE more suitable for mel-spectrogram compression. Furthermore, as shown in Figure 3, we visualize the latent representation of AudioMAE and VAE on the ESC-50 [65] dataset using tSNE [66]. The visualization demonstrates that the latent representation of AudioMAE can group similar audio at a closer region in the latent space. In contrast, the representation of VAE exhibits more overlap between different audio classes. This indicates that the representations for the AudioMAE and VAE are distinct. AudioMAE contains more information on the semantic side, while VAE representation is less semantically structured. Therefore according to the definition of LOA in Section III-A, AudioMAE is more suitable than VAE on calculating the LOA. C. Conditioning Information to LOA Translation with GPT-This subsection introduces the design of the function M. As introduced in Section III-A, the input to the model G : Y → x can be calculated using the AudioMAE. However, during inference, when we perform audio generation with the condition C', the ground truth LOA Y A(x) is unavailable. Therefore, = we need another model that can generate Ŷ given C, denoted by Me: CY, where 0 represents trainable parameters. Specifically, we treat the generation of Y as a language modelling task and choose the GPT-2 (Generative Pre-trained Transformer 2) [22] model as the backbone. GPT-2 is based on a transformer architecture and was originally trained on 8 million documents for a total of 40 GB of text using an unsupervised learning approach [22]. GPT-2 has been used in a variety of natural language processing tasks, such as text completion, question answering, and language translation [67], [68]. Initialized with pre-trained weights, we finetune the GPT2 model based on teacher forcing [69], so that during model training, ŷɩ will be generated based on both the condition C and the ground truth sequence Y1, ..., Yl-1, where yɩ is the l–th vector in LOA sequence Y. Specifically, the GPT2 model Me is trained to maximize the likelihood of a sequence Pr(y1, 2, ..., YL|C), which can be interpreted into the following optimization objective: L argmax, Ec [Pr(y1|C; 0) [[ Pr(yı|Y1, ..., Yl−1, C;0)], (3) 1== where Ec represents the expectation operator with respect to the variable C. We calculate the mean squared error loss [54] between yɩ and ŷ Mo(y1, ..., Yl−1, C) to optimize Equation (3). We directly optimize the regression of continuous vectors y₁, without discretizing the AudioMAE feature space and estimating the token index. The condition C in Equation (3) can encompass a flexible range of data representations, including audio representations, text embeddings, phoneme embeddings, or visual clues. We adopt the mixture of experts [70] approach and use multiple encoders as feature extractors to calculate C. Given K systems as the feature extraction modules, the shape of the output from the k-th system Ck, k = {1, ..., K} is Lk × Dk, in which Lk is the sequence length of the k-th system and Dk is the dimension of the feature. We apply a linear transformation layer after the output of each feature extraction module to unify the embedding dimension to Do for an easier process of the GPT-2 model. For modules that extract global features from the input without sequential information, such as CLAP [71] or ImageBind [18], we have Lk = 1. The final condition C = [C₁,...CK] is a concatenation of Ck along the sequence length dimension. The final condition C has a shape of L× Do, where L=1Lk. We introduce several condition modules we used in this paper as follows. K CLAP or contrastive language and audio pretraining [71], is a system that learns a joint audio-text embedding space, in which paired audio and language data have a closer distance in the latent space. CLAP has been successfully applied as a conditioning module to audio generation such as AudioLDM [4]. In this study, we employ a pre-trained CLAP¹ text encoder as the default conditioning module for extracting text embeddings as conditions. However, in scenarios where text captions (e.g., "A man is speaking happily with background static noise") are unavailable, such as for text-to-speech tasks, we use the CLAP audio encoder as the conditioning module instead of using CLAP text encoder, in the same way as [4]. https://github.com/LAION-AI/CLAPFLAN-T5. The CLAP model, as a module that calculates global-level conditions, has been found to have issues in capturing the temporal information in the text data [72]. To allow for this, we use another pretrained text encoder to capture the semantic information of the textual input, which might contain useful details such as temporal orders. Specifically, we utilize FLAN-T5 [73], which is an enhanced version of the text-to-text transfer transformer (T5) model [74] based on the finetuning on a mixture of tasks². Phoneme Encoder is a widely adopted module in text-tospeech research for extracting helpful information regarding phonemes [2], [11], which are the smallest units of sound in a language that can distinguish one word from another [75]. In this work, we follow the structure introduced in NaturalSpeech [2] to build a phoneme encoder, in the form of a stack of transformer encoder layers. We preprocess the textual input into phonemes using the open-source tool Espeak phonemizers and append a stop token after each phoneme sequence to mark the end of the sequence for the transformer model.Except for the phoneme encoder, which does not have a readily available pre-trained weights, the parameters of all other pre-trained feature extraction models are kept frozen during the experiment. D. LOA to Audio Generation with Latent Diffusion Model We model the process G : Y x with a latent diffusion model (LDM) [23], which is a variant of the denoising diffusion probabilistic models (DDPM) [36]. In contrast to DDPM, which directly models the training data, the LDM learns the reverse diffusion process in a variational autoencoder (VAE)based compressed latent space [76], which can reduce the computational cost. Similar ideas have been adapted to audio generation, such as AudioLDM [4]. 1) Latent Diffusion Model: We follow the formulation in [36] to implement the LDM. Given a VAE representation z, the forward transition is defined as a T steps Markov process in a way that does not include trainable parameters. Given the data zł−1 at diffusion step t - 1, the data distribution of zt at step 2, ..., T can be formulated as q(zt|zt−1) = √1 − ßt¾t−1 + √ßt€t, in which the noise schedule hyper-parameter ẞt Є [0,1] determines how quickly the noise is blended into the data. By recursive substitution of q(zt|zt-1) in Equation (4) [36], we can derive the distribution of zt given zo as 9(zt|20) t=atЄt, (5) where at = П11 ẞt and et ~ N(0, 1). At the final step t = T, the distribution of z will be close to a standard Gaussian distribution [36]. The LDM learns a backward transition from the prior distribution N(0, 1) to the data distribution z. The reverse process models the conditional distribution Pr(zo...T|Y; 0) = 2https://huggingface.co/google/flan-t5-large https://github.com/espeak-ng/espeak-ng =Pr‍(z0|21, Y; 6) II— ±2 Pr(zt−1|zt, Y; ó) · Pr(zÃ), in which Y is the LOA as the condition signal and the > denotes the parameter of the model for learning the reverse diffusion. If we marginalize 21...T we can derive the lower bound of log[Pr(z0|Y; 0)] based on the evidence lower bound (ELBO) and Bayes' rule [36]: log[Pr(zo|Y; ø)] ≥ log[Pr(z0|21, Y; 0)]— T Σ KL[Pr(zt-1 zt, Y; 0)||9(Zt−1|Zt, 20)], (6) t=where KL() is the function for calculating KL divergence, and q(zt-1 t, 20)) is the target conditional diffusion distribution that has a closed-form solution given zo and zł [36]. Following [36], we can derive the loss function that maximizes the lower bound of Equation (6) as: argmin [Ezo, Y, t~{1,...,T} ||G(√√αtzo±√√1 – at€t, t, Y; ø)—et ||]. (7) As shown in Figure 1, we utilize a Transformer-UNet (T-UNet) architecture as the function G in Equation (7), which is similar to the UNet used in AudioLDM [4] but with more transformer layers. The T-UNet architecture consists of a series of encoders with downsampling and a series of decoders with upsampling, and there are skip connections between encoders and decoders at the same scale. To enhance the modelling capacity of the T-UNet, we insert multiple transformer blocks after the convolution operation in each encoder and decoder block. Specifically, we have trans + 1 transformer blocks, in which the first trans transformer blocks are a stack of self-attention layers [77] and feed-forward networks. To incorporate the condition information Y from the ground truth LOA or Ŷ from M() (Section III-C), as shown in Figure 1, the last transformer block changes the self-attention layer to crossattention, which accepts the LOA as key and value and fuses with the feature from the previous transformer block as the query. Except for text-to-speech generation, we add an extra cross-attention layer in the transformer block to accept the text embedding from FLAN-T5 [73] as an extra condition to enhance the audio-text relationship learning. w = 2.w = 3.w = 4.w = 1.Fig. 4. The samples generated with different classifier-free guidance scales. The text prompt is "A cat is meowing". 2) Classifier-free Guidance: For diffusion models, controllable generation can be achieved by introducing guidance at each sampling step. Classifier-free guidance [78], [79] (CFG) has been the state-of-the-art technique for guiding diffusion models. During training, we randomly discard our condition Y in Equation (7) with a fixed probability (e.g., 10%) to train both the conditional LDMs G(zt, t, Y; 6) andthe unconditional LDMs G(zt, t, o). For generation, we use LOA Ŷ or Y as the condition and perform sampling with a modified noise estimation G' (zt, t, Y; ø): G' (zt, t, Y; 0) = wG(zt, t; ø) + (1 − w)G(zt, t, Y; 0), (8) where w determines the guidance scale. 3) Joint Finetuning: We perform joint finetuning with the GPT-2 and latent diffusion models based on Equation (1), (7), and (3). As demonstrated by Table V, we found that joint finetuning significantly enhances the overall performance of the AudioLDM 2 system. As depicted in Figure 1, the probabilistic switcher controls the source of the conditioning signal during the joint training process. During training, the switcher dynamically chooses between ground truth AudioMAE features and GPT-generated AudioMAE features, with probabilities set to Pgt and Ppred, respectively. A. Dataset IV. EXPERIMENT SETUP The datasets used in this work include AudioSet (AS) [59], WavCaps [80], AudioCaps (AC) [27], VGGSound (VS) [81], Free Music Archive (FMA) [82], Million Song Dataset (MSD) [83], LJSpeech (LJS) [28], and GigaSpeech (GGS) [84]. AudioSet is the largest audio classification dataset at the time of writing, with around two million ten-seconds of audio and 527 different classes. WavCaps is a dataset with ChatGPT-assisted weakly-labeled audio captions. WavCaps contains 403,050 audio clips with an average duration of 68 seconds. AudioCaps is a subset of AudioSet with handcrafted captions, containing about 46,000 ten-second audio clips. VGGSound is a large-scale single-label audio-visual dataset, which contains over 200,videos. We only utilize the audio data and the labels in the VGGSound. FMA is a large music dataset without captions, containing 106,574 music tracks from 16,341 artists and 14,854 albums. For the Million Song Dataset, we only utilize the labelled subset proposed in [85], which contains around 510,000 music tracks with metadata such as tags, titles, and artist names. LJSpeech is a single-speaker speech dataset with 13,100 short audio clips and detailed transcriptions. GigaSpeech is a multi-speaker large-scale English speech recognition corpus with around 10,000 hours of audio labeled with transcriptions. The test and development set of GigaSpeech are not included during training. All the audio data used in this work are resampled to 16 kHz for easier comparison with previous works [4], [15]. We use only the audio data with paired text labels to train the GPT-2 model by optimizing Equation (3). We train the latent diffusion model with all the audio data regardless of annotation by optimizing the objective in Equation (6) in a self-supervised manner. B. Evaluation Metrics We mainly focus on the text-to-audio generation task to evaluate the effectiveness of AudioLDM 2. We follow the evaluation protocol of AudioGen [3], which calculates both objective metrics such as Frechet Audio Distance (FAD), Kullback-Leibler Divergence (KL), and subjective metrics including Overall Impression (OVL) and Audio and Text Relation (REL). We also include an additional metric CLAP score [15] to measure the correspondancy between the generated audio and text prompt. FAD is a reference-free audio quality measure that is calculated based on the distribution distance between the feature of the target and generated audios, extracted from the VGGish [86] model. KL divergence measures the similarity between the generated and target audio with the label calculated by the audio tagging model, Patch-out Transformer [87], in the same way as AudioGen [3]. CLAP score measures the similarity between audio and text based on a pair of pretrained audio and text encoders [71], given by exer CLAPScore (x, r) (9) max (||ex||||er||, €) where x and r denote audio and text data, respectively, e is a small value that can avoid zero division, ea is the output of the CLAP audio encoder and ĕr is the output of CLAP text encoder. The value range of the CLAP score is between -1 and 1 and a larger value indicates a stronger correlation between audio and text information. = , We use a similar evaluation protocol for text-to-music generation. For the text-to-speech task, we utilize the commonly used mean opinion score (MOS) for evaluation [75]. C. Subjective Evaluation We use Amazon Mechanical Turkª, a crowd-sourced platform, to evaluate subjective metrics including OVL, REL, and MOS. The instructions on how to perform evaluation are clearly illustrated for the raters with examples. Specifically, for OVL, raters were asked How would you rate the overall quality of this music? Consider its resemblance to real-world audio and its naturalness, with a five-point scale ranging from 5Excellent quality to 1-Bad quality. Similarly, for REL, the question posed was, How would you rate the relevance of music to the text description? with a similar five-point scale for responses. In evaluating MOS, the question was, How natural does this recording sound? Take into account emotion, prosody, and other human-like details, with options ranging from completely unnatural speech to perfectly natural speech. To ensure the credibility of the evaluation result, we set requirements for the crowd-source worker with a minimum average approval rate of 60% and with at least 50 approvals in the record. Each audio clip is evaluated by at least 10 different raters. All three subjective metrics have a Likert scale [88] between one and five, where a larger number indicates better performance. Study raters received payment at or above the US minimum wage. We average the scores among all raters and samples as the final score for a system. D. Model Architecture Details We perform the experiment with two sizes of the latent diffusion model, AudioLDM 2 and AudioLDM 2-Large, with transformer layer numbers trans = 2 and trans 6 (Section III-D), respectively. We use a pre-trained AudioMAES 4https://requester.mturk.com/ Shttps://github.com/facebookresearch/AudioMAE =with a patch size of 16 × 16 and no overlapping, resulting in a 768-dimension feature sequence with length 512 for every ten seconds of mel spectrogram. In a similar way to the idea introduced in [89], on calculating the LOA Y, we gather the output of the last 16 transformer layers from the AudioMAE encoder and perform averaging as the final Y. We perform self-supervised pre-training on both AudioLDM 2 and AudioLDM 2-Large with the audio data mentioned in Section IV-A. The GPT-2 model we employ has an embedding dimension of 768 with 12 layers of transformers. For joint fine-tuning, we set the probability of using ground truth LOA Y and LOA estimation Ŷ as Pgt 0.25, and Ppred 0.75, respectively. TABLE I = = THE SETUP OF THE PRIMARY EXPERIMENTS WE PERFORMED. FULL REPRESENTS A COMBINATION OF FIVE DIFFERENT DATASETS, INCLUDING AC, AS, WC, VS, AND MSD. THE MODEL WITH † USE CLAP TEXT ENCODER AND FLAN-T5 TO CALCULATE CONDITIONS WHILE THE MODEL WITH USES CLAP AUDIO ENCODER AND THE PHONEME ENCODER AS THE CONDITIONAL MODULES. Model λ Param Dataset AudioLDM 2-AC+ 8 346M AudioLDM 2-MSD+346M AC MSD AudioLDM 2-Full+346M FULL Task TTA TTM TTA/TTM AudioLDM 2-AC-Large+ AudioLDM 2-Full-Large+ AC 8 712M FULL 1 346M LJS AudioLDM 2-LJS-Pretrained 1 346M LJS+GGS712M TTA TTA/TTM TTS TTS AudioLDM 2-LJS‡ Table I summerize the experiments we performed in this paper. For the generation of audio and music, we combine the text embeddings from the CLAP text encoder and FLAN-Tas conditioning and designate YX=8 as the target sequence for GPT. The conditioning modules for speech generation are configured differently, primarily due to the need to better preserve the fine-grained phoneme information in speech signals through a smaller \ value. Thus, for speech generation, we concatenate the output of CLAP audio encoder and the phoneme encoder as the input sequence of the GPT-2 model, and designate YX=1 as the target sequence to retain more details. For the speech data, since there are no available audio captions (different from transcriptions), we adopt a similar approach as AudioLDM [4] to utilize the CLAP audio encoder to compute the embedding as a condition during model training, and employ the CLAP text encoder during inference. This method also facilitates prompt-based speaker control, as demonstrated in Figure 6. E. Training and Inference Setup The latent diffusion model and the GPT-2 model are initially during model training. We train the latent diffusion model and finetune the GPT-2 model on eight NVIDIA A100 80GB GPUs. We follow the settings described in AudioLDM [4] and change the default classifier-free guidance scale during the Denoising Diffusion Implicit Models (DDIM) [90] sampling to 3.5. For both GPT-2 finetuning and the latent diffusion model, we utilize the AdamW [91] optimizer with a learning rate of 10-4 and 10000 steps of linear warming up without decay. V. RESULT We evaluated our proposed system on three primary audio generation tasks: text-to-audio, text-to-music, and textto-speech. The three basic systems were trained on three different datasets: AudioCaps (general audio), MSD (music), and LJSpeech (speech), and are denoted as AudioLDM 2AC, AudioLDM 2-MSD, and AudioLDM 2-LJS, respectively. The model AudioLDM 2-Full represents a version capable of performing both audio and music generation simultaneously, with training data scaled up to 29510 hours, including all available data mentioned in Section IV-A. In contrast with AudioLDM [4], we do not perform additional model finetuning on AudioCaps for model trained with the full-scale datasets. Models with the suffix Large indicate larger-sized model variants, such as AudioLDM 2-Full-Large. A. Text-to-Audio Generation We compare the performance of our proposed model with several state-of-the-art systems, including AudioGen-Large [3], Make-an-Audio [15], AudioLDM [4], Make-an-Audio 2 [92], and TANGO [51]. To generate the samples for subjective evaluation, we adopt AudioLDM-M, an AudioLDM with 652M parameters, from HuggingFace and run with 100 reverse diffusion steps. The result of Make-an-Audio 2 is provided by the author [92]. We use the pre-trained TANGO model open-sourced on GitHub to reproduce their result. As shown in Table II, our proposed AudioLDM 2-AC significantly outperforms the previous systems across all three objective metrics. The previous best-performing system, TANGO, achieves a CLAP score of 17.6, while our proposed system surpasses it with a substantially higher CLAP score of 24.9. AudioLDM 2-Large also attains the best KL divergence score of 0.98, considerably improving upon the previous SOTA of 1.27. For the FAD score, our model reaches 1.42, establishing a new SOTA for text-to-audio generation. Our subjective evaluation results are mostly consistent with the objective metrics, confirming the effectiveness of AudioLDM 2-AC, which achieves an OVL of 3.88 and a REL of 3.90, surpassing AudioLDM and the previous SOTA TANGO by a significant margin. The difference between AudioLDM 2AC and the GroundTruth, which are real audios from the trained separately. We randomly choose > = {1,2,4,8} during AudioCaps dataset [27], is merely 0.16 and 0.18 for OVL pre-training of the latent diffusion model to enhance the model robustness under conditions YX with different λ. Y is only used as key and value in the T-UNet cross-attention layers therefore Y can have varying length. We train the latent diffusion model based on 10 seconds of random segment from the training set. For easier modeling of the T-UNet, we zeropad the 10 seconds of audio segment into 10.24 seconds and REL, respectively, demonstrating the strong performance of our proposed system. The AudioLDM-M we used is not finetuned on the AudioCaps dataset, which may explain its degraded performance compared with the metric score reported 6https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation 7https://github.com/declare-lab/tango TABLE II COMPARISON OF MODEL PERFORMANCES ON THE AUDIOCAPS EVALUATION SET. GT-AUDIOMAE DENOTE DIRECTLY APPLYING THE GROUND TRUTH "LANGUAGE OF AUDIO" Y, TO THE FUNCTION G FOR AUDIO GENERATION, AS DETAILED IN SECTION III-A. AudioLDM 2 SIGNIFICANTLY SURPASSES PREVIOUS METHODS IN BOTH SUBJECTIVE AND OBJECTIVE ASSESSMENTS. ALL MODELS ARE TRAINED USING THE AUDIOCAPS TRAINING SUBSET. MODELS MARKED WITH * ARE EXCLUSIVELY TRAINED ON THIS SUBSET, WHILE THOSE WITH # ARE FINE-TUNED ON IT. Model GroundTruth Duration (h) Param FAD↓ KL↓↓ CLAP↑ OVL ↑ REL ↑ 0.4.4.GT-AudioMAE AudioGen-Large 1.0.19 0.3.4.1 B 1.1.Make-an-Audio453 M 2.1.AudioLDM-Large#739 M 1.1.AudioLDM-M416 M 4.1.0.3.3.Make-an-Audio937 M 2.1.0.3.3.TANGO*866 M 1.1.0.3.3.AudioLDM 2-AC*346 M 1.1.0.3.3.AudioLDM 2-Full AudioLDM 2-AC-Large* AudioLDM 2-Full-Large346 M 1.1.0.3.3.712 M 712 M 1.42 0.1.86 1.0.3.3.0.3.3.A pencil scribbling on a notepad.1.4.7.Time Magical fairies laughter echoing through an enchanted forest. A kitten mewing for attention. ° 1.4.7.Time A monkey laughs before getting hit on the head by a large atomic bomb. ° 1.54.7.1.4.7.Time Time A accordion is speaking1.4.7.Time A playful glockenspiel in a whimsical children's song, sparking imaginations.1.7.TimeA classical orchestra performing a grand symphony.1.4.7.Time A traditional Irish fiddle playing a lively reel 1.4.7.Time Fig. 5. Examples for text-to-audio generation. in [4]. We observe the trend of overfitting during AudioLDM 2 training on the AudioCaps training set, due to the limited dataset size. To address this issue we measure the FAD score on the AudioCaps validation set every five epochs and treat the checkpoint before the FAD result shows degradations as the final model. To investigate the scalability of AudioLDM 2-AC-Large in terms of model size and dataset scale, we further trained AudioLDM 2 on a much larger dataset containing 29,hours of data using two different model sizes. To avoid the overfitting issue and potentially misleading objective metrics result, the model trained with a larger dataset, including AudioLDM 2-Full and AudioLDM 2-Full-Large are not finetuned on the AudioCaps training set, as performed by previous works [4]. As shown in Table II, the FAD score generally shows improvement after scaling up the model size, while the KL divergence and CLAP scores do not exhibit clear improvements, indicating that scaling the model size might be more beneficial for enhancing audio quality than audiotext relations. Despite the significant increase in training data, we did not observe significant improvements in the objective evaluation metrics. On the contrary, all three metrics showed degraded performance after training on more data. This is potentially because our test set has a limited distribution, while the large-scale training data covers a much wider distribution. The mismatch between the training and test data distributions results in poorer objective scores. Nevertheless, when compared with the AudioLDM-M (FAD 4.53) in Table II, which is also a large-scale pre-trained textto-audio model without finetuning on AudioCaps, AudioLDM 2 with full-scale training data achieves significantly better performance (FAD 1.42 ~2.13), showing a substantial improvement over AudioLDM-M. B. Text-to-Music Generation In this section, we compare our proposed model with other text-to-music generation models, including MusicGen [34], MusicLM [8], MeLoDy [13], Mousai [93], AudioLDM [4], and Riffusion [6]. The output of AudioLDM is obtained in the same way as Table II. MusicGen is reproduced using the official Github repository³. As shown in Table III, our proposed method significantly outperforms these strong baselines. For instance, AudioLDM 2-Full outperforms MusicGen by 36%, 11%, and 3.4% on FAD, KL and CLAP scores, respectively. The AudioLDM 2MSD model, which is only trained on music data, does not achieve better performance on objective metrics than the more general AudioLDM 2-Full. This result suggests that learning audio generation from a general perspective can benefit the performance in specialised domains as well, demonstrating the advantages of our proposed general framework. The general https://github.com/facebookresearch/audiocraft TABLE III PERFORMANCE COMPARISON ON THE MUSICCAPS EVALUATION SET. THE SUPERSCRIPT * INDICATES RESULTS REPRODUCED USING PUBLICLY AVAILABLE IMPLEMENTATIONS. THE OPEN-SOURCE VERSION OF MUSICGEN-MEDIUM EXCLUDES VOCAL SOUNDS, RESULTING IN SLIGHTLY INFERIOR PERFORMANCE COMPARED TO THE ORIGINAL REPORT [34]. GT-AUDIOMAE DENOTE DIRECTLY APPLYING THE GROUND TRUTH “LANGUAGE OF AUDIO” Y, TO THE FUNCTION G FOR AUDIO GENERATION, AS DETAILED IN SECTION III-A. ALL GENERATED AUDIO CLIPS WERE RESAMPLED TO 16KHZ PRIOR TO EVALUATION. Model TABLE IV TEXT-TO-SPEECH PERFORMANCE EVALUATED ON THE LJSPEECH TEST Model GroundTruth GT-AudioMAE FastSpeechAudioLDM 2-LJS SET. Mean Opinion Score↑ 4.63 ± 0.4.14±0.3.78 ± 0.GroundTruth GT-AudioMAE 2.Riffusion 14.Mousai 7.FAD↓ KL↓ CLAP↑ 0.0.27 0.2.06 0.1.OVL↑ REL↑ AudioLDM 2-LJS-Pretrained 3.65 ± 0.4.00 ± 0.3.3.4.3.Text: I can heat things up. MeLoDy 5.MusicLM 4.MusicGen-Medium 3.1.0.MusicGen-Medium* 0 0.15 0.3 0.45 0.6 0.75 0.9 1.1 1.2 1.Time0.0.3 0.0.6 0.75 0.Time 1.1.4.1.0.3.3.AudioLDM-M* 3.1.0.3.3.Text: What green is conveniently leaving out of her story is her level of cooking experience pre-meal kit. AudioLDM 2-MSD 4.1.0.3.3.AudioLDM 2-Full 3.1.0.3.3.TimeTime model AudioLDM 2-Full achieves a significantly higher 3.REL score than the other systems, indicating better textual understanding ability. The AudioLDM-M model achieves a significantly higher CLAP score than the remaining systems, which may stem from being directly conditioned by the same CLAP model during training. Therefore, the CLAP score value of AudioLDM-M in Table III is only provided for reference and may not reflect the true performance of the model, as also indicated by the subjective evaluation score. The high performance of AudioLDM may also stem from the diversity of audio training data, which also includes music and sound effects, which further supports the benefits of training a general-purpose model. However, the subjective evaluation in Table III indicates that the subjective performance of AudioLDM-M is not as good as suggested by the objective metrics. Since MeLoDy and MusicLM are not open-sourced, some of their objective and subjective metrics scores are not available for comparison. Due to the substantially lower objective scores, Riffusion and Mousai are not included in our subjective evaluation against other baseline models. C. Text-to-Speech Generation We compare our proposed model with the widely-adopted FastSpeech29 model on the LJSpeech test set. To study the upper bound of our system, we add a setting called GTAudioMAE that utilizes the ground truth LOA Y to the function G for audio generations. Our proposed AudioLDM 2-LJS is trained on the LJSpeech training split. To further explore the potential of our system, we pre-train the GPT2 model in function M on the GigaSpeech dataset before finetuning on LJSpeech. This version is denoted as AudioLDM 2-LJS-Pretrained. As shown in Table IV, with the pre-trained GPT-2 model, AudioLDM 2-LJS-Pretrained achieves a MOS of 4.00, significantly outperforming FastSpeech2. Our subjective evaluation "https://huggingface.co/facebook/fastspeech2-en-1jspeech Speaker prompt: A young girl is speaking Speaker prompt: A young male reporter is speaking Fig. 6. Examples of speaker-prompted text-to-speech generation. We use speaker prompts to describe the characteristics of the speaker and provide the model with the text transcription. shows AudioLDM 2-LJS-Pretrained exhibits greater fluctuations in emotion, punctuation, and tone. This demonstrates the benefits of pretraining on diverse datasets like GigaSpeech before finetuning on smaller corpora. Without pretraining, our proposed model still achieves a competitive MOS (Mean Opinion Score) of 3.65, which is comparable with the 3.MOS of our baseline FastSpeech2. D. Ablation Studies Setting TABLE V ABLATION STUDIES ON THE AUDIOCAPS DATASET. FAD↓↓ KL↓ CLAP↑ AudioLDM1.67 1.0.a. w/o Joint finetuning 2.1.0.b. w/o CLAP embedding (GPT) 2.1.0.C. w/o FLAN-T5 embedding (GPT) 2.1.0.d. w/o FLAN-T5 crossattn (T-UNet) 1.1.0.e. w/o CLAP and FLAN-T5 (GPT) 2.1.0.In order to validate our design choices of AudioLDM 2, we conducted a series of ablation studies on the text-to-audio generation task on the AudioCaps dataset. The results are shown in Table V. When the joint finetuning process between the GPT-2 model and the latent diffusion model was disabled (a), thereby only optimizing them separately, all three evaluation metrics exhibited a marked deterioration, suggesting joint finetuning is helpful for the GPT-2 model to better cooperate with the LDM model. The GPT-2 model accepts inputs from both the CLAP and FLAN-T5 modules for text-to-audio generation. The removal of either module resulted in a degradation of the evaluation metrics (b-c). However, the CLAP score was improved when only the CLAP module was used as an input (c).This improvement is likely due to the conditioning directly matching the evaluation metric. The removal of the crossattention mechanism in the T-UNet model (d), which accepts the FLAN-T5 embeddings, led to a significant degradation in both the KL divergence and CLAP scores. However, it improved the FAD score, from 1.67 to 1.38. These results indicate that while AudioMAE conditioning alone can achieve better FAD, the use of FLAN-T5 conditioning provides additional language semantic information that assists the learning of the audio and text relationships. Besides, we study the effect of removing both CLAP and FLAN-T5 representations and directly use text as input to the GPT-2 model to predict LOA (e). The experimental result shows that our model in this setting maintains competitive performance with an FAD of 2.11 and KL of 1.06. However, the CLAP score exhibits a noticeable degradation, which indicates that the CLAP and FLAN-T5 representations can potentially improve the relationship between the text and the generated audio. VI. CONCLUSION AND FUTURE WORKS In this paper, we have presented AudioLDM 2 for audio generation, achieving state-of-the-art or comparative performance on text-to-audio, text-to-music, and text-to-speech generation tasks. As a universal audio representation, the language of audio (LOA) enables self-supervised pre-training of the latent diffusion model, providing a robust foundation for the audio generation task. We further demonstrate the versatility of our proposed method by performing audio in-context learning. AudioLDM 2 opens doors for future works on audio generation from a unified perspective. Future work includes enabling the multi-task learning of the GPT-2 model to generate audio, music, and speech simultaneously with a single model. Additionally, we plan to investigate more effective representations for the "language of audio” by exploring the integration of other audio self-supervised models, such as HuBERT [61] and wav2vec [56], into our system. ACKNOWLEDGMENTS This research was partly supported by the British Broadcasting Corporation Research and Development (BBC R&D), Engineering and Physical Sciences Research Council (EPSRC) Grant EP/T019751/1 "AI for Sound", and a PhD scholarship from the Centre for Vision, Speech and Signal Processing (CVSSP), Faculty of Engineering and Physical Science (FEPS), University of Surrey. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) license to any Author Accepted Manuscript version arising. The authors wish to thank the associate editor and the reviewers for their helpful comments to further improve this work. REFERENCES [1] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, "A comprehensive survey of AI-generated content: A history of generative AI from GAN to ChatGPT," arXiv preprint:2303.04226, 2023. [2] X. Tan, J. Chen, H. Liu, J. Cong, C. Zhang, Y. Liu, X. Wang, Y. Leng, Y. Yi, L. He et al., “NaturalSpeech: End-to-end text to speech synthesis with human-level quality," arXiv preprint:2205.04421, 2022. [3] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. Défossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi, "AudioGen: Textually guided audio generation," International Conference on Learning Representations, 2022. [4] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley, "AudioLDM: Text-to-audio generation with latent diffusion models," International Conference on Machine Learning, 2023. [5] D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou, and X. Qiu, "SpeechGPT: Empowering large language models with intrinsic crossmodal conversational abilities," arXiv preprint:2305.11000, 2023. [6] S. Forsgren and H. Martiros, "Riffusion: Stable diffusion for real-time music generation, 2022,” URL https://riffusion.com/about, vol. 6, 2022. [7] X. Liu, Z. Zhu, H. Liu, Y. Yuan, M. Cui, Q. Huang, J. Liang, Y. Cao, Q. Kong, M. D. Plumbley et al., "WavJourney: Compositional audio creation with large language models," arXiv preprint:2307.14335, 2023. [8] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., “MusicLM: Generating music from text,” arXiv preprint:2301.11325, 2023. [9] R. Bresin, A. de Witt, S. Papetti, M. Civolani, and F. Fontana, "Expressive sonification of footstep sounds,” Proceedings of Interactive Sonification Workshop, 2010. [10] J. Engel, L. Hantrakul, C. Gu, and A. Roberts, “DDSP: Differentiable digital signal processing," International Conference on Learning Representations, 2020. [11] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, “Fastspeech 2: Fast and high-quality end-to-end text to speech," in International Conference on Learning Representations, 2021. [12] D. Herremans and E. Chew, "MorpheuS: Automatic music generation with recurrent pattern constraints and tension profiles,” in Proceedings of IEEE TENCON, 2016, pp. 282-285. [13] M. W. Lam, Q. Tian, T. Li, Z. Yin, S. Feng, M. Tu, Y. Ji, R. Xia, M. Ma, X. Song et al., “Efficient neural music generation," arXiv preprint:2305.15719, 2023. [14] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu, "Diffsound: Discrete diffusion model for text-to-sound generation," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 1720-1733, 2023. [15] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao, “Make-An-Audio: Text-to-audio generation with prompt-enhanced diffusion models," International Conference on Machine Learning, 2023. [16] H. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, "VoiceFixer: Toward general speech restoration with neural vocoder," arXiv preprint:2109.13731, 2021. [17] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, "Data2Vec: A general framework for self-supervised learning in speech, vision and language," in International Conference on Machine Learning, 2022, pp. 1298-1312. [18] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra, “ImageBind: One embedding space to bind them all," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 15 180–15 190. [19] Q. Kong, K. Chen, H. Liu, X. Du, T. Berg-Kirkpatrick, S. Dubnov, and M. D. Plumbley, “Universal source separation with weakly labelled data," arXiv preprint:2305.07447, 2023. [20] Y. Okamoto, K. Imoto, S. Takamichi, R. Yamanishi, T. Fukumori, Y. Yamashita et al., “Onoma-to-wave: Environmental sound synthesis from onomatopoeic words," Transactions on Signal and Information Processing, vol. 11, no. 1, 2022. [21] H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, C. Feichtenhofer et al., “Masked autoencoders that listen," Advances in Neural Information Processing Systems, 2022. [22] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, "Language models are unsupervised multitask learners," 2019. [23] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, "Highresolution image synthesis with latent diffusion models," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 10684-10695. [24] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., “A survey of large language models," arXiv preprint:2303.18223, 2023. [25] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, "SoundStream: An end-to-end neural audio codec," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495–507, 2021.[26] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi et al., “AudiOLM: A language modeling approach to audio generation," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 42, pp. 2523-2544, 2023. [27] C. D. Kim, B. Kim, H. Lee, and G. Kim, "AudioCaps: Generating captions for audios in the wild,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019, pp. 119–132. [28] K. Ito and L. Johnson, “The LJSpeech dataset," https://keithito.com/ LJ-Speech-Dataset/, 2017. [29] R. Sheffer and Y. Adi, “I hear your true colors: Image guided audio generation," in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023. [30] V. Iashin and E. Rahtu, "Taming visually guided sound generation,” in British Machine Vision Conference, 2021. [31] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov, "GradTTS: A diffusion probabilistic model for text-to-speech," in International Conference on Machine Learning, 2021, pp. 8599-8608. [32] J. Kim, S. Kim, J. Kong, and S. Yoon, “Glow-TTS: A generative flow for text-to-speech via monotonic alignment search," Advances in Neural Information Processing Systems, pp. 8067-8077, 2020. [33] Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank et al., "Noise2Music: Text-conditioned music generation with diffusion models," arXiv preprint:2302.03917, 2023. [34] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Défossez, "Simple and controllable music generation," arXiv preprint:2306.05284, 2023. [35] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, "W2V-Bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,” in IEEE Automatic Speech Recognition and Understanding Workshop. IEEE, 2021, pp. 244–250. [36] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models," in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 6840–6851. [37] Y. Song, J. Sohl-Dickstein, D. Kingma, A. Kumar, S. Ermon, and B. Poole, "Score-based generative modeling through stochastic differential equations," in International Conference on Learning Representations, 2021. [38] P. Dhariwal and A. Nichol, “Diffusion models beat GANS on image synthesis," in Advances in Neural Information Processing Systems, 2021. [39] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical text-conditional image generation with CLIP latents," arXiv preprint:2204.06125, 2022. [40] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi, "Photorealistic text-toimage diffusion models with deep language understanding,” arXiv preprint:2205.11487, 2022. [41] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi, "Image super-resolution via iterative refinement," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 47134726, 2022. [42] N. Chen, Y. Zhang, H. Zen, R. Weiss, M. Norouzi, and W. Chan, "WaveGrad: Estimating gradients for waveform generation," in International Conference on Learning Representations, 2021. [43] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, "DiffWave: A versatile diffusion model for audio synthesis,” in International Conference on Learning Representations, 2021. [44] Y. Leng, Z. Chen, J. Guo, H. Liu, J. Chen, X. Tan, D. Mandic, L. He, X.-Y. Li, T. Qin et al., “BinauralGrad: A two-stage conditional diffusion probabilistic model for binaural audio synthesis," Advances in Neural Information Processing Systems, 2022. [45] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni et al., “Make-a-video: Text-to-video generation without text-video data," in International Conference on Learning Representations, 2022. [46] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet, and T. Salimans, “Imagen video: High definition video generation with diffusion models," arXiv preprint:2210.02303, 2022. [47] Z. Chen, Y. Wu, Y. Leng, J. Chen, H. Liu, X. Tan, Y. Cui, K. Wang, L. He, S. Zhao, J. Bian, and D. Mandic, "ResGrad: Residual denoising diffusion probabilistic models for text to speech," arXiv preprint:2212.14518, 2022. [48] M. Lam, J. Wang, R. Huang, D. Su, and D. Yu, "Bilateral denoising diffusion models," in International Conference on Learning Representations, 2022. [49] S. Lee, H. Kim, C. Shin, X. Tan, C. Liu, Q. Meng, T. Qin, W. Chen, S. Yoon, and T. Liu, “Priorgrad: Improving conditional denoising diffusion models with data-driven adaptive prior," in International Conference on Learning Representations, 2022. [50] Z. Chen, X. Tan, K. Wang, S. Pan, D. Mandic, L. He, and S. Zhao, "Infergrad: Improving diffusion models for vocoder by considering inference in training,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2022. [51] D. Ghosal, N. Majumder, A. Mehrish, and S. Poria, “Text-to-audio generation using instruction-tuned LLM and latent diffusion model," arXiv preprint:2304.13731, 2023. [52] X. Liu, H. Liu, Q. Kong, X. Mei, M. D. Plumbley, and W. Wang, "Simple pooling front-ends for efficient audio classification,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023. [53] H. Liu, X. Liu, Q. Kong, W. Wang, and M. D. Plumbley, “Learning the spectrogram temporal resolution for audio classification," arXiv preprint:2210.01719, 2022. [54] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, "WaveNet: A generative model for raw audio," in ISCA Speech Synthesis Workshop, 2016, pp. 125–125. [55] X. Tan, T. Qin, J. Bian, T.-Y. Liu, and Y. Bengio, “Regeneration learning: A learning paradigm for data generation," arXiv preprint:2301.08846, 2023. [56] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, "wav2vec 2.0: A framework for self-supervised learning of speech representations," Advances in Neural Information Processing Systems, vol. 33, pp. 12449–12460, 2020. [57] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," International Conference on Learning Representations, 2014. [58] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," in International Conference on Learning Representations, 2020. [59] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, "AudioSet: An ontology and humanlabeled dataset for audio events," in IEEE International Conference on Acoustics, Speech and Signal Processing, 2017, pp. 776-780. [60] Y. Li, R. Yuan, G. Zhang, Y. Ma, X. Chen, H. Yin, C. Lin, A. Ragni, E. Benetos, N. Gyenge et al., “MERT: Acoustic music understanding model with large-scale self-supervised training," arXiv preprint:2306.00107, 2023. [61] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, "HUBERT: Self-supervised speech representation learning by masked prediction of hidden units," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451-3460, 2021. [62] S. Schneider, A. Baevski, R. Collobert, and M. Auli, "Wav2Vec: Unsupervised pre-training for speech recognition," INTERSPEECH, pp. 3465-3469, 2019. [63] D. Niizumi, D. Takeuchi, Y. Ohishi, N. Harada, and K. Kashino, "Byol for audio: Self-supervised learning for general-purpose audio representation," in International Joint Conference on Neural Networks, 2021. [64] J. Kong, J. Kim, and J. Bae, "HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis," Advances in Neural Information Processing Systems, vol. 33, pp. 17022-17033, 2020. [65] K. J. Piczak, “ESC: Dataset for environmental sound classification," in Proceedings of the ACM International Conference on Multimedia, 2015, pp. 1015–1018. [66] L. Van der Maaten and G. Hinton, "Visualizing data using t-SNE." Journal of Machine Learning Research, vol. 9, 2008. [67] Y. Qu, P. Liu, W. Song, L. Liu, and M. Cheng, "A text generation and prediction system: Pre-training on new corpora using BERT and GPT2," in IEEE International Conference on Electronics Information and Emergency Communication, 2020, pp. 323-326. [68] T. Klein and M. Nabi, “Learning to answer by learning to ask: Getting the best of GPT-2 and BERT worlds," arXiv preprint: 1911.02365, 2019. [69] A. M. Lamb, A. G. ALIAS PARTH GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio, “Professor forcing: A new algorithm for training recurrent networks," Advances in Neural Information Processing Systems, vol. 29, 2016. [70] S. Masoudnia and R. Ebrahimpour, “Mixture of experts: A literature survey," Artificial Intelligence Review, vol. 42, pp. 275-293, 2014. [71] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation," in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023. [72] H.-H. Wu, O. Nieto, J. P. Bello, and J. Salomon, "Audio-text models do not yet leverage natural language," in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023. [73] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-finetuned language models," arXiv preprint:2210.11416, 2022. [74] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer," The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020. [75] X. Tan, Neural Text-to-Speech Synthesis, ser. Artificial Intelligence: Foundations, Theory, and Algorithms. Springer Singapore, 2023. [76] D. P. Kingma and M. Welling, "Auto-encoding variational Bayes," arXiv preprint:1312.6114, 2013. [77] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need," Advances in Neural Information Processing Systems, vol. 30, 2017. [78] J. Ho and T. Salimans, “Classifier-free diffusion guidance,” in NeurIPS Workshop on Deep Generative Models and Downstream Applications, 2021. [79] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen, “GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models," in International Conference on Machine Learning, 2022, pp. 16784-16804. [80] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, and W. Wang, "WavCaps: A ChatGPT-assisted weakly-labelled audio captioning dataset for audio-language multimodal research," arXiv preprint:2303.17395, 2023. [81] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, "VGGSound: A large-scale audio-visual dataset,” in IEEE International Conference on Acoustics, Speech and Signal Processing, 2020, pp. 721–725. [82] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, "FMA: A dataset for music analysis," in International Society for Music Information Retrieval Conference, 2017. [83] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P. Lamere, "The million song dataset," International Society for Music Information Retrieval Conference, pp. 591–596, 2011. [84] G. Chen, S. Chai, G. Wang, J. Du, W. Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang et al., “GigaSpeech: An evolving, multidomain asr corpus with 10,000 hours of transcribed audio," in INTERSPEECH, 2021, pp. 4376-4380. [85] S. Doh, M. Won, K. Choi, and J. Nam, "Toward universal text-to-music retrieval," arXiv preprint:2211.14558, 2022. [86] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al., “CNN architectures for large-scale audio classification," in IEEE International Conference on Acoustics, Speech and Signal Processing, 2017, pp. 131– 135. [87] K. Koutini, J. Schlüter, H. Eghbal-Zadeh, and G. Widmer, “Efficient training of audio transformers with patchout," INTERSPEECH, pp. 2753-2757, 2021. [88] R. Likert, "A technique for the measurement of attitudes." Archives of Psychology, 1932. [89] Z. Chen, N. Kanda, J. Wu, Y. Wu, X. Wang, T. Yoshioka, J. Li, S. Sivasankaran, and S. E. Eskimez, "Speech separation with largescale self-supervised learning," in IEEE International Conference on Acoustics, Speech and Signal Processing, 2023. [90] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models," in International Conference on Learning Representations, 2020. [91] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization," in International Conference on Learning Representations, 2019. [92] J. Huang, Y. Ren, R. Huang, D. Yang, Z. Ye, C. Zhang, J. Liu, X. Yin, Z. Ma, and Z. Zhao, “Make-An-Audio 2: Temporal-enhanced text-toaudio generation," arXiv preprint:2305.18474, 2023. [93] F. Schneider, Z. Jin, and B. Schölkopf, "Mousai: Text-to-music generation with long-context latent diffusion," arXiv preprint:2301.11757, 2023.