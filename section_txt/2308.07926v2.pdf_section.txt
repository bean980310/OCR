--- ABSTRACT ---
We present the content deformation field (CoDeF) as a new type of video representation, which consists of a canonical content field aggregating the static contents in the entire video and a temporal deformation field recording the transformations from the canonical image (i.e., rendered from the canonical content field) to each individual frame along the time axis. Given a target video, these two fields are jointly optimized to reconstruct it through a carefully tailored rendering pipeline. We advisedly introduce some regularizations into the optimization process, urging the canonical content field to inherit semantics (e.g., the object shape) from the video. With such a design, CoDeF naturally supports lifting image algorithms for video processing, in the sense that one can apply an image algorithm to the canonical image and effortlessly propagate the outcomes to the entire video with the aid of the temporal deformation field. We experimentally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog. Project page can be found here. * Equal contribution 7 Corresponding author --- --1.
--- INTRODUCTION ---
The field of image processing has witnessed remarkable advancements, largely attributable to the power of generative models trained on extensive datasets, yielding exceptional quality and precision. However, the processing of video content has not achieved comparable progress. One challenge lies in maintaining high temporal consistency, a task complicated by the inherent randomness of neural networks. Another challenge arises from the nature of video datasets themselves, which often include textures of inferior quality compared to their image counterparts and necessitate greater computational resources. Consequently, the quality of video-based algorithms significantly lags behind those focused on images. This contrast prompts a question: is it feasible to represent video in the form of an image to seamlessly apply established image algorithms to video content with high temporal consistency? In pursuit of this objective, researchers have suggested the generation of video mosaics from dynamic videos [40, 47] in the era preceding deep learning, and the utilization of a neural layered image atlas [16, 23,66] subsequent to the proposal of implicit neural representations. Nonetheless, these methods exhibit two principal deficiencies. First, the capacity of these representations, particularly in faithfully reconstructing intricate details within a video, is restricted. Often, the reconstructed video overlooks subtle motion details, such as blinking eyes or slight smiles. The second limitation pertains to the typically distorted nature of the estimated atlas, which consequently suffers from impaired semantic information. Existing image processing algorithms, therefore, do not perform optimally as the estimated atlas lacks sufficient naturalness. We propose a novel approach to video representation that utilizes a 2D hash-based image field coupled with a 3D hash-based temporal deformation field. The incorporation of multi-resolution hash encoding [29] for the representation of temporal deformation significantly enhances the ability to reconstruct general videos. This formulation facilitates tracking the deformation of complex entities such as water and smog. However, the heightened capability of the deformation field presents a challenge in estimating a natural canonical image. An unnatural canonical image can also estimate the corresponding deformation field with a faithful reconstruction. To navigate this challenge, we suggest employing annealed hash during training. Initially, a smooth deformation grid is utilized to identify a coarse solution applicable to all rigid motions, with highfrequency details added gradually. Through this coarse-tofine training, the representation achieves a balance between the naturalness of the canonical and the faithfulness of the reconstruction. We observe a noteworthy enhancement in reconstruction quality compared to preceding methods. This improvement is quantified as an approximately 4.increase in PSNR, along with an observable increase in the naturalness of the canonical image. Our optimization process requires a mere approximate 300 seconds to estimate the canonical image with the deformation field while the previous implicit layered representations [16] takes more than 10 hours. Building upon our proposed content deformation field, we illustrate lifting image processing tasks such as promptguided image translation, super-resolution, and segmentation—to the more dynamic realm of video content. Our approach to prompt-guided video-to-video translation employs ControlNet [69] on the canonical image, propagating the translated content via the learned deformation. The translation process is conducted on a single canonical image and obviates the need for time-intensive inference models (e.g., Diffusion models) across all frames. Our translation outputs exhibit marked improvements in temporal consistency and texture quality over the state-of-the-art zero-shot video translations with generative models [36,64]. When contrasted with Text2Live, which relies on a neural layered atlas, our model is proficient in handling more complex motion, producing more natural canonical images, and thereby achieving superior translation results. Additionally, we extend the application of image algorithms such as super-resolution, semantic segmentation, and keypoints detection to the canonical image, leading to their practical applications in video contexts. This includes video superresolution, video object segmentation, video keypoints tracking, among others. Our proposed representation consistently delivers superior temporal consistency and highfidelity synthesized frames, demonstrating its potential as a groundbreaking tool in video processing. 2.
--- RELATED WORK ---
Implicit Neural Representations. Implicit representations in conjunction with coordinate-based Multilayer Perceptrons (MLPs) have demonstrated its powerful capability in accurately representing images [4, 49, 51], videos [16, 21,49, 66], and 3D/4D representations [26, 27, 31-34, 56]. These techniques have been employed in a range of applications, including novel view synthesis [27], image superresolution [4], and 3D/4D Reconstruction [56,62]. Furthermore, for the purpose of speeding up the training, a various of acceleration [29, 46] techniques have been explored to replace the original Fourier positional encoding with some discrete representation like multi-resolution feature grid or hash table. Moreover, the adoption of an implicit deformation field [20,32,33,35] has displayed a remarkable capability to overfit dynamic scenes. Inspired by these works, our primary objective is to reconstruct videos by utilizing a canonical image which inherit semantics for video processing purposes. Consistent Video Editing. Our research is closely aligned --- --\ 2 aan Ss Multi-resolution A N Video Reconstruction \ LIT TTT 7 DOOOOOOO H-———— Deformation Field: D-——__ Lifting Image Algorithms to Videos (e.g., ControlNet, Real-ESRGAN, SAM) aha = for Real te for ‘SAM for Video Translation_} [Video Super-resolution | |Video Segmentation Figure 2. Illustration of the proposed video representation, CoDeF, which factorizes an arbitrary video into a 2D content canonical field and a 3D temporal deformation field. Each field is implemented with a multi-resolution 2D or 3D hash table using an efficient MLP. Such a new type of representation naturally supports lifting image algorithms for video processing, in the way of directly applying the established algorithm on the canonical image (i.e., rendered from the canonical content field) and then propagating the results along the time axis through the temporal deformation field. with the domain of consistent video editing [15, 16, 19, 23], which predominantly features two primary approaches: propagation-based
--- METHOD ---
s exhibit two principal deficiencies. First, the capacity of these representations, particularly in faithfully reconstructing intricate details within a video, is restricted. Often, the reconstructed video overlooks subtle motion details, such as blinking eyes or slight smiles. The second limitation pertains to the typically distorted nature of the estimated atlas, which consequently suffers from impaired semantic information. Existing image processing algorithms, therefore, do not perform optimally as the estimated atlas lacks sufficient naturalness. We propose a novel approach to video representation that utilizes a 2D hash-based image field coupled with a 3D hash-based temporal deformation field. The incorporation of multi-resolution hash encoding [29] for the representation of temporal deformation significantly enhances the ability to reconstruct general videos. This formulation facilitates tracking the deformation of complex entities such as water and smog. However, the heightened capability of the deformation field presents a challenge in estimating a natural canonical image. An unnatural canonical image can also estimate the corresponding deformation field with a faithful reconstruction. To navigate this challenge, we suggest employing annealed hash during training. Initially, a smooth deformation grid is utilized to identify a coarse solution applicable to all rigid motions, with highfrequency details added gradually. Through this coarse-tofine training, the representation achieves a balance between the naturalness of the canonical and the faithfulness of the reconstruction. We observe a noteworthy enhancement in reconstruction quality compared to preceding methods. This improvement is quantified as an approximately 4.increase in PSNR, along with an observable increase in the naturalness of the canonical image. Our optimization process requires a mere approximate 300 seconds to estimate the canonical image with the deformation field while the previous implicit layered representations [16] takes more than 10 hours. Building upon our proposed content deformation field, we illustrate lifting image processing tasks such as promptguided image translation, super-resolution, and segmentation—to the more dynamic realm of video content. Our approach to prompt-guided video-to-video translation employs ControlNet [69] on the canonical image, propagating the translated content via the learned deformation. The translation process is conducted on a single canonical image and obviates the need for time-intensive inference models (e.g., Diffusion models) across all frames. Our translation outputs exhibit marked improvements in temporal consistency and texture quality over the state-of-the-art zero-shot video translations with generative models [36,64]. When contrasted with Text2Live, which relies on a neural layered atlas, our model is proficient in handling more complex motion, producing more natural canonical images, and thereby achieving superior translation results. Additionally, we extend the application of image algorithms such as super-resolution, semantic segmentation, and keypoints detection to the canonical image, leading to their practical applications in video contexts. This includes video superresolution, video object segmentation, video keypoints tracking, among others. Our proposed representation consistently delivers superior temporal consistency and highfidelity synthesized frames, demonstrating its potential as a groundbreaking tool in video processing. 2. Related Work Implicit Neural Representations. Implicit representations in conjunction with coordinate-based Multilayer Perceptrons (MLPs) have demonstrated its powerful capability in accurately representing images [4, 49, 51], videos [16, 21,49, 66], and 3D/4D representations [26, 27, 31-34, 56]. These techniques have been employed in a range of applications, including novel view synthesis [27], image superresolution [4], and 3D/4D Reconstruction [56,62]. Furthermore, for the purpose of speeding up the training, a various of acceleration [29, 46] techniques have been explored to replace the original Fourier positional encoding with some discrete representation like multi-resolution feature grid or hash table. Moreover, the adoption of an implicit deformation field [20,32,33,35] has displayed a remarkable capability to overfit dynamic scenes. Inspired by these works, our primary objective is to reconstruct videos by utilizing a canonical image which inherit semantics for video processing purposes. Consistent Video Editing. Our research is closely aligned --- --\ 2 aan Ss Multi-resolution A N Video Reconstruction \ LIT TTT 7 DOOOOOOO H-———— Deformation Field: D-——__ Lifting Image Algorithms to Videos (e.g., ControlNet, Real-ESRGAN, SAM) aha = for Real te for ‘SAM for Video Translation_} [Video Super-resolution | |Video Segmentation Figure 2. Illustration of the proposed video representation, CoDeF, which factorizes an arbitrary video into a 2D content canonical field and a 3D temporal deformation field. Each field is implemented with a multi-resolution 2D or 3D hash table using an efficient MLP. Such a new type of representation naturally supports lifting image algorithms for video processing, in the way of directly applying the established algorithm on the canonical image (i.e., rendered from the canonical content field) and then propagating the results along the time axis through the temporal deformation field. with the domain of consistent video editing [15, 16, 19, 23], which predominantly features two primary approaches: propagation-based methods and layered representationbased techniques. Propagation-based methods [13-15, 43, 53,60] center on editing an initial frame and subsequently disseminating those edits throughout the video sequence. While this approach offers advantages in terms of computational efficiency and simplicity, it may be prone to inaccuracies and inconsistencies during the propagation of edits, particularly in situations characterized by complex motion or occlusion. Conversely, layered representationbased techniques [16, 23, 24, 40,47] entail decomposing a video into distinct layers, thereby facilitating greater control and flexibility during the editing process. Text2Live [1] introduces the application of CLIP [37] models for video editing by modifying an optimized atlas [16] using text inputs, thereby yielding temporally consistent video editing results. Our work bears similarities to Text2Live in the context of employing an optimized representation for videos. However, our methodology diverges in several aspects: we optimize a more semantically-aware canonical representation incorporating a hash-based deformable design and attain higher-fidelity video processing. Video Processing via Generative Models. The advancement of diffusion models has markedly enhanced the synthesis quality of text-to-image generation [6, 11,50], surpassing the performance of prior methodologies [25, 41,65, 68]. State-of-the-art diffusion models, such as GLIDE [30], Dall-E 2 [38, 39], Stable Diffusion [42], and Imagen [45], have been trained on millions of images, resulting in exceptional generative capabilities. While existing text-to-image (T21) models enable free-text generation, incorporating additional conditioning factors [2,3,9,28,44,54,58,69] such as edge, depth map, and normal map is essential for achieving precise control. In an effort to enhance controllability, researchers have proposed several approaches. For instance, PITI [58] involves retraining an image encoder to map latents to the T2I latent space. InstructPix2Pix [2], on the other hand, fine-tunes T2I models using synthesized image condition pairs. ControlNet [69] introduces additional control conditions for Stable Diffusion through an auxiliary branch, thereby generating images that faithfully adhere to input condition maps. A recent research direction concentrates on the processing of videos utilizing text-toimage (T2I) models exclusively. Approaches like Tune-AVideo [64], Text2 Video-Zero [17], FateZero [36], Vid2VidZero [59], and Video-P2P [22] explore the latent space of DDIM [50] and incorporate cross-frame attention maps to facilitate consistent generation. Nevertheless, these methods may experience compromised temporal consistency due to the inherent randomness of generation, and the control condition may not be achieved with precision. Text-to-video generation has emerged as a prominent research area in recent years, with prevalent approaches encompassing the training of diffusion models or autoregressive transformers on extensive datasets. Although textto-video architectures such as NUWA [63], CogVideo [12], Phenaki [55], Make-A-Video [48], Imagen Video [10], and Gen-1 [7] are capable of generating video frames that semantically correspond to the input text, they may exhibit limitations in terms of precise control over video conditions or low resolution due to substantial computational demands. 3. Method Problem Formulation. Given a video V comprised of frames {Ij, I2,...,[1v}, one can naively apply the image processing algorithm %¥ to each frame individually for --- --corresponding video tasks, yet may observe undesirable inconsistencies across frames. An alternative strategy involves enhancing algorithm 4 with a temporal module, which requires additional training on video data. However, simply introducing a temporal module is hard to guarantee theoretical consistency and may result in performance degradation due to insufficient training data. Motivated by these challenges, we propose representing a video V using a flattened canonical image I, and a deformation field D. By applying the image algorithm V on [,, we can effectively propagate the effect to the whole video with the learned deformation field. This novel video representation serves as a crucial bridge between image algorithms and video tasks, allowing directly lifting of stateof-the-art image methodologies to video applications. The proposed representations ought to exhibit the following essential characteristics: ¢ Fitting Capability for Faithful Video Reconstruction. The representation should possess the ability to accurately fit large rigid or non-rigid deformations in videos. ¢ Semantic Correctness of the Canonical Image. A distorted or semantically incorrect canonical image can lead to decreased image processing performance, especially considering that most of these processes are trained on natural image data. ¢ Smoothness of the Deformation Field. The assurance of smoothness in the deformation field is an essential feature that guarantees temporal consistency and correct propagation. 3.1. Content Deformation Fields Inspired by the dynamic NeRFs [32, 33], we propose to represent the video in two distinct components: the canonical field and the deformation field. These two components are realized through the employment of a 2D and a 3D hash table, respectively. To enhance the capacity of these hash tables, two minuscule MLPs are integrated. We present our proposed representation for reconstructing and processing videos, as illustrated in Fig. 2. Given a video Y comprising frames {I), Ib,..., Jv}, we train an implicit deformable model tailored to fit these frames. The model is composed of two coordinate-based MLPs: the deformation field D and the canonical field C. The canonical field C serves as a continuous representation encompassing all flattened textures present in the video V. It is defined by a function F x > ¢, which maps a 2D position x (x,y) to a color c (r,g,b). In order to speed up the training and enable the network to capture the high-frequency details, we adopt the multi-resolution hash encoding yp : R? — R2+?*! to map the coordinate x into a feature vector, where L is the number of levels for multi-resolution and F is the number of feature dimensions for per layer. The function ‘Yop(x) = (x, Fi(x),...,Fx(x)) facilitates the model’s ability to capture high-frequency details, where F;(x) is the features linearly interpolated by x at i" resolution. The deformation field D captures the observation-to-canonical deformation for every frame within a video. For a specific frame J;, D establishes the correspondence between the observed and canonical positions. Dynamic NeRFs [32,33] implement the deformation field in 3D space by using the Fourier positional encoding and an extra learnable time code. This implementation ensures the smoothness of the deformation field. Nevertheless, this straightforward implementation can not be seamlessly transferred into video representation for two reasons (i.e. low training efficiency and inadequate representative capability). Therefore, we propose to represent the deformation field as a 3D hash table with a tiny MLP following. Specifically, an arbitrary position x in the t’” frame is first encoded by a 3D hash encoding function 73p(x, t) to get high-dimension features. Then a tiny MLP D : (+3p(x, t)) + x’ maps the embedded features its corresponding position x’ in canonical field. We elaborate our 3D hash encoding based deformation field in detail as follows. 3D Hash Encoding for Deformation Field. Specifically, an arbitrary point in the video can be conceptualized as a position x3p : (z,y,t) within an orthogonal 3D space.We represent our video space using the 3D hash encoding technique, as depicted on the left side of Fig. 2. This technique encapsulates the 3D space as a multi-resolution feature grid. The term multi-resolution refers to a composition of grids with varying degrees of resolution, and feature grid denotes a grid populated with learnable features at each vertex. In our framework, the multi-resolution feature grid is organized into L distinct levels. The dimensionality of the learnable features is represented as F’. Furthermore, the resolution of the J” layer, denoted as N;, exhibits a geometric progression between the coarsest and finest resolutions, denoted collectively as [Niin, Niax|, using Ni = [Nin yb =exp (Ams — Nan) Considering the queried points x3p at I" layer, the input coordinate is scaled by that level’s grid resolution. And the queried features of x3p are tri-linear interpolated from its 8-neighboring corner points(seen in Fig. 2). For attaining the corner points of x3p, rounding down and up are first operated as Lx4p] = Xap - Ni], [Xp] = [xan - Ni, (2) and we map its each corner to an entry in the level’s respective feature vector array, which has fixed size of at most T. For the coarse level, the parameters of low resolution grid are fewer than 7’, where the mapping is 1 : 1. --- --Thus, the features can be directly looked up by its index. On the contrary. For the finer resolution, the point is mapped by the hash function, h(xsp) = (@Liximi) mod T, @) where @ denotes the bit-wise XOR operation and {7;} are unique large prime numbers following [29]. The output color value at coordinate x for frame t can be computed as © =C(D(q30(x,t))). 4) This output can be supervised using the ground truth color present in the input frame. 3.2. Model Design The proposed representation can effectively model and reconstruct both the canonical content and the temporal deformation for an arbitrary video. However, it faces challenges in meeting the requirements for robust video processing. In particular, while 3D hash deformation possesses powerful fitting capability, it compromises the smoothness of temporal deformation. This trade-off makes it notably difficult to maintain the inherent semantics of the canonical image, creating a significant barrier to the adaptation of established image algorithms for video use. To achieve precise video reconstruction while preserving the inherent semantics of the canonical image, we propose the use of annealed multi-resolution hash encoding. To further enhance the smoothness of deformation, we introduce flow-based consistency. In challenging cases, such as those involving large occlusions or complex multiobject scenarios, we suggest utilizing additional semantic information. This can be achieved by using semantic masks in conjunction with the grouped deformation fields. Annealed 3D Hash Encoding for Deformation. For the finer resolution, the hash encoding enhance the complex deformation fitting performance but introducing the discontinuity and distortion in canonical field (Seen in Fig. 9). Inspired by the annealed strategy utilized in dynamic NeRFs [32], we employ the annealed hash encoding technique for progressive frequency filter for deformation. More specifically, we use a progressive controlling weights for those features interpolated in different resolution. The weight for the J" layer in training step k is computed as 1 = cos(m - clamp(m(j — Moeg)/Netep, 9, 1))wj(k) : (5) where Nyeg is a predefined step for beginning annealing and m represents a hyper parameters for controlling the annealing speed, and Netep is the number for annealing step. Flow-guided Consistency Loss. Corresponding points identified by flows with high confidence should be the same points in the canonical field. We compute the flow-guided Input Video Layered Neural Atlas g = ia z E Fy E EFigure 3. Qualitative comparison between layered neural atlas [16] and our CoDeF regarding video reconstruction, which reflects the capacity of the video representation and also plays a fundamental role in faithful video processing. Details are best appreciated when zoomed in. consistency loss according to this observation. For two consecutive frames J; and I;41, we employ RAFT [52] to detect the forward flows F;_,;41 and backward flows Fi41-+i. The confident region of a frame J; can be defined as Maow = |Warp(Warp(Ii, Fi+si¢1), Fits) Lil < €, (6) where € represents a hyperparameter for the error threshold. This loss can be formulated as Laow = Y-|IDlrsv(x,#)) — Doral + Feast +1) = Feaall * Mis (7) where F7£,,,, and Mit, are the optical flow and the flow confidence at x . The flow loss efficiently regularize the smoothness of the deformation field especially for the smooth region. Grouped Content Deformation Fields. Although the representation can learn to reconstruct a video using a single content deformation field, complex motions arising from overlapped multi-objects may lead to conflicts within one canonical. Consequently, the boundary region might suffer from inaccurate reconstruction. For challenging instances featuring large occlusions, we propose an option to introduce the layers corresponding to multiple content deformation fields. These layers would be defined based on semantic segmentation, thereby improving the accuracy and robustness of video reconstruction in these demanding scenarios. We leverage the Segment-Anything-track (SAMtrack) [5] to attain the segmentation of each video frame --- --Original Video Ours Text2Live Tune-A-Video FateZero Text Prompt: Iron man Figure 4. Qualitative comparison on the task of text-guided video-to-video translation across different methods, including Text2Live [1], Tune-A-Video [64], FateZero [36], and directly lifting ControlNet [69] through our CoDeF. We strongly encourage the readers to see the videos on the project page for a detailed evaluation of temporal consistency and synthesis quality. I; into K semantic layers with mask Mj,.... Mi._,. And for each layer, we use a group of canonical fields and deformation fields to represent those separate motion of different objects. These models are subsequently formulated as groups of implicit fields: D : {D1,...,.DK},C : {Ci,...,Cx}. In theory, for semantic layer k in frame #, it is sufficient to sample pixels in the region Mj, for efficient reconstruction. However, hash encoding can result in random and unstructured patterns in unsupervised regions, which decreases the performance of image-based models trained on natural images. To tackle this issue, we sample a number of points outside of the region Mj, and train them using DL» loss with the ground truth color. In this way, we effectively regularize Mj. with the background loss Lye. Consequently, the canonical image attains a more natural appearance, leading to enhanced processing results. Training Objectives. The representation is trained by minimizing the objective function Lyec. This function corresponds to the Lz loss between the ground truth color and the predicted color c for a given coordinate x. To regularize and stabilize the training process, we introduce additional regularization terms as previously discussed. The total loss is calculated using the following equation L = Lyee + Ar * Leow; (8) where ; represents the hyper-parameters for loss weights. It’s important to note that when training the grouped deformation field, we include an additional regularizer, denoted as Az * Log. --- --1-th frame 10-th frame 20-th frame 30-th frame Canonical Image Figure 5. Visualization of point correspondence across frames, which is directly extracted from the temporal deformation field after reconstructing the video with CoDeF. 3.3. Application to Consistent Video Processing Upon the optimization of the content deformation field, the canonical image I, is retrieved by setting the deformation of all points to zero. It is important to note that the size of the canonical image can be flexibly adjusted to be larger than the original image size depending on the scene movement observed in the video, thereby allowing more content to be included. The canonical image I, is then utilized in executing various downstream algorithms for consistent video processing. We evaluated the following state-of-the-art (SOTA) algorithms: (/) ControlNet [69]: Used for prompt-guided video-to-video translation. (2) Segment-anything (SAM) [18]: Applied for video object tracking. (3) R-ESRGAN [61]: Employed for video superresolution. Additionally, the canonical image allows users to conveniently edit the video by directly modifying the image. We further illustrate this capability through multiple manual video editing examples. 4.
--- EXPERIMENT ---
ally show that CoDeF is able to lift image-to-image translation to video-to-video translation and lift keypoint detection to keypoint tracking without any training. More importantly, thanks to our lifting strategy that deploys the algorithms on only one image, we achieve superior cross-frame consistency in processed videos compared to existing video-to-video translation approaches, and even manage to track non-rigid objects like water and smog. Project page can be found here. * Equal contribution 7 Corresponding author --- --1. Introduction The field of image processing has witnessed remarkable advancements, largely attributable to the power of generative models trained on extensive datasets, yielding exceptional quality and precision. However, the processing of video content has not achieved comparable progress. One challenge lies in maintaining high temporal consistency, a task complicated by the inherent randomness of neural networks. Another challenge arises from the nature of video datasets themselves, which often include textures of inferior quality compared to their image counterparts and necessitate greater computational resources. Consequently, the quality of video-based algorithms significantly lags behind those focused on images. This contrast prompts a question: is it feasible to represent video in the form of an image to seamlessly apply established image algorithms to video content with high temporal consistency? In pursuit of this objective, researchers have suggested the generation of video mosaics from dynamic videos [40, 47] in the era preceding deep learning, and the utilization of a neural layered image atlas [16, 23,66] subsequent to the proposal of implicit neural representations. Nonetheless, these methods exhibit two principal deficiencies. First, the capacity of these representations, particularly in faithfully reconstructing intricate details within a video, is restricted. Often, the reconstructed video overlooks subtle motion details, such as blinking eyes or slight smiles. The second limitation pertains to the typically distorted nature of the estimated atlas, which consequently suffers from impaired semantic information. Existing image processing algorithms, therefore, do not perform optimally as the estimated atlas lacks sufficient naturalness. We propose a novel approach to video representation that utilizes a 2D hash-based image field coupled with a 3D hash-based temporal deformation field. The incorporation of multi-resolution hash encoding [29] for the representation of temporal deformation significantly enhances the ability to reconstruct general videos. This formulation facilitates tracking the deformation of complex entities such as water and smog. However, the heightened capability of the deformation field presents a challenge in estimating a natural canonical image. An unnatural canonical image can also estimate the corresponding deformation field with a faithful reconstruction. To navigate this challenge, we suggest employing annealed hash during training. Initially, a smooth deformation grid is utilized to identify a coarse solution applicable to all rigid motions, with highfrequency details added gradually. Through this coarse-tofine training, the representation achieves a balance between the naturalness of the canonical and the faithfulness of the reconstruction. We observe a noteworthy enhancement in reconstruction quality compared to preceding methods. This improvement is quantified as an approximately 4.increase in PSNR, along with an observable increase in the naturalness of the canonical image. Our optimization process requires a mere approximate 300 seconds to estimate the canonical image with the deformation field while the previous implicit layered representations [16] takes more than 10 hours. Building upon our proposed content deformation field, we illustrate lifting image processing tasks such as promptguided image translation, super-resolution, and segmentation—to the more dynamic realm of video content. Our approach to prompt-guided video-to-video translation employs ControlNet [69] on the canonical image, propagating the translated content via the learned deformation. The translation process is conducted on a single canonical image and obviates the need for time-intensive inference models (e.g., Diffusion models) across all frames. Our translation outputs exhibit marked improvements in temporal consistency and texture quality over the state-of-the-art zero-shot video translations with generative models [36,64]. When contrasted with Text2Live, which relies on a neural layered atlas, our model is proficient in handling more complex motion, producing more natural canonical images, and thereby achieving superior translation results. Additionally, we extend the application of image algorithms such as super-resolution, semantic segmentation, and keypoints detection to the canonical image, leading to their practical applications in video contexts. This includes video superresolution, video object segmentation, video keypoints tracking, among others. Our proposed representation consistently delivers superior temporal consistency and highfidelity synthesized frames, demonstrating its potential as a groundbreaking tool in video processing. 2. Related Work Implicit Neural Representations. Implicit representations in conjunction with coordinate-based Multilayer Perceptrons (MLPs) have demonstrated its powerful capability in accurately representing images [4, 49, 51], videos [16, 21,49, 66], and 3D/4D representations [26, 27, 31-34, 56]. These techniques have been employed in a range of applications, including novel view synthesis [27], image superresolution [4], and 3D/4D Reconstruction [56,62]. Furthermore, for the purpose of speeding up the training, a various of acceleration [29, 46] techniques have been explored to replace the original Fourier positional encoding with some discrete representation like multi-resolution feature grid or hash table. Moreover, the adoption of an implicit deformation field [20,32,33,35] has displayed a remarkable capability to overfit dynamic scenes. Inspired by these works, our primary objective is to reconstruct videos by utilizing a canonical image which inherit semantics for video processing purposes. Consistent Video Editing. Our research is closely aligned --- --\ 2 aan Ss Multi-resolution A N Video Reconstruction \ LIT TTT 7 DOOOOOOO H-———— Deformation Field: D-——__ Lifting Image Algorithms to Videos (e.g., ControlNet, Real-ESRGAN, SAM) aha = for Real te for ‘SAM for Video Translation_} [Video Super-resolution | |Video Segmentation Figure 2. Illustration of the proposed video representation, CoDeF, which factorizes an arbitrary video into a 2D content canonical field and a 3D temporal deformation field. Each field is implemented with a multi-resolution 2D or 3D hash table using an efficient MLP. Such a new type of representation naturally supports lifting image algorithms for video processing, in the way of directly applying the established algorithm on the canonical image (i.e., rendered from the canonical content field) and then propagating the results along the time axis through the temporal deformation field. with the domain of consistent video editing [15, 16, 19, 23], which predominantly features two primary approaches: propagation-based methods and layered representationbased techniques. Propagation-based methods [13-15, 43, 53,60] center on editing an initial frame and subsequently disseminating those edits throughout the video sequence. While this approach offers advantages in terms of computational efficiency and simplicity, it may be prone to inaccuracies and inconsistencies during the propagation of edits, particularly in situations characterized by complex motion or occlusion. Conversely, layered representationbased techniques [16, 23, 24, 40,47] entail decomposing a video into distinct layers, thereby facilitating greater control and flexibility during the editing process. Text2Live [1] introduces the application of CLIP [37] models for video editing by modifying an optimized atlas [16] using text inputs, thereby yielding temporally consistent video editing results. Our work bears similarities to Text2Live in the context of employing an optimized representation for videos. However, our methodology diverges in several aspects: we optimize a more semantically-aware canonical representation incorporating a hash-based deformable design and attain higher-fidelity video processing. Video Processing via Generative Models. The advancement of diffusion models has markedly enhanced the synthesis quality of text-to-image generation [6, 11,50], surpassing the performance of prior methodologies [25, 41,65, 68]. State-of-the-art diffusion models, such as GLIDE [30], Dall-E 2 [38, 39], Stable Diffusion [42], and Imagen [45], have been trained on millions of images, resulting in exceptional generative capabilities. While existing text-to-image (T21) models enable free-text generation, incorporating additional conditioning factors [2,3,9,28,44,54,58,69] such as edge, depth map, and normal map is essential for achieving precise control. In an effort to enhance controllability, researchers have proposed several approaches. For instance, PITI [58] involves retraining an image encoder to map latents to the T2I latent space. InstructPix2Pix [2], on the other hand, fine-tunes T2I models using synthesized image condition pairs. ControlNet [69] introduces additional control conditions for Stable Diffusion through an auxiliary branch, thereby generating images that faithfully adhere to input condition maps. A recent research direction concentrates on the processing of videos utilizing text-toimage (T2I) models exclusively. Approaches like Tune-AVideo [64], Text2 Video-Zero [17], FateZero [36], Vid2VidZero [59], and Video-P2P [22] explore the latent space of DDIM [50] and incorporate cross-frame attention maps to facilitate consistent generation. Nevertheless, these methods may experience compromised temporal consistency due to the inherent randomness of generation, and the control condition may not be achieved with precision. Text-to-video generation has emerged as a prominent research area in recent years, with prevalent approaches encompassing the training of diffusion models or autoregressive transformers on extensive datasets. Although textto-video architectures such as NUWA [63], CogVideo [12], Phenaki [55], Make-A-Video [48], Imagen Video [10], and Gen-1 [7] are capable of generating video frames that semantically correspond to the input text, they may exhibit limitations in terms of precise control over video conditions or low resolution due to substantial computational demands. 3. Method Problem Formulation. Given a video V comprised of frames {Ij, I2,...,[1v}, one can naively apply the image processing algorithm %¥ to each frame individually for --- --corresponding video tasks, yet may observe undesirable inconsistencies across frames. An alternative strategy involves enhancing algorithm 4 with a temporal module, which requires additional training on video data. However, simply introducing a temporal module is hard to guarantee theoretical consistency and may result in performance degradation due to insufficient training data. Motivated by these challenges, we propose representing a video V using a flattened canonical image I, and a deformation field D. By applying the image algorithm V on [,, we can effectively propagate the effect to the whole video with the learned deformation field. This novel video representation serves as a crucial bridge between image algorithms and video tasks, allowing directly lifting of stateof-the-art image methodologies to video applications. The proposed representations ought to exhibit the following essential characteristics: ¢ Fitting Capability for Faithful Video Reconstruction. The representation should possess the ability to accurately fit large rigid or non-rigid deformations in videos. ¢ Semantic Correctness of the Canonical Image. A distorted or semantically incorrect canonical image can lead to decreased image processing performance, especially considering that most of these processes are trained on natural image data. ¢ Smoothness of the Deformation Field. The assurance of smoothness in the deformation field is an essential feature that guarantees temporal consistency and correct propagation. 3.1. Content Deformation Fields Inspired by the dynamic NeRFs [32, 33], we propose to represent the video in two distinct components: the canonical field and the deformation field. These two components are realized through the employment of a 2D and a 3D hash table, respectively. To enhance the capacity of these hash tables, two minuscule MLPs are integrated. We present our proposed representation for reconstructing and processing videos, as illustrated in Fig. 2. Given a video Y comprising frames {I), Ib,..., Jv}, we train an implicit deformable model tailored to fit these frames. The model is composed of two coordinate-based MLPs: the deformation field D and the canonical field C. The canonical field C serves as a continuous representation encompassing all flattened textures present in the video V. It is defined by a function F x > ¢, which maps a 2D position x (x,y) to a color c (r,g,b). In order to speed up the training and enable the network to capture the high-frequency details, we adopt the multi-resolution hash encoding yp : R? — R2+?*! to map the coordinate x into a feature vector, where L is the number of levels for multi-resolution and F is the number of feature dimensions for per layer. The function ‘Yop(x) = (x, Fi(x),...,Fx(x)) facilitates the model’s ability to capture high-frequency details, where F;(x) is the features linearly interpolated by x at i" resolution. The deformation field D captures the observation-to-canonical deformation for every frame within a video. For a specific frame J;, D establishes the correspondence between the observed and canonical positions. Dynamic NeRFs [32,33] implement the deformation field in 3D space by using the Fourier positional encoding and an extra learnable time code. This implementation ensures the smoothness of the deformation field. Nevertheless, this straightforward implementation can not be seamlessly transferred into video representation for two reasons (i.e. low training efficiency and inadequate representative capability). Therefore, we propose to represent the deformation field as a 3D hash table with a tiny MLP following. Specifically, an arbitrary position x in the t’” frame is first encoded by a 3D hash encoding function 73p(x, t) to get high-dimension features. Then a tiny MLP D : (+3p(x, t)) + x’ maps the embedded features its corresponding position x’ in canonical field. We elaborate our 3D hash encoding based deformation field in detail as follows. 3D Hash Encoding for Deformation Field. Specifically, an arbitrary point in the video can be conceptualized as a position x3p : (z,y,t) within an orthogonal 3D space.We represent our video space using the 3D hash encoding technique, as depicted on the left side of Fig. 2. This technique encapsulates the 3D space as a multi-resolution feature grid. The term multi-resolution refers to a composition of grids with varying degrees of resolution, and feature grid denotes a grid populated with learnable features at each vertex. In our framework, the multi-resolution feature grid is organized into L distinct levels. The dimensionality of the learnable features is represented as F’. Furthermore, the resolution of the J” layer, denoted as N;, exhibits a geometric progression between the coarsest and finest resolutions, denoted collectively as [Niin, Niax|, using Ni = [Nin yb =exp (Ams — Nan) Considering the queried points x3p at I" layer, the input coordinate is scaled by that level’s grid resolution. And the queried features of x3p are tri-linear interpolated from its 8-neighboring corner points(seen in Fig. 2). For attaining the corner points of x3p, rounding down and up are first operated as Lx4p] = Xap - Ni], [Xp] = [xan - Ni, (2) and we map its each corner to an entry in the level’s respective feature vector array, which has fixed size of at most T. For the coarse level, the parameters of low resolution grid are fewer than 7’, where the mapping is 1 : 1. --- --Thus, the features can be directly looked up by its index. On the contrary. For the finer resolution, the point is mapped by the hash function, h(xsp) = (@Liximi) mod T, @) where @ denotes the bit-wise XOR operation and {7;} are unique large prime numbers following [29]. The output color value at coordinate x for frame t can be computed as © =C(D(q30(x,t))). 4) This output can be supervised using the ground truth color present in the input frame. 3.2. Model Design The proposed representation can effectively model and reconstruct both the canonical content and the temporal deformation for an arbitrary video. However, it faces challenges in meeting the requirements for robust video processing. In particular, while 3D hash deformation possesses powerful fitting capability, it compromises the smoothness of temporal deformation. This trade-off makes it notably difficult to maintain the inherent semantics of the canonical image, creating a significant barrier to the adaptation of established image algorithms for video use. To achieve precise video reconstruction while preserving the inherent semantics of the canonical image, we propose the use of annealed multi-resolution hash encoding. To further enhance the smoothness of deformation, we introduce flow-based consistency. In challenging cases, such as those involving large occlusions or complex multiobject scenarios, we suggest utilizing additional semantic information. This can be achieved by using semantic masks in conjunction with the grouped deformation fields. Annealed 3D Hash Encoding for Deformation. For the finer resolution, the hash encoding enhance the complex deformation fitting performance but introducing the discontinuity and distortion in canonical field (Seen in Fig. 9). Inspired by the annealed strategy utilized in dynamic NeRFs [32], we employ the annealed hash encoding technique for progressive frequency filter for deformation. More specifically, we use a progressive controlling weights for those features interpolated in different resolution. The weight for the J" layer in training step k is computed as 1 = cos(m - clamp(m(j — Moeg)/Netep, 9, 1))wj(k) : (5) where Nyeg is a predefined step for beginning annealing and m represents a hyper parameters for controlling the annealing speed, and Netep is the number for annealing step. Flow-guided Consistency Loss. Corresponding points identified by flows with high confidence should be the same points in the canonical field. We compute the flow-guided Input Video Layered Neural Atlas g = ia z E Fy E EFigure 3. Qualitative comparison between layered neural atlas [16] and our CoDeF regarding video reconstruction, which reflects the capacity of the video representation and also plays a fundamental role in faithful video processing. Details are best appreciated when zoomed in. consistency loss according to this observation. For two consecutive frames J; and I;41, we employ RAFT [52] to detect the forward flows F;_,;41 and backward flows Fi41-+i. The confident region of a frame J; can be defined as Maow = |Warp(Warp(Ii, Fi+si¢1), Fits) Lil < €, (6) where € represents a hyperparameter for the error threshold. This loss can be formulated as Laow = Y-|IDlrsv(x,#)) — Doral + Feast +1) = Feaall * Mis (7) where F7£,,,, and Mit, are the optical flow and the flow confidence at x . The flow loss efficiently regularize the smoothness of the deformation field especially for the smooth region. Grouped Content Deformation Fields. Although the representation can learn to reconstruct a video using a single content deformation field, complex motions arising from overlapped multi-objects may lead to conflicts within one canonical. Consequently, the boundary region might suffer from inaccurate reconstruction. For challenging instances featuring large occlusions, we propose an option to introduce the layers corresponding to multiple content deformation fields. These layers would be defined based on semantic segmentation, thereby improving the accuracy and robustness of video reconstruction in these demanding scenarios. We leverage the Segment-Anything-track (SAMtrack) [5] to attain the segmentation of each video frame --- --Original Video Ours Text2Live Tune-A-Video FateZero Text Prompt: Iron man Figure 4. Qualitative comparison on the task of text-guided video-to-video translation across different methods, including Text2Live [1], Tune-A-Video [64], FateZero [36], and directly lifting ControlNet [69] through our CoDeF. We strongly encourage the readers to see the videos on the project page for a detailed evaluation of temporal consistency and synthesis quality. I; into K semantic layers with mask Mj,.... Mi._,. And for each layer, we use a group of canonical fields and deformation fields to represent those separate motion of different objects. These models are subsequently formulated as groups of implicit fields: D : {D1,...,.DK},C : {Ci,...,Cx}. In theory, for semantic layer k in frame #, it is sufficient to sample pixels in the region Mj, for efficient reconstruction. However, hash encoding can result in random and unstructured patterns in unsupervised regions, which decreases the performance of image-based models trained on natural images. To tackle this issue, we sample a number of points outside of the region Mj, and train them using DL» loss with the ground truth color. In this way, we effectively regularize Mj. with the background loss Lye. Consequently, the canonical image attains a more natural appearance, leading to enhanced processing results. Training Objectives. The representation is trained by minimizing the objective function Lyec. This function corresponds to the Lz loss between the ground truth color and the predicted color c for a given coordinate x. To regularize and stabilize the training process, we introduce additional regularization terms as previously discussed. The total loss is calculated using the following equation L = Lyee + Ar * Leow; (8) where ; represents the hyper-parameters for loss weights. It’s important to note that when training the grouped deformation field, we include an additional regularizer, denoted as Az * Log. --- --1-th frame 10-th frame 20-th frame 30-th frame Canonical Image Figure 5. Visualization of point correspondence across frames, which is directly extracted from the temporal deformation field after reconstructing the video with CoDeF. 3.3. Application to Consistent Video Processing Upon the optimization of the content deformation field, the canonical image I, is retrieved by setting the deformation of all points to zero. It is important to note that the size of the canonical image can be flexibly adjusted to be larger than the original image size depending on the scene movement observed in the video, thereby allowing more content to be included. The canonical image I, is then utilized in executing various downstream algorithms for consistent video processing. We evaluated the following state-of-the-art (SOTA) algorithms: (/) ControlNet [69]: Used for prompt-guided video-to-video translation. (2) Segment-anything (SAM) [18]: Applied for video object tracking. (3) R-ESRGAN [61]: Employed for video superresolution. Additionally, the canonical image allows users to conveniently edit the video by directly modifying the image. We further illustrate this capability through multiple manual video editing examples. 4. Experiments 4.1. Experimental Setup We conduct experiments to underscore the robustness and versatility of our proposed method. Our representation is robust with a variety of deformations, encompassing rigid and non-rigid objects, as well as complex scenarios such as smog. The default parameters for our experiments are set with the anneal begin and end steps at 4000 and 8000, respectively. The total iteration step is capped at 10000. On a single NVIDIA A6000 GPU, the average training duration is approximately 5 minutes when utilizingvideo frames. It should be noted that the training time varies with several factors such as the length of the video, the type of motion, and the number of layers. By adjusting the training parameters accordingly, the optimization duration can be varied from | to 10 minutes. 4.2. Evaluation The evaluation of our representation is concentrated on two main aspects: the quality of the reconstructed video with the estimated canonical image, and the quality of downstream video processing. Owing to the lack of accurate evaluation metrics, conducting a precise quantitative Figure 6. Video object tracking results achieved by lifting an image segmentation algorithm [18] through our CoDeF. Low Resolution High Resolution BA wisi: Figure 7. Video super-resolution results achieved by lifting an image super-resolution algorithm [61] through our CoDeF. analysis remains challenging. Nevertheless, we include a selection of quantitative results for further examination. Reconstruction Quality. In a comparative analysis with the Neural Image Atlas, our model, as demonstrated in Fig. 3, exhibits superior robustness to non-rigid motion, effectively reconstructing subtle movements with heightened precision (e.g. eyes blinking, face textures). Quantitatively, the video reconstruction PSNR of our algorithm on the collected video datasets is 4.4 dB higher. In comparison between the atlas and our canonical image, our results provide a more natural representation, and thus, facilitate the easier application of established image algorithms. Besides, our method makes a significant progress in training efficiency, i.e., 5 minutes (ours) vs. 10 hours (atlas). Downstream Video Processing. We provide an expanded range of potential applications associated with the pro --- --posed representations, including video-to-video translation, video keypoint tracking, video object tracking, video superresolution, and user-interactive video editing. (a) Video-to-video Translation. By applying image translation to the canonical image, we can perform video-to-video translation. A qualitative comparison is presented encompassing several baseline methods that fall into three distinct categories: (1) per-frame inference with image translation models, such as ControlNet [69]; (2) layered video editing, exemplified by Text-to-live [1]; and (3) diffusionbased video translation, including Tune-A-Video [64] and FateZero [36]. As depicted in Fig. 4, the per-frame image translation models yield high-fidelity content, accompanied by significant flickering. The alternative baselines exhibit compromised generation quality or comparatively low temporal consistency. The proposed pipeline effectively lifts image translation to video, maintaining the high quality associated with image translation algorithms while ensuring substantial temporal consistency. A thorough comparison is better appreciated by viewing the accompanying videos. (b) Video Keypoint Tracking. By estimating the deformation field for each individual frame, it is feasible to query the position of a specific keypoint in one frame within the canonical space and subsequently identify the corresponding points present in all frames as in Fig. 5. We show the demonstration of tracking points in non-rigid objects such as fluids in the videos on the project page. (c) Video Object Tracking. Using the segmentation algorithms on the canonical image, we are able to facilitate the propagation of masks throughout all video sequences leveraging the content deformation field. As illustrated in Fig. 6, our pipeline proficiently yields masks that maintain consistency across all frames. (d) Video Super-resolution. By directly applying the image super-resolution algorithm to the canonical image, we can execute video super-resolution to generate high-quality video as in Fig. 7. Given that the deformation is represented by a continuous field, the application of super-resolution will not result in flickering. (e) User interactive Video Editing. Our representation allows for user editing on objects with unique styles without influencing other parts of the image. As exemplified in Fig. 8, users can manually adjust content on the canonical image to perform precise edits in areas where the automatic editing algorithm may not be achieving optimal results. 4.3. Ablation Study To validate the effect of the proposed modules, we conducted an ablation study. On substituting the 3D hash encoding with positional encoding, there is a notable decrease in the reconstruction PSNR of the video by 3.dB. In the absence of the annealed hash, the canonical image loses its natural appearance, as evidenced by the Input Video Ey = a |Figure 8. User interactive video editing achieved by editing only one image and propagating the outcomes along the time axis using our CoDeF. We strongly encourage the readers to see the videos on the project page to appreciate the temporal consistency. w/o Anneal Ours Canonical Image Transferred A i} z EB Co Figure 9. Ablation study on the effectiveness of annealed hash. The unnaturalness in the canonical image will harm the performance of downstream tasks. presence of multiple hands in Fig. 9. Furthermore, without incorporating the flow loss, smooth areas are noticeably affected by pronounced flickering. For a more extensive comparison, please refer to the videos on the project page. 5.
--- CONCLUSION ---
and Discussion In this paper, we have investigated representing videos as content deformation fields, focusing on achieving temporally consistent video processing. Our approach demonstrates promising results in terms of both fidelity and temporal consistency. However, there remain several challenges to be addressed in future work. One of the primary issues pertains to the per-scene optimization required in our methodology. We anticipate that advancements in feed-forward implicit field techniques [57, 67] could potentially be adapted to this direction. Another challenge arises in scenarios involving extreme changes in viewing points. To tackle this issue, the incorporation of 3D prior knowledge [8] may prove beneficial, as it can provide additional information and constraints. Lastly, the --- --handling of large non-rigid deformations remains a concern. To address this, one potential solution involves employing multiple canonical images [33], which can better capture and represent complex deformations. ReferencesOmer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In Eur. Conf. Comput. Vis., 2022. Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yingiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint arXiv:2304.08465, 2023. Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In JEEE Conf. Comput. Vis. Pattern Recog., 2021. Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Adv. Neural Inform. Process. Syst., 2021. Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023. Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789, 2023. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Adv. Neural Inform. Process. Syst., 2020. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205. 15868, 2022. Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk. In Adv. Neural Inform. Process. Syst., 2020. Varun Jampani, Raghudeep Gadde, and Peter V Gehler. Video propagation networks. In IEEE Conf. Comput. Vis. Pattern Recog., 2017. Ondfej JamriSka, Sarka Sochorova, Ondfej Texler, Michal Lukaé, Jakub FiSer, Jingwan Lu, Eli Shechtman, and DanielSykora. Stylizing video by example. ACM Trans. Graph., 38(4):1-11, 2019. Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Trans. Graph., 40(6):1-12, 2021. Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. Chenyang Lei, Yazhou Xing, Hao Ouyang, and Qifeng Chen. Deep video prior for video consistency and propagation. IEEE Trans. Pattern Anal. Mach. Intell., 45(1):356-371, 2022. Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In JEEE Conf: Comput. Vis. Pattern Recog., 2021. Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. arXiv preprint arXiv:2303.04761, 2023. Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman, David Salesin, William T Freeman, and Michael Rubinstein. Layered neural rendering for retiming people in video. arXiv preprint arXiv:2009.07833, 2020. Erika Lu, Forrester Cole, Weidi Xie, Tali Dekel, Bill Freeman, Andrew Zisserman, and Michael Rubinstein. Associating objects and their effects in video through coordination games. In Adv. Neural Inform. Process. Syst., 2022. Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Generating images from captions with attention. arXiv preprint arXiv:1511.02793, 2015. Mateusz Michalkiewicz, Jnony K Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders Eriksson. Implicit surface representations as layers in neural networks. In Jnt. Conf. Comput. Vis., 2019. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Eur. Conf: Comput. Vis., 2020. Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. Thomas Miiller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1102:15, 2022. --- ---Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In JEEE Conf. Comput. Vis. Pattern Recog., 2019. Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Int. Conf. Comput. Vis., 2021. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo MartinBrualla, and Steven M. Seitz. Hypernerf: A higherdimensional representation for topologically varying neural radiance fields. ACM Trans. Graph., 40(6), 2021. Songyou Peng, Michael Niemeyer, Lars Mescheder, Mare Pollefeys, and Andreas Geiger. Convolutional occupancy networks. In Eur. Conf. Comput. Vis., 2020. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In JEEE Conf: Comput. Vis. Pattern Recog., 2021. Chenyang Qi, Xiaodong Cun, Yong Zhang, C! Xintao Wang, Ying Shan, and Qifeng Chen. Fusing attentions for zero-shot text-based video editing. arXiv preprint arXiv:2303.09535, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. Mach. Learn., 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Int. Conf. Mach. Learn., 2021. Alex Rav-Acha, Pushmeet Kohli, Carsten Rother, and Andrew Fitzgibbon. Unwrap mosaics: A new representation for video editing. In SIGGRAPH, pages 1-11, 2008. Scott E Reed, Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. Learning what and where to draw. In Adv. Neural Inform. Process. Syst., 2016. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjérm Ommer. High-resolution image synthesis with latent diffusion models. In [EEE Conf. Comput. Vis. Pattern Recog., 2022. Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox. Artistic style transfer for videos. In Pattern Recognition: 38th German Conference, GCPR 2016, Hannover, Germany, September 12-15, 2016, Proceedings 38, pages 26-36. Springer, 2016. enyang Lei, Fatezero:[44] [45]Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In JEEE Conf. Comput. Vis. Pattern Recog., 2023. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Adv. Neural Inform. Process. Syst., 2022. Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In JEEE Conf: Comput. Vis. Pattern Recog., 2022. Jonathan Shade, Steven Gortler, Li-wei He, and Richard Szeliski. Layered depth images. In Proceedings of the 25th annual conference on Computer graphics and interactive techniques, 1998. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Adv. Neural Inform. Process. Syst., 2020. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Adv. Neural Inform. Process. Syst., 2020. Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Eur. Conf: Comput. Vis., 2020. Ondyej Texler, David Futschik, Michal Kuéera, Ondiej Jamriska, Sarka Sochorova, Menclei Chai, Sergey Tulyakov, and Daniel Sykora. Interactive video stylization using fewshot patch-based training. ACM Trans. Graph., 39(4):73-1, 2020. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In IEEE Conf. Comput. Vis. Pattern Recog., 2023. Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021. --- ---Qiangian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, and Fang Wen. Pretraining is all you need for image-to-image translation. arXiv preprint arXiv:2205.12952, 2022. Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023. Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of time. In IEEE Conf. Comput. Vis. Pattern Recog., 2019. Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In Int. Conf. Comput. Vis., 2021. Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction. arXiv preprint arXiv:2212.05231, 2022. Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Niiwa: Visual synthesis pretraining for neural visual world creation. In Eur Conf. Comput. Vis., 2022. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Finegrained text to image generation with attentional generative adversarial networks. In JEEE Conf: Comput. Vis. Pattern Recog., 2018. Vickie Ye, Zhengqi Li, Richard Tucker, Angjoo Kanazawa, and Noah Snavely. Deformable sprites for unsupervised video decomposition. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In IEEE Conf. Comput. Vis. Pattern Recog., 2021. Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Int. Conf. Comput. Vis., 2017. Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.
