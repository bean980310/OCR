--- ABSTRACT ---
The text-driven image and video diffusion models have achieved unprecedented success in generating realistic and diverse content. Recently, the editing and variation of existing images and videos in diffusion-based generative models have garnered significant attention. However, previous works are limited to editing content with text or providing coarse personalization using a single visual clue, ren dering them unsuitable for indescribable content that requires fine-grained and detailed control. In this regard, we propose a generic video editing framework called Make-AProtagonist, which utilizes textual and visual clues to edit videos with the goal of empowering individuals to become the protagonists. Specifically, we design a visual-textualbased video generation model coupled with a mask-guided fusion method to integrate source video, target visual and textual clues. Extensive results demonstrate the versatile --- --and remarkable editing capabilities of Make-A-Protagonist. 1.
--- INTRODUCTION ---
“The protagonist sets the scene for the entire story.” — Ben Okri [24] Diffusion-based generative models have demonstrated remarkable success in generating photorealistic and diverse images [27, 28, 31] and videos [34, 38, 41] conditioned on text. However, the generation process, while more diverse, lacks controllability when relying solely on text descriptions. To generate the desired content, researchers explore two approaches to modify text-conditioned generation: incorporating external control signals [5, 13, 23, 40] and editing existing content with textual information [8, 19, 21, 25]. However, both approaches rely on text descriptions to convey the desired content, which raises an important question: what if the content cannot be accurately described through text? As an illustration, we consider the source video shown in Figure 1. Previous studies [19, 25] are capable of replacing the “Suzuki Jimny” with a recognizable brand, such as “Porsche” or “Mercedes-Benz” since these options are identifiable by the text model. Nevertheless, they are unable to substitute it with an unnamed car. For example, the reference image in 2rd row of Fig. | is generated by a text-toimage model with the text prompt “a black car”, so it does not have a corresponding name that can be precisely recognized by the language model. In contrast, people have the strong motivation to personalize their own content and to change the protagonist, which may not be accurately described through text. This motivation has served as inspiration for the development of personalized models [22, 30] and image/video variation techniques [5, 27]. Nevertheless, personalized models necessitate fine-tuning for each specific target using multiple target images and exhibit sensitivity to hyperparameters and training configuration. Furthermore, image and video variation models tend to exhibit bias toward the background of the reference images. Additionally, both types of generation models are typically limited to generating content based on a single visual reference clue, which proves ineffective when multiple protagonists need to be modified. These limitations motivate us to design a framework for generic video editing with text and an arbitrary number of reference images. One recent work [38] can be applied to generic video editing by combining personalized modeling [30] with oneshot video editing. Specifically, this work first fine-tunes a text-to-image generation (T2I) model with the reference image. After that, it inflates the T2I model with temporal layers and optimizes it on the source video. However, the image-video tuning process has a severe limitation: the two models should be retrained when changing the refer ence image. In addition, it also inherits the issues of per sonalized modeling, sensitive to hyperparameters. To this end, this paper disentangles the personally edited content (protagonists) from the source video to realize end-to-end one-stage generic video editing. To achieve generic video editing, the framework necessitates the ability of leveraging visual clues, leveraging textual clues and maintaining source motion. Therefore, we build our framework based on Stable UnCLIP *, which takes CLIP image and text embeddings as conditions. The image embedding is directly added to the model features while text embedding is utilized via cross-attention. Following [34, 38, 41], we inflate the T2I model (Stable UnCLIP) to one-shot text-to-video generation (T2V) model by including additional temporal modules. After tuning the T2V model with the source video, we propose a novel mask-guided fusion to incorporate the visual clue, textual clue and source motion with the mask of the protagonist of the source video. The mask is obtained from pre-trained models [4, 16, 18], eliminating the need for data annotation. With the diffusion-based T2V model and effective mask-guided fusion method, our framework (MakeA-Protagonist) achieves commendable performance in both conventional and novel video editing tasks, including protagonist editing while keeping the background, background editing of the source content, and text-to-video editing with the protagonist. Our contributions can be summarized as: * We present the first end-to-end framework for generic video editing with both visual and textual clues. * We design a visual-textual-based video generation model and a novel mask-guided fusion to incorporate the visual clue, textual clue and source motion, realizing strong video editing performance. ¢ Extensive results demonstrate the versatile applications of Make-A-Protagonist and the superiority over previous video editing works. 2.
--- RELATED WORK ---
Visual Content Generation. Content generation has made remarkable advancements with powerful generative models [14, 37, 39]. Recently, equipped with vast and diverse image-text pairs from the internet, diffusion-based generative models [27, 28, 31] have outperformed GAN-based
--- METHOD ---
to integrate source video, target visual and textual clues. Extensive results demonstrate the versatile --- --and remarkable editing capabilities of Make-A-Protagonist. 1. Introduction “The protagonist sets the scene for the entire story.” — Ben Okri [24] Diffusion-based generative models have demonstrated remarkable success in generating photorealistic and diverse images [27, 28, 31] and videos [34, 38, 41] conditioned on text. However, the generation process, while more diverse, lacks controllability when relying solely on text descriptions. To generate the desired content, researchers explore two approaches to modify text-conditioned generation: incorporating external control signals [5, 13, 23, 40] and editing existing content with textual information [8, 19, 21, 25]. However, both approaches rely on text descriptions to convey the desired content, which raises an important question: what if the content cannot be accurately described through text? As an illustration, we consider the source video shown in Figure 1. Previous studies [19, 25] are capable of replacing the “Suzuki Jimny” with a recognizable brand, such as “Porsche” or “Mercedes-Benz” since these options are identifiable by the text model. Nevertheless, they are unable to substitute it with an unnamed car. For example, the reference image in 2rd row of Fig. | is generated by a text-toimage model with the text prompt “a black car”, so it does not have a corresponding name that can be precisely recognized by the language model. In contrast, people have the strong motivation to personalize their own content and to change the protagonist, which may not be accurately described through text. This motivation has served as inspiration for the development of personalized models [22, 30] and image/video variation techniques [5, 27]. Nevertheless, personalized models necessitate fine-tuning for each specific target using multiple target images and exhibit sensitivity to hyperparameters and training configuration. Furthermore, image and video variation models tend to exhibit bias toward the background of the reference images. Additionally, both types of generation models are typically limited to generating content based on a single visual reference clue, which proves ineffective when multiple protagonists need to be modified. These limitations motivate us to design a framework for generic video editing with text and an arbitrary number of reference images. One recent work [38] can be applied to generic video editing by combining personalized modeling [30] with oneshot video editing. Specifically, this work first fine-tunes a text-to-image generation (T2I) model with the reference image. After that, it inflates the T2I model with temporal layers and optimizes it on the source video. However, the image-video tuning process has a severe limitation: the two models should be retrained when changing the refer ence image. In addition, it also inherits the issues of per sonalized modeling, sensitive to hyperparameters. To this end, this paper disentangles the personally edited content (protagonists) from the source video to realize end-to-end one-stage generic video editing. To achieve generic video editing, the framework necessitates the ability of leveraging visual clues, leveraging textual clues and maintaining source motion. Therefore, we build our framework based on Stable UnCLIP *, which takes CLIP image and text embeddings as conditions. The image embedding is directly added to the model features while text embedding is utilized via cross-attention. Following [34, 38, 41], we inflate the T2I model (Stable UnCLIP) to one-shot text-to-video generation (T2V) model by including additional temporal modules. After tuning the T2V model with the source video, we propose a novel mask-guided fusion to incorporate the visual clue, textual clue and source motion with the mask of the protagonist of the source video. The mask is obtained from pre-trained models [4, 16, 18], eliminating the need for data annotation. With the diffusion-based T2V model and effective mask-guided fusion method, our framework (MakeA-Protagonist) achieves commendable performance in both conventional and novel video editing tasks, including protagonist editing while keeping the background, background editing of the source content, and text-to-video editing with the protagonist. Our contributions can be summarized as: * We present the first end-to-end framework for generic video editing with both visual and textual clues. * We design a visual-textual-based video generation model and a novel mask-guided fusion to incorporate the visual clue, textual clue and source motion, realizing strong video editing performance. ¢ Extensive results demonstrate the versatile applications of Make-A-Protagonist and the superiority over previous video editing works. 2. Related Work Visual Content Generation. Content generation has made remarkable advancements with powerful generative models [14, 37, 39]. Recently, equipped with vast and diverse image-text pairs from the internet, diffusion-based generative models [27, 28, 31] have outperformed GAN-based methods in the text-to-image generation (T2I). In view of the impressive performance in T2I, text-to-video generation (T2V) [11, 12, 20, 34, 41] has attracted much attention. T2V models often utilize pre-trained T2I models to leverage the abundant image-text resources. Additionally, temporal modules [34, 41] are introduced to facilitate video representation learning. Furthermore, temporal consistency “https://hug gingface.co/stabilityai/stable-diffusion-2- 1-unclip-small --- --“A Suzuki Jimny driving down a mountain road” ii Source Video if “A car driving down a road in the snow” Inverted Latents aA key Frame Attention cLIP Map Vision | | Reference s]ua}e7] payanu} —_— s CLIP jource Image Embed —— Attention Map jf Image i \ t_— CLIP +[ResBlock Text Embed } \ 7 Self-Attn -— + Cross-Attnuo Mask-Guided Fusion Figure 2. The overall inference framework of Make-A-Protagonist. Masks and control signals are extracted from the source video. Source video, visual and textual clues are fused in the video generation model with the mask-guided fusion to enable generic video editing. is also established in latent space to enhance video generation [15, 20]. Visual Content Editing and Variation with Text. An alternative direction for content generation is the editing of existing images [2, 8, 21, 36] and videos [1, 19, 25, 33, 38] using text, instead of uncontrolled generation solely based on text descriptions. SDEdit [21] applies noise to the image and recovers the image for editing purposes. Promptto-prompt [8] and Plug-and-Play [36] modify the crossattention map by changing the text description. For video editing, Text2Live [1] divides the video into layers and edits each layer separately using a text description. TuneA-Video [38] inflates and fine-tunes the T2I model on a single video and generates new videos of similar motion. Video-P2P [19] and FateZero [25] extend the Prompt-toprompt into video level. TokenFlow [7] edits several key frames with prompt-to-prompt and then propagates them to the whole video via feature matching. However, previous methods solely focus on content editing through text, making them unsuitable for indescribable content. Visual Content Variation and Personalization. To address the indescribable content, DALL-E 2 [27] and Gen1 [5] perform image and video variation using CLIP [26] image embedding. However, since the CLIP vision model extracts information from the whole image, background information is inevitably incorporated into the variation. Personalized models [6, 22, 30] present an alternative approach for handling indescribable content. However, DreamBooth [30] and DreamMix [22] necessitate multiple images to fine-tune a T2I and T2V model for concept learning, and the fine-tuning process is sensitive to training configuration. Considering the limitations of previous works, this paper presents a framework for generic video editing with both image and text descriptions. 3. Make-A-Protagonist 3.1. Overview To realize generic video editing with both visual and textual clues, we introduce a video generation model that can take both CLIP image embedding and CLIP text embedding as conditions (Sec. 3.2). In addition, since the CLIP image embedding is directly added to the features, it lacks spatial control compared with the cross-attention used by text embedding. Therefore, we propose a mask-guided fusion technique, leveraging the mask of the protagonist in the source video to accurately control the spatial location during inference (Sec. 3.3). Furthermore, we introduce attention fusion and ControlNet to precisely control the semantic and spatial information of the background and protagonist, respectively. Equipped with the video generation model and mask-guided fusion, our framework achieves three applications (Sec. 3.4). The overall framework is depicted in Fig. 2. 3.2. Visual-Textual-based Video Generation Model Latent Diffusion Models (LDMs) with Image Embedding. LDMs consist of two main components: an autoencoder that encodes an RGB image « into a latent z = E(x) with the encoder €, and a decoder D that reconstructs the image x © D(z) from z. In the latent space, a U-Net [29] <containing residual blocks and self-/cross-attention is used --- --—> Source Flow —> Text Flow Diffusion Source Video A Suzuki Jimny driving down a mountain road —»> Reference Flow Reconstruction Loss yndjno aouasajay CLIP e Vision Figure 3. Training process of visual-textual-based video generation model. The model is trained with the video caption and randomly sampled reference frame. to remove the noise with the objective: | /ar-e —V1l—ar- 20 —€6 (2,t,C,Z)I|,. qd) where C and Z denote the embedding of the text and image, respectively. Noise ¢ is added to zp according to step t to obtain z,;. Note that instead of directly predicting the noise € [10], following [5, 11], we use v-parameterization [32] to improve color consistency of videos. Regarding text and image conditions, the text condition serves as the key and value for cross-attention while the image condition is directly added to features of residual blocks, which aligns with [28] and [27], respectively. DDIM Inversion. Deterministic DDIM sampling is used to generate each frame from a latent noise in T’ denoising steps: min Exo,e,t a1 = Sori (Jar 2 — V1 = a1 £0) + 1 = an (far eg + VI = a 2%), where t : 1 — T denotes the timestamp, a; is a parameter for noise scheduling [35]. Same with Eq. 1, we use v-parameterization for denoising. Since Make-A-Protagonist focuses on video editing, instead of using random noise during inference, we use DDIM inversion [35] to transform the source video into the initial latent code that requires denoising. This approach enables the preservation of both the motion and structural information presented in the source video, while also enhancing the temporal consistency within the latent space. The DDIM inversion is illustrated as: zegi = Varqi (Var % — V1 — a4 €0) + /1- ary (Jar co + V1 — a4 1). Video Generation Model. In this paper, we propose a visual-textual video generation model based on LDMs (Latent Difference Models) to integrate source video, visual (2) (3) and textual clues to achieve generic video editing. The training process of our video generation model is depicted in Fig. 3. Following [34, 38, 41], we initialize our model with text-to-image LDMs and we modify the U-Net to capture and exploit temporal correlations via temporal convolution, temporal attention, and spatio-temporal attention [38]. Our video generation model takes both textual and visual clues as conditioning inputs. To obtain the textual clue, we employ BLIP-2 [17] to extract the video caption. The caption is then transformed into a text embedding using the CLIP Text Model [26], which serves as the key and value in the cross-attention mechanism within the U-Net. To ensure the ability to incorporate image embedding, we randomly select one frame from the video as the reference frame at each iteration. The reference frame is encoded using CLIP Vision Model [26] and added to residual blocks in U-Net. Note that text and image embeddings are applied to all the blocks in the U-Net. 3.3. Mask-Guided Fusion Source Video Masks. Since the visual clues (CLIP image embedding) are added directly to the intermediate features in the residual blocks instead of operations with spatial information (e.g., cross-attention), the model cannot precisely control the location of the reference object. Therefore, we leverage the masks of the protagonist in the source video to separate foreground and background for spatial control. Specifically, we first extract the protagonist mask in the first frame with Segment Anything [16]. Then XMem [4] is adopted to track the mask across the whole video. Overall Fusion Process. During the inference stage, the video generation model employs the DDIM inversed latent code as a starting point to denoise the code using the target textual and visual clues, as indicated by Eq. 2. Since the CLIP Vision Model [26] encodes the whole reference image, the background in the reference image is inevitably in --- --troduced into the generated result. To address this, we mask out the desired reference part via Grounded-SAM [16, 18]. However, when attempting to change the background of the source video, e.g. “in the desert” in Fig. 2, the editing process may not yield satisfactory results due to the masked reference image. The reason behind this is that the reference image embedding, which is directly added to the features, exerts a more dominant influence compared to the textual clues employed by the cross-attention mechanism. Therefore, we incorporate the DALL-E 2 Prior [27], capable of converting text embeddings into image embeddings, to enhance the representation of textual clues. The prior embedding and reference image embedding are fused together with the source masks. Despite splitting the protagonist and background with the source masks, the protagonist and background may not be consistent across the video as the spatial information introduced by the source masks is quite coarse. Instead, selfattention maps can serve as good guidance for spatial location in each frame, and the spatial locations across the source video are consistent. Therefore, we can seek the background self-attention maps from the source video and use these maps to extract information from the target background. The attention fusion technique can improve the representation of the target background while keeping a similar temporal consistency as the source video. As for the protagonist part, we additionally leverage ControlNet [40] to provide more precise spatial control. Feature fusion, attention fusion, and ControlNet are introduced as follows. Feature Fusion. Given the latent feature zf € RF*xCxHXW in residual block, the image embedding Z € R!*C is added to all frames and spatial location. We leverage the video protagonist masks M € R’*#*™ to fuse the reference image embedding Zz € R!*© and prior converted text embedding Zp € R!*°: of = 2h +M-Ip+(1—M)-Tp. (4) In addition, since Zz represent the information of the protagonist, we introduce a timestamp parameter Tp to control the ending step of the feature fusion operation: ep [Eh +M-Te+(l-M) Tp ift<te og 2 +Ir otherwise. Since the spatial location is determined in the early steps of the denoising process while details are refined in the latter stages, introducing tp can lead the model to contain more detailed information about the reference image. Attention Fusion. For the self-attention latent feature 2! © RPXCXHXW | frame-wise self-attention maps Ay € RP *xHWxHW are calculated from the query and key projected features Q(z) and K(z/). We leverage the video protagonist masks M ¢€ R**#*W to fuse these self attention maps with the source video self-attention maps A? obtained during DDIM inversion: A, =M-A,+(1—M)- Ap. (6) Similarly, a timestamp parameter 7,4 is introduced to control the ending step of the attention fusion operation: Ay=M-A,+(1—M)- AP ift< A= 4 PRS M)-Ar Hts ta Gy At otherwise. ControlNet. Source masks are coarse spatial constraint that applies the same information within them. Incorporating visual clues is difficult and such coarse constraint cannot lead to satisfactory results in most cases. Therefore, we additionally use a ControlNet [40] to enhance the spatial requirements within the mask. Specifically, the ControlNet branch takes one type of condition (pose or depth in this paper) together with the CLIP image embedding of the reference image. Then the ControlNet features are masked out by the source video masks and added to intermediate features in the diffusion U-Net. Equipped with feature fusion, attention fusion and ControlNet, Make-A-Protagonist can generate consistent video with both visual and textual clues. 3.4. Applications of Make-A-Protagonist We divide a video into background and protagonist, and Make-A-Protagonist can edit either or both of them. Fig. | illustrates three applications supported by our method. In this section, we denote the reference image embedding Tp and the prior converted text embedding Zp in Eq. 5 as protagonist embedding and background embedding. Protagonist Editing: In this application (2nd row in Fig. 1), the background of the edited video should remain the same as the source video. Thus, instead of employing DALLE 2 Prior [27] to convert the text into background embedding, we utilize the CLIP Vision Model [26] to encode each frame of the source video as the background embedding (Zp), while the image embedding from the reference image serves as the protagonist embedding (Zp). Background Editing: To edit the background while preserving the protagonist (3rd row in Fig. 1), we encode the frames of the source videos as the protagonist embedding Tr, and the text describing the background is converted to background embedding Zp by DALL-E 2 Prior [27]. Text-to-Video Editing with Protagonist: By leveraging a reference image and text, both the background and protagonist can be edited simultaneously (4th row in Fig.1). In this application, the reference image is encoded as the protagonist embedding (Zr) using the CLIP Vision Model [26], and the text is transformed into background embedding (Zp) using DALL-E 2 Prior [27]. --- --g | E ! 3 a ul > [8 i — : = °. A man playing basketball fo} od< Em < e eal A man playing basketball on the beach, anime style = o v g a i r~ A man playing basketball on the beach, anime style ir < o oO > = ° [o} [-*) £ G Q a a | a. E A [V] playing basketball on the beach, anime styleSs ° A man playing basketball on the beach, anime style A car moving in the desert Figure 4. Comparison with other video generation and editing methods. Make-A-Protagonist can change the protagonist with the visual clue while editing the background with the textual clue. 4.
--- EXPERIMENT ---
s 4.1. Implementation Details Our video generation model is based on text-to-image (T21) latent diffusion models with image embedding [28] (Stable UnCLIP). After initializing with the T2I LDM and inserting the temporal modules into the model, the video generation model is fine-tuned on 8 frames from a video at a resolution of 768 x 768. The model is trained for 200 steps with a learning rate of 3 x 10~°. During inference, we use DDIM sampler [35] with classifier-free guidance [9] for 20 steps. It takes about 10 minutes to train a single video and 30 seconds for inference. 4.2. Comparison with Baselines Baselines. Since there are no previous methods conducting generic video editing, we compare Make-A-Protagonist with text-based editing [25, 38], video variation [5], and --- --Table 1. Quantitative evaluation. Make-A-Protagonist achieves comparable model evaluation with DreamBooth-V while overwhelming preference in the user study. Model Evaluation User Study CLIP-T + DINO? | Quality + Subject Prompt t Make-A-Protagonist 0.337 0.485 71.6% 67.2% 69.8% DreamBooth-V [30, 38] 0.301 0.509 21.0% 21.2% 12.4% Both Equally — — TA% 11.6% 17.8% Source Video Ours A <man> riding a <motorbike> on the beach, anime style Figure 5. Example of editing two protagonists in one video. personalized [30] methods: (1) Tune-A-Video [38] finetunes an inflated LDM on a single video to generate related content. (2) FateZero [25] edits an existing video by controlling the cross-attention maps. (3) Gen-J [5] uses reference images or text to change the source video. Since Gen-can only use one condition (image or text), we use the reference image without text input. (4) DreamBooth-V [30, 38] is a baseline with a similar application as ours, combining DreamBooth [30] and Tune-A-Video [38]. Specifically, we first train a personalized DreamBooth model with one reference image and the corresponding text token [V] and then use this personalized model as the initialization for single video fine-tuning [38]. During inference, text token [V] can be used to generate content with the protagonist. Qualitative Evaluation. We compare the baselines on two videos in Fig 4. First, text-based editing methods [25, 38] cannot integrate the visual clues, limiting their capability to generate indescribable protagonists. Second, Gen-1 [5] cannot use both visual and textual clues simultaneously for generic editing. Moreover, Gen-1 [5] incorporates the background information from the reference image in the generated results. Third, DreamBooth-V [30, 38] can perform generic video editing as Make-A-Protagonist. However, one reference image is not enough to learn a good DreamBooth model for complex protagonists (e.g., the man in the left video) and thus DreamBooth-V struggles to generate realistic videos. Furthermore, even though the personalized model is well-trained, it requires a separate model for each protagonist with a careful selection of training configurations. In addition, DreamBooth-V is limited to editing one protagonist in a video while Make-A-Protagonist demonstrates the ability to edit multiple protagonists in Fig. 5. Quantitative Evaluation. We conduct a quantitative comparison with DreamBooth-V [30, 38] on generic video editing. Following [30], prompt fidelity is measured by the average cosine similarity between the prompt and each video frame ViT-B/32 CLIP [26] embeddings, denoted as CLIPT. Subject fidelity is measured by the average cosine similarity between the ViT-S/16 DINO [3] embeddings of each generated video frame and the reference image, denoted as DINO. In addition, we conduct a user study to compare the two methods in terms of video quality, subject fidelity, and prompt fidelity. We asked 20 users to answer questionnaires of 25 comparative questions. Each comparative question shows the reference image, text prompt, and videos generated by the two methods in random order. For video quality, users are asked to answer the question: “Which of the two videos is more natural and realistic?”, and we include a “Cannot Determine/Both Equally” option. Similarly, we ask “Which of the two videos better reproduces the identity (e.g. item type and details) of the reference image?” and “Which of the two videos is better described by the text? (No need to take the reference image into account)” for subject fidelity and prompt fidelity, respectively. As shown in Tab. 1, Make-A-Protagonist achieves a better prompt fidelity evaluated by CLIP while DreamBooth-V --- --A <car> driving down a track at night (a) Source Video r= Ss Reference Image (b) w.o. ControlNet (c) w.o. DALL-E 2 Prior (d) w.o. Feature Fusion (e) w.o. Attention Fusion (f) Ours Figure 6. Qualitative ablation studies. Removing the proposed components impairs the subject fidelity, prompt fidelity and video quality. Table 2. Quantitative ablation studies. | CLIP-T+ DINO t+ w.o ControlNet [40] 0.324 0.w.o. DALL-E 2 Prior [27] 0.297 0.w.o. Feature Fusion 0.293 0.w.o. Attention Fusion 0.335 0.Make-A-Protagonist | 0.337 0.obtains a higher DINO score. However, we conjecture that the DINO score does not fully reflect the generated quality, since DreamBooth-V tends to overfit to the reference image. The overfitting leads to less realistic generated results but a higher DINO score (refer to qualitative comparison in the supplementary material). Regarding the user study, we find an overwhelming preference for Make-A-Protagonist in terms of video quality, subject fidelity, and prompt fidelity, demonstrating the superiority of our method. 4.3. Ablation Studies To verify the effectiveness of each component in Make-AProtagonist, we conduct ablation studies on the ControlNet [40], DALL-E 2 Prior [27], feature fusion and attention fusion both quantitatively (Tab. 2) and qualitatively (Fig. 6). ControlNet. Removing ControlNet can preserve the protagonist information but the motion is not consistent. In the first row of Tab. 2, removing ControlNet slightly influences the subject fidelity (DINO score) but impairs prompt fidelity (CLIP-T score). In addition, due to lacking precise spatial location for the protagonist, the orientation of the car is wrong in the first several frames (Fig. 6(b)). DALL-E 2 Prior. Deactivating DALL-E 2 Prior results in the text embedding being solely used in cross-attention. Consequently, the textual clue becomes weaker compared to the visual clue, leading to generating results that fail to align with the text description. Instead, since the CLIP image embedding for the background is zero, it would not influence the representation of the reference image. There fore, the image similarity is slightly increased. Feature Fusion. Feature fusion is crucial for maintaining the visual information, resulting in a 0.013 improvement in DINO score. Furthermore, removing feature fusion equals to integrating the CLIP image embedding of the reference image to the whole frames, and the DALL-E 2 Prior embedding is also excluded. Thus, the prompt fidelity is poor when deactivating feature fusion, which is also demonstrated in Fig. 6(d). Attention Fusion. Attention fusion aims to improve the temporal consistency by providing spatial information for the background. Comparing Fig. 6(e) and Fig. 6(f), attention fusion can address the inconsistent lane markings. With the integration of DALL-E 2 Prior and mask-guided fusion, Make-A-Protagonist showcases remarkable performance in both quantitative and qualitative assessments. 5.
--- CONCLUSION ---
This paper introduces Make-A-Protagonist, the first end-toend framework for generic video editing using textual and visual clues. To edit the protagonist and background with these clues, we design a visual-textual-based video generation model coupled with a mask-guided fusion method to integrate diverse information sources. The fusion approach effectively employs the masks, control signals and attention maps from the source video to provide precise spatial locations for editing both protagonist and background. By combining the video generation model with this fusion approach, Make-A-Protagonist empowers versatile and powerful generic video editing applications, including background editing, protagonist editing, and text-to-video editing with the protagonist. Limitations & Social Impacts. One limitation of our approach is that representing visual clues with CLIP image embedding may not be optimal. It may struggle to encompass the full range of possible variations in a subject. Additionally, the effectiveness of visual representation varies depending on the subject; for instance, our model demonstrates superior performance with cars compared to humans. --- --Regarding social impacts, while personalized image generation offers numerous benefits, it simultaneously heightens the risk of misuse. Therefore, it is better to introduce a safety check for the source video and reference image to a leviate the possible negative impacts. References 1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In ECCV, 2022.2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022.3] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.4] Ho Kei Cheng and Alexander G. Schwing. XMem: Longterm video object segmentation with an atkinson-shiffrin memory model. In ECCV, 2022. 2,5] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011, 2023. 2,3, 4, 6,6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.7] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023.8] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2,9] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu sion probabilistic models. In NeurIPS, 2020.11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2,12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022.13] Lianghua Huang, Di Chen, Yu Liu, Shen Yujun, Deli Zhao, and Zhou Jingren. Composer: Creative and controllable image synthesis with composable conditions. arXiv preprint arxiv:2302.09778, 2023.14] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. arXiv preprint arXiv:2303.05511, 2023.Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-toimage diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023.Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dolladr, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. 2,4,Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint 1rXiv:2303.05499, 2023. 2,aoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. rXiv:2303.04761, 2023. 2,iengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, iang Wang, Yujun Shen, Deli Zhao, Jinren Zhou, an jeniu Tan. Videofusion: Decomposed diffusion modis for high-quality video generation. arXiv preprint arXiv:2303.08320, 2023. 2,Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, JunYan Zhu, and Stefano Ermon. Sdedit: Image synthesis an editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2,Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedi: Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. 2,Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023.Ben Okri. The Famished Road. Jonathan Cape, 1991.Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv:2303.09535, 2023. 2, 3, 6,Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In JCML, 2021. 3, 4, 5,Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2, 3,4, 5,Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjérn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 4,wa iS} Hon @ --- ---Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015.Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022. 2, 3,Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022.Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022.Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, and Sungroh Yoon. Edit-a-video: Single video editing with object-aware consistency. arXiv preprint arXiv:2303.07945, 2023.Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2,Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 4,Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for textdriven image-to-image _ translation. arXiv preprint arXiv:2211.12572, 2022.Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017.Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. 2, 3, 4, 6,Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023. 2,5,Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Ly, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. 2,
