--- ABSTRACT ---
Text injection for automatic speech recognition (ASR), wherein unpaired text-only data is used to supplement paired audio-text data, has shown promising improvements for word error rate. This study examines the use of text injection for auxiliary tasks, which are the non-ASR tasks often performed by an E2E model. In this work, we use joint end-to-end and internal language model training (JEIT) as our text injection algorithm to train an ASR model which performs two auxiliary tasks. The first is capitalization, which is a de-normalization task. The second is turn-taking prediction, which attempts to identify whether a user has completed their conversation turn in a digital assistant interaction. We show results demonstrating that our text injection method boosts capitalization performance for long-tail data, and improves turn-taking detection recall. Index Terms: speech recognition, text injection, auxiliary tasks 1.
--- INTRODUCTION ---
Automatic speech recognition (ASR) has long been an integral part of important technologies, including voice dictation, digital assistants, and video captioning [1]. While ASR systems are typically evaluated based on word error rate (WER), this is not the only metric of concern in production applications; several “auxiliary tasks” must be integrated with the ASR task ina full system. These tasks may include: capitalization and punctuation, which improves readability; voice activity detection (VAD) and end-of-query (EOQ) detection, which are important for implementing low-latency systems; and natural conversation understanding, which involves predicting the cadence and turn-taking aspects of an ongoing conversation. In this study, we focus on improving the quality of such auxiliary tasks in an end-to-end (E2E) ASR setting via text injection. We build on two recent capabilities for speech models. First is the E2E integration of auxiliary tasks with the ASR task into a single model. In the past, auxiliary tasks were usually performed by separate models downstream of ASR [2, 3, 4, 5]. Recent work has successfully explored integrating auxiliary tasks, such as endpointing [6, 7, 8], capitalization [9], natural conversation understanding [10], and speaker diarization [11] into the same model as ASR prediction. E2E integration of ASR and auxiliary tasks has a key drawback, however. When folded into an E2E ASR model, pure text-to-text tasks (such as capitalization) can no longer be trained on plentiful text-only data (i.e., text data with no associated audio); instead, their training examples will be limited to the transcripts available in paired audio-text labeled data. This puts E2E methods at a disadvantage, since text-only data is generally more plentiful and easier to obtain than labeled audio data, and can be used to more easily expose the model to rare words and other long-tail phenomena which may be difficult to collect in labeled audio form [12]. The second capability enabling the current study is the use of “text injection” as a means of improving ASR quality [13]. An ASR model’s internal language model (ILM) is the notional part of the network that predicts the next token given the previous token history, independent of audio input. Though it is usually infeasible to exactly separate the influence of audio input from previous token predictions, several methods have been developed to estimate ILM scores [14, 15]. Text-only data can then be used to refine the ILM capabilities of the ASR network [16, 17]. In this work, we propose a method to utilize text injection techniques for improving auxiliary task performance in an E2E ASR model. Doing so allows auxiliary tasks to access the multi-task learning benefits of co-training with ASR while still including rich text-only data in their training corpora. We focus our study on two tasks: capitalization and conversational turn-taking prediction. The former is a strongly text-based task, since capitalization is merely a form of de-normalization from spoken to written domain, and capitalized words are not pronounced differently. The latter task clearly involves combining linguistic and acoustic understanding — the prosody of the input speech as well as the semantics of the current recognition are both informative for predicting whether a pause is only momentary or if the user has finished speaking. We integrate these tasks into a production-ready model, streaming E2E RNN-T ASR model [18, 19]. We show results demonstrating that text injection can meaningfully improve auxiliary task performance, particularly in long-tail settings. 2.
--- RELATED WORK ---
While auxiliary tasks are usually performed by separate models from ASR [20, 21], E2E approaches to auxiliary task modeling have been recently popular for production-grade systems. Joint training of ASR with endpointing [7], capitalization [9, 22], intended query detection [23, 24], sentence segmentation [25], and more, have been explored. Our work builds most closely on Wang et al.[9], who co-train ASR, capitalization, and turntaking prediction by building multiple parallel label sequences. To our knowledge, this is the first attempt to refine auxiliary tasks in an E2E ASR model using text-only data. There has long been interest in utilizing unpaired text data for the ASR task. Several approaches to LM fusion, the use of an external LM to improve ASR recognition quality, have been proposed [26]. These
--- METHOD ---
boosts capitalization performance for long-tail data, and improves turn-taking detection recall. Index Terms: speech recognition, text injection, auxiliary tasks 1. Introduction Automatic speech recognition (ASR) has long been an integral part of important technologies, including voice dictation, digital assistants, and video captioning [1]. While ASR systems are typically evaluated based on word error rate (WER), this is not the only metric of concern in production applications; several “auxiliary tasks” must be integrated with the ASR task ina full system. These tasks may include: capitalization and punctuation, which improves readability; voice activity detection (VAD) and end-of-query (EOQ) detection, which are important for implementing low-latency systems; and natural conversation understanding, which involves predicting the cadence and turn-taking aspects of an ongoing conversation. In this study, we focus on improving the quality of such auxiliary tasks in an end-to-end (E2E) ASR setting via text injection. We build on two recent capabilities for speech models. First is the E2E integration of auxiliary tasks with the ASR task into a single model. In the past, auxiliary tasks were usually performed by separate models downstream of ASR [2, 3, 4, 5]. Recent work has successfully explored integrating auxiliary tasks, such as endpointing [6, 7, 8], capitalization [9], natural conversation understanding [10], and speaker diarization [11] into the same model as ASR prediction. E2E integration of ASR and auxiliary tasks has a key drawback, however. When folded into an E2E ASR model, pure text-to-text tasks (such as capitalization) can no longer be trained on plentiful text-only data (i.e., text data with no associated audio); instead, their training examples will be limited to the transcripts available in paired audio-text labeled data. This puts E2E methods at a disadvantage, since text-only data is generally more plentiful and easier to obtain than labeled audio data, and can be used to more easily expose the model to rare words and other long-tail phenomena which may be difficult to collect in labeled audio form [12]. The second capability enabling the current study is the use of “text injection” as a means of improving ASR quality [13]. An ASR model’s internal language model (ILM) is the notional part of the network that predicts the next token given the previous token history, independent of audio input. Though it is usually infeasible to exactly separate the influence of audio input from previous token predictions, several methods have been developed to estimate ILM scores [14, 15]. Text-only data can then be used to refine the ILM capabilities of the ASR network [16, 17]. In this work, we propose a method to utilize text injection techniques for improving auxiliary task performance in an E2E ASR model. Doing so allows auxiliary tasks to access the multi-task learning benefits of co-training with ASR while still including rich text-only data in their training corpora. We focus our study on two tasks: capitalization and conversational turn-taking prediction. The former is a strongly text-based task, since capitalization is merely a form of de-normalization from spoken to written domain, and capitalized words are not pronounced differently. The latter task clearly involves combining linguistic and acoustic understanding — the prosody of the input speech as well as the semantics of the current recognition are both informative for predicting whether a pause is only momentary or if the user has finished speaking. We integrate these tasks into a production-ready model, streaming E2E RNN-T ASR model [18, 19]. We show results demonstrating that text injection can meaningfully improve auxiliary task performance, particularly in long-tail settings. 2. Related Work While auxiliary tasks are usually performed by separate models from ASR [20, 21], E2E approaches to auxiliary task modeling have been recently popular for production-grade systems. Joint training of ASR with endpointing [7], capitalization [9, 22], intended query detection [23, 24], sentence segmentation [25], and more, have been explored. Our work builds most closely on Wang et al.[9], who co-train ASR, capitalization, and turntaking prediction by building multiple parallel label sequences. To our knowledge, this is the first attempt to refine auxiliary tasks in an E2E ASR model using text-only data. There has long been interest in utilizing unpaired text data for the ASR task. Several approaches to LM fusion, the use of an external LM to improve ASR recognition quality, have been proposed [26]. These methods have the drawback of increasing total parameter count (due to the size of the LM model), and computation cost during inference. Text injection [13] solves these issues by using LM-style unpaired text data to train the --- --ASR model itself. Some methods focus on fine-tuning an existing ASR model trained on audio-text data; ILM adaptation of the ASR decoder has been shown to work well [27, 28, 29]. The text injection method we employ here is joint end-to-end and ILM training (JEIT), which was introduced by Meng et al [30]. We choose JEIT as our method due to its lightweight nature; its primary focus on refining the ASR decoder makes comparison to standard methods straightforward, since the behavior of the audio encoder is preserved. Other methods inject text data directly into the encoder, with fixed and learned duration models to align text and audio sequences [16, 17]. All of the above works focus on improving ASR quality, both for standard and long-tail data; to the best of our knowledge, adapting these techniques for auxiliary tasks is a novel contribution to the literature. 3. Auxiliary Tasks 3.1. Capitalization Capitalization is the process of restoring the correct case (uppercase or lowercase) of noisy text. Notably, capitalization is specific to the written domain, and has no marker in spoken speech. This task is important for maintaining readability in ASR output, especially for long-form captioning cases. 3.2. Conversational turn-taking Turn-taking is an active area of research for E2E speech modeling [10, 31]. While humans typically adjust their speech when interacting with voice assistants [31], natural human speech patterns during conversation are often filled with natural disfluenci For digital assistant products, it is desirable that voice assistants have the ability to predict when the speaker is expecting a response, versus when they merely pause with the intention to resume speaking. We model this phenomenon similar to Chang et al. [10], who classify pauses in speech as being within a complete thought, or after having a finished complete thought. That is, when a user stops speaking, the model should predict whether they will continue speaking after a brief pause or whether a system response is expected. Because the active region of interest is pauses in the audio, we refer to this task in this paper as “pause prediction.” 4. Model 4.1, Multi-output HAT decoder HAT is a decoder structure for RNN-T in which the (blank) probability is computed separately from next token prediction, facilitating more accurate ILM estimation [14]. Wang et al. [9] propose a variant of HAT decoder which introduces multiple joint networks, one for each task (in our case, these are ASR, capitalization, and pause prediction). All of the parallel joint networks are conditioned on features from both the prediction network and audio encoders. The model is trained using an RNN-T objective [18], where at each timestep the model may choose to emit a wordpiece token, or to insert a special token (blank) which indicates nonemission. Formally, let X be the input utterance and Y be the label sequence. The ASR output space Vase consists of {y° = (blank), y',y’,...}.. Let T = |X| be the number of input audio frames and U = |Y| be the length of the transcript. The acoustic encoder produces f(X) = [fo,-.., fri], fe € R*, and the prediction network produces g(X) = [go,...,gu—1], gu € R”?. As in the original HAT implementation, the joint ASR | Cap | Pause) | Blank Posterior Softmax | Sigmoid | t tf t Projection | Projection | Tanh { Tanh_ | Projection Projection tt t Label Acoustic Decoder Encoder tT t Yanpairod Ypnired Xpaired Figure 1: Model diagram for JEIT training. The blue arrows denote the data flow for paired audio-text data. The red arrows denote the path that unpaired text data takes through the network. Baseline
