--- ABSTRACT ---
A critical obstacle preventing NeRF models from being deployed broadly in the wild is their reliance on accurate camera poses. Consequently, there is growing interest in extending NeRF models to jointly optimize camera poses and scene representation, which offers an alternative to offthe-shelf SfM pipelines which have well-understood failure modes. Existing approaches for unposed NeRF operate under limiting assumptions, such as a prior pose distribution or coarse pose initialization, making them less effective in a general setting. In this work, we propose a novel approach, LU-NeRF, that jointly estimates camera poses and neural radiance fields with relaxed assumptions on pose configuration. Our approach operates in a local-to-global manner, where we first optimize over local subsets of the data, dubbed “mini-scenes.” LU-NeRF estimates local pose and geometry for this challenging few-shot task. The mini-scene poses are brought into a global reference frame through a robust pose synchronization step, where a final global optimization of pose and scene can be performed. We show our LU-NeRF pipeline outperforms prior attempts at unposed NeRF without making restrictive assumptions on the pose prior. This allows us to operate in the general SE(3) pose setting, unlike the baselines. Our results also indicate our model can be complementary to feature-based S{M pipelines as it compares favorably to COLMAP on lowtexture and low-resolution images. 1.
--- INTRODUCTION ---
NeRF [35] was introduced as a powerful method to tackle the problem of learning neural scene representations and photorealistic view synthesis, and subsequent research has focused on addressing its limitations to extend its applicability to a wider range of use cases (see [55, 60] for surveys). One of the few remaining hurdles for view synthesis in the wild is the need for accurate localization. As images captured in the wild have unknown poses, these ap “Work done during an internship at Google. Project website Varun Jampani? Abhishek Kar? Ameesh Makadia” 2Google Research proaches often use structure-from-motion (SfM) [49, 41] to determine the camera poses. There is often no recourse when SfM fails (see Fig. 7 for an example), and in fact, even small inaccuracies in camera pose estimation can have a dramatic impact on photorealism. Few prior attempts have been made to reduce the reliance on SfM by integrating pose estimation directly within the NeRF framework. However, the problem is severely underconstrained (see Fig. 1) and current approaches make additional assumptions to make the problem tractable. For example, NeRf—— [57] focuses on pose estimation in forward-facing configurations, BARF [30] initialization must be close to the true poses, and GNeRF [33] assumes a 2D camera model (upright cameras on a hemisphere). We propose an approach for jointly estimating the camera pose and scene representation from images from a single scene while allowing for a more general camera configuration than previously possible. Conceptually, our approach is organized in a local-to-global learning framework using NeRFs. In the local processing stage we partition the scene into overlapping subsets, each containing only a few images (we call these subsets mini-scenes). Knowing images in a mini-scene are mostly nearby is what makes the joint estimation of pose and scene better conditioned than performing the same task globally. In the global stage, the overlapping mini-scenes are registered in a common reference frame through pose synchronization, followed by jointly refining all poses and learning the global scene representation. This organization into mini-scenes requires learning from a few local unposed images. Although methods exist for few-shot novel view synthesis [62, 28, 39, 21, 13, 12], and separately for optimizing unknown poses [30, 33, 57], the combined setting presents new challenges. Our model must reconcile the ambiguities prevalent in the local unposed setting — in particular the mirror symmetry ambiguity [40], where two distinct 3D scenes and camera configurations produce similar images under affine projection. We introduce a Local Unposed NeRF (LU-NeRF) model to address these challenges in a principled way. The information from the LU-NeRFs (estimated poses, confidences, and mirror symmetry analysis) is used to register all cam --- --BARF GNeRF Pose error: 25.05° (R), 0.54 (T) Note: initial pose error 43° (R), 0.71 (T) Pose error: 8.77° (R), 0.53 (T) Note: 2D cameras, elevation range (-90°, 90°) LU-NeRF+Sync (ours) Pose error: 0.09° (R), 0.00 (T) Note: Unconstrained LU-NeRF, full SO(3) averaging + Novel views synthesized from our model. Figure 1. Jointly optimizing camera poses and scene representation over a full scene is difficult and underconstrained. This example is the Lego scene with 100 images from the Blender dataset. Left: When provided noisy observations of the true camera locations, BARF [30] cannot converge to the correct poses. Middle: GNeRF [33] assumes a 2D camera representation (azimuth, elevation) which is accurate for the Blender dataset which has that exact configuration (upright cameras on a sphere). However, GNeRF also requires an accurate prior distribution on poses for sampling. The Lego images live on one hemisphere, but when GNeRF’s prior distribution is the full sphere it also fails to localize the images accurately. Right: Our full model, LU-NeRF+Sync, is able to recover poses almost perfectly in this particular example. By taking a local-to-global approach, we avoid having strong assumptions about camera representation or pose priors. Following [30, 33] pose errors for each method are reported after optimal global alignment of estimated poses to ground truth poses. To put the translation errors in context, the Blender cameras are on a sphere of radius 4.03. eras in a common reference frame through pose synchronization [20, 43, 24], after which we refine the poses and optimize the neural scene representations using all images. In summary, our key contributions are: ¢ A local-to-global pipeline that learns both the camera poses in a general configuration and a neural scene representation from only an unposed image set. ¢ LU-NeRF, a novel model for few-shot local unposed NeRF. LU-NeRF is tailored to the unique challenges we have identified in this setting, such as reconciling mirror-symmetric configurations. Each phase along our local-to-global process is designed with robustness in mind, and the consequence is that our pipeline can be successful even when the initial mini-scenes contain frequent outliers (see Sec 4 for a discussion on different mini-scene construction techniques). The performance of our method surpasses prior works that jointly optimize camera poses and scene representation, while also being flexible enough to operate in the general SE(3) pose setting unlike prior techniques. Our experiments indicate that our pipeline is complementary to the feature-based Sf{M pipelines used to initialize NeRF models, and is more reliable in low-texture or low-resolution settings. 2.
--- RELATED WORK ---
Structure from motion (SfM). Jointly recovering 3D scenes and estimating camera poses from multiple views of a scene is the classic problem in Computer Vision [25]. Numerous techniques have been proposed for SfM [41, 49] with unordered image collections and visual-SLAM for sequential data [54, 38]. These techniques are largely built upon local features [32, 45, 22, 52] and require accurate detection and matching across images. The success of these techniques has led to their widespread adoption, and existing deep-learning approaches for scene representation and novel view synthesis are designed with the implicit assumption that the S{M techniques provide accurate poses in the wild. For example, NeRF [35] and its many successors (e.g. [5, 6, 37]) utilize poses estimated offline with COLMAP [49, 31]. However, COLMAP can fail on textureless regions and low-resolution images. The local-to-global framework proposed in this work is inspired by the “divide-and-conquer” Sf{M and SLAM
--- METHOD ---
to tackle the problem of learning neural scene representations and photorealistic view synthesis, and subsequent research has focused on addressing its limitations to extend its applicability to a wider range of use cases (see [55, 60] for surveys). One of the few remaining hurdles for view synthesis in the wild is the need for accurate localization. As images captured in the wild have unknown poses, these ap “Work done during an internship at Google. Project website Varun Jampani? Abhishek Kar? Ameesh Makadia” 2Google Research proaches often use structure-from-motion (SfM) [49, 41] to determine the camera poses. There is often no recourse when SfM fails (see Fig. 7 for an example), and in fact, even small inaccuracies in camera pose estimation can have a dramatic impact on photorealism. Few prior attempts have been made to reduce the reliance on SfM by integrating pose estimation directly within the NeRF framework. However, the problem is severely underconstrained (see Fig. 1) and current approaches make additional assumptions to make the problem tractable. For example, NeRf—— [57] focuses on pose estimation in forward-facing configurations, BARF [30] initialization must be close to the true poses, and GNeRF [33] assumes a 2D camera model (upright cameras on a hemisphere). We propose an approach for jointly estimating the camera pose and scene representation from images from a single scene while allowing for a more general camera configuration than previously possible. Conceptually, our approach is organized in a local-to-global learning framework using NeRFs. In the local processing stage we partition the scene into overlapping subsets, each containing only a few images (we call these subsets mini-scenes). Knowing images in a mini-scene are mostly nearby is what makes the joint estimation of pose and scene better conditioned than performing the same task globally. In the global stage, the overlapping mini-scenes are registered in a common reference frame through pose synchronization, followed by jointly refining all poses and learning the global scene representation. This organization into mini-scenes requires learning from a few local unposed images. Although methods exist for few-shot novel view synthesis [62, 28, 39, 21, 13, 12], and separately for optimizing unknown poses [30, 33, 57], the combined setting presents new challenges. Our model must reconcile the ambiguities prevalent in the local unposed setting — in particular the mirror symmetry ambiguity [40], where two distinct 3D scenes and camera configurations produce similar images under affine projection. We introduce a Local Unposed NeRF (LU-NeRF) model to address these challenges in a principled way. The information from the LU-NeRFs (estimated poses, confidences, and mirror symmetry analysis) is used to register all cam --- --BARF GNeRF Pose error: 25.05° (R), 0.54 (T) Note: initial pose error 43° (R), 0.71 (T) Pose error: 8.77° (R), 0.53 (T) Note: 2D cameras, elevation range (-90°, 90°) LU-NeRF+Sync (ours) Pose error: 0.09° (R), 0.00 (T) Note: Unconstrained LU-NeRF, full SO(3) averaging + Novel views synthesized from our model. Figure 1. Jointly optimizing camera poses and scene representation over a full scene is difficult and underconstrained. This example is the Lego scene with 100 images from the Blender dataset. Left: When provided noisy observations of the true camera locations, BARF [30] cannot converge to the correct poses. Middle: GNeRF [33] assumes a 2D camera representation (azimuth, elevation) which is accurate for the Blender dataset which has that exact configuration (upright cameras on a sphere). However, GNeRF also requires an accurate prior distribution on poses for sampling. The Lego images live on one hemisphere, but when GNeRF’s prior distribution is the full sphere it also fails to localize the images accurately. Right: Our full model, LU-NeRF+Sync, is able to recover poses almost perfectly in this particular example. By taking a local-to-global approach, we avoid having strong assumptions about camera representation or pose priors. Following [30, 33] pose errors for each method are reported after optimal global alignment of estimated poses to ground truth poses. To put the translation errors in context, the Blender cameras are on a sphere of radius 4.03. eras in a common reference frame through pose synchronization [20, 43, 24], after which we refine the poses and optimize the neural scene representations using all images. In summary, our key contributions are: ¢ A local-to-global pipeline that learns both the camera poses in a general configuration and a neural scene representation from only an unposed image set. ¢ LU-NeRF, a novel model for few-shot local unposed NeRF. LU-NeRF is tailored to the unique challenges we have identified in this setting, such as reconciling mirror-symmetric configurations. Each phase along our local-to-global process is designed with robustness in mind, and the consequence is that our pipeline can be successful even when the initial mini-scenes contain frequent outliers (see Sec 4 for a discussion on different mini-scene construction techniques). The performance of our method surpasses prior works that jointly optimize camera poses and scene representation, while also being flexible enough to operate in the general SE(3) pose setting unlike prior techniques. Our
