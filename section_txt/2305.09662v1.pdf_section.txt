--- ABSTRACT ---
Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively smallscale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train ona curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we finetune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation, Make-An-Animation uses a UNet architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-theart performance on text-to-motion generation. Generated samples can be viewed at https://azadis. github.io/make-an 1.
--- INTRODUCTION ---
As the world shifts towards virtual spaces, and as embodied agents become more capable, it will be increasingly important to be able to generate plausible human motion. Speech or text are perhaps the most natural ways to prompt generative models, which is one reason behind the explosion in text-to-x generation research. Human motion generation from text has diverse applications both in virtual and real worlds. For instance, it enables control of robots with speech, faster video game development, creation of special effects featuring humans, as well as novel metaverse userinteraction modes whereby users can control the actions of their or others’ avatars via voice or text. A person is chopping down a tree with an axe. A person is ice skating. Figure 1. Samples generated by Make-An-Animation for text conditional motion generation. The lighting of the body models represents progress across time. Darker color indicates later frames in the sequence. In the top image, for a better visualization, frames are distributed horizontally. Text-conditioned human motion generation is challenging for a number of reasons. Firstly, there exists a huge diversity of human motions and text descriptions which describe those motions (i.e., the task is many-to-many distribution matching). Secondly, because motion capture data is expensive to acquire, human motion datasets are limited in scale and diversity. Recently, text-to-motion generation has seen rapid improvements in the quality and diversity of poses generated. In particular, diffusion models, which have yielded impressive results in text-conditioned image and video genera --- --tion, have proven effective in generating human body poses (85) [16] (38). However, these models struggle with in-thewild prompts that are outside of the distribution of motion capture data. To address these shortcomings, we present Make-AnAnimation, a text-conditioned human motion generation model which learns to map diverse human poses to natural language descriptions through in-the-wild image-text datasets and significantly improves on prior state-of-the-art models which only rely on motion capture data. Core to our approach is a large-scale dataset of human poses extracted from image-text datasets. Our key observation is that we should not be limited to learning human motion from motion capture data when we have access to large-scale in-the-wild video and images of human poses. So, to address the limited scale and diversity of motion capture datasets, we extract a large-scale Text Pseudo-Pose (TPP) dataset from image-text datasets filtered for images containing humans. This TPP dataset contains 35M (text, static pose) pairs. Make-An-Animation is trained in two stages: (1) We train a text-conditioned static 3D pose generation diffusion model on the TPP dataset. In this stage, Make-AnAnimation learns the distribution of human poses and their alignment with text. (2) Then, we extend the pre-trained diffusion model to motion generation via the addition of temporal convolution and attention layers which model the new temporal dimension and train on widely-used motion capture data. In this second stage, the model learns motion, i.e., how to connect poses in a temporally coherent manner. Crucially, it does not have to re-learn the distribution of feasible poses or their alignment with text. The Make-An-Animation architecture is a U-Net, similar to recent text-to-video diffusion models. We condition the U-Net on text representations extracted from a language model trained on large-scale language data. We represent human motion as a sequence of 3D SMPL body parameters, with a 6D continuous SMPL representation forbody joints and the root orient. We additionally represent the global position per frame via a 3D vector indicating the position in each of the x, y, z dimensions. Through human evaluation on a collected set of 400 diverse text prompts, we demonstrate that our method outperforms prior works in terms of generated pose realism and text alignment. To summarize, our main contributions are: ¢ We present Make-An-Animation — a text-conditioned human motion generation model which improves on prior state-of-the-art models, especially on diverse, inthe-wild text prompts. ¢ We show, for the first time, how to leverage largescale image datasets to learn in-the-wild human poses for generation. We show through ablations that pretraining on our collected Text Pseudo-Pose dataset significantly improves the generalization to prompts outside the distribution of widely-used motion capture datasets while showing a comparable performance on the mocap test set. ¢ We present a U-Net architecture for human motion generation which leverages a language model pretrained on large-scale language data and naturally extends a static text-to-pose generation diffusion model to motion generation via the addition of temporal convolution and attention layers. 2.
--- RELATED WORK ---
s Human pose and motion generation Prior works in human pose and motion synthesis have explored generative models either unconditionally [3 , or conditioned on various input signals such as a prior motion 20} [29], an action class or music Using text descriptions as a guidance in pose and motion synthesis has been a more recent research direction where many existing works use 2D keypoints as pose representations. [41] selects a base pose from 8 clusters based on an input text fed to a GAN model to generate a human image. [39] uses a GAN to generate a set of heatmaps for body keypoints conditioned on the text input. proposes a coarseto-fine approach where a refinement stage is introduced on top of the initial coarse estimate of the keypoint heatmaps. Different from the above
--- METHOD ---
outperforms prior works in terms of generated pose realism and text alignment. To summarize, our main contributions are: ¢ We present Make-An-Animation — a text-conditioned human motion generation model which improves on prior state-of-the-art models, especially on diverse, inthe-wild text prompts. ¢ We show, for the first time, how to leverage largescale image datasets to learn in-the-wild human poses for generation. We show through ablations that pretraining on our collected Text Pseudo-Pose dataset significantly improves the generalization to prompts outside the distribution of widely-used motion capture datasets while showing a comparable performance on the mocap test set. ¢ We present a U-Net architecture for human motion generation which leverages a language model pretrained on large-scale language data and naturally extends a static text-to-pose generation diffusion model to motion generation via the addition of temporal convolution and attention layers. 2. Related Works Human pose and motion generation Prior works in human pose and motion synthesis have explored generative models either unconditionally [3 , or conditioned on various input signals such as a prior motion 20} [29], an action class or music Using text descriptions as a guidance in pose and motion synthesis has been a more recent research direction where many existing works use 2D keypoints as pose representations. [41] selects a base pose from 8 clusters based on an input text fed to a GAN model to generate a human image. [39] uses a GAN to generate a set of heatmaps for body keypoints conditioned on the text input. proposes a coarseto-fine approach where a refinement stage is introduced on top of the initial coarse estimate of the keypoint heatmaps. Different from the above methods, [3] uses SMPL to represent a 3D body pose, generated by an LSTM GAN from a text input. Many existing works approach text-to-motion by learning to align text and pose or motion embeddings in the feature space. JL2P |] proposes to learn the joint embedding of text and pose using an autoencoder with curriculum learning. proposes a two-stream model to encode upper and lower body motions separately. AvatarCLIP [15] uses a pre-trained VPoser model to generate candidate poses, which are then used to optimize a motion VAE. MotionCLIP trains an auto-encoder while simultaneously reconstructing motion and aligning the motion manifold with CLIP’s latent space. TEMOS [24] trains a joint latent space through separate text and motion transformer encoders, allowing non-deterministic motion sampling. T2M [9] also uses a VAE model, but encodes motion as snippets and introduces an extra sampling for motion length conditioned on the input text. Leveraging recent advancements in diffusion models, Motion Diffusion Model (MDM) , MotionDiffuse (B8]} and FLAME adopt a diffusion process from text to motion with a transformer, which can significantly improve the diversity of synthesized samples. MDM and MotionDiffuse --- --Attention ' Linear ' l« ¢ ' i 1D Conv H ' ' ResBlock ! ' RoFormerSelfAttention ! ' 2D1x1Conv |! ; Q Kk Vv ' Attention ' ResBlock ail : ' ' Jet ResBlock ' Ganitam i | Attention Linear ; embeddings t t ¢ i ———— ' ' ; GroupNorm ' T5 Text ResBlock ' eam ' ' ' Encoder ————— ' ( (Uincar } 2D1x1Conv : q Attention— | kv aK vi! Attention | SiLU ] [. GroupNorm ' i { i { i ' q ry ! ' i ResBlock Sen (en if Linear } ' Text —— ' Timestep \ I @ (2) we @) Embedding ee Text Embeddings Figure 2. Make-An-Animation Model Architecture. Our diffusion model is built on a U-Net architecture inspired by recent image and video generation models. The U-Net consists of a sequence of Residual Blocks with 1x1 2D-convolution layers and Attention Blocks with cross-attention on textual information. To model the temporal dimension, we add 1D temporal convolution layers after each 1x2D-convolution, as well as temporal attention layers after each cross-attention layer. These temporal layers (greyed out in the figure) are only added in the motion fine-tuning stage. are trained on HumanML3D [9] which partially limits their choice of representation to stick figures for text-to-motion generation, while our curated large-scale dataset provides millions of SMPL pose labels. MDM supports converting poses from stick figures to SMPL bodies only through an optimization SIMPLIFY-X[22] procedure that often results in unrealistic body poses. Diffusion generative models Diffusion models are a class a generative model based on the thermodynamic stochastic diffusion process. In the forward diffusion process, a sample is drawn from the data distribution and is gradually noised by a diffusion process. A neural network learns the reverse process to gradually denoise the sample. Diffusion models have recently enabled rapid advancements in image generation. first demonstrated diffusion models for unconditional image generation. In comparison to GANs and VAEs, diffusion models have shown superior training stability, avoid mode dropout, and yield stronger performance. Adapting diffusion models for conditional generation, [6] introduced classifier-guided diffusion. More recently, classifier free guidance has been introduced to enable conditioning without the need for a separate classifier. Imagen showed that conditioning on text embeddings extracted from a pre-trained language model enhanced text alignment, and that perfor mance improved with text encoder size. Our U-Net architecture shares similarities with Imagen. Namely, Make-AnAnimation uses the same text encoder, T5-XXL [26], and a similar U-Net architecture. Other improvements have been suggested to improve training stability and performance, such as learning the reverse diffusion variances and vparameterization [31]. Our method leverages both of these improvements. 3. Text-to-3D Human Motion Generation 3.1. Dataset 3D Human motion datasets. We use the AMASS dataset of 3D human motions | and its textual annotations from the HumanML3D dataset [10]. We borrow the original SMPL annotations from AMASS instead of the processed data from HumanML3D dataset since (1) we found their motion representation contains redundant information, and (2) an extra optimization step would be required to convert their motion representations to the original SMPL format resulting in a degradation in the quality and speed of motion generation in the SMPL format. Similar to HumanML3D, we double the size of the dataset by mirroring the motions and editing the textual description accordingly, resulting in 26850 motion examples in the training set. Additionally, we benefit from the GTA-Human dataset [4] built from an open-world action game providing SMPL annotations over --- --laid on the virtual humans for 20000 motion samples. This dataset does not have any textual annotation, thus we use it for an unconditional training of our model simultaneous to its text-conditional training. Since the existing mocap datasets are limited in the number of samples and their diversity, we also collected a large-scale dataset of in-the-wild human poses from image datasets as follows. Large-scale Text Pseudo-Pose (TPP) dataset. Similar to [2], we have collected a dataset containing 35M pairs of human poses and their text descriptions from a few largescale image-text datasets. We processed all images by running Detectron2 keypoint detector to find images with a single human, then extracted their 3D pseudo-pose SMPL annotations using a pre-trained PPMAF-X model [37]. This large-scale data overcomes the limitations of the existing mocap datasets by providing a wide variety of human poses and a huge number of (text, 3D pose) sample pairs. We do not extract any face or hand expressions from the aforementioned datasets. 3.2. Diffusion Models Background Diffusion models are a class of generative model that convert Gaussian noise into samples from a learned distribution by iterative denoising. The forward process is a Gaussian noising process following a Markove chain, {x,}7_9 where xo is drawn from the data distribution, in our case a set of 3D SMPL body poses. We can express the forward process as q(x;|a,~1) = N(./arx1—1, (1 — a:)I) where a; € (0, 1). As is standard in training diffusion models, we learn to reverse the forward process by minimizing a noise-prediction loss (13): Leimple = EewN(0,1),t~U(1,T] [lee - ca(a1,¢,1)|3| a) where c is the optional conditioning for the diffusion model, which in our cases are text descriptions. In addition to this simple loss, we follow and optimize the hybrid loss, which also uses a loss L,,) that adds a constraint on the estimated variational lower bound (VLB). The L,,7, term is applied the same way as in [21]. We follow [LL and parameterize our models using vparameterization (v, = a,€ — ox) rather than predicting € or x. 3.3. Make-An-Animation We represent an avatar body pose, P, via 3D SMPL body parameters and an avatar motion as a sequence of body poses for N frames, [P,, P2,-++ , Py]. We use 6D continuous SMPL representation for 21 body joints and the root orient. We additionally represent the global position per frame via a 3D vector indicating the avatar’s position in each of Method R-Precision FID | Diversity Real 0.797%0-002 9.0029-002 9 .5#0-J2LP{(N] «=—-0.486*-99211.02+-46 7,676 +.MDM[[B5]] 0.611297 —0.544+-944 9.559+-T2M[(9}] 0.740993 1.067+-99? 9.1 88%-MAA 0.6755*-902 0.7740*0-007_g.93%0.Table 1. Quantitative results on the HumanML3D test set. We run our evaluation 20 times and + indicates the 95% confidence interval. the x, y, z dimensions, resulting in each P; € R13°. MakeAn-Animation is built from three major components: (1) a pre-trained language model trained on a large-scale language data (TS-XXL (26}), (2) a text-to-3D diffusion based pose generation model trained on our collected TPP dataset, and (3) a set of temporal convolution and attention layers extending the static pose generation model to the temporal dimension for motion generation and capturing the dependencies between the frames. We follow and use v-prediction parameterization (v_ = aye — 04x) to train our pose and motion diffusion models for numerical stability. Our denoising model in the above U-Net architecture operates on all motion frames at the same time resulting in a higher temporal coherence in the generated motions compared to the autoregressive frameworks and does not require any specific loss on the motion velocity for a smooth motion synthesis. We also benefit from classifier free guidance at inference by conditioning our model on a null text during training for 10% of the time. 3.3.1 Text-to-3D Pose Generation We train a U-Net based diffusion model to generate 3D SMPL pose parameters conditioning on text embeddings from a large frozen language model [26]. Inspired by the image and video generation models [32], we reshape the pose inputs to B x C x 1x 1 with B and C indicating batch size and channel dimension of 135, respectively. Our UNet model is built from a sequence of (1) Residual blocks with 1x1 2D-convolution layers conditioned on the diffusion time-step embedding and text embedding, and (2) Attention Blocks attending to the textual information and the diffusion time-step. We train this network on our large-scale TPP dataset where we set translation parameters to zero in all training examples, i.e., human at the center of the scene. 3.3.2. Temporal and Attention Layers In order to expand the aforementioned pose generation model to learn the temporal dimension for motion generation, we modify the convolutional and attention layers of the U-Net as follows. --- --ee 9s 2S 9% preference for MAA, © MAA vs MAA vs MAA vs MAA vs TEMOS MotionCLiP MDM T2M (a) | Which motion corresponds better with the textual description? ee 9s 2S 9% preference for MAA, © MAA vs MAA vs MAA vs MAA vs TEMOS MotionCLiP MDM TM (b) Which motion is more realistic? Figure 3. Human Evaluation. Here we compare each baseline against our model, MAA, in terms of text alignment and motion realism. The results are reported as the percentage of the majority vote raters that preferred our method to each baseline on our curated 400 prompts set. 0.asso % preference for MAA0.MAA vs Transformer maa MAA wi vs (both pretrained with TPP) w/o temporal attention ‘wie GTA, Figure 4. Ablation Study. Here we compare each ablated model against our model, MAA, in terms text alignment and motion realism. The results are reported as the percentage of the majority vote raters that preferred our method to each baseline on our curated 400 prompts set. We reshape the input motion sequence to a tensor of shape B x C x N x 1 x 1, where C is the length of the pose representation and NN is the number of frames. Inspired by (32) [12], we stack a 1D temporal convolution layer following each 1x1 2D-convolution. This allows us to train the new 1D temporal layers from scratch while loading the pre-trained convolution weights from the pose generation model. We set the kernel size of these temporal convolution layers to 3 opposed to the unit kernel size of the 2D convolutions. We apply a similar dimension decomposition strategy to the attention layers, where we stack a newly initialized temporal attention layer to each pre-trained attention block from the pose generation network. For the temporal attention layers we utilize a Rotary Position Embedding [33}. 4.
--- EXPERIMENT ---
s We compare the performance of our motion generation model with the existing state-of-the-art text-to-humanmotion generation models through human evaluation on 400 crowd-sourced prompts and automatic metrics on the HumanML3pD test set. 4.1. Automatic Metrics We perform an automatic evaluation on the HumanML3D test set in terms of Fréchet Distance (FID), RPrecision and Diversity scores. FID measures both diversity and quality of the samples comparing its distribution with the ground truth test set while R-Precision measures faithfulness between each generated motion and its input prompt. Diversity measures the variance of the generated motions across all descriptions. To compute the above scores, a motion encoder was jointly trained with a text encoder via a contrastive loss by [9]. We use the same encoder to extract text and motion embeddings. Since this model has been trained and optimized on the 263-dimensional pose representation vector, we transform both the ground-truth SMPL data and our generated samples similarly following the transformations provided by [LI] before computing the above metrics. We upsample our generations to 196 frames and keep that fixed for the ground truth samples and our outputs while computing the above metrics. Opposed to our baselines, we have not optimized our model on the 263dimensional motion representation, but achieve a comparable performance in all metrics as reported in Table[I] 4.2. Human Evaluation To evaluate the generalization ability of our model in synthesizing more challenging human poses and motions, we collected an evaluation set from Amazon Mechanical Turk (AMT) that consists of 400 prompts. We asked annotators for prompts that described an action along with some context of the scene. We filtered out prompts based on ethical concerns to remove any references to children or NSFW content. These prompts were selected without generating any poses or images for them, and were kept fixed for all --- --of our evaluations. We generated animated poses from all models with no translation to measure the correctness and quality of the body pose and motion. We show raters a text description and the corresponding rendered avatars from two models and ask them which output better matches the input text. For each comparison, we use the majority vote from 7 different annotators on 400 prompts as the final result, reported in Figure [3] This study confirms the superiority of our model and the impact of our large-scale TPP dataset in human motion generation and overcoming the limitations of the mocap datasets. A few motion samples generated by our model are shown in Figure[7] 4.3. Ablation Studies To disentangle and study the impact of our U-Net architecture versus the impact on our TPP dataset on our results, we implemented and trained an alternative diffusion model as a decoder-only transformer with a causal attention mask to generate 3D pose reprsentations from text. This model operates on a sequence of tokenized captions and their text embeddings, the diffusion time-step embedding, the noised body pose and root orient representations, and two final pose and orientation queries to predict the unnoised pose and root orientation, respectively. We pre-trained this model on our TPP dataset, then expanded the model for motion synthesis via adding three additional tokens per frame for pose, root orient, and translation paramters. We trained this model on the mocap datasets described in Sec|3.1/and compared its performance with our U-Net model via Amazon Mechanical Turk. As a result, 58% of the raters preferred the generations from our U-Net architecture. This experiment confirms the superiority of the U-Net model versus its auto-regressive alternatives. We also ablated the impact of temporal attention modules on the final quality of the samples resulting in 54% of the users preferring samples from a U-Net with temporal attention. Training with the GTA data unconditionally made the convergence of the model faster although it didn’t impact the samples’ quality after convergence. Additionally, we experimented with training our U-Net model on the mocap training set from scratch and without pre-training. However, we observed stability and convergence issues in this setting even with various learning rates and warming up updates. The pre-training stage on the large-scale dataset was therefore a stable solution resulting in high quality motion samples in the end. All ablation results are summarized in Figure[4] 4.4. Qualitative Studies In Figure|6} we visualize the generalization capability of our model in synthesizing novel motions. We investigate the existence of similar prompts or motions in the training data for each synthesized motion via a nearest neigh A person is playing football in playground A person dancing in the street Figure 5. Diverse samples generated by Make-An-Animation for text conditional motion generation. The lighting of the body models represents progress across time. Darker color indicates later frames in the sequence. For a better visualization, frames are down-sampled along the temporal dimension and distributed horizontally. --- --MAA Generations A person wrestles a bear in the forest A person is practicing the flute Aperson is sawing a branch off of a tree Nearest Neighbour Example a person puts both arms into the air a person clapping with his hands aman raises his hands to his chest, claps them together, then lowers them back to his sides Figure 6. Left: samples generated by Make-An-Animation for text conditional motion generation. Right: The nearest neighbor example for each of the generated samples found from the mocap training set based on motion and text clip similarity scores. The lighting of the body models represents progress across time. Darker color indicates later frames in the sequence. For a better visualization, frames are distributed horizontally. bor search. We extract the motion embedding and text embedding for each sample in our test set of prompts as well as our mocap training data using a pre-trained motionCLIP model that has learned a joint motion and language embedding space. For each synthesized motion, we find the top 6 motions and prompts from the training data with the highest cosine similarity scores. As shown in the figure, our model has generalized to new motions that did not exist in the training data. In addition, Figure [5] illustrates multiple diverse motions generated by our model for different text prompt. 5.
--- CONCLUSION ---
Current text-to-motion models are hindered by relatively small-scale motion capture datasets, as evident by their poor performance on more diverse, in-the-wild prompts. Motivated by this shortcoming, we present Make-AnAnimation, a human motion generation model trained on a collection of large-scale static pseudo-pose and motion capture data. As we demonstrate through our ablations and human studies comparing to prior works, pre-training on the large-scale TPP dataset significantly improves performance on captions outside of the distribution of motion capture --- --A person is doing a handstand on a cactus A person kicks a soccer ball A person is rowing a small dingy ina river A person is playing tennis at the Recreation Center A volleyball player bumps a ball up A photographer is looking through the camera A person is shoveling the driveway after a blizzard A person is sitting down on the ground Figure 7. Samples generated by Make-An-Animation for text conditional motion generation. The lighting of the body models represents progress across time. Darker color indicates later frames in the sequence. For a better visualization, frames are distributed horizontally. data. Furthermore, our novel U-Net architecture enables ters. a seamless transition between static pose pre-training and dynamic pose (i.e., motion) fine-tuning. Altogether, this work paves the way to leverage large-scale image and video datasets for learning generation of human 3D pose parame --- --ReferencesChaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. IEEE International Conference on 3D Vision (3DV), pages 719-728, 2019. Samaneh Azadi, Thomas Hayes, Akbar Shah, Guan Pang, Devi Parikh, and Sonal Gupta. Text-conditional contextualized avatars for zero-shot personalization. arXiv preprint arXiv:2304.07410, 2023. Rania Briq, Pratika Kochar, , and Juergen Gall. Towards better adversarial synthesis of human images from text. arXiv preprint arXiv:2107.01869, 2021. Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei, Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, and Ziwei Liu. Playing for 3d human recovery. arXiv preprint arXiv:2110.07588, 2021. Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi Shinoda. Implicit neural representations for variable length human motion generation. European Conference on Computer Vision (ECCV), 2022. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021. Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Synthesis of compositional animations from textual descriptions. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1396-1406, October 2021. Jan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, 2014. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5152-5161, June 2022. Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5152-5161, June 2022. Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. Proceedings of the 28th ACM International Conference on Multimedia, pages 2021-2029, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot textdriven generation and animation of 3d avatars. ACM Transactions on Graphics (TOG), 41(4):1-19, 2022. Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Freeform language-based motion synthesis & editing. arXiv preprint arXiv:2209.00349, 2022. Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz. Dancing to music. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aistt++. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. AMASS: Archive of motion capture as surface shapes. In Jnternational Conference on Computer Vision, pages 5442-5451, Oct. 2019. Julieta Martinez, Michael J. Black, and Javier Romero. On human motion prediction using recurrent neural networks. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162-8171. PMLR, 2021. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from a single image. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Mathis Petrovich, Michael J. Black, and Giil Varol. Actionconditioned 3D human motion synthesis with transformer VAE. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10985-10995, October 2021. Mathis Petrovich, Michael J. Black, and Giil Varol. Temos: Generating diverse human motions from textual descriptions. European Conference on Computer Vision (ECCV), 2022. Sigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung, and Daniel Cohen-Or. Modi: Unconditional motion synthesis from diverse data. arXiv preprint arXiv:2206.08010, 2022. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 2020. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, Umapada Pal, and Michael Blumenstein. Tips: Text-induced pose synthesis. European Conference on Computer Vision (ECCV), 2022. --- --[29] [30]Alejandro Hernandez Ruiz, Juergen Gall, and Francesc Moreno-Noguer. Human motion prediction via spatiotemporal inpainting. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Style and pose control for image synthesis of humans from a single monocular view. arXiv preprint arXiv:2205.11487, 2022. T. Salimans and J Ho. Progressive distillation for fast sampling of diffusion models. International Conference on Learning Representations, 2022. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Guy Tevet, Brian Gordon, Amir Hertz, Amit H. Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. European Conference on Computer Vision (ECCV), 2022. Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano, and Daniel Cohen-Or. Human motion diffusion model. arXiv preprint arXiv:2209. 14916, 2022. Sijie Yan, Zhizhong Li, Yuanjun Xiong, Huahan Yan, and Dahua Lin. Convolutional sequence generation for skeletonbased action synthesis. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 4393-4401, 2019. Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: Towards well-aligned full-body model regression from monocular images. arXiv preprint arXiv:2207.06400, 2022. Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. Yifei Zhang, Rania Briq, Julian Tanke, and Juergen Gall. Adversarial synthesis of human pose from text. 42nd DAGM German Conference of Pattern Recognition, pages 145-158, 2021. Rui Zhao, Hui Su, and Qiang Ji. Bayesian adversarial human motion synthesis. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6224-6233, 2020. Xingran Zhou, Siyu Huang, Bin Li, Yingming Li, Jiachen Li, and Zhongfei Zhang. Text guided person image synthesis. Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019.
