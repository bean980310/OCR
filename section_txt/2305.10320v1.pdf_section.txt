--- ABSTRACT ---
The core of Multi-view Stereo(MVS) is the matching process among reference and source pixels. Cost aggregation plays a significant role in this process, while previous methods focus on handling it via CNNs. This may inherit the natural limitation of CNNs that fail to discriminate repetitive or incorrect matches due to limited local receptive fields. To handle the issue, we aim to involve Transformer into cost aggregation. However, another problem may occur due to the quadratically growing computational complexity caused by Transformer, resulting in memory overflow and inference latency. In this paper, we overcome these limits with an efficient Transformer-based cost aggregation network, namely CostFormer. The Residual DepthAware Cost Transformer(RDACT) is proposed to aggregate long-range features on cost volume via self-attention mechanisms along the depth and spatial dimensions. Furthermore, Residual Regression Transformer(RRT) is proposed to enhance spatial attention. The proposed method is a universal plugin to improve learning-based MVS methods. 1
--- INTRODUCTION ---
Given a series of calibrated images from different views in one scene, Multi-view Stereo (MVS) aims to recover the 3D information of the observed scene. It is a fundamental problem in computer vision and widely applied to robot navigation, autonomous driving, augmented reality, and etc. Recent learning-based MVS networks [Yao ef al., 2018; Gu et al., 2020; Wang et al., 2021b] have achieved inspiring success both in the quality and the efficiency of 3D reconstruction. Generally, deep MVS approaches consist of the following five steps: feature extraction from multi-view images via CNN network with shared weights, differentiable warping to align all source features to the reference view, matching cost computation from reference features and aligned source features, matching cost aggregation or regularization, depth or disparity regression. *These authors contributed equally to this work. ‘Corresponding authors. B02 . one 9G Potentatennet wet aa aa] E e cua |W ove . 5 oseo36« * Py e af @ oul @ o z a 6 3 i 02 04 08 o8 10 aGPU Mem. (68) Run-time (s) Figure 1: Comparison with state-of-the-art MVS methods on DTU. Relationship between error, GPU memory and run-time with image size 1152x864. Current progresses in learning-based MVS primarily concentrate on the limitation of reconstruction quality [Wei er al., 2021; Yang et al., 2020a], memory consumption [Yan et al., 2020; Wei et al., 2021], and efficiency [Wang et al., 2021b; Wang et al., 2021a]. The basic network architecture of these works is based on the pioneering backbone network called MVSNet [Yao er al., 2018], which provides an elegant and stable baseline. However, instead of taking the inheritance of network design principle in MVSNet [Yao er al., 2018] for granted, we can rethink the task of MVS problem as a dense correspondence problem [Hosni er al., 2012] alternatively. The core of MVS is a dense pixelwise correspondence estimation problem that searches the corresponding pixel of a specific pixel in the reference image along the epipolar line in all warped source images. No matter which task this correspondence estimation problem is applied to, the matching task can be boiled down to a classical matching pipeline [Scharstein and Szeliski, 2002]: (1) feature extraction, and (2) cost aggregation. In learning-based MVS methods, the transition from traditional hand-crafted features to CNN-based features inherently solves the former step of the classical matching pipeline via providing powerful feature representation learned from large-scale data. However, handling the cost aggregation step by matching similarities between features without any prior usually suffers from the challenges due to ambiguities generated by repetitive patterns or background clutters [Cho et al., 2021]. Consequently, a typical solution in MVSNet and its variants [Yao er al., 2018; Gu et al., 2020; Wang et al., 2021b] is to apply a 3D CNN or an RNN to reg --- --ularize the cost volume among reference and source views, rather than directly rely on the quality of the initial correlation clues in cost volume. Although formulated variously in previous methods, these methods either use hand-crafted techniques that are agnostic to severe deformations or inherit the limitation of CNNs, e.g. limited receptive fields, unable to discriminate incorrect matches that are locally consistent. In this work, we focus on the cost aggregation step of cost volume and propose a novel cost aggregation Transformer (CostFormer) to tackle the issues above. Our CostFormer is based on Transformer [Vaswani et al., 2017], which is renowned for its global receptive field and long-range dependent representation. By aggregating the matching cost in the cost volume, our aggregation network can explore global correspondences and refine the ambiguous matching points effectively with the help of the self-attention (SA) mechanism in Transformer. Though the promising performances of Vision Transformers have been proven in many applications [Dosovitskiy et al., 2020; Sun er al., 2021], the time and memory complexity of the key-query dot product interaction in conventional SA grow quadratically with the spatial resolution of inputs. Hence, replacing 3D CNN with Transformer may result in unexpected extra occupancy in memory and latency in inference. Inspired by [Wang er al., 2021b], we further introduce the Transformer architecture into an iterative multi-scale learnable PatchMatch pipeline. It inherits the advantages of the long-range receptive field in Transformers, improving the reconstruction performance substantially. Meantime, it also maintains a balanced trade-off between efficiency and performance, which is competitive in the inference speed and parameters magnitude compared with other methods. Our main contributions are as follows: (1) In this paper, we propose a novel Transformer-based cost aggregation network called CostFormer, which can be plugged into learning-based MVS methods to improve cost volume effectively. (2) CostFormer applies an efficient Residual Depth-Aware Cost Transformer to cost volume, extending 2D spatial attention to 3D depth and spatial attention. (3) CostFormer applies an efficient Residual Regression Transformer between cost aggregation and depth regression, keeping spatial attention. (4) The proposed CostFormer brings benefits to learning-based MVS methods when evaluating DTU [Aanees et al., 2016], Tanks & Temples [Knapitsch et al., 2017] ETH3D [Schops et al., 2017] and BlendedMVS [Yao et al., 2020] datasets. 2
--- RELATED WORK ---
2.1 Learning-based MVS
--- METHOD ---
s focus on handling it via CNNs. This may inherit the natural limitation of CNNs that fail to discriminate repetitive or incorrect matches due to limited local receptive fields. To handle the issue, we aim to involve Transformer into cost aggregation. However, another problem may occur due to the quadratically growing computational complexity caused by Transformer, resulting in memory overflow and inference latency. In this paper, we overcome these limits with an efficient Transformer-based cost aggregation network, namely CostFormer. The Residual DepthAware Cost Transformer(RDACT) is proposed to aggregate long-range features on cost volume via self-attention mechanisms along the depth and spatial dimensions. Furthermore, Residual Regression Transformer(RRT) is proposed to enhance spatial attention. The proposed method is a universal plugin to improve learning-based MVS methods. 1 Introduction Given a series of calibrated images from different views in one scene, Multi-view Stereo (MVS) aims to recover the 3D information of the observed scene. It is a fundamental problem in computer vision and widely applied to robot navigation, autonomous driving, augmented reality, and etc. Recent learning-based MVS networks [Yao ef al., 2018; Gu et al., 2020; Wang et al., 2021b] have achieved inspiring success both in the quality and the efficiency of 3D reconstruction. Generally, deep MVS approaches consist of the following five steps: feature extraction from multi-view images via CNN network with shared weights, differentiable warping to align all source features to the reference view, matching cost computation from reference features and aligned source features, matching cost aggregation or regularization, depth or disparity regression. *These authors contributed equally to this work. ‘Corresponding authors. B02 . one 9G Potentatennet wet aa aa] E e cua |W ove . 5 oseo36« * Py e af @ oul @ o z a 6 3 i 02 04 08 o8 10 aGPU Mem. (68) Run-time (s) Figure 1: Comparison with state-of-the-art MVS methods on DTU. Relationship between error, GPU memory and run-time with image size 1152x864. Current progresses in learning-based MVS primarily concentrate on the limitation of reconstruction quality [Wei er al., 2021; Yang et al., 2020a], memory consumption [Yan et al., 2020; Wei et al., 2021], and efficiency [Wang et al., 2021b; Wang et al., 2021a]. The basic network architecture of these works is based on the pioneering backbone network called MVSNet [Yao er al., 2018], which provides an elegant and stable baseline. However, instead of taking the inheritance of network design principle in MVSNet [Yao er al., 2018] for granted, we can rethink the task of MVS problem as a dense correspondence problem [Hosni er al., 2012] alternatively. The core of MVS is a dense pixelwise correspondence estimation problem that searches the corresponding pixel of a specific pixel in the reference image along the epipolar line in all warped source images. No matter which task this correspondence estimation problem is applied to, the matching task can be boiled down to a classical matching pipeline [Scharstein and Szeliski, 2002]: (1) feature extraction, and (2) cost aggregation. In learning-based MVS methods, the transition from traditional hand-crafted features to CNN-based features inherently solves the former step of the classical matching pipeline via providing powerful feature representation learned from large-scale data. However, handling the cost aggregation step by matching similarities between features without any prior usually suffers from the challenges due to ambiguities generated by repetitive patterns or background clutters [Cho et al., 2021]. Consequently, a typical solution in MVSNet and its variants [Yao er al., 2018; Gu et al., 2020; Wang et al., 2021b] is to apply a 3D CNN or an RNN to reg --- --ularize the cost volume among reference and source views, rather than directly rely on the quality of the initial correlation clues in cost volume. Although formulated variously in previous methods, these methods either use hand-crafted techniques that are agnostic to severe deformations or inherit the limitation of CNNs, e.g. limited receptive fields, unable to discriminate incorrect matches that are locally consistent. In this work, we focus on the cost aggregation step of cost volume and propose a novel cost aggregation Transformer (CostFormer) to tackle the issues above. Our CostFormer is based on Transformer [Vaswani et al., 2017], which is renowned for its global receptive field and long-range dependent representation. By aggregating the matching cost in the cost volume, our aggregation network can explore global correspondences and refine the ambiguous matching points effectively with the help of the self-attention (SA) mechanism in Transformer. Though the promising performances of Vision Transformers have been proven in many applications [Dosovitskiy et al., 2020; Sun er al., 2021], the time and memory complexity of the key-query dot product interaction in conventional SA grow quadratically with the spatial resolution of inputs. Hence, replacing 3D CNN with Transformer may result in unexpected extra occupancy in memory and latency in inference. Inspired by [Wang er al., 2021b], we further introduce the Transformer architecture into an iterative multi-scale learnable PatchMatch pipeline. It inherits the advantages of the long-range receptive field in Transformers, improving the reconstruction performance substantially. Meantime, it also maintains a balanced trade-off between efficiency and performance, which is competitive in the inference speed and parameters magnitude compared with other methods. Our main contributions are as follows: (1) In this paper, we propose a novel Transformer-based cost aggregation network called CostFormer, which can be plugged into learning-based MVS methods to improve cost volume effectively. (2) CostFormer applies an efficient Residual Depth-Aware Cost Transformer to cost volume, extending 2D spatial attention to 3D depth and spatial attention. (3) CostFormer applies an efficient Residual Regression Transformer between cost aggregation and depth regression, keeping spatial attention. (4) The proposed CostFormer brings benefits to learning-based MVS methods when evaluating DTU [Aanees et al., 2016], Tanks & Temples [Knapitsch et al., 2017] ETH3D [Schops et al., 2017] and BlendedMVS [Yao et al., 2020] datasets. 2 Related Work 2.1 Learning-based MVS Methods Powered by the great success of deep learning-based techniques, many learning-based methods have been proposed to boost the performance of Multi-view Stereo. MVSNet [Yao et al., 2018] is a landmark for the end-to-end network that infers the depth map on each reference view for the MVS task. Feature maps extracted by a 2D CNN on each view are reprojected to the same reference view to build a variance-based cost volume. A 3D CNN is further used to regress the depth map. Following this pioneering work, lots of efforts have been devoted to boosting speed and reducing memory occupation. To relieve the burden of huge memory cost, recurrent neural networks are utilized to regularize the cost volume in AA-RMVSNet [Wei et al., 2021]. Following a coarse-to-fine manner to develop a computationally efficient network, a recent strand of works divide the single cost volume into several cost volumes at multiple stages, like CasMVSNet [Gu et al., 2020], CVP-MVSNet [Yang et al., 2020a], UCSNet [Cheng et al., 2020], and etc. Inspired by the traditional PatchMatch stereo algorithm, PatchMatchNet [Wang et al., 2021b] inherits the pipeline in PatchMatch stereo in an iterative manner and extend it into a learning-based end-to-end network. 2.2. Vision Transformer The success of Transformer [Vaswani et al., 2017] and its variants [Dosovitskiy et al., 2020; Liu et al., 2021] have motivated the development of Neural Language Processing in recent years. Borrowing inspiration from these works, Transformer has been successfully extended to vision tasks and proven to boost the performance of image classification [Dosovitskiy et al., 2020]. Following the pioneering work, many efforts are devoted to boosting the development of various vision tasks with the powerful representation ability of Transformer. In [Li et al., 2021], the application of Transformer in the classic stereo disparity estimation task is investigated thoughtfully. Swin Transformer [Liu et al., 2021] involves the hierarchical structure into Vision Transformers and computes the representation with shifted windows. Considering Transformer’s superiority in extracting global content information via attention mechanism, many works attempt to utilize it in the task of feature matching. Given a pair of images, CATs [Cho et al., 2021] explore global consensus among correlation maps extracted from a Transformer, which can fully leverage the self-attention mechanism and model long-range dependencies among pixels. LoFTR [Sun et al., 2021] also leverages Transformers with a coarse-to-fine manner to model dense correspondence. STTR [Li er al., 2021] extends the feature matching Transformer architecture to the task of stereo depth estimation task in a sequence-to-sequence matching perspective. TransMVSNet [Ding et al., 2021] is the most relevant concurrent work compared with ours, which utilizes a Feature Matching Transformer (FMT) to leverage self-attention and cross-attention to aggregate long-range context information within and across images. Specifically, the focus of TransMVSNet is on the enhancement of feature extraction before cost aggregation, while our proposed CostFormer aims to improve the cost aggregation process on cost volume. 3 Methodology In this section, we introduce the detailed architecture of the proposed CostFormer which focuses on the cost aggregation step of cost volume. CostFormer contains two specially designed modules called Residual-Depth Aware Cost Transformer (RDACT) and Residual Regression Transformer (RRT), which are utilized to explore the relation between pixels within a long range and the relation between different --- --(WW) waing (ECost Computation INITAPROP| STAGE3 —>{ eoacr J mer J ' cc \ 3D CNN Cost Aggregation Y L ¥ ¥ ' o @ ooteors Ww o+@- >a >a aed ee Xie oes owe rn J) Warped Features 1 INIT&PROP STAGE2 p> Rpacr |— rer |— | 1 Cost Volume ' y an; -@-4J- @-g-@e- eS 1 ~ OT, Ieee == SSS SSS ae (A9] Transformed Cost Volume 1 ¥ _ ’ ¥ INIT&PROP, STAGE1 ae ' — 1 ! Aggregated Cost Volume y . 1 vi — >w> > Transformed Aggregated Cost Volume ce [itrarRor) nialiatin and Propagation STAGEO RR} Residual Regression Transformer Figure 2: Structure of CostFormer based on PatchMatchNet. depth hypotheses during the evaluation process. In Section Preliminary, we give a brief preliminary on the pipeline of our method. Then we show the construction of RDACT and RRT respectively. Finally, we show
--- EXPERIMENT ---
s. 3.1 Preliminary In general, the proposed RDACT and RRT can be integrated with arbitrary cost volume of learning-based MVS networks. Based on the patch match architecture [Wang er al., 2021b], we further explore the issue of cost aggregation on cost volume. As shown in Figure 2, CostFormer based on PatchMatchNet [Wang et al., 2021b] extracts feature maps from multi-view images and performs initialization and propagation to warp the features maps in source views to reference view. Given a pixel p at the reference view and its corresponding pixel p;,; at the -th source view under the j-th depth hypothesis d; is defined as: pig = Ki- (Roi (Ko' + p+ dj) + tos) (1) where Ro,; and to; denote the rotation and translation between the reference view and i-th source view. Ko and K; are the intrinsic matrices of the reference and i-th source view. The warped feature maps at the i-th source view F;(p;,;) are bilinearly interpolated to remain the original resolution. Then, a cost volume is constructed from the similarity of feature maps, and 3D CNNs are applied to regularize the cost volume. Warped features from all source views are integrated into a single cost for each pixel p and depth hypothesis d; by computing the cost per hypothesis S;(p,j)% via group-wise correction as follows: Si(p. J)? = ¢ < Fo(p)9, Fi(pig)? > RF 2) where G is the group number, C’ is the channel number, < -,- > is the inner product, Fo(p)9 and F;(p;,;)% are grouped reference feature map and grouped source feature map at the i-th view respectively. Then they aggregate over the views with a pixel-wise view weight w;(p) to get S(p, j). Taking no account of Transformer at the cost aggregation (CA) step, a CA module firstly utilizes a small network with 3D convolution with 1 x 1 x 1 kernels to obtain a single cost, C € R¥XW*D. Fora spatial window of K, pixels {p, }{<, can be organized as a grid, per pixel additional offsets {Ape He, can be learned for spatial adaptation. The aggregated spatial cost C(p, j) is defined as: Ke i —SrwndiC(p + Pe + Aves i) B) Vege ede (where wy, and d;, weight the cost C based on feature and depth similarity. Given the sampling positions (p + px + Apr) icorresponding features from Fo are extracted via bilinear interpolation. Then group-wise correlation is applied between the features at each sampling location and p. The results are concatenated into a volume on which 3D convolution layers with 1x1x1 kernels and sigmoid non-linearities are applied to output normalized weights {wy Vey: The absolute difference in inverse depth between each sampling point and pixel p with their j-th hypotheses are collected. Then a sigmoid function on the inverted differences is applied to obtain {dy} ey. The remarkable thing is that such cost aggregation inevitably suffers from challenges due to ambiguities generated by repetitive patterns or background clutters. The local mechanisms in ambiguities exist in many operations, such as local propagation and spatial adaptation by small learnable slight offset. CostFormer significantly alleviates these problems through RDACT and RRT. The original CA module is also repositioned between RDACT and RRT. After RRT, soft argmin is applied to get the regressed depth. Finally, a depth refinement module is designed to refine the depth regression. For CascadeMVS and other cascade architectures, CostFormer can be plugged into similarly. C(p, j) = 3.2. Residual Depth-Aware Cost Transformer In this section, we explore the details of the Residual DepthAware Cost Transformer (RDACT). Each RDACT consists of two parts. The first part is a stack of Depth-Aware Transformer layer (DATL) and Depth-Aware Shifted Transformer layer (DASTL), which deal with the cost volumes to explore the relations sufficiently. The second part is the Re --- --UCSNet PatchmatchNet Figure 3: Comparison of different methods on the DTU evaluation set. The backbone of CostFormer is PatchMatchNet here. Embedding Cost layer (REC) which recovers the cost volume from the first part. Given a cost volume Cy € R4?*“**G, temporary intermediate cost volumes C},C9,...,C, € RY*W*?** are firstly extracted by DATL and DASTL alternatively: Cy = DASTL;(DATL;(Cx—1)), & = 1,2,..., LD (4) where DATL, is the k-th Depth-Aware Transformer layer with regular windows, DASTL;, is the k-th Depth-Aware Transformer layer with shifted windows, E is the embedding dimension number of DATL; and DASTL,. Then a Re-Embedding Cost layer is applied to the last C;,, namely C,, to recover G from E. The output of RDACT is formulated as: Cout = REC(Cz) + Co (5) where REC is the Re-Embedding Cost layer, and it can be a 3D convolution with G output channels. If FE = G, Cous can be simply formulated as: Cout = Ci + Co (6) This residual connection allows the aggregation of different levels of cost volumes; Co. instead of Co is then aggregated by the original aggregation network described in section 3.1. The whole RDACT is shown in the red window in Figure 2. Before introducing the construction of DATL and DASTL, we dive into the details of core constitutions called DepthAware Multi-Head Self-Attention (DA-MSA) and DepthAware Shifted Multi-Head Self-Attention (DAS-MSA). Both DA-MSA and DAS-MSA are based on Depth-Aware SelfAttention Mechanism. In order to explain Depth-Aware Self-Attention Mechanism, we supply the knowledge about Depth-Aware Patch Embedding and Depth-Aware Windows as preliminary. Depth-Aware Patch Embedding: Obviously, directly applying the attention mechanism for feature maps at pixel-wise level is quite costly in terms of GPU memory usage. In order to tackle this issue, we propose a Depth-Aware Patch Embedding to reduce the high memory cost and get an additional regularization. Specifically, given a grouped cost volume before aggregation C € R?*W*PxG 4 depth-aware patch embedding is firstly applied to C to get tokens. It consists of a 3D convolution with kernel size h x w x d and a layer normalization.To downsample the spatial sizes of cost volume and keep the depth hypotheses, we set h and w to more than | and d as 1. So the sample ratio is adaptive for memory cost and run time. Before convolution, cost volume will be padded to fit the spatial sizes and downsampling ratio. After layer normalization(LN), these embedded patches are further partitioned by depth-aware windows. Depth-Aware Windows: Beyond the nonlinear and linear global self-attention, local self-attention within a window has been proven to be more effective and efficient. As an example of 2D windows, Swin Transformer [Liu er al., 2021] directly applies multi-head self-attention mechanisms on nonoverlapping 2D windows to avoid the big computation complexity of global tokens. Extended from the 2D spatial window, an embedded cost volume patch € R#™*W*xD°xG with depth information is partitioned into non-overlapping 3D windows. These local windows are then transposed and reshaped to local cost tokens. Assuming the sizes of these windows are hs x ws x ds, the total number of tokens is --- --COLMAP 0 T PatchmatchNet Qr 37 = 9mm Figure 4: Comparison of different methods on Tanks&Temples. The Recall reported by official benchmark is presented. pL] xf) x« (=). These local tokens are further processed by the multi-head self-attention mechanism. Depth-Aware Self-Attention Mechanism: For a cost window token X € R’s*wsX4sxG_ the query, key, and value matrices Q, K and V € R'*s*4s*G are computed as: Q=XPo,K=XPK,V =XPy (7) where Pg, Px, and Py € ROXG are projection matrices shared across different windows. By introducing depth and spatial aware relative position bias Bl € Ritsxhs)x(wsxws)x(dsxds) for each head, the depth-aware self-attention(DA-SA1) matrix within a 3D local window is thus computed as: Qikit Ja + BYv(8) Where Q1, K1 and V1 € R’:s4s*G are reshaped from Q, K and V € RisXws*4sxG_ The process of DATL with LayerNorm(LN) and multi-head DA-SA1 at the current level is formulated as: X! = DA-MSAI((LN(X'!)) +.X!! (9) DA-SAI = Attention1(Q1, K1, V1) = SoftMaz( By introducing depth-aware relative position bias B2 € IR¢*¢s for each head, the depth-aware self-attention(DASA2) matrix along the depth dimension is an alternative module to DATL and thus computed as: one + B2)V(10) Where Q2, K2 and V2 € R’»s*4s*G are reshaped from Q, K and V € R'*s*4s*G_ BY and B2 will be along the depth dimension and lie in the range of [—d, + 1,ds — 1]. Along the height and width dimension, B1 lies in the range of [-hs + 1,h, — 1] and [-w, + 1,ws — lj. In practice, we parameterize a smaller-sized bias matrix BI € RGhs-1)x(@ws—1)x(2ds-1) from B1 and perform the attention functionfor f times in parallel, and then concatenate the depth-aware multi-head self-attention (DA-MSA) outputs. The process of DATL with LayerNorm(LN), multi-head DA-SA2 = Attention2(Q2, K2, V2) = SoftMaz( DA-SA1, and DA-SA2 at the current level is formulated as: X! = DA-MSAI(LN(DA-MSA2(LN(X!~1)))) + XU"! qd) Then, an MLP module that has two fully-connected layers with GELU non-linearity between them is used for further feature transformations: X! = MLP(LN(X!))) +X! (12) Compared with global attention, local attention makes it possible for computation in high resolution. However, there is no connection across local windows with fixed partitions. Therefore, regular and shifted window partitions are used alternately to enable cross-window connections. So at the next level, the window partition configuration is shifted along the height, width, and depth axes by fe SS 4), Depth-aware self-attention will be computed in these shifted windows(DAS-MSA); the whole process of DASTL can be formulated as: X'+1 — DAS-MSAI(LN(DAS-MSA2(LN(X°)))) +X! (13) X41 = MLP(LN(X'1)) + X44 (14) DAS-MSA1 and DAS-MSA2 correspond to multi-head Attention! and Attention2 within a shifted window, respectively. Assuming the number of stages is n, there are n RDACT blocks in CostFormer. 3.3. Residual Regression Transformer After aggregation, the cost C € R¥XWXP will be used for depth regression. To further explore the spatial relation under some depth, a Transformer block is applied to C before softmax. Inspired by the RDACT, the whole process of Residual Regression Transformer(RRT) can be formulated as: Cy = RST, (RT¢(Ce-1)), k = 1,2,..., (15) Cout = RER(Cx) + Co (16) --- --Methods Tntermediate Group (F-score 7) ‘Advanced Group (F-score 7) ‘ Mean Fam. Fra. Hor Lig. _M60__Pan._-Pla.___‘Tra. Mean Aud._Bal. Cou. Mus. Pal.__ Tem. MVSNet [Yao ef al., 20187 BB 35.99 28.55 25.07 50.79 53.96 50.86 47.90 34.69 CasMVSNet [Gu er al., 2020] 56.84 76.37 5845 46.26 55.81 56.11 54.06 58.18 49.51 31.12 1981 3846 29.10 43.87 27.36 28.UCS-Net [Cheng e7 al., 2020] 54.83 76.09 53.16 43.03 54.00 55.60 51.49 57.38 47.89 - - - - - - CVP-MVSNet [Yang e7 al., 2020b] 54.03 76.50 36.34 55.12 57.28 54.28 57.PVA-MVSNet [Yi et al., 2020] 54.46 69.36 46.01 55.74 57.23 54.75 56.70 - - - - AA-RMVSNet [Wei et al., 2021] 61.51 7111 51.53 64.02 64.05 59.47 60.85 33.53 32.05 46.01 29.28 32.PatchmatchNet [Wang et al. 53.15 66.99 43.24 54.87 52.87 49.54 54.21 32.31 30.04 41.80 28.31 32.UniMVSNet [Peng et 64.36 81.20 53.11 63.46 66.09 64.84 62.23 38.96 39.74 52.89 33.80 34.MVSTR [Zhu er al. 36.93 76.92 30.16 56.73 56.53 51.22 5658 47: 32.85 33.87 45.46 27.95 27.TransMVS [Ding et al., 2022] 63.52 80.92 56.94 62.54 63.06 60.00 60.20 58. 37.00 34.77 46.49 34.69 36.MVSTER [Wang er al., 2022] - : : - - - - - 37.53 35.65 49.37 32.16 39.CostFormer(PatchMatchNet) | 56.27(43.12) | 7246 34.27 5583 56.80 S088 55.05 52.32 | 34.07G1,76) 32.17 43.95 2862 36.CostFormer(PatchMatchNet*) | 57.10(+3.95) | 74.22 S441 56.65 54.46 51.45 57.65 51.70 | 34.31(+2.00) 3158 44.55 28.79 35.CostFormer(UniMVSNet~ ) 64.40(+0.04) | 81.45 53.88 62.94 66.12 65.35 61.31 57.90 | 39.55(+0.59) 40.21 5281 34.40 35.CostFormer(UniMVSNet*) 64.51(+0.15) | 81.31 55.57 63.46 66.24 65.39 61.27 __57.30 | 39.43(40.47) 39.88 53.38 34.07 34.Table 1: Quantitative results of different methods on the Tanks & Temples benchmark (higher is better). * is pretrained on DTU and fine-tuned on BlendedMVS. - is not pretrained on DTU and trained from scratch on BlendedMVS where RT;, is the k-th Regression Transformer layer with regular windows, RST;, is the k-th Regression Transformer layer with shifted windows, RER is the re-embedding layer to recover the depth dimension from Cz, and it can be a 2D convolution with D output channels. RRT also computes self-attention in a local window. Compared with RDACT, RRT focuses more on spatial relations. Compared with regular Swin [Liu er al., 2021] Transformer block, RRT treats the depth as a channel, the number of channels is actually 1 and this channel is squeezed before the Transformer. The embedding parameters are set to fit the cost aggregation of different iterations. If the embedding dimension number equals D, C,.,4 can be simply formulated as: Cour = Cx + Co (17) As a stage may iterate many times with different depth hypotheses, the number of RRT blocks should be set the same as the number of iterations. The whole RRT is shown in the yellow window in Figure 2. 4 Training 4.1 Loss function Final loss combines with the losses of all iterations at all stages and the loss from the final refinement module:Loss = Soyo! + Dre k=1i=(18) where L* is the regression or unification loss of the i-th iteration at k-th stage. L,¢¢ is the regression or unification loss from refinement module. If refinement module does not exist, the Le loss is set to zero. 4.2, Common training settings CostFormer is implemented by Pytorch [Paszke er al., 2019]. For RDACT, we set the depth number at stages 3, 2, 1 as 4, 2, 2; patch size at height, width and depth axes as 4, 4, 1; window size at height, width and depth axes as 7, 7, 2. If the backbone is set as PatchMatchNet, embedding dimension number at stages 3, 2, | are set as 8, 8, 4. For RRT, we set the depth number as 2 at all stages, patch size as | at all axes; window size as 8 at all axes. If the backbone is set as PatchMatchNet, embedding dimension number at iteration 2, 2,at stages 3, 2, 1 as 32, 64, 16, 16, 8. All models are trained on Nvidia GTX V100 GPUs. After depth estimation, we reconstruct point clouds similar to MVSNet [Yao et al., 2018]. 5 Experiments In this section, we introduce multiple MVS datasets and evaluate our method on these datasets. The results will be further reported in detail. 5.1 DATASETS The datasets used in the evaluation are DTU [Aanes et al., 2016], BlendedMVS [Yao et al., 2020], ETH3D [Schéps er al., 2017], Tanks & Temples [Knapitsch et al., 2017], and YFCC-100M [Thomee et al., 2016]. The DTU dataset is an indoor multi-view stereo dataset with 124 different scenes, there are 49 views under seven different lighting conditions in one scene. Tanks & Temples is collected in a more complex and realistic environment, and it’s divided into the intermediate and advanced set. ETH3D benchmark consists of calibrated high-resolution images of scenes with strong viewpoint variations. It is divided into training and test datasets. While the training dataset contains 13 scenes, the test dataset contains 12 scenes. BlendedMVS dataset is a large-scale synthetic dataset, consisting of 113 indoor and outdoor scenes and split into 106 training scenes and 7 validation scenes. 5.2. Main Settings and Results on DTU For the evaluation on the DTU [Aanes et al., 2016] evaluation set, we only use the DTU training set. During the training phase, we set the image resolution to 640 x 512. We compare our method to recent learning-based MVS methods, including CasMVSNet [Gu et al., 2020] and PatchMatchNet [Wang et al., 2021b] which are also set as backbones of CostFormer. We follow the evaluation metrics provided by the DTU dataset. The quantitative results on the DTU evaluation set are summarized in Table 2, which indicates that the plugand-play CostFormer improves the cost aggregation. Partial visualization results of Table 2 are shown in Figure 3. Complexity Analysis: For the complexity analysis of CostFormer, we plug it into PatchMatchNet [Wang er al., 2021b] and first compare the memory consumption and run-time with --- --Methods Acc. (mm) Comp. (mm) Overall (mm) Furu [Furukawa and Ponce, 2010] 0.613 0.941 0.Tola [Tola et al., 2012] 0.342 1.190, 0.Gipuma [Galliani et al., 2015] 0.283 0.873 0.Colmap [Schénberger and Frahm, 2016] 0.400 0.644 0.SurfaceNet [Ji et al., 20177 0.450 1.040 0.745, MVSNet [Yao et al., 2018] 0.396 0.527 0.R-MVSNet [Yao et al., 2019] 0.383, 0.452 0.P-MVSNet [Luo et al., 2019] 0.406 0.434 0.Point-MVSNet [Chen et al., 2019] 0.342 0411 0.Fast-MVSNet [Yu and Gao, 2020] 0.336 0.403 0.CasMVSNet [Gu et al., 2020] 0.325 0.385 0.UCS-Net [Cheng et al., 2020] 0.338 0.349 0.CVP-MVSNet [Yang et al., 2020b] 0.296 0.406 0.PVA-MVSNet [Yi ef al., 2020] 0.379 0.336 0.PatchMatchNet [Wang et al., 2021b] 0.427 0.277 0.AA-RMVSNet [Wei et ai.. 0.376 0.339 0.UniMVSNet [Peng et al., 0.352 0.278 0.CostFormer(Based on PatchMatchNet) 0.424 0.262 0.343 (+0.0093) CostFormer(Based on CasMVSNet) 0.378 0.313 0.345 (+0.0097) COstFormer(Based on UniMVSNet) 0.301 0.322 0.312 (+0.0035) Table 2: Quantitative results of different methods on DTU. this backbone. For a fair comparison, a fixed input size of 1152 x 864 is used to evaluate the computational cost on a single GPU of NVIDIA Telsa V100. Memory consumption and run-time of PatchMatchNet [Wang et al., 2021b] are 2323MB and 0.169s. They are only increased to 2693MB and 0.231s by the plug-in. Based on the reports of PatchMatchNet [Wang et al., 2021bl], we then get the comparison results of other state-ofthe-art learning-based methods. Memory consumption and run-time are reduced by 61.9% and 54.8% compared to CasMVSNet [Gu et al., 2020], by 48.8% and 50.7% compared to UCSNet [Cheng et al., 2020] and by 63.5% and 77.3% compared to CVP-MVSNet [Yang et al., 2020b]. Combining the results(lower is better) are shown in Table 3 and Figure 1, GPU memory and run-time of CostFormer are set as 100%. Method GPU Memory (%)_| Run-time (%) | Overall (mm) CasMVSNet [Gu ef al., 2020] 262.47% 221.24% 0.UCSNet [Cheng er al., 2020] 195.31% 202.84% 0.CVP-MVSNet [Yang et al., 2020b]_|_273.97% 40.53% 0.Ours 100.00% 100.00% 0.Table 3: Comparison with other SOTA learning-based MVS methods on DTU. Relationship between overall performance, GPU memory and run-time. Comparison with Transformers We also compare CostFormer with other Transformers [Zhu et al., 2021; Wang et al., 2022; Ding et al., 2021; Liao et al., 2022] which are used in MVS methods and not plug-and-play. For a fair comparison, only direct improvements(higer is better) and incremental cost of run time(low is better) from pure Transformers under similar depth hypotheses are summarized in Table 4. the Blended MVS [Yao et al., 2020] dataset. We compare our method to those recent learning-based MVS methods, including PatchMatchNet [Wang et al., 2021b] and UniMVSNet [Peng et al., 2022] which are also set as backbones of CostFormer. The quantitative results on the Tanks & Temples Knapitsch et al., 2017] set are summarized in Table 1, which indicates the robustness of CostFormer. Partial visualization results of Table 1 are shown in Figure 4. We would like to clarify that UniMVSNet™ in Table | only uses BlendedMVS or training which uses less data (no DTU) than the UniMVSNet baseline. 5.4 Main Settings and Results on ETH3D We use the PatchMatchNet [Wang er al., 2021b] as backbone and adopt the trained model used in the Tanks & Temples dataset [Knapitsch er al., 2017] to evaluate the ETH3D [Schéps er al., 2017] dataset. As shown in Table 5, our method outperforms others on both the training and particularly challenging test datasets(higher is better). ‘Training ‘Testing Methods Fiscore? | Timesyt | FiscoreT | Times) MVE [Fuhrman et al., 20147 20.47 13278.69 30.37, 10550.Gipuma [Galliani e7 al., 2015] 36.38 387.717 45.18 689.PMVS [Furukawa and Ponce. 20107 46.06 836.66 44.16 ‘957.‘COLMAP [Schonberger and Frahm, 20167 67.66 2690.62 73.01 1658.PVSNet [Xu and Tao, 20207 6748 = 72.08 829.et al., 202 1a] 66.36 = 74.29 = Net [Wang ef al., 202 1b] 64.21 452.63 73.12, 492.PatchMatch-RL [Lee ef al., 20217 67.78 = 72.38 = CostFormer(Ours) 68.92(44.71) 566.18 75.24(+2.12) 547.Table 5: Quantitative results of different methods on ETH3D. 5.5 Main Settings and Results on BlendedMVS dataset We use the model used in ETH3D. On BlendedMVS [Yao er al., 2020] evaluation set, we set N = 5 and image resolution as 576 x 768. End point error (EPE), | pixel error (e1), andpexels error (e3) are used as the evaluation metrics. Quantitative results(lower is better) of different methods are shown in Table 6. Method Trans Improvement Gam) | Delta Time () | Delta Time (7) 0.0140 203595 F1821% 0.0160, 403678 #135 A2% E 30.0130 40.2658 : MVSTER(CNN Fusion) TWang ef al, 20227 _| #00040, F0.0T6s FISITG CostFormer(CNN Fusion) 0.0097 20.0625 $36,69% Table 4: Quantitative improvement of performance and incremental cost of run time of different Transformers on DTU evaluation set. 5.3 Main Settings and Results on Tanks & Temples For the evaluation on Tanks & Temples [Knapitsch er al., 2017], we use the DTU [Aanes et al., 2016] dataset and Method EPE | el(% | 3% MVSNet [Yao ef al., 2018] 149_| 21.98 8.MVSNet-s [Darmon e7 al., 2021 T35_ | 25.91 8.CVP-MVSNet [Yang ef al., 2020a] T90_[ 19.73 10.VisMVSNet [Zhang e7 al., 2020] T47_[ 1847__|CasMVSNet [Gu er al, 2020] T9s_[ 1525__| 7.EPPMVSNet [Ma ef al., 20217 T17_| 12.66 6.TransMVSNet [Ding ef al., 2021] 073 | 832 3.CostFormer(Based on PatchmatchNet) | 0.84 | 1237 |CostFormer(Based on UniMVSNet) 0.43_|_7.05 2.Table 6: Quantitative results of different methods on BlendedMVS 6
--- CONCLUSION ---
In this work, we explore whether cost Transformer can improve the cost aggregation and propose a novel CostFormer with the cascade RDACT and RRT modules. The experimental results on DTU [Aanes et al., 2016] , Tanks & Temples [Knapitsch er al., 2017], ETH3D [Schéps et al., 2017], and --- --BlendedMVS [Yao et al., 2020] show that our method is competitive, efficient, and plug-and-play. Cost Transformer can be your need for better cost aggregation in multi-view stereo. References [Aanees et al., 2016] Henrik Aanzes, Rasmus Ramsbgl Jensen, George Vogiatzis, Engin Tola, and An ders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. Int. J. Comput. Vis., 120(2):153-168, 2016. [Chen et al., 2019] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo network. In JCCV, pages 1538-1547. IEEE, 2019. [Cheng et al., 2020] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In CVPR, pages 2521-2531. IEEE, 2020. [Cho et al., 2021] Seokju Cho, Sunghwan Hong, Sangryul Jeon, Yunsung Lee, Kwanghoon Sohn, and Seungryong Kim. Cats: Cost aggregation transformers for visual correspondence. Advances in Neural Information Processing Systems, 34, 2021. [Darmon et al., 2021] Frangois Darmon, Bénédicte Bascle, Jean-Clément Devaux, Pascal Monasse, and Mathieu Aubry. Deep multi-view stereo gone wild. CoRR, abs/2104.15119, 2021. [Ding et al.,2021] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmysnet: Global context-aware multiview stereo network with transformers. arXiv preprint arXiv:2111.14600, 2021. [Ding er al., 2022] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvsnet: Global context-aware multi-view stereo network with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8585-8594, 2022. [Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. (Fuhrmann et al., 2014] Simon Fuhrmann, Fabian Langguth, and Michael Goesele. Mve - a multi-view reconstruction environment. In GCH, pages 11-18. Eurographics Association, 2014. [Furukawa and Ponce, 2010] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE Trans. Pattern Anal. Mach. Intell., 32(8):1362—1376, 2010. [Galliani et al., 2015] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In JCCV, pages 873-881. IEEE Computer Society, 2015. [Gu et al., 2020] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495— 2504, 2020. [Hosni et al.,2012] Asmaa Hosni, Christoph Rhemann, Michael Bleyer, Carsten Rother, and Margrit Gelautz. Fast cost-volume filtering for visual correspondence and beyond. JEEE Transactions on Pattern Analysis and Machine Intelligence, 35(2):504—-511, 2012. [Ji et al., 2017] Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d neural network for multiview stereopsis. In JCCV, pages 2326-2334. IEEE Computer Society, 2017. [Knapitsch et al., 2017] Arno Knapitsch, Jaesik Park, QianYi Zhou, and Vladlen Koltun. Tanks and temples: benchmarking large-scale scene reconstruction. ACM Trans. Graph., 36(4):78:1-78:13, 2017. [Lee et al., 2021] Jae Yong Lee, Joseph DeGol, Chuhang Zou, and Derek Hoiem. Patchmatch-rl: Deep mvs with pixelwise depth, normal, and visibility. In Proceedings of the IEEE/CVF International Conference on Computer Vision, October 2021. (Li et al., 2021] Zhaoshuo Li, Xingtong Liu, Nathan Drenkow, Andy Ding, Francis X Creighton, Russell H Taylor, and Mathias Unberath. Revisiting stereo depth estimation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6197-6206, 2021. [Liao et al., 2022] Jinli Liao, Yikang Ding, Yoli Shavit, Dihe Huang, Shihao Ren, Jia Guo, Wensen Feng, and Kai Zhang. Wt-mvsnet: Window-based transformers for multi-view stereo, 2022. {Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1001210022, 2021. [Luo er al., 2019] Keyang Luo, Tao Guan, Lili Ju, Haipeng Huang, and Yawei Luo. P-mvsnet: Learning patch-wise matching confidence aggregation for multi-view stereo. In ICCV, pages 10451-10460. IEEE, 2019. [Ma et al., 2021] Xinjun Ma, Yue Gong, Qirui Wang, Jingwei Huang, Lei Chen, and Fan Yu. Epp-mvsnet: Epipolarassembling based depth prediction for multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5732-5740, 2021. [Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank --- --Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, highperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024-8035. Curran Associates, Inc., 2019. [Peng er al., 2022] Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and Ronggang Wang. Rethinking depth estimation for multi-view stereo: A unified representation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [Scharstein and Szeliski, 2002] Daniel Scharstein and Richard Szeliski. A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. Jnternational journal of computer vision, 47(1):7—42, 2002. [Schénberger and Frahm, 2016] Johannes L. Schénberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, pages 4104-4113. IEEE Computer Society, 2016. [Schéps et al.,2017] Thomas —_ Schdps, Johannes L. Sch6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. In CVPR, pages 2538-2547. IEEE Computer Society, 2017. [Sun et al., 2021] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8922-8931, 2021. [Thomee et al., 2016] Bart Thomee, David A. Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: the new data in multimedia research. Commun. ACM, 59(2):64-73, 2016. [Tola et al., 2012] Engin Tola, Christoph Strecha, and Pascal Fua. Efficient large-scale multi-view stereo for ultra highresolution image sets. Mach. Vis. Appl., 23(5):903-920, 2012. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [Wang et al.,2021a] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, and Marc Pollefeys. Itermvs: Iterative probability estimation for efficient multi-view stereo. arXiv preprint arXiv:2112.05126, 2021. [Wang et al., 2021b] Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Pablo Speciale, and Marc Pollefeys. Patchmatchnet: Learned multi-view patchmatch stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14194-14203, 2021. [Wang et al., 2022] Xiaofeng Wang, Zheng Zhu, Fangbo Qin, Yun Ye, Guan Huang, Xu Chi, Yijia He, and Xingang Wang. Mvster: Epipolar transformer for efficient multiview stereo, 2022. [Wei er al., 2021] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet: Adaptive aggregation recurrent multi-view stereo network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6187-6196, 2021. [Xu and Tao, 2020] Qingshan Xu and Wenbing Tao. Pvsnet: Pixelwise visibility-aware multi-view stereo network. CoRR, abs/2007.07714, 2020. [Yan et al., 2020] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multiview stereo net with dynamic consistency checking. In European Conference on Computer Vision, pages 674-689. Springer, 2020. [Yang et al., 2020a] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4877-4886, 2020. [Yang et al., 2020b] Jiayu Yang, Wei Mao, Jose M. Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In CVPR, pages 4876-4885. IEEE, 2020. [Yao et al., 2018] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European Conference on Computer Vision (ECCV), pages 767-783, 2018. [Yao et al., 2019] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In CVPR, pages 5525-5534. Computer Vision Foundation / IEEE, 2019. [Yao et al., 2020] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan. Blendedmvs: A large-scale dataset for generalized multiview stereo networks. Computer Vision and Pattern Recognition (CVPR), 2020. [Yi et al., 2020] Hongwei Yi, Zizhuang Wei, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Pyramid multi-view stereo net with self-adaptive view aggregation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, ECCV (9), volume 12354 of Lecture Notes in Computer Science, pages 766782. Springer, 2020. [Yu and Gao, 2020] Zehao Yu and Shenghua Gao. Fastmvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement. In CVPR, pages 1946-1955. IEEE, 2020. [Zhang et al., 2020] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. British Machine Vision Conference (BMVC), 2020. --- --[Zhu er al., 2021] Jie Zhu, Bo Peng, Wanging Li, Haifeng Shen, Zhe Zhang, and Jianjun Lei. Multi-view stereo with transformer. ArXiv, abs/2112.00336, 2021.
