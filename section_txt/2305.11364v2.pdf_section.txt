--- ABSTRACT ---
Large language models (LLMs) can be used to generate smaller, more refined datasets via few-shot prompting for benchmarking, fine-tuning or other use cases. However, understanding and evaluating these datasets is difficult, and the failure modes of LLMgenerated data are still not well understood. Specifically, the data can be repetitive in surprising ways, not only semantically but also syntactically and lexically. We present LinguisticLens, a novel interactive visualization tool for making sense of and analyzing syntactic diversity of LLM-generated datasets. LinguisticLens clusters text along syntactic, lexical, and semantic axes. It supports hierarchical visualization of a text dataset, allowing users to quickly scan for an overview and inspect individual examples. The live demo is available at https: //shorturl . at/ZHOUV. Index Terms: Visualization—Text—MLStatsModel— *e-mails: ereif|kahng|petridis@ google.com 1
--- INTRODUCTION ---
Large language models (LLMs) are becoming ubiquitous for their ability to solve a wide range of linguistic tasks with prompting that does not require additional model training [1,6,22]. This ability also lets them generate smaller, more refined datasets for finetuning [13, 25,27], benchmarking [29], low-resource tasks or languages [4, 15], and counterfactual testing (e.g., examples that are identical except for having different religious or gender-based identities [12]). A critical challenge lies in making sense of these generated datasets and evaluating their quality. Given that the desired tasks are often novel and have no existing dataset or ground truth by definition, automatically evaluating the quality of these generated examples with certain metrics is not straightforward. Although crowd workers can evaluate the quality of individual examples, it is costly, and finding patterns across large amounts of text examples remains a challenge. Moreover, understanding the specific failure modes of LLMs is still an evolving area, and these undesirable generated output trends can be hard to spot. In particular, generated examples often overfit to the seed examples in unexpected ways. One such pathology is syntactic overfitting, where generated examples are grammatically similarly or identical to the seed data. This can be difficult to find as a single overfit example is not a problem, but if larger portions of the dataset has the same syntactic structure, it is --- --a significant issue for dataset diversity. The same is true for lexical overfitting, where specific words appear frequently in the generated dataset more often than is desired. In this paper, we present LinguisticLens, a novel interactive visualization tool for making sense of synthetically-generated text datasets. LinguisticLens specifically focuses on analyzing the syntactic and lexical diversity of datasets. It allows users to explore groups of examples that are clustered based on their syntactic structure and lexical overlap. Clusters can be based on other text similarity methods too, including embedding similarity, and we find that our approach is more effective for analyzing the diversity of synthetic datasets. LinguisticLens runs on web browsers and users simply need to provide their datasets as CSV files. A live demo can be found at https://shorturl.at/zZHOUV. The source code is available at https: //github.com/PAIR-code/interpretability. 2 BACKGROUND: SYNTHESIZING DATASETS USING LLMs This section provides a brief background about how people generate datasets using LLMs. Suppose we want to create a small dataset of music recommendations to fine tune a music recommendation model that returns a set of artists based on a short query provided by a user. An example datapoint might have a query, ‘oldies but goodies’, and a label ‘Aretha Franklin, The Beach Boys, Stevie Wonder, The Supremes, Bill Withers.’ Within the prompt, we can provide a few examples that the model can use to generate similar datapoints. This is known as few-shot, or in-context, learning. For example, the prompt could be: Query: {oldies but goodies} Recommended Artists: {Aretha Franklin, Madonna} Query: {music that makes you want to dance} Recommended Artists: {Kraftwerk, The Cure, B-52s} Query: { The model will continue the text following this pattern (e.g., see below), and from this we can parse a new set of examples. With this approach, LLMs can create hundreds or thousands of these synthetic examples. Our goal is to make sense of them. chill out music} Recommended Artists: {Bonobo, Massive Attack} Query: {female vocalists} Recommended Artists: {Carole King, Joni Mitchell} 3
--- RELATED WORK ---
3.1 Evaluating Datasets Generated by LLMs Evaluating LLM-generated datasets is not a straightforward task. In the best case, one can measure downstream performance of a model trained on that data [4,25]. When this is impossible (e.g., benchmarks or a new task), the dataset quality must be evaluated with defined metrics [13]. Automatic
