--- ABSTRACT ---
Text-to-image diffusion models can generate diverse, high-fidelity images based on user-provided text prompts. Recent research has extended these models to support text-guided image editing. While text guidance is an intuitive editing interface for users, it often fails to ensure the precise concept conveyed by users. To address this issue, we propose Custom-Edit, in which we (i) customize a diffusion model with a few reference images and then (ii) perform text-guided editing. Our key discovery is that customizing only language-relevant parameters with augmented prompts improves reference similarity significantly while maintaining source similarity. Moreover, we provide our recipe for each customization and editing process. We compare popular customization methods and validate our findings on two editing methods using various datasets. 1.
--- METHOD ---
s and validate our findings on two editing methods using various datasets. 1. Introduction Recent work on deep generative models has led to rapid advancements in image editing. Text-to-image models [19, 22] trained on large-scale databases [23] allow intuitive editing [7, 15] of images in various domains. Then, to what extent can these models support precise editing instructions? Can a unique concept of the user, especially one not encountered during large-scale training, be utilized for editing? Editing with a prompt acquired from a wellperforming captioning model [13] fails to capture the appearance of reference, as shown in Fig. 1. We propose Custom-Edit, a two-step approach that involves (i) customizing the model [6, 12, 21] using a few reference images and then (ii) utilizing effective text-guided editing methods [7, 15, 16] to edit images. While prior customization studies [6, 12, 21] deal with the random generation of images (noiseimage), our work focuses on image editing (image—image). As demonstrated in Fig. 1, customization improves faithfulness to the reference’s appearance by a large margin. This paper shows that customizing * Corresponding Authors Reference Source BLIP-Edit Custom-Edit Strawberry cup... V* patterned teapot... ‘V* patterned teapot Red and gold tea kettle... Figure 1. Our Custom-Edit allows high-fidelity text-guided editing, given a few references. Edited images with BLIP2 [13] captions show the limitation of textual guidance in capturing the finegrained appearance of the reference. only language-relevant parameters with augmented prompts significantly enhances the quality of edited images. Moreover, we present our design choices for each customization and editing process and discuss the source-reference tradeoff in Custom-Edit. 2. Diffusion Models Throughout the paper, we use Stable Diffusion [19], an open-source text-to-image model. The diffusion model [5, 8, 24, 26] is trained in the latent space of a VAE [11], which downsamples images for computation efficiency. The model is trained to reconstruct the clean latent representation x from a perturbed representation x, given the text condition c, which is embedded with the CLIP text encoder [18]. The diffusion model is trained with the following objective: T SO Exp ,ellle = o(xe, #,6)|I7]; (1) t=where ¢€ is an added noise, t is a time step indicating a perturbed noise level, and eg is a diffusion model with a U-Net [20] architecture with attention blocks [27]. During training, the text embeddings are projected to the keys and --- --..\V" patterned teapot... @ CUo000S219 Sel Diffusion U-Net Reference 8] Q 8] Q z||2 Ale = (el/2 5||ie 4Diffusion U-Net mer gooooo).. patterned teapot... @ (a) Customization process [trainable JQ Fixed ... V¥ patterned teapot... Q* OU00C 0 t-2 || EfKv 2 {|S} Kv 5/8 5/|caleae) 2| Diffusion U-Net Injection Injection Output (P2P) (P2P) : : 2||8| 2 z||E]5 8 KV é BE KV =z ed Diffusion U-Net r o00000'a Strawberry cup o (b) Editing process Figure 2. Our Custom-Edit consists of two processes: the customization process and the editing process. (a) Customization. We customize a diffusion model by optimizing only language-relevant parameters (i.e., custom embedding V* and attention weights) on a given set of reference images. We also apply the prior preservation loss to alleviate the language drift. (b) Editing. We then transform the source image to the output using the customized word. We leverage the P2P and Null-text inversion methods [7, | 6] for this process. values of cross-attention layers, and the text encoder is kept frozen to preserve its language understanding capability. Imagen [22] and eDiffi [1] have shown that leveraging rich language understandings of large language models by freezing them is the key to boosting the performance. 3. Custom-Edit Our goal is to edit images with complex visual instructions given as reference images (Fig. 1). Therefore, we propose a two-step approach that (i) customizes the model on given references (Sec. 3.1) and (ii) edits images with textual prompts (Sec. 3.2). Our method is presented in Fig. 2. 3.1. Customization Trainable Parameters. We optimize only the keys and values of cross-attention and the ‘[rare token]’, following Custom-Diffusion [12]. As we discuss in Sec. 4, our results indicate that training these /anguage-relevant parameters is crucial for successfully transferring reference concepts to source images. Furthermore, training only these parameters requires less storage than Dreambooth [21]. Augmented Prompts. We fine-tune the abovementioned parameters by minimizing Eq. (1). We improve CustomDiffusion for editing by augmenting the text input as ‘[rare token] [modifier] [class noun]’ (e.g., ‘V* patterned teapot’ ). We find that ‘[modifier]’ encourages the model to focus on learning the appearance of the reference. Datasets. To keep the language understanding while finetuning on the reference, we additionally minimize prior preservation loss [21] over diverse images belonging to the same class as the reference. Thus, we use CLIP-retrieval [3] to retrieve 200 images and their captions from the LAION dataset [23] using the text query ‘photo of a [modifier] [class noun]’. 3.2. Text-Guided Image Editing Prompt-to-Prompt. We use Prompt-to-Prompt [7] (P2P), a recently introduced editing framework that edits images by only modifying source prompts. P2P proposes attention injection to preserve the structure of a source image. For each denoising step t, let us denote the attention maps of the source and edited image as M; and M,", respectively. P2P then injects a new attention map Edit(M,, M,*,t) into the model ¢g. Edit is an attention map editing operation, including prompt refinement and word swap. Additionally, P2P enables local editing with an automatically computed mask. P2P computes the average of cross-attention M; and Mry related to the word w and thresholds them to produce the binary mask B(M;,)U B(M;). Before editing with P2P, we utilize Null-Text Inversion [16] to boost the source preservation. Refer to Sec. C for a more description. Operation Choice. Due to the limited number of reference images, the customized words favor only a limited variety of structures. This inspired us to propose the following recipe. First, we use prompt refinement for the Edit function. Word swap fails when the customized words do not prefer the swapped attention map. Second, we use mask B(M,) rather than B(M,) U B(Mf), as the customized words are likely to generate incorrect masks. Source-Reference Trade-Off. A key challenge in image editing is balancing the edited image’s source and reference similarities. We refer to t/T as strength, where P2P injects self-attention from t = T to t = T. In P2P, we observed that a critical factor in controlling the trade-off is the injection --- --Edited Reference Source Edited V* wooden pot A bottle of wine and A V* wooden pot A cactus wearing A V* wooden pot a glass on a table of wine... sunglasses and a wearing... hat in the desert Asea turtle A V* tortoise plushy A painting of a swimming under the swimming... raccoon wearing a wearing... surface of the water crown V* ceramic bird Two small birds Two V* ceramic bird A blue jay perched A V* ceramic bird sitting on a branch sitting... on top of a basket perched... full of macarons V* pencil drawing Photo of a giraffe ‘V* pencil drawing Photo of anold man —-V* pencil drawing of drinking from a blue of a giraffe... in a cowboy hat an old... bucket smoking a cigar Two cats are sitting Two V* cat are A basket filled with ... filled with on a mirror in front sitting... apples sits ona V* cat sits... of a bathroom wooden chair Figure 3. Custom-Edit results. Our method transfers the reference’s appearance to the source image with unprecedented fidelity. The structures of the source are well preserved. We obtain source prompts using BLIP2 [13]. Except for the pencil drawing example, we use local editing of P2P with automatically generated masks. --- --of self-attention rather than cross-attention. Higher strength denotes higher source similarity at the expense of reference similarity. In Sec. 4, we also show results with SDEdit [15], which diffuses the image from t = 0 to t = 7 and denoises it back. As opposed to P2P, higher strength in SDEdit means higher reference similarity. 4.
