--- ABSTRACT ---
Stable diffusion, a generative model used in text-to-image synthesis, frequently encounters resolution-induced composition problems when generating images of varying sizes. This issue primarily stems from the model being trained on pairs of single-scale images and their corresponding text descriptions. Moreover, direct training on images of unlimited sizes is unfeasible, as it would require an immense number of text-image pairs and entail substantial computational expenses. To overcome these challenges, we propose a twostage pipeline named Any-Size-Diffusion (ASD), designed to efficiently generate well-composed images of any size, while minimizing the need for high-memory GPU resources. Specifically, the initial stage, dubbed Any Ratio Adaptability Diffusion (ARAD), leverages a selected set of images with a restricted range of ratios to optimize the text-conditional diffusion model, thereby improving its ability to adjust composition to accommodate diverse image sizes. To support the creation of images at any desired size, we further introduce a technique called Fast Seamless Tiled Diffusion (FSTD) at the subsequent stage. This method allows for the rapid enlargement of the ASD output to any high-resolution size, avoiding seaming artifacts or memory overloads. Experimental results on the LAION-COCO and MM-CelebA-HQ benchmarks demonstrate that ASD can produce well-structured images of arbitrary sizes, cutting down the inference time by 2 x compared to the traditional tiled algorithm.
--- INTRODUCTION ---
In text-to-image synthesis, Stable Diffusion (SD) (Rombach et al. 2022) has emerged as a significant advancement. Existing SD models (Ruiz et al. 2023; Meng et al. 2023) transform text aligned with image components into high-quality images, typically sized at 512 x 512 pixels. Despite these models having the ability to handle varying sizes, they noticeably struggle with resolution changes, resulting in poor composition (e.g., improper cropping and unnatural appearance), a problem demonstrated in Figure 1(a). The root of this issue lies in the models trained mainly on pairs of text and images of a uniform size, overlooking the complexities of handling images at multiple resolutions. Consequently, this leads to observed deficiencies in image composition. In pursuit of generating well-structured images at arbitrary aspect ratios, guided by textual descriptions, the Mul * Equal Contribution +Contact Acute teddy bear in front of a plain white wall. The teddy bear has a warm, brown fur that looks soft and fluffy, sitting on the brown wooden tabletop. (c) Ours ASD 900 x 1024 1024 x 512 1024 xFigure 1: Resolution-induced Poor Composition. Given the text, (a) SD2., and (b) MD2.;, a MultiDiffusion model, raise poor composition issues in red boxes when synthesizing images of varying sizes, as opposed to (c) our ASD. tiDiffusion methodology (Bar-Tal et al. 2023) leverages a pretrained text-conditional diffusion (e.g., stable diffusion), as a reference model and controls image synthesis through the utilization of several reference diffusion processes. Remarkably, the entire process is realized without requiring further training or fine-tuning. While efficient, it does not completely resolve the limitations associated with handling the reference model’s multi-resolution images. As a result, the production of images may exhibit suboptimal compositional quality. The underlying reason is also tied to the reference model’s training on images constrained to a singlescale size, as illustrated in Figure 1(b). A direct and appealing solution to the problem is to train the SD model to cope with every possible image size. Yet, this approach encounters an immediate and significant barrier: the infinite diversity of image ratios, which makes it practically unfeasible. Furthermore, it’s challeng --- --ing to gather an extensive collection of high-resolution images and corresponding text pairs. Even with a plethora of high-quality datasets available, the intrinsic pixel-based nature of SD requires substantial computational resources, particularly when dealing with high-resolution images of various sizes. The problem is further aggravated when considering the use of megapixel images for SD training, as this involves extensive repeated function equations and gradient computations in the high-dimensional space of RGB images (Ho, Jain, and Abbeel 2020). Even when a trained model is ready, the inference step is also time-consuming and memory-intensive. Through empirical observation, we have found that attempts to generate 4K HD images using the SD model trigger out-of-memory errors when executed on a GPU with a 32GB capacity. The key insight of this paper is to introduce a pioneering Any-Size-Diffusion (ASD) model, executed in two stages, which has the capability to synthesize high-resolution images of arbitrary sizes from text prompts. In its dualphase approach, our ASD not only efficiently handles the resolution-induced poor composition but also successfully circumvents out-of-memory challenges. At the outset, we are faced with the complexity of accommodating all conceivable image sizes, a challenge that might seem intractable. To address this, in the first stage, we introduce a multi-aspect ratio training strategy that operates within a well-defined, manageable range of ratios. This strategy is used to optimize our proposed Any Ratio Adaptability Diffusion (ARAD) model. As a result, it enables the production of well-composed images that are adaptable to any size within a specified range, while also ensuring a reduced consumption of GPU memory. In order to yield images that can fit any size, in the second stage, we propose an additional method called Fast Seamless Tiled Diffusion (FSTD) to magnify the image output originating from the preceding ARAD. Contrary to the existing tiled diffusion methods (Alvaro Barbero Jiménez 2023), which address the seaming issue but compromise on the speed of inference, our proposed FSTD designs an implicit overlap within the tiled sampling diffusion process. This innovation manages to boost inference speed without the typical seaming problems, achieving the high-fidelity image magnification. To sum up, the contributions of this paper are as follows: ¢ We are the first to develop the Any-Size-Diffusion (ASD) model, a two-phase pipeline that synthesizes highresolution images of any size from text, addressing both composition and memory challenges. ¢ We introduce a multi-aspect ratio training strategy, implemented within a defined range of ratios, to optimize ARAD, allowing it to generate well-composed images adaptable to any size within a specified range. ¢ We propose an implicit overlap in FSTD to enlarge images to arbitrary sizes, effectively mitigating the seaming problem and simultaneously accelerating the inference time by 2x compared to the traditional tiled algorithm.
--- RELATED WORK ---
Stable Diffusion. Building upon the foundations laid by the Latent Diffusion Model (LDM) (Rombach et al. 2022), diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2021) have achieved substantial success across various domains, including text-to-image generation (Nichol et al. 2022; Ramesh et al. 2022; Saharia et al. 2022), image-toimage translation (Dhariwal and Nichol 2021; Nichol and Dhariwal 2021), and multi-modal generation (Ruan et al. 2023). Owing to their robust ability to capture complex distributions and create diverse, high-quality samples, diffusion models excel over other generative
--- METHOD ---
allows for the rapid enlargement of the ASD output to any high-resolution size, avoiding seaming artifacts or memory overloads.
--- EXPERIMENT ---
al results on the LAION-COCO and MM-CelebA-HQ benchmarks demonstrate that ASD can produce well-structured images of arbitrary sizes, cutting down the inference time by 2 x compared to the traditional tiled algorithm. Introduction In text-to-image synthesis, Stable Diffusion (SD) (Rombach et al. 2022) has emerged as a significant advancement. Existing SD models (Ruiz et al. 2023; Meng et al. 2023) transform text aligned with image components into high-quality images, typically sized at 512 x 512 pixels. Despite these models having the ability to handle varying sizes, they noticeably struggle with resolution changes, resulting in poor composition (e.g., improper cropping and unnatural appearance), a problem demonstrated in Figure 1(a). The root of this issue lies in the models trained mainly on pairs of text and images of a uniform size, overlooking the complexities of handling images at multiple resolutions. Consequently, this leads to observed deficiencies in image composition. In pursuit of generating well-structured images at arbitrary aspect ratios, guided by textual descriptions, the Mul * Equal Contribution +Contact Acute teddy bear in front of a plain white wall. The teddy bear has a warm, brown fur that looks soft and fluffy, sitting on the brown wooden tabletop. (c) Ours ASD 900 x 1024 1024 x 512 1024 xFigure 1: Resolution-induced Poor Composition. Given the text, (a) SD2., and (b) MD2.;, a MultiDiffusion model, raise poor composition issues in red boxes when synthesizing images of varying sizes, as opposed to (c) our ASD. tiDiffusion methodology (Bar-Tal et al. 2023) leverages a pretrained text-conditional diffusion (e.g., stable diffusion), as a reference model and controls image synthesis through the utilization of several reference diffusion processes. Remarkably, the entire process is realized without requiring further training or fine-tuning. While efficient, it does not completely resolve the limitations associated with handling the reference model’s multi-resolution images. As a result, the production of images may exhibit suboptimal compositional quality. The underlying reason is also tied to the reference model’s training on images constrained to a singlescale size, as illustrated in Figure 1(b). A direct and appealing solution to the problem is to train the SD model to cope with every possible image size. Yet, this approach encounters an immediate and significant barrier: the infinite diversity of image ratios, which makes it practically unfeasible. Furthermore, it’s challeng --- --ing to gather an extensive collection of high-resolution images and corresponding text pairs. Even with a plethora of high-quality datasets available, the intrinsic pixel-based nature of SD requires substantial computational resources, particularly when dealing with high-resolution images of various sizes. The problem is further aggravated when considering the use of megapixel images for SD training, as this involves extensive repeated function equations and gradient computations in the high-dimensional space of RGB images (Ho, Jain, and Abbeel 2020). Even when a trained model is ready, the inference step is also time-consuming and memory-intensive. Through empirical observation, we have found that attempts to generate 4K HD images using the SD model trigger out-of-memory errors when executed on a GPU with a 32GB capacity. The key insight of this paper is to introduce a pioneering Any-Size-Diffusion (ASD) model, executed in two stages, which has the capability to synthesize high-resolution images of arbitrary sizes from text prompts. In its dualphase approach, our ASD not only efficiently handles the resolution-induced poor composition but also successfully circumvents out-of-memory challenges. At the outset, we are faced with the complexity of accommodating all conceivable image sizes, a challenge that might seem intractable. To address this, in the first stage, we introduce a multi-aspect ratio training strategy that operates within a well-defined, manageable range of ratios. This strategy is used to optimize our proposed Any Ratio Adaptability Diffusion (ARAD) model. As a result, it enables the production of well-composed images that are adaptable to any size within a specified range, while also ensuring a reduced consumption of GPU memory. In order to yield images that can fit any size, in the second stage, we propose an additional method called Fast Seamless Tiled Diffusion (FSTD) to magnify the image output originating from the preceding ARAD. Contrary to the existing tiled diffusion methods (Alvaro Barbero Jiménez 2023), which address the seaming issue but compromise on the speed of inference, our proposed FSTD designs an implicit overlap within the tiled sampling diffusion process. This innovation manages to boost inference speed without the typical seaming problems, achieving the high-fidelity image magnification. To sum up, the contributions of this paper are as follows: ¢ We are the first to develop the Any-Size-Diffusion (ASD) model, a two-phase pipeline that synthesizes highresolution images of any size from text, addressing both composition and memory challenges. ¢ We introduce a multi-aspect ratio training strategy, implemented within a defined range of ratios, to optimize ARAD, allowing it to generate well-composed images adaptable to any size within a specified range. ¢ We propose an implicit overlap in FSTD to enlarge images to arbitrary sizes, effectively mitigating the seaming problem and simultaneously accelerating the inference time by 2x compared to the traditional tiled algorithm. Related Work Stable Diffusion. Building upon the foundations laid by the Latent Diffusion Model (LDM) (Rombach et al. 2022), diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2021) have achieved substantial success across various domains, including text-to-image generation (Nichol et al. 2022; Ramesh et al. 2022; Saharia et al. 2022), image-toimage translation (Dhariwal and Nichol 2021; Nichol and Dhariwal 2021), and multi-modal generation (Ruan et al. 2023). Owing to their robust ability to capture complex distributions and create diverse, high-quality samples, diffusion models excel over other generative methods (Goodfellow et al. 2014). In the field, Stable Diffusion (SD) (Rombach et al. 2022) has emerged as a leading model for generating photo-realistic images from text. While adept at producing naturalistic images at certain dimensions (e.g., 512 x 512), it often yields unnatural outputs with sizes beyond this threshold. This constraint principally originates from the fact that existing stable diffusion models are exclusively trained on images of a fixed size, leading to a deficiency in high-quality composition on other sizes. In this paper, we introduce our Any-Size-Diffusion (ASD) model, designed to generate high-fidelity images without size constraints. Diffusion-based Image Super-Resolution. The objective of Image Super-Resolution (SR) is infer a high-resolution (HR) image from a corresponding low-resolution (LR) counterpart. The utilization of generative models to magnify images often omits specific assumptions about degradation, leading to challenging situations in real-world applications. Recently, diffusion-based methods (Sahak et al. 2023; Saharia et al. 2023; Li et al. 2023; Ma et al. 2023) have shown notable success in real-world SR by exploiting generative priors within these models. Though effective, these approaches introduce considerable computational complexity during training, with a quadratic increase in computational demands as the latent space size increases. An optimized method, known as StableSR(Wang et al. 2023), was developed to enhance performance while reducing GPU memory consumption. However, this method can become timeinefficient when processing images divided into numerous overlapping regions. In the next phase of our ASD pipeline, we present a fast seamless tiled diffusion technique, aimed at accelerating inference time in image SR. Method To resolve the issue of resolution-induced poor composition when creating high-fidelity images of various sizes from any text prompt, we propose a straightforward yet efficient approach called Any Size Diffusion (ASD). This approach simplifies the process of text-to-image synthesis by breaking it down into two stages (see Figure 2). * Stage-I, termed as Any Ratio Adaptability Diffusion (ARAD), trains on multiple aspect-ratio images and generates an image conditioned on a textual description and noise size, avoiding poor composition issues. --- --4% Frozen @ LoRA Trainer ~ Gandalf from the lord of rings \ Input text & size extcondition +—> 0000 0000—. \__ T-steps__ Generated images upscale _/ Figure 2: The Any-Size-Diffusion (ASD) pipeline, including: 1) Stage-I, Any Ratio Adaptability Diffusion, translates text into images, adapting to various aspect ratios, and 2) is responsible for transforming low-resolution images from the Stage-I into high-resolution versions of any specified size. For procedure (c), the implicit overlap in tiled sampling, only the solid green line region is sent to the UNetModel for current denoising. At Stage-II, the dashed green arrow represents regions that are directly copied from the preceding denoised latents, potentially enhancing efficiency and consistency within the overall process. ¢ Stage-II, known as Fast Seamless Tiled Diffusion (FSTD), magnifies the image from Stage-I to a predetermined larger size, ultimately producing a high-resolution synthesized image, adjustable to any specified size. Pipeline As depicted in Figure 2, ARAD is implemented based on the text-conditioned latent diffusion model (LDM) (Rombach et al. 2022) for arbitrary ratio image synthesis. During the inference process, ARAD receives a user-defined text prompt and noise size (e.g., “Gandalf from the lord for the rings’). Initially, the pre-trained text encoder (Cherti et al. 2023) is adapted to process this prompt, subsequently generating a contextual representation referred to as a textual embedding To(y). Then, a random noise of the base resolution size, denoted as ¢, is initialized. The noisy input conditioned on the textual embedding p(ely) is progressively denoised by the UNetModel (Cherti et al. 2023). This process is iterated through T times, leveraging the DDPM algorithm (Song, Meng, and Ermon 2020) to continuously remove noises and restore the latent representation z. Ultimately, a decoder D is employed to convert the denoised latent back into an image I € R4*W™3 Here, H and W represent the height and width of the image, respectively. Subsequently, the FSTD model accepts the image generated in the previous step as input and performs inference based on the image-conditional diffusion (Wang et al. 2023). In detail, the image is magnified by a specified size. A pretrained visual encoder € is employed to map the resulting image I’ € R¥#’*'*3 into a latent representation z = E(I'). A normal distribution-based noise « ~ N’(0, 1) is then added to it, yielding the noisy latent variable z’ = A(z). The image, conditioned on itself p(z’|z), undergoes progressive iterations by the UNetModel, utilizing our proposed tiled sampling J ¢ R!*™*3 for T cycles. Lastly, the decoder D is employed to project the denoised latent variable into the final output, effectively transforming the latent space back into the image domain. Any Ratio Adaptability Diffusion (ARAD) In this stage, ARAD is proposed to make the model have the capability of generating an image, adjustable to varying aspect ratios, resolving the issue of resolution-induced poor composition. This stage is mainly achieved by our designed multi-aspect ratio training strategy. Multi-aspect ratio training. Instead of directly training on the original image and text pairs, we employ our aspectratio strategy to map the original image into an image with a specific ratio. To be more precise, we define a set of ratios {r1,12,...,7%}, each corresponding to specific sizes {s1, 82,...,5n}, where n represents the number of predefined aspect ratios. For each training image x € R?*W*3, we calculate the image ratio as r = H/W. This ratio r is --- --(c) Ours implicitly overlap Figure 3: Comparison of various tiling strategies: (a) without overlapping, (b) with explicit overlapping, and (c) with implicit overlapping. Green tiles are explicit overlaps, and the orange tile is our implicit overlap at step t-1. then compared with each predefined ratio, selecting the one m with the smallest distance as the reference ratio. The index m is determined by —rl|}, () where f(m) represents the smallest distance between the current ratio and the predefined ratio. Therefore, if the image has a ratio similar to the m*” predefined size s,,,, the original size of the training image is resized to s,,. Forward ARAD process. During the training process, a pair consisting of an image and its corresponding text (xr, y) is processed, where x represents an image in the RGB space R’*W*3 and y denotes the associated text. A fixed visual encoder, €, is used to transform the resized image s,,, into a spatial latent code z. Meanwhile, the corresponding text is converted into a textual representation 79(y) via OpenCLIP (Cherti et al. 2023). For the total steps T’, conditional distributions of the form p(z:|y), t = 1---T', can be modeled using a denoising autoencoder €9(2;, t, y). Consequently, the proposed ARAD can be learned using an objective function argmin f(m) = {|rimr|,e-+ rman. Irn Larap = Be(a).yewn(0.1.| lle €0(20,t, 709) 13] » Q) Fast Seamless Tiled Diffusion (FSTD) In the second stage, we propose FSTD, a training-free approach built on StableSR (Wang et al. 2023) that amplifies the ARAD-generated image to any preferred size. To achieve efficient image super-resolution without heavy computational demands during inference, we devise an implicit overlap technique within the tiled sampling method. Tiled sampling. For clarity, consider an upscaled image Z € R4’xW’x3_ partitioned into M petite tiles, symbolized as {P; € R’*ex3 | 1 < i < M}, where w and h denote the width and height of each tile. We initially encode each tile P; using an encoder function €, adding the random noise, to generate a set of noisy latent representations Z = {2Z; = E(P;) +e |e ~ N(0,1), 1 < i < M}. Subsequently, each noisy tile is processed by the UNetModel conditioned on the original tile for T steps, resulting in a set of denoised latents Z’ = {2Z’;| €; ~ N(0,1), 1 < i < M}. Finally, a decoder fp is applied to convert them back into image space, culminating in the reconstructed image T= {Phe RS | Pl = fp(Z)), 1 <i< Mh. @G) Herein, P/ represents the i” tile decoded from its corresponding denoised latent tile. However, a seaming problem emerges when any two tiles in the set are disjoint, as depicted in Figure 3(a). To tackle this, we implement overlaps between neighboring tiles that share common pixels (Figure 3(b)). While increasing explicit overlap can effectively mitigate this issue, it substantially escalates the denoising time. As a consequence, the inference time quadratically increases with the growth in overlapping patches. Indeed, it’s practically significant to strike a balance between inference time and the amount of overlap. Implicit overlap in tiled sampling. To speed up the inference time while avoiding the seaming problem, we propose an implicit overlap in tiled sampling. As depicted in Figure 3(c), the magnified image is divided into L nonoverlapping tiles and we keep the quantity of disjoint noisy latent variables constant during the reverse sampling process. Prior to each sampling step, we apply a random offset to each tile, effectively splitting Z into two components: Z* (the shifted region with tiling) and Z° (the constant region without tiling). This can be mathematically represented as Z = Z°UZ°. Take note that at the initial time step, Z° = ©. At each sampling, the shifted part, Z*, is a collection of L disjoint tiles, denoted as Z* = {Z$ | 1 <i < L}. Here, each Z? symbolizes a shifted tile. The shifted portion, 2°, comprises L disjoint tiles that change dynamically throughout the sampling process. Within this segment, each tile is expressed as Z?,, = Zy,+Ay;,e;+Ae; forl <i < L. Here, Ax; and Ay denote the random offsets for tile 2? implemented in the preceding step. As for the constant section without tiling, denoted as Z°, the pixel value is sourced from the corresponding latent variable in the previous sampling step. It is worth noting that after each time step, Z° is non-empty, symbolically represented as Z° # ©. This approach ensures implicit overlap during tiled sampling, effectively solving the seaming issue. Experiments Experimental Settings Datasets. The ARAD of our ASD is trained on a subset of LAION-Aesthetic (Schuhmann 2022) with 90k textimage pairs in different aspect ratios. It is evaluated on MALAION-COCO with 21,000 images across 21 ratios (selecting from LAION-COCO (Schuhmann et al. 2022)), and MA-COCO built from MS-COCO (Lin et al. 2014) containing 2,100 images for those ratios. A test split of MMCelebA-HQ (Xia et al. 2021), consisting of 2,824 face image pairs in both low and high resolutions, is employed to evaluate our FSTD and whole pipeline. --- --A panoramic dream castle under the blue sky at the Disney. [1024 x 2048] (a) SR-Plus (b) SR-Tile (©) SR-Tile-Plus — (d) AR-Plus (e) AR-Tile — (f) ASD (Ours) A white, grand and elegant victorian mansion with intricate details in a sunny day. [4096 x 1024] — (c) SR-Tile-Plus _ “) ASD (Ours) Figure 4: Qualitative comparison of our proposed ASD method with other baselines, including (a) SR-Plus, (b) SR-Tile, (c) SR-Tile-Plus, (d) AR-Plus, (e) AR-Tile and (f) our proposed ASD. The yellow box indicates the resolution-induced poor composition. The orange box indicates better composition. The red solid line box is the zoom-in of the red dashed line box, aiming to inspect if there are any seaming issues. Our ASD outperforms others in both composition quality and inference time. Table 1: Quantitative evaluation against baselines. (a) SR-Plus, (b) SR-Tile, (c) SR-Tile-Plus, (d) AR-Plus, (ec) AR-Tile and (f) our ASD. ‘S’ and ‘A’ denote single and arbitrary ratios, respectively. All tests run on a 32G GPU. Notably, under the same GPU memory, our ASD achieves at least 9x higher resolution than the original SD model. Exp | Stage-I Stage-II Capability MM-CelebA-HQ | Ratio Tile Overlap Composition Max Resolution Seam FID |. IS t CLIP t (a) s xK x Poor 2048? N 118.83 211 27.(b) Ss v xK Poor 18432? Y 111.96 (6.87) 2.46 (40.35) 27.46 (+ 0.24) (c) Ss v v Poor 18432” N 111.06 (7.77) 2.53(+0.42) 27.55 (+ 0.33) (d) A xK xK Excellent 2048? N 92.80 (26.03) 3.97(+ 1.86) 29.15 (+ 1.93) (e) A v xK Excellent 18432? Y 85.66 (33.17) 3.98 (+ 1.87) 29.17 4 1.95) (f) A v v Excellent 18432? N 85.34(.22.49) 4,044 1.93) 29,23 (4 2.01) --- --Implementation Details. Our proposed method is implemented in PyTorch (Paszke et al. 2019). A multi-aspect ratio training method is leveraged to finetune ARAD (using LoRA (Hu et al. 2021)) for 10,000 steps with a batch size of 8. We use Adam (Kingma and Ba 2014) as an optimizer and the learning rate is set to 1.0e-4. Our FSTD (the second stage model) is training-free and is built upon StableSR (Wang et al. 2023). During inference, DDIM sampler (Song, Meng, and Ermon 2020) of 50 steps is adopted in ARAD to generate the image according to the user-defined aspect ratio. In the second stage, we follow StableSR to use 200 steps DDPM sampler (Ho, Jain, and Abbeel 2020) for FSTD. Evaluation metrics. For benchmarks, we employ common perceptual metrics to assess the generative text-to-image models, including FID (Heusel et al. 2017), IS (Salimans et al. 2016) and CLIP (Radford et al. 2021). IS correlates with human judgment, important to evaluate the metric on a large enough number of samples. FID captures the disturbance level very well and is more consistent with the noise level than the IS. CLIP score is used to measure the cosine similarity between the text prompt and the image embeddings. Besides, the extra metrics (e.g., PSNR, SSIM (Wang et al. 2004) and LPIPS (Zhang et al. 2018)) are employed to assess the super-resolution ability of the second stage of our ASD. PSNR and SSIM scores are evaluated on the luminance channel in the YCbCr color space. LPIPS quantifies the perceptual differences between images. Baseline Comparisons Based on state-of-the-art diffusion models, we build the following six baselines for comparison. ¢ SR-Plus: employs SD 2.1 (Rombach et al. 2022) for the direct synthesis of text-guided images with varying sizes. ¢ SR-Tile: utilizes SD 2.1 for initial image generation, magnified using StableSR (Wang et al. 2023) with a nonoverlap in tiled sampling(Alvaro Barbero Jiménez 2023). ¢ SR-Tile-Plus: A two-stage method that initiates with SD 2.1 (Rombach et al. 2022) and refines the output using our proposed FSTD, facilitating the synthesis of images of arbitrary dimensions. ¢ AR-Plus: deploys our proposed ARAD model for direct, text-driven image synthesis across a spectrum of sizes. ¢ AR-Tile: commences with our ARAD model for initial image generation, followed by magnification via StableSR employing a non-overlap in tiled sampling. ¢ ASD: is our proposed novel framework, integrating ARAD in Stage I and FTSD in Stage II, designed to synthesize images with customizable dimensions. Quantitative evaluation. As reported in Table 1, our proposed ASD method consistently outperforms the baseline methods. Specifically, our ASD model shows a 33.49 reduction in FID score compared to (a) SR-Plus, and an increase of 1.92 and 2.01 in IS and CLIP scores, respectively. On a 32GB GPU, SR-Plus fails to synthesize images exceeding 2048? resolution. In contrast, our ASD effectively mitigates this constraint, achieving at least 9x higher resolution than SR-Plus under identical hardware conditions. Additionally, Table 2: Comparison of our ARAD and other diffusionbased approaches. We compare their compositional ability to handle the synthesis of images across 21 different sizes. M MA-LAION-COCO MA-COCO lethod FID| ISt CLIPt FID| ISt CLIPt SDo1 14.32 31.25 31.92 42.50 30.20 31.MD, 14.57 28.95 32.11 43.25 28.92 30.ARAD 13.98 34.03 32.60 40.28 29.77 31.576X768 512 X 10. (a) SDo4 (b) MD24 (c) ARAD Figure 5: Comparison of visual results. Composition quality of the text-to-image synthesis using (a) SD2.1, a stable diffusion 2.1, (b) MD2.1, a multi-diffusion based on SD 2.1, and (c) our ARAD. Color boxes indicate poor composition. we also have the following observations: (i) Utilizing multiaspect ratio training results in notable improvements across various comparisons, specifically reducing FID scores from 118.83 to 92.80 in (a)-(d), 111.96 to 85.66 in (b)-(e), and 111.06 to 85.34 in (c)-(f). (ii) Introducing a tiled algorithm at the second stage enables the generation of images with unlimited resolution, while simultaneously enhancing performance, e.g., FID scores improve from 92.80 to 85.66 when comparing (a)-(b) and (d)-(c). (iii) Implementing overlap in tiled sampling effectively addresses the seaming issue, as evidenced by the comparisons between (b)-(c) and (e)-(f). Qualitative comparison. As depicted in Fig. 4, the images synthesized by ASD exhibit superior composition quality (e.g. proper layout) when compared to other baseline methods. Additionally, ASD can generate 4K HD images that are not only well-composed but also free from seaming artifacts. Specifically, when guided by a text description, the AR-Plus method is observed to generate a more complete castle than SR-Plus, as demonstrated in Fig.4(a) vs. Fig.4(d). Compared with SR-Plus, AR-Tile can produce realistic images but is hindered by the presence of seaming issues (see Fig. 4(e)). In contrast, Fig. 4(f) shows that our ASD successfully eliminates seaming artifacts and ensures the production of wellcomposed images, while minimizing GPU memory usage. --- --Table 3: Performance on ARAD trained on the various types of aspect ratios. “All” denotes the 9 aspect ratios. Types MA-LAION-COCO MA-COCO FID| ISt CLIPt FID] ISft CLIPt 3 1436 32.53 32.38 41.28 29.58 31.5 14.10 33.61 32.58 40.25 29.63 31.All 13.98 34.03 32.60 40.28 29.77 31.(a) Zoomed LR (b) w/o overlap Figure 6: The super-resolution results of <4 for different methods. We visually compare the (a) Zoomed LR (bicubic method), tiled diffusion with (b) non-overlap and (c) explicit overlap tiles; and (d) our FSTD which uses implicit overlap in tiled sampling. Notably, (d) is 2x faster than (c). ARAD Analysis (c) Explicit (d) Implicit To verify the superiority of our proposed ARAD in addressing resolution-induced poor composition issues, we conduct the ablation study, specifically at the initial stage. Impact of ARAD. Table 2 highlights the performance of ARAD, showing improvements of 13.98, 34.03, and 32.in FID, IS, and CLIP, respectively, on MA-LAION-COCO over original SD 2.1 and MultiDiffusion (Bar-Tal et al. 2023) (MD21). This superiority is further illustrated in Fig. 5. While SD2.; and MD2.; exhibit composition problems, our ASD produces images that are consistent with user-defined textual descriptions. For example, MD»2 ; incorrectly generates two overlapped blue suits from a prompt for a white suit, a mistake not present in our ASD’s results. Influence on the number of aspect ratios. Table 3 reveals the model’s performance across various aspect ratios. The data shows that increasing the number of aspect ratios in the training dataset improves performance, with FID scores falling from 14.36 to 13.98. A comparison between 3 and 5 aspect ratios highlights a significant improvement, as the FID score drops from 14.36 to 14.10. Further increasing the aspect ratios continues this trend, reducing the FID score to 13.98. This pattern emphasizes the importance of aspect ratios in enhancing model performance. Table 4: The versatility of tiled sampling in FSTD. We conduct ablation on the MM-CelebA-HD benchmark. “w/o”, “explicit”, and “implicit” describe non-overlapping, explicit, and implicit overlap in tile sampling respectively. “fixed”, and “random” refer to different tile offset strategies. Here, the overlap of two adjacent tiles is 32x32. Method MM-CelebA-HQ Time Overlap & Offset PSNRt SSIMt LPIPS| FID| _ per frame w/o overlap 26.89 0.76 0.09 22.80 75.explicit 27.49 0.76 0.09 24.15 166.8s implicit & fixed 26.83 0.75 0.08 21.37 -75.01s implicit & random 27.53 0.76 0.08 22.25 75.19s FSTD Analysis Although we have proved the effectiveness of the proposed FSTD in Fig. 4 and Table 1, we now explore its design in the image super-resolution performance on MM-CelebA-HD. Table 4 report the ablation study on the versatility of tiled sampling; see more details in the supplementary material. Importance of tiles with overlap. The first two lines from Table 4 reveal a comparison between the perceptual performance of explicit overlap and non-overlap in tiled sampling. Specifically, the explicit overlap exhibits superior performance (e.g., 27.49 vs. 26.89 on PSNR). However, nonoverlap tiled sampling offers an approximately 2x faster inference time compared to the explicit overlap. Despite this advantage in speed, Fig. 6(b) clearly exposes the seaming problem associated with non-overlap tiled sampling, highlighting the trade-off between performance and efficiency. Implicit vs. explicit overlap. An analysis of the results presented in Table 4 and Fig.6(c)-(d) confirms that the use of implicit overlap in tiled sampling yields the best performance across both perceptual metrics and visual representation. Further examination of the last column in Tabledemonstrates that the inference time for implicit overlap in tiled sampling is nearly equivalent to that of tiling without overlap. Moreover, the implementation of implicit overlap successfully reduces the inference time from 166.8s to approximately 75.0s. This ablation study validates the superiority of our proposed FSTD method, accentuating its capacity to achieve an optimal balance between performance quality and inference time. Effect of various offset strategies. The last two lines of Table 4 demonstrate the advantage of using a random offset in implicit overlap tiled sampling. Specifically, when comparing the fixed and random offset methods in implicit overlap, the random offset yields a PSNR value of 27.53, outperforming the fixed offset, which registered at 26.83. The results for other perceptual metrics and visual performance indicators are found to be nearly identical, further emphasizing the preference for a random offset in this context.
--- CONCLUSION ---
In this study, we address the challenge of resolution-induced poor composition in creating high-fidelity images from any text prompt. We propose Any Size Diffusion (ASD), a method consisting of ARAD and FSTD. Trained with multiaspect ratio images, ARAD generates well-composed im --- --ages within specific sizes. FSTD, utilizing implicit overlap in tiled sampling, enlarges previous-stage output to any size, reducing GPU memory consumption. Our ASD is validated both quantitatively and qualitatively on real-world scenes, offering valuable insights for future work. --- --Appendix Details of Our ASD Methodology In this section, we present a comprehensive analysis of our proposed Any Size Diffusion (ASD) pipeline. We first delineate the multi-aspect ratio training strategy implemented in Stage-I. Subsequently, we provide a thorough examination of the implicit overlap characteristics inherent to the tiled sampling approach adopted in Stage II. Stage-I: Any Ratio Adaptability Diffusion (ARAD) Algorithm 1 presents a detailed description of our proposed multi-aspect ratio training strategy. We establish nine predefined key-value pairs, where each key represents a distinct aspect ratio and each value corresponds to a specific size. The training images, which vary in size, are processed according to this multi-aspect ratio strategy, as specified in lines 4-12 of Algorithm 1. Algorithm 1: Multi-Aspect Ratio Training Strategy 1 Input: Image Z € RY*“ 3, Ratio-Size Dictionary Dyss = {11 1 1,172 1 82,°°+ , 79: So}2 Output: Resized Image for Model Training ; F op — HL. 3 Compute image ratio r = 7 ; 4 Initialize min Value = 00 ; 5 Initialize minIndex = 0; 6 for each ratio r; in {r1,72,--+ ,r9} do 7 Compute distance = |r — ri| ; 8 if distance < minValue then 9 minValue = distance ; 10 minIndex =i aT t=ti+1; 12 end ; 13 Retrieve r,,,, 5, from D,._,, using minIndex ; 14 Resize image TZ to s,,, for model training Stage-II: Fast Seamless Tiled Diffusion (FSTD). Algorithm 2 outlines the step-by-step procedure of our proposed Fast Seamless Tiled Diffusion (FSTD) technique, designed to efficiently upscale a low-resolution image to a target resolution of H x W. Initially, as delineated from line 1 to line 7 in Algorithm 2, we partition the input low-resolution image into M/ non-overlapping tiles, with each being represented as a separate latent variable {L1,L2,--- , Liz}. These latent variables collectively form a set Z, as described in line 8. In particular, Z is composed of two distinct components, as assumed in line 11: the shifted region Z* and the constant region Z°. The former is designated for processing through tiled denoising, while the latter remains excluded from the tiling process at the current time step. At Algorithm 2: Procedures for our proposed FSTD. Input: an image Z ¢ R4*W*Output: an image Z’ ¢ R?*”*2 » Step 1: Tiled Sampling Preparation Divided the input image TZ into a set of M disjoint tiles: {P?**3 | 1 <i< M}; for each tile P; in {P,, Po,--- , Pyg} do /* add a random noise to latent */ Ty, =E(Pi) +end ; 8 Hence, we have M disjoint latents {L1,L2,---,L}; aus w a 10 » Step 2: Implicit Overlap Tiled Sampling 11 Suppose that Z* = {L1, Lo,--- , Lys} and Z° = @, and Z = 2° U Z°, and the total step is T ; 13 Initialize 2%. = {L1,L2,---, Lu}; 14 Initialize Z6 = O ; 15 Initialize Zp = ZU ZG;17 * Implicit Overlaps for Tiles 18 for each time step t in {J —1,--- ,0} do 19 Set a random offset (Ax, Ay); 20 for each tile L; in {L, Lo,--- , Lys} do 21 Lo Liseiys = Lier ariyitayi22 end ; 23 /* Update Z* and Z° */ 24 Zp= {Livery »D2,20,yo0"** Laeuyat and the number of the tiles keeps constant ; 25 Zf = Zi41 \ Zs 26 /* Denoise updated latents */ 27 for each tile Lj, y, in Z? do 28 L Apply UNetModel to denoise (Li,x,.,; )29 end ; 30 | 2, = 2) UZ and Zp FO; 31 end;33 After denoising for T timesteps, Z’ = Zo ; 34 Consequently, we have Z’ and Z’ = D(2’). --- --the initial T*” time step, as illustrated in lines 13-15, 2° is initialized as {L,L2,--- ,Lyc}, and Z° is initialize as an empty set. This is due to the initial offsets of each tile being set to zero. At the heart of the algorithm, as exhibited from lines 17 to 31, is a novel mechanism termes ‘Implicit Overlap in Tiled Sampling’. This mechanism is conceived to significantly reduce both inference time an GPU memory consumption compared to the non-tiled sampling method used in stable diffusion processes. For each time step, the algorithm randomly offsets the positions of these latent variables while maintaining their quantities invariant. This results in an updated shifted region, denote as ZF © {Ly .01,yr) L2,02,y21°1* +L M,em.yu }> and a novel constant region Zf € Z,41 \ Z. Notably, from the secon denoising step onwards, Zf becomes a non-empty set, with each pixel within this constant region retaining the same value as the corresponding pixel in the preceding denoise latent variables. After T’ time steps, the iterative procedure obtains a new denoised latent set, denoted as 2’. In the final stage, the algorithm decodes this latent set Z’ back into an image of the user-defined size H x W x 3, thereby producing the final super-resolved output image. Implementation Details This section elaborates on the implementation details associated with the multi-aspect ratio training approach, and delineate configurations integral to the tiled sampling strategy. Multi-aspect ratio training. We establish a set of nine distinct aspect ratios, each associated with a corresponding size specification, as enumerated in Table 5. Prior to the commencement of training, images are systematically resized to a predetermined dimensionality, as prescribed by Algorithm 1. Subsequently, images exhibiting identical aspect ratios are agglomerated into a stochastic batch, which is then utilized as the input for model training. Explicit and implicit overlaps in tiled sampling. In the context of tiled sampling, we explore two distinct strategies: explicit and implicit overlap configurations. Notably, in both strategies, the dimensions of the input image and tiles can be parameterized by the user. For the explicit overlap configuration, we mathematically formulate the relationship between the number of tiles and their overlaps as follows: Wimage x image (Wiite — overlap) — (Htite — overlap Muites = | ys (4) where Wimage and Himage represent the width and height of the input image, respectively, while W;;,e and Hie denote the corresponding dimensions of each tile. In contrast, the implicit overlap strategy conventionally configures the size of each tile to 512 x 512 with zero overlaps between adjacent tiles. Moreover, the spatial dimensions of latent variables are 64x 64. To introduce a form of overlap, we employ a random offset strategy, designed to control the translational shifts of these tiled latent representations, thereby achieving implicit overlap of tiles. Specifically on the MM-CelebA-HQ (Xia et al. 2021) benchmark, we utilize high-resolution images with dimensions of 1024 x 1024 alongside corresponding low-resolution images sized 256 x 256. Table 5: Dictionary of nine pre-defined ratio-size pairs. Each ratio corresponds to a pre-defined image size. # Ratio Size 1 1.0000 512 x2 0.7500 576 x3 1.3330 768 x4 0.5625 576 x5 1.7778 1024 x6 0.6250 640 x7 1.6000 1024 x8 0.5000 512 x9 2.0000 1024 xTable 6: The Effect of different random offset ranges in FSTD. We conduct ablation on the MM-CelebA-HD benchmark by upscaling the low-resolution images of sizes 256 x 256 to 1024x1024. The range of random offset is 16, 32 and 48. The row in gray is the default setting of our method. Offset MM-CelebA-HQ Time range PSNRt SSIMt LPIPS| FID| perimage 16 27.51 0.76 0.09 22.58 ~—-75.39s 32 27.53 0.76 0.08 22.25 75.19s 48 27.52 0.76 0.08 22.06 75.21s More Ablation Study In this section, we present extensive ablation studies to conduct an in-depth analysis of our proposed model. Our analysis is divided into two primary components: the first component focuses specifically on our Fast Seamless Tiled Diffusion (FSTD) technique, while the second component encompasses a comprehensive evaluation of the entire pipeline, which we denote as Any Size Diffusion (ASD). Impact of the random offset range in FSTD. In Table 6, we present a detailed analysis, revealing that the range of the random offset has minimal influence on the super-resolution performance of images within the MM-CelebA-HQ dataset. Specifically, with offset ranges of 16, 32, and 48, the PSNR scores exhibit remarkable consistency, recording values of 27.51, 27.53, and 27.52, respectively. Furthermore, the inference times across these distinct offset ranges remain similarly uniform. This observation underscores the robustness of our approach, as it performs consistently well under varying offset parameters, thereby demonstrating its resilience to changes in this aspect of the configuration. Table 7: Performance comparison across various numbers of explicit tiles. We conduct ablation on the MMCelebA-HD benchmark by upscaling the low-resolution images in 256 x 256 to 1024x1024. The number of tiles in total with respect to overlap could be calculated by Eq 4. Method MM-CelebA-HQ Time Overlap Nites PSNR} SSIM?-LPIPS| FID) _ per image wio 16? 26.89 0.76 0.09 22.80 75.1s 16 212 27.50(+0.61) 0.76 0.09 23.21 148.32 32? 27.49 0.76 0.09 24.15 166.48 64 27.64 0.76 0.09 24.25 182.6s (+ 33.9) --- --—- SR-Plus —®— Ours ASD~ 31.692 OOM g* 31.3 30.802 | 31.154 | 7" 5SFad i} | oe S> 10.1S) 8 | -10.512? 1024" 2048 4096? 8192? 163842?Generated Image Resolutions (Pixels) Figure 7: Comparison of SR-Plus and ours ASD in terms of GPU memory cost (G) vs. image resolution (pixels). SR-Plus is the original SD 2.1 model used to generate images of varying sizes. OOM is the abbreviation of an out-ofmemory error. Experiments are conducted on a 32G GPU. Influence on the number of tiles or overlap region in explicit overlap tiled sampling. Table 7 illustrates the tradeoff between perceptual quality and computational efficiency in relation to tile overlap in image super-resolution. Notably, as the number of tiles increases, there is a corresponding improvement in perceptual metrics, although this comes at the cost of increased computational time. For instance, tiles with a 16-pixel overlap exhibit superior perceptual metrics compared to non-overlap tiles, yielding a notable improvement of 0.63 in PSNR score. However, this enhancement comes with a substantial increase in inference time, which increases from 75.1s to 148.7s. Further, compared to the results presented in the second row of the table, a tile overlap of 48 pixels yields a PSNR score improvement from 27.50 to 27.64, while incurring an additional inference time of 33.9s. Performance on the generation of images with different resolutions. Fig. 7 highlights the comparative efficiency of our proposed Any Size Diffusion (ASD) method relative to the baseline SR-Plus model, operating under a 32G GPU memory constraint. Specifically, in the context of generating images with dimensions of 1024? and 20487, our ASD algorithm consistently exhibits more efficient GPU memory usage than the SR-Plus model. For instance, when generating images of size 10247, ASD consumes 13.44G of memory compared to SR-Plus’s 17.45G; for 2048? images, the consumption is 27.73G for ASD versus 31.69G for SR-Plus. Importantly, while the SR-Plus model is constrained to a maximum resolution of 2048?—beyond which it exceeds available GPU memory—our ASD method is developed to accommodate image resolutions of up to 184327. This represents a significant 9x increase over the SR-Plus model’s maximum capacity. Emma Watson as a powerful mysterious sorceress, casting lightning magic, detailed clothing (a) SR-Plus Figure 8: Visual comparison: SR-Plus (original SD 2.1) vs. AR-Plus (fine-tuned SD 2.1 with multi-aspect ratio training). The yellow box means poor composition. (b) AR-Plus Achievement of 4K and 8K image-resolutions. Fig.demonstrates the enhanced high-resolution image generation capabilities of our ASD method by contrasting it with SR-Plus, the original SD 2.1 model (Rombach et al. 2022). SR-Plus degrades in composition for resolutions exceeding 512 x 512 pixels. In comparison, AR-Plus, developed through multi-aspect ratio training of SD 2.1, addresses this degradation but is bounded to a 2048 x 2048 pixel output under a 32GB GPU constraint. A non-overlapping tiled algorithm improves this limitation but introduces seaming artifacts, as shown in Fig. 9(a). Our solution, implementing implicit overlap in tiled sampling, resolves this seaming issue, with results depicted in Fig. 9(b). Thus, our ASD method effectively generates high-quality 4K and 8K images. References Bar-Tal, O.; Yariv, L.; Lipman, Y.; and Dekel, T. 2023. MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation. In JCML. Cherti, M.; Beaumont, R.; Wightman, R.; Wortsman, M.; Ilharco, G.; Gordon, C.; Schuhmann, C.; Schmidt, L.; and Jitsev, J. 2023. Reproducible Scaling Laws for Contrastive Language-Image Learning. In CVPR, 2818-2829. Dhariwal, P.; and Nichol, A. 2021. Diffusion Models Beat Gans on Image Synthesis. N/JPS, 34: 8780-8794. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; --- --Emma Watson as a powerful mysterious sorceress, casting lightning magic, detailed clothing oe (a) 2048 x 4096 (b) 2048 x 4096 (a) 2048x2048 —(b) 2048 xFigure 9: Comparative visualization of tiled diffusion techniques: (a) AR-Tile without overlaps vs. (b) our ASD with implicit overlaps. Our ASD method enables the generation of 4K and 8K images while effectively avoiding seam issues. --- --Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Generative Adversarial Nets. In NJPS, volume 27. Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S. 2017. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. N/PS, 30. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising Diffusion Probabilistic Models. NIPS, 33: 6840-6851. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; and Chen, W. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685. Kingma, D. P.; and Ba, J. 2014. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv: 1412.6980. Li, R.; Zhou, Q.; Guo, S.; Zhang, J.; Guo, J.; Jiang, X.; Shen, Y.; and Han, Z. 2023. Dissecting Arbitrary-scale Superresolution Capability from Pre-trained Diffusion Generative Models. arXiv preprint arXiv:2306.00714. Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollar, P.; and Zitnick, C. L. 2014. Microsoft COCO: Common Objects in Context. In ECCV, 740-755. Ma, Y.; Yang, H.; Yang, W.; Fu, J.; and Liu, J. 2023. Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. arXiv. Meng, C.; Rombach, R.; Gao, R.; Kingma, D.; Ermon, S.; Ho, J.; and Salimans, T. 2023. On Distillation of Guided Diffusion Models. In CVPR, 14297-14306. Nichol, A. Q.; and Dhariwal, P. 2021. Improved Denoising Diffusion Probabilistic models. In JCML, 8162-8171. Nichol, A. Q.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin, P.; Mcgrew, B.; Sutskever, I.; and Chen, M. 2022. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In JCML, 16784— 16804. Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; et al. 2019. Pytorch: An imperative style, high-performance deep learning library. NIPS, 32. Radford, A.; Wook Kim, J.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; Krueger, G.; and Sutskever, I. 2021. Learning Transferable Visual Models From Natural Language Supervision. In ICML, 8821-8831. Ramesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M. 2022. Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv preprint arXiv:2204.06125. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution Image Synthesis with Latent Diffusion Models. In CVPR, 10684-10695. Ruan, L.; Ma, Y.; Yang, H.; He, H.; Liu, B.; Fu, J.; Yuan, N. J.; Jin, Q.; and Guo, B. 2023. Mm-diffusion: Learning Multi-modal Diffusion Models for Joint Audio and Video Generation. In CVPR, 10219-10228. Ruiz, N.; Li, Y.; Jampani, V.; Pritch, Y.; Rubinstein, M.; and Aberman, K. 2023. DreamBooth: Fine Tuning Textto-Image Diffusion Models for Subject-Driven Generation. In CVPR, 22500-22510. Sahak, H.; Watson, D.; Saharia, C.; and Fleet, D. 2023. Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild. arXiv preprint arXiv:2302.07864. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan, B.; Salimans, T.; et al. 2022. Photorealistic Text-toImage Diffusion Models with Deep Language Understanding. NIPS, 35: 36479-36494. Saharia, C.; Ho, J.; Chan, W.; Salimans, T.; Fleet, D. J.; and Norouzi, M. 2023. Image Super-Resolution via Iterative Refinement. TPAMI, 45(4): 4713-4726. Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Radford, A.; and Chen, X. 2016. Improved Techniques for Training GANs. NIPS, 29. Schuhmann, C. 2022. LAION-AESTHETICS. https://laion. ai/blog/laion-aesthetics/. Accessed: 2022-8-16. Schuhmann, C.; Kopf, A.; Vencu, R.; Coombes, T.; and Beaumont, R. 2022. LAION COCO: 600M Synthetic Captions from LAION2B-EN. https://laion.ai/blog/laion-coco/. Accessed: 2022-9-15. Song, J.; Meng, C.; and Ermon, S. 2020. Denoising Diffusion Implicit Models. In JCLR. Song, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Ermon, S.; and Poole, B. 2021. Score-Based Generative Modeling through Stochastic Differential Equations. In JCLR. Wang, J.; Yue, Z.; Zhou, S.; Chan, K. C.; and Loy, C. C. 2023. Exploiting Diffusion Prior for Real-World Image Super-Resolution. arXiv preprint arXiv:2305.07015. Wang, Z.; Bovik, A.; Sheikh, H.; and Simoncelli, E. 2004. Image Quality Assessment: from Error Visibility to Structural Similarity. TIP, 13(4): 600-612. Xia, W.; Yang, Y.; Xue, J.-H.; and Wu, B. 2021. TediGAN: Text-Guided Diverse Face Image Generation and Manipulation. In CVPR, 2256-2265. Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR, 586-595. Alvaro Barbero Jiménez. 2023. Mixture of Diffusers for Scene Composition and High Resolution Image Generation. arXiv preprint arXiv:2302.02412.
