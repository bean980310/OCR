--- ABSTRACT ---
As recent advances in Neural Radiance Fields (NeRF) have enabled high-fidelity 3D face reconstruction and novel view synthesis, its manipulation also became an essential task in 3D vision. However, existing manipulation methods require extensive human labor, such as a user-provided semantic mask and manual attribute search unsuitable for non-expert users. Instead, our approach is designed to require a single text to manipulate a face reconstructed with NeRF. To do so, we first train a scene manipulator, a latent code-conditional deformable NeRF, over a dynamic scene to control a face deformation using the latent code. However, representing a scene deformation with a single latent code is unfavorable for compositing local deformations observed in different instances. As so, our proposed Positionconditional Anchor Compositor (PAC) learns to represent a manipulated scene with spatially varying latent codes. Their renderings with the scene manipulator are then optimized to yield high cosine similarity to a target text in CLIP embedding space for text-driven manipulation. To the best of our knowledge, our approach is the first to address the text-driven manipulation of a face reconstructed with NeRF. Extensive results, comparisons, and ablation studies demonstrate the effectiveness of our approach. 1.
--- INTRODUCTION ---
Easy manipulation of 3D face representation is an essential aspect of advancements in 3D digital human contents[32]. Though Neural Radiance Field[20] (NeRF) made a big step forward in a 3D scene reconstruction, many of its manipulative methods targets color[4, 34] or rigid ge --- --ometry [45, 15, 41, 14] manipulations, which are inappropriate for detailed facial expression editing tasks. While a recent work proposed a regionally controllable face editing method [13], it requires an exhaustive process of collecting user-annotated masks of face parts from curated training frames, followed by manual attribute control to achieve a desired manipulation. Face-specific implicit representation methods [6, 47] utilize parameters of morphable face models [36] as priors to encode observed facial expressions with high fidelity. However, their manipulations are not only done manually but also require extensive training sets of approximately 6000 frames that cover various facial expressions, which are laborious in both data collection and manipulation phases. On the contrary, our approach only uses a single text to conduct facial manipulations in NeRF, and trains over a dynamic portrait video with approximately 300 training frames that include a few types of facial deformation examples as in Fig. la. In order to control a face deformation, our method first learns and separates observed deformations from a canonical space leveraging HyperNeRF[23]. Specifically, perframe deformation latent codes and a shared latent codeconditional implicit scene network are trained over the training frames. Our key insight is to represent the deformations of a scene with multiple, spatially-varying latent codes for manipulation tasks. The insight originates from the shortcomings of naively adopting the formulations of HyperNeRF to manipulation tasks, which is to search for a single latent code that represents a desired face deformation. For instance, a facial expression that requires a combination of local deformations observed in different instances is not expressible with a single latent code. In this work, we define such a problem as “linked local attribute problem” and address this issue by representing a manipulated scene with spatially varying latent codes. As a result, our manipulation could express a combination of locally observed deformations as seen from the image rendering highlighted with red boundary in Fig. 2a. To this end, we first summarize all observed deformations as a set of anchor codes and let MLP learn to compose the anchor codes to yield multiple, position-conditional latent codes. The reflectivity of the latent codes on visual attributes of a target text is then achieved by optimizing the rendered images of the latent codes to be close to a target text in CLIP[27] embedding space. In summary, our work makes the following contributions: ¢ Proposal of a text-driven manipulation pipeline of a face reconstructed with NeRF. ¢ Design of a manipulation network that learns to represent a scene with spatially varying latent codes. ¢ First to conduct text-driven manipulation of a face reconstructed with NeRF to the best of our knowledge. WHEE Interpolative slicing volume °———* Slicing surfaces from learned codes Interpolative slicing surfaces Non-interpolative slicing surface w (hyper space) “wy = W(x) X (eyes) yay t (b) (c) Figure 2: (a) Illustration of linked local attribute problem in hyper space. Expressing scene deformation with perscene latent code cannot compose local facial deformation observed in different instances. (b) Types of facial deformations observed during scene manipulator training. (c) Renderings of interpolated latent codes with a scene manipulator. 2.
--- RELATED WORK ---
s NeRF and Deformable NeRF Given multiple images taken from different views of a target scene, NeRF[20] synthesizes realistic novel view images with high fidelity by using an implicit volumetric scene function and volumetric rendering scheme[ 12], which inspired many follow-ups [1, 35, 19, 37, 44]. As NeRF assumes a static scene, recent works [22, 23, 26, 16] propose
--- METHOD ---
s require extensive human labor, such as a user-provided semantic mask and manual attribute search unsuitable for non-expert users. Instead, our approach is designed to require a single text to manipulate a face reconstructed with NeRF. To do so, we first train a scene manipulator, a latent code-conditional deformable NeRF, over a dynamic scene to control a face deformation using the latent code. However, representing a scene deformation with a single latent code is unfavorable for compositing local deformations observed in different instances. As so, our proposed Positionconditional Anchor Compositor (PAC) learns to represent a manipulated scene with spatially varying latent codes. Their renderings with the scene manipulator are then optimized to yield high cosine similarity to a target text in CLIP embedding space for text-driven manipulation. To the best of our knowledge, our approach is the first to address the text-driven manipulation of a face reconstructed with NeRF. Extensive results, comparisons, and ablation studies demonstrate the effectiveness of our approach. 1. Introduction Easy manipulation of 3D face representation is an essential aspect of advancements in 3D digital human contents[32]. Though Neural Radiance Field[20] (NeRF) made a big step forward in a 3D scene reconstruction, many of its manipulative methods targets color[4, 34] or rigid ge --- --ometry [45, 15, 41, 14] manipulations, which are inappropriate for detailed facial expression editing tasks. While a recent work proposed a regionally controllable face editing method [13], it requires an exhaustive process of collecting user-annotated masks of face parts from curated training frames, followed by manual attribute control to achieve a desired manipulation. Face-specific implicit representation methods [6, 47] utilize parameters of morphable face models [36] as priors to encode observed facial expressions with high fidelity. However, their manipulations are not only done manually but also require extensive training sets of approximately 6000 frames that cover various facial expressions, which are laborious in both data collection and manipulation phases. On the contrary, our approach only uses a single text to conduct facial manipulations in NeRF, and trains over a dynamic portrait video with approximately 300 training frames that include a few types of facial deformation examples as in Fig. la. In order to control a face deformation, our method first learns and separates observed deformations from a canonical space leveraging HyperNeRF[23]. Specifically, perframe deformation latent codes and a shared latent codeconditional implicit scene network are trained over the training frames. Our key insight is to represent the deformations of a scene with multiple, spatially-varying latent codes for manipulation tasks. The insight originates from the shortcomings of naively adopting the formulations of HyperNeRF to manipulation tasks, which is to search for a single latent code that represents a desired face deformation. For instance, a facial expression that requires a combination of local deformations observed in different instances is not expressible with a single latent code. In this work, we define such a problem as “linked local attribute problem” and address this issue by representing a manipulated scene with spatially varying latent codes. As a result, our manipulation could express a combination of locally observed deformations as seen from the image rendering highlighted with red boundary in Fig. 2a. To this end, we first summarize all observed deformations as a set of anchor codes and let MLP learn to compose the anchor codes to yield multiple, position-conditional latent codes. The reflectivity of the latent codes on visual attributes of a target text is then achieved by optimizing the rendered images of the latent codes to be close to a target text in CLIP[27] embedding space. In summary, our work makes the following contributions: ¢ Proposal of a text-driven manipulation pipeline of a face reconstructed with NeRF. ¢ Design of a manipulation network that learns to represent a scene with spatially varying latent codes. ¢ First to conduct text-driven manipulation of a face reconstructed with NeRF to the best of our knowledge. WHEE Interpolative slicing volume °———* Slicing surfaces from learned codes Interpolative slicing surfaces Non-interpolative slicing surface w (hyper space) “wy = W(x) X (eyes) yay t (b) (c) Figure 2: (a) Illustration of linked local attribute problem in hyper space. Expressing scene deformation with perscene latent code cannot compose local facial deformation observed in different instances. (b) Types of facial deformations observed during scene manipulator training. (c) Renderings of interpolated latent codes with a scene manipulator. 2. Related Works NeRF and Deformable NeRF Given multiple images taken from different views of a target scene, NeRF[20] synthesizes realistic novel view images with high fidelity by using an implicit volumetric scene function and volumetric rendering scheme[ 12], which inspired many follow-ups [1, 35, 19, 37, 44]. As NeRF assumes a static scene, recent works [22, 23, 26, 16] propose methods to encode dynamic scenes of interest. The common scheme of the works is to train a latent code per training frame and a single latentconditional NeRF model shared by all trained latent codes to handle scene deformations. Our work builds on this design choice to learn and separate the observed deformations from a canonical space, yet overcome its limitation during the manipulation stage by representing a manipulated scene with spatially varying latent codes. --- --Text-driven 3D Generation and Manipulation Many works have used text for images or 3D manipulation[38, 9, 25, 11, 29, 10]. CLIP-NeRF[38] proposed a disentangled conditional NeRF architecture in a generative formulation supervised by text embedding in CLIP[27] space, and conducted text-and-exemplar driven editing over shape and appearance of an object. Dreamfields [9] performed generative text-to-3D synthesis by supervising its generations in CLIP embedding space to a generation text. We extend from these lines of research to initiate CLIP-driven manipulation of face reconstructed with NeRF. NeRF Manipulations Among many works that studied NeRF manipulations[18, 45, 36, 13, 34, 33, 7, 48, 15], EditNeRF[18] train conditional NeRF on a shape category to learn implicit semantics of the shape parts without explicit supervision. Then, its manipulation process propagates user-provided scribbles to appropriate object regions for editing. NeRF-Editing[45] extracts mesh from trained NeRF and lets the user perform the mesh deformation. A novel view of the edited scene can be synthesized without re-training the network by bending corresponding rays. CoNeRF[ 13] trains controllable neural radiance fields using user-provided mask annotations of facial regions so that the user can control desired attributes within the region. However, such methods require laborious annotations and manual editing processes, whereas our method requires only a single text for detailed manipulation of faces. Neural Face Models Several works[42, 28, 47] built 3D facial models using neural implicit shape representation. Of the works, i3DMM[42] disentangles face identity, hairstyle, and expression, making decoupled components to be manually editable. Face representation works based on NeRF have also been exploited[39, 36, 47]. Wang et al.[39] proposed compositional 3D representation for photo-realistic rendering of a human face, yet requires guidance images to extract implicitly controllable codes for facial expression manipulation. NerFACE[36] and IMavatar[47] model the appearance and dynamics of a human face using learned 3D Morphable Model[2] parameters as priors to achieve controllability over pose and expressions. However, the methods require a large number of training frames that cover many facial expression examples and manual adjustment of the priors for manipulation tasks. 3. Preliminaries 3.1. NeRF NeRF [20] is an implicit representation of geometry and color of a space using MLP. Specifically, given a point coordinate x = (x,y,z) and a viewing direction d, an MLP function F is trained to yield density and color of the point as (c,0) = F(x,d). M number of points are sampled along aray r = 0 + td using distances, {t;}“o, that are collected from stratified sampling method. F' predicts color and density of each point, all of which are then rendered to predict pixel color of the ray from which it was originated as M C(x) = SOT, = exp(—o14;))ei, 0) i=where 6; = tiz1 — ti, and T; = exp(— mn o;6;) is an accumulated transmittance. F is then trained to minimize the rendering loss supervised with correspondingly known pixel colors. 3.2. HyperNeRF Unlike NeRF that is designed for a static scene, HyperNeRF [23] is able to encode highly dynamic scenes with large topological variations. Its key idea is to project points to canonical hyperspace for interpretation. Specifically, given a latent code w, a spatial deformation field T’ maps a point to a canonical space, and a slicing surface field H determines the interpretation of the point for a template NeRF F. Specifically, x’ = T(x,w), (2) w= H(x,w), (3) (c,0) = F(x’, w,d), (4) where w < wy, € {wi---wn} = W is a trainable perframe latent code that corresponds to each N number of training frames. Then, the rendering loss is finally defined as |Cn(e") — Calt")|I3; (5) L= n€{1---N}, rmER" where C;,(r”) is ground truth color at n-th training frame of aray r” and R” is a set of rays from n-th camera. Note that (x’, w) and H(x, w) are often referred to canonical hyperspace and slicing surface, since x’ can be interpreted differently for different w as illustrated in Fig. 2a. 4. Proposed Method We aim to manipulate a face reconstructed with NeRF given a target text that represents a desired facial expressions for manipulation (e.g., “crying face”, “wink eyes and smiling mouth’). To this end, our proposed method first trains a scene manipulator, a latent code-conditional neural field that controls facial deformations using its latent code (§4.1). Then, we elaborate over the pipeline to utilize a target text for manipulation (§4.2), followed by proposing an MLP network that learns to appropriately use the learned deformations and the scene manipulator to render scenes with faces that reflect the attributes of target texts (§4.3). --- --(Gp trained & fixed during manipulation (GG) train for manipulation r ‘|| sien d 4 F Lui w few lh Ambient Template Slicing Surface NeRF (a) Scene Manipulator Figure 3: (a) Network structure of scene manipulator G. conditional Anchor Compositor (PAC) for manipulation. 4.1. Scene Manipulator First, we construct a scene manipulator using HyperNeRF[23] so that deformations of a scene can be controlled by fixing the parameters of the scene manipulator and manipulating its latent code. Specifically, we train a dynamic scene of interest with a network formulated as Eq.(4) following [23], after which we freeze the trained parameters of T, H, F, and W and use w as a manipulation handle. In addition, we empirically found that the deformation network T tends to learn rigid deformations, such as head pose, while slicing surface field H learns non-rigid and detailed deformations, such as shapes of mouth and eyes. As so, we select and fix a trained latent code for T and only manipulate a latent code fed to H. In summary, as illustrated in Fig. 3(a), our latent code-conditional scene manipulator G is defined as G(x, d,w) := where * represents that the parameters are trained and fixed for manipulation, and we is a fixed latent code of the desired head pose chosen from a set of learned latent codes W. In the supplementary material, we report further
--- EXPERIMENT ---
al results and discussions over head pose controllability of Wr. F(T(x,0p),H(x,w).d), 6) Lipschitz MLP _ Since G is only trained to be conditioned over a limited set of trainable latent codes W, a subspace of w outside the learned latent codes that yields plausible deformations needs to be formulated to maximize the expressibility of G for manipulation. Meanwhile, HyperNeRF was shown to moderately render images from latent codes linearly interpolated from two learned latent codes. Thus, a valid latent subspace W can be formulated to include not only the learned latent codes but codes linearly interpolated between any two learned latent codes as well. Specifically, Wd {y*Wt+(L—7)*0;| W,0; EW, o<y<y. However, we learned that the fidelity of images from (b) Vanilla Inversion Anchor: nats Network (c) Position-conditional Anchor Compositor (PAC) (b) Vanilla inversion method for manipulation. (c) Position interpolated latent codes needs to be higher to be leveraged for manipulation. As so, we regularize the MLPs of the scene manipulator to be more Lipschitz continuous during its training phase. Note that Lipschitz bound of a neural network with LZ number of layers and piecewise linear functions such as ReLU can be approximated asc = [[/., |W" ||, [17, 43], where W’ is an MLP weight at i-th layer. Since a function f that is c-Lipschitz has the property f(r) successful regularization of c would make smaller differences between outputs of adjacent latent codes, which induce interpolated deformations to be more visually natural. As so, we follow [17] and regularize trainable matrix at /-th layer of F by introducing extra trainable parameters c! as — f(we)|lp < ellwr — wally, (8) softplus(c' =o(Wae +8), Wi = Wh -min(1, () (9) [Wi lloe where wi is the j-th row of a trainable matrix at /-th layer W!, and || - ||. is matrix oo-norm. Trinable Lipschitz constants from the layers are then minimized via gradient-based optimization with loss function defined as L Liip = Il softplus(c'). (10) l=In summary, networks in Eq. (4) are trained to retrieve F, T, H, and W using our scene manipulator objective function Lsu = AcLe + MipLuip, (11) where A, and Aj;p are hyper-parameters. 4.2. Text-driven Manipulation Given a trained scene manipulator G, one manipulation method is to find a single optimal latent code w whose rendered image using G' yields the highest cosine similarity with a target text in CLIP[27] embedding space, so that the --- --manipulated images can reflect the visual attributes of a target text. Specifically, given images rendered with G' and w at a set of valid camera poses [R|t] as Tin and a target text for manipulation p, the goal of the method is to solve the following problem: w* = arg max Deuw(ZipinsP): (12) w where Dc,jp measures the cosine similarity of features between rendered images and a target text extracted from pretrained CLIP model. As illustrated in Fig. 3b, a straightforward vanilla approach to find an optimal latent embedding w* is inversion, a gradient-based optimization of w that maximizes Eq.(12) by defining a loss function as Lotrp = 1—Deur (Livin ;P). However, we show that this method is sub-optimal by showing that it inevitably suffers from what we define as a linked local attributes problem, which we then solve with our proposed method. Linked local attribute problem Solutions from the vanilla inversion method are confined to represent deformations equivalent to those from W. However, W cannot represent all possible combinations of locally observed deformations, as interpolations between two learned latent codes, which essentially comprise WV, cause facial attributes in different locations to change simultaneously. For example, consider a scene with deformations in Fig. 2b and renderings of interpolations between two learned latent codes in Fig. 2c. Not surprisingly, neither the learned latent codes nor the interpolated codes can express opened eyes with opened mouth or closed eyes with a closed mouth. Similar experiments can be done with any pair of learned latent codes and their interpolations to make the same
--- CONCLUSION ---
. We may approach this problem from the slicing surface perspective of canonical hyperspace introduced in Sec. 3.2. As in Fig. 2a, hyperspace allows only one latent code to represent an instance of a slicing surface representing a global deformation of all spatial locations. Such representation causes a change in one type of deformation in one location to entail the same degree of change to another type of deformation in different locations during interpolation. Our method is motivated by the observation and is therefore designed to allow different position x to be expressed with different latent codes to solve the linked local attribute problem. 4.3. Position-conditional Anchor Compositor For that matter, Position-conditional Anchor Compositor (PAC) is proposed to grant our manipulation pipeline the freedom to learn appropriate latent codes for different spatial positions. Specifically, we define anchor codes {wy!,--- wf} = W* CW, asubset of learned latent codes where each rep Interpolative, valid region Extrapolative, unseen region Figure 4: Illustration of barycentric interpolation of latent codes for validly expressive regions when K = 3. resent different types of observed facial deformations, to set up a validly explorable latent space as a prior. We retrieve anchor codes by extracting facial expression parameters using DECA[5] from images rendered from all codes in W over a fixed camera pose. Then, we cluster the extracted expression parameters using DBSCAN[3] and select the latent code corresponding to the expression parameter closest to the mean for each cluster. For instance, we may get k =anchor codes in the case of the example scenes in Fig. la and Fig. 2b. Then for every spatial location, a position-conditional MLP yields appropriate latent codes by learning to compose these anchor codes. By doing so, a manipulated scene can be implicitly represented with multiple, point-wise latent codes. Specifically, the anchor composition network P : RG@+4w) _; R! learns to yield w* for every spatial position x via barycentric interpolation[8] of anchors as Ayn = PRO WE), wE= lolx we, (13) k where d,, is the dimension of a latent code, @ is concatenation, and o;, is softmax activation along k network outputs. Also, denote ajx,4) = %(Gjx,4]) aS anchor composition ratio (ACR) for ease of notation. As in the illustrative example in Fig. 4, the key of the design is to prevent the composited code from diverging to extrapolative region of the latent. Thus, barycentric interpolation defines a safe bound of composited latent code for visually natural renderings. Finally, a set of points that are sampled from rays projected at valid camera poses and their corresponding set of latent codes [w] are queried by G, whose outputs are rendered as images to be supervised in CLIP embedding space for manipulation as Loire =1- Dow(Zigin* sD); (14) Total variation loss on anchor composition ratio As, the point-wise expressibility of PAC allows adjacent latent codes to vary without mutual constraints, P is regularized with total variation (TV) loss. Smoother ACR fields allows similar latent embeddings to cover certain facial po --- --“closed eyes and smiling mouth ” ‘wink eyes and smiling mouth” Reference frowning eyes and pursed lips” ‘wink eyes and widely opened mou “frowning eyes and “closed, frowning eyes and “wink, frowning eyes and closed mouth ” closed mouth” closed mouth” Figure 5: Qualitative results manipulated with descriptive texts using our method. Local facial deformations can easily be controlled using texts only. sitions to yield more naturally rendered images. Specifically, Qx,4] is rendered to valid camera planes using the rendering equation in Eq. (1) for regularization. Given a ray ¥,,,(t) = 0 + td,,,, ACR can be rendered for each anchor & at an image pixel located at (u, v) of a camera plane, and regularized with TV loss as M Qu = ere — exp(—010i) Of, (t),k]> (15) i=l Lacr= Ss l@x(utajy — Akuvll2 + |]@ru(v4a) — Geuvll2 k,uyv (16) In summary, text-driven manipulation is conducted by optimizing P and minimizing the following loss Leait = AcLiPLciip + AACRLACR a7) where Ac rp and A4cr are hyper-parameters. 5. Experiments Dataset We collected portrait videos from six volunteers using Apple iPhone 13, where each volunteer was asked to make four types of facial deformations shown in Fig. la and Fig. 2b. A pre-trained human segmentation network was used to exclude descriptors from the dynamic part of the scenes during camera pose computation using COLMAP[31]. Examples of facial deformations observed “closed eyes and opened mouth”) “wink, frowning] eyes and closed mouth” “surprised ¥ face” “happy face” Reference NeRF+FT — Nerfies+/ HyperNeRF+/ Ours Figure 6: Text-driven manipulation results of our method and the baselines. Our result well reflects the implicit attributes of target emotional texts while preserving visual quality and face identity. during training for each scene are reported in the supplementary material. Manipulation Texts We selected two types of texts for manipulation experiments. First is a descriptive text that characterizes deformations of each facial parts. Second is an emotional expression text, which is an implicit representation of a set of multiple local deformations on all face parts hard to be described with descriptive texts. We selected 7 frequently used and distinguishable emotional expression texts for our experiment: “crying”, ”disappointed”, ’surprised”, “happy”, ”angry”, ”scared” and ”sleeping”. To reduce text embedding noise, we followed [24] by averaging augmented embeddings of sentences with identical meanings. Baselines Since there is no prior work that is parallel to our problem definition, we formulated 3 baselines with existing state-of-the-art methods for comparisons: (1) NeRF +FT is a simple extension from NeRF [20] that fine-tunes the whole network using CLIP loss, (2) Nerfies+/ uses Nerfies[22] as a deformation network followed by conducting vanilla inversion method introduced in Sec. §4.2 for manipulation, and (3) HyperNeRF+/ replaces Nerfies in (2) with HyperNeRF [23]. Text-driven Manipulation We report qualitative manipulation results of our methods driven with a set of descriptive sentences in Fig. 5. Our method not only faithfully reflects the descriptions, but also can easily control local facial deformations with simple change of words in sentences. We also report manipulated results driven by emotional ex --- --Reference “angry” “crying” “surprised” “disappointed” “happy” “scared” “sleeping” Figure 7: Extensive face manipulation results driven by a set of frequently used emotional expression texts using our method. Manipulating to emotional expression texts are challenging, as they implicitly require compositions of subtle facial deformations that are hard to be described. Our method reasonably reflects the attributes of the manipulation texts. pression texts in Fig. 7. As can be seen, our method conducts successful manipulations even if the emotional texts are implicit representations of many local facial deformations. For instance, result manipulated with ’crying” in first row of Fig. 7 is not expressed with mere crying-looking eyes and mouth, but also includes crying-looking eyebrows and skin all over the face without any explicit supervision on local deformations. We also compare our qualitative results to those from the baselines in Fig. 6. Ours result in the highest reflections of the target text attributes. NeRF+FT shows significant degradation in visual quality, while Nerfies+/ moderately suffers from low reconstruction quality and reflection of target text attributes. HyperNeRF+ J shows the highest visual quality out of all baselines, yet fails to reflect the visual attributes of target texts. High reflectivity on various manipulation texts can be attributed to PAC that resolves the linked local attribute problem. In Fig. 8, we visualize @.., for each anchor code k, which is the rendering of ACR ay,,4) in Eq. (15), over an image plane. Whiter regions of the renderings are closer to one, which indicates that the corresponding anchor code is mostly composited to yield the latent code of the region. Also, we display image renderings from each anchor code on the left to help understand the local attributes for each anchor code. As can be seen, PAC composes appropriate anchor codes for different positions. For example, when manipulating for sleeping face, PAC reflects closed eyes R-Prec.[40] LPIPS[46] | CFS ¢ NeRF + FT 0.763 0.350 0.Nerfies + / 0.213 0.222 0.HyperNeRF +1 0.342 0.198 0.Ours 0.780 (+0.017) 0.082 (-0.116) 0.749 (+0.028) Table 1: Quantitative results. R-Prec. denotes R-precision, and CFS denotes cosine face similarity. We notate performance ranks as best and second best. TRt VRt FP + NeRF + FT 2.85 0.18 0.Nerfies + J 0.33 3.61 4.HyperNeRF + / 2.52 4.42 4.Ours 4.15 (+1.30) 4.58 (+0.16) 4.67 (+0.28) Table 2: User study results. TR, VR, and FP denote text reflectivity, visual realism, and face identity preservability, respectively. Best and second best are highlighted. from one anchor code and neutral mouth from other anchor codes. In the cases of crying, angry, scared, and disappointed face, PAC learns to produce complicated compositions of learned deformations, which are inexpressible with a single latent code. Quantitative Results First of all, we measured Rprecision[40] to measure the text attribute reflectivity of --- --Manipulated Results ‘aisanp “happy” — “angry” — “surprised” —“erving sleeping” —“scared’ Image Renderings by Anchor Codes sopoz z0ysuy <q sdeyl YOV Paropudy, Figure 8: Renderings of learned ACR maps for each anchor codes over different manipulation texts. the manipulations. We used facial expression recognition model[30] pre-trained with AffectNet[21] for top-R retrievals of each text. Specifically, 1000 novel view images are rendered per face, where 200 images are rendered from a face manipulated with each of the five texts that are distinguishable and exist in AffectNet labels: “happy”, ”surprised”, fearful”, ’angry”, and ”sad”. Also, to estimate the visual quality after manipulation, we measured LPIPS[46] between faces with no facial expressions (neutral faces) without any manipulations and faces manipulated with 7 texts, each of which are rendered fromnovel views. Note that LPIPS was our best estimate of visual quality since there can be no pixel-wise ground truth of text-driven manipulations. Lastly, to measure how much of the facial identity is preserved after manipulation, we measured the cosine similarity between face identity features! extracted from neutral faces and text-manipulated faces, all of which are rendered from 200 novel views. Table | reports the average results over all texts, which shows that our method outperforms in all criteria. User Study Users were asked to score from 0 to 5 oncriteria; (i) Text Reflectivity: how well the manipulated renderings reflect the target texts, (ii) Visual Realism: how realistic do the manipulated images look, and (iii) Face identity Preservability: how well do the manipulated images preserve the identity of the original face, over our method and the baselines. The following results are reported in Table. 2. Our method outperforms all baselines, and espe 'https://github.com/ronghuaiyang/arcface-pytorch Figure 9: Renderings from linearly interpolated latent codes. Lipschitz-regularized scene manipulator interpolates unseen shapes more naturally. — 5 = Figure 10: (a) Qualitative results of the ablation study. Manipulations are done using ”crying face” as target text. (b) Rendered ACR maps with and without Lacr. cially in text reflectivity by a large margin. Note that the out-performance in user responses align with that from the quantitative results, which supports the consistency of evaluations. Interpolation We experiment with the effect of Lipschitz regularization on the scene manipulator by comparing the visual quality of images rendered from linearly interpolated latent codes, and report the results in Fig. 9. Lipschitzregularized scene manipulator yields more visually natural images, which implies that learned set of anchorcomposited latent codes [w%] are more likely to render realistically interpolated local deformations under Lipschitzregularized scene manipulator. Ablation Study We conducted an ablation study on our regularization methods: Lj, and Lace. As shown in Fig. 10a, manipulation without £;;, suffers from low visual quality. Manipulation without £4cr yields unnatural renderings of face parts with large deformation range such as mouth and eyebrows. This can be interpreted with learned ACR maps of PAC in Fig. 10b. ACR maps learned with Lacr introduces reasonable continuities of latent codes on boundaries of the dynamic face parts, thus yielding naturally interpolated face parts. --- --6. Conclusion We have presented FaceCLIPNeRF, a text-driven manipulation pipeline of a 3D face using deformable NeRF. We first proposed a Lipshitz-regularized scene manipulator, a conditional MLP that uses its latent code as a control handle of facial deformations. We addressed the linked local attribute problem of conventional deformable NeRFs, which cannot compose deformations observed in different instances. As so, we proposed PAC that learns to produce spatially-varying latent codes, whose renderings with the scene manipulator were trained to yield high cosine similarity with target text in CLIP embedding space. Our experiments showed that our method could faithfully reflect the visual attributes of both descriptive and emotional texts while preserving visual quality and identity of 3D face. Acknowledgement This material is based upon work supported by the Air Force Office of Scientific Research under award number FA2386-22-1-4024, KAIST-NAVER hypercreative AI center, and the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.20190-00075, Artificial Intelligence Graduate School Program (KAIST)). References {1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5855-5864, 2021.2] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In Proceedings of the 26th an nual conference on Computer graphics and interactive tech niques, pages 187-194, 1999.3] Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226-231, 1996.4] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Unified implicit neural styliza tion. arXiv preprint arXiv:2204.01943, 2022.5] Yao Feng, Haiwen Feng, Michael J. Black, and Timo Bolkart. Learning an animatable detailed 3D face model from in-the-wild images. volume 40, 2021.6] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias NieBner. Dynamic neural radiance fields for monocular 4d facial avatar reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8649-8658, 2021.7] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juyong Zhang. Headnerf: A real-time nerf-based parametric head model. In Proceedings of the IEEE/CVF Conference [[[18] [20] i) on Computer Vision and Pattern Recognition, pages 20374— 20384, 2022.Kai Hormann. Barycentric interpolation. In Approximation Theory XIV: San Antonio 2013, pages 197-218. Springer, 2014.Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 867-876, 2022.Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. 2022.Nikolay Jetchev. Clipmatrix: Text-controlled creation of 3d textured meshes. arXiv preprint arXiv:2109.12922, 2021.James T Kajiya and Brian P Von Herzen. Ray tracing volume densities. ACM SIGGRAPH computer graphics, 18(3):165174, 1984.Kacper Kania, Kwang Moo Yi, Marek Kowalski, Tomasz Trzcitiski, and Andrea Tagliasacchi. CoNeRF: Controllable Neural Radiance Fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. 2,Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing nerf for editing via feature field distillation. In Advances in Neural Information Processing Systems, volume 35, 2022.Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey Tulyakov, and Gerard Pons-Moll. Control-nerf: Editable feature volumes for scene rendering and manipulation. arXiv preprint arXiv:2204.10850, 2022. 2,Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5521-5531, 2022.Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, and Or Litany. Learning smooth neural functions via lipschitz regularization. arXiv preprint arXiv:2202.08345, 2022.Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5773-5783, 2021.Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul P Srinivasan, and Jonathan T Barron. Nerf in the dark: High dynamic range view synthesis from noisy raw images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16190-16199, 2022.Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. 1, 2, 3,Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor. Affectnet: A database for facial expression, valence, --- --[22] [23][28]and arousal computing in the wild. JEEE Transactions on Affective Computing, 10(1):18-31, 2017.Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5865-5874, 2021. 2,Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan B Goldman, Ricardo MartinBrualla, and Steven M. Seitz. Hypernerf: A higherdimensional representation for topologically varying neural radiance fields. ACM Trans. Graph., 40(6), dec 2021. 2, 3, 4,Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In International Conference of Computer Vision, pages 2085-2094, 2021.Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209. 14988, 2022.Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. arXiv preprint arXiv:2011.13961, 2020.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Jnternational Conference on Machine Learning, pages 8748-8763. PMLR, 2021. 2, 3,Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola, Jaime Garcia, Xavier Giro-i Nieto, and Francesc MorenoNoguer. H3d-net: Few-shot high-fidelity 3d head reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5620-5629, 2021.Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18603-18613, 2022.Andrey V Savchenko. Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices. arXiv preprint arXiv:2203. 13436, 2022.Johannes Lutz Schénberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise View Selection for Unstructured Multi-View Stereo. In European Conference on Computer Vision (ECCV), 2016.Sahil Sharma and Vijay Kumar. 3d face reconstruction in deep learning era: A survey. Archives of Computational Methods in Engineering, pages 1-33, 2022.Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue Wang, and Yebin Liu. Ide-3d: Interactive disentangled editing for high-resolution 3d-aware portrait synthesis. arXiv preprint arXiv:2205.15517, 2022.Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi Zhang, Yebin Liu, and Jue Wang. Fenerf: Face editing in neural radiance fields. In Proceedings of the IEEE/CVF Con [35][40] [42] [43] [44] [45] ference on Computer Vision and Pattern Recognition, pages 7672-1682, 2022. 1,Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2846-2855, 2021.Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias NieBner. Face2face: Real-time face capture and reenactment of rgb videos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2387-2395, 2016. 2,Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T. Barron, and Pratul P. Srinivasan. Ref-NeRF: Structured view-dependent appearance for neural radiance fields. CVPR, 2022.Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image driven manipulation of neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3835-3844, 2022.Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason Saragih, Jessica Hodgins, and Michael Zollhofer. Learning compositional radiance fields of dynamic human heads. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5704— 5713, 2021.Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Finegrained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1316— 1324, 2018.Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene rendering. In International Conference on Computer Vision (ICCV), October 2021.Tarun Yenamandra, Ayush Tewari, Florian Bernard, HansPeter Seidel, Mohamed Elgharib, Daniel Cremers, and Christian Theobalt. i3dmm: Deep implicit 3d morphable model of human heads. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12803-12813, 2021.Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability of deep learning. arXiv preprint arXiv:1705.10941, 2017.Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 57525761, 2021.Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, and Lin Gao. Nerf-editing: geometry editing of neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18353-18364, 2022. 2,--- ---Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586-595, 2018. 7,Yufeng Zheng, Victoria Fernandez Abrevaya, Marcel C Biihler, Xu Chen, Michael J Black, and Otmar Hilliges. Im avatar: Implicit morphable head avatars from videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13545-13555, 2022. 2,Yiyu Zhuang, Hao Zhu, Xusen Sun, and Xun Cao. Mofanerf: Morphable facial neural radiance field. arXiv preprint arXiv:2112.02308, 2021.
