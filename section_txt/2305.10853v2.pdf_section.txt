--- ABSTRACT ---
This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that generates both image and depth map data from a given text prompt, allowing users to generate RGBD images from text prompts. The LDM3D model is fine-tuned on a dataset of tuples containing an RGB image, depth map and caption, and validated through extensive experiments. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360°-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize content creation and digital experiences. A short video summarizing the approach can be found at https://t.ly/tdi2. 1.
--- INTRODUCTION ---
The field of computer vision has seen significant advancements in recent years, particularly in the area of generative AI. In the domain of image generation, Stable Diffusion has revolutionized content creation by providing open software to generate arbitrary high-fidelity RGB images from text prompts. This work builds on top of Stable Diffusion [20] v1.4 and proposes a Latent Diffusion Model for vasudev.lal@intel.com 3D (LDM3D). Unlike the original model, LDM3D is capable of generating both image and depth map data from a given text prompt as can be seen in Figure 1. It allows users to generate complete RGBD representations of text prompts, bringing them to life in vivid and immersive 360° views. Our LDM3D model was fine-tuned on a dataset of tuples containing an RGB image, depth map and caption. This dataset was constructed from a subset of the LAION-400M dataset, a large-scale image-caption dataset that contains over 400 million image-caption pairs. The depth maps used in fine-tuning were generated by the DPT-Large depth estimation model [18, 19], which provides highly accurate relative depth estimates for each pixel in an image. The use of accurate depth maps was crucial in ensuring that we are able to generate 360° views that are realistic and immersive, allowing users to experience their text prompts in vivid detail. To showcase the potential of LDM3D, we have developed DepthFusion, an application that uses the generated 2D RGB images and depth maps to compute a 360° projection using TouchDesigner [1]. TouchDesigner is a versatile platform that enables the creation of immersive and interactive multimedia experiences. Our application harnesses the power of TouchDesigner to create unique and engaging 360° views that bring text prompts to life in vivid detail. DepthFusion has the potential to revolutionize the way we experience digital content. Whether it’s a description of a tranquil forest, a bustling cityscape, or a futuristic sci-fi --- --Concat RGBD Diffusion U-Net Figure 1. LDM3D overview. Illustrating the training pipeline: the 16-bit grayscale depth maps are packed into 3-channel RGB-like depth images, which are then concatenated with the RGB images along the channel dimension. This concatenated RGBD input is passed through the modified KL-AE and mapped to the latent space. Noise is added to the latent representation, which is then iteratively denoised by the U-Net model. The text prompt is encoded using a frozen CLIP-text encoder and mapped to various layers of the U-Net using crossattention. The denoised output from the latent space is fed into the KL-decoder and mapped back to pixel space as a 6-channel RGBD output. Finally, the output is separated into an RGB image and a 16-bit grayscale depth map. Blue frame: text-to-image inference pipeline. Initiating from a Gaussian distributed noise sample in the 64x64x4-dimensional latent space. Given a text prompt, this pipeline generates an RGB image and its corresponding depth map. world, DepthFusion can generate immersive and engaging 360° views that allow users to experience their text prompts in a way that was previously impossible. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. In summary, our contributions are threefold. (1) We propose LDM3D, a novel diffusion model that outputs RGBD images (RGB images with corresponding depth maps) given a text prompt. (2) We develop DepthFusion, an application to create immersive 360°-view experiences based on RGBD images generated with LDM3D. (3) Through extensive experiments, we validate the quality of our generated RGBD images and 360°-view immersive videos. 2.
--- RELATED WORK ---
Monocular depth estimation is the task of estimating depth values for each pixel of a single given RGB image. Recent work has shown great performance in depth estimation using deep learning models based on convolutional neural networks [11, 12, 14,22, 28,29]. Later, attentionbased Transformer models were adopted to overcome the issue of a limited receptive field in CNNs, allowing the model to consider global contexts when predicting depth values [3,5, 19,30]. Most recently diffusion models have also been applied to depth estimation to leverage the revolutionary generation capabilities of such
--- EXPERIMENT ---
s. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360°-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize content creation and digital experiences. A short video summarizing the approach can be found at https://t.ly/tdi2. 1. Introduction The field of computer vision has seen significant advancements in recent years, particularly in the area of generative AI. In the domain of image generation, Stable Diffusion has revolutionized content creation by providing open software to generate arbitrary high-fidelity RGB images from text prompts. This work builds on top of Stable Diffusion [20] v1.4 and proposes a Latent Diffusion Model for vasudev.lal@intel.com 3D (LDM3D). Unlike the original model, LDM3D is capable of generating both image and depth map data from a given text prompt as can be seen in Figure 1. It allows users to generate complete RGBD representations of text prompts, bringing them to life in vivid and immersive 360° views. Our LDM3D model was fine-tuned on a dataset of tuples containing an RGB image, depth map and caption. This dataset was constructed from a subset of the LAION-400M dataset, a large-scale image-caption dataset that contains over 400 million image-caption pairs. The depth maps used in fine-tuning were generated by the DPT-Large depth estimation model [18, 19], which provides highly accurate relative depth estimates for each pixel in an image. The use of accurate depth maps was crucial in ensuring that we are able to generate 360° views that are realistic and immersive, allowing users to experience their text prompts in vivid detail. To showcase the potential of LDM3D, we have developed DepthFusion, an application that uses the generated 2D RGB images and depth maps to compute a 360° projection using TouchDesigner [1]. TouchDesigner is a versatile platform that enables the creation of immersive and interactive multimedia experiences. Our application harnesses the power of TouchDesigner to create unique and engaging 360° views that bring text prompts to life in vivid detail. DepthFusion has the potential to revolutionize the way we experience digital content. Whether it’s a description of a tranquil forest, a bustling cityscape, or a futuristic sci-fi --- --Concat RGBD Diffusion U-Net Figure 1. LDM3D overview. Illustrating the training pipeline: the 16-bit grayscale depth maps are packed into 3-channel RGB-like depth images, which are then concatenated with the RGB images along the channel dimension. This concatenated RGBD input is passed through the modified KL-AE and mapped to the latent space. Noise is added to the latent representation, which is then iteratively denoised by the U-Net model. The text prompt is encoded using a frozen CLIP-text encoder and mapped to various layers of the U-Net using crossattention. The denoised output from the latent space is fed into the KL-decoder and mapped back to pixel space as a 6-channel RGBD output. Finally, the output is separated into an RGB image and a 16-bit grayscale depth map. Blue frame: text-to-image inference pipeline. Initiating from a Gaussian distributed noise sample in the 64x64x4-dimensional latent space. Given a text prompt, this pipeline generates an RGB image and its corresponding depth map. world, DepthFusion can generate immersive and engaging 360° views that allow users to experience their text prompts in a way that was previously impossible. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. In summary, our contributions are threefold. (1) We propose LDM3D, a novel diffusion model that outputs RGBD images (RGB images with corresponding depth maps) given a text prompt. (2) We develop DepthFusion, an application to create immersive 360°-view experiences based on RGBD images generated with LDM3D. (3) Through extensive experiments, we validate the quality of our generated RGBD images and 360°-view immersive videos. 2. Related Work Monocular depth estimation is the task of estimating depth values for each pixel of a single given RGB image. Recent work has shown great performance in depth estimation using deep learning models based on convolutional neural networks [11, 12, 14,22, 28,29]. Later, attentionbased Transformer models were adopted to overcome the issue of a limited receptive field in CNNs, allowing the model to consider global contexts when predicting depth values [3,5, 19,30]. Most recently diffusion models have also been applied to depth estimation to leverage the revolutionary generation capabilities of such methods. Diffusion models have demonstrated amazing capabilities in generating highly detailed images based on an input prompt or condition [16,20,23]. The use of depth estimates has previously been used in diffusion models as an additional condition to perform depth-to-image generation [31]. Later, [24] and [6] showed that monocular depth estimation can also be modeled as a denoising diffusion process through the use of images as an input condition. In this work we propose a diffusion model that simultaneously generates an RGB image and its corresponding depth map given a text prompt as input. While our proposed model may be functionally comparable to an image generation and depth estimation model in cascade, there are several differences, challenges, and benefits of our proposed combined model. An adequate monocular depth estimation model requires large and diverse data [15, 19], however, as there is no depth ground truth available for generated images it is hard for offthe-shelf depth estimation models to adapt to the outputs of the diffusion model. Through joint training, the generation of depth is much more infused with the image generation process allowing the diffusion model to generate more detailed and accurate depth values. Our proposed model also differs from the standard monocular depth estimation task as the reference images are now novel images that are also generated by the model. A similar task of generating multiple images simultaneously can be linked to video generation using diffusion models [8,9, 26]. Video diffusion models mostly build on [9] which proposed a 3D U-Net to jointly model a fixed number of continuous frame images which --- --are then used to compose a video. However, since we only require two outputs (depth and RGB) which do not necessarily require the same spatial and temporal dependencies as videos, we utilize a different approach in our model. 3. Methodology This section describes the LDM3D model’s methodology, training process, and distinct characteristics that facilitate concurrent RGB image and depth map creation, as well as immersive 360-degree view generation based on LDM3D output. 3.1. LDM-3D 3.1.1 Model Architecture LDM3D is a 1.6 billion parameter KL-regularized diffusion model, adapted from Stable Diffusion [20] with minor modifications, allowing it to generate images and depth maps simultaneously from a text prompt, see Fig. 1. The KL-autoencoder used in our model is a variational autoencoder (VAE) architecture based on [7], which incorporates a KL divergence loss term. To adapt this model for our specific needs, we modified the first and last Conv2d layers of the KL-autoencoder. These adjustments allowed the model to accommodate the modified input format, which consists of concatenated RGB images and depth maps. The generative diffusion model utilizes a U-Net backbone [21] architecture, primarily composed of 2D convolutional layers. The diffusion model was trained on the learned, low-dimensional, KL-regularized latent space, similar to [20]. Enabling more accurate reconstructions and efficient high-resolution synthesis, compared to transformer-based diffusion model trained in pixel space. For text conditioning, a frozen CLIP-text encoder [17] is employed, and the encoded text prompts are mapped to various layers of the U-Net using cross-attention. This approach effectively generalizes to intricate natural language text prompts, generating high-quality images and depth maps in a single pass, only having 9,600 additional parameters compared to the reference Stable Diffusion model. 3.1.2 Preprocessing the data The model was fine-tuned on a subset of the LAION400M [25] dataset, which contains image and caption pairs. The depth maps utilized in fine-tuning the LDM3D model were generated by the DPT-Large depth estimation model running inference at its native resolution of 384 x 384. Depth maps were saved in 16-bit integer format and were converted into 3-channel RGB-like arrays to more closely match the input requirements of the stable diffusion model which was pre-trained on RGB images. To achieve this conversion, the 16-bit depth data was unpacked into three separate 8-bit channels. It should be noted that one of these channels is zero for the 16-bit depth data, but this structure is designed to be compatible with a potential 24-bit depth map input. This reparametrization allowed us to encode depth information in an RGB-like image format while preserving complete depth range information. The original RGB images and the generated RGB-like depth maps were then normalized to have values within the [0, 1] range. To create an input suitable for the autoencoder model training, the RGB images and RGB-like depth maps were concatenated along the channel dimension. This process resulted in an input image of size 512x512x6, where the first three channels correspond to the RGB image and the latter three channels represent the RGB-like depth map. The concatenated input allowed the LDM3D model to learn the joint representation of both RGB images and depth maps, enhancing its ability to generate coherent RGBD outputs. 3.1.3 Fine-tuning Procedure. The fine-tuning process comprises two stages, similar to the technique presented in [20]. In the first stage, we train an autoencoder to generate a lower-dimensional, perceptually equivalent data representation. Subsequently, we fine-tune the diffusion model using the frozen autoencoder, which simplifies training and increases efficiency. This method outperforms transformer-based approaches by effectively scaling to higher-dimensional data, resulting in more accurate reconstructions and efficient high-resolution image and depth synthesis without the complexities of balancing reconstruction and generative capabilities. Autoencoder fine-tuning. The KL-autoencoder was finetuned on a training set consisting of 8233 samples, an validation set containing 2059 samples. Each sample in these sets included a caption as well as a corresponding image and depth map pair, as previously described in the preprocessing section. For the fine-tuning of our modified autoencoder, we used a KL-autoencoder architecture with a downsampling factor of 8 time the pixel space image resolution. This downsampling factor was found to be optimal in terms of fast training process and high-quality image synthesis [20]. During the fine-tuning process, we used the Adam optimizer with a learning rate of 10~° and a batch size of 8. We trained the model for 83 epochs, and we sampled the outputs after each epoch to monitor the progress. The loss function for both the images and depth data consisted of a combination of perceptual loss [32] and patch-based adversarial-type loss [10], which were originally used in the --- --pre-training of the KL-AE [7]. Lautoencoder = main max (Lest D(E(x))) — Lywy(D(E(x))) + log Dy(x) A) + Lreg (2; E, D)) Here D(E(x)) are the reconstructed images, Lrec(x, D(E(x))) is the perceptual — reconstruction loss, Laay(D(E(x))) is the adversarial loss, Dy(x) is a patch based discriminator loss, and Lyeg(x; E, D) is the KL-regularisation loss. Diffusion model fine-tuning Following the autoencoder fine-tuning, we proceeded to the second stage, which involved fine-tuning the diffusion model. This was achieved using the frozen autoencoder’s latent representations as input, with a latent input size of 64x64x4. For this stage, we employed the Adam optimizer with a learning rate of 10~° and a batch size of 32 . We train the diffusion model for 178 epochs with the loss function: Lupwap := Ee(x),e ~ N(0, 1), t [lle — €6(21, €)||3] 2) where €9(z,,t) is the predicted noise by the denoising U-Net, and t is uniformly sampled. We initiate the LDM3D fine-tuning using the weights from the Stable Diffusion v1.4 [20] model as a starting point. We monitor the progress throughout fine-tuning by sampling the generated images and depth maps, assessing their quality and ensuring the model’s convergence. Compute Infrastructure All training runs reported in this work are conducted on an Intel AI supercomputing cluster comprising of Intel Xeon processors and Intel Habana Gaudi AI accelerators. The LDM3D model training run is scaled out to 16 accelerators (Gaudis) on the corpus of 9,600 tupples (text caption, RGB image, depth map). The KL-autoencoder used in our LDM3D model was trained on Nvidia A6000 GPUs. 3.1.4 Evaluation In line with previous studies, we assess text-to-image generation performance using the MS-COCO [13] validation set. To measure the quality of the generated images, we employ Fréchet Inception Distance (FID), Inception Score (IS), and CLIP similarity metrics. The autoencoder’s performance is evaluated using the relative FID score, a popular metric for comparing the quality of reconstructed images with their corresponding original input images. The evaluation was carried out on 27,265 samples, 512x512-sized from the LAION-400M dataset. 3.2. Immersive Experience Generation AI models for image generation have become prominent in the space of AJ art, they are typically designed for 2D representations of diffused content. In order to project imagery onto a 3D immersive environment, modifications in mapping and resolution needed to be considered to achieve an acceptable result. Another previous limitation of correctly projected outputs occurs when perception is lost due to the monoscopic perspective of a single point of view. Modern viewing devices and techniques require disparity between two view points to achieve the experience of stereoscopic immersion. Recording devices typically capture footage from two cameras at a fixed distance so that a 3D output can be generated based on the disparity and camera parameters. In order to achieve the same from single images, however, an offset in pixel space must be calculated. With the LDM3D model, a depth map is extracted separately from RGB color space and can be used to differentiate a proper “eft” and “right” perspective of the same image space in 3D. First, the initial image is generated and its corresponding depth map is stored, see Fig. 2a. Using TouchDesigner [1], the RGB color image is projected to the outside of an equirectangular spherical polar object in 3D space see Fig. 2b. The perspective is set at origin 0,0,0 inside of the spherical object as the center of viewing the immersive space. The vertex points of the sphere are defined as an equal distance in all directions from the point of origin. The depth map is then used as instructions to manipulate the distance from origin to the corresponding vertex point based on monotone color values. Values closer to 1.0 move the vertex points closer to the origin, while values of 0.0 are scaled to a further distance from the origin. Values of 0.5 result in no vertex manipulation. From a monoscopic view at 0,0,0, no alteration in image can be perceived since the “rays” extend linearly from the origin outward. However, with the dual perspective of stereoscopic viewpoints, the pixels of the mapped RGB image are distorted in a dynamic fashion to give the illusion of depth. This same effect can also be observed while moving the single viewpoint away from origin 0,0,0 as the vertex distances scale equally against their initial calculation. Since the RGB color space and depth map pixels occupy the same regions, objects that have perceived geometric shapes are given approximate depth via their own virtual geometric dimensions in the render engine within TouchDesigner. Fig. 2 explains the entire pipeline. This approach is not limited to the TouchDesigner platform and may also be replicated inside similar rendering engines and software suites that have the ability to utilize RGB and depth color space in their pipelines. --- --oe ee ee . "Abt fice”init_image pipeline A blue office Se ! iffusion . RGED ! MiDaS depthmap Unet | — . . Inference L (a) Step 1: Img-to-img inference pipeline for LDM3D. initiating from a panoramic image and corresponding depth map computed using DPT-Large [18,19]. The RGBD input is processed through the LDM3D image-to-image pipeline, generating a transformed image and depth map guided by the given text prompt. ( Equirectangular : : : : : : : : ; ) ‘ to spherical Meshing Lomsp pheri 24bit color image Projection Camera placement Textured Sphere at origin (00.0) with Mesh _LDM3D Depthmap to vertex 16bit depth map ‘manipulation Mesh Refinement LA Lt (b) Step 2: LDM3D generated image is projected on a sphere, using vertex manipulation based on diffused depth map, followed by meshing. Viewpoint shows depth proximity. Frame assembly into movie file output. Camera movement perspective. (c) Step 3: Image generation from different viewpoints, and video assembly. Figure 2. Immersive experience generation pipeline. --- --RGB Images LDM3D (Ours) Depth Maps DPT-Large Figure 3. Qualitative comparison of images to Stable diffusion v1.4 [20] and depth maps to DPT-Large [18, 19], on 512 ximages from the COCO validation dataset. Captions from top to bottom:”a close up of a sheet of pizza on a table”, ”A picture of some lemons on a table”, ”A little girl with a pink bow in her hair eating broccoli”, ”A man is on a path riding a horse”,”A muffin in a black muffin wrap next to a fork”, ”a white polar bear drinking water from a water source next to some rocks”. 4. Results In the following, we show the high quality of the generated images and depth maps of our LDM3D model. We also show the impact on performance of the autoencoder when adding the depth modality. 4.1. Qualitative Evaluation A qualitative analysis of the generated images and depth maps reveals that our LDM3D model can effectively generate visually coherent outputs that correspond well to the provided text prompts. The generated images exhibit fine details and complex structures, while the depth maps accurately represent the spatial information of the scenes, see Fig. 3. These results highlight the potential of our model for various applications, including 3D scene reconstruction and immersive content creation, see Fig. 2. A video with examples of the immersive 360-views that can be generated using our complete pipeline can be found at https://t.ly/TYASA. 4.2. Quantitative Image Evaluation Our LDM3D model demonstrates impressive performance in generating high-quality images and depth maps from text prompts. When evaluated on the MS-COCO validation set, the model achieves competitive scores to the Stable diffusion baseline using FID and CLIP similarity metrics, see Tab. 1. There is a degradation in the inception score (IS), which might indicate that our model generates images that are close to the real images in terms of their feature distributions, as could be derived by the similar FID scores, but they might lack diversity or some aspects of image quality that IS captures. Nevertheless, IS is considered to be a less robust metric than FID because it struggles with capturing intra-class diversity [4], is highly sensitive to model parameters and implementations, whereas FID is better at assessing the similarity between distributions of real and generated images while being less sensitive to minor changes in network weights that don’t impact image quality [2]. The high CLIP similarity score indicates that the model maintains a high level of detail and fidelity with respect to the text prompts. Method FID, ISt CLIPt SD v1.4 28.08 34.1740.76 26.13 42.SD v1.5 27.39 34.02+0.79 26.13 + 2.LDMSD (ours) 27.82 28.79+0.49 26.6142.Table 1. Text-to-Image synthesis. Evaluation of text-conditional image synthesis on the 512 x 512-sized MS-COCO [13] dataset with 50 DDIM [27] steps. Our model is on par with the Stable diffusion models with the same number of parameters (1.06B). IS and CLIP similarity scores are averaged over 30k captions from the MS-COCO dataset. In addition, we investigate the relationship between key hyperparameters and the quality of the generated images. We plot the FID and IS scores against the classifier-free diffusion guidance scale factor (Fig. 4), the number of denoising steps (Fig. 5), and the training step (Fig. 7). Additionally, we plotted the CLIP similarity score against the classifier-free diffusion guidance scale factor in see Fig. 6. Fig. 4 indicates that the optimal classifier-free diffusion --- --guidance scale factor that produces the best balance between image quality and diversity is around s=5, higher than reported on Stable diffusion v1.4 (s=3). Fig. 6 indicated that the alignment of the generated images with the input text prompts is nearly unaffected as the scale factor changes for scale factors larger than 5. Fig. 5 indicates that the image quality increases with the number of denoising steps, the most significant improvement occurs when increasing the DDIM steps from 50 to 100. 100Q 80 {=i i i i 0 5 10Classifier-free diffusion guidance scale Figure 4. FID / IS vs. Classifier-free diffusion guidance scale factor. Evaluation of text-conditional image synthesis onsamples, 512 x 512-sized from MS-COCO [13] dataset, withDDIM [27] steps. I T —e— FID -s- IS 55 + 16.[al 7116.iz = 54+ + 16.53 + 4i | | | | 50 100 150 200DDIM steps Figure 5. FID /IS vs. DDIM steps. Evaluation of text-conditional image synthesis on 2000 samples, 512 x 512-sized from MSCOCO [13] dataset, s=3. 4.3. Quantitative Depth Evaluation Our LDM3D model jointly outputs images and their corresponding depth maps. Since there is no ground truth depth IS) a T | CLIP similarity & I | 24 + | l | | 0 5 10Classifier-free diffusion guidance scale Figure 6. CLIP similarity score vs. Classifier-free diffusion guidance scale factor. Averaged on 2000 samples, 512 x 512-sized generated from MS-COCO [13] dataset captions, with 50 DDIM [27] steps.FID0 1 2 3Training Step -ok Figure 7. FID vs. Training Step. Evaluation of text-conditional image synthesis on 2000 samples, 512 x 512-sized from MSCOCO [13] dataset: with 50 DDIM [27] steps, s=3. reference for these images, we define a reference model against which to compute depth metrics. For this, we select the ZoeDepth metric depth estimation model. LDM3D outputs depth in disparity space, as it was fine-tuned using depth maps produced by DPT-Large. We align these depth maps to reference ones produced by ZoeDepth. This alignment is done in disparity space in a global least-squares fitting manner similar to the approach in [19]. Points to be fitted to are determined via random sampling applied to the intersected validity maps of the estimated and target depth maps, where valid depth is simply defined to be nonnegative. The alignment procedure computes per-sample scale and shift factors that are applied to the LDM3D and DPT-Large depth maps to align the depths to ZoeDepth --- --| wrt. ZoeDepth-N AbsRel RMSE [m] LDM3D 0.0911 0.DPT-Large | 0.0779 0.Table 2. Depth evaluation comparing LDM3D and DPT-Large with respect to ZoeDepth-N that serves as a reference model. RGB (LDM3D) Depth (LDM3D) Depth (DPT-L) Depth (ZoeD-N) Figure 8. Depth visualization to accompany Tab. 2. values. All depth maps are then inverted to bring them into metric depth space. The two depth metrics we compute are absolute relative error (AbsRel) and root mean squared error (RMSE). Metrics are aggregated over a 6k subset of images from the 30k set used for image evaluation. Tab. 2 shows that LDM3D achieves similar depth accuracy as DPT-Large, demonstrating the success of our finetuning approach. A corresponding visualization is shown in Fig. 8. 4.4, Autoencoder Performance We first evaluate the performance of our fine-tuned KLAE using the relative FID score, see Tab. 3. Our findings show a minor but measurable decline in the quality of reconstructed images compared to the pre-trained KL-AE. This Model rFID_Abs.Rel. pre-trained KL-AE, RGB 0.763 fine-tuned KL-AE, RGBD 1.966 0.Table 3. Comparison of KL-autoencoder fine-tuning approaches. The pre-trained KL-AE was evaluated on 31,471 images, and the fine-tuned KL-AE on 27,265 images, 512x512-sized from the LAION-400M [25] dataset. can be attributed to the increased data compression ratio when incorporating depth information alongside RGB images in the pixel space, but keeping the latent space dimensions unchanged. Note that the adjustments made to the AE are minimal, adding only 9,615 parameters to the pretrained AE. We expect that further modifications to the AE can further improve performance. In the current architecture, this decrease in quality is compensated by fine-tuning the diffusion U-Net. The resulting LDM3D model performs on par with vanilla Stable Diffusion as shown in the previous sections. 5.
--- CONCLUSION ---
In conclusion, this research paper introduces LDM3D, a novel diffusion model that generates RGBD images from text prompts. To demonstrate the potential of LDM3D we also develop DepthFusion, an application that creates immersive and interactive 360-view experiences using the generated RGBD images in TouchDesigner. The results of this research have the potential to revolutionize the way we experience digital content, from entertainment and gaming to architecture and design. The contributions of this paper pave the way for further advancements in the field of multiview generative AI and computer vision. We look forward to seeing how this space will continue to evolve and hope that the presented work will be useful for the community. References 1] Touchdesigner. https: //derivative.ca. Accessed: 2022-12-03. 1,2] Shane Barratt and Rishi Sharma. A note on the inception score, 2018.3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4009-4018, June 2021.4] Ali Borji. Pros and cons of gan evaluation measures: New developments, 2021.5] Zeyu Cheng, Yi Zhang, and Chengkai Tang. Swindepth: Using transformers and multi-scale fusion for monocular-based depth estimation. [EEE Sensors Journal, 21(23):26912-26920, 2021.--- ---(17] Yiqun Duan, Zheng Zhu, and Xianda Guo. Diffusiondepth: Diffusion denoising approach for monocular depth estimation, 2023.Patrick Esser, Robin Rombach, and Bjérm Ommer. Taming transformers for high-resolution image synthesis. arXiv preprint arXiv:2012.09841, 2020. 3,Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks, 2018.Yevhen Kuznietsov, Jorg Stuckler, and Bastian Leibe. Semisupervised deep learning for monocular depth map prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.Tro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 2016 Fourth International Conference on 3D Vision (3DV), pages 239248, 2016.Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015. 4, 6,Armin Masoumian, Hatem A Rashwan, Saddam Abdulwahab, Julidn Cristiano, M Salman Asif, and Domenec Puig. Gcndepth: Self-supervised monocular depth estimation based on graph convolutional network. Neurocomputing, 517:81-92, 2023.Yue Ming, Xuyang Meng, Chunxiao Fan, and Hui Yu. Deep learning for monocular depth estimation: A review. Neurocomputing, 438:14—33, 2021.Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 16784-16804. PMLR, 17-23 Jul 2022.Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. JCCV, 2021. 1, 5,[24] [25]René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. JEEE transactions on pattern analysis and machine intelligence, 44(3):1623—1637, 2020. 1, 2,5, 6,Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjérn Ommer. High-resolution image synthesis with latent diffusion models. arXiv preprint arXiv:2112.10752, 2022. 1, 2,3, 4,Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015.Anirban Roy and Sinisa Todorovic. Monocular depth estimation using neural regression forest. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016.Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David J Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023.Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarcezyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 3,Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video generation without text-video data. In The Eleventh International Conference on Learning Representations, 2023.Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 6,Dan Xu, Elisa Ricci, Wanli Ouyang, Xiaogang Wang, an Nicu Sebe. Multi-scale continuous crfs as sequential deep networks for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.Dan Xu, Wei Wang, Hao Tang, Hong Liu, Nicu Sebe, an Elisa Ricci. Structured attention guided convolutional neural fields for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, an Elisa Ricci. Transformer-based attention networks for continuous pixel-wise prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 16269-16279, October 2021.--- --[31] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv preprint arXiv:2302.05543, 2023.[32] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric, 2018.
