--- ABSTRACT ---
We present our work on developing a multilingual, efficient text-to-text transformer that is suitable for handling long inputs. This model, called mLongT5, builds upon the architecture of LongTS, while leveraging the multilingual datasets used for pretraining mT5 and the pretraining tasks of UL2. We evaluate this model on a variety of multilingual summarization and question-answering tasks, and the results show stronger performance for mLongT5 when compared to existing multilingual models such as mBART or M-BERT. 1
--- INTRODUCTION ---
In recent years, there has been development of making transformer-based models more efficient so that they can handle longer input sequences. Many of the models though have been English-only, making them inapplicable to other languages. In this paper, we present our work in extending one of these models to be able to handle multilingual data. Our model, called mLongT5S, takes advantage of the efficient architecture of LongT(Guo et al., 2022), and has been pretrained on the multilingual mC4 dataset (Xue et al., 2021) to be able to work on multilingual tasks. We have applied mLongTS to a variety of multilingual summarization and question-answering tasks, and results show that mLongTS exhibits strong performance in these domains. The configurations! and checkpoints” have all been open-sourced. 2
--- RELATED WORK ---
There are two areas of related work — efficient transformer models that can handle long inputs, and multilingual models. ‘https: //github.com/google/flaxformer/tree/ main/flaxformer/t5x/configs/longt5/models *https://github.com/google-research/longtThere has been much interest of late in making transformer models more efficient, such as to handle longer inputs. Example of these include ETC (Ainslie et al., 2020), Big Bird (Zaheer et al., 2020), LongT5 (Guo et al., 2022), and Longformer (Beltagy et al., 2020). These models were successful in taking various approaches to address the quadratic growth of the attention mechanism in transformers. Unfortunately though, these models are trained on English datasets, limiting their use in multilingual domains. With respect to multilingual models, these would include mTS5 (Xue et al., 2021), mBART (Liu et al., 2020), and the recent umT5 (Chung et al., 2023). These models re-used architectures used by English models but are pretrained on a larger, multilingual corpus, with mT5 and umTS trained on 101 languages and mBART on 25. While these models showed strong performance on being able to handle a wide variety of languages, they suffered the same restrictions as their original English models on not being able to scale up to longer sequences. 3. Model mLongTS builds upon the architecture of LongTS(Guo et al., 2022). LongT5 was developed to efficiently handle long inputs by utilizing a more efficient attention mechanism. The model was shown to have strong performance on a variety of downstream tasks, and thus is the foundation for mLongT5S. 3.1 Datasets To make mLongTS5 multilingual, we leverage the mC4 dataset used for training the multilingual model mT5 (Xue et al., 2021), which consists of 101 languages. This dataset has recently been updated, as described by Chung et al. (2023), and was used for training umT5 and creating a new SentencePiece model (Kudo and Richardson, 2018). As such, we then make use of the same SentencePiece --- --model used for umT5, thus allowing mLongTS to handle multilingual inputs. 3.2. Pretraining Tasks One key difference with our model and LongTis the changing of tasks for pretraining the model. LongTS made use of PEGASUS’ Principle Sentences Generation (PSG) (Zhang et al., 2020) for pretraining its models. While this was shown to have strong performance for various downstream tasks, the one weakness of PSG is that it is less suitable for multilingual training. PSG relies on being able to split a piece of text into sentences, with current implementation best suited for Latin-based languages. The need to break text into sentences properly for 101 different languages makes it then a challenging task to use in a multilingual setting. To overcome this, we instead decided to apply UL2’s pretraining tasks (Tay et al., 2022). Their pretraining task, called Mixture-of-Denoisers (MoD), has the model learning from a mixture of tasks, and has been shown to work better than T5’s original pretraining task (Raffel et al., 2019). More importantly, MoD can be more easily applied to other languages compared to PSG, thus making it ideal for pretraining mLongT5. 3.3. Pretraining Details Pretraining mLongTS has many similarities to how LongTS was pretrained. It is pretrained for one million steps, and we pretrained model sizes of Base, Large, and XL. We also use the same pretraining lengths, 4,096 for the inputs and 910 for the targets. One small difference is increasing the batch size from 128 to 256, allowing the model to train on the same number of tokens as mT5. For the mC4 dataset, we used version 3.1.0, which is the version update by Chung et al. (2023). For dataset sampling, we use the UniMax sampling
--- CONCLUSION ---
We have presented our new model mLongTS. It has the benefits of the efficient architecture of LongT5, with the ability to handle multingual inputs and outputs. As our report shows, the model is able to perform well on a variety of summarization and question-answering tasks. e 3: WikiLingua summarization results. These results are using the GEM version of the task. Approach EM Fl mTS (base - 512 input) 37.16 49.mTS (base - 1k input) 43.09 56.mTS (base - 2k input) 44.63 58.mTS (base - 4k input) 45.41 58.mT5 (large - 512 input) 40.96 54.mTS (large - 4k input) 52.77 66.mTS (x1 - 512 input) 43.84 56.mTS (x1 - 4k input) 55.03 68.mLongTS (base - 4k input) 50.76 62.mLongTS (base - 8k input) 51.21 63.mLongTS (base - 16k input) 52.43 64.mLongTS (large - 4k input) 54.04 66.mLongTS (large - 8k input) 55.56 68.mLongTS (large - 16k input) 55.93 68.mLongTS (x1 - 4k input) 58.52 70.mLongTS (x1 - 8k input) 59.6 71.mLongTS (xl - 16k input) 60.42 72.Table 4: TyDi QA results. Limitations mLongTS has the same limitations as seen in the original LongT5 model, in that they are more suited for tasks of lengthier inputs. Tasks with shorter inputs will be better served by models like mT5 and umTS, which can take advantage of full attention. References Joshua Ainslie, Santiago Ontafidn, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483. --- --Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. 2023. UniMax: Fairer and more effective language sampling for large-scale multilingual pretraining. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454-470. Jacob Devlin. 2018. Multilingual BERT README. https: //github.com/google-research/bert/ blob/master/multilingual.md. Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondiej DuSek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jnamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Joao Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the Ist Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96-120, Online. Association for Computational Linguistics. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontafion, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724— 736, Seattle, United States. Association for Computational Linguistics. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XLsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693-4703, Online. Association for Computational Linguistics. Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics. Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. 2020. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4034-4048, Online. Association for Computational Linguistics. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association for Computational Linguistics, 8:726-742. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683. Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2020. MLSUM: The multilingual summarization corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8051-8067, Online. Association for Computational Linguistics. Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. 2022. UL2: Unifying language learning paradigms. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mTS: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for longer sequences. In Advances in Neural Information Processing Systems, volume 33, pages 17283-17297. Curran Associates, Inc. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2020. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 11328-11339. PMLR. --- --A XL-Sum We show the full results of running our mLongTmodels on XL-Sum in Table 5. These results are those that had been uploaded to GitHub > by the authors along with the updated datasets. When computing ROUGE scores, we use similar computations as done in the respective paper, with exceptions to Chinese, Japanese and Thai. For these languages, we use the SPM we used in our model for the tokenization of the results in order to compute ROUGE. 3https: //github.com/csebuetn1p/x1- sum --- --mTS5 (base) mLongT5 (base) mLongTS (large) mLongT5S (xl) Language R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L Amharic 20.05 7.41 18.08 | 16.70 5.91 14.73 | 20.29 7.99 18.09 | 22.37 8.90 19.Arabic 34.91 4.79 29.16 | 26.39 11.01 22.45 | 27.65 12.25 23.57 | 32.09 15.04 27.Azerbaijani 21.42 9.52 19.33 | 17.52 7.10 15.77 | 19.92 8.80 18.08 | 22.68 9.89 20.Bengali 29.57 12.11 25.13 | 21.39 8.22 18.65 | 24.69 10.04 21.25 | 26.83 11.32 22.Burmese 15.96 5.15 14.18 | 45.28 26.62 34.76 | 49.07 29.52 38.10 | 51.60 31.69 40.Chinese (Simp.) 39.41 7.79 33.41 | 38.90 21.78 32.59 | 42.62 24.70 35.80 | 48.42 29.99 41.Chinese (Trad.) 37.19 17.14 31.62 | 39.45 22.40 32.51 | 43.32 25.56 35.95 | 48.82 30.80 41.English 37.60 15.15 29.88 | 32.85 11.38 25.64 | 35.59 13.63 28.02 | 39.51 17.00 31.French 35.34 16.17 28.20 | 30.06 12.93 24.21 | 31.88 14.32 25.61 | 34.82 16.17 28.Gujarati 21.96 7.74 19.86 | 19.59 6.08 17.61 | 22.38 7.94 20.15 | 25.52 9.92 22.Hausa 39.44 17.68 31.67 | 34.61 13.73 27.30 | 38.04 16.07 30.32 | 40.58 18.57 32.Hindi 38.59 16.88 32.01 | 34.81 14.29 28.71 | 37.42 16.71 31.22 | 40.92 19.73 34.Igbo 31.61 0.16 24.53 | 25.82 8.05 20.19 | 30.41 10.01 23.68 | 31.31 9.88 24.Indonesian 37.00 17.02 30.76 | 32.15 13.05 26.59 | 35.17 15.23 29.07 | 38.87 18.00 32.Japanese 48.15 23.85 37.36 | 45.56 27.12 36.51 | 48.60 29.95 39.00 | 50.77 32.06 40.Kirundi 31.99 14.37 25.83 | 25.61 10.07 20.26 | 29.36 12.78 23.67 | 31.67 14.55 25.Korean 23.67 1.45 22.36 | 20.25 9.20 19.00 | 23.18 10.42 21.38 | 25.30 11.63 23.Kyrgyz 18.38 7.96 16.50} 14.08 5.27 1246 | 16.01 630 14.14 | 18.19 7.81 16.Marathi 22.01 9.54 19.92 | 20.33 8.62 1841 | 23.35 10.56 21.22 | 25.90 12.03 23.Nepali 26.65 10.25 24.28 | 23.96 8.94 21.80 | 26.24 10.33 23.91 | 28.87 11.59 26.Oromo 18.70 6.17 16.19 | 14.88 438 12.7 17.91 5.65 15.28 | 19.52. 6.50 17.Pashto 38.47 15.55 31.91 | 35.01 3.79 28.84 | 38.63 16.06 32.00 | 41.37 17.61 33.Persian 36.94 16.19 30.07 | 35.47 14.66 28.40 | 37.70 16.45 30.49 | 40.64 18.89 33.Pidgin 37.96 15.12 29.87 | 33.86 12.01 26.68 | 35.86 13.72 28.24 | 38.0 15.08 29.Portuguese 37.17 15.90 28.56 | 31.67 12.51 24.46 | 34.04 14.51 26.65 | 37.66 17.57 29.Punjabi 30.70 12.21 25.52 | 28.61 0.43 23.66 | 31.92 12.75 26.17 | 3445 14.81 28.Russian 32.22 13.64 26.17 | 22.11 8.29 8.62 | 24.39 10.00 20.54 | 28.20 12.72 23.Scottish Gaelic 29.02 10.99 22.88 | 26.98 8.87 21.57 | 29.80 10.64 23.44 | 31.74 12.61 25.Serbian (Cyrillic) | 23.78 7.98 20.14 | 20.30 5.86 6.74 | 21.92 6.98 18.35 | 27.5 11.46 23.Serbian (Latin) 21.64 666 18.23 | 18.14 4.75 4.96 | 21.79 6.92 18.14 | 25.86 10.17 21.Sinhala 27.29 13.38 23.47 | 22.69 10.02 19.96 | 25.24 11.52 21.98 | 27.78 13.20 24.Somali 31.56 1.58 24.22 | 27.85 9.08 21.10 | 30.29 10.69 23.29 | 31.64 11.11 24.Spanish 31.51 1.88 24.07 | 26.82 9.05 20.47 | 28.71 10.56 22.04 | 32.20 13.10 24.Swahili 37.67 17.85 30.91 | 31.79 13.25 25.67 | 34.29 15.22 27.82 | 37.29 17.22 30.Tamil 24.33 1.06 22.07 | 20.68 8.67 8.71 | 24.08 10.74 21.71 | 26.8 12.23 24.Telugu 19.86 7.03 17.61 | 15.11 4.69 3.48 | 17.98 6.12 16.10 | 21.20 7.77 18.Thai 3740 17.28 28.88 | 35.98 21.39 26.65 | 38.11 22.92 28.26 | 40.70 25.23 30.Tigrinya 25.32 8.02 21.17 | 22.27 7.08 8.61 | 26.30 8.90 22.05 | 28.53 10.13 24.Turkish 32.93 15.57 29.26 | 25.52 1.54 22.83 | 28.56 13.62 25.72 | 31.33 15.61 28.Ukrainian 23.99 10.14 20.92 | 20.97 8.16 8.17 | 23.34 9.74 20.29 | 27.05 12.16 23.Urdu 39.56 18.37 32.84 | 37.11 5.97 30.14 | 39.90 18.53 32.75 | 43.03 21.40 35.Uzbek 16.83 6.34 15.41 | 1460 5.36 3.39 | 17.26 642 15.49 | 19.18 7.80 17.Vietnamese 32.88 16.22 26.08 | 31.58 15.41 25.02 | 34.54 17.63 27.59 | 38.17 20.49 30.Welsh 32.66 1.60 26.12 | 29.96 9.40 23.96 | 33.66 12.26 27.01 | 36.49 15.34 29.Yoruba 31.66 1.66 25.09 | 25.87 8.99 20.27 | 29.49 10.50 23.26 | 32.20 12.34 25.Table 5: Full results for XL-Sum.
