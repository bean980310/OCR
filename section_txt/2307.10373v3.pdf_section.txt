--- ABSTRACT ---
The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and motion of the input video. Our method is based on a key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-toimage editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos. 1
--- INTRODUCTION ---
The evolution of text-to-image models has recently facilitated advances in image editing and content creation, allowing users to control various proprieties of both generated and real images. Nevertheless, expanding this exciting progress to video is still lagging behind. A surge of large-scale text-to-video generative models has emerged, demonstrating impressive results in generating clips solely from textual descriptions. However, despite the progress made in this area, existing video models are still in their infancy, being limited in resolution, video length, or the complexity of video dynamics they can represent. In this paper, we harness the power of a state-of-the-art pre-trained text-to-image model for the task of text-driven editing of natural videos. Specifically, our goal is to generate high-quality videos that adhere to the target edit expressed by an input text prompt, while preserving the spatial layout and motion of the original video. The main challenge in leveraging an image diffusion model for video editing is to ensure that the edited content is consistent across all video frames — ideally, each physical point in the 3D world undergoes coherent modifications across time. Existing and concurrent video editing methods that are based on image diffusion models have demonstrated that global appearance coherency across the edited frames can be achieved by extend ing the self-attention module to include multiple frames 2022}{Khachatryan et al.}/2023b --- --Ceylan et al.| {2023 (2023). Nevertheless, this approach is insufficient for achieving the desired level 0: temporal consistency, as motion in the video is only implicitly preserved through the attention module. Consequently, professionals or semi-professionals users often resort to elaborate video editing pipelines that entail additional manual work. In this work, we propose a framework to tackle this challenge by explicitly enforcing the original inter-frame correspondences on the edit. Intuitively, natural videos contain redundant information across frames, e.g., depict similar appearance and shared visual elements. Our key observation is that the internal representation of the video in the diffusion model exhibits similar properties. That is, the level of redundancy and temporal consistency of the frames in the RGB space and in the diffusion feature space are tightly correlated. Based on this observation, the pillar of our approach is to achieve consistent edit by ensuring that the features of the edited video are consistent across frames. Specifically, we enforce that the edited features convey the same inter-frame correspondences and redundancy as the original video features. To do so, we leverage the original inter-frame feature correspondences, which are readily available by the model. This leads to an effective method that directly propagates the edited diffusion features based on the original video dynamics. This approach allows us to harness the generative prior of state-of-the-art image diffusion model without additional training or fine-tuning, and can work in onjunction with an off-the-shelf diffusion-based image editing method (e.g., (2022); Hertz et al. (2022); Zhang & Agrawala (2023); Tumanyan et al. (2023). ‘0 summarize, we make the following key contributions: ¢ A technique, dubbed TokenFlow, that enforces semantic correspondences of diffusion fea tures across frames, allowing to significantly increase temporal consistency in videos generated by a text-to-image diffusion model. ¢ Novel empirical analysis studying the proprieties of diffusion features across a video. ¢ State-of-the-art editing results on diverse videos, depicting complex motions. 2
--- RELATED WORK ---
Text-driven image & video synthesis Seminal works designed GAN architectures to synthesize images conditioned on text embeddings (Reed et al} 2016} Zhang et al.||2016). With the evergrowing scale of vision-language datasets and pretraining strategies (Radford et al.||faa a 202), there has been a remarkable progress in text-driven image generation capabilities. Users can sytnesize high-quality visual content using simple text prompts. Much of this progress Dhariwal| as state is also attributed to diffusion models (Sohl-Dickstein et al. 2015} |Croitoru et al.| 2022] & Nichol 2021} Ho et al.| 2020} ) which have been establishe of-the-art text-to-image generators ( Rombach et al} 3023] Sheynin et al. : . Such models have been extended or text-to-video generation, by extending 2D architectures to the temporal dimension (e.g., using temporal attention (2022b)) and performing large-scale training on video datasets et al.| {2022a} [Blattmann et al. Singer et al.|/2022). Recently, Gen-1 2023) taiored a diffusion model architecture for the task of video editing, by conditioning the network on structure/appearance representations. Nevertheless, due to their extensive computation and memory requirements, existing video diffusion models are still in infancy and are largely restricted to short clips, or exhibit lower visual quality compared to image models. On the other side of the spectrum, a promising recent trend of works leverage a pre-trained image diffusion model for video synthesis tasks, without additional training 2023). Our work falls into this category, employing a pretrained text-to-image diffusion model for e task of video editing, without any training or finetuning. Consistent video stylization A common approach for video stylization involves applying image editing techniques (e.g., style transfer) on a frame-by-frame basis, followed by a post-processin: stage to address temporal inconsistencies in the edited video [Us tapars), Although these
--- METHOD ---
edits it according to a target text prompt (middle and bottom rows), while preserving the semantic layout and motion in the original scene. ABSTRACT The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and motion of the input video. Our method is based on a key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-toimage editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos. 1 INTRODUCTION The evolution of text-to-image models has recently facilitated advances in image editing and content creation, allowing users to control various proprieties of both generated and real images. Nevertheless, expanding this exciting progress to video is still lagging behind. A surge of large-scale text-to-video generative models has emerged, demonstrating impressive results in generating clips solely from textual descriptions. However, despite the progress made in this area, existing video models are still in their infancy, being limited in resolution, video length, or the complexity of video dynamics they can represent. In this paper, we harness the power of a state-of-the-art pre-trained text-to-image model for the task of text-driven editing of natural videos. Specifically, our goal is to generate high-quality videos that adhere to the target edit expressed by an input text prompt, while preserving the spatial layout and motion of the original video. The main challenge in leveraging an image diffusion model for video editing is to ensure that the edited content is consistent across all video frames — ideally, each physical point in the 3D world undergoes coherent modifications across time. Existing and concurrent video editing methods that are based on image diffusion models have demonstrated that global appearance coherency across the edited frames can be achieved by extend ing the self-attention module to include multiple frames 2022}{Khachatryan et al.}/2023b --- --Ceylan et al.| {2023 (2023). Nevertheless, this approach is insufficient for achieving the desired level 0: temporal consistency, as motion in the video is only implicitly preserved through the attention module. Consequently, professionals or semi-professionals users often resort to elaborate video editing pipelines that entail additional manual work. In this work, we propose a framework to tackle this challenge by explicitly enforcing the original inter-frame correspondences on the edit. Intuitively, natural videos contain redundant information across frames, e.g., depict similar appearance and shared visual elements. Our key observation is that the internal representation of the video in the diffusion model exhibits similar properties. That is, the level of redundancy and temporal consistency of the frames in the RGB space and in the diffusion feature space are tightly correlated. Based on this observation, the pillar of our approach is to achieve consistent edit by ensuring that the features of the edited video are consistent across frames. Specifically, we enforce that the edited features convey the same inter-frame correspondences and redundancy as the original video features. To do so, we leverage the original inter-frame feature correspondences, which are readily available by the model. This leads to an effective method that directly propagates the edited diffusion features based on the original video dynamics. This approach allows us to harness the generative prior of state-of-the-art image diffusion model without additional training or fine-tuning, and can work in onjunction with an off-the-shelf diffusion-based image editing method (e.g., (2022); Hertz et al. (2022); Zhang & Agrawala (2023); Tumanyan et al. (2023). ‘0 summarize, we make the following key contributions: ¢ A technique, dubbed TokenFlow, that enforces semantic correspondences of diffusion fea tures across frames, allowing to significantly increase temporal consistency in videos generated by a text-to-image diffusion model. ¢ Novel empirical analysis studying the proprieties of diffusion features across a video. ¢ State-of-the-art editing results on diverse videos, depicting complex motions. 2 RELATED WORK Text-driven image & video synthesis Seminal works designed GAN architectures to synthesize images conditioned on text embeddings (Reed et al} 2016} Zhang et al.||2016). With the evergrowing scale of vision-language datasets and pretraining strategies (Radford et al.||faa a 202), there has been a remarkable progress in text-driven image generation capabilities. Users can sytnesize high-quality visual content using simple text prompts. Much of this progress Dhariwal| as state is also attributed to diffusion models (Sohl-Dickstein et al. 2015} |Croitoru et al.| 2022] & Nichol 2021} Ho et al.| 2020} ) which have been establishe of-the-art text-to-image generators ( Rombach et al} 3023] Sheynin et al. : . Such models have been extended or text-to-video generation, by extending 2D architectures to the temporal dimension (e.g., using temporal attention (2022b)) and performing large-scale training on video datasets et al.| {2022a} [Blattmann et al. Singer et al.|/2022). Recently, Gen-1 2023) taiored a diffusion model architecture for the task of video editing, by conditioning the network on structure/appearance representations. Nevertheless, due to their extensive computation and memory requirements, existing video diffusion models are still in infancy and are largely restricted to short clips, or exhibit lower visual quality compared to image models. On the other side of the spectrum, a promising recent trend of works leverage a pre-trained image diffusion model for video synthesis tasks, without additional training 2023). Our work falls into this category, employing a pretrained text-to-image diffusion model for e task of video editing, without any training or finetuning. Consistent video stylization A common approach for video stylization involves applying image editing techniques (e.g., style transfer) on a frame-by-frame basis, followed by a post-processin: stage to address temporal inconsistencies in the edited video [Us tapars), Although these methods effectively reduce high-frequency temporal flickering, they are not designed to handle frames that exhibit substantial variations in content, which often occur when applying text-based image editing techniques (Qi et af] 2023). propose to decompose a video into a set of 2D atlases, each provides a unified representation of the background or of a foreground object throughout the video. Edits applied to the 2D atlases are automaticall: mapped back to the video, thus achieving temporal consistency with minimal effort. (2022); [Lee et al.| (2023b) leverage this representation to perform text-driven editing. However, atlas representation is limited to videos with simple motion and requires long training, limiting the applicability of this technique and of the methods built upon it. Our work is also related to classical works that demonstrated that small patches in a natural video extensively repeat across frames 2005) (Shahar et al.||2011}|Cheung et al. , and thus consistent editing can by simplified by editing --- --Sample Frames Per-frame . xX xX xX Figure 3: Diffusion features across time. Left: Given an input video (top row), we apply DDIM inversion on each frame and extract features from the highest resolution decoder layer in €g. We apply PCA on the features (i.e., output tokens from the self-attention module) extracted from all frames and visualize the first three components (second row). We further visualize an x-t slice (marked in red on the original frame) for both RGB and features (bottom row). The feature representation is consistent across time — corresponding regions are encoded with similar features across the video. Middle: Frames and feature visualization for an edited video obtained by applying an image editing method ) on each frame; inconsistent patterns in RGB are also evident in the feature space (e.g., on the dog’s body). Right: Our method enforces the edited video to convey the same level of feature consistency as the original video, which translates into a coherent and high-quality edit in RGB space. Features (PCA) x-t slice a subset of keyframes and propagating the edit across the video by establishing patch correspondences using handcrafted features and oj 2020) flow (Ruder et al. | 2016} |Jamriska et al.| [Jamri8ka et al.| et al. 2019) or by training a patch-based GAN (Texler et al|] evertheless, such propagation methods struggle to handle videos with Wate ae cl | 020) or ¢ with complex dynamics. Importantly, they rely on a user provided consistent edit of the keyframes, which remains a labor-intensive task yet to be automated. combines keyframe editing with a propagation method by (2019). They edit keyframes using a text-to-image diffusion model while enforcing optical flow constraints on the edited keyframes. However, since optical flow estimation between distant frames is not reliable, their method fails to consistently edit keyframes that are far apart (as seen in our Supplementary Material - SM), and as a result, fails to consistently edit most videos. Our work shares a similar motivation as this approach that benefits from the temporal redundan cies in natural videos. We show that such redun- @) ©) , © an dancies are also present in the feature space of a Reconstructed Warped —_Nearest-Neighbour Target Source» Target Field text-to-image diffusion model, and leverage this property to achieve consistency. Target I Controlled generation via diffusion features manipulation Recently, a surge of works demonstrated how text-to-image diffusion models can be readily adapted to various editing and generation tasks, by performing simple operations on the intermediate feature rej eo of the diffusion network (Chefer et al. Ma et al [2023] [Tumanyan| | 2023) [Hertz et al:| (2022; [Patashnik et al. 2023 Cao _et_al J (2023). (2023); to reconstruct nearby frames. This is done by: (a) -| 2023) demonstrated semantic ap- swapping each feature in the target by its nearest feapearance swapping using diffusion feature corre- ture in the source, in all layers and all generation time spondences. Here ral (2028 observed that by steps, and (b) simple warping in RGB space, using manipulating the cross-attention layers, it is pos- a nearest neighbour field (c), computed between the sible to control the relation between the spatial source and target features extracted from the highest layout of the image to each word in the rales, soo) g- resolution decoder layer. The target is faithfully re and-Play Diffusion (PnP,/Tumanyan et al.| constructed, demonstrating the high level of spatial analyzed the spatial features and the se 1 (2025) granularity and shared content between the features. Target II Figure 2: Fine-grained feature correspondences. Features (i.e., output tokens from the self-attention modules) extracted from of a source frame are used --- --Compute NN field Y Extract tokens inversion Noisy Sampled keyframes ee oe ier & painting” a Figure 4: TokenFlow pipeline. Top: Given an input video Z, we DDIM invert each frame, extract its tokens, i.e., output features from the self-attention modules, from each timestep and layer, and compute inter-frame features correspondences using a nearest-neighbor (NN) search. Bottom: The edited video is generated as follows: at each denoising step ¢, (I) we sample keyframes from the noisy video J; and jointly edit them using an extended-attention block; the set of resulting edited tokens is Tyase. (II) We propagate the edited tokens across the video according to the pre-computed correspondences of the original video features. To denoise J, we feed each frame to the network, and replace the generated tokens with the tokens obtained from the propagation step (II). painting” observed that by extending the self-attention module to operate on more than a single frame, it is possible to generate frames that share a common global appearance. |Qi et al|| 2025}; property to achieve globally-coherent video edits. Nevertheless, as demonstrated in Sec. [5] inflating the self-attention module is insufficient for achieving fine-grained temporal consistency. Prior and concurrent works either compromise visual quality, or exhibit limited temporal consistency. In this work, we also perform video editing via simple operations in the feature space of a pre-trained text-to-image model, we explicitly encourage the features of the model to be temporally consistent through TokenFlow. maps and found that they capture semantic information at high spatial granularity. Tune-A-Video 3 PRELIMINARIES Diffusion Models Diffusion probabalistic models (DPM) (Sohl-Dickstein et al.| fetal] 2022} [Dhariwal & Nichol] 2021} [Mo et al] [2020} [Nichol & Dharivall 2021) are a classgenerative models that aim to approximate a data distribution q through a progressive denosing process. Starting from a Gaussian 1.i.d noisy image a7 ~ N(0, I), the diffusion model eg, gradually denoises it, until reaching a clean image a) drawn from the target distribution g. DPM can learn a conditional distribution by incorporating additional guiding signals, such as text conditioning. derived DDIM, a deterministic sampling algorithm given an initial noise a. By applying this algorithm in the reverse order (a.k.a. DDIM inversion) starting from the clean ao, it allows to obtain the intermediate noisy images {2x;}7_, used to generate it. Stable Diffusion Stable Diffusion (SD) (Rombach et al.|/2022) is a prominent text-to-image diffusion model that operates in a latent image space. A pretrained encoder maps RGB images to this space, and a decoder decodes latents back to high-resolution images. In more detail, SD is based on a U-Net architecture (Ronneberger ea] 2015), which comprises of residual, self-attention, and cross-attention blocks. The residual block convolves the activations from a previous layer, while cross-attention manipulates features according to the text prompt. In the self-attention block, features are projected into queries Q, keys K, and values V. The Attention operation computes the affinities between the d-dimensional projections Q, K to yield the output of the layer: T A-V where A = Attention(Q;K) and Attention(Q; K) = softmax (2%) (1) --- --Input video Input video Bl “A Van Gogh portrait” “Ice oa of oma car” “A marble sculpture” ‘Sand sculpture a car on the beach” Input video “A robotic wolf” “Maui from Moana Movie” “A colourful polygonal illustration” “A Pixar animation” Figure 5: Results. Sample results ‘ our =! We refer the reader to our webpage and SM for more examples and full-video results. 4 METHOD Given an input video Z = [J', ..., J"], and a text prompt P describing the target edit, our goal is to generate an edited video J = [J',..., J”] that adheres to the text P, while preserving the original motion and semantic layout of Z. To achieve this, our framework leverages a pretrained and fixed text-to-image diffusion model eg. Naively leveraging €g for video editing, by applying an image editing method on each frame inde dently (e.g.,|Hertz et al. (2022); Tumanyan et al. (2023); Meng et al. (2022); Zhang & Agrawala! (2033), middle ), results in content inconsistencies across frames (e.g., Fig. column). Our key nding is that these inconsistencies can be alleviated by enforcing consistency among the internal diffusion features across frames, during the editing process. Natural videos typically depict coherent and shared content across time. We observe that the internal representation of natural videos in €g has similar properties. This is illustrated in Fig. visualize the features extracted from a given video (first column). As seen, the features depict a shared and consistent representation across frames, i.e., corresponding regions exhibit similar representation. We further observe that the original video features provide fine-grained correspondences between frames, using a simple nearest neighbour search (Fig |2). Moreover, we show that these corresponding features are interchangeable for the diffusion model — we can ne (raat).one frame by swapping its features by their corresponding ones in a nearby frame (Fig|2[a)). Nevertheless, when an edit is applied to each frame individually, the consistency of the features breaks (Fig. B]middle column). This implies that the level of consistency of in RGB space is correlated with the consistency of the internal features of the frames. Hence, our key idea is to manipulate the features of the edited video to preserve the level of consistency and inter-frame correspondences of the original video features. As illustrated in Fig. [4] our framework, dubbed TokenFlow, alternates at each generation timestep between two main components: (i) sampling a set of keyframes and jointly editing them according to P;; this stage results in shared global appearance across the keyframes, and (ii) propagating the features from the keyframes to all of the frames based on the correspondences provided by the original --- ---Input a “A shiny metal sculpture” “An origami of a stork” TAV PNP Gen-Ours Figure 6: Comparison. We compare our method against Tune-A-Video cTaV, [Wu et all 2022)), PnPDiffusion (Tumanyan et al.|/2023) applied per frame, Gen-1 (Esser et al.|/2023), Text2 Video-Zero (Khachatryan| and Fate-Zero (Oi et al PP ero ( 2023). We refer the reader to our supplementary material for full-video parisons. video features; this stage explicitly preserves the consistency and fine-grained shared representation of the original video features. Both stages are done in combination with an image editing technique & (e.g, Hanan otal) 3023), Intuitively, the benefit of alternating between keyframe editing and propagation is twofold: first, sampling random keyframes at each generation step increases the robustness to a particular selection. Second, since each generation step results in more consistent features, the sampled keyframes in the next step will be edited more consistently. Pre-processing: extracting diffusion features. Given an input video Z, we apply DDIM inversion (see Sec [3) on each frame I’, which yields a sequence of latents [a/,...,a',]. For each generation timestep t, we feed the latent } of each frame i € [n] to the model and extract the tokens («;) from the self-attention module of every layer in the network €g (fig. |4} top). We will later use these tokens to establish inter-frame correspondences between diffusion features. 4.1 KEYFRAME SAMPLING AND JOINT EDITING Our observations imply that given the features of a single edited frame, we can generate the next frames by propagating its features to their corresponding locations. Most videos, however, can not be represented by a single keyframe. To account for that, we consider multiple keyframes, from which we obtain a set of features (tokens), Thase, that will later be propagated to the entire video. Specifically, at each generation step, we randomly sample a set of keyframes {J‘}ic, in fixed frame intervals (see SM for details). We _joinly edit the keyframes by extending the selfattention block to simultaneously process them (Wie al) 2023 , thus encouraging them to share a global appearance. In more detail, the input to the modified block are the self-attention features from all keyframes {Q' }icn, {K' Sick, {V' }icx where Q’, K', V" are the queries, keys, and values of frame i € K,&K = {i1,...i,}. The keys of all frames are concatenated, and the extended-attention is: -K"] ) @Q) vd i [Ke extattn(Q'[K",...K"*]) soft --- --The output of the block for frame i is given by: o(J') = A-[V",...V*] where A= extattn(Q'; [K™ K"*}) 6) Intuitively, each keyframe queries all other keyframes, and aggregates information from them. This results in a roughly unified appearance in the edited frames (Wu et al.| |2022} |Khachatryan et al} 2023b} |Ceylan et al.|{2023 2023). We define Tyase = {6(J") }icx, for each layer in the ig. [4 mid networ! bottom ale). 4.2 EDIT PROPAGATION VIA TOKENFLOW Given Tyase, We propagate it across the video based on the token correspondences extracted from the original video. At each generation step t, we compute the nearest neighbor (NN) of each original frame’s tokens, b(xi), and its two adjacent keyframess’ tokens, o(2'*), o(2'~) where i+ is the index of the closest future keyframe, and i— the index of the closest past keyframe. Denote the resulting NN fields ‘+, y'~: * [pl = argmin D (o(a')[p], o(@"*)[a]) (4) Where p,q are spatial locations in the token feature map, and D is cosine distance. For simplicity, we omit the generation timestep t; our method is applied in all time-steps and self-attention layers. Once we obtain 7*, we use it to propagate the edited frames’ tokens Ty,s¢ to the rest of the video, by linearly combining the tokens in Ty,se corresponding to each spatial location p and frame 7: Fy (Leases 4?) = wi OS Yb [pl] + (= wi) (Tl bl] (5) Where o(J'*) € Tyase and w; € (0,1) is a scalar proportional to the distance between frame i and its adjacent keyframes (see SM), ensuring a smooth transition. Note that F also modifies the tokens of the sampled keyframes. That is, we modify the self-attention blocks to output a linear combination of the tokens in T,,,. for all frames, including the keyframes, according to the original video token correspondences. Overall algorithm We summarize our video editing algorithm in Alg. We first perform DDIM inversion on the in- Tput: Algorithm 1 TokenFlow editing put video Z and extract the sequence T=lr,...,1" > Input Video of noisy latents {v%}7_, for all frames P > Target text prompt i € [n] (fig | top). We then denoise wv > Diffusion-based image editing technique the video, alternating between keyframes editing and TokenFlow propagation: At each generation step t, we randomize k < n keyframe indices, and denoise K = fi,.. them using an image editing technique Vi € [n], t € [T] , i} < sample keyframe indices Fy << '* Vi €[n] compute NN field {Ti_ihiex + Gl{Ij }jex; Extattn] . Trase + O({J{_,}iex) extract keyframes’ tokens q-[3] Fig. pve (1). We then denoise the entire video Sia €o[I1; TokenFlow(F, (Toase))] , by combining the image-editing tech- Output: 7 = [Jo,..., Jo nique with TokenFlow (Eq. [5] Fig. |4]()) at every self-attention block in every layer of the network. Note that each layer includes a residual connection between the input and output of the self-attention block, thus performing TokenFlow at each layer is necessary. 5 RESULTS We evaluate our method on DAVIS videos (Pont-Tuset et al.|/2017) and on Internet videos depicting animals, food, humans, and various objects in motion. The spatial resolution of the videos is 384 x 672 or 512 x 512 pixels, and they consist of 40 to 200 frames. We use various text prompts on each video to obtain diverse editing results. Our evaluation dataset comprises of 61 text-video pairs. We utilize PnP-Diffusion (Tumanyan et al.|/2023) as the frame editing method, and we use the same hyper-parameters for all our results. PnP-Diffusion may fail to accurately preserve the structure of each frame due to inaccurate DDIM inversion (see Fig. |3} middle column, right frame: the dog’s head is distorted). Our method improves robustness to this, as multiple frames contribute to the generation of each frame in the video. Our framework can be combined with any diffusion-based image editing technique that accurately preserves the structure of the images; results with different --- --image editing techniques (e.g. [Meng et al| 2022}; (2023)) are available in the SM. Fig Bland{|show sample frames from the edited videos. Our edits are temporally consistent and adhere to the edit prompt. The man’s head is changed to Van-Gogh or marble (top left); importantly, the man’s identity and the scene’s background are consistent throughout the video. The patterns of the polygonal wolf (bottom left) are the same across time: the body is consistently orange while the chest is blue. We refer the reader to the SM for implementation details and video results. Baselines. We compare our method to state-of-the-art, and concurrent works: (i) Fate-Zero and (ii) Text2Video-Zero , that utilize a text-to-image mode or video editing using self-attention inflation. (iii) Re-render a Video (Yang et al.||2023) that edits keyframes by adding optical flow optimization to self-attention inflation of an image model, and then propagates the edit from the keyframes to the rest of the video using an off-the-shelf propagation method. (iv) Tune-a-Video (Wu et al.|/2022) that fine-tunes the text-to-image model on the given test video. (v) Gen-1 (2023), a video diffusion model that was trained on a large-scale image and video dataset. (vi) Per-frame diffusion-based image editing baseline, PnP-Diffusion (Tumanyan| . We additionally consider the two following baselines: (i) Text2LIVE (Bar-Tal et al] (2022) which utilize a layered video representation (NLA) (Kasten et al.||2021) and perform test-time training using CLIP losses. Note that NLA requires foreground/background separation masks and takes ~ 10 hours to train. (ii) Applying PnP-Diffusion on a single keyframe and propagating the edit to the entire video using (2019). 5.1 QUALITATIVE EVALUATION Fig.|6|provides a qualitative comparison of our method to prominent baselines; please refer to SM for the full videos. Our method (bottom row) outputs videos that better adhere to the edit prompt while maintaining temporal consistency of the resulting edited video, while other methods struggle to meet both these goals. Tune-A-Video (second row) inflates the 2D image model into a video model, and fine-tunes it to overfit the motion of the video; thus, it is suitable for short clips. For long videos it struggles to capture the motion resulting with meaningless edits, e.g., the shiny metal sculpture. Applying PnP for each frame independently (third row) results in exquisite edits adhering to the edit prompt but, as expected, lack any temporal consistency. The results of Gen-1 (fourth row) also suffer from some temporal inconsistencies (the beak of the origami stork changes color). Moreover, their frame quality is significantly worse than that of a text-to-image diffusion model. The edits of Text2Video-Zero and Fate-Zero (fifth and sixth row) suffer from severe jittering as these methods rely heavily on the extended attention mechanism to implicitly encourage consistency. The results of Rerender-a-Video exhibit notable long-range inconsistencies and artifacts arising primarily from their reliance on optical flow estimation for distant frames (e.g. keyframes), which is known to be sub-optimal (See our video results in the SM; when the wolf turns its head, the nose color changes). We provide qualitative comparison to Text2LIVE and to a RGB propagation baseline in the SM. 5.2. QUANTITATIVE EVALUATION We evaluate our method in terms of: Table 1: We evaluate our method in temporal consistency (i) edit fidelity measured by comput- by computing warp-error and conducting a user study, and ing the average similarity between jn fidelity to the target text prompt using CLIP similarity. the CLIP embedding (Radford et al.| See Sec. [5|for more details. of each edited frame and the target text prompt; (ii) temporal con- Warp-err | User preference| CLIP sistency: rrollowing (x10~*) | of our method [score t (2023); (20T8a), tempo- LDM recon. 2.0 = 0.ral consistency is measured by (a) PnP-Diffusion ~~ [| Ins "fF 94% YF 0.computing the optical flow of the Text2Video-Zero 12.5 78% 0.original video using Tune-a-Video 30.0 82% 0., warping the edited frames Fate-Zero 6.9 71% 0.according to it, and measuring the Genl = 70% 0.warping error, and (b) a user study; Rerender-a-Video 1.8 71% 0.We adopt a Two-alternative Forced yrs wjointattention | ~ 5.9 ~~~ 90% ~~ 7 0.Choice (2AFC) Bors}: | suggested Qurg w/o rand keyframes 3.7 - 0.in [Kolkin et al.] (2019); [Park et al.) Ours 3.0 _ 0., where participants are shown e input video, ours and a baseline result, and are asked to determine which video is more tem porally consistent and better preserves the motion of the original video. The survey consists of 2000-3000 judgments per baseline obtained using Amazon mechanical turk. We note that warpingerror could not be measured for Gen! since their product platform does not output the same number of input frames. Table[I]compares our method to baselines. Our method achieves the highest CLIP --- --F = =_— —E q oe So, =F ee | | Figure 7: Limitations. Our method edits the video according to the feature correspondences of the original video, hence it cannot handle edits that requires structure deviations. score, showing a good fit between the edited video and the input guidance prompt. Furthermore, our method has a low warping error, indicating temporally consistent results. We note that Re-rendera-Video optimizes for the warping error and uses optical flow to propagate the edit, and hence has the lowest warping error; However, this reliance on optical flow often creates artifacts and longrange inconsistencies which are not reflected in the warping error. Nonetheless, they are apparent in the user study, that shows users significantly favoured our method over all baselines in terms of temporal consistency. Additionally, we consider the reference baseline of passing the original video through the LDM auto-encoder without performing editing (LDM recon.). This baseline provides an upper bound on the temporal consistency achievable by LDM auto-encoder. As expected, the CLIP similarity of this baseline is poor as it does not involve any editing. However, this baseline does not achieve zero warp error either due to the imperfect reconstruction of the LDM auto-encoder, which hallucinates high-frequency information. We further evaluate our correspondences and video representation by measuring the accuracy of video reconstruction using TokenFlow. Specifically, we reconstruct the video using the same pipeline of our editing method, only removing the keyframes editing part. Table[2]reports the PSNR and LPIPS distance of this reconstruction, compared to vanilla DDIM reconstruction. As seen, TokenFlow reconstruction slightly improves DDIM inversion, demonstrating robust frame representation. This improvement can be attributed to the keyframe randomization; It increases robustness to challenging frames since each frame is reconstructed from multiple other frames during the generation. Notably, our evaluation focuses on accurate correspondences within the feature space during generation, rather than RGB frame correspondences evaluation, which is not essential to our method. 5.3 ABLATION STUDY First, we ablate the use of TokenFlow, Sec.[4.2| for en- Table 2: We reconstruct the video using forcing temporal consistency. In this
