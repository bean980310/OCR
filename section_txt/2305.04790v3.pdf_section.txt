--- ABSTRACT ---
We present a vision and language model named MultiModal-GPT to conduct multiround dialogue with humans. MultiModal-GPT is capable of following diverse instructions, such as generating detailed captions, counting specific objects, and addressing general inquiries posed by users. The model is efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) incorporated in both the gated-cross-attention and self-attention components of the language model. Our approach involves constructing instruction templates that incorporate vision and language data for multi-modality instruction tuning, enabling the model to comprehend and adhere to human directives. We observe that the quality of training data is crucial for effective dialogue performance, as a limited dataset with short responses may cause the model to generate brief replies to any instruction. To further enhance MultiModal-GPT’s conversational abilities, we employ language-only instructionfollowing data for joint training alongside visual-language instructions. Utilizing the same instruction template for both types of data results in a significant improvement in dialogue performance. Our experiments demonstrate MultiModal-GPT’s proficiency in maintaining continuous dialogues with humans. The code, dataset, and demo can be found at https://github.com/open-mmlab/Multimodal-GPT 1
--- METHOD ---
3.1. Architecture The proposed MultiModal-GPT is based on the open-flamingo model (i. As shown in Figure[T] MultiModal-GPT consists of a vision encoder from CLIP (13), a perceiver resampler to receive the spatial features from the vision encoder, and a language decoder LLaMA C9). Note that the language decoder is conditioned on the spatial features from the perceiver resampler by cross-attention in order to encode the feature of vision into text. Please refer to i) for more details of the model architecture. 3.2 Joint Training We use both language-only instruction-following data and vision and language instruction-following data to train the MultiModal-GPT jointly. As shown in Fig[T] We freeze the whole open-flamingo model and add LoRA [4] to the self-attention, cross-attention, and FFW part in the language decoder to finetune MultiModal-GPT. The MultiModal-GPT is trained by predicting the next token of the text, and only the {response } and <EOS> tokens in the input sequence are involved in the loss calculation. 4
