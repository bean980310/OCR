--- ABSTRACT ---
Previous research observed accuracy degradation when replacing the attention softmax with a pointwise activation such as ReLU. In the context of vision transformers, we find that this degradation is mitigated when dividing by sequence length. Our experiments training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute. 1
--- INTRODUCTION ---
The transformer architecture [26] is ubiquitous in modern machine learning. Attention, a central component of the transformer [2], includes a softmax which produces a probability distribution over tokens. Softmax is costly due to an exponent calculation and a sum over sequence length which makes parallelization challenging [24, 7]. In this report we explore point-wise alternatives to the softmax operation which do not necessarily output a probability distribution. As a highlight, we observe that attention with ReLU divided by sequence length can approach or match traditional softmax attention in terms of scaling behavior as a function of compute for vision transformers. This result presents new opportunities for parallelization, as ReLU-attention can be parallelized over the sequence length dimension with fewer gather operations than traditional attention. 2
--- RELATED WORK ---
Previous research has explored substituting softmax with ReLU [25, 14] or squared ReLU [15]. However, these approaches do not divide by sequence length, which we experimentally find is important to reach accuracy comparable to softmax. In addition, previous research [21] has replaced softmax while still requiring normalization over the sequence length axis to ensure Fs) 0.82 7 — softmax a — relu/seqien g yoo] © s/32 8 0.cs m sf6 bea a © BB2 § eo” B/l6 5° 8° 3 °° W0-76 5 0.~30.742 0.o £ 20.72£ G 0.0.70 bo $ 90.0.68 < ° ra & 10? TPU core hours 10? TPU core hours Figure 1: Replacing softmax with relu/seqlen approaches or matches the scaling performance of traditional attention for vision transformers [10] with qk-layernorm [8]. This figure displays results for small to large vision transformers trained on ImageNet-21k [9] for 30 epochs. We report ImageNet-1k accuracy for ImageNet-21k models by taking the top class among those that are in ImageNet-1k, without fine-tuning. Attention with ReLU can be parallelized over the sequence length dimension with less gather operations than softmax attention. --- --Training dataset i21k. Model S/32. Training dataset i21k. Model S/16. Training dataset i21k. Model S/8. 0.0.0.0.ImageNet-1k accuracy (%) § x Pa ImageNet-1k accuracy (%) g Ry & ImageNet-1k accuracy (%) g Ry & 0.650 0.00 05 10 15Exponent a for scaling inverse seqlen 0.0 05 10 15 2.Exponent a for scaling inverse seqlen 00 os 10 15Exponent a for scaling inverse seqlen Training dataset ilk. Model S/32. Training dataset ilk. Model S/16. Training dataset ilk. Model S/8. 0.ImageNet-1k accuracy (%) ImageNet-1k accuracy (%) ImageNet-1k accuracy (%) 00 oS 10 15Exponent a for scaling inverse seqlen 0.0 05 10 15 2.Exponent a for scaling inverse seqlen 00 os 10 15Exponent a for scaling inverse seqlen - softmax —e relu —‘— squared relu —+- gelu —*— softplus —® identity —*-— relué —* sigmoid Figure 2: Replacing softmax with L~“h where h € {relu, relu’, gelu, softplus, identity, relu6, sigmoid} and L is sequence length. We typically observe the best results when a is close to 1. There is no clear best non-linearity at a % 1, so we use ReLU in our main experiment for its speed. he attention weights sum to one. This retains the downside of requiring a gather. After writing an initial version of this note, it was brought to our attention hat the variant of ReLU-atttention we study was also explored with a theoretical motivation [3, 12]. Moreover, there is extensive literature which removes activation functions altogether so that attention is inear [16, 22, 18], which is useful for long sequence engths.! In our experiments, removing the activation entirely reduced accuracy. 3
--- METHOD ---
Attention. Attention transforms d-dimensional queries, keys, and values {qi,ki,v;}#_, with a two step procedure. First, attention weights a,; are produced viaaiy=o (= [aha kx] ; j 1Concretely, with linear attention, the order of matrix multiplies can be switched from (qk )u to q(k' v) which changes the compute required from O(dL?) to O(d?L) where q,k,v € REX¢ are the queries, keys, and values and L is sequence length. (1) where @ is typically softmax. Next, the attention . L weights are used to compute outputs 0; = )>¥_, aijv;. This report explores point-wise alternatives to ¢. ReLU-attention. We observe that ¢ = L~!relu is a promising alternative to ¢ = softmax in Equation 1. We refer to attention with ¢ = L~'relu as ReLUattention. Scaled point-wise attention. More generally, our
--- EXPERIMENT ---
s training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute. 1 Introduction The transformer architecture [26] is ubiquitous in modern machine learning. Attention, a central component of the transformer [2], includes a softmax which produces a probability distribution over tokens. Softmax is costly due to an exponent calculation and a sum over sequence length which makes parallelization challenging [24, 7]. In this report we explore point-wise alternatives to the softmax operation which do not necessarily output a probability distribution. As a highlight, we observe that attention with ReLU divided by sequence length can approach or match traditional softmax attention in terms of scaling behavior as a function of compute for vision transformers. This result presents new opportunities for parallelization, as ReLU-attention can be parallelized over the sequence length dimension with fewer gather operations than traditional attention. 2 Related work Previous research has explored substituting softmax with ReLU [25, 14] or squared ReLU [15]. However, these approaches do not divide by sequence length, which we experimentally find is important to reach accuracy comparable to softmax. In addition, previous research [21] has replaced softmax while still requiring normalization over the sequence length axis to ensure Fs) 0.82 7 — softmax a — relu/seqien g yoo] © s/32 8 0.cs m sf6 bea a © BB2 § eo” B/l6 5° 8° 3 °° W0-76 5 0.~30.742 0.o £ 20.72£ G 0.0.70 bo $ 90.0.68 < ° ra & 10? TPU core hours 10? TPU core hours Figure 1: Replacing softmax with relu/seqlen approaches or matches the scaling performance of traditional attention for vision transformers [10] with qk-layernorm [8]. This figure displays results for small to large vision transformers trained on ImageNet-21k [9] for 30 epochs. We report ImageNet-1k accuracy for ImageNet-21k models by taking the top class among those that are in ImageNet-1k, without fine-tuning. Attention with ReLU can be parallelized over the sequence length dimension with less gather operations than softmax attention. --- --Training dataset i21k. Model S/32. Training dataset i21k. Model S/16. Training dataset i21k. Model S/8. 0.0.0.0.ImageNet-1k accuracy (%) § x Pa ImageNet-1k accuracy (%) g Ry & ImageNet-1k accuracy (%) g Ry & 0.650 0.00 05 10 15Exponent a for scaling inverse seqlen 0.0 05 10 15 2.Exponent a for scaling inverse seqlen 00 os 10 15Exponent a for scaling inverse seqlen Training dataset ilk. Model S/32. Training dataset ilk. Model S/16. Training dataset ilk. Model S/8. 0.ImageNet-1k accuracy (%) ImageNet-1k accuracy (%) ImageNet-1k accuracy (%) 00 oS 10 15Exponent a for scaling inverse seqlen 0.0 05 10 15 2.Exponent a for scaling inverse seqlen 00 os 10 15Exponent a for scaling inverse seqlen - softmax —e relu —‘— squared relu —+- gelu —*— softplus —® identity —*-— relué —* sigmoid Figure 2: Replacing softmax with L~“h where h € {relu, relu’, gelu, softplus, identity, relu6, sigmoid} and L is sequence length. We typically observe the best results when a is close to 1. There is no clear best non-linearity at a % 1, so we use ReLU in our main experiment for its speed. he attention weights sum to one. This retains the downside of requiring a gather. After writing an initial version of this note, it was brought to our attention hat the variant of ReLU-atttention we study was also explored with a theoretical motivation [3, 12]. Moreover, there is extensive literature which removes activation functions altogether so that attention is inear [16, 22, 18], which is useful for long sequence engths.! In our experiments, removing the activation entirely reduced accuracy. 3 Method Attention. Attention transforms d-dimensional queries, keys, and values {qi,ki,v;}#_, with a two step procedure. First, attention weights a,; are produced viaaiy=o (= [aha kx] ; j 1Concretely, with linear attention, the order of matrix multiplies can be switched from (qk )u to q(k' v) which changes the compute required from O(dL?) to O(d?L) where q,k,v € REX¢ are the queries, keys, and values and L is sequence length. (1) where @ is typically softmax. Next, the attention . L weights are used to compute outputs 0; = )>¥_, aijv;. This report explores point-wise alternatives to ¢. ReLU-attention. We observe that ¢ = L~!relu is a promising alternative to ¢ = softmax in Equation 1. We refer to attention with ¢ = L~'relu as ReLUattention. Scaled point-wise attention. More generally, our experiments will explore ¢ = L~“h for a € [0,1] and h € {relu, relu”, gelu, softplus, identity, relu6, sigmoid} 6, 13}. Sequence length scaling. We observe that scaling yy a term involving sequence length L is beneficial for high accuracy. This scaling is absent from prior work which removes softmax [15, 18]. While the central justification for sequence length scaling is empirical, we provide brief analytical motivation. Transformers are currently designed with softmax atention for which ya ay = This implies that E,;[a;j] = L~1. While it is unlikely that this is a ry condition, @ = L~'relu does ensure that E,[a;j] is O(L~') at initialization. Preserving this condition may alleviate the need to change other hy --- --Training dataset i21k. Model $/32. Training dataset i21k. Model S/16. Training dataset i21k. Model S/8. ° u a g yo76 30.70.74 70.74 F£ £ £ 5 5S072 S072 S8 8070 070 0.we we we 2 2Z 0.68 Z 0.68 Z 0.a a a 8 8£0.66 £0.66 £ 0.0.64 1 0.64 0.0.0 05 1.0 us 2:0 0.0 OS 1.0 15 2:0 0.0 05 1.0 15 2.Exponent a for scaling inverse seqlen Exponent a for scaling inverse seqlen Exponent a for scaling inverse seqlen —* relu —*— squared relu -- relu without qk-layernorm -e- squared relu without qk-layernorm Figure 3: The effect of removing qk-layernorm [8] on attention with ReLU and squared ReLU scaled by L~* where L is sequence length. Results are shown for the $/32, S/16, and S/8 vision transformer models [10, 4] trained on ImageNet-21k. Training dataset i21k. Model $/32. Training dataset i21k. Model S/16. Training dataset i21k. Model S/8. ek yore yore Sore, ° = = = © 70.74 70.74 B0.g g g S S S 30.72 30.72 9 0.5 540.70 40.70 40.2 22 2S 0.68 S 0.68 S 0.D D D o o o £0.66 £0.66 £0.0.64 0.64 0.0.0 0.5 10 v5 210 0.0 05 10 15 210 0.0 OSs 10 15 2:Exponent a for scaling inverse seqlen Exponent a for scaling inverse seqlen Exponent a for scaling inverse seqlen —e relu —*— squared relu ~e- relu with gating “a squared relu with gating Figure 4: The effect of using a gate attention unit [15] on attention with ReLU and squared ReLU scaled by L~“ where L is sequence length. Results are shown for the $/32, 8/16, and S/8 vision transformer models [10, 4] trained on ImageNet-21k. perparameters when replacing softmax. ImageNet-21k we train for 30 epochs, and in our ex periments on ImageNet-1k we train for 300 epochs. As a result, both training runs use a roughly similar number of steps of around 9e5. We use ViTs with qk-layernorm [8] as this was previously observed to be necessary to prevent instability when scaling model size. However, we ablate that this is not an important component at the scales we test. We use i21k and ilk to mean ImageNet-21k and ImageNet-1k, respectively, At initialization the elements of q and k are O(1) and so (are) will also be O(1). Activation functions such as ReLU preserve O(1),? and so a factor L~! is necessary for Ej[a;;] to be O(L~'). 4 Experiments Experimental setup. Our experiments use ImageNet-21k and ImageNet-1k [9] training config urations from the BigVision codebase [4] without modifying hyperparameters.® In our experiments on ?With the exception of squared ReLU. ’For ImageNetlk we use the base config https: //github.com/google-research/big_vision/blob/main/ and report ImageNet-1k accuracy for ImageNet-21k models by taking the top class among those that are in ImageNet-1k, without fine-tuning. When evaluating transfer performance on downstream tasks we big_vision/configs/vit_i1k.py. For ImageNet21k we use the base config https: //github.com/google-research/big_ vision/blob/main/big_vision/configs/vit_i21k. py. --- --use a 10-shot linear probe averaged over three seeds. The downstream tasks are Caltech Birds [27], Caltech101 [11], Stanford Cars [19], CIFAR-100 [20], DTD [5], ColHsit [17], Pets [23], and UC Merced [28]. Main experiment. Figure | illustrates that ReLUattention matches the scaling trends for softmax attention for ImageNet-21k training. On the x-axis we display the total core hours required for the experiment. As an advantage, ReLU-attention enables parallelization over the sequence length dimension with fewer gather operations than softmax attention. Effect of sequence length scaling. Figure 2 examines the effect of sequence length scaling for various point-wise alternatives to softmax. Concretely, we replace softmax with L~°h for a € [0,1] and h € {relu, relu”, gelu, softplus, identity}. On the z-axis we display a. The y-axis displays accuracy for the $/32, 8/16, and $/8 vision transformer models [10, 4]. The best results are typically achieved when a is close to 1. Since there is not clear best non-linearity, we use ReLU in our main experiment as it is faster. Effect of qk-layernorm. Our main experiments use qk-layernorm [8] in which queries and keys are passed through LayerNorm [1] before computing attention weights. We use qk-layernorm by default as it was found to be necessary to prevent instability when scaling up model size [8]. Figure 3 shows the effect of removing qk-layernorm. The results indicate that qk-layernorm does not have a large effect for these models, but this may change at scale. Effect of adding a gate. Previous work removing softmax adds a gated unit and does not scale by sequence length [15]. Concretely, in the gated attention unit [15] an extra projection produces output which is combined through elementwise-multiplication before the out projection. In Figure 4 we investigate whether the presence of a gate removes the need for sequence length scaling. Overall we observe that the best accuracy is still achieved with sequence length scaling, with or without the gate. Note that gating increases the core hours required for the experiment by roughly 9.3% for the S/8 model with ReLU. 5
--- CONCLUSION ---
This report leaves many open questions. In particular, we are unsure why the factor L~! improves performance or if this term could be learned. Moreover, it is likely that there is a better activation function that we do not explore. Acknowledgements We thank Lucas Beyer, Mostafa Dehghani, and David Fleet for their helpful comments and suggestions. We thank the members of the Google DeepMind PAGI team for their support of this effort, Jascha Sohldickstein, Noah Fiedel, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Avi Singh, Azade Nova, Ben Adlam, Bernd Bohnet, Daniel Freeman, Gamaleldin Elsayed, Gaurav Mishra, Hanie Sedghi, Isabelle Simpson, Izzeddin Gur, JD Co-Reyes, James Harrison, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kelvin Xu, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Max Bileschi, Merrie Morris, Roman Novak, Rosanne Liu, Sharad Vikram, Tris Warkentin, Yundi Qian. References 1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 2) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. 3] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable incontext learning with in-context algorithm selection. arXiv preprint arXiv:2306.046387, 2023. 4] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Better plain vit baselines for imagenet1k. arXiv preprint arXiv:2205.01580, 2022. URL https: //arxiv.org/abs/2205.01580. a Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR), 2014. https: //arxiv.org/abs/1311.3618. [6] George E Dahl, Tara N Sainath, and Geoffrey E Hinton. Improving deep neural networks for lvcsr using rectified linear units and dropout. InIEEE international conference on acoustics, speech and signal processing, pages 8609-8613. IEEE, 2013. (7| Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances --- --[8] [9] [10] [11]13) 14)in Neural Information Processing Systems, 35:16344— 16359, 2022. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009. https://ieeexplore. ieee. org/document/5206848. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16xwords: Transformers for image recognition at scale. n International Conference on Learning Representations (ICLR), 2021. https: //arxiv.org/abs/2010. 11929. Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested onobject categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178-178. EEE, 2004. Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei. What can a single attention layer learn? a study through the random features lens. arXiv preprint arXiv:2307.11353, 2023. Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for deep attention networks. In International Conference on Machine Learning, pages 4376-4386. PMLR, 2020. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In Hal Daumé III and Aarti Singh, editors, Proceedings of the 37th International Conference on [17] [18] [19] [20] [21][27] Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5156-5165. PMLR, 13— 18 Jul 2020. URL https: //proceedings .mlr.press/ vi19/katharopoulos20a. html. Jakob Nikolas Kather, Frank Gerrit Zollner, Francesco Bianconi, Susanne M Melchers, Lothar R Schad, Timo Gaiser, Alexander Marx, and Cleo-Aron Weis. Collection of textures in colorectal cancer histology. Zenodo https://doi. org/10, 5281, 2016. Soroush Abbasi Koohpayegani and Hamed Pirsiavash. Sima: Simple softmax-free attention for vision transformers. arXiv preprint arXiv:2206.08898, 2022. Jonathan Krause, Michael Stark, Jia Deng, and Li FeiFei. 3d object representations for fine-grained categorization. In International Conference on Computer Vision (ICCV) Workshops, 2013. https:// ieeexplore.ieee.org/document/6755945. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009. https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR. pdf. Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer, Sashank Reddi, and Sanjiv Kumar. Robust training of neural networks using scale invariant architectures. In International Conference on Machine Learning, pages 12656-12684. PMLR, 2022. Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, and Li Zhang. Soft: Softmax-free transformer with linear complexity. Advances in Neural Information Processing Systems, 34:21297-21309, 2021. Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 3498-3505. IEEE, 2012. Markus N Rabe and Charles Staats. Self-attention does not need o(n?) memory. arXiv preprint arXiv:2112.05682, 2021. Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, and Jiang Bian. A study on relu and softmax in transformer. arXiv preprint arXiv:2302.06461, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. 2010. --- --[28] Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In Proceedings of the 18th SIGSPATIAL international conference on advances in geographic information systems, pages 270-279, 2010.
