--- ABSTRACT ---
The development of deep learning software libraries enabled significant progress in the field by allowing users to focus on modeling, while letting the library to take care of the tedious and time-consuming task of optimizing execution for modern hardware accelerators. However, this has benefited only particular types of deep learning models, such as Transformers, whose primitives map easily to the vectorized computation. The models that explicitly account for structured objects, such as trees and segmentations, did not benefit equally because they require custom algorithms that are difficult to implement in a vectorized form. SynJax directly addresses this problem by providing an efficient vectorized implementation of inference algorithms for structured distributions covering alignment, tagging, segmentation, constituency trees and spanning trees. This is done by exploiting the connection between algorithms for automatic differentiation and probabilistic inference. With SynJax we can build large-scale differentiable models that explicitly model structure in the data. The code is available at https: //github.com/google-deepmind/synjax. 1
--- CONCLUSION ---
One of the main challenges in creating deep neural models over structured distributions is the difficulty of their implementation on modern hardware accelerators. SynJax addresses this problem and makes large scale training of structured models feasible and easy in JAX. We hope that this will encourage research into finding alternatives to auto-regressive modeling of structured data. Limitations SynJax is quite fast, but there are still some areas where the improvements could be made. One of the main speed and memory bottlenecks is usage of big temporary tensors in the dynamic programming algorithms needed for computation of log-partition function. This could be optimized with custom kernels written in Pallas.? There are some speed gains *https://jax.readthedocs. io/en/latest/pallas that would conceptually be simple but they depend on having a specialized hardware. For instance, matrix multiplication with semirings currently does not use hardware acceleration for matrix multiplication, such as TensorCore on GPU, but instead does calculation with regular CUDA cores. We have tried to address this with log-einsum-exp trick (Peharz et al., 2020) but the resulting computation was less numerically precise than using a regular log-semiring with broadcasting. Maximum spanning tree algorithm would be much faster if it could be vectorized — currently it’s executing as an optimized Numba CPU code. Acknowledgements We are grateful to Chris Dyer, Aida Nematzadeh and other members of language team in Google DeepMind for early comments on the draft of this work. We appreciate Patrick Kidger’s work on Equinox and Jaxtyping that made development of SynJax much easier. We also appreciate that Sasha Rush open-sourced Torch-Struct, a library that influenced many aspects of SynJax. References Ossama Abdel-Hamid, Li Deng, Dong Yu, and Hui Jiang. 2013. Deep segmental neural networks for speech recognition. In Interspeech, volume 36. Wilker Aziz. 2015. Grasp: Randomised Semiring Parsing. Prague Bulletin of Mathematical Linguistics, 104:51-62. Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski, Thomas Keck, lurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Laurent Sartran, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan, MiloS Stanojevi¢, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. 2020. The DeepMind JAX Ecosystem. Jasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019. Interpretable neural predictions with differentiable binary variables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2963-2977, Florence, Italy. Association for Computational Linguistics. Jasmijn Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’ an. 2017. Graph convolutional encoders for syntax-aware neural machine --- --translation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1957-1967, Copenhagen, Denmark. Association for Computational Linguistics. Yonatan Bisk and Ke Tran. 2018. Inducing grammars with and for neural machine translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 25-35, Melbourne, Australia. Association for Computational Linguistics. Oscar Chang, Dongseong Hwang, and Olivier Siohan. 2023. Revisiting the Entropy Semiring for Neural Speech Recognition. In The Eleventh International Conference on Learning Representations. Peter Chang, Giles Harper-Donnelly, Aleyna Kara, Xinglong Li, Scott Linderman, and Kevin Murphy. 2022. Dynamax. Do Kook Choe and Eugene Charniak. 2016. Parsing as language modeling. In Proceedings of theConference on Empirical Methods in Natural Language Processing, pages 2331-2336, Austin, Texas. Association for Computational Linguistics. Shay B. Cohen, Giorgio Satta, and Michael Collins. 2013. Approximate PCFG parsing using tensor decomposition. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 487-496, Atlanta, Georgia. Association for Computational Linguistics. Charles J. Colbourn, Wendy J. Myrvold, and Eugene Neufeld. 1996. Two Algorithms for Unranking Arborescences. Journal of Algorithms, 20(2):268-281. Caio Corro and Ivan Titov. 2019. Differentiable Perturband-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder. In International Conference on Learning Representations. Corinna Cortes, Mehryar Mohri, Ashish Rastogi, and Michael Riley. 2008. On the computation of the relative entropy of probabilistic automata. International Journal of Foundations of Computer Science, 19(01):219-242. David F Crouse. 2016. On implementing 2D rectangular assignment algorithms. [EEE Transactions on Aerospace and Electronic Systems, 52(4):1679-1696. Marco Cuturi and Mathieu Blondel. 2017. Soft-DTW: A Differentiable Loss Function for Time-Series. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’ 17, page 894-903. JMLR.org. Adnan Darwiche. 2003. A Differential Approach to Inference in Bayesian Networks. J. ACM, 50(3):280-305. Nathan Day, Andrew Hemmaplardh, Robert E. Thurman, John A. Stamatoyannopoulos, and William S. Noble. 2007. Unsupervised segmentation of continuous genomic data. Bioinformatics, 23(11):1424— 1426. Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, and Rif A. Saurous. 2017. TensorFlow Distributions. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. 2016. Recurrent neural network grammars. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 199-209, San Diego, California. Association for Computational Linguistics. Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In COLING 1996 Volume 1: The 16th International Conference on Computational Linguistics. Jason Eisner. 2002. Parameter estimation for probabilistic finite-state transducers. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 1-8, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Jason Eisner. 2016. Inside-Outside and ForwardBackward Algorithms Are Just Backprop (tutorial paper). In Proceedings of the Workshop on Structured Prediction for NLP, pages 1-17, Austin, TX. Association for Computational Linguistics. Yao Fu, Chuanqi Tan, Bin Bi, Mosha Chen, Yansong Feng, and Alexander M. Rush. 2020. Latent template induction with gumbel-crfs. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY, USA. Curran Associates Inc. Joshua Goodman. 1999. Semiring Parsing. Computational Linguistics, 25(4):573-606. Alex Graves, Santiago Fernandez, Faustino Gomez, and Jiirgen Schmidhuber. 2006. Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks. In Proceedings of the 23rd International Conference on Machine Learning, pages 369-376. Andreas Griewank. 1992. Achieving logarithmic growth of temporal and spatial complexity in reverse automatic differentiation. Optimization Methods and Software, 1(1):35-54. Awni Hannun. 2017. Sequence Modeling with CTC. Distill. https://distill.pub/2017/cte. Serhii Havrylov, German Kruszewski, and Armand Joulin. 2019. Cooperative learning of disjoint syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume I (Long and Short Papers), pages 1118-1128, Minneapolis, Minnesota. Association for Computational Linguistics. --- --Julia Hockenmaier, Aravind K Joshi, and Ken A Dill. 2007. Routes are trees: the parsing perspective on protein folding. Proteins: Structure, Function, and Bioinformatics, 66(1):1-15. Liang Huang, He Zhang, Dezhong Deng, Kai Zhao, Kaibo Liu, David A Hendrix, and David H Mathews. 2019. LinearFold: linear-time approximate RNA folding by 5’-to-3’ dynamic programming and beam search. Bioinformatics, 35(14):i295—-1304. Rebecca Hwa. 2000. Sample selection for statistical grammar induction. In 2000 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 45-52, Hong Kong, China. Association for Computational Linguistics. Patrick Kidger and Cristian Garcia. 2021. Equinox: neural networks in JAX via callable PyTrees and filtered transformations. Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. 2017. Structured attention networks. In International Conference on Learning Representations. Yoon Kim, Chris Dyer, and Alexander Rush. 2019. Compound probabilistic context-free grammars for grammar induction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2369-2385, Florence, Italy. Association for Computational Linguistics. Nikita Kitaev, Steven Cao, and Dan Klein. 2019. Multilingual constituency parsing with self-attention and pre-training. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3499-3505, Florence, Italy. Association for Computational Linguistics. Terry Koo, Amir Globerson, Xavier Carreras, and Michael Collins. 2007. Structured Prediction Models via the Matrix-Tree Theorem. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 141-150, Prague, Czech Republic. Association for Computational Linguistics. Marco Kuhlmann, Carlos Gémez-Rodriguez, and Giorgio Satta. 2011. Dynamic programming algorithms for transition-based dependency parsers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 673-682, Portland, Oregon, USA. Association for Computational Linguistics. John D. Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of the Eighteenth International Conference on Machine Learning, CML ’01, page 282-289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. 2015. Numba: A Ilvm-based python jit compiler. In Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC, LLVM °15, New York, NY, USA. Association for Computing Machinery. Zhifei Li and Jason Eisner. 2009. First- and secondorder expectation semirings with applications to minimum-risk training on translation forests. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 40-51, Singapore. Association for Computational Linguistics. Liang Lu, Lingpeng Kong, Chris Dyer, Noah A Smith, and Steve Renals. 2016. Segmental recurrent neural networks for end-to-end speech recognition. In Proceedings of the 17th Annual Conference of the International Speech Communication Association (INTERSPEECH 2016). Chunchuan Lyu and Ivan Titov. 2018. AMR parsing as graph prediction with latent alignment. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 397-407, Melbourne, Australia. Association for Computational Linguistics. Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064-1074, Berlin, Germany. Association for Computational Linguistics. André Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34— 44, Cambridge, MA. Association for Computational Linguistics. Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. 2018. Learning Latent Permutations with Gumbel-Sinkhorn Networks. In International Conference on Learning Representations. Arthur Mensch and Mathieu Blondel. 2018. Differentiable dynamic programming for structured prediction and attention. In Proceedings of the 35th International Conference on Machine Learning, volumeof Proceedings of Machine Learning Research, pages 3462-3471. PMLR. Tsvetomila Mihaylova, Vlad Niculae, and André F. T. Martins. 2020. Understanding the Mechanics of SPIGOT: Surrogate Gradients for Latent Structure Learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2186-2202, Online. Association for Computational Linguistics. --- --Kevin P. Murphy. 2012. Machine Learning: A Probabilistic Perspective. Adaptive Computation and Machine Learning Series. The MIT Press. Maria Nadejde, Siva Reddy, Rico Sennrich, Tomasz Dwojak, Marcin Junczys-Dowmunt, Philipp Koehn, and Alexandra Birch. 2017. Predicting target language CCG supertags improves neural machine translation. In Proceedings of the Second Conference on Machine Translation, pages 68-79, Copenhagen, Denmark. Association for Computational Linguistics. S. B. Needleman and C. D. Wunsch. 1970. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of Molecular Biology, 48:443-453. Mathias Niepert, Pasquale Minervini, and Luca Franceschi. 2021. Implicit mle: backpropagating through discrete exponential family distributions. Advances in Neural Information Processing Systems, 34:14567-14579. Max Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris J Maddison. 2020. Gradient estimation with stochastic softmax tricks. In Advances in Neural Information Processing Systems, volume 33, pages 5691-5704. Curran Associates, Inc. Robert Peharz, Steven Lang, Antonio Vergari, Karl Stelzner, Alejandro Molina, Martin Trapp, Guy Van Den Broeck, Kristian Kersting, and Zoubin Ghahramani. 2020. Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits. In Proceedings of the 37th International Conference on Machine Learning, \CML’20. JMLR.org. Du Phan, Neeraj Pradhan, and Martin Jankowiak. 2019. Composable Effects for Flexible and Accelerated Probabilistic Programming in NumPyro. In Program Transformations for ML Workshop at NeurIPS 2019. Marin Vlastelica Poganéi¢, Anselm Paulus, Vit Musil, Georg Martius, and Michal Rolinek. 2020. Differentiation of Blackbox Combinatorial Solvers. In International Conference on Learning Representations. Lawrence R. Rabiner. 1990. A tutorial on hidden markov models and selected applications in speech recognition. In Readings in Speech Recognition, pages 267-296. Elsevier. Alexander Rush. 2020. Torch-Struct: Deep Structured Prediction Library. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 335-342, Online. Association for Computational Linguistics. Sakakibara, Underwood, Mian, and Haussler. 1994. Stochastic Context-Free Grammars for Modeling RNA. In 1994 Proceedings of the Twenty-Seventh Hawaii International Conference on System Sciences, volume 5, pages 284-293. IEEE. Sunita Sarawagi and William W Cohen. 2004. Semimarkov conditional random fields for information extraction. In Advances in Neural Information Processing Systems, volume 17. MIT Press. Laurent Sartran, Samuel Barrett, Adhiguna Kuncoro, Milo& Stanojevi¢, Phil Blunsom, and Chris Dyer. 2022. Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale. Transactions of the Association for Computational Linguistics, 10:1423-1439. David A. Smith and Noah A. Smith. 2007. Probabilistic Models of Nonprojective Dependency Trees. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 132-140, Prague, Czech Republic. Association for Computational Linguistics. Milo’ Stanojevié. 2022. Unbiased and efficient sampling of dependency trees. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1691-1706, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Milo’ Stanojevi¢ and Shay B. Cohen. 2021. A Root of a Problem: Optimizing Single-Root Dependency Parsing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10540-10557, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Mitchell Stern, Jacob Andreas, and Dan Klein. 2017. A minimal span-based neural constituency parser. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 818-827, Vancouver, Canada. Association for Computational Linguistics. Charles Sutton and Andrew McCallum. 2007. An Introduction to Conditional Random Fields for Relational Learning. Introduction to statistical relational learning, page 93. Simo Sirkka and Angel F. Garefa-Ferndndez. 2021. Temporal Parallelization of Bayesian Smoothers. IEEE Transactions on Automatic Control, 66(1):299— 306. R. E. Tarjan. 1977. Finding optimum branchings. Networks, 7(1):25-35. Ke M. Tran, Yonatan Bisk, Ashish Vaswani, Daniel Marcu, and Kevin Knight. 2016. Unsupervised neural hidden Markov models. In Proceedings of the Workshop on Structured Prediction for NLP, pages 63-71, Austin, TX. Association for Computational Linguistics. W. T. Tutte. 1984. Graph Theory, volume 21 of Encyclopedia of Mathematics and Its Applications. AddisonWesley, Menlo Park, CA. --- --L.G. Valiant. 1979. The complexity of computing the permanent. Theoretical Computer Science, 8(2):189201. Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261-272. Zhiyong Wang and Jinbo Xu. 2011. A conditional random fields method for RNA sequence-structure relationship modeling and conformation sampling. Bioinformatics, 27(13):i102-i110. Ronald J. Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Mach. Learn., 8(3—-4):229-256. David Bruce Wilson. 1996. Generating Random Spanning Trees More Quickly than the Cover Time. In Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, STOC ’96, page 296-303, New York, NY, USA. Association for Computing Machinery. Songlin Yang, Wei Liu, and Kewei Tu. 2022. Dynamic programming in rank space: Scaling structured inference with low-rank HMMs and PCFGs. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4797-4809, Seattle, United States. Association for Computational Linguistics. Guangyao Zhou, Antoine Dedieu, Nishanth Kumar, Wolfgang Lehrach, Miguel Lazaro-Gredilla, Shrinu Kushagra, and Dileep George. 2023. Pgmax: Factor graphs for discrete probabilistic graphical models and loopy belief propagation in jax. Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2020. Please Mind the Root: Decoding Arborescences for Dependency Parsing. In Proceedings of theConference on Empirical Methods in Natural Language Processing (EMNLP), pages 4809-4819, Online. Association for Computational Linguistics. Ran Zmigrod, Tim Vieira, and Ryan Cotterell. 2021. Efficient Computation of Expectations under Spanning Tree Distributions. 9:675-690. Distribution parameters CTC b= 64, n=124, 1=Alignment CRF b= 16, n=256, m=Semi-Markov CRF b= 1, n= 64, nt= 32, k=Tree CRF b= 128, n=128, nt=Linear-Chain CRF b= 128, n=256, nt=PCFG b= 1, n= 48, nt= 64, pt=HMM b= 1, n=128, nt=Non-Projective CRF | b=1024, n=Projective CRF b= 128, n=Table 2: Sizes of tested distributions. A Empirical comparisons A.1 Comparison with Torch-Struct We compared against the most recent Torch-Struct? commit from 30 Jan 2022. To make Torch-Struct run faster we have also installed its specialized kernel for semiring matrix multiplication genbmm* from its most recent commit from 11 Oct 2021. While Torch-Struct supports some of the same distributions as SynJax we did not manage to do speed comparison over all of them. For example, AlignmentCRF of Torch-Struct was crashing due to mismatch of PyTorch, Torch-Struct and genbmm changes about in-place updates. We compile SynJax with jax. jit and during benchmarking do not count the time that is taken for compilation because it needs to be done only once. We also tried to compile Torch-Struct using TorchScript by tracing but that did not work out of the box. Comparisons are done on A100 GPU on Colab Pro+. The results are shown in Table | in the main text. Table 2 shows sizes of the distributions being tested. A.2. Comparison with Zmigrod et al. Non-Projective spanning trees present a particular challenge because they cannot be vectorized easily due to dynamic structures that are involved in the algorithm. The main algorithms and libraries for parsing this type of trees are from Zmigrod et al. (2020)° and Stanojevié and Cohen (2021)°. The first one is expressed as a recursive algorithm, while the second one operates over arrays of fixed size in iterative way. This makes Stanojevi¢ and Cohen algorithm much more amendable to Numba optimization. We took that code and just annotated https: //github.com/harvardnlp/pytorch-struct *https://github.com/harvardn1p/genbmm Shttps://github.com/rycolab/spanningtrees https: //github.com/stanojevic/ Fast-MST-Algorithm --- --it with Numba primitives. This made the algorithm significantly faster, especially for big graphs, as can be seen from Figure 3. o.010 library Ses StanojeviéCohen Se Zmigrod-et-al oe synlaxnodes in the graph Figure 3: Speed comparison of Non-Projective Spanning Tree libraries. A.3 Comparison of Maximum Projective Spanning Tree Algorithms Eisner’s algorithm is virtually the only projective parsing algorithm actively used, if we do not count the transition based parsers. We have found that replacing Eisner’s algorithm with Kuhlmann et al. (2011) tabulation of arc-hybrid algorithm can provide large speed gains both on CPU and GPU. See Figure 4. In this implementation graph size does not make a big difference because it is implemented in a vectorized way so most operations are parallelized. ot T library a op - 1 ~ ~ ~ Se CPU Synjax Eisner nodes in the graph Figure 4: Speed comparison of Projective Maximum Spanning Tree algorithms.
