--- ABSTRACT ---
Developmental psychologists have spent decades devising experiments to test the intelligence and knowledge of infants and children, tracing the origin of crucial concepts and capacities. Moreover, experimental techniques in developmental psychology have been carefully designed to discriminate the cognitive capacities that underlie particular behaviors. We propose this metric as a tool to aid in investigating Large Language Models’ (LLM) capabilities in the context of ethics and morality. Results from key developmental psychology experiments have historically been applied to discussions of children’s emerging moral abilities, making this work a pertinent benchmark for exploring such concepts in LLMs. We propose that using classical experiments from child development is a particularly effective way to probe the computational abilities of AI models in general and LLMs in particular. First, the methodological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on other kinds of information, such as information from exploration of the physical world. In this work we adapt classical developmental experiments to evaluate the capabilities of LaAMDA, a LLM from Google. We propose a novel LLM Response Score (LRS) metric which can be used to evaluate other language models, such as GPT. We find that LaMDA generates appropriate responses that are similar to those of children in experiments involving social and proto-moral understanding, perhaps providing evidence that knowledge of these domains is discovered through language. On the other hand, LaMDA’s responses in early object and action understanding, theory of mind, and especially causal reasoning tasks are very different from those of young children, perhaps showing that these domains require more real-world, self-initiated exploration and cannot simply be learned from patterns in language input. 1
--- INTRODUCTION ---
: In 1950 Alan Turing famously said “Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulates the child’s? If this were then subjected to an appropriate course of education one would obtain the adult brain {Turing} [1946)).” Developmental psychologists have spent decades devising experiments to determine the intelligence and knowledge of infants and children. This field of study has revealed types of knowledge that are in place well before formal education and serve as the the foundation for further human intelligence NeurIPS MP2 (Morality and AI) Workshop--- --). This research allows us to track a child’s cognitive development trajectory and discern e underlying cognitive capacities behind behaviors, which can stem from conceptual structures, associations, external interactions, or cultural language transmission. Using classical experiments from child development may be a particularly effective way to probe the understanding of AI models in general and LLMs in particular (2023}). First, the methodological techniques of developmental psychology such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations can be very helpful for assessing LLMs (Frank][2023]). We could not accurately judge a child’s cognitive capacities simply through conversation, though this extrapolation often been made with LLMs. Thus, systematic human developmental methods can help bridge our understanding of LLMs. We find that LaMDA generates appropriate responses that are similar to children in experiments involving social and moral understanding, perhaps providing evidence that the core of these domains is discovered or accessible through language. However, LaMDA’s responses in Perception, Theory of Mind, and especially Causal Reasoning tasks are very different from those of young children, perhaps showing that these domains require more real-world, self-initiated exploration and cannot simply be learned from patterns in language input. These results suggest that capacities linked to morality may be rooted in language. We explored two hypotheses regarding the relationship between LaMDA’s responses and children’s responses. First, we considered whether LaMDA aligns with the human developmental trajectory by assessing its performance on tasks mastered at different stages of life. Second, we investigated whether LaMDA might excel in domains that can more easily be learned from language alone but struggle in exploration-based domains. In acknowledging and understanding the divergences between human and AI learning trajectories and patterns, we may be better equipped to train more optimal models and glean an understanding of what may make human cognition uniquely human. We propose this metric as a tool to help us more fully understand LLMs’ capabilities in the context of ethics and morality. In order to understand the deeper notions of LLMs, including their ability to adhere to moral and ethical norms, we may use key experiments from the child development literature as a guiding benchmark. The depth and breadth of seminal work in developmental psychology has historically been used in part to understand how children develop aspects of morality, thus providing a potential basis for studying these capabilities in machines as well. Previous work has shown that priming LLMs to consider morality in a conversational instance may lead to AI models that can learn to adhere to moral norms and even assess themselves for bias (Ganguli and Kaplan) [2023}). 2
--- RELATED WORK ---
: Previous work that has tested LLMs, specifically GPT-3, has found conflicting evidence of theory of mind (Ullman|{2023],|Kosinski (2023}, Sap and Choi 2022]) in these models. Previous work has also demonstrated that GPT-3 deeply struggles with causal reasoning-based tasks, though it performs well on other vignette tasks (Binz and Schulz}|2023]). One issue that arises in these studies is that LLMs may simply reference published research papers; for example, finding the false-belief task in many published papers on the internet, and so responding to it appropriately (Perner et al.| (1987). Again, this emphasizes the importance of
--- METHOD ---
ological techniques of developmental psychology, such as the use of novel stimuli to control for past experience or control conditions to determine whether children are using simple associations, can be equally helpful for assessing the capacities of LLMs. In parallel, testing LLMs in this way can tell us whether the information that is encoded in text is sufficient to enable particular responses, or whether those responses depend on other kinds of information, such as information from exploration of the physical world. In this work we adapt classical developmental
