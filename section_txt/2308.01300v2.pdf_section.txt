--- ABSTRACT ---
Motivated by the remarkable achievements of DETR-based approaches on COCO object detection and segmentation benchmarks, recent endeavors have been directed towards elevating their performance through selfsupervised pre-training of Transformers while preserving a frozen backbone. Noteworthy advancements in accuracy have been documented in certain studies. Our investigation delved deeply into a representative approach, DETReg, and its performance assessment in the context of emerging models like -Deformable-DETR. Regrettably, DETReg proves inadequate in enhancing the performance of robust DETR-based models under full data conditions. To dissect the underlying causes, we conduct extensive experiments on COCO and PASCAL VOC probing elements such as the selection of pre-training datasets and strategies for pretraining target generation. By contrast, we employ an optimized approach named Simple Self-training which leads to marked enhancements through the combination of an improved box predictor and the Objects365 benchmark. The culmination of these endeavors results in a remarkable AP score of 59.3% on the COCO val set, outperforming HDeformable-DETR + Swin-L without pre-training by 1.4%. Moreover, a series of synthetic pre-training datasets, generated by merging contemporary image-to-text(LLaVA) and text-to-image (SDXL) models, significantly amplifies object detection capabilities. Keywords Object detection, DETR, Pre-training 1
--- INTRODUCTION ---
Recently, the DETR-based approaches (Carion et al., 2020; Jia et al., 2023; Li et al., 2023; Zhang et al., 2022; Zhu et al., 2020) have achieved significant progress and 3Xi’an Jiaotong5CUHK ?Peking University 4University of Pennsylvania & yuhui.yuan@ microsoft.com \University of Toronto Liverpool University ®Microsoft Research Asia pushed the frontier on both object detection and segmentation tasks. For example, DINO-DETR (Zhang et al., 2022), H-Deformable-DETR (Jia et al., 2023), and GroupDETRv2 (Chen et al., 2022) have set new state-of-the-art object detection performance on COCO benchmark. MaskDINO (Li et al., 2023) further extends DINO-DETR and establishes the best results across COCO instance segmentation and panoptic segmentation tasks. To some degree, this is the first time that end-to-end transformer approaches can achieve an even better performance than the conventional heavily tuned strong detectors (Li et al., 2021; Liu et al., 2022b) based on convolution, e.g., Cascade Mask-RCNN and HTC++. Despite the great success of these DETR-based approaches, they still choose a randomly initialized Transformer and thus fail to unleash the potential of a fully pretrained detection architecture like (Wei et al., 2021), which already verifies the benefits of aligning the pre-training architecture with the downstream architecture. Figure la and 1b illustrate the distribution of the number of parameters and GFLOPs within a standard Deformable-DETR network based on ResNet50 backbone. We can see that the Transformer encoder and decoder occupy 34% of the parameters and 65% of the GFLOPs, which means there exists much room for improvement along the path of performing pretraining on the Transformer part within DETR. Several recent works have improved DETR-based object detection models by performing self-supervised pretraining on the Transformer encoder and decoder while freezing the backbone. For example, UP-DETR (Dai et al., 2021) pre-trains Transformer to detect random patches in an image, DETReg (Bar et al., 2022) pre-trains Transformer to match object locations and features with priors generated from Selective Search algorithm, and most recently, Siamese DETR locates the target boxes with the query features extracted from a different view’s corresponding box. However, these works utilize either the vanilla DETR network (AP=42.1% in terms of object detection performance --- --Maetal. (a) #parameters (b) #GFLOPs 1 1 . 60 mam Baseline |+ mam DETReg =e. 49.6 49.a 50F < 43.7 45.2 45.42.40 | DETR Deformable-DETR H-Deformable-DETR (c) COCO object detection results of DETReg. Fig. 1: The distribution of the number of parameters and GFLOPs within Deformable-DETR network with a ResNetbackbone, and the pre-training performance of DETReg. As shown in (a) and (b), we can see that around 34% parameters and 65% GFLOPs are distributed in the randomly initialized Transformer encoder and decoder. According to (c), DETReg only improves the vanilla DETR and Deformable-DETR by +1.6% and +0.3% while showing no gains over the stronger H-Deformable-DETR. on COCO) or the Deformable-DETR variant (AP=45.2%). Their results fall significantly short when pre-training on the latest much stronger DETR model like H-DeformableDETR (Jia et al., 2023) (AP=49.6%). In Figure Ic, we present the object detection results of different DETR models on COCO under two conditions: without pre-training of the Transformer component (referred to as the baseline) and with pre-training using the DETReg method. In both cases, the backbones of these models are ResNet50 initialized with SwAV (Caron et al., 2020). Notably, in the case of the #-Deformable-DETR, the utilization of the DETReg pre-training actually leads to a performance decrease rather than an improvement. In this work, we first take a closer look at how much self-supervised pre-training methods, exemplified by DETReg, can improve over the increasingly potent DETR models on COCO object detection benchmark. Our investigation unveils a significant limitation in the efficacy of DETReg when applied to fortified DETR networks bolstered by improvements like the SwAV pre-trained backbone, deformable techniques in Deformable-DETR, and the hybrid matching scheme in H-Deformable-DETR. We pinpoint the crux of the issue as originating from unreliable box proposals generated by unsupervised methods like Selective Search, which contribute to noisy localization targets, and the weak semantic information provided through feature reconstruction which is not an efficient classification target either. These drawbacks make the self-supervised pretraining methods ineffective when applied to an already strong DETR model. To fix this, we propose to use a COCO object detector to get more accurate pseudo-boxes with informative pseudo-class labels. Extensive ablation experiments underscore the impact of three pivotal factors: the choice of pre-training datasets (ImageNet vs. Objects365), localization pre-training targets (Selective Search proposals vs. pseudo-box predictions), and classification pre-training targets (object-embedding vs. pseudo-class predictions). Our findings reveal that a Simple Self-training scheme, employing pseudo-box and pseudo-class predictions as pre-training targets, outperforms the DETReg approach in various settings. Notably, this simple design yields discernible pre-training enhancements even for the state-of-theart DETR network without accessing the pre-training benchmark’s ground-truth label. For example, with a ResNetbackbone and the Objects365 pre-training dataset, Simple Self-training elevates DETReg’s COCO object detection results on H-Deformable-DETR by 3.6%. Furthermore, a remarkable performance is observed with the Swin-L backbone, yielding competitive results 59.3%. Additionally, we delve into an exploration of contemporary image-to-text and text-to-image generation models, aiming to create a sequence of synthetic datasets for object detection pre-training. Empirically, our observations yield encouraging outcomes, as pre-training with these synthetic datasets demonstrates commendable performance even when compared against the widely adopted Objectsbenchmark, which entails substantial annotation costs. In general, our efforts are poised to provide a more authentic assessment of the progress in the formidable task of DETR pre-training. 2
--- RELATED WORK ---
DETR for object detection. Since the emergence of DETR (Carion et al., 2020) as the first fully end-to-end object detector, many works have extended DETR with novel techniques to achieve state-of-the-art results on various vision tasks. To accelerate the convergence of the original DETR, Deformable-DETR (Zhu et al., 2020) proposes a novel multi-scale deformable self/cross-attention to focus on a Sparse set of important sampling points around a reference point. Furthermore, based on DAB-DETR (Liu et al., 2022a) with a different query formulation, DINODETR (Zhang et al., 2022) introduces a query denoising scheme and sets new records on object detection tasks. Besides, to address the training efficiency bottleneck caused by one-to-one matching in DETR, H-Deformable-DETR (Jia --- --Revisiting DETR Pre-training for Object Detection et al., 2023) and Group-DETR (Chen et al., 2022) propose to train with more queries in the transformer decoder with an additional one-to-many matching scheme, which helps to achieve even faster convergence and better performance. Self-supervised pre-training. Self-supervised learning (SSL) has achieved remarkable results in image classification
--- METHOD ---
. In both cases, the backbones of these models are ResNet50 initialized with SwAV (Caron et al., 2020). Notably, in the case of the #-Deformable-DETR, the utilization of the DETReg pre-training actually leads to a performance decrease rather than an improvement. In this work, we first take a closer look at how much self-supervised pre-training methods, exemplified by DETReg, can improve over the increasingly potent DETR models on COCO object detection benchmark. Our investigation unveils a significant limitation in the efficacy of DETReg when applied to fortified DETR networks bolstered by improvements like the SwAV pre-trained backbone, deformable techniques in Deformable-DETR, and the hybrid matching scheme in H-Deformable-DETR. We pinpoint the crux of the issue as originating from unreliable box proposals generated by unsupervised methods like Selective Search, which contribute to noisy localization targets, and the weak semantic information provided through feature reconstruction which is not an efficient classification target either. These drawbacks make the self-supervised pretraining methods ineffective when applied to an already strong DETR model. To fix this, we propose to use a COCO object detector to get more accurate pseudo-boxes with informative pseudo-class labels. Extensive ablation
--- EXPERIMENT ---
s on COCO and PASCAL VOC probing elements such as the selection of pre-training datasets and strategies for pretraining target generation. By contrast, we employ an optimized approach named Simple Self-training which leads to marked enhancements through the combination of an improved box predictor and the Objects365 benchmark. The culmination of these endeavors results in a remarkable AP score of 59.3% on the COCO val set, outperforming HDeformable-DETR + Swin-L without pre-training by 1.4%. Moreover, a series of synthetic pre-training datasets, generated by merging contemporary image-to-text(LLaVA) and text-to-image (SDXL) models, significantly amplifies object detection capabilities. Keywords Object detection, DETR, Pre-training 1 Introduction Recently, the DETR-based approaches (Carion et al., 2020; Jia et al., 2023; Li et al., 2023; Zhang et al., 2022; Zhu et al., 2020) have achieved significant progress and 3Xi’an Jiaotong5CUHK ?Peking University 4University of Pennsylvania & yuhui.yuan@ microsoft.com \University of Toronto Liverpool University ®Microsoft Research Asia pushed the frontier on both object detection and segmentation tasks. For example, DINO-DETR (Zhang et al., 2022), H-Deformable-DETR (Jia et al., 2023), and GroupDETRv2 (Chen et al., 2022) have set new state-of-the-art object detection performance on COCO benchmark. MaskDINO (Li et al., 2023) further extends DINO-DETR and establishes the best results across COCO instance segmentation and panoptic segmentation tasks. To some degree, this is the first time that end-to-end transformer approaches can achieve an even better performance than the conventional heavily tuned strong detectors (Li et al., 2021; Liu et al., 2022b) based on convolution, e.g., Cascade Mask-RCNN and HTC++. Despite the great success of these DETR-based approaches, they still choose a randomly initialized Transformer and thus fail to unleash the potential of a fully pretrained detection architecture like (Wei et al., 2021), which already verifies the benefits of aligning the pre-training architecture with the downstream architecture. Figure la and 1b illustrate the distribution of the number of parameters and GFLOPs within a standard Deformable-DETR network based on ResNet50 backbone. We can see that the Transformer encoder and decoder occupy 34% of the parameters and 65% of the GFLOPs, which means there exists much room for improvement along the path of performing pretraining on the Transformer part within DETR. Several recent works have improved DETR-based object detection models by performing self-supervised pretraining on the Transformer encoder and decoder while freezing the backbone. For example, UP-DETR (Dai et al., 2021) pre-trains Transformer to detect random patches in an image, DETReg (Bar et al., 2022) pre-trains Transformer to match object locations and features with priors generated from Selective Search algorithm, and most recently, Siamese DETR locates the target boxes with the query features extracted from a different view’s corresponding box. However, these works utilize either the vanilla DETR network (AP=42.1% in terms of object detection performance --- --Maetal. (a) #parameters (b) #GFLOPs 1 1 . 60 mam Baseline |+ mam DETReg =e. 49.6 49.a 50F < 43.7 45.2 45.42.40 | DETR Deformable-DETR H-Deformable-DETR (c) COCO object detection results of DETReg. Fig. 1: The distribution of the number of parameters and GFLOPs within Deformable-DETR network with a ResNetbackbone, and the pre-training performance of DETReg. As shown in (a) and (b), we can see that around 34% parameters and 65% GFLOPs are distributed in the randomly initialized Transformer encoder and decoder. According to (c), DETReg only improves the vanilla DETR and Deformable-DETR by +1.6% and +0.3% while showing no gains over the stronger H-Deformable-DETR. on COCO) or the Deformable-DETR variant (AP=45.2%). Their results fall significantly short when pre-training on the latest much stronger DETR model like H-DeformableDETR (Jia et al., 2023) (AP=49.6%). In Figure Ic, we present the object detection results of different DETR models on COCO under two conditions: without pre-training of the Transformer component (referred to as the baseline) and with pre-training using the DETReg method. In both cases, the backbones of these models are ResNet50 initialized with SwAV (Caron et al., 2020). Notably, in the case of the #-Deformable-DETR, the utilization of the DETReg pre-training actually leads to a performance decrease rather than an improvement. In this work, we first take a closer look at how much self-supervised pre-training methods, exemplified by DETReg, can improve over the increasingly potent DETR models on COCO object detection benchmark. Our investigation unveils a significant limitation in the efficacy of DETReg when applied to fortified DETR networks bolstered by improvements like the SwAV pre-trained backbone, deformable techniques in Deformable-DETR, and the hybrid matching scheme in H-Deformable-DETR. We pinpoint the crux of the issue as originating from unreliable box proposals generated by unsupervised methods like Selective Search, which contribute to noisy localization targets, and the weak semantic information provided through feature reconstruction which is not an efficient classification target either. These drawbacks make the self-supervised pretraining methods ineffective when applied to an already strong DETR model. To fix this, we propose to use a COCO object detector to get more accurate pseudo-boxes with informative pseudo-class labels. Extensive ablation experiments underscore the impact of three pivotal factors: the choice of pre-training datasets (ImageNet vs. Objects365), localization pre-training targets (Selective Search proposals vs. pseudo-box predictions), and classification pre-training targets (object-embedding vs. pseudo-class predictions). Our findings reveal that a Simple Self-training scheme, employing pseudo-box and pseudo-class predictions as pre-training targets, outperforms the DETReg approach in various settings. Notably, this simple design yields discernible pre-training enhancements even for the state-of-theart DETR network without accessing the pre-training benchmark’s ground-truth label. For example, with a ResNetbackbone and the Objects365 pre-training dataset, Simple Self-training elevates DETReg’s COCO object detection results on H-Deformable-DETR by 3.6%. Furthermore, a remarkable performance is observed with the Swin-L backbone, yielding competitive results 59.3%. Additionally, we delve into an exploration of contemporary image-to-text and text-to-image generation models, aiming to create a sequence of synthetic datasets for object detection pre-training. Empirically, our observations yield encouraging outcomes, as pre-training with these synthetic datasets demonstrates commendable performance even when compared against the widely adopted Objectsbenchmark, which entails substantial annotation costs. In general, our efforts are poised to provide a more authentic assessment of the progress in the formidable task of DETR pre-training. 2 Related Work DETR for object detection. Since the emergence of DETR (Carion et al., 2020) as the first fully end-to-end object detector, many works have extended DETR with novel techniques to achieve state-of-the-art results on various vision tasks. To accelerate the convergence of the original DETR, Deformable-DETR (Zhu et al., 2020) proposes a novel multi-scale deformable self/cross-attention to focus on a Sparse set of important sampling points around a reference point. Furthermore, based on DAB-DETR (Liu et al., 2022a) with a different query formulation, DINODETR (Zhang et al., 2022) introduces a query denoising scheme and sets new records on object detection tasks. Besides, to address the training efficiency bottleneck caused by one-to-one matching in DETR, H-Deformable-DETR (Jia --- --Revisiting DETR Pre-training for Object Detection et al., 2023) and Group-DETR (Chen et al., 2022) propose to train with more queries in the transformer decoder with an additional one-to-many matching scheme, which helps to achieve even faster convergence and better performance. Self-supervised pre-training. Self-supervised learning (SSL) has achieved remarkable results in image classification methods such as MoCo (He et al., 2020), SimCLR (Chen et al., 2020), and BYOL (Grill et al., 2020). However, SSL on object detection has shown limited transferability. To overcome this challenge, many works have proposed pretext tasks that leverage region or pixel localization cues to enhance the pre-training signals. For example, InsLoc (Yang et al., 2021a) uses contrastive learning on foreground patches to learn instance localization. UniVIP (Liet al., 2022) exploits scene similarity, scene-instance correlation, and instance discrimination to capture semantic affinity. CP? (Wang et al., 2022) employs pixel-wise contrastive learning to facilitate both image-level and pixellevel representation learning. Unlike most of these methods that aim to improve conventional object detectors such as Faster R-CNN or Cascade R-CNN, we focus on designing an effective pre-training scheme for the state-of-the-art DETR-based detector. DETR pre-training. DETR typically relies on a supervised pre-trained backbone on ImageNet and random initialization of the transformer encoder and decoder. Some recent works have explored pre-training the transformer component of DETR for enhanced object detection performance. For example, UP-DETR (Dai et al., 2021) introduces an unsupervised pretext task to detect and reconstruct random patches of the input. DETReg (Bar et al., 2022) refines the pretext task by using unsupervised region proposals from Selective Search (Uijlings et al., 2013) instead of random patches and also reconstructs the object embeddings of these regions from its SwAV (Caron et al., 2020) backbone to learn invariant representations. Siamese DETR (Huang et al., 2023) employs a siamese self-supervised learning approach to pre-train DETR in a symmetric pipeline where each branch takes one view as input and aims to locate and discriminate the corresponding regions from another view. However, these pre-training methods only yield minor improvements to a strong DETR variant like DeformableDETR. Self-training. Self-training is a powerful technique for improving various computer vision tasks, such as image classification (Li et al., 2023; Sahito et al., 2022), object detection (Vandeghen et al., 2022; Yang et al., 2021b), and segmentation (Zhu et al., 2021). A common self-training method is NoisyStudent (Xie et al., 2020), which trains a teacher model on labeled data and uses it to generate pseudolabels on unlabeled images. These pseudo-labels are then used to train a student model, and this process is repeated to obtain better models by updating the teacher model with the previous student model. The ASTOD (Vandeghen et al., 2022) framework applies an iterative self-training process for object detection, using multiple image views to produce high-quality pseudo-labels. ST++(Yang et al., 2022) is a recent self-training algorithm for segmentation tasks, which uses confidence scores to filter out incorrect pseudo-labels. (Zoph et al., 2020) has demonstrated that self-training outperforms traditional pre-training methods in various scenarios, including low and high data regimes, and can even succeed when pre-training methods fail. Unlike these complex self-training schemes that use an iterative approach to refine pseudo-labels, we propose a Simple Self-training scheme that generates pseudo-labels only once by keeping a fixed number of the most confident predictions. 3 Approach In this work, we focus on studying how to perform pretraining over the Transformer encoder and decoder parts within DETR for object detection tasks following (Bar et al., 2022; Dai et al., 2021). The goal of DETR pre-training is to design an effective pretext task that can make the best use of a large-scale unlabeled dataset that has no ground-truth bounding box annotations. 3.1 Formulation The conventional DETR model has three components, the backbone extracting the image feature, the encoder enhancing the feature with a self-attention mechanism, and the decoder turning query inputs into object class and location predictions through cross-attention with image features. The existing self-supervised pre-training methods share a similar scheme that optimizes the encoder and decoder network parameters on the pre-training dataset while freezing a pretrained backbone. After pre-training, all three components are tuned together on the downstream dataset. The pipeline is illustrated in Figure 2. Preliminary. In the following article, we formulate the general self-supervised pre-training process as several equations. We use f9,, foc, fo, to represent the backbone, Transformer encoder, and Transformer decoder within a DETR network parameterized by 6g, 0c, and Op. The input images from the pre-training and downstream dataset are de noted as X = {X1,--- ,Xw} and X = {x1,--- ,xqr} respectively, where N>>M. The ground-truth label of downstream data is Y={y1,--- ,ywlyi=(ci, b;)}, where c; is the category label and b; is the box location label. Typically, the domain-specific pre-training data labels are lack --- --Maetal. Pre-training mao Backbone J Backbone Downstream) Dataset Fig. 2: The overall framework of self-supervised pre-training scheme. There are two steps to pre-train the DETR network. In the first step, we freeze the backbone and pre-train a randomly initialized Transformer encoder and decoder with the well-designed pre-training target on a largescale pre-training benchmark. In the second step, we initialize the encoder and decoder with pre-trained weights and fine-tune all the parameters of the DETR network on the downstream dataset supervised by ground-truth labels. ing and most works choose to generate the pseudo-labels, ie, Y = {¥,,--- , Vy} instead. Pre-train. We illustrate the mathematical formulations of the DETR pre-training with Equation | and 2. Specifically, the pre-training input x; is forwarded through the backbone fg,, encoder fg,, and decoder fg, to get the prediction Z;. Here 9g, 9, Op represent the learnable parameters for the three network components respectively. 0g is initialized with SwAV (Caron et al., 2020) self-supervised pretraining method and frozen during pre-training. O¢ and 4p are randomly initialized and then optimized to minimize the pre-training loss Lpre(-), which is calculated with network output Z; and pre-training target y;. Zi= Foo (Foc (foe (Xi), Q), a) N Op, Oe, Q = argmin > Lyre (Zis ¥i)> (2) 60.06,2 *—y where Q = {qi,:-- ,qx} represents the learnable object query of decoder and will also be jointly optimized with the encoder/decoder parameters. 8. OE, (@) represent the decoder parameters, encoder parameters, and object query after pre-training. In the following section 3.2, we will illustrate the formulation of Lp,. in different methods. Fine-tune. We obtain the optimized encoder and decoder parameter 6, a during pre-training. Then we tune the same network on the downstream data x;. Here, we initialize the backbone, encoder and decoder parameter with 6g, 0, Ap, and denote the network output as z;. All parameters of the three components and learnable query Q are optimized to minimize the downstream loss £4; (-) between z; and downstream label y;. 2i = Fx, (Sa; (Lo (xi), Q), @) M 40, 0€,8,Q = argmin S> La(2i,¥i), (4) 0 ,0,08,0 i=where 8, Oe, Op, ) are optimized decoder, encoder, backbone parameters, and object query after downstream finetuning. | Transformer Encoder EPEEE — | ‘Transformer Encoder eucea ImageNet (Self-\Supervised Pre-trained Random Initiaized Detection Self-Supervised Pre-trained Transformer Decoder. = KE ose Transformer Decoder. = K-— rout 3.2 Instantiations Assume the target of the i-th pre-training input can be denoted as ¥; = {Y¥i1,--: . Yim}, where m is the number of objects in each target. The network output consists of k bounding box predictions, which is the same as the number of object queries. We denote the corresponding prediction as Z; = {Zi1,++- ,Zix}. Typically, the number of targets in y; is less than 30, while we set our DETR network to output 100 or 300 predictions, som < k. Thus we pad the targets with no-object category @ following DETR (Carion et al., 2020) to be of size k. Then, DETR performs one-to-one alignment via Hungarian bipartite matching algorithm (Kuhn, 1955) over y; and Z;. We illustrate the mathematical formulation in Equation 5, which computes the optimal label assignment for each prediction by minimizing the matching cost function Lmatcn(-): k o; = argmin > Latch (Vij > Zic,(7))s (5) wieXe Gy where 5), represents all permutations over k elements and oi(j) maps the targeted box j to the most similar predicted box within the i-th input. The matching cost function Lmatch(-) measures the predictions from two aspects including the localization accuracy and classification accuracy following DETR (Carion et al., 2020). Most self-supervised pre-training methods differentiate through the design of pretext tasks, which results in different structures for the pre-training target y, and implementations of the pre-training loss Cpre. A good pretext task design can improve the final prediction performance. In the following, we first introduce the instantiation of a representative method called DETReg (Bar et al., 2022). Then, we propose two more effective pre-training schemes: DETReg + Pseudo-box and Simple Self-training. Both methods focus on enhancing the localization and classification pre-training target quality. We compare the pre-training pipeline of three methods in Figure 3. DETReg. DETReg uses an unsupervised region proposal method named Selective Search (ss) to generate the tar --- --Revisiting DETR Pre-training for Object Detection get boxes. The j-th “box proposal” for the 7-th input is denoted as b;; € [0,1]*. We select the top k Selective Search box proposals {b;,,--- , b,c} and pair them with the binary category target padded to the size of network query number k (k > k) {pi} 1p Prey -++ , Pig = O}, where pj; = 1 indicates the element is a box proposal while pj; = 0 indicates a padded @. To compensate for the lack of semantic information in the binary category, the DETReg network incorporates another object embedding reconstruction branch to predict the object embeddings {fj1,--- fix Eij € R*} of detected boxes, which is supervised by the target object descriptor sway FP!" with P™ indicati P with f;; indicating the object embed {fin s+ fiz } ding extracted from the image patch in the j-th box proposal on the i-th input with a fixed SwAV backbone. Therefore, the pre-training target and network prediction are denoted as Equation 6: _ 55 788 pewav, _ og Vij = (Bij. bi; .fi; ), Zig = (Bij, biz, fiz). (6) The pre-training loss is the sum of binary classification loss LE2(-), box loss Lyox(-), and embedding loss Lemb(-) through all & outputs as below: Lore(¥i > ALE (DF, Bis. j)) = — @) + NL gps 40) Loox (By) + Bia. (3)) + AcLemo(£ Be Fics): where £i"(.) is the binary classification loss which can be implemented as Cross Entropy Loss or Focal Loss. Lpox(-) is the sum of L1 and GIoU Loss, and Lemp(-) is the L1 Loss. Xe, Ap, and - are loss coefficients and o;(j) maps the target box j to the assigned predicted box o;(j) with lowest cost within the 7-th input. DETReg + Pseudo-box. The unsupervised box proposals like Selective Search boxes are of very low quality. To handle this, we employ two off-the-shelf well-trained COCO object detectors to predict the pseudo-boxes for the pre-training data to replace the Selective Search proposals. Specifically, we replace the (pz? b .bi; ) in Equation 6 andwith (pi; pee prene). We use H-Deformable-DETR with ResNet50 or Swin-L backbone as our detector network. We first train them on COCO, then use the trained detector to predict pseudo-boxes on the pre-training dataset, and the top 30 predictions are selected as k. Simple Self-training. We further replace the binary cat=pseudo wae pseudo egory target Pi; with category predictions €¢;; € {@,c1,+++ , Cn} of aforementioned COCO object detectors fo, zswav as the classification target and remove f;; since we already have detailed class information. Due to that the detector is Localization method AP. |AP50|APzs | APs | APas | AP [AR@10| AR@30| AR@Selective Search 05 | 16] 02 )o2)o3)12] 37 | 83 15.H-Deformable-DETR +R50 —|28.4| 40.4 | 30.2 |12.7] 26.7]43.1] 26.5 | 37.4 | 47.Table 1: Objects356 AP and AR score for Selective Search box proposals, and pseudo-box predictions of H-Deformable-DETR-based COCO detectors with R50 and Swin-L backbone. trained on COCO and the pseudo-category labels it predicts are the 80 COCO categories, the binary classification turns into a multiclass classification. The formulation is shown below: _ _ps z-pseud _ _ Vij = eee °), Big = (Giz, Big), (8) cine qpseudo Lore (Yi Zi) Las ( Cys Cio: (j)) Up (9) —pseudo — Loox(bj;—, bie,(j)) + AL ern 4p where £'™(.) is the multiclass classification loss. 3.3 Discussion We utilize ImageNet and Objects365 as the two pretraining benchmarks. To display the quality of Selective Search proposals and pseudo-boxes generated by two offthe-shelf COCO object detectors, we report their boxes’ Average Precision and Average Recall on Objects365 validation set in Table 1. As can be seen, pseudo-boxes generated by COCO object detectors are far more accurate than Selective Search boxes. We also visualize their box proposals in Figure 4. Unlike the conventional self-training scheme (Xie et al., 2020; Zoph et al., 2020) that relies on applying complicated augmentation strategy to boost the quality of pseudo-labels, adjusting NMS threshold carefully, and re-generating more accurate pseudo-labels based on the fine-tuned models in an iterative manner, our Simple Self-training method directly generate the pseudo-labels for one time without those tricks, resulting in a much simpler approach. 4 Experiment 4.1 Implementation Details Datasets. Our object detection network is pre-trained on the ImageNet or Objects365 (Shao et al., 2019) benchmark, then fine-tuned on COCO train2017 and evaluated on COCO val2017, or fine-tuned on PASCAL VOC trainval07+12 and evaluated on PASCAL VOC test2007. For the pre-training benchmarks, ImageNet has 1.2 Million images --- --Maetal. Pre-training Dataset Object embed )« — (Target embed) [al os > Box proposals + se J —) (a) DETReg fal Pre-training Dataset Object embed )« _»(Target embed. Backbone (b) DETReg+pseudo-box ImageNet (Self-)Supervised Comme | —— Pre-trained COCO Supervised Pre-trained CJ Random Initialized (c) Simple Self-training Fig. 3: The pre-training pipelines of DETReg, DETReg+pseudo-box, and Simple Self-training. In DETReg and DETReg+pseudo-box, we use an extra frozen backbone branch to get the target object embeddings from the image crops. The binary-class outputs of the Transformer predict whether the detected boxes contain an object. Ground-Truth Selective Search H-Def-DETR + RH-Def-DETR + Swin-L. Fig. 4: Qualitative comparisons of the top 30 generated bounding boxes of different methods on Objects365. The methods include Selective Search and trained H-Deformable-DETR detectors with R50 or Swin-L backbones. which mostly contain one object since the dataset is created for classification. Objects365 is a large-scale dataset for object detection with 2 Million images. The image scene is more complicated with around 15 ground-truth bounding boxes per image on average. We use Objects365 as the default pre-training benchmark for all experiments in sections 4.2 and 4.4, as its complex scenes bring better pretraining performance for the Simple Self-training approach. Architectures. We use two kinds of DETR backbones including ResNet50 which is self-supervised pre-trained by SwAV on ImageNet and Swin-L which is supervised pretrained on ImageNet. We pre-train three DETR-based architectures in Section 4.3 including vanilla DETR (Carion et al., 2020), Deformable-DETR (Zhu et al., 2020), and H-Deformable-DETR (Jia et al., 2023), which is a recent state-of-the-art object detector based on a combination of an improved Deformable-DETR and an effective hybrid matching scheme. The Transformer module in those architectures is composed of 6 encoder layers and 6 decoder layers. The vanilla DETR and DeformableDETR are plain without tricks, while 1-Deformable-DETR is improved with iterative bounding box refinement, twostage (Zhu et al., 2020), mixed query selection, and look forward twice scheme (Zhang et al., 2022). By default, we use H-Deformable-DETR with ResNet50 backbone for the ablation study. Training. We pre-train the network on ImageNet forepochs following DETReg or on Objects365 for 3 epochs to ensure the same iteration number according to their different dataset sizes. For fine-tuning, we train for 150 epochs with --- --Revisiting DETR Pre-training for Object Detection Method Framework|Backbone|#epoch| AP_APso APs APs APar AP1. Swin (Liu et al., 2021) HTC | Swin-L | 36. |57.1 75.6 62.5 42.4 60.7 71.Group-DETR (Chen et al., 2022) DETR | Swin-L | 36 [584 - - 41.0 62.5 73.DINO-DETR (Zhang et al., 2022) DETR | Swin-L 58.5 77.0 64.1 41.5 62.3 74.‘H-Deformable-DETR (Jia et al., 2023) | DETR | Swin-L 57.9 76.9 63.7 42.4 61.9 73.Ours (pre-trained H-Deformable-DETR)| DETR | Swin-L | 24 jo. 377.9 65.1 44.1 62.9 73.Table 2: System-level comparisons with the state-of-the-art DETRbased single-scale evaluation results on COCO va1 set. Method DETR model | Pretrain }#query/#epoch] AP AP59 AP75 APs APyy APL from scratch DETR - 100 | 150 |40.3 61.3 42.2 18.2 44.6 60.DETReg DETR ImageNet|} 100 | 150 |40.2 60.7 42.3 17.6 44.3 59.ours DETR _|ImageNet| 100 | 150 |41.9 62.7 44.0 20.7 46.0 62.from scratch} DDETR-MS - 300 | 50 |45.2 64.2 49.4 27.2 49.3 59.DETReg DDETR-MS |ImageNet 50 [43.5 61.4 47.3 24.2 47.1 58.ours DDETR-MS |ImageNet 50 |46.0 64.4 50.0 26.6 49.8 61.from scratch}H-DDETR-MS - 12 |49.6 67.5 54.1 31.9 64.DETReg [H-DDETR-MS|ImageNet| 300 12 |49.5 66.8 53.9 30.5 53.5 63.ours ‘H-DDETR-MS|ImageNet] 300 | 12 |51.6 69.4 56.4 35.0 55.3 66.Table 3: Comparisons with self-supervised pre-training method DETReg on the COCO downstream benchmark. Method | DETR model | Pretrain |#query|#epoch| APs AP APL from scratch] — DETR - 100 | 150 60.DETReg DETR [ImageNet| 100 | 150 |60.9 82.0 65.9 15.1 40.8 69.ours DETR ImageNet| 100 150 |63.5 83.8 68.6 22.5 44.3 72.from scratch} DDETR-MS - 300 | 50 |61.1 83.1 68.0 25.5 47.4 67.DETReg DDETR-MS |ImageNet| 300 | 50 |63.6 82.6 70.2 27.5 49.7 70.ours DDETR-MS |ImageNet| 300 | 50 |67.8 85.4 75.5 30.9 54.7 74.from scratch}H-DDETR-MS - 300 12 163.8 82.4 70.0 26.5 50.0 70.DETReg [H-DDETR-MS|ImageNet| 300 12 |67.7 84.5 74.9 35.1 55.1 74.ours ‘H-DDETR-MS|ImageNet] 300 | 12 |71.6 87.0 79.2 33.1 60.3 78.Table 4: Comparisons with self-supervised pre-training method DETReg on the PASCAL VOC downstream benchmark. Method AP__APso _APzs, 49.5 66.8 53.49.2 66.5 53.AP, 53.5 63.DETReg ~ B14 532DETReg+pseudo-box ImageNet 50.9 68.3, 7 33.6 14.6, 64.0365 52.0 69.6 56.7 36.1 55.9 65.7 . ImageNet 51.6 69.4 56.4 35.0 55.3 66.Simple Self-training 0365 52.8 70.9 57.6 37.0 56.6 67.Table 5: Effect of pre-training dataset choices. vanilla DETR, 50 epochs with Deformable-DETR, andepochs with H-Deformable-DETR, or 24 epochs with HDeformable-DETR in Section 4.2 for better performance. The learning rate drops at 120/150, 40/50, 11/12, and 20/24 respectively. The batch size for pre-training and finetuning are both 16. Metrics. We measure the object detection accuracy for the top 100 detected bounding boxes. Specifically, we compute AP, APs5o and AP7; as the average precision when using JoU thresholds across the range of 0.50 to 0.95, and exactly Method Localization target | Classification target | AP APs0 AP75 APs AP APL. from scratch - - 49.6 67.5 54.1 31.9 53.3 64.DETReg Selective Search |Object-embedding loss|49.2 66.5 53.6 31.4 53.2 63.DETReg+pseudo-box|Pseudo-box prediction] Object-embedding loss|52.0 69.6 56.7 36.1 55.9 65.3, Simple Self-training |Pseudo-box predictionPseudo-class prediction|52.8 70.9 57.6 37.0 56.6 67.Supervised Ground-truth Ground-tuth ‘(68.2 71.5 58.1 37.3 57.0 67.Table 6: Fine-tuning results on COCO after pre-training with different methods using various localization and classification pre-training targets on Objects365. Method AP APso. AP 75, APs APw APL from scratch 63.8 82.4 70.0 3 50.0 70.DETReg 67.7 84.7 741 55.9 74.DETReg+pseudo-box 71.6 87.0 79.1 36.1 59.0 77.Simple Self-training 71.6 87.9 79.7 33.5 60.2 78.Supervised 72.6 88.0 80.7 37.6 62.6 78.Table 7: Fine-tuning results on PASCAL VOC after pre-training with different methods on Objects365. Hpseudo-box_ | _ AP APso AP75 APs AP APL 30 52.0 69.6 56.7 36.1 55.9 65.60 51.6 69.1 56.6 34.8 55.4 65.100 51S 68.9 56.3 34.9 54.7 65.Table 8: Ablation experiments on the number of pseudo-boxes for the DETReg+pseudo-box method. Method Encoder | Decoder APso APz5 APs AP APL v v 696 56.7 361 559DETReg+pseudo-box v 67.1 53.5 53.2 63.v 69.6 56.1 55.3 65.v v 709 576 3870 566Simple Self-training v 682 543 324 541 63.v 696 564 349 55.4 66.Table 9: Effect of Transformer encoder or decoder pre-training. of 0.50 or 0.75; and also APs, APyy, AP as the AP for small, medium, large bounding boxes. 4.2 Comparison to the State-of-the-art Table 2 shows the object detection result on COCO validation set of H-Deformable-DETR network pre-trained on Objects365 benchmark with our method in comparison with other state-of-the-art object detection systems. Our Simple Self-training approach significantly boosts the performance of H-Deformable-DETR from 57.9% to 59.3% with fewer training epochs. We expect our approach to achieve better results with bigger batch size and epoch number, for instance, batch size of 256 and epoch of 60 are used for the self-supervised pre-training in Siamese DETR (Huang et al., 2023). 4.3 Results on different DETR architectures As shown in Table 3 and Table 4, we display the results of DETReg and Simple Self-training with different DETR architectures on the COCO and PASCAL VOC benchmarks. --- --Maetal.20 T+18.9 i EE DETReg EH DETReg+pseudo box 2 simple self-training= a 10F < on T 1 15% 10% 25% 50% Labeled Data (%) Fig. 5: Ablation experiments on low-data regimes. The value shows the performance improvement of three pre-training schemes compared to the from scratch baseline. The line of from scratch shows the baseline results without pre-training as the ResNet50 backbone is initialized with SwAV and the Transformer is randomly initialized. The results show that with the reported experiment setting, the DETReg pre-training fails to bring improvement to the from scratch baseline on the COCO benchmark, while can get small gains on the PASCAL VOC benchmark. Our Simple Self-training can effectively improve the baseline performance for all three DETR architectures on both benchmarks. 4.4 Ablation Experiments and Analysis Choice of pre-training dataset. We also investigate the impact of pre-training datasets with the 1-DeformableDETR architecture in Table 5. Compared to ImageNet, pretraining with the Objects365 benchmark yields better performance with the DETReg+pseudo-box and Simple Selftraining approach, which is not the case with the DETReg approach. As DETReg+pseudo-box and Simple Selftraining employ accurate pseudo-boxes of COCO detectors as the pre-training targets, they can benefit from a more complex image scene that contains richer objects like Objects365, while Selective Search’s chaotic proposals on Objects365 may not be better localization targets than its proposals on ImageNet. It has been proved that ImageNet is a good benchmark for pre-training general representation ability, which can be transferred to multiple downstream tasks. However, for pre-training a specific detection network, a large-scale object detection benchmark like Objects365 is more helpful if the pseudo-box has good quality. Therefore, we use Objects365 as the default pre-training benchmark for the following studies. Pre-training methods. We present the downstream results on the COCO benchmark of the from scratch and dif ferent pre-training methods in Table 6 and results on the PASCAL VOC benchmark in Table 7. All methods except from scratch are pre-trained on the Objects365 benchmark. The middle three pre-training methods do not utilize Objects365 ground-truth labels, while the last is supervised by ground-truth and thereby serves as an upper bound. The difference between the three unsupervised pretraining pipelines is illustrated in Figure 3. As shown in the Table 6 and 7, the DETReg+pseudo-box method builds upon DETReg and improves its localization targets by utilizing more accurate COCO detector pseudo-boxes, leading to significant improvement. The Simple Self-training method discards the object-embedding loss and instead supervises the multi-class classification head with the class predictions of COCO detectors, resulting in further performance gains. For the supervised method, we replace the pseudobox and pseudo-class targets in the Simple Self-training with ground-truth and achieve an upper-bound performance that is only slightly better than our Simple Self-training strategy. This step-by-step comparison demonstrates how we can progressively improve the pre-training performance by introducing better localization and classification targets. Additionally, we observe that better localization pre-training targets are more impactful than better classification targets for object detection tasks. Pseudo-box Number. In Table 8, we ablate with the number of pseudo-boxes in the DETReg + pseudo-box method. We observe that using more than 30 pseudo-boxes for pretraining does not improve the performance, despite more pseudo-boxes exhibiting higher recall on the ground-truth (as shown in Table 1, where AR@10, 30, 100 means AR with 10, 30, 100 proposed boxes) and providing more supervision signals. A possible explanation is that each Objects365 image contains approximately 15 box annotations, and the predictions beyond the top 30 may have low confidence and less meaningful information, as a result of incorporating noise into the pseudo-box target. Encoder/Decoder Pre-training. We evaluate the importance of Transformer encoder and decoder pre-training in the DETReg+pseudo-box and Simple Self-training approaches in Table 9. We first report the performance of using both the encoder and decoder pre-trained parameters, then we report the results of only loading the encoder or decoder pre-trained parameters and random initializing the other part. In both pre-training approaches, we observe that the encoder pre-training contributes more than the decoder pre-training, which is reasonable considering the high ratio of encoder GFLOPs shown in 1b. Fine-tuning dataset size. We investigate the effectiveness of three pre-training schemes compared to training from scratch when only a limited amount of data is available for --- --Revisiting DETR Pre-training for Object Detection coco PASCAL VOC Text prompt Gernerative model Pretraining dataset Localization target Classification target AP APs) AP7z5 | AP APs 9 AP- - 0365 Pseudo-box prediction | Pseudo-class prediction | 52.8 70.9 57.6 | 71.6 87.9 79.COCO captions ControlNet Control-COCO 2M Ground-truth Ground-truth 51.1 69.2 55.8 | 71.7 87.8 79.COCO captions ControlNet Control-COCO 2M Pseudo-box prediction | Pseudo-class prediction | 52.6 70.6 57.5 | 72.0 87.8 80.LLaVA captions ControlNet LLaVAControl-COCO 2M Ground-truth Ground-truth 50.7 69.6 55.4 | 71.6 87.5 79.LLaVA captions ControlNet LLaVAControl-COCO 2M | Pseudo-box prediction | Pseudo-class prediction | 52.9 70.8 57.9 | 72.3 87.7 80.COCO captions SDXL SDXL-COCO 2M Pseudo-box prediction | Pseudo-class prediction | 52.5 70.7 57.3 | 72.1 87.6 79.LLaVA captions SDXL SDXL-COCO 2M Pseudo-box prediction | Pseudo-class prediction | 52.9 71.0 58.0 | 72.0 87.6 80.Table 10: Evaluation results of pre-training with synthetic images similar to COCO generated by text-to-image generative models ControlNet and SDXL. The text prompts given to the generative models are COCO ground-truth captions (represented as COCO captions) or the generated captions by the large multimodal model LLaVA based on COCO images (represented as LLaVA captions). Text prompt Gernerative model Pretraining dataset Localization target PASCAL VOC Classification target AP) APso APLLaVA captions ControlNet LLaVA captions SDXL SDXL-0365 2M LLaVAControl-O365 2M | Pseudo-box prediction Pseudo-box prediction Pseudo-class prediction | 52.4 70.5 57.2 | 71.8 87.6 79.Pseudo-class prediction | 52.6 70.6 57.6 | 71.6 87.4 79.Table 11: Evaluation results of pre-training with synthetic images similar to Objects365 generated by ControlNet and SDXL. Since Objectsdoes not have ground-truth captions, the text prompts given to the generative models are generated captions by LLaVA based on Objectsimages (represented as LLaVA captions). fine-tuning in Figure 5. Specifically, we fine-tune the pretrained network on 5%, 10%, 25%, and 50% of the COCO training set and evaluate it on the full COCO validation set. All three pre-training schemes greatly speed up the convergence. We observe that DETReg only yields slightly higher performance than random initialization. The Simple Selftraining approach remains the most effective, particularly when only a very small amount of data (5%) is available. Qualitative analysis. Without fine-tuning, we visualize the discriminability scores (Zong et al., 2022) of the pretrained encoder in Figure 6 to investigate what the encoder has learned in pre-training. From the figures, we can see that DETReg’s feature discriminability is seriously disturbed by the background. However, when we utilize improved localization and classification targets in the DETReg+pseudobox and Simple Self-training approach, finer details are captured. Notably, the Simple Self-training method demonstrates performance that is almost on par with pre-training using ground-truth. We also visualize the deformable cross-attention of the pre-trained decoder in Figure 7. The colored dots in the image represent the sampling points from all resolution scales, where the color indicates the attention weights, with a lighter color indicating higher attention. As random initialization shows, the initial key points are sampled radially from the center to the edge. All pre-training methods learn to scatter the sampling points across the entire object of interest with different patterns, while the Simple Self-training pre-trained decoder can sample key points from an accurate range of objects and distribute attention weight more effectively. 4.5 Results with synthetic data generated by T2I Last, we investigate the effectiveness of pre-training with synthetic data, which is generated using recent largescale text-to-image generation models. Specifically, we leverage two representative text-to-image models, ControlNet (Zhang and Agrawala, 2023) and SDXL (Podell et al., 2023), to generate images. These models take original captions from the COCO dataset or captions generated by LLaVA (Liu et al., 2023) as prompts for image synthesis. ControlNet uses predicted depth maps from DPT (Ranftl et al., 2021) as conditional input to generate images that match both the depth maps and captions. On the other hand, SDXL generates images solely based on the provided captions without any additional conditions. We create a synthetic dataset comprising 2.3 Million generated images. Figure 8 displays some examples. Upon analyzing the images produced by ControlNet, we find that they closely resemble the layout of the original --- ---Maetal. Random initialization DETReg DETReg+ Pseudo-box Simple Self-training Supervised-pretraining Fig. 6: Visualizations of discriminability scores in the encoder on COCO val images. Random initialization DETReg DETReg+ Pseudo-box Simple Self-training Supervised-pretraining Fig. 7: Visualizations of deformable cross-attention based on the last Transformer decoder layer on COCO val images. images due to the conditioning on depth maps. This characteristic allows us to use COCO ground-truth data to supervise the pretraining process when using synthetic images generated by ControlNet. Additionally, we also explore the Simple Self-training approach on the synthetic data by pretraining with pseudo-box and pseudo-class predictions that are generated by trained COCO detectors. The process involves pre-training the H-Deformable-DETR model with synthetic images for 3 epochs, followed by fine-tuning on COCO or PASCAL VOC benchmarks for 12 epochs. The results of this evaluation are presented in Table 10. Interestingly, pre-training with the synthetic dataset generated based on COCO demonstrates comparable improvements to pre-training with Objects365 real data using the Simple Self-training scheme. This outcome indicates that text-to image synthesis is an effective method for scaling up the original dataset for pre-training. Furthermore, the results on the PASCAL VOC benchmark showcase the generalization ability of pre-training with synthetic data generated based on COCO. Table 11 shows the results of pre-training with the synthetic data generated based on Objects365 by first captioning Objects365 image with LLaVA and then synthesizing new images from the caption. They are not as good as pretraining with COCO-based synthetic data on both downstream benchmarks. 5
--- CONCLUSION ---
We investigate the effectiveness of DETReg, a representative self-supervised pre-training approach for DETR, across --- --Revisiting DETR Pre-training for Object Detectionthree distinct DETR architectures. Our findings, unfortunately, do not reveal any performance enhancements of DETReg in recent architectures, thereby challenging the validity of previous conclusions. In response, we reevaluate crucial design aspects, including pre-training targets for localization and classification. As a result of this analysis, we introduce several impactful enhancements and a Simple Selftraining scheme that significantly boosts performance across strong DETR architectures. Additionally, we leverage the powerful text-to-image generative models to construct synthetic datasets for pre-training purposes. Remarkably, our approach yields improvements on par with the achievements of pre-training with Objects365. Moving forward, we plan to extend DETR pre-training to encompass a broader spectrum of vision tasks, such as instance segmentation and pose estimation. We hope our work can stimulate the research community to reassess the actual capacity of existing selfsupervised pre-training methods when employed in the context of strong DETR models and advance the progress on this challenging task. --- --12 Maetal. Original images COCO captions LLaVA captions COCO captions LLaVA captions + ControlNet + ControlNet +SDXL +SDXL Fig. 8: Examples of synthetic images using different captions and generative models. The original images are sampled from COCO t rain set. --- --Revisiting DETR Pre-training for Object DetectionData availability statement The author confirmed that the data supporting the findings of this study are available within the article. Raw data that support the findings of this study and the generated synthetic dataset are available from the corresponding author, upon reasonable request. References Bar A, Wang X, Kantorov V, Reed CJ, Herzig R, Chechik G, Rohrbach A, Darrell T, Globerson A (2022) Detreg: Unsupervised pretraining with region priors for object detection. In: CVPR, pp 14605-Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S (2020) End-to-end object detection with transformers. In: ECCV, Springer, pp 213-Caron M, Misra I, Mairal J, Goyal P, Bojanowski P, Joulin A (2020) Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS 33:9912-Chen Q, Wang J, Han C, Zhang S, Li Z, Chen X, Chen J, Wang X, Han S, Zhang G, et al. (2022) Group detr v2: Strong object detector with encoder-decoder pretraining. arXiv preprint arXiv:Chen T, Kornblith S, Norouzi M, Hinton G (2020) A simple framework for contrastive learning of visual representations. In: ICML, PMLR, pp 1597-Dai Z, Cai B, Lin Y, Chen J (2021) Up-detr: Unsupervised pre-training for object detection with transformers. In: CVPR, pp 1601-Grill JB, Strub F, Altché F, Tallec C, Richemond P, Buchatskaya E, Doersch C, Avila Pires B, Guo Z, Gheshlaghi Azar M, et al. (2020) Bootstrap your own latent-a new approach to self-supervised learning. NeurIPS 33:21271-He K, Fan H, Wu Y, Xie S, Girshick R (2020) Momentum contrast for unsupervised visual representation learning. In: CVPR, pp 9729-Huang G, Li W, Teng J, Wang K, Chen Z, Shao J, Loy CC, Sheng L (2023) Siamese detr. In: CVPR, pp 15722-Jia D, Yuan Y, He H, Wu X, Yu H, Lin W, Sun L, Zhang C, Hu H (2023) Detrs with hybrid matching. In: CVPR, pp 19702-Kuhn HW (1955) The hungarian method for the assignment problem. Naval research logistics quarterly 2(1-2):83-Li F, Zhang H, Xu H, Liu S, Zhang L, Ni LM, Shum HY (2023) Mask dino: Towards a unified transformer-based framework for object detection and segmentation. In: CVPR, pp 3041-Li Y, Wu CY, Fan H, Mangalam K, Xiong B, Malik J, Feichtenhofer C (2021) Improved multiscale vision transformers for classification and detection. arXiv preprint arXiv:Li Z, Zhu Y, Yang F, Li W, Zhao C, Chen Y, Chen Z, Xie J, Wu L, Zhao R, et al. (2022) Univip: A unified framework for self-supervised visual pre-training. In: CVPR, pp 14627-Liu H, Li C, Wu Q, Lee YJ (2023) Visual instruction tuning Liu S, Li F, Zhang H, Yang X, Qi X, Su H, Zhu J, Zhang L (2022a) Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S, Guo B (2021) Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV, pp 10012-Liu Z, Hu H, Lin Y, Yao Z, Xie Z, Wei Y, Ning J, Cao Y, Zhang Z, Dong L, et al. (2022b) Swin transformer v2: Scaling up capacity and resolution. In: CVPR, pp 12009-Podell D, English Z, Lacey K, Blattmann A, Dockhorn T, Miiller J, Penna J, Rombach R (2023) Sdxl: Improving latent diffusion models for high-resolution image synthesis. 2307 .Ranftl R, Bochkovskiy A, Koltun V (2021) Vision transformers for dense prediction. In: ICCV, pp 12179-Sahito A, Frank E, Pfahringer B (2022) Better self-training for image classification through self-supervision. In: AJCAI, Springer, pp 645-Shao S, Li Z, Zhang T, Peng C, Yu G, Zhang X, Li J, Sun J (2019) Objects365: A large-scale, high-quality dataset for object detection. In: ICCV, pp 8430-Uijlings JR, Van De Sande KE, Gevers T, Smeulders AW (2013) Selective search for object recognition. ICV 104(2):154-Vandeghen R, Louppe G, Van Droogenbroeck M (2022) Adaptive selftraining for object detection. arXiv preprint arXiv:Wang F, Wang H, Wei C, Yuille A, Shen W (2022) Cp 2: Copypaste contrastive pretraining for semantic segmentation. In: ECCV, Springer, pp 499-Wei F, Gao Y, Wu Z, Hu H, Lin S (2021) Aligning pretraining for detection via object-level contrastive learning. NeurIPS 34:22682-—Xie Q, Luong MT, Hovy E, Le QV (2020) Self-training with noisy student improves imagenet classification. In: CVPR, pp 10687-Yang C, Wu Z, Zhou B, Lin S (2021a) Instance localization for selfsupervised detection pretraining. In: CVPR, pp 3987-Yang L, Zhuo W, Qi L, Shi Y, Gao Y (2022) St++: Make self-training work better for semi-supervised semantic segmentation. In: CVPR, pp 4268-Yang Q, Wei X, Wang B, Hua XS, Zhang L (2021b) Interactive selftraining with mean teachers for semi-supervised object detection. In: CVPR, pp 5941-Zhang H, Li F, Liu S, Zhang L, Su H, Zhu J, Ni LM, Shum HY (2022) Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:Zhang L, Agrawala M (2023) Adding conditional control to text-toimage diffusion models. arXiv preprint arXiv:Zhu X, Su W, Lu L, Li B, Wang X, Dai J (2020) Deformable detr: Deformable transformers for end-to-end object detection. In: ICLR. Zhu Y, Zhang Z, Wu C, Zhang Z, He T, Zhang H, Manmatha R, Li M, Smola AJ (2021) Improving semantic segmentation via efficient self-training. PAMI Zong Z, Song G, Liu Y (2022) Detrs with collaborative hybrid assignments training. arXiv preprint arXiv:Zoph B, Ghiasi G, Lin TY, Cui Y, Liu H, Cubuk ED, Le Q (2020) Rethinking pre-training and self-training. NeurIPS 33:3833-
