--- INTRODUCTION ---
Personalized avatar creation—the ability to map one’s facial features to a 3D virtual replica that can be animated, customized, and rendered—is an emerging technology with great promise for cinema, the metaverse, and telepresence. Advances in this area may lead to digital twins with greater verisimilitude in detail and in animation --- --SIGGRAPH Conference Proceedings, Aug 6-10, 2023 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, and Sameh Khamis that are more easily integrated into downstream applications and pipelines. Single-shot personalized avatar creation enables reconstructing face avatars from individual RGB images with greater convenience and flexibility than methods that require more specialized capture setups or procedures. Traditional approaches to animatable 3D avatar creation are often based on 3D Morphable Models (3DMM) [Blanz and Vetter 1999], which disentangle shape and appearance variation into a low-dimensional face representation. Building on these, more recent approaches often leverage either explicit (textured) template meshes [Danééek et al. 2022; Feng et al. 2021; Grassal et al. 2022; Khakhulin et al. 2022; Li et al. 2017; Tran and Liu 2019] or neural implicit representations [Mildenhall et al. 2021; Park et al. 2019; Sitzmann et al. 2019]. Template-based approaches enable easy asset extraction and intuitive editing, but are often unable to capture high-quality geometry and textures. Emerging implicit face models can achieve greater realism by modeling more complex geometric features such as hair [Cao et al. 2022b; Giebenhain et al. 2022; Zheng et al. 2022a]. However, implicit face representations often compromise on interpretability and are less intuitive to control; the entangled latent spaces learned by these highly parameterized models are difficult to edit. Our approach aims to combine the interpretability and editability advantages of template-based 3DMMs with the quality and topological flexibility of implicit 3D representations. Crucially, we decouple appearance and geometry into two branches of our network architecture. By incorporating a UV parameterization network to learn continuous and consistent texture maps, we can export avatars as textured meshes to support downstream applications such as texture map editing and relighting in a traditional graphics pipeline (See Figure 1). On the other hand, by representing geometry with an implicit signed distance field (SDF), our facial shape is less limited by resolution and topology compared to mesh-based approaches. We show that our proposed hybrid representation effectively captures the geometry, appearance, and expression space of faces. We demonstrate that single-shot in-the-wild portrait images can be effectively mapped to avatars based on our proposed representation, and that these avatars improve upon the previous state-of-the-art in photo-realism, geometry, and monocular expression transfer. Moreover, we demonstrate compelling capability for enabling direct texture editing and disentangled attribute editing such as facial geometry and appearance attributes. In summary, contributions of our work include: e We propose a hybrid morphable face model combining the high-quality geometry and flexible topology of implicit representations with the editability of explicit UV texture maps. e We present a single-shot inversion framework to map a single in-the-wild RGB image to our implicit 3D morphable model representation. The inverted avatar supports novel view rendering, non-linear facial reanimation, disentangled shape and appearance control, direct texture map editing, and textured mesh extraction for downstream applications. e We demonstrate state-of-the-art reconstruction accuracy for photo-realistic rendering, geometry, and expression accuracy in the single-view reconstruction setting. Table 1. Comparison to recent prior work. To the best of our knowledge, our method is the first implicit 3D face model to generalize across single-image inputs while supporting flexible topology and explicit texture map control. Generalizable Single-Image Implicit Explicit Representation Texture Control EMOCA [2022] v v x v ROME [2022] v v x x Neural Parametric Head Models [2022] x x v x IM-Avatar [2022a] x x v x Neural Head Avatars [2022] x x v v Volumetric Avatars from a Phone Scan [2022] / x v v HeadNeRF [2022] v v v x Ours v v v v 2
--- RELATED WORK ---
2.1 Mesh-based 3D Morphable Models The seminal work by Blanz and Vetter proposed a linear 3D Morphable Model (3DMM) [Blanz and Vetter 1999] that models facial shape and textures on a template mesh using linear subspaces computed by principal component analysis (PCA) from 200 facial scans. This low-dimensional facial shape and texture space makes 3DMMs suitable for robustly capturing facial animation as well as reconstructing 3D faces in monocular settings. To reconstruct shape, texture, and lighting from a photo, previous work employed continuous optimization using constraints such as facial landmarks and pixel colors [Cao et al. 2014, 2016; Garrido et al. 2013, 2016; Ichim et al. 2015; Li et al. 2017; Romdhani and Vetter 2005; Shi et al. 2014; Thies et al. 2016] and more recently deep learning-based inference [B R et al. 2021; Danééek et al. 2022; Deng et al. 2019b; Dib et al. 2021a,b; Dou et al. 2017; Feng et al. 2021; Genova et al. 2018; Luo et al. 2021; Tewari et al. 2019; Tewari et al. 2017; Tuan Tran et al. 2017; Wu et al. 2019]. While approaches relying on 3DMMs tend to be robust, they are ineffective for reconstructing high-fidelity geometry and texture details due to the linearity and low dimensionality of the model. Various other
--- METHOD ---
reconstructs a high-quality editable 3D digital avatar (columns 2 and 3) by combining implicit geometry representations with explicit texture maps. The proposed approach naturally supports novel view synthesis from large pose shifts, an expressive and non-linear facial animation space (columns 4 through 6), direct user access to texture map editing (column 7), and 3D asset extraction for further downstream applications such as relighting (column 8). Original image courtesy of COD Newsroom/flickr (top) and Malcolm Slaney/flickr (bottom). There is a growing demand for the accessible creation of high-quality 3D avatars that are animatable and customizable. Although 3D morphable models provide intuitive control for editing and animation, and robustness for single-view face reconstruction, they cannot easily capture geometric and appearance details. Methods based on neural implicit representations, such as signed distance functions (SDF) or neural radiance fields, approach photorealism, but are difficult to animate and do not generalize well to unseen data. To tackle this problem, we propose a novel method for constructing implicit 3D morphable face models that are both generalizable and intuitive for editing. Trained from a collection of high-quality 3D scans, our face model is parameterized by geometry, expression, and texture latent codes with a learned SDF and explicit UV texture parameterization. Once trained, we can reconstruct an avatar from a single in-the-wild image by leveraging the learned prior to project the image into the latent space of our model. Our implicit morphable face models can be used to render an “Work done during an internship at NVIDIA. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGGRAPH Conference Proceedings, Aug 6-10,© 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0159-7/23/08. https://doi.org/10.1145/3588432.avatar from novel views, animate facial expressions by modifying expression codes, and edit textures by directly painting on the learned UV-texture maps. We demonstrate quantitatively and qualitatively that our method improves upon photo-realism, geometry, and expression accuracy compared to state-of-the-art methods. CCS Concepts: « Computing methodologies > Modeling/Geometry. Additional Key Words and Phrases: Neural Avatars, Implicit Representations, Texture Maps, Animation, Inversion ACM Reference Format: Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, and Sameh Khamis. 2023. Single-Shot Implicit Morphable Faces with Consistent Texture Parameterization. In Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Proceedings (SIGGRAPH ’23 Conference Proceedings), August 6— 10, 2023, Los Angeles, CA, USA. ACM, New York, NY, USA, 13 pages. https: //doi.org/10.1145/3588432.1 INTRODUCTION Personalized avatar creation—the ability to map one’s facial features to a 3D virtual replica that can be animated, customized, and rendered—is an emerging technology with great promise for cinema, the metaverse, and telepresence. Advances in this area may lead to digital twins with greater verisimilitude in detail and in animation --- --SIGGRAPH Conference Proceedings, Aug 6-10, 2023 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, and Sameh Khamis that are more easily integrated into downstream applications and pipelines. Single-shot personalized avatar creation enables reconstructing face avatars from individual RGB images with greater convenience and flexibility than methods that require more specialized capture setups or procedures. Traditional approaches to animatable 3D avatar creation are often based on 3D Morphable Models (3DMM) [Blanz and Vetter 1999], which disentangle shape and appearance variation into a low-dimensional face representation. Building on these, more recent approaches often leverage either explicit (textured) template meshes [Danééek et al. 2022; Feng et al. 2021; Grassal et al. 2022; Khakhulin et al. 2022; Li et al. 2017; Tran and Liu 2019] or neural implicit representations [Mildenhall et al. 2021; Park et al. 2019; Sitzmann et al. 2019]. Template-based approaches enable easy asset extraction and intuitive editing, but are often unable to capture high-quality geometry and textures. Emerging implicit face models can achieve greater realism by modeling more complex geometric features such as hair [Cao et al. 2022b; Giebenhain et al. 2022; Zheng et al. 2022a]. However, implicit face representations often compromise on interpretability and are less intuitive to control; the entangled latent spaces learned by these highly parameterized models are difficult to edit. Our approach aims to combine the interpretability and editability advantages of template-based 3DMMs with the quality and topological flexibility of implicit 3D representations. Crucially, we decouple appearance and geometry into two branches of our network architecture. By incorporating a UV parameterization network to learn continuous and consistent texture maps, we can export avatars as textured meshes to support downstream applications such as texture map editing and relighting in a traditional graphics pipeline (See Figure 1). On the other hand, by representing geometry with an implicit signed distance field (SDF), our facial shape is less limited by resolution and topology compared to mesh-based approaches. We show that our proposed hybrid representation effectively captures the geometry, appearance, and expression space of faces. We demonstrate that single-shot in-the-wild portrait images can be effectively mapped to avatars based on our proposed representation, and that these avatars improve upon the previous state-of-the-art in photo-realism, geometry, and monocular expression transfer. Moreover, we demonstrate compelling capability for enabling direct texture editing and disentangled attribute editing such as facial geometry and appearance attributes. In summary, contributions of our work include: e We propose a hybrid morphable face model combining the high-quality geometry and flexible topology of implicit representations with the editability of explicit UV texture maps. e We present a single-shot inversion framework to map a single in-the-wild RGB image to our implicit 3D morphable model representation. The inverted avatar supports novel view rendering, non-linear facial reanimation, disentangled shape and appearance control, direct texture map editing, and textured mesh extraction for downstream applications. e We demonstrate state-of-the-art reconstruction accuracy for photo-realistic rendering, geometry, and expression accuracy in the single-view reconstruction setting. Table 1. Comparison to recent prior work. To the best of our knowledge, our method is the first implicit 3D face model to generalize across single-image inputs while supporting flexible topology and explicit texture map control. Generalizable Single-Image Implicit Explicit Representation Texture Control EMOCA [2022] v v x v ROME [2022] v v x x Neural Parametric Head Models [2022] x x v x IM-Avatar [2022a] x x v x Neural Head Avatars [2022] x x v v Volumetric Avatars from a Phone Scan [2022] / x v v HeadNeRF [2022] v v v x Ours v v v v 2 RELATED WORK 2.1 Mesh-based 3D Morphable Models The seminal work by Blanz and Vetter proposed a linear 3D Morphable Model (3DMM) [Blanz and Vetter 1999] that models facial shape and textures on a template mesh using linear subspaces computed by principal component analysis (PCA) from 200 facial scans. This low-dimensional facial shape and texture space makes 3DMMs suitable for robustly capturing facial animation as well as reconstructing 3D faces in monocular settings. To reconstruct shape, texture, and lighting from a photo, previous work employed continuous optimization using constraints such as facial landmarks and pixel colors [Cao et al. 2014, 2016; Garrido et al. 2013, 2016; Ichim et al. 2015; Li et al. 2017; Romdhani and Vetter 2005; Shi et al. 2014; Thies et al. 2016] and more recently deep learning-based inference [B R et al. 2021; Danééek et al. 2022; Deng et al. 2019b; Dib et al. 2021a,b; Dou et al. 2017; Feng et al. 2021; Genova et al. 2018; Luo et al. 2021; Tewari et al. 2019; Tewari et al. 2017; Tuan Tran et al. 2017; Wu et al. 2019]. While approaches relying on 3DMMs tend to be robust, they are ineffective for reconstructing high-fidelity geometry and texture details due to the linearity and low dimensionality of the model. Various other methods extended 3DMMs to capture non-linear shapes [Chandran et al. 2020; Li et al. 2020; Tewari et al. 2018; Tran et al. 2019; Tran and Liu 2018, 2019; Wang et al. 2022b], photo-realistic appearance using neural rendering or optimization [Gecer et al. 2019; Nagano et al. 2018; Saito et al. 2017; Thies et al. 2019], or reflectance and geometry details for relightable avatar generation [Chen et al. 2019; Huynh et al. 2018; Lattas et al. 2020; Yamaguchi et al. 2018]. Recent approaches predict geometry offsets over the template mesh to reconstruct non-facial regions such as hair [Grassal et al. 2022; Khakhulin et al. 2022]. We refer the reader to Egger et al. [2020] for an in-depth survey of 3DMM techniques and Tewari et al. [2022] for a report of recent advancements in neural rendering. Since mesh-based 3DMMs represent geometry with a shared template mesh, their fixed topology limits the ability to scale the model to capture complex geometry such hair or fine-scale details. Additionally, their ability to synthesize photo-realistic facial textures may be limited by the resolution of the template mesh and discrete texture map. By parameterizing geometry with a signed distance function and color with a continuous texture map, our method is able to avoid such resolution issues and scale more efficiently with model capacity while retaining 3DMM-like intuitive parameters to individually control geometry and textures. Our consistent texture parameterization enables not only direct texture editing in UV space, --- --Single-Shot Implicit Morphable Faces with Consistent Texture Parameterization but also semantic correspondence between our face model and an input image via facial landmarks, which can be leveraged to improve single-shot reconstruction quality. 2.2 Implicit Representations for Modeling and Rendering While single-shot 3D reconstruction methods have explored various explicit 3D representations such as voxels [Girdhar et al. 2016; Tulsiani et al. 2017; Wu et al. 2018; Yan et al. 2016; Yang et al. 2018; Zhu et al. 2017], point clouds [Fan et al. 2017], meshes [Xu et al. 2019], geometric primitives [Niu et al. 2018; Zou et al. 2017], and depth maps [Wu et al. 2020], implicit representations have recently been leveraged to achieve higher resolution reconstruction using occupancy or signed distance fields (SDFs) [Chen and Zhang 2019; Mescheder et al. 2019; Xu et al. 2019]. Implicit representations such as neural radiance fields (NeRFs) [Mildenhall et al. 2021] and signed distance fields (SDFs) [Park et al. 2019] have demonstrated high reconstruction quality for 3D shapes and volumetric scenes. PIFu [Saito et al. 2019] and follow-up works [Cao et al. 2022a; Saito et al. 2020] use implicit fields to model human bodies and clothing. AtlasNet [Groueix et al. 2018] demonstrated 3D shape generation by predicting a set of parametric surface elements given an input image or point cloud. NeuTex [Xiang et al. 2021] replaces the radiance prediction of NeRFs with a learned UV texture parameterization conditioned on lighting direction. Although our method also employs a UV cycle consistency loss, we 1) operate in a SDF setting and condition our parameterization on geometry and expression latent codes to generalize across samples rather than overfit to a single scene, 2) employ sparse facial landmark constraints to facilitate learning a semantically intuitive and consistent parameterization, and 3) explicitly leverage 2D to 3D facial landmark correspondences enabled by the learned consistent parameterization during singleimage reconstruction. Implicit representations have also given rise to higher quality 3D generative models [Chan et al. 2022; Or-El et al. 2022; Xue et al. 2022], and follow-up work has studied inverting an image into the latent space of a pre-trained 3D GAN [Ko et al. 2023; Lin et al. 2022; Roich et al. 2022] for single-view 3D reconstruction. However, without careful optimization and additional priors [Xie et al. 2022; Yin et al. 2022], this 3D GAN inversion tends to be less robust due to unknown camera poses [Ko et al. 2023] and multiview nature of NeRF training in the monocular setting. On the other hand, the compact face representation of our model provides robust initialization in the single-shot reconstruction setting. 2.3 Implicit Face Models Compared to traditional mesh-based 3DMMs for face modeling, implicit representations naturally offer flexible topology and non-linear expression animation through latent code conditioning. While some approaches learn to reconstruct an implicit 3DMM from an input 3D face scan [Alldieck et al. 2021; Cao et al. 2022b; Giebenhain et al. 2022; Yenamandra et al. 2021; Zanfir et al. 2022; Zheng et al. 2022b], other works have explored modeling an implicit face model from RGB videos [Grassal et al. 2022; Ma et al. 2022; Zheng et al. 2022a,c]. However, the above approaches either do not support or cannot generalize to single-shot in-the-wild images. Multi-view methods have also been used to reconstruct implicit head models [Athar et al. 2021, 2022; Hong et al. 2022; Kellnhofer et al. 2021; Li et al. 2022; SIGGRAPH Conference Proceedings, Aug 6-10,Ramon et al. 2021; Wang et al. 2022a]. HeadNeRF [Hong et al. 2022] is the closest to our work and learns a parametric head model from multi-view images during training; at test-time, an input image can be inverted for 3D reconstruction. However, HeadNeRF performs volumetric rendering at a limited image resolution and relies on upsampling CNN modules, resulting in flickering artifacts from depth error during novel view synthesis. Furthermore, existing implicit morphable models do not support texture manipulation beyond interpolation; by contrast, our learned explicit texture paramterization enables intuitive and out-of-domain edits such as adding tattoos or mustaches (see Fig. 1). 3. METHOD 3.1 Implicit Morphable Face Parameterization We disentangle each facial avatar into identity and expression, where identity is encoded by geometry and color latent codes while expression is captured by an expression latent code. To attain both high-quality geometry and interpretable texture, our model consists of an implicit geometry branch and a UV texture parameterization branch. The geometry branch contains a multilayer perceptron (MLP) that maps 3D points p to SDF values SDF(p) during sphere tracing. The UV texture branch consists of a parameterization MLP that maps p to spherical coordinates UV(p), a parameterization regularizer MLP that learns the inverse mapping from UV(p) back to p, and a color network that predicts the output RGB at UV(p). See Figure 2 for a diagram of our model pipeline. Please refer to the supplement for model architecture details. We train our model on the Triplegangers [2022] 3D scan dataset for its volume and diversity of subjects and expressions. Although the RenderPeople [2022] dataset additionally models hair and clothing, it only contains 120 neutral expression subjects, making it less suitable for reconstructing an avatar from unconstrained in-the-wild photos. Our training samples consist of a 3D head mesh, UV diffuse texture map, and six diffusely lit frontal RGB images. The dataset contains 515 different subjects each with 20 expressions, for a total of 10,300 data samples. Our full model learns an AutoDecoder dictionary of 515 geometry codes, 515 color codes, and 10,300 expression codes, as subjects express the same sentiment differently. Different expressions for the same training subject share the same geometry and color codes, allowing the model to disentangle expression from the underlying geometry and texture. Please refer to the supplement for examples of our training data. 3.2. Training Losses Our model is trained on geometry, color, and regularization losses: L = Loeom + Leotor + Lreg (1) Following Figure 2, let f be the SDF MLP, g the UV parameterization MLP, g' the inverse UV parameterization MLP, and X the set of randomly sampled surface points during training. The geometry loss consists of surface, Eikonal [Gropp et al. 2020], normal, and UV losses. The surface loss ¢,,,,¢ optimizes the SDF zero level set, the Eikonal loss f ;,9nq) regularizes the SDF gradients, and the normal loss formal aligns the SDF gradients with the ground truth mesh normals i. The UV loss fy regularizes the learned mapping to --- --SIGGRAPH Conference Proceedings, Aug 6-10, 2023 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, and Sameh Khamis SDF Geometry Branch UV Texture Branch Latent Codes and Coordinates — Weeom Texture Map Editing Novel View Synthesis 22 $f Expression Animation Sphere Tracing L____, =Ei~ gene Fig. 2. Our Pipeline. Avatars are represented by geometry, expression, and color latent codes {wgeom, Wexpr> Weolor } with each being 512 dimensional. At each 3D coordinate p during sphere tracing, the SDF network f and UV parameterization network g are conditioned on Wgeom, Wexpr, and positional encoding PE(p) to predict the signed distance SDF(p) and UV coordinates UV (p), respectively. The inverse UV parameterization network g~! regularizes the learned mapping to be a surface parameterization g | (UV(p); Wgeom, Wexpr) = p, while the color network h predicts the associated RGB texture RGB(p) = h(UV(p); Weolor, Wexpr)- After training, the avatar can be rendered freely with direct control over its texture and facial expression, or extracted as a stand-alone textured mesh asset. follow an invertible surface parameterization, which enables corre spondences between texture and geometry used in our single-shot inversion pipeline, described in Section 3.5.bourf = 5q Dy FOI @) xeXfeikonal = Ex(||Vxf(x)ll - D) (3) 1 a‘normal = F3q, Dy IWaf) ~ A) @) xeX 1 <1tuo = 5g Dy Weg GCI ) xeX Lyeom = tsurf + feikonal + normal + fuo (6) The color loss consists of a reconstruction loss ex on the ground truth texture T, as well as perceptual [Zhang et al. 2018] and reconstruction losses fimg over the facial region Trace between the ground truth image f and rendered image | obtained via sphere tracing: = 1 Tttrex = Bq Dy IP) ~ hg) 7) xexX fimg = LPIPS(Lpace: pace) + Wiface ~ Ifacell” (8) Leolor = trex + timg (9) Finally, we enforce the compactness in the learned latent space by penalizing the magnitude of the geometry, color, and expression codes: 2 2Lreg = \lwgeomll” + llWcotorll” + |lwexprl (10) 3.3. Learning UV Parameterizations To learn an interpretable texture space and coherent semantic correspondence across subjects, we add an auxiliary loss term to Lreg i. De-lighting and Encoder Initialization Siii. Code Optimization iii. Model Fine-tuning Fig. 3. Single-shot inversion pipeline. We de-light the input image and initialize the latent codes using a pre-trained encoder (top row). We then perform PTI [Roich et al. 2022] to get the final reconstruction (bottom row). Original image courtesy of Brett Jordan/flickr. that enforces the parameterization to be consistent through a sparse set of facial landmark constraints:Gandmark = 77 Dy 9) — 991? + Ibe =F MGAVIP 4) xeL The first term enforces the learned UV mapping to match the ground truth UV mapping g for the set of 3D facial landmark points L, and the second term enforces this mapping to be invertible. Fig.demonstrates the consistency of our learned UV parameterization. Although mostly consistent, it is difficult to obtain perfect registrations around the inner mouth and eyes due to the billboard geometry and errors originating from the ground truth data. --- --Single-Shot Implicit Morphable Faces with Consistent Texture Parameterization Input Image Reconstruction and Animation Fig. 4. Non-linear animation space. By linearly interpolating between source and target expression codes, our model exhibits non-linear deformation trajectories on the 3D mouth vertices visualized. Original image courtesy of David Shankbone/flickr. 3.4 Animation After training, an avatar can be animated by manipulating its expression latent code. For a source subject with expression code wexpr, target expression code Wexpr> and animation timesteps t € [0, 1], we define the expression animation trajectory by: Wexpr(t) = Wexpr +t * (Wexpr — Wexpr) (12) Unlike traditional linear 3DMM approaches, our expression space follows non-linear trajectories learned from high-quality 3D scans, as shown in Fig. 4. 3.5 Single-Shot Inversion In order to reconstruct and animate unseen subjects, we project an input RGB image into the latent space of our pre-trained model and lightly fine-tune the model weights similar to Pivotal Tuning Inversion (PTI) [Roich et al. 2022]. To handle unseen lighting conditions, we de-light the input image using LUMOS [Yeh et al. 2022] and initialize the geometry, color, and expression codes through a separately trained encoder. We empirically find this encoder initialization to be important in obtaining robust results for in-the-wild input images (See Figure 9). Image Encoder. We attain latent code initializations by training a DeepLabV3+ [Chen et al. 2018] encoder to reconstruct each training image I and its corresponding latent codes W already computed from the previous AutoDecoder training stage: Lene = ||f- 1)? + |W - WI? (13) W = [wgeom: Weolor’ Wexpr] (14) One major challenge when inverting in-the-wild images is handling unseen identities, accessories, hairstyles, and occlusion present in real-world images, as Triplegangers contain limited identities with no variations in hairstyles or background. Therefore, we augment the encoder’s training dataset with synthetically augmented Triplegangers images from [Yeh et al. 2022], which improves the robustness of the initialization and final inversion reconstruction, shown in Fig. 9. SIGGRAPH Conference Proceedings, Aug 6-10,Optimization. After initializing the latent codes for an input image T using our encoder, we freeze the model weights and optimize the latent codes while minimizing image, silhouette, multi-view consistency, facial landmark, and regularization losses: fimg = LPIPS(Ipace:1face) + Wiface —Tfacell” (15) Githouette= >, F) (16) x€lfaceAx#l face fp = ArcFace(l,I, Tanda) (17) fandmark = >, (ld projon(g (dy? (18) deD(f) _ 2 2freg = ||Wgeoml|l” + I Weolorll* + |lwexpr ll (19) where the silhouette loss ¢,;;pouette iterates over points contained in the ground truth face region Trace: but not in the predicted face region Iface, to bring the points closer to the SDF zero level set. ArcFace [Deng et al. 2019a] measures the face similarity between different views and I,gnq is a predicted render from a randomly perturbed camera pose. D is an off-the-shelf facial landmark detector [King 2009] and dis the ground truth facial landmark UV mapping enforced in Eq. 11. Note that our consistent UV parameterization directly enables correspondences for the facial landmark alignment loss fandmark; Fig. 10 demonstrates the benefits of incorporating this loss. The regularization loss f,eg is important to ensure that the optimized codes stay near the manifold of the pre-trained latent space for expression animation. We obtain face masks using a pre-trained BiSeNet [Yu et al. 2018] and optimize for 800 steps. Fine-tuning. To reconstruct finer details in the input image, we freeze the latent codes after optimization and fine-tune the model weights on the above losses. We omit the silhouette loss, as we find it tends to bloat the geometry when the model weights are unfrozen. Although fine-tuning the model improves reconstruction quality, it may also hinder its capability for animation or novel view synthesis. Therefore, we only perform model fine-tuning for 60 steps. 4 RESULTS We present results of our proposed method with comparisons to EMOCA [Danééek et al. 2022], ROME [Khakhulin et al. 2022] and FaceVerse [Wang et al. 2022b], three recent mesh-based approaches for single-shot 3D avatar generation, and HeadNeRF [Hong et al. 2022], an implicit approach using neural radiance fields. Our method achieves higher fidelity texture and geometry reconstruction in the facial region compared to the baselines. Qualitatively and quantitatively, our method also demonstrates more faithful expression and pose transfer between in-the-wild source and target images. Finally, our learned texture map is intuitive to edit and propagates naturally during animation. 4.1 Implementation Details Our model is trained in two stages. In the first stage, we withhold the ground truth multi-view images, as we find that supervising with both texture maps and multi-view images negatively impacts the model’s ability to learn a consistent UV mapping. In the second stage, --- --SIGGRAPH Conference Proceedings, Aug 6-10, 2023 Connor Z. Lin, Koki Nagano, Jan Kautz, Eric R. Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, and Sameh Khamis Input Image —_De-lit Image Ours HeadNeRF ROME EMOCA Ours HeadNeRF ROME EMOCA Target Expression and Pose Reconstruction Expression and Pose Transfer Fig. 5. Single-shot reconstruction on FFHQ with expression and pose transfer. On the left, we show the input FFHQ source image, de-lit input image using LUMOS [Yeh et al. 2022], and reconstruction results for each method. On the right, we show monocular performance capture and retargeting, where we reconstruct and transfer the expression and pose from a target image (right-most column) to the source image identity (left-most column). On the left from top to bottom, original images are courtesy of José Carlos Cortizo Pérez/filckr, Montclair Film/flickr, Pham Toan/flickr, Javier Morales/flickr, Khiet Nguyen/flickr, and Malcolm Slaney/flickr. On the right from top to bottom, original images are courtesy of Adam Charnock/flickr, Daughterville Festival/flickr, Delaney Turner/flickr, South African Tourism/flickr, Pat (Cletch) Williams/flickr, and Collision Conf/flickr. Table 2. Quantitative results on single-shot in-the-wild reconstruction (left) and self-expression retargeting (right). Left: image, pose, and identity metrics are computed on 500 images sampled from FFHQ. Depth metrics are computed on the H3DS dataset. Image, identity, and depth metrics are computed only on the facial region. EMOCA is evaluated using its smaller face crop. Right: FACS coefficients and facial landmarks are computed after expression and pose transfer on 32 expression pairs sampled from the Triplegangers test split. Reconstruction LPIPS| DISTS! SSIMT Posel IDTt Li RMSE Depth) Depth] Retargeting FACS] Facial EMOCA 0.1122 0.1268 0.9182 0.0681 0.0697 | 0.0300 0.0677 Landmarks ROME 0.1054 0.1130 0.9317 0.0600 0.3866 | 0.0237 0.0513 EMOCA 4.712 0.HeadNeRF 0.1090 0.1199 0.9268 0.0606 0.2334 | 0.0379 0.0695, ROME 3.204 0.Ours (optimization-free) 0.1427 0.1465 0.9053 0.0549 ~—-0.1082 | 0.0357 0.0658 HeadNeRF 3.848 0.Ours (encoder-free) 0.0890 0.0921 0.9441 0.0533 0.4600 | 0.0241 0.0527 Ours 1.733 0.Ours 0.0879 0.0905 0.9451 0.0563 0.4670 | 0.0228 0.Table 3. Quantitative comparison with FaceVerse [Wang et al. 2022b] on poses for in-the-wild FFHQ images using Deep3DFaceRecon [Deng 500 sampled FFHQ images for single-shot in-the-wild reconstruction. et al. 2019b]. We perform sphere tracing for 50 steps per ray and Reconstruction LPIPS| DISTS| SSIMT use a dimensionality of 512 for the geometry, color, and expression latent codes. We train our AutoDecoder for 1000 epochs (approx. one FaceVerse 0.1280 0.1119 0.9126 . . Ours 0.0879 0.0905 0.9451 week) and our inversion encoder for 200 epochs (approx. one day) across 8 NVIDIA A40 GPUs. We use a Triplegangers training/test we freeze the UV networks {g, g} and supervise using the multi- split of 386/129 for the quantitative expression
