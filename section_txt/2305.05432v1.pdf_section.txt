--- ABSTRACT ---
Webpages have been a rich resource for language and vision-language tasks. Yet only ieces of webpages are kept: image-caption airs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data underused. To study multimodal webpage understanding, we introduce he Wikipedia Webpage 2M (WikiWeb2M suite; the first to retain the full set of images, text, and structure data available in a age. WikiWeb2M can be used for tasks like age description generation, section summarization, and contextual image captioning. Keywords: Multimodal Data, Webpages, Machine Learning, Text Generation, Vision and Language
