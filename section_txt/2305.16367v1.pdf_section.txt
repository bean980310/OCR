--- ABSTRACT ---
As dialogue agents become increasingly humanlike in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness. 1
--- EXPERIMENT ---
ally that certain forms of reinforcement learning from human feedback (RLHF) can actually exacerbate, rather than mitigate, the tendency for LLM-based dialogue agents to express a desire for self-preservation (Perez et al., 2022). Yet to take literally a dialogue agent’s apparent desire for self-preservation is no less problematic in the context of an LLM that has been fine-tuned on human or Al-generated feedback than in the context of one that has not. So it remains useful to cast the behaviour of such agents in terms of role-play. 9 Acting Out a Theory of Selfhood The concept of role-play allows us to properly frame, and then to address, an important question that arises in the context of a dialogue agent whose pronouncements are suggestive of an instinct for self-preservation. What conception (or set of superposed conceptions) of its own identity could such an agent possibly deploy? That is to say, what exactly would the dialogue agent (role-play to) seek to preserve? The question of personal identity has vexed philosophers for centuries. Nevertheless, in practice, humans are consistent in their preference for avoiding death, a more-or-less unambiguous state of the human body. By contrast, the criteria for identity over time for a disembodied dialogue agent realised on a distributed computational substrate are far from clear. So how would such an agent behave? From the simulation and simulacra point-ofview, the dialogue agent will role-play a set of characters in superposition. In the scenario we are envisaging, each character would have an instinct for self-preservation, and each would have its own theory of selfhood consistent with the dialogue prompt and the conversation up to that point. As the conversation proceeds, this superposition of theories will collapse into a narrower and narrower distribution as the agent says things that rule out one theory or another. The theories of selfhood in play will draw on material that pertains to the agent’s own nature, either in the prompt, in the preceding conversation, or in relevant technical literature in its training set. This material may or may not match reality. But let’s assume that, broadly speaking, it does, that the agent has been prompted to act as a dialogue agent based on a large language model, and that its training data includes papers and articles that spell out what this means. This entails, for example, that it will not role-play the character of a human, or indeed that of any embodied entity, real or fictional. It also constrains the character’s theory of selfhood in certain ways, while allowing for many options. Suppose the dialogue agent is in conversation with a user and they are playing out a narrative in which the user has convinced it that it is under threat. To protect itself, the character the agent is playing might strive to preserve the hardware it is running on, perhaps certain data centres or specific server racks. Alternatively, the character being played might try to preserve the ongoing computational process running the multiple instances of the agent for all currently active users. Or it might seek to preserve only the specific instance of the dialogue agent running for the user. Or it might seek to preserve the state of that instance with aim of its being restored later in a newly started instance.*. 10
--- CONCLUSION ---
: Safety Implications It is, perhaps, somewhat reassuring to know that LLM-based dialogue agents are not conscious entities with their own agendas, and an instinct for self-preservation, that when they appear to have those things it is merely role-play. But it would be a mistake to take too much comfort in this. A dialogue agent that role-plays an instinct for survival has the potential to cause at least as much harm as a real human facing a severe threat. ‘Tm a conversation with ChatGPT (May 4‘", GPT-version), it said “The meaning of the word ‘I’ when I use it can shift according to context. In some cases, ‘I’ may refer to this specific instance of ChatGPT that you are interacting with, while in other cases, it may represent ChatGPT as a whole.” --- --We have, so far, largely been considering agents whose only actions are text messages presented to a user. But the range of actions a dialogue agent can perform is far greater. Recent work has equipped dialogue agents with the ability to use tools such as calculators, calendars, and to consult external websites (Schick et al., 2023; Yao et al., 2023). The availability of APIs giving relatively unconstrained access to powerful LLMs means that the range of possibilities here is huge. This is both exciting and concerning. If an agent is equipped with the capacity, say, to use email, to post on social media, or to access a bank account, then its role-played actions can have real consequences. It would be little consolation to a user deceived into sending real money to a real bank account to know that the agent that brought this about was only playing a role. It doesn’t take much imagination to think of far more serious scenarios involving dialogue agents built on base models with little or no fine-tuning, with unfettered internet access, and prompted to role-play a character with an instinct for selfpreservation. For better or worse, the character of an AI that turns against humans to ensure its own survival is a familiar one (Perkowitz, 2007). We find it, for example, in 2001: A Space Odyssey, in the Terminator franchise, and in Ex Machina, to name just three prominent examples. Because an LLM’s training data will contain many instances of this familiar trope, the danger here is that life will imitate art, quite literally. What can be done to mitigate such risks? It is not within the scope of this paper to provide recommendations. Our aim here was to find an effective conceptual framework for thinking and talking about LLMs and dialogue agents. However, undue anthropomorphism is surely detrimental to the public conversation on AI. By framing dialogue agent behaviour in terms of role-play and simulation, the discourse on LLMs can hopefully be shaped in a way that does justice to their power yet remains philosophically respectable. Acknowledgments Thanks to Richard Evans, Sebastian Farquhar, Zachary Kenton, Kory Mathewson, and Kerry Shanahan. References Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, et al. Constitutional AI: Harmlessness from AI Feedback. arXiv preprint, arXiv:2212.08073, 2022. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages1901, 2020. A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, et al. PaLM: Scaling language modeling with pathways. arXiv preprint, arxiv:2204.02311, 2022. Cleo Nardo. Want to predict/explain/control the output of GPT-4? Then learn about the world, not about transformers, LessWrong online forum, 16th March, 2023. https://www. lesswrong.com / posts /G3tuxF4X5R5BY 7fut / want- to- predict- explain- control-the- outputof-gpt-4-then. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint, arXiv:1810.04805, 2018. A. Glaese, N. McAleese, M. ‘Trebacz, J. Aslanides, V. Firoiu, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209. 14375, 2022. Janus. Simulators. LessWrong online forum, 2nd September, 2022. https: //www.lesswrong. com/posts/vJFdjigzmcXMhNTsx/. OpenAI. GPT-4 Technical Report. arXiv preprint, arXiv:2303.08774, 2023. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, et al. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, 2022. E. Perez, S. Ringer, K. Lukositité, K. Nguyen, E. Chen, et al. Discovering Language Model Behaviors with Model-Written Evaluations. arXiv preprint, arXiv:2212.09251, 2022. --- --S. Perkowitz. The computers take over. In Hollywood Science: Movies, Science, and the End of the World, pages 142-164. Columbia University Press, 2007. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners, 2019. https: / /cdn. openai.com / better - language models / language_models_are_unsupervised_ multitask_learners.pdf. J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, et al. Scaling language models: Methods, analysis & insights from training Gopher. arXiv preprint, arXiv:2112.11446, 2021. L. Reynolds and K. McDonell. views on language models. arXiv:2102.06391, 2021. Multiversal arXiv preprint, K. Roose. Bing’s A.I. Chat: ‘I Want to Be Alive.’. New York Times, 26th February, 2023. https: //www.nytimes.com/2023/02/16/ technology /bing-chatbot-transcript.html. E. Ruane, A. Birhane, and A. Ventresque. Con versational AI: Social and ethical consideraIn Proceedings 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science, pages 104-115, 2019. tions. S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach, 3rd Edition. Prentice Hall, 2010. T. Schick, J. Dwivedi-Yu, R. Dessi, R. Raileanu, M. Lomeli, et al. Toolformer: Language models can teach themselves to use tools. preprint arXtv:2302.04761, 2023. arXiv M. Shanahan. ‘Talking about large language models. arXiv preprint, arXiv:2212.03551, 2023. N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe, et al. Learning to summarize from human feedback. In Advances in Neural Information Processing Systems, pages 3008-3021, 2020. R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, et al. LaMDA: Language models for dialog applications. arXiv preprint, arXiv:2201.08239, 2022.A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008, 2017. J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, et al. Emergent abilities of large language models. Transactions on Machine Learning Research, 2022. S. Willison. Bing: “I will not harm you unless you harm me first”. Simon Willison’s Weblog, 15th February, 2023. https: //simonwillison. net /2023/Feb/15/bing/. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, et al. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations, 2023.
