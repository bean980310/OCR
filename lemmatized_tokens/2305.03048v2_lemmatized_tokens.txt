abstract large language model llm capture procedural knowledge world recent work leveraged llm ability generate abstract plan simplify challenging control task either action scoring action modeling however transformer architecture inherits several constraint make difficult llm directly serve agent limited inut length inefficiency bias incompatibility nonext environment maintain compatibility trainable actor propose instead use knowledge llm simplify control problem rather solving propose plan eliminate track pet framework plan module transates task description list eliminate module mask irrelevant object receptacle observation current finally track module determines whether agent accomplished alf world instruction following benchmark pet framework lead significant improvement sota generalization human goal specification introduction human abstractly plan everyday task without execution example given task make breakfast roughly plan first pick mug make coffee grabbing egg scramble embodied agent endowed capability generalize effectively leveraging reasoning carnegie mellon university ariel university microsoft research nvidia research correspondence yue wu heat apple put fridge eliminate take apple see apple action pickup apple update progress finished taking apple j figure pet framework plan module us llm generate plan eliminate module us qa model mask irrelevant object observation track module us qa model track completion recent work huang et b ahn et yao et used llm bommasani et abstract planning embodied gaming agent shown incipient success extracting procedural world knowledge llm linguistic orm posthoc alignment executable action environment however treat llm acor focus adapting llm output executable action either micheli fleuret constraint ahn et using llm actor work environment imited interaction huang et ahn et consisting object imits generalization modality addition scenario considered largely simplified real world ahn et al provides available object possible interaction start imits task set provided huang et al limit environment object single table hand successfully cut lettuce room one find knife since multiple drawer cabinet chaplot et min et blukis et realistic scenario lead plan eliminate track diverse complicated set task large changing action space furthermore text description observation increase function number receptacle object agent see combined growing state becomes verbose fi llm work explore alternative mechanism leverage prior knowledge encoded llm withow impacting trainable nature actor propose framework figure plan eliminate track pet plan module simplifies complex task breaking us pretrained llm generate list inpu task description employing example prompt training set similar huang et al ahn et al eliminate module address challenge long observation us qa language model score mask object receptacle irrelevant current track module us qa language model determine current complete move next finally action attention agen us architecture accommodate long variable length action space agent observes masked observation take action conditioned current focus instruction following indoor household alfworld shridhar et interactive text environment benchmark experiment analysis demonstrate llm remove taskirrelevant object observation qa also generate accuracy addition multiple llm may used coordination assist agent different aspect contribution follows pet novel framework leveraging pretrained llm embodied agent work show p e serf complementary role simultaneously addressed tackle control task action attention agent handle changing action space text environment improvement sota generalization human goal via planning tracking related work language conditioned policy considerable portion prior work study imitation learning tellex et mei et nair et stepputtis et jang et shridhar et sharma et reinforcement learning misra et jiang et cideron et goyal et nair et akakzia et policy conditioned natural language instruction goal macmahon et kollar et prior research used language embeddings improve generalization new instruction nair et lack domain knowledge captured llm pet framework enables planning progress tracking observation filtering use llm designed compatible language conditional policy llm control llm recently achieved success planning huang et al show llm generate plausible plan task generated directly executed control environment ahn et al solves executability issue training action scoring model llm action choice demonstrates success robot however llm score work simple environment action limited ahn et fails environment object diverse action shridhar et song et al us generate lowlevel command executed respective control policy work improves ahn et al action diversity addition llm require demonstration example making length prompt infeasible alfworld micheli fleuret model expert trajectory alfworld demonstrated impressive evaluation result however lm requires fully environment consistent expert trajectory fully action space requirement greatly limit generalization domain even form task specification show pet framework achieves better generalization human goal specification agent trained hierarchical planning natural language due structured nature natural language andreas et al explored associating task description modular later work extend approach using single conditional policy mei et matching template oh et recent work shown llm proficient planner huang et ahn et lin et therefore motivates u revisit idea hierarchical task plan plan eliminate track ning progress tracking knowledge pet first work combining llm planner llm progress tracker conditional policy text game game complex interactive simulation game state action space natural lanugage fertile ground machine learning research addition language understanding successful play requires skill like memory planning exploration trial error common sense alfworld shridhar et simulator extends common game simulator textworld coté et al create analog alfred scene agent large action space et al learns representation state action two different model computes q function inner product representation could generalize large action space considered small number action fulda et al ahn et al explore action elimination setting affordances zahavy et al train model eliminate invalid action zork external environment signal however functionality depends existence external elimination signal plan eliminate track section explain framework plan eliminate track pet plan module mp llm generates list input task description using sample training set example eliminate module mg us qa language model score mask object receptacle irrelevant current track module mr us zeroshot qa language model determine current complete move next note plan generative task eliminate track classification task also implement agent action attention score permissible action trained imitation learning expert agen observes masked observation take action conditioned current problem setting define task description observation string time step list permissible action executed observation string define example training set planning module target output figure plan module generation full example chosen training set based roberta embedding similarity task query description example concatenated task query get prompt finally prompt llm generate desired receptacle object within observation rf respectively classification receptacle object defined environment shridhar et task j assume exists list sy solves plan task real world often complex need one step completed motivated ability human plan given complex task design plan module mp generate list task description inspired contextual prompting technique planning llm huang et use llm plan module mp given task description j compose query question middle step required jt require mp generate list sy sx specifically select top example task training set based roberta liu et embedding similarity query task concatenate example task example format build prompt pz mp fig concat qre srp ore spe illustration prompt format shown figure heat apple put fridge q middle step required put two spraybottles toilet srp take spraybottle plan eliminate track place spraybottle toilet take spraybottle lace spraybottle toilet expected list achieve task take apple heat apple place apple ridge middle room looking quickly around see cabinet cabinet cabinet cabinet cabinet coffeemachine countertop countertop diningtable drawer drawer drawer drawer drawer fridge garbagecan sinkbasin microwave task heat apple put fridge go middle room looking quickly around see cabinet cabinet cabinet cabinet cabinet countertop countertop diningtable fridge figure eliminate module receptacle masking use qa model filter irrelevant observation scene see original observation long receptacle shown red relevant task completion receptacle filtered qa model making observation shorter eliminate typical alfworld scene start around receptacle containing object case around receptacle kitchen many cabinet drawer easily take agent prior knowledge step agent find desired object repeating process visiting receptacle opening closing observe many receptacle object irrelevant specific asks training evaluation e easily filtered knowledge task example fig task heat apple removing irrelevant receptacle like coffeemachine garbagecan object like knife could significantly shorten observation herefore propose leverage commonsense knowledge captured large qa model design eliminate module mg mask irrelevant receptacle object task create prompt format p ask go receptacle p task object relevant object using qa model meg manner compute score flo mag object pr mg po ri receptacle r observation every belief score whether qa model belief object relevant remove observation fio remove jp threshold hyperparameters environment middle room looking quickly around see garbagecan sinkbasin toaster go sinkbasinon sinkbasin see nothing go diningtableon diningtable see apple bread cup peppershaker take apple diningtable subtasks take apple heat apple place apple fridge context diningtable see apple bread take apple diningtable finish task take apple tracking module update progress tracker figure track module progress tracking every step take last step context append query whether current completed get prompt qa model generates answer prompt answer yes update tracker next track agent utilize plan first need know execute human actor typically start first item task one one completion therefore similar section use qa model design track module mr perform completion specifically illustrated figure list sr keep track progress tracker p initialized indicates agent currently working sp compose context last step agent observation current system design allow finished agent mean recover undoes previous test time plan eliminate track current question finish task efficiency set min step note reset whenever progress tracker update hence emplate p concat finish task feed p zeroshot qa model compute probability oken yes follows paz yes pag pa ie pate yes pa pate pa hen increment tracker p track next subask f tracking end prematurely meaning p len environment returned done fall back conditioning study rate end section term precision recall agent since number permissible action vary lot environment agent need handle arbitrary dimension action space shridhar et al address challenge generating action generation process lead degenerate performance even training set draw inspiration field text summarizaion model built handle variable input length see et al generates summary pointing mechanism extract output word word similarly pointing model could used select action list permissible action action attention interested learning policy output optimal action among permissible action eschew long large action space problem representing observation averaging history individually encoding action fig proposed action attention ramework first represent historical observation h average embeddings individual observation history eq h list embeddings current permissible action eq eq compute query q using transformer query head mg task embedding h current observation embedding list action embeddings h eq compute key k action using transformer key head mx task embedding h current observation embedding embedding action finally compute query key action score policy eq avg jc embed embed embed q mo embed h embed h k mx embed h embed embed softmax q k permissible action experiment result present experiment follows first explain environment setup baseline experiment compare pet baseline different split environment finally conduct ablation study analyze pet framework part part show pet generalizes better human goal specification efficient behavior cloning training experimental detail alfworld environment alfworld shridhar et set textworld environment coté et parallel alfred embodied dataset shridhar et alfworld includes task type require solving multiple compositional training task instance tasktype object receptacle room evaluation task instance seen split task novel take place room seen training evaluation task instance unseen split task take place novel room example task could rinse egg put training instance alfworld come expert collected training demonstration human goal specification human goal specification evaluation contain unseen verb unseen noun shridhar et comparison template goal use way goal specification addition sentence structure human goal specification diverse compared template goal therefore human goal experiment good testing generalization model scenario lm plan module generation experimented black et industryscale llm parameter smith et plan eliminate track ot al ay middle go diningtable go microwave room looking diningtable open goto examine heat quickly around see apple microwave cabinet cabinet applesee cabinet mug cup cabinet take apple diningtable heat et x v j za j gsc k k n action attenti heat applei figure agent action attention action attention block framework computes key k permissible action output action score key query q observation template goal specification model seen unseen seen unseen butler dagger shridhar et bc shridhar et gpt micheli fleuret action attention comparison different model term completion rate per evaluation split seen unseen without human annotated goal pet gpt template goal specification generalizes better eliminate module masking choose tafjord clark reported common sense qa performance par brown et eing order magnitude smaller use decision hreshold macaw score object masked track module progress racking use model eliminate module answer question actor model design action attention agent mg mx transformer head hidden dimension last layer fed two linear head generate k q embedding action observation use liu et emedding dimension generation use training generated plan module evaluation experimental setup unlike original benchmark shridhar et experiment model trained behavior cloning although shridhar et al observe model benefit greatly dagger training dagger assumes expert possible state inefficient impractical experiment training slower dagger compared behavior cloning human goal specification include performance butler dagger completeness row trained without interaction environment mle gpt behavior cloning pet week dagger hour behavior cloning addition demonstrate model surpass dagger training performance butler shridhar et agent trained dagger even agent option interact environment baseline first baseline butler butler agent shridhar et consists encoder aggregator decoder time step encoder take initial observation current observation task string stas generates representation r recurrent aggregator combine r last recurrent state produce h decoded string representing action addition butler agent us beam search get stuck condition event failed action second baseline gpt micheli fleuret demonstration alfworld training set specifically gpt generate action step mimic expert using standard maximum likelihood loss human goal specification plan eliminate track overall result template human goal compare performance action attention assisted pet butler shridhar et gpt micheli fleuret table human goal specification pet outperforms sota gpt seen unseen split although pet gpt template goal specification gpt requires fully textbased expert trajectory thus loses adaptability different environment setting qualitatively human goal specification task goal specification gpt often get stuck repeating action producing single wrong move hand since plan module pet trained task generalizes variation human goal specification shown section quantitatively gpt suffers relative performance drop transferring template specification whereas pet incurs drop setting closest pet butler behavior cloning butler bc since butler bc per orms poorly also include dagger training result nevertheless action attention assisted pet outperorms butler dagger eing much efficient section ablation plan eliminate track n table analyze contribution pet module sequentially adding component action attention agent training trajectory samled training set data set size chosen match size seen validation set efficient sparse setting note treat plan track single module ablation since hey work separately adding plan track greatly improves compleion rate relatively provides evidence hypothesis solving embodied task reduces complexity observe relatively insignificant improvement absolute performance adding eliminate without tracking hand applying eliminate plan track observe relaive improvement plan track alone herefore deduce plan track boost performance eliminate evaluation since easier remove irrelevant object objective focuset automated analysis pet module plan module experiment different llm radford et black et parameter smith et model table report generation accuracy roberta liu et embedding cosine similarity observe llm achieve high accuracy template goal specification variation sentence structure human goal specification generates subtasks similar ground truth term embedding similarity smaller model perform significantly worse eliminate module evaluate masking performance macaw hree split alfworld fig illustrate auc curve relevance score model assigns object object exper interacted completing task since macaw qa model queried manner demonstrates consistent masking performance al hree split environment even unseen lit addition note object receptacle accuacy generally lower object accuracy spawning location described section experiment decision threshol ow recall reduces number bjects observation average track module since alignment information provided environment explore alternative performance metric detection event completion ideally tracker shoul record last shed environment fully solved expert agreement measure report precision recall precision recall larger mode precise miss detection therefore limiting theoretical performance smaller model much less accurate according human evaluation limit overall model performance theory experiment fin model produce similar overall result may suggest overall result could improve llm better precision recall qualitative analysis plan module show two type failure example generation table first type error caused generating synonym ground truth second type error caused inaccu plan eliminate track template goal human goal llm seen unseen seen unseen radford et black et smith et table evaluation different llm plan module term accuracy roberta embedding cosine similarity bracket per evaluation split seen unseen without human annotated goal parameter achieves overall best performance dataset split greatly exceeds performance smaller model hard task human goal specification addition generates almost perfect embedding similarity task roc curve train receptacle identification roc curve receptacle identification roc curve receptacle identification bos ba ozoo oo oo oa oa o positive rate falze positive rate false positive rate roc curve train object identification roc curve object identification roc curve object identification box eon oz oz oz oo oo oo oa oa positive rate false positive rate false positive rete figure plot auc score relevance identification across task environment model ground truth obtained accessed expert top receptacle relevance identification bottom object relevance identification qa model achieves average score receptacle object incorrectly mask receptacle contains object model ablation seen unseen action attention attention eliminate attention plan track attention pet comparison different ablation pet trained sampled set demonstration training set term completion rate per evaluation split seen unseen applying eliminate module alone insignificant effect overall performance compared plan track however applying eliminate module together plan track result much significant performance improvement racies human goal specification note action attention framework us roberta liu et embedding known robust synonym variation eliminate module observe main source elimination error occurs module interest agent fails find receptacle often object simulator spawn according common sense noted documentation environment object like apple egg chance spawning unexpected receptacle like garbagecan tvstand however generation unlikely real deployment thus mistake eliminate module reasonable hat subfor task track module experimentally find task particularly helpful require counting procedure shown table pet break task place two soapbar cabinet two repeating set take soapbar cabinet planning tracking therefore simplify hard problem counting plan eliminate track human goal specification example task chill cup place cabinet ar cool mug coffeemachine gen chill mug coffeemachine task take pencil desk put side desk gt take pencil shelf gen white pencil another spot desk table failure example plan module human goal specification task gt generated gen first example generated plan differs ground truth meaning agrees second example generated plan largely differs ground truth due mistake human goal specification another side desk instead shelf conclusion limitation future work n work propose plan eliminate track pet framework us llm assist embodied agent three step pet framework requires designed compatible embodied agent n experiment combine pet novel acion attention agent handle dynamic action space alfworld action attention agent greatly outperforms butler baseline addition since pet framework trained fit training set asks demonstrates better generalization unseen human goal specification task finally ablation study show plan track module together imrove performance eliminate module achieve best performance result show llm good source common sense procedural knowledge embodied agent multiple llm may used coordination improve effectiveness one major limitation current system design track module progress tracker finished example agent executing picked pan put pan countertop picked pan put fridge undo pickup action since progress tracker take consideration previous progress undone system may break situation future work focus adding dynamic replanning address limitation explore way llm assist learning pick white pencil policy reading instruction manual environment reference ahn brohan brown chebotar cortes david finn fu gopalakrishnan hausman herzog ho hsu ibarz ichter irpan jang ruano jeffrey jesmonth joshi julian kalashnikov kuang lee levine lu luu parada pastor quiambao rao rettinghouse reyes sermanet sievers tan toshev vanhoucke xia xiao xu xu yan zeng say grounding language robotic affordances url http akakzia cola oudeyer chetouani sigaud grounding language skill via goal generation arxiv preprint andreas klein levine modular multitask reinforcement learning policy sketch international conference machine learning pp pmlr black gao wang leahy biderman large scale autoregressive language modeling march url http use software please cite using metadata blukis paxton fox garg artzi persistent spatial semantic representation highlevel natural language instruction execution url http bommasani hudson adeli altman r arora von arx bernstein bohg bosselut brunskill brynjolfsson buch card castellon chatterji chen creel davis demszky donahue doumbouya durmus ermon etchemendy ethayarajh finn gale gillespie goel goodman grossman guha hashimoto henderson hewitt ho hong hsu huang icard jain jurafsky kalluri karamcheti keeling khani khattab koh krass krishna kuditipudi kumar ladhak lee lee leskovec levent li li plan eliminate track malik manning mirchandani mitchell munyikwa nair narayan narayanan newman nie niebles nilforoshan nyarko ogut orr papadimitriou l park piech portelance potts raghunathan reich ren rong roohani ruiz ryan ré sadigh sagawa santhanam shih srinivasan tamkin taori thomas tramér wang wang wu wu wu xie yasunaga zaharia zhang zhang zhang zhang zheng zhou liang opportunity risk foundation model url http brown mann ryder subbiah kaplan dhariwal neelakantan shyam sastry askell et al language model learner advance neural information processing system chaplot gandhi gupta salakhutdinov object goal navigation using semantic exploration url http cideron seurin strub pietquin higher improving instruction following hindsight generation experience replay ieee symposium series computational intelligence ssci pp ieee coté kadar yuan kybartas barnes fine moore hausknecht asri adada et al textworld learning environment game workshop computer game pp springer coté kadar yuan kybartas barnes fine moore hausknecht asri adada et al textworld learning environment game workshop computer game pp springer fulda rick murdoch wingate rock affordance extraction via word embeddings arxiv preprint goyal niekum mooney guiding reinforcement learning using natural language mapping pixel reward conference robot learning pp pmlr chen gao li deng ostendorf deep reinforcement learning natural language action space arxiv preprint huang abbeel pathak mordatch language model planner extracting actionable knowledge embodied agent url http huang xia xiao chan liang florence zeng tompson mordatch chebotar sermanet brown jackson luu levine hausman ichter b inner monologue embodied reasoning planning language model url http jang irpan khansari kappler ebert lynch levine finn zeroshot task generalization robotic imitation learning conference robot learning pp pmlir jiang gu murphy finn language abstraction hierarchical deep reinforcement learning advance neural information processing system kollar tellex roy roy toward understanding natural language direction international conference humanrobot interaction hri pp ieee lin huang liu gu sommerer ren x grounded planning embodied task language model arxiv preprint liu ott goyal du joshi chen levy lewis zettlemoyer stoyanov roberta robustly optimized bert pretraining approach arxiv preprint macmahon stankiewicz kuiper b walk talk connecting language knowledge action route instruction def mei bansal walter listen attend walk neural mapping navigational instruction action sequence thirtieth aaai conference artificial intelligence micheli fleuret language model fewshot butler arxiv preprint min chaplot ravikumar bisk salakhutdinov film following instruction language modular experiment analysis demonstrate llm remove taskirrelevant object observation qa also generate accuracy addition multiple llm may used coordination assist agent different aspect contribution follows pet novel framework leveraging pretrained llm embodied agent work show p e serf complementary role simultaneously addressed tackle control task action attention agent handle changing action space text environment improvement sota generalization human goal via planning tracking related work language conditioned policy considerable portion prior work study imitation learning tellex et mei et nair et stepputtis et jang et shridhar et sharma et reinforcement learning misra et jiang et cideron et goyal et nair et akakzia et policy conditioned natural language instruction goal macmahon et kollar et prior research used language embeddings improve generalization new instruction nair et lack domain knowledge captured llm pet framework enables planning progress tracking observation filtering use llm designed compatible language conditional policy llm control llm recently achieved success planning huang et al show llm generate plausible plan task generated directly executed control environment ahn et al solves executability issue training action scoring model llm action choice demonstrates success robot however llm score work simple environment action limited ahn et fails environment object diverse action shridhar et song et al us generate lowlevel command executed respective control policy work improves ahn et al action diversity addition llm require demonstration example making length prompt infeasible alfworld micheli fleuret model expert trajectory alfworld demonstrated impressive evaluation result however lm requires fully environment consistent expert trajectory fully action space requirement greatly limit generalization domain even form task specification show pet framework achieves better generalization human goal specification agent trained hierarchical planning natural language due structured nature natural language andreas et al explored associating task description modular later work extend approach using single conditional policy mei et matching template oh et recent work shown llm proficient planner huang et ahn et lin et therefore motivates u revisit idea hierarchical task plan plan eliminate track ning progress tracking knowledge pet first work combining llm planner llm progress tracker conditional policy text game game complex interactive simulation game state action space natural lanugage fertile ground machine learning research addition language understanding successful play requires skill like memory planning exploration trial error common sense alfworld shridhar et simulator extends common game simulator textworld coté et al create analog alfred scene agent large action space et al learns representation state action two different model computes q function inner product representation could generalize large action space considered small number action fulda et al ahn et al explore action elimination setting affordances zahavy et al train model eliminate invalid action zork external environment signal however functionality depends existence external elimination signal plan eliminate track section explain framework plan eliminate track pet plan module mp llm generates list input task description using sample training set example eliminate module mg us qa language model score mask object receptacle irrelevant current track module mr us zeroshot qa language model determine current complete move next note plan generative task eliminate track classification task also implement agent action attention score permissible action trained imitation learning expert agen observes masked observation take action conditioned current problem setting define task description observation string time step list permissible action executed observation string define example training set planning module target output figure plan module generation full example chosen training set based roberta embedding similarity task query description example concatenated task query get prompt finally prompt llm generate desired receptacle object within observation rf respectively classification receptacle object defined environment shridhar et task j assume exists list sy solves plan task real world often complex need one step completed motivated ability human plan given complex task design plan module mp generate list task description inspired contextual prompting technique planning llm huang et use llm plan module mp given task description j compose query question middle step required jt require mp generate list sy sx specifically select top example task training set based roberta liu et embedding similarity query task concatenate example task example format build prompt pz mp fig concat qre srp ore spe illustration prompt format shown figure heat apple put fridge q middle step required put two spraybottles toilet srp take spraybottle plan eliminate track place spraybottle toilet take spraybottle lace spraybottle toilet expected list achieve task take apple heat apple place apple ridge middle room looking quickly around see cabinet cabinet cabinet cabinet cabinet coffeemachine countertop countertop diningtable drawer drawer drawer drawer drawer fridge garbagecan sinkbasin microwave task heat apple put fridge go middle room looking quickly around see cabinet cabinet cabinet cabinet cabinet countertop countertop diningtable fridge figure eliminate module receptacle masking use qa model filter irrelevant observation scene see original observation long receptacle shown red relevant task completion receptacle filtered qa model making observation shorter eliminate typical alfworld scene start around receptacle containing object case around receptacle kitchen many cabinet drawer easily take agent prior knowledge step agent find desired object repeating process visiting receptacle opening closing observe many receptacle object irrelevant specific asks training evaluation e easily filtered knowledge task example fig task heat apple removing irrelevant receptacle like coffeemachine garbagecan object like knife could significantly shorten observation herefore propose leverage commonsense knowledge captured large qa model design eliminate module mg mask irrelevant receptacle object task create prompt format p ask go receptacle p task object relevant object using qa model meg manner compute score flo mag object pr mg po ri receptacle r observation every belief score whether qa model belief object relevant remove observation fio remove jp threshold hyperparameters environment middle room looking quickly around see garbagecan sinkbasin toaster go sinkbasinon sinkbasin see nothing go diningtableon diningtable see apple bread cup peppershaker take apple diningtable subtasks take apple heat apple place apple fridge context diningtable see apple bread take apple diningtable finish task take apple tracking module update progress tracker figure track module progress tracking every step take last step context append query whether current completed get prompt qa model generates answer prompt answer yes update tracker next track agent utilize plan first need know execute human actor typically start first item task one one completion therefore similar section use qa model design track module mr perform completion specifically illustrated figure list sr keep track progress tracker p initialized indicates agent currently working sp compose context last step agent observation current system design allow finished agent mean recover undoes previous test time plan eliminate track current question finish task efficiency set min step note reset whenever progress tracker update hence emplate p concat finish task feed p zeroshot qa model compute probability oken yes follows paz yes pag pa ie pate yes pa pate pa hen increment tracker p track next subask f tracking end prematurely meaning p len environment returned done fall back conditioning study rate end section term precision recall agent since number permissible action vary lot environment agent need handle arbitrary dimension action space shridhar et al address challenge generating action generation process lead degenerate performance even training set draw inspiration field text summarizaion model built handle variable input length see et al generates summary pointing mechanism extract output word word similarly pointing model could used select action list permissible action action attention interested learning policy output optimal action among permissible action eschew long large action space problem representing observation averaging history individually encoding action fig proposed action attention ramework first represent historical observation h average embeddings individual observation history eq h list embeddings current permissible action eq eq compute query q using transformer query head mg task embedding h current observation embedding list action embeddings h eq compute key k action using transformer key head mx task embedding h current observation embedding embedding action finally compute query key action score policy eq avg jc embed embed embed q mo embed h embed h k mx embed h embed embed softmax q k permissible action experiment result present experiment follows first explain environment setup baseline experiment compare pet baseline different split environment finally conduct ablation study analyze pet framework part part show pet generalizes better human goal specification efficient behavior cloning training experimental detail alfworld environment alfworld shridhar et set textworld environment coté et parallel alfred embodied dataset shridhar et alfworld includes task type require solving multiple compositional training task instance tasktype object receptacle room evaluation task instance seen split task novel take place room seen training evaluation task instance unseen split task take place novel room example task could rinse egg put training instance alfworld come expert collected training demonstration human goal specification human goal specification evaluation contain unseen verb unseen noun shridhar et comparison template goal use way goal specification addition sentence structure human goal specification diverse compared template goal therefore human goal experiment good testing generalization model scenario lm plan module generation experimented black et industryscale llm parameter smith et plan eliminate track ot al ay middle go diningtable go microwave room looking diningtable open goto examine heat quickly around see apple microwave cabinet cabinet applesee cabinet mug cup cabinet take apple diningtable heat et x v j za j gsc k k n action attenti heat applei figure agent action attention action attention block framework computes key k permissible action output action score key query q observation template goal specification model seen unseen seen unseen butler dagger shridhar et bc shridhar et gpt micheli fleuret action attention comparison different model term completion rate per evaluation split seen unseen without human annotated goal pet gpt template goal specification generalizes better eliminate module masking choose tafjord clark reported common sense qa performance par brown et eing order magnitude smaller use decision hreshold macaw score object masked track module progress racking use model eliminate module answer question actor model design action attention agent mg mx transformer head hidden dimension last layer fed two linear head generate k q embedding action observation use liu et emedding dimension generation use training generated plan module evaluation experimental setup unlike original benchmark shridhar et experiment model trained behavior cloning although shridhar et al observe model benefit greatly dagger training dagger assumes expert possible state inefficient impractical experiment training slower dagger compared behavior cloning human goal specification include performance butler dagger completeness row trained without interaction environment mle gpt behavior cloning pet week dagger hour behavior cloning addition demonstrate model surpass dagger training performance butler shridhar et agent trained dagger even agent option interact environment baseline first baseline butler butler agent shridhar et consists encoder aggregator decoder time step encoder take initial observation current observation task string stas generates representation r recurrent aggregator combine r last recurrent state produce h decoded string representing action addition butler agent us beam search get stuck condition event failed action second baseline gpt micheli fleuret demonstration alfworld training set specifically gpt generate action step mimic expert using standard maximum likelihood loss human goal specification plan eliminate track overall result template human goal compare performance action attention assisted pet butler shridhar et gpt micheli fleuret table human goal specification pet outperforms sota gpt seen unseen split although pet gpt template goal specification gpt requires fully textbased expert trajectory thus loses adaptability different environment setting qualitatively human goal specification task goal specification gpt often get stuck repeating action producing single wrong move hand since plan module pet trained task generalizes variation human goal specification shown section quantitatively gpt suffers relative performance drop transferring template specification whereas pet incurs drop setting closest pet butler behavior cloning butler bc since butler bc per orms poorly also include dagger training result nevertheless action attention assisted pet outperorms butler dagger eing much efficient section ablation plan eliminate track n table analyze contribution pet module sequentially adding component action attention agent training trajectory samled training set data set size chosen match size seen validation set efficient sparse setting note treat plan track single module ablation since hey work separately adding plan track greatly improves compleion rate relatively provides evidence hypothesis solving embodied task reduces complexity observe relatively insignificant improvement absolute performance adding eliminate without tracking hand applying eliminate plan track observe relaive improvement plan track alone herefore deduce plan track boost performance eliminate evaluation since easier remove irrelevant object objective focuset automated analysis pet module plan module experiment different llm radford et black et parameter smith et model table report generation accuracy roberta liu et embedding cosine similarity observe llm achieve high accuracy template goal specification variation sentence structure human goal specification generates subtasks similar ground truth term embedding similarity smaller model perform significantly worse eliminate module evaluate masking performance macaw hree split alfworld fig illustrate auc curve relevance score model assigns object object exper interacted completing task since macaw qa model queried manner demonstrates consistent masking performance al hree split environment even unseen lit addition note object receptacle accuacy generally lower object accuracy spawning location described section experiment decision threshol ow recall reduces number bjects observation average track module since alignment information provided environment explore alternative performance metric detection event completion ideally tracker shoul record last shed environment fully solved expert agreement measure report precision recall precision recall larger mode precise miss detection therefore limiting theoretical performance smaller model much less accurate according human evaluation limit overall model performance theory experiment fin model produce similar overall result may suggest overall result could improve llm better precision recall qualitative analysis plan module show two type failure example generation table first type error caused generating synonym ground truth second type error caused inaccu plan eliminate track template goal human goal llm seen unseen seen unseen radford et black et smith et table evaluation different llm plan module term accuracy roberta embedding cosine similarity bracket per evaluation split seen unseen without human annotated goal parameter achieves overall best performance dataset split greatly exceeds performance smaller model hard task human goal specification addition generates almost perfect embedding similarity task roc curve train receptacle identification roc curve receptacle identification roc curve receptacle identification bos ba ozoo oo oo oa oa o positive rate falze positive rate false positive rate roc curve train object identification roc curve object identification roc curve object identification box eon oz oz oz oo oo oo oa oa positive rate false positive rate false positive rete figure plot auc score relevance identification across task environment model ground truth obtained accessed expert top receptacle relevance identification bottom object relevance identification qa model achieves average score receptacle object incorrectly mask receptacle contains object model ablation seen unseen action attention attention eliminate attention plan track attention pet comparison different ablation pet trained sampled set demonstration training set term completion rate per evaluation split seen unseen applying eliminate module alone insignificant effect overall performance compared plan track however applying eliminate module together plan track result much significant performance improvement racies human goal specification note action attention framework us roberta liu et embedding known robust synonym variation eliminate module observe main source elimination error occurs module interest agent fails find receptacle often object simulator spawn according common sense noted documentation environment object like apple egg chance spawning unexpected receptacle like garbagecan tvstand however generation unlikely real deployment thus mistake eliminate module reasonable hat subfor task track module experimentally find task particularly helpful require counting procedure shown table pet break task place two soapbar cabinet two repeating set take soapbar cabinet planning tracking therefore simplify hard problem counting plan eliminate track human goal specification example task chill cup place cabinet ar cool mug coffeemachine gen chill mug coffeemachine task take pencil desk put side desk gt take pencil shelf gen white pencil another spot desk table failure example plan module human goal specification task gt generated gen first example generated plan differs ground truth meaning agrees second example generated plan largely differs ground truth due mistake human goal specification another side desk instead shelf conclusion limitation future work n work propose plan eliminate track pet framework us llm assist embodied agent three step pet framework requires designed compatible embodied agent n experiment combine pet novel acion attention agent handle dynamic action space alfworld action attention agent greatly outperforms butler baseline addition since pet framework trained fit training set asks demonstrates better generalization unseen human goal specification task finally ablation study show plan track module together imrove performance eliminate module achieve best performance result show llm good source common sense procedural knowledge embodied agent multiple llm may used coordination improve effectiveness one major limitation current system design track module progress tracker finished example agent executing picked pan put pan countertop picked pan put fridge undo pickup action since progress tracker take consideration previous progress undone system may break situation future work focus adding dynamic replanning address limitation explore way llm assist learning pick white pencil policy reading instruction manual environment reference ahn brohan brown chebotar cortes david finn fu gopalakrishnan hausman herzog ho hsu ibarz ichter irpan jang ruano jeffrey jesmonth joshi julian kalashnikov kuang lee levine lu luu parada pastor quiambao rao rettinghouse reyes sermanet sievers tan toshev vanhoucke xia xiao xu xu yan zeng say grounding language robotic affordances url http akakzia cola oudeyer chetouani sigaud grounding language skill via goal generation arxiv preprint andreas klein levine modular multitask reinforcement learning policy sketch international conference machine learning pp pmlr black gao wang leahy biderman large scale autoregressive language modeling march url http use software please cite using metadata blukis paxton fox garg artzi persistent spatial semantic representation highlevel natural language instruction execution url http bommasani hudson adeli altman r arora von arx bernstein bohg bosselut brunskill brynjolfsson buch card castellon chatterji chen creel davis demszky donahue doumbouya durmus ermon etchemendy ethayarajh finn gale gillespie goel goodman grossman guha hashimoto henderson hewitt ho hong hsu huang icard jain jurafsky kalluri karamcheti keeling khani khattab koh krass krishna kuditipudi kumar ladhak lee lee leskovec levent li li plan eliminate track malik manning mirchandani mitchell munyikwa nair narayan narayanan newman nie niebles nilforoshan nyarko ogut orr papadimitriou l park piech portelance potts raghunathan reich ren rong roohani ruiz ryan ré sadigh sagawa santhanam shih srinivasan tamkin taori thomas tramér wang wang wu wu wu xie yasunaga zaharia zhang zhang zhang zhang zheng zhou liang opportunity risk foundation model url http brown mann ryder subbiah kaplan dhariwal neelakantan shyam sastry askell et al language model learner advance neural information processing system chaplot gandhi gupta salakhutdinov object goal navigation using semantic exploration url http cideron seurin strub pietquin higher improving instruction following hindsight generation experience replay ieee symposium series computational intelligence ssci pp ieee coté kadar yuan kybartas barnes fine moore hausknecht asri adada et al textworld learning environment game workshop computer game pp springer coté kadar yuan kybartas barnes fine moore hausknecht asri adada et al textworld learning environment game workshop computer game pp springer fulda rick murdoch wingate rock affordance extraction via word embeddings arxiv preprint goyal niekum mooney guiding reinforcement learning using natural language mapping pixel reward conference robot learning pp pmlr chen gao li deng ostendorf deep reinforcement learning natural language action space arxiv preprint huang abbeel pathak mordatch language model planner extracting actionable knowledge embodied agent url http huang xia xiao chan liang florence zeng tompson mordatch chebotar sermanet brown jackson luu levine hausman ichter b inner monologue embodied reasoning planning language model url http jang irpan khansari kappler ebert lynch levine finn zeroshot task generalization robotic imitation learning conference robot learning pp pmlir jiang gu murphy finn language abstraction hierarchical deep reinforcement learning advance neural information processing system kollar tellex roy roy toward understanding natural language direction international conference humanrobot interaction hri pp ieee lin huang liu gu sommerer ren x grounded planning embodied task language model arxiv preprint liu ott goyal du joshi chen levy lewis zettlemoyer stoyanov roberta robustly optimized bert pretraining approach arxiv preprint macmahon stankiewicz kuiper b walk talk connecting language knowledge action route instruction def mei bansal walter listen attend walk neural mapping navigational instruction action sequence thirtieth aaai conference artificial intelligence micheli fleuret language model fewshot butler arxiv preprint min chaplot ravikumar bisk salakhutdinov film following instruction language modular method plan eliminate track misra langford artzi mapping instruction visual observation action reinforcement learning arxiv preprint nair mitchell chen savarese finn et al learning robot behayior offline data annotation conference robot learning pp pmlr oh singh lee kohli task generalization deep reinforcement learning international conference machine learning pp pmlr radford wu child luan amodei sutskever language model unsupervised multitask learner see liu manning get point summarization network arxiv preprint sharma torralba andreas j skill induction planning latent language arxiv preprint shridhar thomason gordon bisk han mottaghi zettlemoyer fox alfred benchmark interpreting grounded instruction everyday task proceeding conference computer vision pattern recognition pp shridhar yuan coté bisk trischler hausknecht alfworld aligning text embodied environment interactive learning arxiv preprint shridhar manuelli fox cliport pathway robotic manipulation conference robot learning pp pmlr smith patwary norick legresley rajbhandari casper liu prabhumoye zerveas korthikanti zheng child r aminabadi bernauer song shoeybi houston tiwary catanzaro b using deepspeed megatron train nlg generative language model corr url http song wu washington sadler chao su grounded planning embodied agent large language model arxiv preprint stepputtis campbell phielipp lee baral ben amor imitation learning robot manipulation task advance neural information processing system tafjord clark macaw arxiv preprint tellex kollar dickerson walter banerjee teller roy understanding natural language command robotic navigation mobile manipulation proceeding aaai conference artificial intelligence volume pp yao rao hausknecht narasimhan keep calm explore language model action generation game url http zahavy haroush merlis mankowitz mannor learn learn action elimination deep reinforcement learning advance neural information processing system